

<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>

</head>

<body>

    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637135571616662500%26DBCODE%3dCJFD%26TABLEName%3dCJFDTEMP%26FileName%3dDZCL201919038%26RESULT%3d1%26SIGN%3dRZBbIZWE4WWgvGxYu6mvcf5%252fplE%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=DZCL201919038&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=DZCL201919038&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>


    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZCL201919038&amp;v=MjAwMDlqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVadEZ5bmxXN3pOSVRmSVlyRzRIOWpOcG85R2JJUUtESDg0dlI0VDY=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#49" data-title="&lt;b&gt;0 引  言&lt;/b&gt; "><b>0 引  言</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#56" data-title="&lt;b&gt;1 基于MSP的多人姿态估计&lt;/b&gt; "><b>1 基于MSP的多人姿态估计</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#60" data-title="&lt;b&gt;1.1 人体检测模块&lt;/b&gt;"><b>1.1 人体检测模块</b></a></li>
                                                <li><a href="#73" data-title="&lt;b&gt;1.2 人体姿态估计模块&lt;/b&gt;"><b>1.2 人体姿态估计模块</b></a></li>
                                                <li><a href="#87" data-title="&lt;b&gt;1.3 人体姿态优化模块&lt;/b&gt;"><b>1.3 人体姿态优化模块</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#112" data-title="&lt;b&gt;2 实验与结果分析&lt;/b&gt; "><b>2 实验与结果分析</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#113" data-title="&lt;b&gt;2.1 实验细节&lt;/b&gt;"><b>2.1 实验细节</b></a></li>
                                                <li><a href="#118" data-title="&lt;b&gt;2.2 实验结果分析&lt;/b&gt;"><b>2.2 实验结果分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#128" data-title="&lt;b&gt;3 结  论&lt;/b&gt; "><b>3 结  论</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#58" data-title="图1 MSP算法流程">图1 MSP算法流程</a></li>
                                                <li><a href="#62" data-title="图2 Mask RCNN网络结构">图2 Mask RCNN网络结构</a></li>
                                                <li><a href="#75" data-title="图3 沙漏网络">图3 沙漏网络</a></li>
                                                <li><a href="#77" data-title="图4 Simple Pose网络">图4 Simple Pose网络</a></li>
                                                <li><a href="#78" data-title="图5 残差模块">图5 残差模块</a></li>
                                                <li><a href="#80" data-title="&lt;b&gt;表1 ResNet50网络配置&lt;/b&gt;"><b>表1 ResNet50网络配置</b></a></li>
                                                <li><a href="#90" data-title="图6 PoseFix算法流程">图6 PoseFix算法流程</a></li>
                                                <li><a href="#93" data-title="图7 4 种错误类型的表现形式">图7 4 种错误类型的表现形式</a></li>
                                                <li><a href="#120" data-title="&lt;b&gt;表2 人体检测器的性能对人体姿态估计的影响&lt;/b&gt;"><b>表2 人体检测器的性能对人体姿态估计的影响</b></a></li>
                                                <li><a href="#123" data-title="&lt;b&gt;表3 人体姿态优化模块对多人姿态估计的影响&lt;/b&gt;"><b>表3 人体姿态优化模块对多人姿态估计的影响</b></a></li>
                                                <li><a href="#126" data-title="&lt;b&gt;表4 在COCO test-dev 上本文算法与其他算法的比较&lt;/b&gt;"><b>表4 在COCO test-dev 上本文算法与其他算法的比较</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" title=" 唐心宇,宋爱国.人体姿态估计及在康复训练情景交互中的应用[J].仪器仪表学报,2018,39(11):195-203." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=DKFX&amp;filename=YQXB201811028&amp;v=MDg2NDk5bk5ybzlIYklRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVp0RnlubFc3ek1QRHpUYkxHNEg=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         唐心宇,宋爱国.人体姿态估计及在康复训练情景交互中的应用[J].仪器仪表学报,2018,39(11):195-203.
                                    </a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" title=" NEWELL A,YANG K,DENG J.Stacked hourglass networks for human pose estimation[C].European Conference on Computer Vision,2016:483-499." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Stacked Hourglass Networks for Human Pose Estimation">
                                        <b>[2]</b>
                                         NEWELL A,YANG K,DENG J.Stacked hourglass networks for human pose estimation[C].European Conference on Computer Vision,2016:483-499.
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title=" WEI S,RAMAKRISHNA V,KANADE T,et al.Convolutional pose machines[C].IEEE Conference on Computer Vision and Pattern Recognition,2016:4724-4732." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Convolutional pose machines">
                                        <b>[3]</b>
                                         WEI S,RAMAKRISHNA V,KANADE T,et al.Convolutional pose machines[C].IEEE Conference on Computer Vision and Pattern Recognition,2016:4724-4732.
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" title=" CHEN Y L,WANG ZH CH,PENG Y X,et al.Cascaded pyramid network for multi-person pose estimation[C].IEEE/CVF Conference on Computer Vision and Pattern Recognition,2018:7103-7112." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Cascaded pyramid network for multi-person pose estimation">
                                        <b>[4]</b>
                                         CHEN Y L,WANG ZH CH,PENG Y X,et al.Cascaded pyramid network for multi-person pose estimation[C].IEEE/CVF Conference on Computer Vision and Pattern Recognition,2018:7103-7112.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" title=" FANG H,XIE S,TAI Y,et al.RMPE:regional multi-person pose estimation[C].IEEE International Conference on Computer Vision,2017:2353-2362." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=RMPE:Regional Multi-person Pose Estimation">
                                        <b>[5]</b>
                                         FANG H,XIE S,TAI Y,et al.RMPE:regional multi-person pose estimation[C].IEEE International Conference on Computer Vision,2017:2353-2362.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" title=" PAPANDREOU G,ZHU T,KANAZAWA N,et al.Towards accurate multi-person pose estimation in the wild[C].IEEE Conference on Computer Vision and Pattern Recognition,2017:3711-3719." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Towards accurate multi-person pose estimation in the wild">
                                        <b>[6]</b>
                                         PAPANDREOU G,ZHU T,KANAZAWA N,et al.Towards accurate multi-person pose estimation in the wild[C].IEEE Conference on Computer Vision and Pattern Recognition,2017:3711-3719.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" title=" XIAO B,WU H P,WEI Y CH.Simple baselines for human pose estimation and tracking[C].European Conference on Computer Vision,2018:472-487." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Simple Baselines for Human Pose Estimation and Tracking">
                                        <b>[7]</b>
                                         XIAO B,WU H P,WEI Y CH.Simple baselines for human pose estimation and tracking[C].European Conference on Computer Vision,2018:472-487.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" title=" CAO Z,SIMON T,WEI S,et al.Realtime multi-person 2d pose estimation using part affinity fields[C].IEEE Conference on Computer Vision and Pattern Recognition,2017:1302-1310." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Realtime multi-person 2d pose estimation using part affinity fields">
                                        <b>[8]</b>
                                         CAO Z,SIMON T,WEI S,et al.Realtime multi-person 2d pose estimation using part affinity fields[C].IEEE Conference on Computer Vision and Pattern Recognition,2017:1302-1310.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" title=" NEWELL A,HUANG Z,DENG J.Associative embedding:End-to-end learning for joint detection and grouping[C].31st Conference on Neural Information Processing System,2017:2277-2287." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Associative embedding End-to-end learning for joint detection and grouping">
                                        <b>[9]</b>
                                         NEWELL A,HUANG Z,DENG J.Associative embedding:End-to-end learning for joint detection and grouping[C].31st Conference on Neural Information Processing System,2017:2277-2287.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_10" title=" REN S,HE K,GIRSHICK R,et al.Faster R-CNN:towards real-time object detection with region proposal networks[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2017,39(6):1137-1149." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Faster R-CNN:towards real-time object detection with region proposal networks">
                                        <b>[10]</b>
                                         REN S,HE K,GIRSHICK R,et al.Faster R-CNN:towards real-time object detection with region proposal networks[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2017,39(6):1137-1149.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_11" title=" LIN T Y,DOLL&#193;R P,GIRSHICK R,et al.Feature pyramid networks for object detection[C].IEEE Conference on Computer Vision and Pattern Recognition,2017:936-944." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Feature pyramid networks for object detection">
                                        <b>[11]</b>
                                         LIN T Y,DOLL&#193;R P,GIRSHICK R,et al.Feature pyramid networks for object detection[C].IEEE Conference on Computer Vision and Pattern Recognition,2017:936-944.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_12" title=" PENG CH,XIAO T,LI Z,et al.MegDet:a large mini-batch object detector[C].IEEE/CVF Conference on Computer Vision and Pattern Recognition,2018:6181-6189." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=MegDet:A Large Mini-Batch Object Detector">
                                        <b>[12]</b>
                                         PENG CH,XIAO T,LI Z,et al.MegDet:a large mini-batch object detector[C].IEEE/CVF Conference on Computer Vision and Pattern Recognition,2018:6181-6189.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_13" title=" 周晓彦,王珂,李凌燕.基于深度学习的目标检测算法综述[J].电子测量技术,2017,40(11):89-93." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZCL201711020&amp;v=MjEzNjJmSVlyRzRIOWJOcm85SFpJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVadEZ5bmxXN3pNSVQ=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[13]</b>
                                         周晓彦,王珂,李凌燕.基于深度学习的目标检测算法综述[J].电子测量技术,2017,40(11):89-93.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_14" title=" CARREIRA J,AGRAWAL P,FRAGKIADAKI K,et al.Human pose estimation with iterative error feedback[C].IEEE Conference on Computer Vision and Pattern Recognition,2016:4733-4742." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Human Pose Estimation with Iterative Error Feedback">
                                        <b>[14]</b>
                                         CARREIRA J,AGRAWAL P,FRAGKIADAKI K,et al.Human pose estimation with iterative error feedback[C].IEEE Conference on Computer Vision and Pattern Recognition,2016:4733-4742.
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_15" title=" HE K,GKIOXARI G,DOLL&#193;R P,et al.Mask R-CNN[C].IEEE International Conference on Computer Vision,2017:2980-2988." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Mask R-CNN">
                                        <b>[15]</b>
                                         HE K,GKIOXARI G,DOLL&#193;R P,et al.Mask R-CNN[C].IEEE International Conference on Computer Vision,2017:2980-2988.
                                    </a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_16" title=" MOON G,CHANG J Y,LEE K M.PoseFix:model-agnostic general human pose refinement network[C].IEEE Conference on Computer Vision and Pattern Recognition,2019:1812.03595." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=PoseFix model-agnostic general human pose refinement network">
                                        <b>[16]</b>
                                         MOON G,CHANG J Y,LEE K M.PoseFix:model-agnostic general human pose refinement network[C].IEEE Conference on Computer Vision and Pattern Recognition,2019:1812.03595.
                                    </a>
                                </li>
                                <li id="35">


                                    <a id="bibliography_17" title=" LIN T,MAIRE M,BELONGIE S,et al.Microsoft coco:Common objects in context[C].European Conference on Computer Vision,2014:740-755." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Microsoft COCO:Common Objects in Context">
                                        <b>[17]</b>
                                         LIN T,MAIRE M,BELONGIE S,et al.Microsoft coco:Common objects in context[C].European Conference on Computer Vision,2014:740-755.
                                    </a>
                                </li>
                                <li id="37">


                                    <a id="bibliography_18" title=" HE K,ZHANG X,REN S,et al.Deep residual learning for image recognition[C].IEEE Conference on Computer Vision and Pattern Recognition,2016:770-778." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep residual learning for image recognition">
                                        <b>[18]</b>
                                         HE K,ZHANG X,REN S,et al.Deep residual learning for image recognition[C].IEEE Conference on Computer Vision and Pattern Recognition,2016:770-778.
                                    </a>
                                </li>
                                <li id="39">


                                    <a id="bibliography_19" title=" RONCHI M R,PERONA P.Benchmarking and error diagnosis in multi-instance pose estimation[C].IEEE International Conference on Computer Vision,2017:369-378." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Benchmarking and error diagnosis in multi-instance pose estimation">
                                        <b>[19]</b>
                                         RONCHI M R,PERONA P.Benchmarking and error diagnosis in multi-instance pose estimation[C].IEEE International Conference on Computer Vision,2017:369-378.
                                    </a>
                                </li>
                                <li id="41">


                                    <a id="bibliography_20" title=" SUN X,XIAO B,LIANG S,et al.Integral human pose regression[C].European Conference on Computer Vision,2018:536-553." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Integral human pose regression">
                                        <b>[20]</b>
                                         SUN X,XIAO B,LIANG S,et al.Integral human pose regression[C].European Conference on Computer Vision,2018:536-553.
                                    </a>
                                </li>
                                <li id="43">


                                    <a id="bibliography_21" title=" ABADI M,AGARWAL A,BARHAM P,et al.TensorFlow:large-scale machine learning on heterogeneous systems[C].2015:1-19." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=TensorFlow:large-scale machine learning on heterogeneous systems">
                                        <b>[21]</b>
                                         ABADI M,AGARWAL A,BARHAM P,et al.TensorFlow:large-scale machine learning on heterogeneous systems[C].2015:1-19.
                                    </a>
                                </li>
                                <li id="45">


                                    <a id="bibliography_22" title=" DENG J,DONG W,SOCHER R,et al.ImageNet:a large-scale hierarchical image database[C].IEEE Conference on Computer Vision and Pattern Recognition,2009:248-255." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=ImageNet:A large-scale hierarchical image database">
                                        <b>[22]</b>
                                         DENG J,DONG W,SOCHER R,et al.ImageNet:a large-scale hierarchical image database[C].IEEE Conference on Computer Vision and Pattern Recognition,2009:248-255.
                                    </a>
                                </li>
                                <li id="47">


                                    <a id="bibliography_23" title=" KINGMA D,BA J.Adam:a method for stochastic optimization[C].International Conference on Learning Representations,2014:1-15." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Adam:A method for stochastic optimization">
                                        <b>[23]</b>
                                         KINGMA D,BA J.Adam:a method for stochastic optimization[C].International Conference on Learning Representations,2014:1-15.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=DZCL" target="_blank">电子测量技术</a>
                2019,42(19),79-84 DOI:10.19651/j.cnki.emt.1902945            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于MSP的多人姿态估计算法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%9F%A9%E5%AD%9D%E5%A4%A9&amp;code=43369873&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">韩孝天</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E4%B8%87%E6%97%BA%E6%A0%B9&amp;code=08537469&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">万旺根</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%8A%E6%B5%B7%E5%A4%A7%E5%AD%A6%E9%80%9A%E4%BF%A1%E4%B8%8E%E4%BF%A1%E6%81%AF%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0017580&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">上海大学通信与信息工程学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%8A%E6%B5%B7%E5%A4%A7%E5%AD%A6%E6%99%BA%E6%85%A7%E5%9F%8E%E5%B8%82%E7%A0%94%E7%A9%B6%E9%99%A2&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">上海大学智慧城市研究院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>基于2D图像的多人姿态估计是理解人类行为的一项基础技术,同时也极具挑战性。近年来,由于卷积神经网络的快速发展,多人姿态估计的准确率得到了显著提升。但与此同时,算法和模型的复杂度也不断提高,这使得算法的研究和应用变得更加困难。本文通过研究人体检测的准确性、人体姿态估计的优化对多人姿态估计准确率的影响,提出了一种简单有效的自上而下的多人姿态估计算法,它由3个模块组成,人体检测、人体姿态估计以及人体姿态优化。在公共数据集上进行了实验分析,结果表明该算法可以明显提高多人姿态估计的准确性。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%A4%9A%E4%BA%BA%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">多人姿态估计;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%BA%BA%E4%BD%93%E6%A3%80%E6%B5%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">人体检测;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%BA%BA%E4%BD%93%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">人体姿态估计;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%BA%BA%E4%BD%93%E5%A7%BF%E6%80%81%E4%BC%98%E5%8C%96&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">人体姿态优化;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    韩孝天,硕士研究生,主要研究方向为基于深度学习的多人姿态估计,计算机视觉,E-mail:872219259@qq.com;;
                                </span>
                                <span>
                                    万旺根,教授,博士生导师,主要研究方向为计算机图形学、信号处理和数据挖掘等,E-mail:wanwg@staff.shu.edu.cn;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-04-11</p>

                    <p>

                            <b>基金：</b>
                                                        <span>上海市科学技术委员会港澳台科技合作项目(NO.18510760300)资助;</span>
                    </p>
            </div>
                    <h1><b>Multi-person pose estimation algorithm based on MSP</b></h1>
                    <h2>
                    <span>Han Xiaotian</span>
                    <span>Wan Wanggen</span>
            </h2>
                    <h2>
                    <span>School of Communication and Information Engineering, Shanghai University</span>
                    <span>Institute of Smart City, Shanghai University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Multi-person pose estimation based on 2D images is a basic technology for understanding human behavior, and it is also very challenging. In recent years, due to the rapid development of convolutional neural networks, the accuracy of multi-person pose estimation has been significantly improved. At the same time, however, the complexity of algorithms and models is increasing, which makes the research and application of algorithms more difficult. A simple and effective top-down multi-person pose estimation algorithm is proposed by studying the accuracy of human detection and the influence of the optimization of human pose estimation on the accuracy of multi-person pose estimation. It consists of three modules, the human detection, human pose estimation, and human pose optimization. The experimental analysis is carried out on the public dataset. The results show that the proposed algorithm can significantly improve the accuracy of multi-person pose estimation.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=multi-person%20pose%20estimation&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">multi-person pose estimation;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=human%20detection&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">human detection;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=human%20pose%20estimation&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">human pose estimation;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=human%20pose%20optimization&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">human pose optimization;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2019-04-11</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="49" name="49" class="anchor-tag"><b>0 引  言</b></h3>
                <div class="p1">
                    <p id="50">人体姿态估计是指在给定的图像或视频中对人体的关键点进行检测,最终输出人体形状的过程。它是计算机视觉中研究人体行为、人物跟踪等问题的研究基础。目前,由于深度学习以及各种硬件设备的突破性发展,人体姿态估计在日常生活中的应用也越来越广泛,比如人机交互、智能监控、影视娱乐、康复训练<citation id="150" type="reference"><link href="3" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>等。</p>
                </div>
                <div class="p1">
                    <p id="51">多人姿态估计是目前人体姿态估计领域的研究重点, 因为它不仅存在单人姿态估计<citation id="151" type="reference"><link href="5" rel="bibliography" /><link href="7" rel="bibliography" /><sup>[<a class="sup">2</a>,<a class="sup">3</a>]</sup></citation>中的复杂背景、视角变化、衣着变化等问题,还存在人数未知,相邻人员明显重叠等问题,因此多人姿态估计具有更大的挑战性。此外,现实生活中,多人场景更加常见,研究多人姿态估计也更具实用价值。</p>
                </div>
                <div class="p1">
                    <p id="52">多人姿态估计具有两种主流方法:自上而下<citation id="152" type="reference"><link href="9" rel="bibliography" /><link href="11" rel="bibliography" /><link href="13" rel="bibliography" /><link href="15" rel="bibliography" /><sup>[<a class="sup">4</a>,<a class="sup">5</a>,<a class="sup">6</a>,<a class="sup">7</a>]</sup></citation>和自下而上<citation id="153" type="reference"><link href="17" rel="bibliography" /><link href="19" rel="bibliography" /><sup>[<a class="sup">8</a>,<a class="sup">9</a>]</sup></citation>。自上而下的方法首先使用目标检测算法得到人体边界框,然后在每个边界框中使用单人姿态估计算法检测人体关键点,并连接成人形。自下而上的方法先检测出图像中所有人的关键点,然后将检测到的关键点分配给不同的人。这两种方法各有优缺点。在自上而下的方法中,由于可利用性能较好的目标检测算法以及单人姿态估计算法,所以人体姿态估计的准确性较高,但该方法会受到边界框的质量影响,从而导致误检、冗余等。在自下而上的方法中,由于不需要检测人体的边界框,所以检测的速度较快,但当两个人或更多的人靠得很近时,组合不同人的姿态会产生歧义。</p>
                </div>
                <div class="p1">
                    <p id="53">在自上而下的方法中,多人姿态估计算法常采用二阶段Faster RCNN<citation id="154" type="reference"><link href="21" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>作为人体检测器,FPN<citation id="155" type="reference"><link href="23" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>和MegDet<citation id="156" type="reference"><link href="25" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>等性能更好的目标检测算法也有一定的使用。目标检测算法大致可以分为二阶段检测算法和一阶段检测算法。两者的主要区别是前者需要先生成候选框,再进行检测,而后者直接进行检测。由于两者方法的差异,前者检测精度较高,后者检测速度较快<citation id="157" type="reference"><link href="27" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="54">人体姿态估计中,多数算法采用不断细化估计的关键点,来提高姿态估计的准确性。Newell等<citation id="158" type="reference"><link href="3" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>提出了一种端到端的可训练的多阶段网络,每个阶段通过端到端的学习来细化前一阶段的姿态估计结果。Carreira等<citation id="159" type="reference"><link href="29" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>提出了一种利用迭代误差反馈的方法来训练卷积神经网络,通过多阶段的训练把预测的结果重新输入到网络中,从而对预测结果进行细化。Chen等<citation id="160" type="reference"><link href="9" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>提出了一种由两个基本网络组成的级联金字塔网络,前一个网络用于预测简单的关键点,后一个网络用于预测较难检测的关键点。这些算法,虽然基于不同的细化网络,但都改善了人体姿态估计的结果。</p>
                </div>
                <div class="p1">
                    <p id="55">本文采用自上而下的方法,基于以上的研究,提出了一种具有3个模块的多人姿态估计算法:使用基于Mask RCNN<citation id="161" type="reference"><link href="31" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>的目标检测算法,获得高质量的人体边界框;基于Simple Pose<citation id="162" type="reference"><link href="15" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>的人体姿态估计算法对每个边界框中的人进行姿态估计;基于PoseFix<citation id="163" type="reference"><link href="33" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>的姿态优化算法对多人姿态估计的结果进一步优化。本文在公共数据集COCO<citation id="164" type="reference"><link href="35" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>上进行实验研究,通过消融实验和对比实验,表明本文提出的算法可以有效提高多人姿态估计的准确性。</p>
                </div>
                <h3 id="56" name="56" class="anchor-tag"><b>1 基于MSP的多人姿态估计</b></h3>
                <div class="p1">
                    <p id="57">本文提出的多人姿态估计算法包含3个模块,人体检测、人体姿态估计以及人体姿态优化,算法的整体流程图如图1所示。</p>
                </div>
                <div class="area_img" id="58">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/DZCL201919038_058.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 MSP算法流程" src="Detail/GetImg?filename=images/DZCL201919038_058.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 MSP算法流程  <a class="btn-zoomin" href="Detail/GetImg?filename=images/DZCL201919038_058.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="59">输入图片首先通过人体检测器,得到图片中每个人的边界框,然后利用人体姿态估计器对每个边界框中的人分别进行单人姿态估计,最后将前一步得到的人姿态估计结果输入到人体姿态优化器中,输出优化后的人体姿态。在训练阶段,3个模块需要单独训练,其中人体姿态优化模块,在训练时需要使用之前得到的人体姿态估计结果。下面详细介绍各个模块的具体内容。</p>
                </div>
                <h4 class="anchor-tag" id="60" name="60"><b>1.1 人体检测模块</b></h4>
                <div class="p1">
                    <p id="61">本文使用Mask RCNN作为人体检测器,它是通过Faster RCNN改进而来,主要是在Faster RCNN基础上添加一个用于检测掩码的分支,同时用RoI Align代替原算法中的RoI Pooling,以获得更好的定位效果。Mask RCNN的网络结构如图2所示。</p>
                </div>
                <div class="area_img" id="62">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/DZCL201919038_062.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 Mask RCNN网络结构" src="Detail/GetImg?filename=images/DZCL201919038_062.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 Mask RCNN网络结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/DZCL201919038_062.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="63">Mask RCNN是1个两阶段目标检测器,阶段一是候选区域生成网络(RPN),用于产生候选区域的边界框;阶段二是对候选区域进行分类和边框回归,并同时为每个候选区域输出1个二进制掩码。图2中,黑色部分的结构和原始的Faster RCNN相同,红色部分是对Faster RCNN的修改。</p>
                </div>
                <div class="p1">
                    <p id="64">Mask RCNN的算法步骤为:</p>
                </div>
                <div class="p1">
                    <p id="65">1)输入图片,通过缩放等预处理操作,将图片变成固定的大小;</p>
                </div>
                <div class="p1">
                    <p id="66">2)通过主干网络提取预处理后的图片特征,生成相应的特征图;</p>
                </div>
                <div class="p1">
                    <p id="67">3)把特征图送入RPN获得候选区域并映射到特征图上;</p>
                </div>
                <div class="p1">
                    <p id="68">4)通过RoI Align对候选区域的特征进行处理,生成固定尺寸的特征图;</p>
                </div>
                <div class="p1">
                    <p id="69">5)根据候选区域的特征图进行分类,边框回归以及掩码预测。</p>
                </div>
                <div class="p1">
                    <p id="70">Mask RCNN输出分类、边框回归以及掩码,所以损失函数定义为:</p>
                </div>
                <div class="p1">
                    <p id="71"><i>L</i>=<i>L</i><sub><i>clx</i></sub>+<i>L</i><sub><i>box</i></sub>+<i>L</i><sub><i>mask</i></sub>(1)</p>
                </div>
                <div class="p1">
                    <p id="72">式中:<i>L</i><sub><i>cls</i></sub>、<i>L</i><sub><i>box</i></sub>和Faster RCNN中的定义相同。<i>L</i><sub><i>mask</i></sub>是平均二元交叉熵损失。</p>
                </div>
                <h4 class="anchor-tag" id="73" name="73"><b>1.2 人体姿态估计模块</b></h4>
                <div class="p1">
                    <p id="74">多数人体姿态估计算法,通过设计复杂的网络结构来融合不同尺度的特征信息或保持图片的分辨率,以便提高人体关键点的检测准确率。如图3所示,沙漏网络<citation id="165" type="reference"><link href="3" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>通过自底向上和自顶向下的过程来获取多尺度特征,自底向上提取特征,自顶向下融合特征,最后通过融合后的特征图进行关键点预测。虽然这些方法有效提高了人体姿态估计的准确率,但他们的网络过于复杂,不利于进一步研究和应用。</p>
                </div>
                <div class="area_img" id="75">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/DZCL201919038_075.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 沙漏网络" src="Detail/GetImg?filename=images/DZCL201919038_075.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 沙漏网络  <a class="btn-zoomin" href="Detail/GetImg?filename=images/DZCL201919038_075.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="76">本文使用Simple Pose中的人体姿态估计模块来估计人体姿态,它的网络结构如图4所示,和沙漏网络相比,它的结构非常简单,但能获得更好的姿态估计结果。两个网络中的白色方块都是残差模块,如图5所示,模块的输入先通过一个1×1的卷积的进行降维,然后输入到一个3×3的卷积中,最后再通过一个1×1的卷积的进行升维,模块的输入和升维后的输出叠加,作为模块的最终输出。</p>
                </div>
                <div class="area_img" id="77">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/DZCL201919038_077.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 Simple Pose网络" src="Detail/GetImg?filename=images/DZCL201919038_077.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 Simple Pose网络  <a class="btn-zoomin" href="Detail/GetImg?filename=images/DZCL201919038_077.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="78">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/DZCL201919038_078.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 残差模块" src="Detail/GetImg?filename=images/DZCL201919038_078.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 残差模块  <a class="btn-zoomin" href="Detail/GetImg?filename=images/DZCL201919038_078.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="79">Simple Pose的主干网络采用ResNet<citation id="166" type="reference"><link href="37" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>,然后在其Conv5_x卷积阶段,添加3个反卷积模块,每个模块由反卷积层、BN层以及ReLU层组成。反卷积层采用256个4×4的卷积核,步长为2,最后使用1×1的卷积,输出所有预测的关键点热力图。ResNet网络主要由残差模块组成,通过叠加残差模块构建深层网络,这种设计方式可以解决深层网络无法训练的问题,根据网络层数的不同,可分为ResNet50、ResNet101等,表1是ResNet50的网络配置。</p>
                </div>
                <div class="area_img" id="80">
                    <p class="img_tit"><b>表1 ResNet50网络配置</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="80" border="1"><tr><td><br />卷积层名称</td><td>输出尺寸</td><td>参数配置</td></tr><tr><td><br />Conv1</td><td>112×112</td><td>7×7,64,步长2</td></tr><tr><td rowspan="2"><br />Conv2_x</td><td rowspan="2">56×56</td><td><br />3×3,最大池化,步长2</td></tr><tr><td><br /><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>[</mo><mrow><mtable><mtr><mtd columnalign="left"><mn>1</mn><mo>×</mo><mn>1</mn><mo>,</mo><mn>6</mn><mn>4</mn></mtd></mtr><mtr><mtd columnalign="left"><mn>3</mn><mo>×</mo><mn>3</mn><mo>,</mo><mn>6</mn><mn>4</mn></mtd></mtr><mtr><mtd columnalign="left"><mn>1</mn><mo>×</mo><mn>1</mn><mo>,</mo><mn>2</mn><mn>5</mn><mn>6</mn></mtd></mtr></mtable></mrow><mo>]</mo></mrow><mo>×</mo><mn>3</mn></mrow></math></td></tr><tr><td><br />Conv3_x</td><td><br />28×28</td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>[</mo><mrow><mtable><mtr><mtd columnalign="left"><mn>1</mn><mo>×</mo><mn>1</mn><mo>,</mo><mn>1</mn><mn>2</mn><mn>8</mn></mtd></mtr><mtr><mtd columnalign="left"><mn>3</mn><mo>×</mo><mn>3</mn><mo>,</mo><mn>1</mn><mn>2</mn><mn>8</mn></mtd></mtr><mtr><mtd columnalign="left"><mn>1</mn><mo>×</mo><mn>1</mn><mo>,</mo><mn>5</mn><mn>1</mn><mn>2</mn></mtd></mtr></mtable></mrow><mo>]</mo></mrow><mo>×</mo><mn>4</mn></mrow></math></td></tr><tr><td><br />Conv4_x</td><td><br />14×14</td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>[</mo><mrow><mtable><mtr><mtd columnalign="left"><mn>1</mn><mo>×</mo><mn>1</mn><mo>,</mo><mn>2</mn><mn>5</mn><mn>6</mn></mtd></mtr><mtr><mtd columnalign="left"><mn>3</mn><mo>×</mo><mn>3</mn><mo>,</mo><mn>2</mn><mn>5</mn><mn>6</mn></mtd></mtr><mtr><mtd columnalign="left"><mn>1</mn><mo>×</mo><mn>1</mn><mo>,</mo><mn>1</mn><mspace width="0.25em" /><mn>0</mn><mn>2</mn><mn>4</mn></mtd></mtr></mtable></mrow><mo>]</mo></mrow><mo>×</mo><mn>6</mn></mrow></math></td></tr><tr><td><br />Conv5_x</td><td>7×7</td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>[</mo><mrow><mtable><mtr><mtd columnalign="left"><mn>1</mn><mo>×</mo><mn>1</mn><mo>,</mo><mn>5</mn><mn>1</mn><mn>2</mn></mtd></mtr><mtr><mtd columnalign="left"><mn>3</mn><mo>×</mo><mn>3</mn><mo>,</mo><mn>5</mn><mn>1</mn><mn>2</mn></mtd></mtr><mtr><mtd columnalign="left"><mn>1</mn><mo>×</mo><mn>1</mn><mo>,</mo><mn>2</mn><mspace width="0.25em" /><mn>0</mn><mn>4</mn><mn>8</mn></mtd></mtr></mtable></mrow><mo>]</mo></mrow><mo>×</mo><mn>3</mn></mrow></math></td></tr><tr><td><br /></td><td>1×1</td><td>平均池化, 1 000维<br />全连接层, softmax</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="81">Simple pose的算法步骤为:</p>
                </div>
                <div class="p1">
                    <p id="82">1)输入图片,根据人体边界框坐标将边界框中的区域裁剪出来并重置为固定的大小;</p>
                </div>
                <div class="p1">
                    <p id="83">2)通过主干网络提取预处理后的图片特征,生成相应的特征图;</p>
                </div>
                <div class="p1">
                    <p id="84">3)通过反卷积等操作,提高特征图的分辨率;</p>
                </div>
                <div class="p1">
                    <p id="85">4)在最终得到的特征图上运用softmax函数,生成人体各关键点热力图。</p>
                </div>
                <div class="p1">
                    <p id="86">5)根据关键点热力图获得图片中人体关键点位置,并连接成人形。</p>
                </div>
                <h4 class="anchor-tag" id="87" name="87"><b>1.3 人体姿态优化模块</b></h4>
                <div class="p1">
                    <p id="88">原始图片和包含位姿信息的图片,可以提供丰富的人体上下文信息和结构信息,即使是有错误的位姿,由于其多数关键点处在正确的位置或者只是在正确位置上的添加了一些抖动,所以也具有一定的结构信息,利用这些信息来训练网络模型,可以有效提高模型对错误姿态的鲁棒性。此外,Ronchi等<citation id="167" type="reference"><link href="39" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>的研究发现,不同算法得到的人体姿态估计中,存在相似的误差分布。基于以上的结论,可以利用原始图片和人体姿态估计中的误差分布来合成误差位姿,训练一个姿态优化模型。</p>
                </div>
                <div class="p1">
                    <p id="89">PoseFix采用了上述结论,PoseFix的作用是对输入图像中所有人体关键点的二维坐标进行细化,算法的网络结构采用Simple Pose的整体架构。在训练阶段,利用人体姿态估计中的误差分布和真实的位姿来合成不同的训练位姿,在测试阶段,将人体姿态估计结果和裁剪后的图片作为输入,输出优化后的人体姿态。PoseFix和Simple Pose的算法流程相似,不过PoseFix的输入是一组裁剪后的图片和一组需要优化的位姿,这些位姿用高斯热图表示,网络先生成热力图形式的优化位姿,然后再利用soft-argmax函数生成坐标表示的最优位姿,PoseFix算法的整体流程如图6所示。</p>
                </div>
                <div class="area_img" id="90">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/DZCL201919038_090.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 PoseFix算法流程" src="Detail/GetImg?filename=images/DZCL201919038_090.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 PoseFix算法流程  <a class="btn-zoomin" href="Detail/GetImg?filename=images/DZCL201919038_090.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="91" name="91">1)训练位姿的合成</h4>
                <div class="p1">
                    <p id="92">本文结合真实位姿和实际位姿中的误差分布来随机生成训练位姿。将位姿误差分为4种类型,抖动、反转、交换以及错失。对于没有误差的关键点,即非常接近真值的关键点,将其定义为正确类型。</p>
                </div>
                <div class="area_img" id="93">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/DZCL201919038_093.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 4 种错误类型的表现形式" src="Detail/GetImg?filename=images/DZCL201919038_093.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 4 种错误类型的表现形式  <a class="btn-zoomin" href="Detail/GetImg?filename=images/DZCL201919038_093.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="94">首先说明一些基本的定义。<i>θ</i><mathml id="130"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mi>p</mi></msubsup></mrow></math></mathml>指某个人<i>p</i>的第<i>j</i>个真值关键点,<i>j</i>′指由<i>j</i>左右反转而来,<i>p</i>′指和<i>p</i>是不相同的人,<i>d</i><mathml id="131"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>k</mi><mi>j</mi></msubsup></mrow></math></mathml>指通过KS表达式得到的从关键点<i>k</i>到<i>j</i>的距离, <i>λ</i>(<i>a</i>,<i>l</i>)是一个偏移向量,<i>a</i>指角度,<i>l</i>指长度。</p>
                </div>
                <div class="p1">
                    <p id="95">正确类型是指关键点和真值关键点间的距离非常小。合成这种类型的关键点是在<i>θ</i><mathml id="132"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mi>p</mi></msubsup></mrow></math></mathml>上添加一个偏移向量<i>λ</i>(<i>a</i>,<i>l</i>),<i>a</i>∈[0,2π),<i>l</i>∈[0,<i>d</i><mathml id="133"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mrow><mn>0</mn><mo>.</mo><mn>8</mn><mn>5</mn></mrow></msubsup></mrow></math></mathml>)。抖动类型是指关键点和真值关键点间的距离较小。合成这种类型的关键点同样是在<i>θ</i><mathml id="134"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mi>p</mi></msubsup></mrow></math></mathml>上添加一个<i>λ</i>(<i>a</i>,<i>l</i>),但此时 <i>l</i>∈[<i>d</i><mathml id="135"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mrow><mn>0</mn><mo>.</mo><mn>8</mn><mn>5</mn></mrow></msubsup></mrow></math></mathml>,<i>d</i><mathml id="136"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mrow><mn>0</mn><mo>.</mo><mn>5</mn></mrow></msubsup></mrow></math></mathml>)。错失类型是指关键点和真值关键点之间的距离很大。合成这种类型的关键点是在<i>θ</i><mathml id="137"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mi>p</mi></msubsup></mrow></math></mathml>,<i>θ</i><mathml id="138"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><msup><mi>p</mi><mo>′</mo></msup></msubsup></mrow></math></mathml>,<i>θ</i><mathml id="139"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><msup><mi>j</mi><mo>′</mo></msup><mi>p</mi></msubsup></mrow></math></mathml>或<i>θ</i><mathml id="140"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><msup><mi>j</mi><mo>′</mo></msup><msup><mi>p</mi><mo>′</mo></msup></msubsup></mrow></math></mathml>上添加一个<i>λ</i>(<i>a</i>,<i>l</i>),<i>a</i>∈[0,2π),<i>l</i>∈[<i>d</i><mathml id="141"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mrow><mn>0</mn><mo>.</mo><mn>5</mn></mrow></msubsup></mrow></math></mathml>,<i>d</i><mathml id="142"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mrow><mn>0</mn><mo>.</mo><mn>1</mn></mrow></msubsup></mrow></math></mathml>)。</p>
                </div>
                <div class="p1">
                    <p id="96">反转类型是指在姿态估计过程中对同一个人的相似部位产生了混淆,从而导致的错误。合成这种类型的关键点是在<i>θ</i><mathml id="143"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><msup><mi>j</mi><mo>′</mo></msup><mi>p</mi></msubsup></mrow></math></mathml>上添加抖动类型的错误。交换类型是指在姿态估计过程中对不同人的相同或相似部位产生了混淆,从而导致的错误。合成这种类型的关键点是在<i>θ</i><mathml id="144"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mi>p</mi></msubsup></mrow></math></mathml>或<i>θ</i><mathml id="145"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><msup><mi>p</mi><mo>′</mo></msup></msubsup></mrow></math></mathml>上添加抖动类型的错误。</p>
                </div>
                <h4 class="anchor-tag" id="97" name="97">2)优化方式的设计</h4>
                <div class="p1">
                    <p id="98">PoseFix采用由粗到细的优化方式,因为这种方式可以得到最好的优化效果。粗到细的优化是指首先输入高斯热图表示的粗糙位姿<i>P</i><sub><i>n</i></sub>,经过后续网络后,获得由one-hot向量表示的精细位姿<i>H</i><sub><i>n</i></sub>,然后再通过<i>H</i><sub><i>n</i></sub>输出由关键点坐标表示的更精细位姿<i>C</i><sub><i>n</i></sub>。</p>
                </div>
                <div class="p1">
                    <p id="99">输入的粗糙位姿<i>P</i><sub><i>n</i></sub>由单模高斯热图产生,<i>P</i><sub><i>n</i></sub>的定义为:</p>
                </div>
                <div class="p1">
                    <p id="100"><mathml id="146"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo><mo>=</mo><mrow><mi>exp</mi></mrow><mrow><mo>(</mo><mrow><mo>-</mo><mfrac><mrow><mrow><mo stretchy="false">(</mo><mi>i</mi><mo>-</mo><mi>i</mi><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false">)</mo></mrow><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mrow><mo stretchy="false">(</mo><mi>j</mi><mo>-</mo><mi>j</mi><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false">)</mo></mrow><msup><mrow></mrow><mn>2</mn></msup></mrow><mrow><mn>2</mn><mi>σ</mi><msup><mrow></mrow><mn>2</mn></msup></mrow></mfrac></mrow><mo>)</mo></mrow></mrow></math></mathml>      (2)</p>
                </div>
                <div class="p1">
                    <p id="101">式中:<i>P</i><sub><i>n</i></sub>是输入的关键点热力图,(<i>i</i><sub><i>n</i></sub>,<i>j</i><sub><i>n</i></sub>)是第<i>n</i>个关键点的二维坐标,σ是高斯峰的标准偏差。</p>
                </div>
                <div class="p1">
                    <p id="102">二维坐标表示的<i>C</i><sub><i>n</i></sub>由<i>H</i><sub><i>n</i></sub>计算得到,数学公式为:</p>
                </div>
                <div class="p1">
                    <p id="103"><mathml id="147"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>C</mi><msub><mrow></mrow><mi>n</mi></msub><mo>=</mo><mrow><mo stretchy="false">(</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>w</mi></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>h</mi></munderover><mi>i</mi></mstyle></mrow></mstyle><mi>Η</mi><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo><mo>,</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>w</mi></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>h</mi></munderover><mi>j</mi></mstyle></mrow></mstyle><mi>Η</mi><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><msup><mrow></mrow><mtext>Τ</mtext></msup></mrow></math></mathml>      (3)</p>
                </div>
                <div class="p1">
                    <p id="104">式中:<i>H</i><sub><i>n</i></sub>是由网络生成的精细化热力图,<i>w</i>指<i>H</i><sub><i>n</i></sub>的宽,<i>h</i>指<i>H</i><sub><i>n</i></sub>的高。</p>
                </div>
                <div class="p1">
                    <p id="105">网络的损失函数<i>L</i>是基于交叉熵的积分损失<citation id="168" type="reference"><link href="41" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>,由<i>L</i><sub><i>H</i></sub>和<i>L</i><sub><i>C</i></sub>两部分组成:</p>
                </div>
                <div class="p1">
                    <p id="106"><i>L</i>=<i>L</i><sub><i>H</i></sub>+<i>L</i><sub><i>C</i></sub>(4)</p>
                </div>
                <div class="p1">
                    <p id="107"><i>L</i><sub><i>H</i></sub>是交叉熵损失。它的定义如下:</p>
                </div>
                <div class="p1">
                    <p id="108"><mathml id="148"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><msub><mrow></mrow><mi>Η</mi></msub><mo>=</mo><mo>-</mo><mfrac><mn>1</mn><mi>Ν</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></munder><mi>Η</mi></mstyle></mrow></mstyle><msubsup><mrow></mrow><mi>n</mi><mo>*</mo></msubsup><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo><mrow><mi>log</mi></mrow><mi>Η</mi><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></math></mathml>      (5)</p>
                </div>
                <div class="p1">
                    <p id="109">式中:<i>H</i><sup>*</sup><sub><i>n</i></sub>是真值热力图,<i>H</i><sub><i>n</i></sub>是使用softmax估计得到的热力图。<i>L</i><sub><i>C</i></sub>是所有坐标<i>L</i>1损失的总和,它的定义如下:</p>
                </div>
                <div class="p1">
                    <p id="110"><mathml id="149"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><msub><mrow></mrow><mi>C</mi></msub><mo>=</mo><mfrac><mn>1</mn><mi>Ν</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mo stretchy="false">∥</mo></mstyle><mi>C</mi><msubsup><mrow></mrow><mi>n</mi><mo>*</mo></msubsup><mo>-</mo><mi>C</mi><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false">∥</mo><msub><mrow></mrow><mn>1</mn></msub></mrow></math></mathml>      (6)</p>
                </div>
                <div class="p1">
                    <p id="111">式中:<i>C</i><sup>*</sup><sub><i>n</i></sub>是第<i>n</i>个关键点的真值坐标向量。</p>
                </div>
                <h3 id="112" name="112" class="anchor-tag"><b>2 实验与结果分析</b></h3>
                <h4 class="anchor-tag" id="113" name="113"><b>2.1 实验细节</b></h4>
                <div class="p1">
                    <p id="114">本文只在COCO2017数据集上进行了实验,该数据集由包含118 288张图片的训练集、5 000张图片的验证集,以及20 288张图片的test-dev测试集组成。所有实验均在由ubuntu16.04操作系统、2.3 GHz的 CPU、12核24线程、以及8个NVIDIA 1080Ti GPU组成的服务器上进行,整个算法主要通过TensorFlow<citation id="169" type="reference"><link href="43" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>实现。</p>
                </div>
                <div class="p1">
                    <p id="115">对于人体检测模型,本文主要训练了基于ResNet101和FPN的Mask RCNN算法,ResNet101在ImageNet<citation id="170" type="reference"><link href="45" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>上进行预训练。在训练阶段本文只使用COCO数据集中的人类数据,为了增加训练数据,在训练阶段使用多尺度抖动技巧。在测试阶段,本文主要在COCO val2017上测试模型的精度,并把生成的边框文件单独保存,以便后续实验的使用。</p>
                </div>
                <div class="p1">
                    <p id="116">对于人体姿态估计模型,在训练阶段,本文先把数据集中标注的人体边界框以固定的长宽比进行扩展,然后从扩展后的边界框中裁剪图片并重置成256×192的大小。训练所用的主干网络都在ImageNet上进行了预训练。我们采用随机翻转、±30°的尺度变换以及±40°的旋转来增强数据。训练时使用Adam<citation id="171" type="reference"><link href="47" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>作为优化器,初始学习率设为5×10<sup>-4</sup>,然后在90 epoch到120 epoch时下降到5×10<sup>-5</sup>,整个实验一共训练140 epoch,Mini-batch的大小为128,使用8块NVIDIA 1080Ti GPU。在测试阶段利用人体检测模型的结果,获得图片中每个人的位置,然后对每个位置的人进行关键点预测。该阶段也使用翻转增强以及把热力图上最高峰点向第二高峰点偏移1/4作为最终输出的测试技巧。</p>
                </div>
                <div class="p1">
                    <p id="117">对于人体姿态优化模型,在训练阶段,由于其网络结构和人体姿态估计模型相似,故采用和人体姿态估计模型相似的训练设置。唯一的不同点为,在训练阶段,主干网络在ImageNet上进行初始化,但剩余部分使用参数<i>σ</i>=0.01的零均值高斯分布进行初始化。在测试阶段输入人体姿态估计的结果和从包含多人的图片中裁剪得到的单人图片组,输出优化后的多人姿态估计结果。</p>
                </div>
                <h4 class="anchor-tag" id="118" name="118"><b>2.2 实验结果分析</b></h4>
                <div class="p1">
                    <p id="119">本文首先在COCO val2017上,比较不同精度的人体检测模型对多人姿态估计的影响,实验结果如表2所示,其中ground truth指使用数据集中标注的人体边界框;<i>AP</i><sub><i>box</i></sub>指检测器在数据集人类数据上的平均准确率,<i>AP</i>到<i>AP</i><sup><i>L</i></sup>为在数据集上使用相应的检测器得到的多人姿态估计平均准确率,Faster RCNN的主干网络为ResNet50,Mask RCNN为ResNet101-FPN。</p>
                </div>
                <div class="area_img" id="120">
                    <p class="img_tit"><b>表2 人体检测器的性能对人体姿态估计的影响</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="120" border="1"><tr><td><br />检测器</td><td><i>AP</i><sub><i>box</i></sub></td><td><i>AP</i></td><td><i>AP</i><sup>50</sup></td><td><i>AP</i><sup>75</sup></td><td><i>AP</i><sup><i>M</i></sup></td></tr><tr><td><br />Ground truth</td><td>100</td><td>72.3</td><td>91.5</td><td>80.3</td><td>69.5</td></tr><tr><td><br />Faster RCNN</td><td>51.4</td><td>69.7</td><td>87.8</td><td>77.0</td><td>66.0</td></tr><tr><td><br />Mask RCNN</td><td>55.3</td><td>70.4</td><td>88.9</td><td>77.9</td><td>67.2</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="121">通过上述对比试验,可以看出使用不同性能的检测器可以获得不同的姿态估计准确率,随着检测器性能的提高多人姿态估计的准确率会有一定的提升,说明对同一个姿态估计模型,使用精度更高的检测器,可以得到更精确的多人姿态估计结果。</p>
                </div>
                <div class="p1">
                    <p id="122">其次,本文在COCO val2017上,经过消融实验,比较人体姿态优化模块对多人姿态估计的影响。我们使用resnet50作为人体姿态估计模块的主干网络,输入图片的大小是256×192,人体检测器使用在COCO val2017上对人检测为56.7AP的Mask RCNN模型,人体姿态优化模型使用主干网络为ResNet152,输入图片大小是384×288的模型。Baseline指不加人体姿态优化模块,+PoseFix指添加人体姿态优化模块,实验结果如表3所示。</p>
                </div>
                <div class="area_img" id="123">
                    <p class="img_tit"><b>表3 人体姿态优化模块对多人姿态估计的影响</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="123" border="1"><tr><td><br />算法</td><td><i>AP</i></td><td><i>AP</i><sup>50</sup></td><td><i>AP</i><sup>75</sup></td><td><i>AP</i><sup><i>M</i></sup></td><td><i>AP</i><sup><i>L</i></sup></td></tr><tr><td><br />baseline</td><td>70.7</td><td>89.0</td><td>78.1</td><td>67.5</td><td>76.8</td></tr><tr><td><br />+PoseFix</td><td>74.2</td><td>89.7</td><td>80.4</td><td>70.8</td><td>80.7</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="124">通过表3可以得出,人体姿态优化模块在所有评估指标上都提高了多人姿态估计的准确性,特别是在<i>AP</i>、<i>AP</i><sup><i>M</i></sup>和<i>AP</i><sup><i>L</i></sup>上,说明人体姿态优化模块可以有效提高多人姿态估计的准确率。</p>
                </div>
                <div class="p1">
                    <p id="125">最后,本文在COCO test-dev上,比较本文提出的算法和其他算法的多人姿态估计结果。比较的算法包括CMU-Pose<citation id="172" type="reference"><link href="17" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>、G-RMI<citation id="173" type="reference"><link href="13" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>、AE<citation id="174" type="reference"><link href="19" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>、RMPE<citation id="175" type="reference"><link href="11" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>、CPN<citation id="176" type="reference"><link href="9" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>、Simple<citation id="177" type="reference"><link href="15" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>。我们使用在COCO test-dev对人检测为58.6AP的人体检测模型,主干网络为RetNet152,输入图片大小为256×192的人体姿态估计模型以及主干网络为ResNet152,输入图片大小为384×288的人体姿态优化模型,实验结果如表4所示。</p>
                </div>
                <div class="area_img" id="126">
                    <p class="img_tit"><b>表4 在COCO test-dev 上本文算法与其他算法的比较</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="126" border="1"><tr><td><br />算法</td><td><i>AP</i></td><td><i>AP</i><sup>50</sup></td><td><i>AP</i><sup>75</sup></td><td><i>AP</i><sup><i>M</i></sup></td><td><i>AP</i><sup><i>L</i></sup></td></tr><tr><td><br />CMU-Pose</td><td>61.8</td><td>84.9</td><td>67.5</td><td>57.1</td><td>68.2</td></tr><tr><td><br />G-RMI</td><td>64.9</td><td>85.5</td><td>71.3</td><td>62.3</td><td>70.0</td></tr><tr><td><br />AE</td><td>65.5</td><td>86.8</td><td>72.3</td><td>60.6</td><td>72.6</td></tr><tr><td><br />RMPE</td><td>72.3</td><td>89.2</td><td>79.1</td><td>68.0</td><td>78.6</td></tr><tr><td><br />CPN</td><td>73.0</td><td>91.7</td><td>80.0</td><td>69.5</td><td>78.1</td></tr><tr><td><br />Simple</td><td>73.7</td><td>91.8</td><td>81.1</td><td>70.3</td><td>80.0</td></tr><tr><td><br />Our(MSP)</td><td>74.6</td><td>89.6</td><td>81.5</td><td>71.1</td><td>81.3</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="127">通过表4可以得出,我们提出的算法在多数指标上都可以有效提高多人姿态估计的结果。在<i>AP</i><sup>50</sup>上检测结果低于Simple和CPN是这是因为他们使用了更好的检测器和更大的输入图片。比如Simple使用在test-dev对人检测为60.9 AP的检测器,人体姿态估计模型输入的图片大小是384×288。本文如果采用相同的主干网络、更好的检测器或是模型融合等技巧,可以获得更高的估计性能。</p>
                </div>
                <h3 id="128" name="128" class="anchor-tag"><b>3 结  论</b></h3>
                <div class="p1">
                    <p id="129">本文提出了一个包含3个模块的多人姿态估计算法MSP,它通过结合目前性能优越的目标检测算法,简单有效的人体姿态估计算法,以及最近提出的姿态优化算法,来提高多人姿态估计的准确率。本文实验表明目标检测器的性能以及姿态估计的优化对多人姿态估计有很大影响。本文提出的算法网络模型比较简单,人体姿态估计和人体姿态优化使用相似的网络架构,有利于算法的研究和应用。虽然,本文提出的算法可以有效提高多人姿态估计的准确率,但是算法的模块较多,姿态估计的实时性较差,未来可以研究使用检测速度较快的一阶段检测器,优化人体姿态估计和人体姿态优化模块的网络模型,以便达到实时估计的要求。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=DKFX&amp;filename=YQXB201811028&amp;v=MjY4MjJ6VGJMRzRIOW5Ocm85SGJJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVadEZ5bmxXN3pNUEQ=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> 唐心宇,宋爱国.人体姿态估计及在康复训练情景交互中的应用[J].仪器仪表学报,2018,39(11):195-203.
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Stacked Hourglass Networks for Human Pose Estimation">

                                <b>[2]</b> NEWELL A,YANG K,DENG J.Stacked hourglass networks for human pose estimation[C].European Conference on Computer Vision,2016:483-499.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Convolutional pose machines">

                                <b>[3]</b> WEI S,RAMAKRISHNA V,KANADE T,et al.Convolutional pose machines[C].IEEE Conference on Computer Vision and Pattern Recognition,2016:4724-4732.
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Cascaded pyramid network for multi-person pose estimation">

                                <b>[4]</b> CHEN Y L,WANG ZH CH,PENG Y X,et al.Cascaded pyramid network for multi-person pose estimation[C].IEEE/CVF Conference on Computer Vision and Pattern Recognition,2018:7103-7112.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=RMPE:Regional Multi-person Pose Estimation">

                                <b>[5]</b> FANG H,XIE S,TAI Y,et al.RMPE:regional multi-person pose estimation[C].IEEE International Conference on Computer Vision,2017:2353-2362.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Towards accurate multi-person pose estimation in the wild">

                                <b>[6]</b> PAPANDREOU G,ZHU T,KANAZAWA N,et al.Towards accurate multi-person pose estimation in the wild[C].IEEE Conference on Computer Vision and Pattern Recognition,2017:3711-3719.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Simple Baselines for Human Pose Estimation and Tracking">

                                <b>[7]</b> XIAO B,WU H P,WEI Y CH.Simple baselines for human pose estimation and tracking[C].European Conference on Computer Vision,2018:472-487.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Realtime multi-person 2d pose estimation using part affinity fields">

                                <b>[8]</b> CAO Z,SIMON T,WEI S,et al.Realtime multi-person 2d pose estimation using part affinity fields[C].IEEE Conference on Computer Vision and Pattern Recognition,2017:1302-1310.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Associative embedding End-to-end learning for joint detection and grouping">

                                <b>[9]</b> NEWELL A,HUANG Z,DENG J.Associative embedding:End-to-end learning for joint detection and grouping[C].31st Conference on Neural Information Processing System,2017:2277-2287.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Faster R-CNN:towards real-time object detection with region proposal networks">

                                <b>[10]</b> REN S,HE K,GIRSHICK R,et al.Faster R-CNN:towards real-time object detection with region proposal networks[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2017,39(6):1137-1149.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Feature pyramid networks for object detection">

                                <b>[11]</b> LIN T Y,DOLLÁR P,GIRSHICK R,et al.Feature pyramid networks for object detection[C].IEEE Conference on Computer Vision and Pattern Recognition,2017:936-944.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=MegDet:A Large Mini-Batch Object Detector">

                                <b>[12]</b> PENG CH,XIAO T,LI Z,et al.MegDet:a large mini-batch object detector[C].IEEE/CVF Conference on Computer Vision and Pattern Recognition,2018:6181-6189.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_13" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZCL201711020&amp;v=MjMwNTVIWklRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVp0RnlubFc3ek1JVGZJWXJHNEg5Yk5ybzk=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[13]</b> 周晓彦,王珂,李凌燕.基于深度学习的目标检测算法综述[J].电子测量技术,2017,40(11):89-93.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Human Pose Estimation with Iterative Error Feedback">

                                <b>[14]</b> CARREIRA J,AGRAWAL P,FRAGKIADAKI K,et al.Human pose estimation with iterative error feedback[C].IEEE Conference on Computer Vision and Pattern Recognition,2016:4733-4742.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Mask R-CNN">

                                <b>[15]</b> HE K,GKIOXARI G,DOLLÁR P,et al.Mask R-CNN[C].IEEE International Conference on Computer Vision,2017:2980-2988.
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=PoseFix model-agnostic general human pose refinement network">

                                <b>[16]</b> MOON G,CHANG J Y,LEE K M.PoseFix:model-agnostic general human pose refinement network[C].IEEE Conference on Computer Vision and Pattern Recognition,2019:1812.03595.
                            </a>
                        </p>
                        <p id="35">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Microsoft COCO:Common Objects in Context">

                                <b>[17]</b> LIN T,MAIRE M,BELONGIE S,et al.Microsoft coco:Common objects in context[C].European Conference on Computer Vision,2014:740-755.
                            </a>
                        </p>
                        <p id="37">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep residual learning for image recognition">

                                <b>[18]</b> HE K,ZHANG X,REN S,et al.Deep residual learning for image recognition[C].IEEE Conference on Computer Vision and Pattern Recognition,2016:770-778.
                            </a>
                        </p>
                        <p id="39">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Benchmarking and error diagnosis in multi-instance pose estimation">

                                <b>[19]</b> RONCHI M R,PERONA P.Benchmarking and error diagnosis in multi-instance pose estimation[C].IEEE International Conference on Computer Vision,2017:369-378.
                            </a>
                        </p>
                        <p id="41">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Integral human pose regression">

                                <b>[20]</b> SUN X,XIAO B,LIANG S,et al.Integral human pose regression[C].European Conference on Computer Vision,2018:536-553.
                            </a>
                        </p>
                        <p id="43">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=TensorFlow:large-scale machine learning on heterogeneous systems">

                                <b>[21]</b> ABADI M,AGARWAL A,BARHAM P,et al.TensorFlow:large-scale machine learning on heterogeneous systems[C].2015:1-19.
                            </a>
                        </p>
                        <p id="45">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=ImageNet:A large-scale hierarchical image database">

                                <b>[22]</b> DENG J,DONG W,SOCHER R,et al.ImageNet:a large-scale hierarchical image database[C].IEEE Conference on Computer Vision and Pattern Recognition,2009:248-255.
                            </a>
                        </p>
                        <p id="47">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Adam:A method for stochastic optimization">

                                <b>[23]</b> KINGMA D,BA J.Adam:a method for stochastic optimization[C].International Conference on Learning Representations,2014:1-15.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="DZCL201919038" />
        <input id="dpi" type="hidden" value="600" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZCL201919038&amp;v=MjAwMDlqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVadEZ5bmxXN3pOSVRmSVlyRzRIOWpOcG85R2JJUUtESDg0dlI0VDY=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>


    <link href="/kxreader/Content/css/LeftDetail?v=NLcKG8I1SJUaVFrQ0iGpF2klAT0OsmHRaVSZ1rKb5xg1" rel="stylesheet"/>

</body>
</html>

