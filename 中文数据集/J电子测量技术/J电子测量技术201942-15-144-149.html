

<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>

</head>

<body>

    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637135659710881250%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dDZCL201915035%26RESULT%3d1%26SIGN%3dDqxXoUfAZr86NCNC0Xw6xhLP9eU%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=DZCL201915035&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=DZCL201915035&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>


    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZCL201915035&amp;v=MTgxOTNvOUdZWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnRGeXZnVkwvTklUZklZckc0SDlqTnE=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#53" data-title="&lt;b&gt;0 引  言&lt;/b&gt; "><b>0 引  言</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#56" data-title="&lt;b&gt;1 接触网定位器视觉检测系统&lt;/b&gt; "><b>1 接触网定位器视觉检测系统</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#60" data-title="&lt;b&gt;2 接触网定位器关键区域的初定位&lt;/b&gt; "><b>2 接触网定位器关键区域的初定位</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#61" data-title="&lt;b&gt;2.1 Faster R-CNN的发展&lt;/b&gt;"><b>2.1 Faster R-CNN的发展</b></a></li>
                                                <li><a href="#68" data-title="&lt;b&gt;2.2 FPN+Faster R-CNN定位器检测模型&lt;/b&gt;"><b>2.2 FPN+Faster R-CNN定位器检测模型</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#77" data-title="&lt;b&gt;3 接触网定位器的精检算法&lt;/b&gt; "><b>3 接触网定位器的精检算法</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#88" data-title="&lt;b&gt;4 实验分析&lt;/b&gt; "><b>4 实验分析</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#89" data-title="&lt;b&gt;4.1 接触网定位器图像集和实验平台&lt;/b&gt;"><b>4.1 接触网定位器图像集和实验平台</b></a></li>
                                                <li><a href="#91" data-title="&lt;b&gt;4.2 实验结果与分析&lt;/b&gt;"><b>4.2 实验结果与分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#98" data-title="&lt;b&gt;5 结  论&lt;/b&gt; "><b>5 结  论</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#58" data-title="图1 定位器视觉检测系统">图1 定位器视觉检测系统</a></li>
                                                <li><a href="#59" data-title="图2 定位器检测系统流程">图2 定位器检测系统流程</a></li>
                                                <li><a href="#71" data-title="图3 RPN网络结构">图3 RPN网络结构</a></li>
                                                <li><a href="#73" data-title="图4 FPN+Faster R-CNN网络结构">图4 FPN+Faster R-CNN网络结构</a></li>
                                                <li><a href="#85" data-title="图5 梯度和level-line">图5 梯度和level-line</a></li>
                                                <li><a href="#86" data-title="图6 直线支持区域">图6 直线支持区域</a></li>
                                                <li><a href="#87" data-title="图7 基于LSD的定位器直线检测结果">图7 基于LSD的定位器直线检测结果</a></li>
                                                <li><a href="#92" data-title="图8 上图:无隧道环境下, 下图:有隧道环境下的四种定位器检测模型效果比较">图8 上图:无隧道环境下, 下图:有隧道环境下的四种定位器检测模型效果比较</a></li>
                                                <li><a href="#95" data-title="&lt;b&gt;表1 不同模型定位器检测帧率的比较&lt;/b&gt;"><b>表1 不同模型定位器检测帧率的比较</b></a></li>
                                                <li><a href="#97" data-title="图9 基于FPN+ Faster R-CNN的定位器检测模型检测结果">图9 基于FPN+ Faster R-CNN的定位器检测模型检测结果</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" title=" GAO S B, LIU Z G, YU L.Detection and monitoring system of the pantograph-catenary in high-speed railway (6C) [C]//International Conference on Power Electronics Systems and Applications-Smart Mobility, Power Transfer &amp;amp; Security.2017:1-7." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Detection and monitoring system of the pantograph-catenary in high-speed railway (6C)">
                                        <b>[1]</b>
                                         GAO S B, LIU Z G, YU L.Detection and monitoring system of the pantograph-catenary in high-speed railway (6C) [C]//International Conference on Power Electronics Systems and Applications-Smart Mobility, Power Transfer &amp;amp; Security.2017:1-7.
                                    </a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" title=" 罗健, 白裔峰, 魏博.高速铁路接触网定位器坡度问题的深化研究[J].铁道工程学报, 2013, 30 (1) :76-80." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=TDGC201301015&amp;v=Mjk4OTBMTXJvOUVZWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnRGeXZnVkwvTk1Tbk1iYkc0SDk=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         罗健, 白裔峰, 魏博.高速铁路接触网定位器坡度问题的深化研究[J].铁道工程学报, 2013, 30 (1) :76-80.
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title=" 范虎伟, 卞春华, 朱挺, 等.非接触式接触网定位器坡度自动检测技术[J].计算机应用, 2010, 30 (S2) :102-103, 128." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY2010S2034&amp;v=MTg5NjI3QmQ3RzRIOUd2clk5R1lJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVadEZ5dmdWTC9OTHo=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         范虎伟, 卞春华, 朱挺, 等.非接触式接触网定位器坡度自动检测技术[J].计算机应用, 2010, 30 (S2) :102-103, 128.
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" title=" 段汝娇, 赵伟, 黄松岭, 等.基于模糊ID3决策树的快速角点检测算法[J].清华大学学报 (自然科学版) , 2011, 51 (12) :1787-1791." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=QHXB201112006&amp;v=MTU0NjJDVVI3cWZadVp0Rnl2Z1ZML05OQ1hUYkxHNEg5RE5yWTlGWW9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnI=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                         段汝娇, 赵伟, 黄松岭, 等.基于模糊ID3决策树的快速角点检测算法[J].清华大学学报 (自然科学版) , 2011, 51 (12) :1787-1791.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" title=" 于博轩, 陈唐龙, 于龙, 等.基于图像处理中霍夫变换的定位器坡度检测[J].城市轨道交通研究, 2014, 17 (5) :32-36." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GDJT201405011&amp;v=MDk4MTJPM3pxcUJ0R0ZyQ1VSN3FmWnVadEZ5dmdWTC9OSWluQmVyRzRIOVhNcW85RVpZUUtESDg0dlI0VDZqNTQ=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                         于博轩, 陈唐龙, 于龙, 等.基于图像处理中霍夫变换的定位器坡度检测[J].城市轨道交通研究, 2014, 17 (5) :32-36.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" title=" 王旭东, 吴积钦, 徐可佳, 等.基于AdaBoost算法的接触网定位器识别[J].高速铁路技术, 2014, 5 (3) :9-12." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GSTL201403003&amp;v=MDc4Njk5WE1ySTlGWjRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVp0Rnl2Z1ZML05JajdmWXJHNEg=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                         王旭东, 吴积钦, 徐可佳, 等.基于AdaBoost算法的接触网定位器识别[J].高速铁路技术, 2014, 5 (3) :9-12.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" title=" 顾会建.基于视频图片的接触网定位器识别方法研究[D].成都:西南交通大学, 2014." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=1014257602.nh&amp;v=MjY2NDZHOUdkZk1yWkViUElRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVp0Rnl2Z1ZML05WRjI2R3I=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                         顾会建.基于视频图片的接触网定位器识别方法研究[D].成都:西南交通大学, 2014.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" title=" KRIZHEVSKY A, SUTSKEVER I, HINTON G.Imagenet classification with deep convolutional neural networks[J].Advances in neural information processing systems, 2012, 25 (2) ." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Imagenet classification with deep convolutional neural networks">
                                        <b>[8]</b>
                                         KRIZHEVSKY A, SUTSKEVER I, HINTON G.Imagenet classification with deep convolutional neural networks[J].Advances in neural information processing systems, 2012, 25 (2) .
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" title=" DENG J, DONG W, SOCHER R, et al.ImageNet:A large-scale hierarchical image database[C]//Computer Vision and Pattern Recognition, 2009.CVPR 2009.IEEE Conference on.IEEE, 2009:248-255." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Imagenet:A large-scale hierarchical image database">
                                        <b>[9]</b>
                                         DENG J, DONG W, SOCHER R, et al.ImageNet:A large-scale hierarchical image database[C]//Computer Vision and Pattern Recognition, 2009.CVPR 2009.IEEE Conference on.IEEE, 2009:248-255.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_10" title=" SERMANET, PIERRE, et al.Overfeat:Integrated recognition, localization and detection using convolutional networks[J].arXiv preprint arXiv:1312.6229, 2013." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Overfeat:Integrated recognition,localization and detection using convolutional networks">
                                        <b>[10]</b>
                                         SERMANET, PIERRE, et al.Overfeat:Integrated recognition, localization and detection using convolutional networks[J].arXiv preprint arXiv:1312.6229, 2013.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_11" >
                                        <b>[11]</b>
                                     GIRSHICK R, DONAHUE J, DARRELL T, et al.Rich feature hierarchies for accurate object detection and semantic segmentation[C]//Proceedings of the IEEE conference on computer vision and pattern recognition.2014:580-587.</a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_12" title=" GIRSHICK R.Fast R-CNN[J].Computer Science, 2015." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fast R-CNN">
                                        <b>[12]</b>
                                         GIRSHICK R.Fast R-CNN[J].Computer Science, 2015.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_13" title=" REN S, HE K, GIRSHICK R, et al.Faster R-CNN:Towards real-time object detection with region proposal networks[J].IEEE Transactions on Pattern Analysis &amp;amp; Machine Intelligence, 2017, 39 (6) :1137." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Faster R-CNN:Towards Real-Time Object Detection with Region Proposal Networks">
                                        <b>[13]</b>
                                         REN S, HE K, GIRSHICK R, et al.Faster R-CNN:Towards real-time object detection with region proposal networks[J].IEEE Transactions on Pattern Analysis &amp;amp; Machine Intelligence, 2017, 39 (6) :1137.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_14" title=" CHEN J, LIU Z, WANG H, et al.High-speed railway catenary components detection using the cascaded convolutional neural networks[C]//IEEE International Conference on Imaging Systems and Techniques.IEEE, 2017:1-6." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=High-speed railway catenary components detection using the cascaded convolutional neural networks">
                                        <b>[14]</b>
                                         CHEN J, LIU Z, WANG H, et al.High-speed railway catenary components detection using the cascaded convolutional neural networks[C]//IEEE International Conference on Imaging Systems and Techniques.IEEE, 2017:1-6.
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_15" title=" 陈东杰, 张文生, 杨阳.基于深度学习的高铁接触网定位器检测与识别[J].中国科学技术大学学报, 2017 (4) ." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZKJD201704006&amp;v=MTM1MDBQeWJCYXJHNEg5Yk1xNDlGWW9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVp0Rnl2Z1ZML04=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[15]</b>
                                         陈东杰, 张文生, 杨阳.基于深度学习的高铁接触网定位器检测与识别[J].中国科学技术大学学报, 2017 (4) .
                                    </a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_16" >
                                        <b>[16]</b>
                                     LIN, TSUNG-YI, et al.Feature pyramid networks for object detection[C].Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.2017.</a>
                                </li>
                                <li id="35">


                                    <a id="bibliography_17" title=" VON GIOI, RAFAEL GROMPONE, et al.LSD:A fast line segment detector with a false detection control.IEEE transactions on pattern analysis and machine intelligence, 2010, 32 (4) :722-732." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=LSD: A Fast Line Segment Detector with a False Detection Control">
                                        <b>[17]</b>
                                         VON GIOI, RAFAEL GROMPONE, et al.LSD:A fast line segment detector with a false detection control.IEEE transactions on pattern analysis and machine intelligence, 2010, 32 (4) :722-732.
                                    </a>
                                </li>
                                <li id="37">


                                    <a id="bibliography_18" title=" ZITNICK C L, DOLL&#193;R P.Edge Boxes:locating object proposals from edges[M]//Computer Vision-ECCV 2014.Springer International Publishing, 2014:391-405." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Edge boxes:Locating object proposals from edges">
                                        <b>[18]</b>
                                         ZITNICK C L, DOLL&#193;R P.Edge Boxes:locating object proposals from edges[M]//Computer Vision-ECCV 2014.Springer International Publishing, 2014:391-405.
                                    </a>
                                </li>
                                <li id="39">


                                    <a id="bibliography_19" title=" UIJLINGS J R R, VAN DE SANDE K E A, Gevers T, et al.Selective search for object recognition[J].International journal of computer vision, 2013, 104 (2) :154-171." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD13080200013634&amp;v=MjEzNTVCYXJLN0h0bk1yWTlGWk9vTUNuODlvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp0RmlubFVyeklJMXNUYWhjPU5qNw==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[19]</b>
                                         UIJLINGS J R R, VAN DE SANDE K E A, Gevers T, et al.Selective search for object recognition[J].International journal of computer vision, 2013, 104 (2) :154-171.
                                    </a>
                                </li>
                                <li id="41">


                                    <a id="bibliography_20" title=" HE K, ZHANG X, REN S, et al.Spatial pyramid pooling in deep convolutional networks for visual recognition[J].IEEE Transactions on Pattern Analysis &amp;amp; Machine Intelligence, 2014, 37 (9) :1904-1916." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Spatial pyramid pooling in deep convolutional networks for visual recognition">
                                        <b>[20]</b>
                                         HE K, ZHANG X, REN S, et al.Spatial pyramid pooling in deep convolutional networks for visual recognition[J].IEEE Transactions on Pattern Analysis &amp;amp; Machine Intelligence, 2014, 37 (9) :1904-1916.
                                    </a>
                                </li>
                                <li id="43">


                                    <a id="bibliography_21" title=" DESOLNEUX A, MOISAN L, MOREL J M.Meaningful alignments[J].International Journal of Computer Vision, 2000, 40 (1) :7-23." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002830642&amp;v=MTE2MzBxZForWnVGaXZsVUxyUElGcz1OajdCYXJPNEh0SE9wNHhGWXU4TlkzazV6QmRoNGo5OVNYcVJyeG94Y01IN1I3&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[21]</b>
                                         DESOLNEUX A, MOISAN L, MOREL J M.Meaningful alignments[J].International Journal of Computer Vision, 2000, 40 (1) :7-23.
                                    </a>
                                </li>
                                <li id="45">


                                    <a id="bibliography_22" title=" HE, KAIMING, et al.Deep residual learning for image recognition[C].Proceedings of the IEEE conference on computer vision and pattern recognition.2016." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep residual learning for image recognition">
                                        <b>[22]</b>
                                         HE, KAIMING, et al.Deep residual learning for image recognition[C].Proceedings of the IEEE conference on computer vision and pattern recognition.2016.
                                    </a>
                                </li>
                                <li id="47">


                                    <a id="bibliography_23" title=" JIA Y.Caffe:An open source convolutional architecture for fast feature embedding.http://caffe.berkeleyvision.org/, 2013." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=An Open Source Convolutional Architecture for Fast Feature Embedding">
                                        <b>[23]</b>
                                         JIA Y.Caffe:An open source convolutional architecture for fast feature embedding.http://caffe.berkeleyvision.org/, 2013.
                                    </a>
                                </li>
                                <li id="49">


                                    <a id="bibliography_24" title=" DALAL N, TRIGGS B.Histograms of oriented gradients for human detection[C]//IEEE Computer Society Conference on Computer Vision &amp;amp; Pattern Recognition.IEEE Computer Society, 2005:886-893." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Histograms of Oriented Gradients for Human Detection">
                                        <b>[24]</b>
                                         DALAL N, TRIGGS B.Histograms of oriented gradients for human detection[C]//IEEE Computer Society Conference on Computer Vision &amp;amp; Pattern Recognition.IEEE Computer Society, 2005:886-893.
                                    </a>
                                </li>
                                <li id="51">


                                    <a id="bibliography_25" title=" FELZENSZWALB P, MCALLESTER D, RAMANAN D.A discriminatively trained, multiscale, deformable part model[J].Cvpr, 2008, 8:1-8." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A discriminatively trained, multiscale, deformable part model">
                                        <b>[25]</b>
                                         FELZENSZWALB P, MCALLESTER D, RAMANAN D.A discriminatively trained, multiscale, deformable part model[J].Cvpr, 2008, 8:1-8.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=DZCL" target="_blank">电子测量技术</a>
                2019,42(15),144-149 DOI:10.19651/j.cnki.emt.1902739            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>FPN在高速列车接触网定位器检测应用</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%B4%94%E5%AE%B5%E6%B4%8B&amp;code=42606883&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">崔宵洋</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9E%97%E5%BB%BA%E8%BE%89&amp;code=09160217&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">林建辉</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%99%88%E6%98%A5%E4%BF%8A&amp;code=10222235&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">陈春俊</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%A8%E5%8A%BC%E7%AB%8B&amp;code=41363725&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">杨劼立</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BE%90%E5%88%9A&amp;code=35988762&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">徐刚</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E8%A5%BF%E5%8D%97%E4%BA%A4%E9%80%9A%E5%A4%A7%E5%AD%A6%E6%9C%BA%E6%A2%B0%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0218487&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">西南交通大学机械工程学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E8%A5%BF%E5%8D%97%E4%BA%A4%E9%80%9A%E5%A4%A7%E5%AD%A6%E7%89%B5%E5%BC%95%E5%8A%A8%E5%8A%9B%E5%9B%BD%E5%AE%B6%E9%87%8D%E7%82%B9%E5%AE%9E%E9%AA%8C%E5%AE%A4&amp;code=1700235&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">西南交通大学牵引动力国家重点实验室</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E8%BD%A6%E9%9D%92%E5%B2%9B%E5%9B%9B%E6%96%B9%E6%9C%BA%E8%BD%A6%E8%BD%A6%E8%BE%86%E8%82%A1%E4%BB%BD%E6%9C%89%E9%99%90%E5%85%AC%E5%8F%B8&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中车青岛四方机车车辆股份有限公司</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>高速列车接触网的定位器夹持导线, 在保证受电弓的受流质量方面发挥着关键的作用。近年来随着计算机视觉技术和硬件的快速发展, 采用高清摄像头对接触网进行图像监测越来越普遍。特征网络金字塔 (feature pyramid networks, FPN) 可以将包含更加抽象、更多语义的深层卷积特征和更高分辨率、更多细节信息的浅层卷积特征融合, 能够有效解决高速列车的接触网定位器因目标较小而导致检测效果较差的问题。首先研究了Faster R-CNN的目标检测算法, 然后结合FPN算法思想对Faster R-CNN进行改进, 提出了基于FPN+Faster R-CNN的高速列车接触网定位器检测模型。整个检测系统流程包含2步:首先利用FPN+Faster R-CNN对定位器关键区域进行初定位, 然后在获得的关键区域内采用LSD (line segment detector) 直线检测算法对定位器做精确检测。通过高速列车4C监测系统对列车某线路接触网采集的图像数据集实验分析, 采用的基于FPN+Faster R-CNN定位器检测模型在检测精确率和回召率方面得到了很大程度的提高, 同时具有较高的实时性。这对实现高速列车的接触网定位器快速而又准确的检测以及保障列车安全行驶具有重要的意义。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%8E%A5%E8%A7%A6%E7%BD%91%E5%AE%9A%E4%BD%8D%E5%99%A8&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">接触网定位器;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%89%B9%E5%BE%81%E9%87%91%E5%AD%97%E5%A1%94%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">特征金字塔网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Faster%20R-CNN&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Faster R-CNN;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=line%20segment%20detector&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">line segment detector;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    *崔宵洋 (通信作者) , 硕士研究生, 主要研究方向为铁路安全监测技术、深度学习和图像处理技术。E-mail:cuiroar@163.com;
                                </span>
                                <span>
                                    林建辉, 教授, 博士, 主要从事铁路安全监测技术、试验研究和信号分析处理工作。E-mail:lin13008104673@126.com;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-02-28</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家重点研发计划“先进轨道交通”重点专项 (2017YFB1201004、2017YFB1201103) 资助;</span>
                    </p>
            </div>
                    <h1><b>Application of FPN in the detection of the high-speed railway catenary steady arms</b></h1>
                    <h2>
                    <span>Cui Xiaoyang</span>
                    <span>Lin Jianhui</span>
                    <span>Chen Chunjun</span>
                    <span>Yang Jieli</span>
                    <span>Xu Gang</span>
            </h2>
                    <h2>
                    <span>Research School of Mechanical Engineering, Southwest Jiaotong University</span>
                    <span>State Key Laboratory of Traction Power, Southwest Jiaotong University</span>
                    <span>CRRC Qingdao Sifang Co., Ltd.</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>The steady arms of the high-speed train catenary system clamp the contact wire and play a key role in ensuring the normal operation of the contact wire. With the rapid development of computer vision technology and hardware, it is more and more common to use the HD camera to monitor overhead catenary system recently. The feature pyramid networks (FPN) can merge the deep convolution features which contain more abstract and semantics information with the shallow features which contain higher resolution and more details information. It can effectively solve the problem that the catenary steady arms of high-speed train have poor detection because of its small target. Firstly, this paper studies the object detection algorithm of Faster R-CNN, then combines the idea of FPN algorithm to improve Faster R-CNN, and proposes a detection model of catenary steady arms based on FPN + Faster R-CNN. The whole detection system process includes two steps: first, FPN + Faster R-CNN is used to locate the key area of the locator, and then LSD (line segment detector) linear detection algorithm is used to detect the locator accurately in the key area. The experimental analysis of the image data set collected by the high-speed train 4 C monitoring system, shows that the precision and recall of the detection system have been greatly improved. Meanwhile, it has a high real-time performance. It provides a strong guarantee for the slope detection of the catenary steady arms lately and ensuring the safe running of high-speed trains.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=catenary%20steady%20arm&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">catenary steady arm;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=feature%20pyramid%20networks&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">feature pyramid networks;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Faster%20R-CNN&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Faster R-CNN;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=line%20segment%20detector&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">line segment detector;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2019-02-28</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="53" name="53" class="anchor-tag"><b>0 引  言</b></h3>
                <div class="p1">
                    <p id="54">高速列车接触网定位器固定着接触线和线路中心线之间的相对横向距离。当高速列车运行时, 为避免受电弓滑板和定位器发生碰撞, 定位器需要按照一定的倾斜度安装。弓网耦合系统的振动可能导致定位器的坡度值超限, 严重影响弓网系统的供电稳定性和列车的安全运行<citation id="100" type="reference"><link href="3" rel="bibliography" /><link href="5" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>]</sup></citation>。相关学者在定位器检测技术方面也做了大量的研究工作<citation id="101" type="reference"><link href="7" rel="bibliography" /><link href="9" rel="bibliography" /><link href="11" rel="bibliography" /><link href="13" rel="bibliography" /><link href="15" rel="bibliography" /><sup>[<a class="sup">3</a>,<a class="sup">4</a>,<a class="sup">5</a>,<a class="sup">6</a>,<a class="sup">7</a>]</sup></citation>, 这些检测算法大多依靠设计精妙的目标描述子和人工提取目标特征, 随后通过机器学习对其训练完成检测任务。此类算法普遍具有检测精确率和实时性较低的弊端, 以及不能很好地适应目标背景复杂和视角变化所带来的干扰情况。</p>
                </div>
                <div class="p1">
                    <p id="55">2012年Krizhevsky提出的AlexNet<citation id="102" type="reference"><link href="17" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>在ImageNet<citation id="103" type="reference"><link href="19" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>竞赛中夺冠之后, 基于深度卷积神经网络的目标检测技术取得了快速的发展。Sermanet<citation id="104" type="reference"><link href="21" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>提出了深度卷积神经网络和滑动窗口技术相结合的算法, 可以在多个尺度下检测目标, 虽然提高了目标的检测精确率, 但由于滑动窗口计算开销太大, 导致了检测速度较慢的问题。为了能够在保证较高的检测精度的同时提高检测速度, Girshick采用基于RP (region proposal) 的目标检测技术以降低图像中的候选窗口数量, 设计了R-CNN<citation id="105" type="reference"><link href="23" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>网络。随后相继提出的Fast R-CNN<citation id="106" type="reference"><link href="25" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>和Faster RCNN<citation id="107" type="reference"><link href="27" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>都是基于RP (region proposal) 技术, 围绕如何高效寻找和处理目标潜在的建议区域这一核心问题而不断优化网络的检测性能。Faster RCNN由于采用了RPN (region proposal network) , 其检测精确率和速度得到了大幅度提高, 广泛应用于人脸识别、行人检测和车辆检测等领域。陈军文<citation id="108" type="reference"><link href="29" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>提出基于Faster R-CNN的检测系统检测接触网的关键零部件, 包括U形钩和紧固件等。陈东杰<citation id="109" type="reference"><link href="31" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>提出基于Faster R-CNN的高速列车接触网检测模型, 同时结合霍夫变换和滤线机制筛选出定位器的最佳检测直线段。由于高速列车的接触网定位器检测目标较小, Faster R-CNN利用了仅仅利用了卷积特征提取层中的高级卷积特征信息, 忽略了浅层卷积特征的细节信息, 然而这些细节信息对于检测定位器这种小目标是非常重要的。基于该事实我们结合特征金字塔<citation id="110" type="reference"><link href="33" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation> (feature pyramid networks, FPN) 检测算法的思想, 提出了基于FPN+ Faster R-CNN的定位器检测系统。</p>
                </div>
                <h3 id="56" name="56" class="anchor-tag"><b>1 接触网定位器视觉检测系统</b></h3>
                <div class="p1">
                    <p id="57">接触网定位器智能化检测技术是当今发展的趋势和研究热点, 采用高清摄像头的非接触式图像检测技术在铁路检测领域应用越来越广, 定位器视觉检测系统如图1所示。系统检测流程包含由粗检到精检两个子过程:首先利用FPN+ Faster R-CNN对定位器所在的关键区域进行初定位, 然后在定位器关键区域内采用LSD<citation id="111" type="reference"><link href="35" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation> (line segment detector) 直线检测精确检测定位器, 如图2所示。</p>
                </div>
                <div class="area_img" id="58">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/DZCL201915035_058.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 定位器视觉检测系统" src="Detail/GetImg?filename=images/DZCL201915035_058.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 定位器视觉检测系统  <a class="btn-zoomin" href="Detail/GetImg?filename=images/DZCL201915035_058.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="59">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/DZCL201915035_059.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 定位器检测系统流程" src="Detail/GetImg?filename=images/DZCL201915035_059.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 定位器检测系统流程  <a class="btn-zoomin" href="Detail/GetImg?filename=images/DZCL201915035_059.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h3 id="60" name="60" class="anchor-tag"><b>2 接触网定位器关键区域的初定位</b></h3>
                <h4 class="anchor-tag" id="61" name="61"><b>2.1 Faster R-CNN的发展</b></h4>
                <div class="p1">
                    <p id="62">R-CNN采用了EdgeBoxes<citation id="112" type="reference"><link href="37" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>或者Selective Search<citation id="113" type="reference"><link href="39" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>等预处理手段来提取目标潜在区域, 由于需要对所有的目标潜在区域进行大量的重复计算, 计算速度出现严重的瓶颈。Fast R-CNN在R-CNN的大框架下采用SPP Net<citation id="114" type="reference"><link href="41" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>方法对整张图像提取一次卷积特征获得特征图, 随后在特征图上找到每个候选框的映射, 并将此映射作为每个候选框的卷积特征输入到后续神经网络, 显著提高了速度性能。为了进一步提高目标识别算法的精确率和实时性, Girshick R等提出了Faster R-CNN, 其最大的创新点在于设计了一种区域建议网络<citation id="115" type="reference"><link href="27" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation> (region proposal network, RPN) 。该区域建议网络是基于全卷积的, 通过端对端的方式对RPN进行训练获得目标的区域建议框, 随后将建议框输入到Fast R-CNN进行目标检测。RPN利用滑动窗口在输入的特征图上滑动, 在每个滑窗位置上得到<i>k</i>个anchor boxes, 整个特征图产生<i>W</i>×<i>H</i>×<i>k</i>个区域建议框 (<i>W</i>, <i>H</i>分别对应特征图的宽度和高度) ;同时把在每个滑窗位置上得到的低维特征向量输入到并行的卷积网络层对候选框进行预测目标分数和位置回归修正。RPN的多任务损失函数定位为公式 (1) :</p>
                </div>
                <div class="p1">
                    <p id="63" class="code-formula">
                        <mathml id="63"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>L</mi><mo stretchy="false"> (</mo><mo stretchy="false">{</mo><mi>p</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">}</mo><mo>, </mo><mo stretchy="false">{</mo><mi>r</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">}</mo><mo stretchy="false">) </mo><mo>=</mo><mfrac><mn>1</mn><mrow><mi>Ν</mi><msub><mrow></mrow><mrow><mi>c</mi><mi>l</mi><mi>s</mi></mrow></msub></mrow></mfrac><mstyle displaystyle="true"><munder><mo>∑</mo><mi>i</mi></munder><mi>L</mi></mstyle><msub><mrow></mrow><mrow><mi>c</mi><mi>l</mi><mi>s</mi></mrow></msub><mo stretchy="false"> (</mo><mi>p</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>p</mi><msubsup><mrow></mrow><mi>i</mi><mo>*</mo></msubsup><mo stretchy="false">) </mo><mo>+</mo><mi>η</mi><mfrac><mn>1</mn><mrow><mi>Ν</mi><msub><mrow></mrow><mrow><mi>r</mi><mi>e</mi><mi>g</mi></mrow></msub></mrow></mfrac><mstyle displaystyle="true"><munder><mo>∑</mo><mi>i</mi></munder><mi>p</mi></mstyle><msubsup><mrow></mrow><mi>i</mi><mo>*</mo></msubsup><mo>⋅</mo><mi>L</mi><msub><mrow></mrow><mrow><mi>r</mi><mi>e</mi><mi>g</mi></mrow></msub><mo stretchy="false"> (</mo><mi>c</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>c</mi><msubsup><mrow></mrow><mi>i</mi><mo>*</mo></msubsup><mo stretchy="false">) </mo><mo>=</mo><mo>-</mo><mfrac><mn>1</mn><mrow><mi>Ν</mi><msub><mrow></mrow><mrow><mi>c</mi><mi>l</mi><mi>s</mi></mrow></msub></mrow></mfrac><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow></munder><mrow><mi>log</mi></mrow></mstyle><mo stretchy="false"> (</mo><mi>p</mi><msub><mrow></mrow><mi>i</mi></msub><mo>⋅</mo><mi>p</mi><msubsup><mrow></mrow><mi>i</mi><mo>*</mo></msubsup><mo>+</mo><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>p</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>p</mi><msubsup><mrow></mrow><mi>i</mi><mo>*</mo></msubsup><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo>+</mo></mtd></mtr><mtr><mtd><mi>η</mi><mfrac><mn>1</mn><mrow><mi>Ν</mi><msub><mrow></mrow><mrow><mi>r</mi><mi>e</mi><mi>g</mi></mrow></msub></mrow></mfrac><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow></munder><mi>p</mi></mstyle><msubsup><mrow></mrow><mi>i</mi><mo>*</mo></msubsup><mo>⋅</mo><mtext>s</mtext><mtext>m</mtext><mtext>o</mtext><mtext>o</mtext><mtext>t</mtext><mtext>h</mtext><msub><mrow></mrow><mrow><mi>L</mi><msub><mrow></mrow><mn>1</mn></msub></mrow></msub><mo stretchy="false"> (</mo><mi>c</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>c</mi><msubsup><mrow></mrow><mi>i</mi><mo>*</mo></msubsup><mo stretchy="false">) </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="area_img" id="64">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/DZCL201915035_06400.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="66">式中:<i>p</i><sub><i>i</i></sub>为anchor预测为目标的概率;<i>p</i><sup>*</sup><sub><i>i</i></sub>表示当anchor为负标签时取0, 当anchor为正标签时取1;<i>c</i><sub><i>i</i></sub>表示预测的包络框4个参数化坐标;<i>c</i><sup>*</sup><sub><i>i</i></sub>表示正anchor对应的标注包络框的坐标 (ground truth) ;<i>L</i><sub><i>cls</i></sub>为两种类别 (前景和后景) 的对数损失, <i>L</i><sub><i>reg</i></sub>为回归损失, 该两项分别由<i>Ncls</i>和<i>N</i><sub><i>reg</i></sub>及平衡权重<i>η</i>归一化。</p>
                </div>
                <div class="p1">
                    <p id="67">Faster R-CNN模型结构可以看作是RPN和Fast R-CNN的高效结合体。RPN不仅可以高质量地获得目标的候选框, 还可以和Fast R-CNN检测网络共享全图的卷积特征, 全面提高了网络的检测性能。基于Faster R-CNN的定位器检测算法主要分为3个过程。首先对图像输入到特征提取深度卷积神经网络, 并将得到的卷积特征分别输入到RPN和Fast R-CNN。然后将RPN网络输出的目标候选框信息和Fast R-CNN输出的高级特征图一并输入到ROI pooling层进行特征映射, 最后全连接层分别对目标进行分类和对目标的包括框进行调整精修。</p>
                </div>
                <h4 class="anchor-tag" id="68" name="68"><b>2.2 FPN+Faster R-CNN定位器检测模型</b></h4>
                <div class="p1">
                    <p id="69">Faster R-CNN目标检测算法仅仅采用了其卷积特征提取网络层中的高级特征信息, 对于浅层的卷积特征并没有加以利用, 导致对于小目标的检测效果不是非常的好。深度卷积神经网络结构随着网络层深度的增加, 每层所提取的卷积特征信息也会变得不同, 比如来自高级卷积层的高级特征信息主要表示高级语义特征, 而取自低级卷积层的低级特征信息主要包含低级细节特征。从以往的经验和常识中判断, 如果目标检测的网络结构能够充分利用来自不同卷积层的多级卷积特征信息, 其检测效果在一定程度上会变得好一些。</p>
                </div>
                <div class="p1">
                    <p id="70">FPN检测算法模型如图3所示:核心思想是高层卷积特征经过上采样处理后与底层卷积特征进行融合形成多尺度特征, 实现在不同尺度的目标下利用不同尺度的卷积特征信息进行独立预测。模型主要有自底向上流程、自顶向下流程和横向连接三部分构成。自底向上流程即为卷积神经网络在前向计算时提取卷积特征的过程, 提取的卷积特征尺度大小会随着卷积层的深度而改变, 把不会改变卷积特征尺度的卷积层归为一阶段。每一阶段的最后一层卷积层包含了最强的卷积特征, 所以提取每一阶段的最后一层卷积特征{<i>C</i><sub>2</sub>, <i>C</i><sub>3</sub>, <i>C</i><sub>4</sub>, <i>C</i><sub>5</sub>}构成特征金字塔。由于<i>C</i><sub>1</sub>特征维度过大, 为了更高的内存使用率和网络计算效率, 不参与构建特征金字塔。自顶向下流程即对前述所提取每步卷积特征进行上采样处理, 保证处理后的高层卷积特征维度与自底向上的低层特征维度相同, 随后将高级特征与低层特征像素相加, 即为横向连接得到融合特征{<i>P</i><sub>2</sub>, <i>P</i><sub>3</sub>, <i>P</i><sub>4</sub>, <i>P</i><sub>5</sub>}。随后利用卷积核对融合后的特征做卷积处理以消除上采样的混叠效应。</p>
                </div>
                <div class="area_img" id="71">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/DZCL201915035_071.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 RPN网络结构" src="Detail/GetImg?filename=images/DZCL201915035_071.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 RPN网络结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/DZCL201915035_071.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="72">原Faster R-CNN中的RPN网络结构是把卷积特征提取网络层的最顶层特征图作为输入, 仅仅利用了单一尺度的特征信息。将FPN网络结构嵌入到RPN网络中, 可以产生不同尺度的特征信息。分别在5种不同尺度下{<i>P</i><sub>2</sub>, <i>P</i><sub>3</sub>, <i>P</i><sub>4</sub>, <i>P</i><sub>5</sub>, <i>P</i><sub>6</sub>}的卷积特征图上提取相对应固定尺度大小{32<sup>2</sup>, 64<sup>2</sup>, 128<sup>2</sup>, 256<sup>2</sup>, 512<sup>2</sup>}的anchor, 同时每一个尺度下的anchor具有3不同的长宽比例{1∶2, 1∶1, 2∶1}, 这样整个特征网络金字塔中共有15中不同的anchor。将FPN网络结构与Faster R-CNN网络融合到一个网络, 整体网络结构如图4所示。</p>
                </div>
                <div class="area_img" id="73">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/DZCL201915035_073.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 FPN+Faster R-CNN网络结构" src="Detail/GetImg?filename=images/DZCL201915035_073.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 FPN+Faster R-CNN网络结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/DZCL201915035_073.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="74">FPN网络结构嵌入到Faster R-CNN之后, ROI需要在不同尺度的特征图上进行映射提取特征信息, 如公式所示:</p>
                </div>
                <div class="p1">
                    <p id="75" class="code-formula">
                        <mathml id="75"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>k</mi><mo>=</mo><mi>k</mi><msub><mrow></mrow><mn>0</mn></msub><mo>+</mo><mrow><mi>log</mi></mrow><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false"> (</mo><msqrt><mrow><mi>w</mi><mi>h</mi></mrow></msqrt><mo>/</mo><mn>2</mn><mn>2</mn><mn>4</mn><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="76">式中:<i>k</i>表示FPN网络结构中特征金字塔的<i>P</i><sub><i>k</i></sub>层;<i>w</i>, <i>h</i>分别表示ROI的宽和高。</p>
                </div>
                <h3 id="77" name="77" class="anchor-tag"><b>3 接触网定位器的精检算法</b></h3>
                <div class="p1">
                    <p id="78">FPN+ Faster R-CNN模型获得了定位器的关键区域之后, 我们根据关键区域的像平面坐标将其从原接触网图像中裁剪下来。首先将其图像原来的RGB颜色空间转换至灰度空间, 由公式 (4) 所示:</p>
                </div>
                <div class="p1">
                    <p id="79"><i>Y</i>=0.3<i>R</i>+0.59<i>G</i>+0.11<i>B</i>      (4) </p>
                </div>
                <div class="p1">
                    <p id="80">然后对定位器图像进行LSD直线检测, 最后筛选检测到的直线段来拟合定位器。</p>
                </div>
                <div class="p1">
                    <p id="81">直线的检测就是寻找图像中梯度变化较大的像素, 所以LSD<citation id="116" type="reference"><link href="35" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation> (line segment detector) 算法于2008年提出, 可以在线性时间内检测到亚像素级准确度的直线段, 速度远远快于直线检测常用的霍夫变换算法。LSD算法中存在着梯度和level-line两个基本概念, 梯度方向和level-line方向正交, 如图5所示。直观来讲, 像素组成的某个区域越细长时越有可能是直线段。LSD首先计算图像上每一个像素和level-line的夹角形成一个level-line场, 合并场里相似方向的像素构成直线支持区域 (line support regions) , 该区域即为潜在直线段的候选, 如图6所示。然后统计直线支持区域的最小外接矩形主方向, 当区域内像素的level-line角度与外接矩形主方向之间的角度差在某个容忍度内, 则把该像素记作校准点 (aligned point) 。最后统计最小外接矩形内所有像素数和校准点个数, 根据contrario准则<citation id="117" type="reference"><link href="43" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>判定该直线支持区域是否为一个直线段。直线支持区域的行成采用的是区域生长算法。首先选择梯度幅值较大的像素点作为种子像素, 把区域角度设置为level-line角度。然后在特定的容忍度范围内, 将种子像素领域中和区域角度相似的像素合并到线性支持区域对其进行更新, 此时区域角度<i>θ</i><sub><i>region</i></sub>的更新如公式 (5) 所示:</p>
                </div>
                <div class="p1">
                    <p id="82" class="code-formula">
                        <mathml id="82"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>θ</mi><msub><mrow></mrow><mrow><mi>r</mi><mi>e</mi><mi>g</mi><mi>i</mi><mi>o</mi><mi>n</mi></mrow></msub><mo>=</mo><mrow><mi>arctan</mi></mrow><mfrac><mrow><mstyle displaystyle="true"><msub><mo>∑</mo><mi>j</mi></msub><mtext>s</mtext></mstyle><mtext>i</mtext><mtext>n</mtext><mo stretchy="false"> (</mo><mi>θ</mi><msubsup><mrow></mrow><mrow><mtext>l</mtext><mtext>e</mtext><mtext>v</mtext><mtext>e</mtext><mtext>l</mtext><mo>-</mo><mtext>l</mtext><mtext>i</mtext><mtext>n</mtext><mtext>e</mtext></mrow><mi>j</mi></msubsup><mo stretchy="false">) </mo></mrow><mrow><mstyle displaystyle="true"><msub><mo>∑</mo><mi>j</mi></msub><mtext>c</mtext></mstyle><mtext>o</mtext><mtext>s</mtext><mo stretchy="false"> (</mo><mi>θ</mi><msubsup><mrow></mrow><mrow><mtext>l</mtext><mtext>e</mtext><mtext>v</mtext><mtext>e</mtext><mtext>l</mtext><mo>-</mo><mtext>l</mtext><mtext>i</mtext><mtext>n</mtext><mtext>e</mtext></mrow><mi>j</mi></msubsup><mo stretchy="false">) </mo></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>5</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="83">式中:<i>θ</i><sub><i>region</i></sub>表示线性支持区域角度;<i>θ</i><mathml id="84"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mtext>l</mtext><mtext>e</mtext><mtext>v</mtext><mtext>e</mtext><mtext>l</mtext><mo>-</mo><mtext>l</mtext><mtext>i</mtext><mtext>n</mtext><mtext>e</mtext></mrow><mi>j</mi></msubsup></mrow></math></mathml>表示第<i>j</i>个像素level-line角度。当不再有像素满足以上准则合并到线性支持区域时, 区域生长算法停止。LSD算法作者R. Von Gioi<citation id="118" type="reference"><link href="35" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>提出当容忍度角度设置为22.5°时, 直线检测的效果最好。为了降低直线检测的误检率, 我们把容忍度角度值设置在3°～22.5°范围内并做测试, 测试结果表明容忍度为5°～15°时检测效果最好。我们从定位器关键区域的直线检测结果中可以看出, 属于定位器上下边缘的直线段是最长且平行。因此根据检测得到的直线段端点坐标计算二范距离并做降序排列, 同时筛选线段方向相似的一组直线段, 最后计算被筛选出线段端点坐标的平均值作为拟合定位器骨架的最佳线段端点, 如图7中的绿线所示。</p>
                </div>
                <div class="area_img" id="85">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/DZCL201915035_085.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 梯度和level-line" src="Detail/GetImg?filename=images/DZCL201915035_085.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 梯度和level-line  <a class="btn-zoomin" href="Detail/GetImg?filename=images/DZCL201915035_085.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="86">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/DZCL201915035_086.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 直线支持区域" src="Detail/GetImg?filename=images/DZCL201915035_086.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 直线支持区域  <a class="btn-zoomin" href="Detail/GetImg?filename=images/DZCL201915035_086.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="87">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/DZCL201915035_087.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 基于LSD的定位器直线检测结果" src="Detail/GetImg?filename=images/DZCL201915035_087.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 基于LSD的定位器直线检测结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/DZCL201915035_087.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h3 id="88" name="88" class="anchor-tag"><b>4 实验分析</b></h3>
                <h4 class="anchor-tag" id="89" name="89"><b>4.1 接触网定位器图像集和实验平台</b></h4>
                <div class="p1">
                    <p id="90">定位器图像集来源于4C弓网检测车于某一高速铁路进行弓网检测实验所拍摄的图像数据。从弓网检测实验所拍摄的接触网图片选取4 000张作为评估算法的图像数据集, 其中2 500张图像作为训练样本集, 剩余1 500张图像作为测试样本集。拍摄背景可以分为无隧道背景和有隧道背景, 在每一个背景下都包含了不同的拍摄角度、不同强度的光照和遮挡情况, 图像样本集的复杂性可以使算法的鲁棒性更强。深度卷积神经网络输入的接触网图像由原6 600×4 400像素等比例缩小至900×600像素, 并对其零均值化, 以减低内存占用和提高计算效率。我们选取在ImageNet上预训练好的Resnet<citation id="119" type="reference"><link href="45" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>模型对本文所提的定位器检测系统进行参数初始化。Caffe<citation id="120" type="reference"><link href="47" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>具有优越的并行计算能力, 被作为深度学习网络的框架。实验平台为NVDIA TITAN X GPU和Ubuntu操作系统。</p>
                </div>
                <h4 class="anchor-tag" id="91" name="91"><b>4.2 实验结果与分析</b></h4>
                <div class="area_img" id="92">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/DZCL201915035_092.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 上图:无隧道环境下, 下图:有隧道环境下的四种定位器检测模型效果比较" src="Detail/GetImg?filename=images/DZCL201915035_092.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图8 上图:无隧道环境下, 下图:有隧道环境下的四种定位器检测模型效果比较  <a class="btn-zoomin" href="Detail/GetImg?filename=images/DZCL201915035_092.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="93">为了进一步客观评价基于FPN+ Faster R-CNN的定位器检测模型的性能, 我们选取在目标检测领域表现优异的HOG<citation id="121" type="reference"><link href="49" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>、DPM<citation id="122" type="reference"><link href="51" rel="bibliography" /><sup>[<a class="sup">25</a>]</sup></citation>模型和Faster R-CNN模型进行比较。HOG和DPM这两个模型都选择在3.60 GHz Intel i7-7700 CPU硬件平台上进行实验。图8展示了前面所提的4种定位器检测模型的检测效果差异。从图8中我们可以很明显地看出FPN+ Faster R-CNN模型具有最高的精确率和回召率。基于HOG+SVM和DPM的定位器识别模型受限于人工设计目标的特征描述子, 它们的检测性能相对来说比较差, 精确率和回召率大约在60%～70%之间。Faster R-CNN采用深度卷积神经网络技术提高了检测精确率和回召率, 达到了90%左右。FPN+ Faster R-CNN融合了来自高级卷积层的抽象语义特征和来自低级卷积层的细节特征信息, 包含了定位器更丰富的特征信息, 检测精确率和回召率可以达到98%。由于在隧道环境拍摄的接触网图像具有较复杂的背景, 以上4种定位器检测模型性能与无隧道环境下相比下降了1%～2%。</p>
                </div>
                <div class="p1">
                    <p id="94">模型检测的速度在接触网定位器检测实际应用中也是一个重要的考虑因素, 表1展示了以上4种模型检测帧率FPS (frames per second) 的结果。与传统的目标检测模型HOG和DPM相比, Faster R-CNN的帧率平均值最高 (4.86) , 大约分别是传统HOG+ SVM和DPM的2.78和11.85倍。FPN+ Faster R-CNN的检测帧率为4.70, 与Faster R-CNN相比其检测速度稍微下降, 主要原因是引入了FPN网络结构, 增加了网络模型的复杂性, 但仍然具有良好的检测实时性。同时, FPN+ Faster R-CNN检测帧率的标准差只有0.14, 具有良好的鲁棒性。说明该检测模型在检测速度方面表现出良好的稳定性。</p>
                </div>
                <div class="area_img" id="95">
                    <p class="img_tit"><b>表1 不同模型定位器检测帧率的比较</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note">（m) </p>
                    <table id="95" border="1"><tr><td><br />FPS</td><td>HOG+<br />SVM</td><td>DPM</td><td>Faster <br />R-CNN</td><td>FPN+Faster <br />R-CNN</td></tr><tr><td><br />平均值</td><td>1.75</td><td>0.41</td><td>4.86</td><td>4.70</td></tr><tr><td><br />标准差</td><td>0.30</td><td>0.26</td><td>0.15</td><td>0.14</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="96">定位器最终检测结果如图9所示, 图片第1行和第2行分别展示在无隧道环境下和有隧道环境下的检测结果。综合以上分析, 基于FPN+ Faster R-CNN的定位器检测模型表现出最佳的检测性能, 具有最高的精确率、回召率和较高的检测速度, 满足实际检测应用。</p>
                </div>
                <div class="area_img" id="97">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/DZCL201915035_097.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图9 基于FPN+ Faster R-CNN的定位器检测模型检测结果" src="Detail/GetImg?filename=images/DZCL201915035_097.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图9 基于FPN+ Faster R-CNN的定位器检测模型检测结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/DZCL201915035_097.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h3 id="98" name="98" class="anchor-tag"><b>5 结  论</b></h3>
                <div class="p1">
                    <p id="99">在本文中, 针对传统高速列车接触网定位器检测系统具有精确率低、实时性差和鲁棒性弱的问题, 我们创新性地提出一种基于FPN+ Faster R-CNN的高速列车接触网定位器检测模型, 整体系统采用由粗检到精检的策略。本文设计的定位器检测系统主要有以下2个部分:1) 在接触网定位器的关键区域初定位过程中, 检测系统采用了FPN+Faster R-CNN, 该网络模型能够融合更加抽象、更多语义的深层卷积特征和更高分辨率、更多细节信息的浅层卷积特征, 有效提高了定位器小目标的检测效果。2) 在FPN+ Faster R-CNN获得定位器所在的关键区域后, 利用LSD直线检测技术检测定位器。与传统机器学习HOG和DPM模型相比, 本文所提出的模型在检测精确率和回召率方面提高了大约30%, 检测速度提高了大约10倍;与Faster R-CNN模型相比, FPN+ Faster R-CNN的精确率和回召率提高了大约7%, 检测速度基本没有下降。本文的研究工作对今后定位器检测更加深入的实际应用奠定了基础, 对保障高速列车的安全行驶有着重要的意义。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Detection and monitoring system of the pantograph-catenary in high-speed railway (6C)">

                                <b>[1]</b> GAO S B, LIU Z G, YU L.Detection and monitoring system of the pantograph-catenary in high-speed railway (6C) [C]//International Conference on Power Electronics Systems and Applications-Smart Mobility, Power Transfer &amp; Security.2017:1-7.
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=TDGC201301015&amp;v=MjkwMzF0Rnl2Z1ZML05NU25NYmJHNEg5TE1ybzlFWVlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVo=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> 罗健, 白裔峰, 魏博.高速铁路接触网定位器坡度问题的深化研究[J].铁道工程学报, 2013, 30 (1) :76-80.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY2010S2034&amp;v=MDQ4MzdCdEdGckNVUjdxZlp1WnRGeXZnVkwvTkx6N0JkN0c0SDlHdnJZOUdZSVFLREg4NHZSNFQ2ajU0TzN6cXE=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> 范虎伟, 卞春华, 朱挺, 等.非接触式接触网定位器坡度自动检测技术[J].计算机应用, 2010, 30 (S2) :102-103, 128.
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=QHXB201112006&amp;v=MzE3MDlqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVadEZ5dmdWTC9OTkNYVGJMRzRIOUROclk5RllvUUtESDg0dlI0VDY=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b> 段汝娇, 赵伟, 黄松岭, 等.基于模糊ID3决策树的快速角点检测算法[J].清华大学学报 (自然科学版) , 2011, 51 (12) :1787-1791.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GDJT201405011&amp;v=MjI0MTMzenFxQnRHRnJDVVI3cWZadVp0Rnl2Z1ZML05JaW5CZXJHNEg5WE1xbzlFWllRS0RIODR2UjRUNmo1NE8=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b> 于博轩, 陈唐龙, 于龙, 等.基于图像处理中霍夫变换的定位器坡度检测[J].城市轨道交通研究, 2014, 17 (5) :32-36.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GSTL201403003&amp;v=MTczODRSN3FmWnVadEZ5dmdWTC9OSWo3ZllyRzRIOVhNckk5Rlo0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1U=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b> 王旭东, 吴积钦, 徐可佳, 等.基于AdaBoost算法的接触网定位器识别[J].高速铁路技术, 2014, 5 (3) :9-12.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=1014257602.nh&amp;v=MzA0Njl1WnRGeXZnVkwvTlZGMjZHckc5R2RmTXJaRWJQSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlo=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b> 顾会建.基于视频图片的接触网定位器识别方法研究[D].成都:西南交通大学, 2014.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Imagenet classification with deep convolutional neural networks">

                                <b>[8]</b> KRIZHEVSKY A, SUTSKEVER I, HINTON G.Imagenet classification with deep convolutional neural networks[J].Advances in neural information processing systems, 2012, 25 (2) .
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Imagenet:A large-scale hierarchical image database">

                                <b>[9]</b> DENG J, DONG W, SOCHER R, et al.ImageNet:A large-scale hierarchical image database[C]//Computer Vision and Pattern Recognition, 2009.CVPR 2009.IEEE Conference on.IEEE, 2009:248-255.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Overfeat:Integrated recognition,localization and detection using convolutional networks">

                                <b>[10]</b> SERMANET, PIERRE, et al.Overfeat:Integrated recognition, localization and detection using convolutional networks[J].arXiv preprint arXiv:1312.6229, 2013.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_11" >
                                    <b>[11]</b>
                                 GIRSHICK R, DONAHUE J, DARRELL T, et al.Rich feature hierarchies for accurate object detection and semantic segmentation[C]//Proceedings of the IEEE conference on computer vision and pattern recognition.2014:580-587.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fast R-CNN">

                                <b>[12]</b> GIRSHICK R.Fast R-CNN[J].Computer Science, 2015.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Faster R-CNN:Towards Real-Time Object Detection with Region Proposal Networks">

                                <b>[13]</b> REN S, HE K, GIRSHICK R, et al.Faster R-CNN:Towards real-time object detection with region proposal networks[J].IEEE Transactions on Pattern Analysis &amp; Machine Intelligence, 2017, 39 (6) :1137.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=High-speed railway catenary components detection using the cascaded convolutional neural networks">

                                <b>[14]</b> CHEN J, LIU Z, WANG H, et al.High-speed railway catenary components detection using the cascaded convolutional neural networks[C]//IEEE International Conference on Imaging Systems and Techniques.IEEE, 2017:1-6.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_15" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZKJD201704006&amp;v=MjA5NDRhckc0SDliTXE0OUZZb1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnRGeXZnVkwvTlB5YkI=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[15]</b> 陈东杰, 张文生, 杨阳.基于深度学习的高铁接触网定位器检测与识别[J].中国科学技术大学学报, 2017 (4) .
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_16" >
                                    <b>[16]</b>
                                 LIN, TSUNG-YI, et al.Feature pyramid networks for object detection[C].Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.2017.
                            </a>
                        </p>
                        <p id="35">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=LSD: A Fast Line Segment Detector with a False Detection Control">

                                <b>[17]</b> VON GIOI, RAFAEL GROMPONE, et al.LSD:A fast line segment detector with a false detection control.IEEE transactions on pattern analysis and machine intelligence, 2010, 32 (4) :722-732.
                            </a>
                        </p>
                        <p id="37">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Edge boxes:Locating object proposals from edges">

                                <b>[18]</b> ZITNICK C L, DOLLÁR P.Edge Boxes:locating object proposals from edges[M]//Computer Vision-ECCV 2014.Springer International Publishing, 2014:391-405.
                            </a>
                        </p>
                        <p id="39">
                            <a id="bibliography_19" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD13080200013634&amp;v=MDM4MzNCTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaW5sVXJ6SUkxc1RhaGM9Tmo3QmFySzdIdG5Nclk5RlpPb01Dbjg5bw==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[19]</b> UIJLINGS J R R, VAN DE SANDE K E A, Gevers T, et al.Selective search for object recognition[J].International journal of computer vision, 2013, 104 (2) :154-171.
                            </a>
                        </p>
                        <p id="41">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Spatial pyramid pooling in deep convolutional networks for visual recognition">

                                <b>[20]</b> HE K, ZHANG X, REN S, et al.Spatial pyramid pooling in deep convolutional networks for visual recognition[J].IEEE Transactions on Pattern Analysis &amp; Machine Intelligence, 2014, 37 (9) :1904-1916.
                            </a>
                        </p>
                        <p id="43">
                            <a id="bibliography_21" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002830642&amp;v=MTI1NTlGWXU4TlkzazV6QmRoNGo5OVNYcVJyeG94Y01IN1I3cWRaK1p1Rml2bFVMclBJRnM9Tmo3QmFyTzRIdEhPcDR4&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[21]</b> DESOLNEUX A, MOISAN L, MOREL J M.Meaningful alignments[J].International Journal of Computer Vision, 2000, 40 (1) :7-23.
                            </a>
                        </p>
                        <p id="45">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep residual learning for image recognition">

                                <b>[22]</b> HE, KAIMING, et al.Deep residual learning for image recognition[C].Proceedings of the IEEE conference on computer vision and pattern recognition.2016.
                            </a>
                        </p>
                        <p id="47">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=An Open Source Convolutional Architecture for Fast Feature Embedding">

                                <b>[23]</b> JIA Y.Caffe:An open source convolutional architecture for fast feature embedding.http://caffe.berkeleyvision.org/, 2013.
                            </a>
                        </p>
                        <p id="49">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Histograms of Oriented Gradients for Human Detection">

                                <b>[24]</b> DALAL N, TRIGGS B.Histograms of oriented gradients for human detection[C]//IEEE Computer Society Conference on Computer Vision &amp; Pattern Recognition.IEEE Computer Society, 2005:886-893.
                            </a>
                        </p>
                        <p id="51">
                            <a id="bibliography_25" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A discriminatively trained, multiscale, deformable part model">

                                <b>[25]</b> FELZENSZWALB P, MCALLESTER D, RAMANAN D.A discriminatively trained, multiscale, deformable part model[J].Cvpr, 2008, 8:1-8.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="DZCL201915035" />
        <input id="dpi" type="hidden" value="600" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZCL201915035&amp;v=MTgxOTNvOUdZWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnRGeXZnVkwvTklUZklZckc0SDlqTnE=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>


    <link href="/kxreader/Content/css/LeftDetail?v=NLcKG8I1SJUaVFrQ0iGpF2klAT0OsmHRaVSZ1rKb5xg1" rel="stylesheet"/>

</body>
</html>

