

<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>

</head>

<body>

    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637135660051506250%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dDZCL201915024%26RESULT%3d1%26SIGN%3d%252f0PMBNUL6xd%252b2uDIUBTj6%252bH8F0c%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=DZCL201915024&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=DZCL201915024&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>


    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZCL201915024&amp;v=MjY5MzBadEZ5dmdWTHJCSVRmSVlyRzRIOWpOcW85SFlJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnU=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#51" data-title="&lt;b&gt;0 引  言&lt;/b&gt; "><b>0 引  言</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#56" data-title="&lt;b&gt;1 相关工作&lt;/b&gt; "><b>1 相关工作</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#57" data-title="&lt;b&gt;1.1 孪生网络&lt;/b&gt;"><b>1.1 孪生网络</b></a></li>
                                                <li><a href="#64" data-title="&lt;b&gt;1.2 生成式对抗网络&lt;/b&gt;"><b>1.2 生成式对抗网络</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#75" data-title="&lt;b&gt;2 孪生生成式对抗网络&lt;/b&gt; "><b>2 孪生生成式对抗网络</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#76" data-title="&lt;b&gt;2.1 整体框架&lt;/b&gt;"><b>2.1 整体框架</b></a></li>
                                                <li><a href="#80" data-title="&lt;b&gt;2.2 骨架关键点&lt;/b&gt;"><b>2.2 骨架关键点</b></a></li>
                                                <li><a href="#82" data-title="&lt;b&gt;2.3 图像生成器&lt;/b&gt;"><b>2.3 图像生成器</b></a></li>
                                                <li><a href="#84" data-title="&lt;b&gt;2.4 姿态判别器&lt;/b&gt;"><b>2.4 姿态判别器</b></a></li>
                                                <li><a href="#88" data-title="&lt;b&gt;2.5 非姿态判别器&lt;/b&gt;"><b>2.5 非姿态判别器</b></a></li>
                                                <li><a href="#91" data-title="&lt;b&gt;2.6 验证器&lt;/b&gt;"><b>2.6 验证器</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#96" data-title="&lt;b&gt;3 实 验&lt;/b&gt; "><b>3 实 验</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#97" data-title="&lt;b&gt;3.1 数据集&lt;/b&gt;"><b>3.1 数据集</b></a></li>
                                                <li><a href="#102" data-title="&lt;b&gt;3.2 评价指标和实验步骤&lt;/b&gt;"><b>3.2 评价指标和实验步骤</b></a></li>
                                                <li><a href="#109" data-title="&lt;b&gt;3.3 权重系数分析&lt;/b&gt;"><b>3.3 权重系数分析</b></a></li>
                                                <li><a href="#117" data-title="&lt;b&gt;3.4 实验结果与分析&lt;/b&gt;"><b>3.4 实验结果与分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#124" data-title="&lt;b&gt;4 结  论&lt;/b&gt; "><b>4 结  论</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#59" data-title="图1 孪生网络结构示意图">图1 孪生网络结构示意图</a></li>
                                                <li><a href="#66" data-title="图2 GAN网络结构示意图">图2 GAN网络结构示意图</a></li>
                                                <li><a href="#79" data-title="图3 SGAN网络结构示意图">图3 SGAN网络结构示意图</a></li>
                                                <li><a href="#100" data-title="&lt;b&gt;表1 数据集信息&lt;/b&gt;"><b>表1 数据集信息</b></a></li>
                                                <li><a href="#112" data-title="图4 Market1501-&lt;i&gt;β&lt;/i&gt;&lt;sub&gt;1&lt;/sub&gt;对top-1的影响">图4 Market1501-<i>β</i><sub>1</sub>对top-1的影响</a></li>
                                                <li><a href="#113" data-title="图5 Market1501-&lt;i&gt;β&lt;/i&gt;&lt;sub&gt;1&lt;/sub&gt;对mAP的影响">图5 Market1501-<i>β</i><sub>1</sub>对mAP的影响</a></li>
                                                <li><a href="#115" data-title="图6 Market1501-&lt;i&gt;β&lt;/i&gt;&lt;sub&gt;2&lt;/sub&gt;对top-1的影响">图6 Market1501-<i>β</i><sub>2</sub>对top-1的影响</a></li>
                                                <li><a href="#116" data-title="图7 Market1501-&lt;i&gt;β&lt;/i&gt;&lt;sub&gt;2&lt;/sub&gt;对mAP的影响">图7 Market1501-<i>β</i><sub>2</sub>对mAP的影响</a></li>
                                                <li><a href="#119" data-title="&lt;b&gt;表2 多种方法在数据集Market1501上的实验效果对比&lt;/b&gt;"><b>表2 多种方法在数据集Market1501上的实验效果对比</b></a></li>
                                                <li><a href="#122" data-title="&lt;b&gt;表3 多种方法在数据集CUHK03上的实验效果对比&lt;/b&gt;"><b>表3 多种方法在数据集CUHK03上的实验效果对比</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" title=" 宋婉茹, 赵晴晴, 陈昌红, 等.行人重识别研究综述[J].智能系统学报, 2017, 12 (6) :770-780." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZNXT201706002&amp;v=MTY3ODZxZlp1WnRGeXZnVkxyQlB5UFRlckc0SDliTXFZOUZab1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjc=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         宋婉茹, 赵晴晴, 陈昌红, 等.行人重识别研究综述[J].智能系统学报, 2017, 12 (6) :770-780.
                                    </a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" title=" SHI Z, HOSPEDALES T M, Tao X.Transferring a semantic representation for person re-identification and search[C]//Computer Vision and Pattern Recognition, 2015:4184-4193." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Transferring a semantic representation for person re-identification and search">
                                        <b>[2]</b>
                                         SHI Z, HOSPEDALES T M, Tao X.Transferring a semantic representation for person re-identification and search[C]//Computer Vision and Pattern Recognition, 2015:4184-4193.
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title=" MA B, YU S, JURIE F.Local Descriptors Encoded by Fisher Vectors for Person Re-identification[C]//International Conference on Computer Vision, 2012:413-422." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Local Descriptors Encoded by Fisher Vectors for Person Re-identification">
                                        <b>[3]</b>
                                         MA B, YU S, JURIE F.Local Descriptors Encoded by Fisher Vectors for Person Re-identification[C]//International Conference on Computer Vision, 2012:413-422.
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" >
                                        <b>[4]</b>
                                     LIAO S, HU Y, ZHU N X.Person re-identification by Local Maximal Occurrence representation and metric learning[C]//IEEE Conference on Computer Vision and Pattern Recognition, 2015:2197-2206.</a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" title=" DING S, LIANG L, WANG G, et al.Deep feature learning with relative distance comparison for person re-identification[J].Pattern Recognition, 2015, 48 (10) :2993-3003." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES15122300162578&amp;v=MDEzMTdCTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaW5sVXJ6SUkxc1RieHM9TmlmT2ZiSzlIOVBPckk5RlplME5DWHN4bw==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                         DING S, LIANG L, WANG G, et al.Deep feature learning with relative distance comparison for person re-identification[J].Pattern Recognition, 2015, 48 (10) :2993-3003.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" >
                                        <b>[6]</b>
                                     罗浩, 姜伟, 范星, 张思朋.基于深度学习的行人重识别研究进展[J/OL], 自动化学报:1-18.</a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" title=" ZHENG Z, ZHENG L, YANG Y.Pedestrian alignment network for large-scale Person re-identification[J].IEEE Transactions on Circuits and Systems for Video Technology, 2017:498-512." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Pedestrian alignment network for large-scale Person re-identification">
                                        <b>[7]</b>
                                         ZHENG Z, ZHENG L, YANG Y.Pedestrian alignment network for large-scale Person re-identification[J].IEEE Transactions on Circuits and Systems for Video Technology, 2017:498-512.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" title=" ZHAO H, TIAN M, SUN S, et al.Spindle net:Person re-identification with human body region guided feature decomposition and fusion[C]//IEEE Conference on Computer Vision and Pattern Recognition, 2017:1077-1085." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Spindle net:Person re-identification with human body region guided feature decomposition and fusion">
                                        <b>[8]</b>
                                         ZHAO H, TIAN M, SUN S, et al.Spindle net:Person re-identification with human body region guided feature decomposition and fusion[C]//IEEE Conference on Computer Vision and Pattern Recognition, 2017:1077-1085.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" title=" BERTINETTO L, VALMADRE J, HENRIQUES J F, et al.Fully-convolutional siamese networks for object tracking[C]//European Conference on Computer Vision, 2016:850-865." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fully-Convolutional Siamese Networks for Object Tracking">
                                        <b>[9]</b>
                                         BERTINETTO L, VALMADRE J, HENRIQUES J F, et al.Fully-convolutional siamese networks for object tracking[C]//European Conference on Computer Vision, 2016:850-865.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_10" title=" INSAFUTDINOV E, PISHCHULIN L, ANDRES B, et al.DeeperCut:A deeper, stronger, and faster multi-person pose estimation model[C]//European Conference on Computer Vision, 2016:34-50." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deepercut:A deeper,stronger,and faster multiperson pose estimation model">
                                        <b>[10]</b>
                                         INSAFUTDINOV E, PISHCHULIN L, ANDRES B, et al.DeeperCut:A deeper, stronger, and faster multi-person pose estimation model[C]//European Conference on Computer Vision, 2016:34-50.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_11" title=" MARMANIS D, DATCU M, ESCH T, et al.Deep learning earth observation classification using imagenet pretrained networks[J].IEEE Geoscience and Remote Sensing Letters, 2016, 13 (1) :105-109." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep learning earth observation classification using image Net pretrained networks">
                                        <b>[11]</b>
                                         MARMANIS D, DATCU M, ESCH T, et al.Deep learning earth observation classification using imagenet pretrained networks[J].IEEE Geoscience and Remote Sensing Letters, 2016, 13 (1) :105-109.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_12" title=" ALIOUA N, AMINE A, ROGOZAN A, et al.Driver head pose estimation using efficient descriptor fusion[J].EURASIP Journal on Image and Video Processing, 2016 (1) :1-14." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJDF91ABE75F707F3C5549995295CA360E5&amp;v=MzAwMThNVzdqWjBRWHJncFJkR0NMR1NSYythQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQnJMVTA1dHRoeDcyNHhhRT1OajdCYXNXeEg2Qysyb2hBRXV3UEN3bzZ2Qg==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[12]</b>
                                         ALIOUA N, AMINE A, ROGOZAN A, et al.Driver head pose estimation using efficient descriptor fusion[J].EURASIP Journal on Image and Video Processing, 2016 (1) :1-14.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_13" title=" ZHENG L, SHEN L, TIAN L, et al.Scalable person re-identification:A benchmark[C]//IEEE International Conference on Computer Vision, 2015:1116-1124." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Scalable person re-identification A benchmark">
                                        <b>[13]</b>
                                         ZHENG L, SHEN L, TIAN L, et al.Scalable person re-identification:A benchmark[C]//IEEE International Conference on Computer Vision, 2015:1116-1124.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_14" >
                                        <b>[14]</b>
                                     ZHONG Z, ZHENG L, CAO D, et al.Re-ranking Person re-identification with k-reciprocal encoding[C]//IEEE Conference on Computer Vision and Pattern Recognition, 2017:3652-3661.</a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_15" title=" SU C, ZHANG S, XING J, et al.Deep attributes driven multi-camera person re-identification[C]//European Conference on Computer Vision, 2016:475-491." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep attributes driven multi-camera person">
                                        <b>[15]</b>
                                         SU C, ZHANG S, XING J, et al.Deep attributes driven multi-camera person re-identification[C]//European Conference on Computer Vision, 2016:475-491.
                                    </a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_16" title=" LI Z, TAO X, GONG S.Learning a discriminative null space for person re-identification[C]//Computer Vision and Pattern Recognition, 2016:1239-1248." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning a Discriminative Null Space for Person Re-identification">
                                        <b>[16]</b>
                                         LI Z, TAO X, GONG S.Learning a discriminative null space for person re-identification[C]//Computer Vision and Pattern Recognition, 2016:1239-1248.
                                    </a>
                                </li>
                                <li id="35">


                                    <a id="bibliography_17" title=" VARIOR R R, HALOI M, GANG W.Gated siamese convolutional neural network architecture for human re-identification[C]//European Conference on Computer Vision, 2016:791-808." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Gated siamese convolutional neural network architecture for human re-identification">
                                        <b>[17]</b>
                                         VARIOR R R, HALOI M, GANG W.Gated siamese convolutional neural network architecture for human re-identification[C]//European Conference on Computer Vision, 2016:791-808.
                                    </a>
                                </li>
                                <li id="37">


                                    <a id="bibliography_18" title=" LI D, CHEN X, ZHANG Z, et al.Learning deep context-aware features over body and latent parts for person reidentification[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017:384-393." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning deep context-aware features over body and latent parts for person re-identification">
                                        <b>[18]</b>
                                         LI D, CHEN X, ZHANG Z, et al.Learning deep context-aware features over body and latent parts for person reidentification[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017:384-393.
                                    </a>
                                </li>
                                <li id="39">


                                    <a id="bibliography_19" title=" SUN Y, ZHENG L, DENG W, et al.SVDNet for pedestrian retrieval[C]//IEEE International Conference on Computer Vision, 2017:3820-3828." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=SVDNet for Pedestrian Retrieval">
                                        <b>[19]</b>
                                         SUN Y, ZHENG L, DENG W, et al.SVDNet for pedestrian retrieval[C]//IEEE International Conference on Computer Vision, 2017:3820-3828.
                                    </a>
                                </li>
                                <li id="41">


                                    <a id="bibliography_20" title=" WEI L, RUI Z, TONG X, et al.DeepReID:Deep Filter Pairing neural network for person re-identification[C]//Computer Vision and Pattern Recognition, 2014:152-159." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=DeepReID:Deep Filter Pairing neural network for person re-identification">
                                        <b>[20]</b>
                                         WEI L, RUI Z, TONG X, et al.DeepReID:Deep Filter Pairing neural network for person re-identification[C]//Computer Vision and Pattern Recognition, 2014:152-159.
                                    </a>
                                </li>
                                <li id="43">


                                    <a id="bibliography_21" title=" LIAO S, HU Y, ZHU X, et al.Person re-identification by local maximal occurrence representation and metric learning[C]//IEEE Conference on Computer Vision and Pattern Recognition, 2015:3685-3693." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Person re-identification by Local Maximal Occurrence representation and metric learning.&amp;quot;">
                                        <b>[21]</b>
                                         LIAO S, HU Y, ZHU X, et al.Person re-identification by local maximal occurrence representation and metric learning[C]//IEEE Conference on Computer Vision and Pattern Recognition, 2015:3685-3693.
                                    </a>
                                </li>
                                <li id="45">


                                    <a id="bibliography_22" title=" VARIOR R R, SHUAI B, LU J, et al.A siamese long shortterm memory architecture for human re-identification[C]//Proceedings of European Conference on Computer Vision, 2016:135-153." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A siamese long short-term memory architecture for human re-identification">
                                        <b>[22]</b>
                                         VARIOR R R, SHUAI B, LU J, et al.A siamese long shortterm memory architecture for human re-identification[C]//Proceedings of European Conference on Computer Vision, 2016:135-153.
                                    </a>
                                </li>
                                <li id="47">


                                    <a id="bibliography_23" title=" ZHONG Z, ZHENG L, CAO D, et al.Re-ranking Person re-identification with k-reciprocal encoding[C]//IEEE Conference on Computer Vision and Pattern Recognition, 2017:3652-3661." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Re-ranking Person Re-identification with k-Reciprocal Encoding">
                                        <b>[23]</b>
                                         ZHONG Z, ZHENG L, CAO D, et al.Re-ranking Person re-identification with k-reciprocal encoding[C]//IEEE Conference on Computer Vision and Pattern Recognition, 2017:3652-3661.
                                    </a>
                                </li>
                                <li id="49">


                                    <a id="bibliography_24" title=" LIAO S, HU Y, ZHU X, et al.Person re-identification by local maximal occurrence representation and metric learning[C]//IEEE Conference on Computer Vision and Pattern Recognition, 2015:2197-2206." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Person re-identification by Local maximal occurrence representation and metric learning">
                                        <b>[24]</b>
                                         LIAO S, HU Y, ZHU X, et al.Person re-identification by local maximal occurrence representation and metric learning[C]//IEEE Conference on Computer Vision and Pattern Recognition, 2015:2197-2206.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=DZCL" target="_blank">电子测量技术</a>
                2019,42(15),155-160+166 DOI:10.19651/j.cnki.emt.1902713            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于孪生对抗SGAN的行人重识别研究</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%88%98%E4%BB%81%E6%98%A5&amp;code=42606881&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">刘仁春</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%AD%9F%E6%9C%9D%E6%99%96&amp;code=07179729&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">孟朝晖</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%B2%B3%E6%B5%B7%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%B8%8E%E4%BF%A1%E6%81%AF%E5%AD%A6%E9%99%A2&amp;code=1044544&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">河海大学计算机与信息学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>针对非重叠跨摄像头下的行人重识别任务易受行人姿态多变影响的问题, 提出一种基于孪生生成式对抗网络 (SGAN) 的行人重识别改进方法。首先, 在孪生网络结构中嵌入生成式对抗网络, 组成SGAN。其次, 在SGAN对抗学习中利用DeeperCut模型提取骨架关键点, 完成姿态迁移, 并擦除与原始行人姿态相关的特征。最后, 行人匹配过程脱离额外姿态特征辅助, 既降低了计算量又提升了识别精度。在大型数据集Market1501和CUHK03上验证该方法的有效性, 一次匹配成功率达到84.06%, 平均精度均值达到86.75%, 优于多数典型行人重识别方法。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%A1%8C%E4%BA%BA%E9%87%8D%E8%AF%86%E5%88%AB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">行人重识别;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=SGAN&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">SGAN;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%A7%BF%E6%80%81%E8%BF%81%E7%A7%BB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">姿态迁移;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%AA%A8%E6%9E%B6%E5%85%B3%E9%94%AE%E7%82%B9&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">骨架关键点;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%89%B9%E5%BE%81%E6%93%A6%E9%99%A4&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">特征擦除;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    刘仁春, 硕士研究生, 主要研究方向为计算机视觉、模式识别和图像处理等。E-mail:blackmam@foxmail.com;
                                </span>
                                <span>
                                    孟朝晖, 副教授, 主要研究方向为计算机视觉、模式识别和图像处理等。E-mail:mengzhaohui@hhu.edu.cn;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-02-21</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家重点研发计划 (2017YFC0405806) 项目资助;</span>
                    </p>
            </div>
                    <h1><b>Research of person re-identification based on SGAN</b></h1>
                    <h2>
                    <span>Liu Renchun</span>
                    <span>Meng Zhaohui</span>
            </h2>
                    <h2>
                    <span>College of Computer and Information, Hohai Univeristy</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Aiming at the problem that person re-identification (Re-ID) task under non-overlapping cross-camera is susceptible to posture changes, an improved method based on SGAN is proposed. Firstly, generative adversarial network is embedded in the structure of siamese network, and SGAN is proposed. Secondly, the key points of the skeleton are extracted by deepercut and the posture transfer is completed in the adversarial learning, and the features related to the persons' posture are erased. Finally, additional assistance posture feature is unnecessary for discriminating, which reduces the amount of calculation and improves the accuracy of recognition. The validity of the method is verified on the datasets Market1501 and CUHK03. The top-1 reaches 84.06%, and mAP reaches 86.75%, which are better than most typical methods of Re-ID.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=person%20re-identification&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">person re-identification;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=SGAN&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">SGAN;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=posture%20transfer&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">posture transfer;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=skeleton-map&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">skeleton-map;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=feature%20erasure&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">feature erasure;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2019-02-21</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="51" name="51" class="anchor-tag"><b>0 引  言</b></h3>
                <div class="p1">
                    <p id="52">行人重识别 (person re-identification, Re-ID) <citation id="126" type="reference"><link href="3" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>是利用计算机视觉技术判断图像或者视频序列中是否存在特定行人的图像检索问题。给定一个监控行人的图像或者视频序列, 检索跨设备条件下同一行人的图像。其在逃犯识别、遗失儿童搜寻等方面能够较好地弥补固定摄像头的视觉局限, 具有很好的应用前景, 是计算机视觉领域的一大研究热点。当前, 由于行人图像的分辨率普遍不高, 行人的姿态动作变化频繁, 不同摄像头的角度、光照、尺度等都不尽相同, 因而成为行人重识别问题的研究重点和难点。</p>
                </div>
                <div class="p1">
                    <p id="53">行人重识别问题的传统研究方法主要分为基于特征表示的方法<citation id="127" type="reference"><link href="5" rel="bibliography" /><link href="7" rel="bibliography" /><sup>[<a class="sup">2</a>,<a class="sup">3</a>]</sup></citation>和基于度量学习的方法<citation id="128" type="reference"><link href="9" rel="bibliography" /><link href="11" rel="bibliography" /><sup>[<a class="sup">4</a>,<a class="sup">5</a>]</sup></citation>两大类。基于特征表示的方法旨在通过提取行人关键特征, 建立一个能较好地描述行人特点的外观模型, 从而将不同的行人区分开;基于度量学习的方法则是构造新的度量空间, 按照一定的规则计算特征距离, 使得同一行人不同图像的特征距离小于不同行人不同图像的特征距离, 从而达到行人重识别的目的。</p>
                </div>
                <div class="p1">
                    <p id="54">近年来, 深度学习在图像处理、语音识别以及自然语言处理等多个领域得到了成功应用, 特别是深度卷积神经网络 (deep convolutional neural networks, DCNN) 在特征提取方面的卓越性能和生成式对抗网络 (generative adversarial networks, GAN) 在图像生成方面的优势<citation id="129" type="reference"><link href="13" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>, 使得行人重识别问题的研究取得了重大进展。但是, 行人姿态的多变、模糊以及遮挡仍然是提取不同行人差异性特征的一大障碍。目前深度学习领域主要有两类方法, 分别是行人图像对齐<citation id="130" type="reference"><link href="15" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>和局部特征学习<citation id="131" type="reference"><link href="17" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>。前者是指对行人图像背景占据比例过大、检测出的行人图像不完整等图像错位问题进行背景裁剪和行人图像补全;后者则是在图像切块 (水平或垂直方向) 或者已预训练的人体姿态提取模型和骨架关键点搭建模型的基础上, 分解整幅行人图像, 以达到学习行人局部特征的目的。尽管它们都使用了深度学习相关的算法, 但在行人判别的过程中均需要引入额外的姿态信息, 这不仅限制了算法在没有额外姿态信息辅助判别情况下的图像泛化能力, 还在不同程度上增加了整个网络的计算量和判别复杂度。</p>
                </div>
                <div class="p1">
                    <p id="55">为此, 本文提出一种基于孪生生成式对抗网络 (siamese-generative adversarial network, SGAN) 的行人重识别改进方法。该方法巧妙地结合了孪生网络权值并行共享和GAN数据生成的优势。在孪生网络结构的基础上, 嵌入GAN来完成行人的姿态迁移, 从而使网络在行人判别过程中对与姿态有关的特征不再敏感, 整个行人特征的判别过程不再需要额外的行人姿态信息。实验表明, 本文所提出的方法与其它典型的行人重识别方法相比, 不但减轻了过程计算量和判别复杂度, 而且提升了行人重识别的准确率。</p>
                </div>
                <h3 id="56" name="56" class="anchor-tag"><b>1 相关工作</b></h3>
                <h4 class="anchor-tag" id="57" name="57"><b>1.1 孪生网络</b></h4>
                <div class="p1">
                    <p id="58">孪生网络<citation id="132" type="reference"><link href="19" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>又称连体网络, 最早被用于美国银行支票上的签名验证。孪生网络一般存在两个分支, 通常用来度量两个输入样本 (文本、图像以及视频序列等) 的相似程度, 即判断输入的样本之间是否具有相同的属性特征 (标签) , 最后结合验证器 (Validtor) 输出结果。网络基本结构如图1所示:</p>
                </div>
                <div class="area_img" id="59">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/DZCL201915024_059.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 孪生网络结构示意图" src="Detail/GetImg?filename=images/DZCL201915024_059.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 孪生网络结构示意图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/DZCL201915024_059.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="60">图1中, 孪生网络的上下分支使用相同的目标网络, 权值共享, 对输入的样本要求一致。由于视频序列的时序信息在网络框架中并未提取和使用, 所以可将视频序列样本当作单帧图像进行处理。考虑到权值共享能大大减少参数数量和训练计算量, 本文使用孪生网络结构作为整个网络的基础框架是适合的。</p>
                </div>
                <div class="p1">
                    <p id="61">孪生网络通过将行人图像映射到高维特征空间, 并在高维特征空间中使用度量函数来表示不同行人图像中行人的特征相似度, 度量函数通常表示为:</p>
                </div>
                <div class="p1">
                    <p id="62"><i>E</i><sub><i>W</i></sub> (<i>X</i><sub>1</sub>, <i>X</i><sub>2</sub>) =‖<i>G</i><sub><i>W</i></sub> (<i>X</i><sub>1</sub>) -<i>G</i><sub><i>W</i></sub> (<i>X</i><sub>2</sub>) ‖      (1) </p>
                </div>
                <div class="p1">
                    <p id="63">式中:<i>x</i><sub>1</sub>、<i>x</i><sub>2</sub>为输入样本;<i>G</i><sub><i>w</i></sub>为空间映射函数, <i>E</i><sub><i>w</i></sub>表示度量结果, 下标<i>W</i>为权重。训练网络的目的就是为了确定合适的权重, 使得<i>x</i><sub>1</sub>和<i>x</i><sub>2</sub>属于同一个样本类别时<i>E</i><sub><i>w</i></sub>最小, 属于不同样本类别时<i>E</i><sub><i>w</i></sub>最大。在行人重识别中, 只需要最小化<i>E</i><sub><i>w</i></sub>即可。</p>
                </div>
                <h4 class="anchor-tag" id="64" name="64"><b>1.2 生成式对抗网络</b></h4>
                <div class="p1">
                    <p id="65">生成式对抗网络 (GAN) 最初由Goodfellow等人提出, 其一般由生成模型 (generative model) 和判别模型 (discriminative model) 两个模块构成。对于判别模型, 因输出目标相对简单, 故损失函数是容易定义的;但对于生成模型, 因期望结果是一个难以用数学公式定义的范式, 所以损失函数较难定义。若将生成模型的反馈部分交给判别模型处理, 通过两个模型的相互博弈学习, 却能得到相当好的输出结果。GAN网络的基本结构如图2所示:</p>
                </div>
                <div class="area_img" id="66">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/DZCL201915024_066.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 GAN网络结构示意图" src="Detail/GetImg?filename=images/DZCL201915024_066.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 GAN网络结构示意图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/DZCL201915024_066.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="67">在训练过程中, 生成器 (generator, G) 会学习真实样本数据的概率分布, 从而生成逼近真实样本的伪数据;判别器 (discriminator, D) 则用来识别输入的样本究竟来源于真实数据还是伪数据。在如此不断的“二元极大极小”博弈对抗过程中, 生成器和判别器均会提升生成能力和识别能力, 最终达到一个平衡状态, 即生成器生成的伪数据样本足够以假乱真, 以致判别器无法判断真假, 此时判别器的输出值稳定在概率值0.5左右。损失函数一般表示为:</p>
                </div>
                <div class="p1">
                    <p id="68" class="code-formula">
                        <mathml id="68"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mi>G</mi></munder><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mi>D</mi></munder><mi>V</mi><mo stretchy="false"> (</mo><mi>D</mi><mo>, </mo><mi>G</mi><mo stretchy="false">) </mo><mo>=</mo><mi>E</mi><msub><mrow></mrow><mrow><mi>x</mi><mo>∼</mo><mi>p</mi><msub><mrow></mrow><mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo></mrow></msub><mo stretchy="false">[</mo><mrow><mi>log</mi></mrow><mi>D</mi><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mo>+</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="69"><i>E</i><sub><i>z</i>～<i>p</i><sub><i>z</i></sub> (<i>z</i>) </sub>[log (<i>D</i> (<i>G</i> (<i>z</i>) ) ) ]      (2) </p>
                </div>
                <div class="p1">
                    <p id="70">式中:等式 (2) 的右侧两项分别表示判别器的熵 (Entropy, E) 和生成器的熵;<i>p</i><sub><i>data</i></sub>、<i>p</i><sub><i>z</i></sub>分别表示真实数据的分布和随机噪声的分布;生成器最小化<i>V</i>, 判别器最大化<i>V</i>, 两者联合调优。</p>
                </div>
                <div class="p1">
                    <p id="71">原始GAN并不要求G和D都是神经网络, 只要满足能拟合相应生成和判别函数即可。但在实际应用中, G、D一般均使用深度神经网络。对于行人重识别, G为生成网络, D为判别网络:</p>
                </div>
                <div class="p1">
                    <p id="72">1) G接受一个随机噪声z, 生成一个监控视频场景下的行人图像, 记为<i>G</i> (<i>z</i>) </p>
                </div>
                <div class="p1">
                    <p id="73">2) D负责判断1) 中行人图像是否来自真实样本数据, 并输出结果<i>D</i> (<i>x</i>) </p>
                </div>
                <div class="p1">
                    <p id="74">若<i>D</i> (<i>x</i>) ≐1, 则表示输入图像是真实图像;<i>D</i> (<i>x</i>) ≐0, 则表示输入图像是由生成器产生的。在理想状态下, 网络训练的最终调优结果是<i>D</i> (<i>x</i>) ≐0.5, 即生成的行人图像质量相当好, 与真样本几乎一致。如此, 生成模型即可用来独立生成行人数据样本。</p>
                </div>
                <h3 id="75" name="75" class="anchor-tag"><b>2 孪生生成式对抗网络</b></h3>
                <h4 class="anchor-tag" id="76" name="76"><b>2.1 整体框架</b></h4>
                <div class="p1">
                    <p id="77">本文的网络结构是在孪生 (Siamese) 网络和生成式对抗网络 (GAN) 的基础上改进组合而来, 姿态迁移渗透网络的全过程。具体来说, 网络结构的基础框架使用孪生网络的对称结构, 而GAN则嵌入其中, 对抗学习与行人姿态无关的特征, 擦除与行人姿态相关的特征, 从而完成行人姿态迁移。因为本文重点研究的是降低网络对行人姿态变化的敏感性, 而行人姿态特征以外的信息又较为复杂, 无法择取一二作为代表, 所以本文将姿态特征以外的特征信息统称为非姿态特征。如果网络学习到的行人特征只与非姿态特征相关而与姿态特征本身无关, 那么GAN就可以精确生成同一个行人的不同姿态的行人图像。</p>
                </div>
                <div class="p1">
                    <p id="78">如图3所示, 输入的三幅原始行人图像 (A1、B1、C1) 均为训练数据集中同一个行人在不同摄像头、不同角度条件下的图像。整个网络包括两个生成器 (Generator) 、两个姿态判别器 (Posture Discriminator) 、两个非姿态判别器 (none-posture Discriminator) 以及一个验证器 (Validtor) , 并分别定义了相应的姿态判别损失函数 (Loss P) 、非姿态判别损失函数 (Loss NP) 以及验证损失函数 (Loss V) 。最后, 姿态判别损失函数、非姿态判别损失函数以及验证损失函数三者联合优化。在验证集和测试集上, 通过步骤⑥使用已训练好的验证器对输入的行人图像进行判别。同时, 整个网络结构还利用了结构和参数十分优秀的预训练模型DeeperCut<citation id="133" type="reference"><link href="21" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>。整体网络结构图如下:</p>
                </div>
                <div class="area_img" id="79">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/DZCL201915024_079.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 SGAN网络结构示意图" src="Detail/GetImg?filename=images/DZCL201915024_079.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 SGAN网络结构示意图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/DZCL201915024_079.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="80" name="80"><b>2.2 骨架关键点</b></h4>
                <div class="p1">
                    <p id="81">行人姿态迁移的前置条件是要有相对明显的行人姿态特征被独立提取出来, 并作为GAN网络中姿态判别器的训练标准。采用将完整的行人图像直接并入主网络进行姿态特征提取的方法, 往往无法得到较为理想的结果<citation id="134" type="reference"><link href="23" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>。因此, 第①步的预处理阶段, 本文使用已预训练好的DeeperCut模型来提取行人图像的骨架关键点<citation id="135" type="reference"><link href="25" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>。</p>
                </div>
                <h4 class="anchor-tag" id="82" name="82"><b>2.3 图像生成器</b></h4>
                <div class="p1">
                    <p id="83">SGAN的图像生成器使用ResNet-50和多个自定义的级联Block。每个Block均包括卷积层 (Conv) 、批量正则化 (BN) 、下采样 (Dropout) 以及修正线性单元 (ReLU) 。行人图像为RGB图像, 输入的随机噪声 (noise) 初始设置为满足标准正态分布的256维向量。第②步的编码阶段, 图像生成器接收2 048维编码的行人图像、64维编码的姿态关键点骨架和256维编码的随机噪声作为输入, 并在第③步生成新的行人图像。如图3所示, 新图像的行人姿态特征仅与C1相似, 相似度越高表示姿态迁移越彻底, 擦除效果越好。</p>
                </div>
                <h4 class="anchor-tag" id="84" name="84"><b>2.4 姿态判别器</b></h4>
                <div class="p1">
                    <p id="85">SGAN的姿态判别器使用姿态判别损失函数对原始行人骨架关键点C2和生成器生成的新图像B进行损失最大化, 旨在从图像生成器生成的新图像中擦除行人图像A的姿态特征信息。第④步的姿态对抗阶段, 匹配骨架关键点C2和行人图像A的维度及通道数, 并通过softmax函数获得一组概率值映射。该组映射表示骨架关键点C2的特征与生成器生成的新图像B的匹配程度, 概率值越接近1, 则表示两幅图的姿态特征越相似, 行人图像A的姿态特征信息残留越少, 姿态迁移彻底, 擦除效果好。</p>
                </div>
                <div class="p1">
                    <p id="86">在对抗过程中, 图像生成器会不断生成与目标相似的图像B来获得更高的概率值, 以博弈姿态判别器。对抗结束后, 生成器生成的新图像B的姿态特征与原始行人骨架关键点C2的姿态特征高度一致。令:<i>B</i><sub><i>t</i></sub>表示<i>B</i>所对应的数据集中真实行人图像, <i>M</i>、<i>N</i>分别表示真实数据分布和生成器<i>G</i>的数据分布, 则该过程涉及的损失函数<i>Loss P</i>定义如下:</p>
                </div>
                <div class="p1">
                    <p id="87" class="code-formula">
                        <mathml id="87"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><msub><mrow></mrow><mi>Ρ</mi></msub><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mrow><mi>D</mi><msub><mrow></mrow><mi>Ρ</mi></msub></mrow></munder><mo stretchy="false"> (</mo><mi>E</mi><msub><mrow></mrow><mrow><mi>B</mi><msub><mrow></mrow><mi>t</mi></msub><mo>∈</mo><mi>Μ</mi></mrow></msub><mo stretchy="false">[</mo><mrow><mi>log</mi></mrow><mi>D</mi><msub><mrow></mrow><mi>Ρ</mi></msub><mo stretchy="false"> (</mo><mo stretchy="false">[</mo><mi>C</mi><mn>2</mn><mo>, </mo><mi>B</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">]</mo><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mo>+</mo><mi>E</mi><msub><mrow></mrow><mrow><mi>B</mi><mo>∈</mo><mi>Ν</mi></mrow></msub><mo stretchy="false">[</mo><mrow><mi>log</mi></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>D</mi><msub><mrow></mrow><mi>Ρ</mi></msub><mo stretchy="false"> (</mo><mo stretchy="false">[</mo><mi>C</mi><mn>2</mn><mo>, </mo><mi>B</mi><mo stretchy="false">]</mo><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <h4 class="anchor-tag" id="88" name="88"><b>2.5 非姿态判别器</b></h4>
                <div class="p1">
                    <p id="89">SGAN的非姿态判别器主要用来判断在网络的同一侧分支中, 原始输入行人图像A和生成器生成的新图像B是否为同一个行人。该阶段属于第④步的非姿态对抗过程, 使用非姿态判别函数对原始行人图像A和生成器生成的新图像B进行损失最大化。姿态判别器的对抗过程与非姿态判别器的对抗过程是同时进行的, 即前者消除生成的新图像B中含有的行人图像A的姿态特征, 后者保证生成的新图像B中含有的行人图像A的非姿态特征不丢失。与前者相似, 后者通过softmax函数获得的一组概率映射也表示行人图像A与新图像B的相似度, 概率值越大, 两幅图像的非姿态特征越相似。对抗结束后, 行人图像A与生成的新图像B的非姿态特征高度一致。该过程涉及的损失函数<i>Loss NP</i>定义如下:</p>
                </div>
                <div class="p1">
                    <p id="90" class="code-formula">
                        <mathml id="90"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><msub><mrow></mrow><mrow><mi>Ν</mi><mi>Ρ</mi></mrow></msub><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mrow><mi>D</mi><msub><mrow></mrow><mrow><mi>Ν</mi><mi>Ρ</mi></mrow></msub></mrow></munder><mo stretchy="false"> (</mo><mi>E</mi><msub><mrow></mrow><mrow><mi>B</mi><msub><mrow></mrow><mi>t</mi></msub><mo>∈</mo><mi>Μ</mi></mrow></msub><mo stretchy="false">[</mo><mrow><mi>log</mi></mrow><mi>D</mi><msub><mrow></mrow><mi>Ν</mi></msub><mo stretchy="false"> (</mo><mo stretchy="false">[</mo><mi>A</mi><mo>, </mo><mi>B</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">]</mo><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mo>+</mo><mi>E</mi><msub><mrow></mrow><mrow><mi>B</mi><mo>∈</mo><mi>Ν</mi></mrow></msub><mo stretchy="false">[</mo><mrow><mi>log</mi></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>D</mi><msub><mrow></mrow><mrow><mi>Ν</mi><mi>Ρ</mi></mrow></msub><mo stretchy="false"> (</mo><mo stretchy="false">[</mo><mi>A</mi><mo>, </mo><mi>B</mi><mo stretchy="false">]</mo><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <h4 class="anchor-tag" id="91" name="91"><b>2.6 验证器</b></h4>
                <div class="p1">
                    <p id="92">网络结构中的验证器参与整个SGAN的生成和对抗过程, 获取姿态迁移和擦除后的结构参数, 负责对两幅输入的原始行人图像A1和A2的编码特征进行相似性比对, 相识度高的为同一个行人, 反之则不是。</p>
                </div>
                <div class="p1">
                    <p id="93">步骤⑥是原始行人图像的编码过程, 依旧使用ResNet-50对其进行特征提取。提取的特征图即为编码结果, 并作为验证器的输入。这里用<i>F</i> (<i>A</i><sub>1</sub>, <i>A</i><sub>2</sub>) 表示验证器的相似置信度, <i>α</i>表示标准标签 (ground truth) , 那么验证交叉熵损失函数<i>Loss V</i>表示如下:</p>
                </div>
                <div class="p1">
                    <p id="94"><i>L</i>=<i>β</i><sub>1</sub><i>L</i><sub><i>P</i></sub>+<i>β</i><sub>2</sub><i>L</i><sub><i>NP</i></sub>+<i>L</i><sub><i>V</i></sub>      (5) </p>
                </div>
                <div class="p1">
                    <p id="95">式中:<i>β</i><sub>1</sub>、<i>β</i><sub>2</sub>为权重系数。</p>
                </div>
                <h3 id="96" name="96" class="anchor-tag"><b>3 实 验</b></h3>
                <h4 class="anchor-tag" id="97" name="97"><b>3.1 数据集</b></h4>
                <div class="p1">
                    <p id="98">Market-1501数据集<citation id="136" type="reference"><link href="27" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>于2015年被构建并公开, 其采集地点为清华大学, 采集时间为夏季。如表1, 该数据集由6个摄像头 (5个高清摄像头, 1个低分辨率摄像头) 拍摄剪辑而成, 共包含1 501个行人、32 668个检测行人矩形框 (Bounding-box) 。每个行人至少被2个摄像头拍摄到, 且在同一个摄像头中可能存在多张图像。训练集包括751个行人, 12 936张行人图像, 平均每个人有17.2张训练数据;测试集包括750个行人, 19 732张行人图像, 平均每个人有26.3张测试数据。3 368张查询行人图像的行人检测矩形框是由人工手动标注的, 而gallery组中的行人检测矩形框是利用可变形组件模型DPM (deformable part model) 检测器检测得到。</p>
                </div>
                <div class="p1">
                    <p id="99">CUHK03数据集<citation id="137" type="reference"><link href="29" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>采集于香港中文大学内, 由2个摄像头拍摄剪辑而成。其包括1 467个行人, 14 097张行人图像, 平均每个行人在单个相机中有4.8张图像。整个数据集划分为1 367个行人训练集和100个行人测试集, 且同时具有人工标注的行人检测框和DPM检测器检测的行人检测框行人。</p>
                </div>
                <div class="area_img" id="100">
                    <p class="img_tit"><b>表1 数据集信息</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="100" border="1"><tr><td>数据集</td><td>行人总数</td><td>训练集</td><td>测试集</td><td>B-box</td><td>摄像头</td></tr><tr><td><br />Market1501</td><td>1 501</td><td>751</td><td>750</td><td>32 668</td><td>5+1</td></tr><tr><td><br />CUHK03</td><td>1 467</td><td>1 367</td><td>100</td><td>14 097</td><td>2</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="101">本文为了实验结果更加真实和客观, 选择使用DPM检测框的行人数据集。</p>
                </div>
                <h4 class="anchor-tag" id="102" name="102"><b>3.2 评价指标和实验步骤</b></h4>
                <div class="p1">
                    <p id="103">实验使用两种评价指标来检验行人重识别算法在数据集Market1501和CUHK03上的相关性能, 它们分别是累计匹配特征 (cumulative match characteristics, CMC) 和平均精度均值 (mean average precision, mAP) 。前者将行人匹配结果按照相关程度的高低进行排序, 并格外重视排在第一位 (top-1) , 即相关度最高的结果为正确结果的概率。当数据量较大或者规则需要时, 可额外参考后续排名的正确率;后者是把行人重识别的结果验证过程当作目标检测任务来处理, 用平均精度的均值 (mean average precision, mAP) 来度量实验结果。AP和mAP的计算公式分别为:</p>
                </div>
                <div class="p1">
                    <p id="104" class="code-formula">
                        <mathml id="104"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>A</mi><mi>Ρ</mi><mo>=</mo><mfrac><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mo stretchy="false"> (</mo></mstyle><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>k</mi><mo stretchy="false">) </mo><mo>⋅</mo><mi>R</mi><mo stretchy="false"> (</mo><mi>k</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo></mrow><mrow><mi>c</mi><mi>o</mi><mi>u</mi><mi>n</mi><mi>t</mi></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>6</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="105">式中:<i>k</i>是行人检索图像的排列序号;<i>P</i> (<i>k</i>) 表示被检索的行人图像中相关的行人图像所占比例大小;<i>R</i> (<i>k</i>) 用来判断被检索的图像是否相关 (相关置为1, 反之置为0) ;<i>count</i>代表相关行人图像的数量。</p>
                </div>
                <div class="p1">
                    <p id="106" class="code-formula">
                        <mathml id="106"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>m</mi><mi>A</mi><mi>Ρ</mi><mo>=</mo><mfrac><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>s</mi><mo>=</mo><mn>1</mn></mrow><mi>S</mi></munderover><mi>A</mi></mstyle><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>s</mi><mo stretchy="false">) </mo></mrow><mi>S</mi></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>7</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="107">式中:<i>S</i>表示行人图像被检索的次数。</p>
                </div>
                <div class="p1">
                    <p id="108">实验步骤分为SGAN-孪生部件的预训练、SGAN-生成式对抗部件的预训练以及整个SGAN微调训练三个阶段。孪生网络作为基础网络结构, 预训练包括步骤⑥的行人图像编码和验证器校验, 同时步骤②与⑥参数共享, 可合并训练。训练采用随机梯度下降法 (stochastic gradient descent, SGD) , 初始动量参数设为0.9, 初始学习速率设为0.01。生成器G采用Adam优化器, 判别器D采用SGD优化器联合训练, 初始学习率分别设置为0.001和0.000 1, 初始<i>β</i><sub>1</sub>和<i>β</i><sub>2</sub>均设置为0.1。最后, 对整个网络进行端到端的微调, 相应的优化器保持不变, 但是初始学习参数重新设置为0.000 01和0.000 1, 初始权重系数不变。</p>
                </div>
                <h4 class="anchor-tag" id="109" name="109"><b>3.3 权重系数分析</b></h4>
                <div class="p1">
                    <p id="110">实验过程中, 由于总损失函数<i>L</i>的权重系数的取值对实验结果起着至关重要的作用, 故在两个数据集上分别分析总损失函数<i>L</i>中的权重系数<i>β</i><sub>1</sub>和<i>β</i><sub>2</sub>对评价指标top-1和mAP的影响, 以探究当top-1和mAP达到最优状态时, <i>β</i><sub>1</sub>和<i>β</i><sub>2</sub>的取值。实验同时采用单张行人图像查询 (single query) 和多张行人图像联合查询 (multiple query) 两种查询方式。</p>
                </div>
                <div class="p1">
                    <p id="111">对于Market1501数据集, 如图4、图5所示, 固定权重系数<i>β</i><sub>2</sub>的取值不变, 在0.0～1.0的范围内考察权重系数<i>β</i><sub>1</sub>对top-1和mAP的影响。可以看出, 当<i>β</i><sub>1</sub>=0.3时, top-1和mAP达到最优状态。</p>
                </div>
                <div class="area_img" id="112">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/DZCL201915024_112.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 Market1501-β1对top-1的影响" src="Detail/GetImg?filename=images/DZCL201915024_112.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 Market1501-<i>β</i><sub>1</sub>对top-1的影响  <a class="btn-zoomin" href="Detail/GetImg?filename=images/DZCL201915024_112.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="113">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/DZCL201915024_113.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 Market1501-β1对mAP的影响" src="Detail/GetImg?filename=images/DZCL201915024_113.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 Market1501-<i>β</i><sub>1</sub>对mAP的影响  <a class="btn-zoomin" href="Detail/GetImg?filename=images/DZCL201915024_113.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="114">同理, 图6、图7揭示了在权重系数<i>β</i><sub>1</sub>保持不变的情况下, 权重系数<i>β</i><sub>2</sub>对实验top-1和mAP的影响, 即<i>β</i><sub>2</sub>=0.7时, top-1和mAP达到最优状态。对于数据集CUHK03, 通过实验发现, 当<i>β</i><sub>1</sub>=0.4, <i>β</i><sub>2</sub>=0.6时, top-1和mAP达到最优状态。</p>
                </div>
                <div class="area_img" id="115">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/DZCL201915024_115.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 Market1501-β2对top-1的影响" src="Detail/GetImg?filename=images/DZCL201915024_115.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 Market1501-<i>β</i><sub>2</sub>对top-1的影响  <a class="btn-zoomin" href="Detail/GetImg?filename=images/DZCL201915024_115.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="116">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/DZCL201915024_116.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 Market1501-β2对mAP的影响" src="Detail/GetImg?filename=images/DZCL201915024_116.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 Market1501-<i>β</i><sub>2</sub>对mAP的影响  <a class="btn-zoomin" href="Detail/GetImg?filename=images/DZCL201915024_116.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="117" name="117"><b>3.4 实验结果与分析</b></h4>
                <div class="p1">
                    <p id="118">整个实验过程在相同的数据集下, 采用Single Query和Multiple Query两种查询方式相结合, 对比了多种典型行人重识别方法, 使用top-1和mAP作为评价指标来评估所有方法的效果。如表2 所示, 在数据集Market1501上, 本文将DADM<citation id="138" type="reference"><link href="31" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>、DNS<citation id="139" type="reference"><link href="33" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>、Gate Re-ID<citation id="140" type="reference"><link href="35" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>、MSCAN<citation id="141" type="reference"><link href="37" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>、DCA<citation id="142" type="reference"><link href="37" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>、SVDNet<citation id="143" type="reference"><link href="39" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>等6种典型行人重识别方法与本文提出的方法进行对比。</p>
                </div>
                <div class="area_img" id="119">
                    <p class="img_tit"><b>表2 多种方法在数据集Market1501上的实验效果对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="119" border="1"><tr><td rowspan="2"><br />序号</td><td rowspan="2">算法</td><td colspan="2"><br />单个/%</td><td colspan="2">联合/%</td></tr><tr><td><br />top-1</td><td>mAP</td><td>top-1</td><td>mAP</td></tr><tr><td><br />01</td><td>DADM</td><td>39.40</td><td>19.60</td><td>49.00</td><td>25.80</td></tr><tr><td><br />02</td><td>DNS</td><td>55.43</td><td>29.87</td><td>71.56</td><td>46.03</td></tr><tr><td><br />03</td><td>Gate Re-ID</td><td>65.88</td><td>39.55</td><td>76.04</td><td>48.45</td></tr><tr><td><br />04</td><td>MSCAN</td><td>—</td><td>—</td><td>76.30</td><td>53.10</td></tr><tr><td><br />05</td><td>DCA</td><td>68.60</td><td>44.35</td><td>80.30</td><td>57.50</td></tr><tr><td><br />06</td><td>SVDNet</td><td>69.41</td><td>55.65</td><td>82.30</td><td>62.10</td></tr><tr><td><br />07</td><td>SGAN</td><td>79.60</td><td>58.55</td><td>84.06</td><td>64.33</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="120">实验数据表明, 在数据集Market1501上, 本文方法的top-1最高达到84.06%, mAP最高达到64.33%, 相比于其它方法都有较为明显的提升。</p>
                </div>
                <div class="p1">
                    <p id="121">对于数据集CUHK03, 本文使用DeepReID<citation id="144" type="reference"><link href="41" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>、LOMO+XQDA<citation id="145" type="reference"><link href="43" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>、S-LSTM<citation id="146" type="reference"><link href="45" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>、K-Reciprocal<citation id="147" type="reference"><link href="47" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>、IDE+XQDA<citation id="148" type="reference"><link href="49" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>、SVDNet 等6种行人重识别方法与本文所提方法进行实验对比, 得到的实验数据汇总如表3所示。</p>
                </div>
                <div class="area_img" id="122">
                    <p class="img_tit"><b>表3 多种方法在数据集CUHK03上的实验效果对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="122" border="1"><tr><td rowspan="2"><br />序号</td><td rowspan="2">算法</td><td colspan="2"><br />单个/%</td><td colspan="2">联合/%</td></tr><tr><td><br />top-1</td><td>mAP</td><td>top-1</td><td>mAP</td></tr><tr><td><br />01</td><td>DeepReID</td><td>19.90</td><td>—</td><td>23.60</td><td>—</td></tr><tr><td><br />02</td><td>LOMO+XQDA</td><td>43.33</td><td>39.60</td><td>52.00</td><td>45.10</td></tr><tr><td><br />03</td><td>S-LSTM</td><td>48.50</td><td>40.10</td><td>57.30</td><td>46.30</td></tr><tr><td><br />04</td><td>K-Reciprocal</td><td>52.75</td><td>58.50</td><td>61.60</td><td>67.20</td></tr><tr><td><br />05</td><td>IDE+XQDA</td><td>58.90</td><td>64.90</td><td>64.40</td><td>60.33</td></tr><tr><td><br />06</td><td>SVDNet</td><td>73.33</td><td>75.60</td><td>81.80</td><td>84.80</td></tr><tr><td><br />07</td><td>SGAN</td><td>82.80</td><td>79.33</td><td>83.50</td><td>86.75</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="123">实验数据表明, 在数据集CUHK03上, 本文方法的top-1最高达到83.50%, mAP最高达到86.75%, 明显高于S-LSTM、IDE+XQDA等方法。所以说, 对于行人重识别的姿态多变问题, SGAN网络是有效的且效果优于多数典型行人重识别方法。</p>
                </div>
                <h3 id="124" name="124" class="anchor-tag"><b>4 结  论</b></h3>
                <div class="p1">
                    <p id="125">虽然深度卷积神经网络的出现给行人重识别研究领域带来了很多新思路和新方法, 但是目前行人重识别任务中的行人姿态多变、模糊及遮挡问题仍然是其研究的一大难点。单纯地使用孪生网络和传统方法来解决该问题, 效果并不理想。考虑到GAN在图像生成方面的独特优势, 本文巧妙地将GAN嵌入孪生网络结构之中, 通过对抗训练使整个网络对行人的姿态变化不再敏感, 从而较好地解决了行人重识别易受行人姿态多变影响的问题。实验在大型行人重识别数据集Market1501和CUHK03上进行, 结果表明本文方法的效果超越多数典型行人重识别方法。如何进一步提高真实场景下行人重识别的准确率和实时性将会是下一步研究的重点。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZNXT201706002&amp;v=MjA4OTBiTXFZOUZab1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnRGeXZnVkxyQlB5UFRlckc0SDk=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> 宋婉茹, 赵晴晴, 陈昌红, 等.行人重识别研究综述[J].智能系统学报, 2017, 12 (6) :770-780.
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Transferring a semantic representation for person re-identification and search">

                                <b>[2]</b> SHI Z, HOSPEDALES T M, Tao X.Transferring a semantic representation for person re-identification and search[C]//Computer Vision and Pattern Recognition, 2015:4184-4193.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Local Descriptors Encoded by Fisher Vectors for Person Re-identification">

                                <b>[3]</b> MA B, YU S, JURIE F.Local Descriptors Encoded by Fisher Vectors for Person Re-identification[C]//International Conference on Computer Vision, 2012:413-422.
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" >
                                    <b>[4]</b>
                                 LIAO S, HU Y, ZHU N X.Person re-identification by Local Maximal Occurrence representation and metric learning[C]//IEEE Conference on Computer Vision and Pattern Recognition, 2015:2197-2206.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES15122300162578&amp;v=MjQzMDlIL2lyUmRHZXJxUVRNbndaZVp0RmlubFVyeklJMXNUYnhzPU5pZk9mYks5SDlQT3JJOUZaZTBOQ1hzeG9CTVQ2VDRQUQ==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b> DING S, LIANG L, WANG G, et al.Deep feature learning with relative distance comparison for person re-identification[J].Pattern Recognition, 2015, 48 (10) :2993-3003.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" >
                                    <b>[6]</b>
                                 罗浩, 姜伟, 范星, 张思朋.基于深度学习的行人重识别研究进展[J/OL], 自动化学报:1-18.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Pedestrian alignment network for large-scale Person re-identification">

                                <b>[7]</b> ZHENG Z, ZHENG L, YANG Y.Pedestrian alignment network for large-scale Person re-identification[J].IEEE Transactions on Circuits and Systems for Video Technology, 2017:498-512.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Spindle net:Person re-identification with human body region guided feature decomposition and fusion">

                                <b>[8]</b> ZHAO H, TIAN M, SUN S, et al.Spindle net:Person re-identification with human body region guided feature decomposition and fusion[C]//IEEE Conference on Computer Vision and Pattern Recognition, 2017:1077-1085.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fully-Convolutional Siamese Networks for Object Tracking">

                                <b>[9]</b> BERTINETTO L, VALMADRE J, HENRIQUES J F, et al.Fully-convolutional siamese networks for object tracking[C]//European Conference on Computer Vision, 2016:850-865.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deepercut:A deeper,stronger,and faster multiperson pose estimation model">

                                <b>[10]</b> INSAFUTDINOV E, PISHCHULIN L, ANDRES B, et al.DeeperCut:A deeper, stronger, and faster multi-person pose estimation model[C]//European Conference on Computer Vision, 2016:34-50.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep learning earth observation classification using image Net pretrained networks">

                                <b>[11]</b> MARMANIS D, DATCU M, ESCH T, et al.Deep learning earth observation classification using imagenet pretrained networks[J].IEEE Geoscience and Remote Sensing Letters, 2016, 13 (1) :105-109.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_12" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJDF91ABE75F707F3C5549995295CA360E5&amp;v=MDU3NDFSZEdDTEdTUmMrYUNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkJyTFUwNXR0aHg3MjR4YUU9Tmo3QmFzV3hINkMrMm9oQUV1d1BDd282dkJNVzdqWjBRWHJncA==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[12]</b> ALIOUA N, AMINE A, ROGOZAN A, et al.Driver head pose estimation using efficient descriptor fusion[J].EURASIP Journal on Image and Video Processing, 2016 (1) :1-14.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Scalable person re-identification A benchmark">

                                <b>[13]</b> ZHENG L, SHEN L, TIAN L, et al.Scalable person re-identification:A benchmark[C]//IEEE International Conference on Computer Vision, 2015:1116-1124.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_14" >
                                    <b>[14]</b>
                                 ZHONG Z, ZHENG L, CAO D, et al.Re-ranking Person re-identification with k-reciprocal encoding[C]//IEEE Conference on Computer Vision and Pattern Recognition, 2017:3652-3661.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep attributes driven multi-camera person">

                                <b>[15]</b> SU C, ZHANG S, XING J, et al.Deep attributes driven multi-camera person re-identification[C]//European Conference on Computer Vision, 2016:475-491.
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning a Discriminative Null Space for Person Re-identification">

                                <b>[16]</b> LI Z, TAO X, GONG S.Learning a discriminative null space for person re-identification[C]//Computer Vision and Pattern Recognition, 2016:1239-1248.
                            </a>
                        </p>
                        <p id="35">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Gated siamese convolutional neural network architecture for human re-identification">

                                <b>[17]</b> VARIOR R R, HALOI M, GANG W.Gated siamese convolutional neural network architecture for human re-identification[C]//European Conference on Computer Vision, 2016:791-808.
                            </a>
                        </p>
                        <p id="37">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning deep context-aware features over body and latent parts for person re-identification">

                                <b>[18]</b> LI D, CHEN X, ZHANG Z, et al.Learning deep context-aware features over body and latent parts for person reidentification[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017:384-393.
                            </a>
                        </p>
                        <p id="39">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=SVDNet for Pedestrian Retrieval">

                                <b>[19]</b> SUN Y, ZHENG L, DENG W, et al.SVDNet for pedestrian retrieval[C]//IEEE International Conference on Computer Vision, 2017:3820-3828.
                            </a>
                        </p>
                        <p id="41">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=DeepReID:Deep Filter Pairing neural network for person re-identification">

                                <b>[20]</b> WEI L, RUI Z, TONG X, et al.DeepReID:Deep Filter Pairing neural network for person re-identification[C]//Computer Vision and Pattern Recognition, 2014:152-159.
                            </a>
                        </p>
                        <p id="43">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Person re-identification by Local Maximal Occurrence representation and metric learning.&amp;quot;">

                                <b>[21]</b> LIAO S, HU Y, ZHU X, et al.Person re-identification by local maximal occurrence representation and metric learning[C]//IEEE Conference on Computer Vision and Pattern Recognition, 2015:3685-3693.
                            </a>
                        </p>
                        <p id="45">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A siamese long short-term memory architecture for human re-identification">

                                <b>[22]</b> VARIOR R R, SHUAI B, LU J, et al.A siamese long shortterm memory architecture for human re-identification[C]//Proceedings of European Conference on Computer Vision, 2016:135-153.
                            </a>
                        </p>
                        <p id="47">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Re-ranking Person Re-identification with k-Reciprocal Encoding">

                                <b>[23]</b> ZHONG Z, ZHENG L, CAO D, et al.Re-ranking Person re-identification with k-reciprocal encoding[C]//IEEE Conference on Computer Vision and Pattern Recognition, 2017:3652-3661.
                            </a>
                        </p>
                        <p id="49">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Person re-identification by Local maximal occurrence representation and metric learning">

                                <b>[24]</b> LIAO S, HU Y, ZHU X, et al.Person re-identification by local maximal occurrence representation and metric learning[C]//IEEE Conference on Computer Vision and Pattern Recognition, 2015:2197-2206.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="DZCL201915024" />
        <input id="dpi" type="hidden" value="600" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZCL201915024&amp;v=MjY5MzBadEZ5dmdWTHJCSVRmSVlyRzRIOWpOcW85SFlJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnU=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>


    <link href="/kxreader/Content/css/LeftDetail?v=NLcKG8I1SJUaVFrQ0iGpF2klAT0OsmHRaVSZ1rKb5xg1" rel="stylesheet"/>

</body>
</html>

