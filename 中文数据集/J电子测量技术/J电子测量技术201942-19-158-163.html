

<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>

</head>

<body>

    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637135576127443750%26DBCODE%3dCJFD%26TABLEName%3dCJFDTEMP%26FileName%3dDZCL201919027%26RESULT%3d1%26SIGN%3d1nJe2MgtEQHekWmLAjLPsJ5Lngk%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=DZCL201919027&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=DZCL201919027&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>


    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZCL201919027&amp;v=MjMyODFUZklZckc0SDlqTnBvOUhZNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnRGeW5tVkx2TUk=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#33" data-title="&lt;b&gt;0 引  言&lt;/b&gt; "><b>0 引  言</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#39" data-title="&lt;b&gt;1 LRSDR-Net深度网络框架&lt;/b&gt; "><b>1 LRSDR-Net深度网络框架</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#42" data-title="&lt;b&gt;1.1 基于多尺度递归网络的图像去糊&lt;/b&gt;"><b>1.1 基于多尺度递归网络的图像去糊</b></a></li>
                                                <li><a href="#51" data-title="&lt;b&gt;1.2 基于自编码网络的视差图估计&lt;/b&gt;"><b>1.2 基于自编码网络的视差图估计</b></a></li>
                                                <li><a href="#55" data-title="&lt;b&gt;1.3 基于对极几何模型的深度图生成&lt;/b&gt;"><b>1.3 基于对极几何模型的深度图生成</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#64" data-title="&lt;b&gt;2 本文算法设计&lt;/b&gt; "><b>2 本文算法设计</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#95" data-title="&lt;b&gt;3 实验结果与分析&lt;/b&gt; "><b>3 实验结果与分析</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#99" data-title="&lt;b&gt;3.1 定性实验对比分析&lt;/b&gt;"><b>3.1 定性实验对比分析</b></a></li>
                                                <li><a href="#106" data-title="&lt;b&gt;3.2 客观定量对比分析&lt;/b&gt;"><b>3.2 客观定量对比分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#110" data-title="&lt;b&gt;4 结  论&lt;/b&gt; "><b>4 结  论</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#41" data-title="图1 基于LRSDR-Net的实时单目深度估计框架">图1 基于LRSDR-Net的实时单目深度估计框架</a></li>
                                                <li><a href="#54" data-title="图2 视差网络生成的左右视图视差图与
融合后视差图结果展示">图2 视差网络生成的左右视图视差图与
融合后视差图结果展示</a></li>
                                                <li><a href="#61" data-title="图3 基于平行光轴的双目对极几何模型原理">图3 基于平行光轴的双目对极几何模型原理</a></li>
                                                <li><a href="#63" data-title="图4 LRSDR-Net深度网络各阶段估计结果展示">图4 LRSDR-Net深度网络各阶段估计结果展示</a></li>
                                                <li><a href="#70" data-title="图5 基于左右视图一致性的多尺度&lt;i&gt;LRSDR&lt;/i&gt;-&lt;i&gt;Net&lt;/i&gt;
深度训练网络结构">图5 基于左右视图一致性的多尺度<i>LRSDR</i>-<i>Net</i>
深度训练网络结构</a></li>
                                                <li><a href="#101" data-title="图6 文献与本文LRSDR-Net在GOPRO
数据集上的实验结果对比">图6 文献与本文LRSDR-Net在GOPRO
数据集上的实验结果对比</a></li>
                                                <li><a href="#104" data-title="图7 基于LRSDR-Net生成的视差图与真值视差图的
实验结果对比">图7 基于LRSDR-Net生成的视差图与真值视差图的
实验结果对比</a></li>
                                                <li><a href="#108" data-title="&lt;b&gt;表1 基于KITTI 2015不同方法的视差图估计性能对比&lt;/b&gt;"><b>表1 基于KITTI 2015不同方法的视差图估计性能对比</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" title=" 何通能,尤加庚,陈德富.基于DenseNet的单目图像深度估计[J].计算机测量与控制,2019,27(2):233-236." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JZCK201902051&amp;v=MTY4MjdUNmo1NE8zenFxQnRHRnJDVVI3cWZadVp0RnlubVZMdlBMemZJWmJHNEg5ak1yWTlBWllRS0RIODR2UjQ=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         何通能,尤加庚,陈德富.基于DenseNet的单目图像深度估计[J].计算机测量与控制,2019,27(2):233-236.
                                    </a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" title=" 李耀宇,王宏民,张一帆,等.基于结构化深度学习的单目图像深度估计[J].机器人,2017,39(6):812-819." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JQRR201706006&amp;v=MDk5NjF6elpmTEc0SDliTXFZOUZZb1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnRGeW5tVkx2UEw=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         李耀宇,王宏民,张一帆,等.基于结构化深度学习的单目图像深度估计[J].机器人,2017,39(6):812-819.
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title=" 冯春,吴小锋,尹飞鸿,等.基于局部特征匹配的双焦单目立体视觉深度估计[J].计算机技术与发展,2016,26(10):55-59." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=WJFZ201610012&amp;v=MDA0OTFOcjQ5RVpvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVadEZ5bm1WTHZQTWlmTmRMRzRIOWY=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         冯春,吴小锋,尹飞鸿,等.基于局部特征匹配的双焦单目立体视觉深度估计[J].计算机技术与发展,2016,26(10):55-59.
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" title=" LIU F,SHEN C,LIN G,et al.Learning depth from single monocular images using deep convolutional neural fields[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2016,38(10):2024-2039." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning depth from single monocular images using deep convolutional neural fields">
                                        <b>[4]</b>
                                         LIU F,SHEN C,LIN G,et al.Learning depth from single monocular images using deep convolutional neural fields[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2016,38(10):2024-2039.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" title=" EIGEN D,FERGUS R.Predicting depth,surface normals and semantic labels with a common multi-scale convolutional architecture[C].Proceedings of the IEEE International Conference on Computer Vision,2015:2650-2658." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Predicting depth,surface normals and semantic labels with a common multi-scale convolutional architecture">
                                        <b>[5]</b>
                                         EIGEN D,FERGUS R.Predicting depth,surface normals and semantic labels with a common multi-scale convolutional architecture[C].Proceedings of the IEEE International Conference on Computer Vision,2015:2650-2658.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" title=" EIGEN D,PUHRSCH C,FERGUS R.Depth map prediction from a single image using a multi-scale deep network[C].Advances in Neural Information Processing Systems,2014:2366-2374." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Depth map prediction from a single image using a multi-scale deep network">
                                        <b>[6]</b>
                                         EIGEN D,PUHRSCH C,FERGUS R.Depth map prediction from a single image using a multi-scale deep network[C].Advances in Neural Information Processing Systems,2014:2366-2374.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" title=" 应自炉,龙祥.多尺度密集残差网络的单幅图像超分辨率重建[J].中国图象图形学报,2019,24(3):410-419." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZGTB201903009&amp;v=MDI5OTVtVkx2UFB5cmZiTEc0SDlqTXJJOUZiWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnRGeW4=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                         应自炉,龙祥.多尺度密集残差网络的单幅图像超分辨率重建[J].中国图象图形学报,2019,24(3):410-419.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" title=" SHI X,CHEN Z,WANG H,et al.Convolutional LSTM network:A machine learning approach for precipitation nowcasting[C].Annual conference on Neural Information Processing Systems,2015:802-810." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Convolutional LSTM network A machine learning approach for precipitation nowcasting">
                                        <b>[8]</b>
                                         SHI X,CHEN Z,WANG H,et al.Convolutional LSTM network:A machine learning approach for precipitation nowcasting[C].Annual conference on Neural Information Processing Systems,2015:802-810.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" title=" 范群贞.运动模糊图像复原方法的研究[J].电子测量技术,2013,36(6):73-76." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZCL201306019&amp;v=MTE5NDJmSVlyRzRIOUxNcVk5RWJZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVadEZ5bm1WTHZQSVQ=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                         范群贞.运动模糊图像复原方法的研究[J].电子测量技术,2013,36(6):73-76.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_10" title=" 沈彤,刘文波,王京.基于双目立体视觉的目标测距系统[J].电子测量技术,2015,38(4):52-54." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZCL201504013&amp;v=MDQ4MjdUNmo1NE8zenFxQnRHRnJDVVI3cWZadVp0RnlubVZMdlBJVGZJWXJHNEg5VE1xNDlFWjRRS0RIODR2UjQ=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[10]</b>
                                         沈彤,刘文波,王京.基于双目立体视觉的目标测距系统[J].电子测量技术,2015,38(4):52-54.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_11" title=" HEISE P,KLOSE S,JENSEN B,et al.PM-Huber:Patchmatch with huber regularization for stereo matching[C].Proceedings of the IEEE International Conference on Computer Vision,2013:2360-2367." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=PM-Huber Patchmatch with huber regularization for stereo matching">
                                        <b>[11]</b>
                                         HEISE P,KLOSE S,JENSEN B,et al.PM-Huber:Patchmatch with huber regularization for stereo matching[C].Proceedings of the IEEE International Conference on Computer Vision,2013:2360-2367.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_12" title=" GEIGER A,LENZ P,URTASUN R.Are we ready for autonomous driving?The KITTI vision benchmark suite[C].2012 IEEE Conference on Computer Vision and Pattern Recognition,IEEE,2012:3354-3361." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Are we ready for autonomous driving?The KITTI vision benchmark suite">
                                        <b>[12]</b>
                                         GEIGER A,LENZ P,URTASUN R.Are we ready for autonomous driving?The KITTI vision benchmark suite[C].2012 IEEE Conference on Computer Vision and Pattern Recognition,IEEE,2012:3354-3361.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_13" title=" NAH S,KIM T H,LEE K M.Deep multi-scale convolutional neural network for dynamic scene deblurring[C].Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,2017:3883-3891." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep multi-scale convolutional neural network for dynamic scene deblurring">
                                        <b>[13]</b>
                                         NAH S,KIM T H,LEE K M.Deep multi-scale convolutional neural network for dynamic scene deblurring[C].Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,2017:3883-3891.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_14" title=" GEIGER A,LENZ P,STILLER C,et al.Vision meets robotics:The KITTI dataset[J].The International Journal of Robotics Research,2013,32(11):1231-1237." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Vision meets robotics:the KITTI dataset">
                                        <b>[14]</b>
                                         GEIGER A,LENZ P,STILLER C,et al.Vision meets robotics:The KITTI dataset[J].The International Journal of Robotics Research,2013,32(11):1231-1237.
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_15" title=" GARG R,BG V K,CARNEIRO G,et al.Unsupervised CNN for single view depth estimation:Geometry to the rescue[C].European Conference on Computer Vision,Springer,Cham,2016:740-756." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Unsupervised CNN for single view depth estimation:Geometry to the rescue">
                                        <b>[15]</b>
                                         GARG R,BG V K,CARNEIRO G,et al.Unsupervised CNN for single view depth estimation:Geometry to the rescue[C].European Conference on Computer Vision,Springer,Cham,2016:740-756.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=DZCL" target="_blank">电子测量技术</a>
                2019,42(19),158-163 DOI:10.19651/j.cnki.emt.1902912            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于LRSDR-Net的实时单目深度估计</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BC%A0%E5%96%86%E9%9F%AC&amp;code=43369877&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">张喆韬</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E4%B8%87%E6%97%BA%E6%A0%B9&amp;code=08537469&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">万旺根</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%8A%E6%B5%B7%E5%A4%A7%E5%AD%A6%E9%80%9A%E4%BF%A1%E4%B8%8E%E4%BF%A1%E6%81%AF%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0017580&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">上海大学通信与信息工程学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%8A%E6%B5%B7%E5%A4%A7%E5%AD%A6%E6%99%BA%E6%85%A7%E5%9F%8E%E5%B8%82%E7%A0%94%E7%A9%B6%E9%99%A2&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">上海大学智慧城市研究院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>针对运动模糊图像的深度图估计以及减少网络训练成本,提出了基于左右视图一致性约束的多尺度去糊递归网络LRSDR-Net。首先,利用多尺度去糊子网络对图像进行去糊重建。在此基础上,利用自编码网络生成视差图像。最后,利用基于平行光轴的双目对极几何模型对视差图进行深度估计生成深度图像。通过与同类方法的主观定性与客观定量对比,实验结果表明,所提方法在视差估计的相对绝对误差、相对平方误差、均方误差与log均方误差等指标上取得了0.116,0.897,4.934,0.207的实验结果,优于其他现有方法的性能指标。实验证明所提方法在处理相机抖动的场景与一般场景的深度估计准确性优于其他现有方法,进一步表明所提方法有效、可行。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E4%BC%B0%E8%AE%A1&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度估计;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%A7%86%E5%B7%AE%E9%87%8D%E5%BB%BA&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">视差重建;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%87%AA%E7%BC%96%E7%A0%81%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">自编码网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">循环神经网络;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    张喆韬,工学硕士,主要研究方向为基于SLAM的场景重建,E-mail:zhangzhetao2016@outlook.com;;
                                </span>
                                <span>
                                    万旺根,教授,博士生导师,主要研究方向为计算机图形学、信号处理和数据挖掘,E-mail:wanwg@staff.shu.edu.cn;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-04-04</p>

                    <p>

                            <b>基金：</b>
                                                        <span>上海市科委港澳台科技合作项目(18510760300)资助;</span>
                    </p>
            </div>
                    <h1><b>Image depth estimation based on multi scale deblur recurrent network with left-right consistency</b></h1>
                    <h2>
                    <span>Zhang Zhetao</span>
                    <span>Wan Wanggen</span>
            </h2>
                    <h2>
                    <span>School of Communication and Information Engineering, Shanghai University</span>
                    <span>Institute of Smart City, Shanghai University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Aiming at the depth map estimation for motion blurred images and reducing the network training cost, this paper proposes a multi scale deblur recurrent network with left-right consistency. First, the multi scale deblur sub-network is used to reconstruct the image. On this basis, a disparity map is generated using an auto-encoder network. Finally, the depth map is generated by depth estimation of the disparity map using a binocular polar geometry model based on parallel optical axes. Through the subjective qualitative and objective quantitative comparison with the similar methods, the experimental results show that the proposed method achieves 0.116, 0.897, 4.934, 0.207 on the relative absolute error, relative square error, mean square error and log mean square error of the disparity estimation. The experimental results are superior to those of other existing methods. The experimental results show that the accuracy of the depth estimation of the camera blur scene and the general scene using our method is better than other existing methods, which further demonstrates that our method is effective and feasible.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=depth%20estimation&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">depth estimation;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=disparity%20reconstruction&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">disparity reconstruction;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=auto-encoder%20network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">auto-encoder network;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=recurrent%20network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">recurrent network;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2019-04-04</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="33" name="33" class="anchor-tag"><b>0 引  言</b></h3>
                <div class="p1">
                    <p id="34">从二维图像构建三维场景是机器视觉领域的核心研究方向。姿态估计、三维重建等研究都依赖于场景图像的深度信息。如何从一幅二维图像中准确的估计场景深度信息<citation id="141" type="reference"><link href="3" rel="bibliography" /><link href="5" rel="bibliography" /><link href="7" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>,<a class="sup">3</a>]</sup></citation>,是近几年学者关注的问题。</p>
                </div>
                <div class="p1">
                    <p id="35">早期的工作主要利用对多视角的场景图像进行三角测量得到像素点对应的深度信息。Pal等通过采集双目图像后进行三角测量得到图像深度。但是该类方法不仅需要多个视角的图像作为输入,而且容易受到基线长度及各个视角图像间像素匹配精度的影响,导致深度估计具有较大的误差。</p>
                </div>
                <div class="p1">
                    <p id="36">近年来,随着深度学习的普及与发展,利用神经网络模型得到图像深度逐渐成为主流研究方向。该类方法只需要一张图像即可生成对应的深度图,克服了传统方法需要多视角图像的问题。Liu等<citation id="142" type="reference"><link href="9" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>通过深度卷积神经场网络估计深度图,但是该方法无法准确估计带有运动模糊的图像深度。Eigen等<citation id="143" type="reference"><link href="11" rel="bibliography" /><link href="13" rel="bibliography" /><sup>[<a class="sup">5</a>,<a class="sup">6</a>]</sup></citation>使用场景图像与其对应的真值深度图训练双尺度深度网络,实现单目图像的深度估计,但是该方法依赖于高质量的真值深度数据进行训练,需要使用激光扫描仪或者深度摄像机采集深度数据,构建训练集的成本很高。</p>
                </div>
                <div class="p1">
                    <p id="37">综上所述,基于神经网络的单目图像深度估计算法存在的问题主要有以下两点:1) 针对实时估计采集图像深度的应用场景,如何修改估计策略以提高对带有运动模糊的采集图像深度估计的准确性。2) 现有模型需要的训练数据采集成本过高,如何构建新的网络结构,使得在训练过程中可以用低成本,易获取的数据来替代真值深度数据。</p>
                </div>
                <div class="p1">
                    <p id="38">针对上述问题,本文提出了基于左右视图一致性约束的多尺度去糊递归网络(multi scale deblur recurrent net with left-right consistency,LRSDR-Net),用于对采集的单目图像进行实时深度估计。该方法首先对输入的图像进行多尺度去糊重建,生成去除运动模糊后的清晰图像。在此基础上,本文把直接估计图像深度问题转变为估计图像视差问题,利用自编码网络得到清晰图像对应的视差图。最后,利用基于平行光轴的双目对极几何模型对视差图进行深度估计生成深度图像。网络使用基于左右视图一致性的多尺度LRSDR-Net深度训练网络进行端到端训练,实验结果表明,本文方法有效、可行。</p>
                </div>
                <h3 id="39" name="39" class="anchor-tag"><b>1 LRSDR-Net深度网络框架</b></h3>
                <div class="p1">
                    <p id="40">为了提高实时图像深度估计精度以及减少网络训练成本,本文提出了LRSDR-Net深度网络框架用于实时深度估计,通过多尺度递归网络从低精度到高精度逐层进行图像去糊。在此基础上,本文把直接估计图像深度问题转变为估计图像视差问题,利用卷积神经网络(CNN)搭建自编码网络生成对应的视差图像,训练过程中使用双目数据集替代深度图像数据集,降低整体网络训练成本。最后,利用基于平行光轴的双目对极几何模型得到深度图像,整体计算框架如图1所示。</p>
                </div>
                <div class="area_img" id="41">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/DZCL201919027_041.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 基于LRSDR-Net的实时单目深度估计框架" src="Detail/GetImg?filename=images/DZCL201919027_041.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 基于LRSDR-Net的实时单目深度估计框架  <a class="btn-zoomin" href="Detail/GetImg?filename=images/DZCL201919027_041.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="42" name="42"><b>1.1 基于多尺度递归网络的图像去糊</b></h4>
                <div class="p1">
                    <p id="43">LRSDR-Net深度网络主要分为3个阶段。首先,本文使用由粗到精的策略,通过多尺度递归网络<citation id="144" type="reference"><link href="15" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>结合LSTM<citation id="145" type="reference"><link href="17" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>对采集图像进行去糊重建。根据式(1)计算得到的尺度因子对输入图像进行下采样。</p>
                </div>
                <div class="p1">
                    <p id="44"><i>s</i><sub><i>i</i></sub>=<i>α</i><mathml id="112"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>s</mi><mrow><mi>n</mi><mo>-</mo><mi>i</mi></mrow></msubsup></mrow></math></mathml>.      (1)</p>
                </div>
                <div class="p1">
                    <p id="45">式中:<i>s</i><sub><i>i</i></sub>表示当前下采样的尺度大小;<i>α</i><sub><i>s</i></sub>表示下采样的尺度基数;<i>n</i>表示图像下采样的次数。本文中,<i>α</i><sub><i>s</i></sub>=0.5,<i>n</i>=4。</p>
                </div>
                <div class="p1">
                    <p id="46">在获取不同尺度采样图像的基础上,选取尺度因子最小的模糊图像进行清晰化重建<citation id="146" type="reference"><link href="19" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>。接着,把重建得到的当前尺度的清晰图像上采样到下一层尺度的大小,与下一层对应的模糊图像一起进行清晰化重建,逐层往下,每层的重建过程由式(2)所示:</p>
                </div>
                <div class="p1">
                    <p id="47"><i><b>P</b></i><sub><i>si</i></sub>=Net<sub><i>Deblur</i></sub>(<i><b>I</b></i><sub><i>si</i></sub>,(<i><b>P</b></i><sub><i>si</i></sub><sub>-1</sub>)<sup>↑</sup>,(<i><b>h</b></i><sub><i>si</i></sub><sub>-1</sub>)<sup>↑</sup>,(<i><b>c</b></i><sub><i>si</i></sub><sub>-1</sub>)<sup>↑</sup>).      (2)</p>
                </div>
                <div class="p1">
                    <p id="48">式中:<i><b>P</b></i><sub><i>si</i></sub>表示在尺度因子<i>s</i><sub><i>i</i></sub>下重建的清晰图像;<i><b>I</b></i><sub><i>si</i></sub>表示输入图像以尺度因子<i>s</i><sub><i>i</i></sub>下采样后得到的模糊图像;<i><b>P</b></i><sub><i>si</i></sub><sub>-1</sub>表示基于上一层尺度因子<i>s</i><sub><i>i</i></sub><sub>-1</sub>重建得到的清晰图像;<i><b>h</b></i><sub><i>si</i></sub><sub>-1</sub>、<i><b>c</b></i><sub><i>si</i></sub><sub>-1</sub>分别表示上一层LSTM记忆细胞的输出与状态向量;(·)<sup>↑</sup>表示上采样操作,用于把<i><b>P</b></i><sub><i>si</i></sub><sub>-1</sub>,<i>h</i><sub><i>si</i></sub><sub>-1</sub>,<i><b>c</b></i><sub><i>si</i></sub><sub>-1</sub>的尺度调整到与当前层的尺度因子一致。</p>
                </div>
                <div class="p1">
                    <p id="49">利用LSTM的特性,每层细胞的输出与状态向量都可以有选择性地传递到下一层,使重建过程可以参考更多的特征信息以获取更高质量的重建效果。当前层的输出根据当前层记忆细胞的输入,上一层记忆细胞的输出和状态进行更新。</p>
                </div>
                <div class="p1">
                    <p id="50">综上完成所有尺度图像的去糊重建后,最后一层输出的重建图像即为输入图像对应的清晰去糊图像。</p>
                </div>
                <h4 class="anchor-tag" id="51" name="51"><b>1.2 基于自编码网络的视差图估计</b></h4>
                <div class="p1">
                    <p id="52">对输入图像去除运动模糊后,本文使用自编码网络进行视差图估计。首先,对输入图像使用一系列级联的卷积层进行高维特征编码,然后使用相同数量的反卷积层进行解码。在解码过程中,编码器内的卷积层结果通过跨层连接送入反卷积层,使网络能够重建更高分辨率的视差细节。同时,视差网络采用多尺度重建策略,逐层完成视差重建,得到基于输入图像的与左右视图匹配的视差图像对。最后,通过视差融合算法结合左右视图视差图像得到最终的视差图。</p>
                </div>
                <div class="p1">
                    <p id="53">部分实验结果如图2所示,图2(a)为网络输入的左视图原始图像,图2(b)为基于图2(a)生成的与右视图匹配的视差图,图2(c)为基于图2(a)生成的与左视图匹配的视差图,图2(d)为融合图2(b)与(c)得到的视差图。</p>
                </div>
                <div class="area_img" id="54">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/DZCL201919027_054.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 视差网络生成的左右视图视差图与
融合后视差图结果展示" src="Detail/GetImg?filename=images/DZCL201919027_054.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 视差网络生成的左右视图视差图与
融合后视差图结果展示  <a class="btn-zoomin" href="Detail/GetImg?filename=images/DZCL201919027_054.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="55" name="55"><b>1.3 基于对极几何模型的深度图生成</b></h4>
                <div class="p1">
                    <p id="56">经上述视差网络生成视差图像后,本文基于平行光轴的双目对极几何模型<citation id="147" type="reference"><link href="21" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>把视差图转换为对应的深度图:</p>
                </div>
                <div class="p1">
                    <p id="57"><mathml id="113"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ζ</mi><msub><mrow></mrow><mrow><mi>x</mi><mi>y</mi></mrow></msub><mo>=</mo><mfrac><mrow><mi>b</mi><mo>×</mo><mi>f</mi></mrow><mrow><mi>D</mi><msub><mrow></mrow><mrow><mi>x</mi><mi>y</mi></mrow></msub></mrow></mfrac></mrow></math></mathml>.      (3)</p>
                </div>
                <div class="p1">
                    <p id="58">式中:<i>Z</i><sub><i>xy</i></sub>为左视图中像素位置<i>x</i>、<i>y</i>对应的深度值;<i>b</i>为双目相机之间的基线距离;<i>f</i>为相机的焦距;<i>D</i><sub><i>xy</i></sub>为与右视图匹配的视差图中像素位置<i>x</i>、<i>y</i>对应的视差值,由公式(4)计算得出。</p>
                </div>
                <div class="p1">
                    <p id="59"><i>D</i><sub><i>xy</i></sub>=<i>x</i><sub><i>l</i></sub>-<i>x</i><sub><i>r</i></sub>.      (4)</p>
                </div>
                <div class="p1">
                    <p id="60">具体双目对极几何模型原理如图3所示。其中,<i>P</i>为观测路标,<i>p</i><sub>1</sub>为路标P投影在左视图<i>C</i><sub>1</sub>成像平面上的像素点,<i>p</i><sub>2</sub>为路标P投影在右视图<i>C</i><sub>2</sub>成像平面上的像素点,<i>x</i><sub><i>l</i></sub>为像素点<i>p</i><sub>1</sub>离左视图<i>C</i><sub>1</sub>成像平面最左端的距离,<i>x</i><sub><i>r</i></sub>为像素点<i>p</i><sub>2</sub>离右视图<i>C</i><sub>2</sub>成像平面最左端的距离。</p>
                </div>
                <div class="area_img" id="61">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/DZCL201919027_061.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 基于平行光轴的双目对极几何模型原理" src="Detail/GetImg?filename=images/DZCL201919027_061.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 基于平行光轴的双目对极几何模型原理  <a class="btn-zoomin" href="Detail/GetImg?filename=images/DZCL201919027_061.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="62">部分实验结果如图4所示,图4(a)为采集的带有运动模糊的原始图像,图4(b)为图4(a)经过去糊网络去糊后生成的清晰图像,图4(c)为图4(b)的基础上,经过视差网络估计得到的视差图像,图4(d)为图4(c)利用基于平行光轴的双目对极几何模型生成的深度图像。</p>
                </div>
                <div class="area_img" id="63">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/DZCL201919027_063.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 LRSDR-Net深度网络各阶段估计结果展示" src="Detail/GetImg?filename=images/DZCL201919027_063.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 LRSDR-Net深度网络各阶段估计结果展示  <a class="btn-zoomin" href="Detail/GetImg?filename=images/DZCL201919027_063.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h3 id="64" name="64" class="anchor-tag"><b>2 本文算法设计</b></h3>
                <div class="p1">
                    <p id="65">现有深度估计网络通常使用带有原始图像与对应深度图的数据集作为训练样本,但是由于深度传感器成本昂贵而且受到场景环境限制,难以实现大规模场景的深度采集,使得网络的训练成本很高。</p>
                </div>
                <div class="p1">
                    <p id="66">因此,本文提出了基于左右视图一致性的多尺度LRSDR-Net深度训练网络结构,该网络使用一对互相匹配的双目图像作为输入,经过数据增强后生成新的左视图<i><b>I</b></i><sup>l</sup>与右视图<i><b>I</b></i><sup>r</sup>。然后,把<i><b>I</b></i><sup>l</sup>与<i><b>I</b></i><sup>r</sup>送入去糊网络生成多尺度左右视图去糊图像<i><b>P</b></i><sup>l,s</sup><sub><sup>1～4</sup></sub>与<i><b>P</b></i><sup>r,s</sup><sub><sup>1～4</sup></sub>,通过与下采样后的输入图像<i><b>GT</b></i><sup>l,s</sup><sub><sup>1～4</sup></sub>与<i><b>GT</b></i><sup>r,s</sup><sub><sup>1～4</sup></sub>计算误差得到第1组损失梯度。</p>
                </div>
                <div class="p1">
                    <p id="67">在此基础上,把基于最大尺度重建的去糊图像<i><b>P</b></i><sup>l,s</sup><sub><sup>4</sup></sub>送入视差网络生成4个不同尺度与左右视图匹配的视差图像对<i><b>D</b></i><sup>l,s</sup><sub><sup>1～4</sup></sub>,<i><b>D</b></i><sup>r,s</sup><sub><sup>1～4</sup></sub>,利用视差图像对本身的左右一致性约束得到第2组损失梯度。</p>
                </div>
                <div class="p1">
                    <p id="68">最后,采样视差图像<i><b>D</b></i><sup>l,s</sup><sub><sup>1～4</sup></sub>,<i><b>D</b></i><sup>r,s</sup><sub><sup>1～4</sup></sub>与去糊图像<i><b>P</b></i><sup>l,s</sup><sub><sup>1～4</sup></sub>,<i><b>P</b></i><sup>r,s</sup><sub><sup>1～4</sup></sub>重建左右视图图像<mathml id="114"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mrow><mi mathvariant="bold-italic">Ι</mi><msup><mrow></mrow><mrow><mtext>l</mtext><mo>,</mo><mtext>s</mtext><msub><mrow></mrow><mrow><mn>1</mn><mo>∼</mo><mn>4</mn></mrow></msub></mrow></msup></mrow><mo stretchy="true">¯</mo></mover><mo>,</mo><mover accent="true"><mrow><mi mathvariant="bold-italic">Ι</mi><msup><mrow></mrow><mrow><mtext>r</mtext><mo>,</mo><mtext>s</mtext><msub><mrow></mrow><mrow><mn>1</mn><mo>∼</mo><mn>4</mn></mrow></msub></mrow></msup></mrow><mo stretchy="true">¯</mo></mover></mrow></math></mathml>,利用重建图像<mathml id="115"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mrow><mi mathvariant="bold-italic">Ι</mi><msup><mrow></mrow><mrow><mtext>l</mtext><mo>,</mo><mtext>s</mtext><msub><mrow></mrow><mrow><mn>1</mn><mo>∼</mo><mn>4</mn></mrow></msub></mrow></msup></mrow><mo stretchy="true">¯</mo></mover><mo>,</mo><mover accent="true"><mrow><mi mathvariant="bold-italic">Ι</mi><msup><mrow></mrow><mrow><mtext>r</mtext><mo>,</mo><mtext>s</mtext><msub><mrow></mrow><mrow><mn>1</mn><mo>∼</mo><mn>4</mn></mrow></msub></mrow></msup></mrow><mo stretchy="true">¯</mo></mover></mrow></math></mathml>与输入图像<i><b>GT</b></i><sup>l,s</sup><sub><sup>1～4</sup></sub>与<i><b>GT</b></i><sup>r,s</sup><sub><sup>1～4</sup></sub>计算误差得到第3组损失梯度。</p>
                </div>
                <div class="p1">
                    <p id="69">通过结合各组梯度,本文构建训练网络的端到端多目标损失函数,整体训练结构如图5所示。</p>
                </div>
                <div class="area_img" id="70">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/DZCL201919027_070.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 基于左右视图一致性的多尺度LRSDR-Net
深度训练网络结构" src="Detail/GetImg?filename=images/DZCL201919027_070.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 基于左右视图一致性的多尺度<i>LRSDR</i>-<i>Net</i>
深度训练网络结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/DZCL201919027_070.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="71">为了在训练过程中提高去糊网络生成清晰图像的精度,本文基于左右视图的真值图像与网络生成的去糊图像计算对应像素间的欧式距离,构建多尺度去糊损失函数,以符号L<sub>d</sub>表示:</p>
                </div>
                <div class="p1">
                    <p id="72" class="code-formula">
                        <mathml id="72"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>L</mi><msub><mrow></mrow><mi>d</mi></msub><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>α</mi></mstyle><msup><mrow></mrow><mtext>l</mtext></msup><mfrac><mrow><mi>k</mi><msubsup><mrow></mrow><mi>i</mi><mtext>l</mtext></msubsup></mrow><mrow><mi>Ν</mi><msubsup><mrow></mrow><mi>i</mi><mtext>l</mtext></msubsup></mrow></mfrac><mrow><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">Ρ</mi><msubsup><mrow></mrow><mrow><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mtext>l</mtext></msubsup><mo>-</mo><mi mathvariant="bold-italic">G</mi><mi mathvariant="bold-italic">Τ</mi><msubsup><mrow></mrow><mrow><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mtext>l</mtext></msubsup><mo stretchy="false">∥</mo></mrow><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo></mtd></mtr><mtr><mtd><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>α</mi></mstyle><msup><mrow></mrow><mtext>r</mtext></msup><mfrac><mrow><mi>k</mi><msubsup><mrow></mrow><mi>i</mi><mtext>r</mtext></msubsup></mrow><mrow><mi>Ν</mi><msubsup><mrow></mrow><mi>i</mi><mtext>r</mtext></msubsup></mrow></mfrac><mrow><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">Ρ</mi><msubsup><mrow></mrow><mrow><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mtext>r</mtext></msubsup><mo>-</mo><mi mathvariant="bold-italic">G</mi><mi mathvariant="bold-italic">Τ</mi><msubsup><mrow></mrow><mrow><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mtext>r</mtext></msubsup><mo stretchy="false">∥</mo></mrow><msup><mrow></mrow><mn>2</mn></msup><mo>.</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>5</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="73">式中:n为尺度因子的数量;<i>α</i><sup>l</sup>、<i>α</i><sup>r</sup>分别表示左右视图的图像权重因子;<i>k</i><sup>l</sup><sub><i>i</i></sub>、<i>k</i><sup>r</sup><sub><i>i</i></sub>分别表示左右视图的尺度权重因子;<i>N</i><sup>l</sup><sub><i>i</i></sub>、<i>N</i><sup>r</sup><sub><i>i</i></sub>分别表示左右视图在<i>s</i><sub><i>i</i></sub>尺度下的像素数量;<i><b>P</b></i><sup>l</sup><sub><i>si</i></sub>、<i><b>P</b></i><sup>r</sup><sub><i>si</i></sub>、<i><b>GT</b></i><sup>l</sup><sub><i>si</i></sub>,<i><b>GT</b></i><sup>r</sup><sub><i>si</i></sub>分别表示左右视图在<i>s</i><sub><i>i</i></sub>尺度下的去糊图像与真值图像。文中,<i>α</i><sup>l</sup>=1,<i>α</i><sup>r</sup>=1,<i>k</i><sup>l</sup><sub><i>i</i></sub>=1,<i>k</i><sup>r</sup><sub><i>i</i></sub>=1表明每个尺度,每个视图之间都是等权重的。</p>
                </div>
                <div class="p1">
                    <p id="74">由于视差网络是在去糊网络的基础上进行视差图估计,所以去糊网络需要根据当前视差网络的状态去调整自身的去糊策略,以提高<i>LRSDR</i>-<i>Net</i>整体的视差估计精度。为此,本文提出了光度匹配损失函数,用于支持视差网络的梯度信息回流到去糊网络中,进而优化两个网络间的适配性,光度匹配损失函数以L<sub>m</sub>表示:</p>
                </div>
                <div class="p1">
                    <p id="75"><mathml id="116"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><msub><mrow></mrow><mi>m</mi></msub><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>L</mi></mstyle><msubsup><mrow></mrow><mi>m</mi><mrow><mtext>l</mtext><mo>,</mo><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msubsup><mo>+</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>L</mi></mstyle><msubsup><mrow></mrow><mi>m</mi><mrow><mtext>r</mtext><mo>,</mo><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msubsup></mrow></math></mathml>.      (6)</p>
                </div>
                <div class="p1">
                    <p id="76">式中:n为尺度因子的数量,可以看到,<i>L</i><sub><i>m</i></sub>是由不同尺度左右视图对应的损失子函数<i>L</i><mathml id="117"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>m</mi><mrow><mtext>l</mtext><mo>,</mo><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msubsup></mrow></math></mathml>,<i>L</i><mathml id="118"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>m</mi><mrow><mtext>r</mtext><mo>,</mo><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msubsup></mrow></math></mathml>组成:</p>
                </div>
                <div class="p1">
                    <p id="77" class="code-formula">
                        <mathml id="77"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>L</mi><msubsup><mrow></mrow><mi>m</mi><mrow><mtext>l</mtext><mo>,</mo><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msubsup><mo>=</mo><mfrac><mn>1</mn><mrow><mi>Ν</mi><msubsup><mrow></mrow><mi>i</mi><mtext>l</mtext></msubsup></mrow></mfrac><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>x</mi><mo>,</mo><mi>y</mi></mrow></munder><mi>λ</mi></mstyle><mfrac><mrow><mn>1</mn><mo>-</mo><mi>S</mi><mi>S</mi><mi>Ι</mi><mi>Μ</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">G</mi><mi mathvariant="bold-italic">Τ</mi><msubsup><mrow></mrow><mrow><mi>x</mi><mi>y</mi></mrow><mrow><mtext>l</mtext><mo>,</mo><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msubsup><mo>,</mo><mover accent="true"><mrow><mi mathvariant="bold-italic">Ι</mi><msubsup><mrow></mrow><mrow><mi>x</mi><mi>y</mi></mrow><mrow><mtext>l</mtext><mo>,</mo><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msubsup></mrow><mo stretchy="true">¯</mo></mover><mo stretchy="false">)</mo></mrow><mn>2</mn></mfrac><mo>+</mo></mtd></mtr><mtr><mtd><mfrac><mn>1</mn><mrow><mi>Ν</mi><msubsup><mrow></mrow><mi>i</mi><mtext>l</mtext></msubsup></mrow></mfrac><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>x</mi><mo>,</mo><mi>y</mi></mrow></munder><mo stretchy="false">(</mo></mstyle><mn>1</mn><mo>-</mo><mi>λ</mi><mo stretchy="false">)</mo><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">G</mi><mi mathvariant="bold-italic">Τ</mi><msubsup><mrow></mrow><mrow><mi>x</mi><mi>y</mi></mrow><mrow><mtext>l</mtext><mo>,</mo><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msubsup><mo>-</mo><mover accent="true"><mrow><mi mathvariant="bold-italic">Ι</mi><msubsup><mrow></mrow><mrow><mi>x</mi><mi>y</mi></mrow><mrow><mtext>l</mtext><mo>,</mo><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msubsup></mrow><mo stretchy="true">¯</mo></mover><mo stretchy="false">∥</mo><mo>,</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>7</mn><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mi>L</mi><msubsup><mrow></mrow><mi>m</mi><mrow><mtext>r</mtext><mo>,</mo><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msubsup><mo>=</mo><mfrac><mn>1</mn><mrow><mi>Ν</mi><msubsup><mrow></mrow><mi>i</mi><mtext>r</mtext></msubsup></mrow></mfrac><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>x</mi><mo>,</mo><mi>y</mi></mrow></munder><mi>λ</mi></mstyle><mfrac><mrow><mn>1</mn><mo>-</mo><mi>S</mi><mi>S</mi><mi>Ι</mi><mi>Μ</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">G</mi><mi mathvariant="bold-italic">Τ</mi><msubsup><mrow></mrow><mrow><mi>x</mi><mi>y</mi></mrow><mrow><mtext>r</mtext><mo>,</mo><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msubsup><mo>,</mo><mover accent="true"><mrow><mi mathvariant="bold-italic">Ι</mi><msubsup><mrow></mrow><mrow><mi>x</mi><mi>y</mi></mrow><mrow><mtext>r</mtext><mo>,</mo><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msubsup></mrow><mo stretchy="true">¯</mo></mover><mo stretchy="false">)</mo></mrow><mn>2</mn></mfrac><mo>+</mo></mtd></mtr><mtr><mtd><mfrac><mn>1</mn><mrow><mi>Ν</mi><msubsup><mrow></mrow><mi>i</mi><mtext>r</mtext></msubsup></mrow></mfrac><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>x</mi><mo>,</mo><mi>y</mi></mrow></munder><mo stretchy="false">(</mo></mstyle><mn>1</mn><mo>-</mo><mi>λ</mi><mo stretchy="false">)</mo><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">G</mi><mi mathvariant="bold-italic">Τ</mi><msubsup><mrow></mrow><mrow><mi>x</mi><mi>y</mi></mrow><mrow><mtext>r</mtext><mo>,</mo><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msubsup><mo>-</mo><mover accent="true"><mrow><mi mathvariant="bold-italic">Ι</mi><msubsup><mrow></mrow><mrow><mi>x</mi><mi>y</mi></mrow><mrow><mtext>r</mtext><mo>,</mo><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msubsup></mrow><mo stretchy="true">¯</mo></mover><mo stretchy="false">∥</mo><mo>.</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>8</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="78">式中:损失子函数的第一项为像素结构相似性差异;第二项为像素光度差异;<i>N</i><sup>l</sup><sub><i>i</i></sub>、<i>N</i><sup>r</sup><sub><i>i</i></sub>分别表示左右视图在<i>s</i><sub><i>i</i></sub>尺度下的像素数量;x、y为像素坐标;λ为结构相似性权重,<i>λ</i>=0.8;<i><b>GT</b></i><sup>l,</sup><sup><i>s</i></sup><sub><sup><i>i</i></sup></sub>、<i><b>GT</b></i><sup>r,</sup><sup><i>s</i></sup><sub><sup><i>i</i></sup></sub>分别表示左右视图在<i>s</i><sub><i>i</i></sub>尺度下的真值图像。<mathml id="119"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mrow><mi mathvariant="bold-italic">Ι</mi><msup><mrow></mrow><mrow><mtext>l</mtext><mo>,</mo><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msup></mrow><mo stretchy="true">¯</mo></mover></mrow></math></mathml>、<mathml id="120"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mrow><mi mathvariant="bold-italic">Ι</mi><msup><mrow></mrow><mrow><mtext>r</mtext><mo>,</mo><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msup></mrow><mo stretchy="true">¯</mo></mover></mrow></math></mathml>为基于视差网络生成的视差图像与去糊图像采样重建得到的左右视图;SSIM(·)为像素结构相似性计算函数,由式(9)所示:</p>
                </div>
                <div class="p1">
                    <p id="79" class="code-formula">
                        <mathml id="79"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mtext>S</mtext><mtext>S</mtext><mtext>Ι</mtext><mtext>Μ</mtext><mo stretchy="false">(</mo><mi mathvariant="bold-italic">G</mi><mi mathvariant="bold-italic">Τ</mi><msub><mrow></mrow><mrow><mi>x</mi><mi>y</mi></mrow></msub><mo>,</mo><mover accent="true"><mrow><mi mathvariant="bold-italic">Ι</mi><msub><mrow></mrow><mrow><mi>x</mi><mi>y</mi></mrow></msub></mrow><mo stretchy="true">¯</mo></mover><mo stretchy="false">)</mo><mo>=</mo></mtd></mtr><mtr><mtd><mfrac><mrow><mo stretchy="false">(</mo><mn>2</mn><mi>μ</mi><msub><mrow></mrow><mrow><mi>G</mi><mi>Τ</mi><msub><mrow></mrow><mrow><mi>x</mi><mi>y</mi></mrow></msub></mrow></msub><mi>μ</mi><msub><mrow></mrow><mrow><mover accent="true"><mrow><mi>Ι</mi><msub><mrow></mrow><mrow><mi>x</mi><mi>y</mi></mrow></msub></mrow><mo stretchy="true">¯</mo></mover></mrow></msub><mo>+</mo><mi>C</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">)</mo><mo stretchy="false">(</mo><mn>2</mn><mi>σ</mi><msub><mrow></mrow><mrow><mi>G</mi><mi>Τ</mi><msub><mrow></mrow><mrow><mi>x</mi><mi>y</mi></mrow></msub><mover accent="true"><mrow><mi>Ι</mi><msub><mrow></mrow><mrow><mi>x</mi><mi>y</mi></mrow></msub></mrow><mo stretchy="true">¯</mo></mover></mrow></msub><mo>+</mo><mi>C</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">)</mo></mrow><mrow><mo stretchy="false">(</mo><mi>μ</mi><msubsup><mrow></mrow><mrow><mi>G</mi><mi>Τ</mi><msub><mrow></mrow><mrow><mi>x</mi><mi>y</mi></mrow></msub></mrow><mn>2</mn></msubsup><mo>+</mo><mi>μ</mi><msubsup><mrow></mrow><mrow><mover accent="true"><mrow><mi>Ι</mi><msub><mrow></mrow><mrow><mi>x</mi><mi>y</mi></mrow></msub></mrow><mo stretchy="true">¯</mo></mover></mrow><mn>2</mn></msubsup><mo>+</mo><mi>C</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">)</mo><mo stretchy="false">(</mo><mi>σ</mi><msubsup><mrow></mrow><mrow><mi>G</mi><mi>Τ</mi><msub><mrow></mrow><mrow><mi>x</mi><mi>y</mi></mrow></msub></mrow><mn>2</mn></msubsup><mo>+</mo><mi>σ</mi><msubsup><mrow></mrow><mrow><mover accent="true"><mrow><mi>Ι</mi><msub><mrow></mrow><mrow><mi>x</mi><mi>y</mi></mrow></msub></mrow><mo stretchy="true">¯</mo></mover></mrow><mn>2</mn></msubsup><mo>+</mo><mi>C</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">)</mo></mrow></mfrac><mo>.</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>9</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="80">式中:<i>μ</i><sub><i>GTxy</i></sub>为真值图像的像素局部均值;<i>μ</i><sub><i>Ixy</i></sub>为重建图像的像素局部均值;<i>σ</i><mathml id="121"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>G</mi><mi>Τ</mi><msub><mrow></mrow><mrow><mi>x</mi><mi>y</mi></mrow></msub></mrow><mn>2</mn></msubsup></mrow></math></mathml>为真值图像的像素局部方差;<i>σ</i><mathml id="122"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mover accent="true"><mrow><mi>Ι</mi><msub><mrow></mrow><mrow><mi>x</mi><mi>y</mi></mrow></msub></mrow><mo stretchy="true">¯</mo></mover></mrow><mn>2</mn></msubsup></mrow></math></mathml>为重建图像的像素局部方差;<i>σ</i><sub><i>GTxyIxy</i></sub>为两幅图像的像素局部协方差;<i>C</i><sub>1</sub>,<i>C</i><sub>2</sub>为常数,<i>C</i><sub>1</sub>=0.001,<i>C</i><sub>2</sub>=0.003。为了简化计算过程,本文使用3×3窗口的平均池化层计算像素局部均值与方差。</p>
                </div>
                <div class="p1">
                    <p id="81">由于生成的视差图在边缘位置存在梯度不连续性,本文参考<i>Pm</i>-<i>huber</i>方法<citation id="148" type="reference"><link href="23" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>,根据真值图像的像素梯度信息构建边缘敏感因子来平滑视差图,平滑损失函数L<sub>sm</sub>定义如下:</p>
                </div>
                <div class="p1">
                    <p id="82"><mathml id="123"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><msub><mrow></mrow><mrow><mi>s</mi><mi>m</mi></mrow></msub><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>λ</mi></mstyle><msubsup><mrow></mrow><mrow><mi>s</mi><mi>m</mi></mrow><mrow><mtext>l</mtext><mo>,</mo><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msubsup><mi>L</mi><msubsup><mrow></mrow><mrow><mi>s</mi><mi>m</mi></mrow><mrow><mtext>l</mtext><mo>,</mo><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msubsup><mo>+</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>λ</mi></mstyle><msubsup><mrow></mrow><mrow><mi>s</mi><mi>m</mi></mrow><mrow><mtext>r</mtext><mo>,</mo><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msubsup><mi>L</mi><msubsup><mrow></mrow><mrow><mi>s</mi><mi>m</mi></mrow><mrow><mtext>r</mtext><mo>,</mo><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msubsup></mrow></math></mathml>.      (10)</p>
                </div>
                <div class="p1">
                    <p id="83">式中:n为尺度因子的数量;<i>λ</i><mathml id="124"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>s</mi><mi>m</mi></mrow><mrow><mtext>l</mtext><mo>,</mo><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msubsup></mrow></math></mathml>、<i>λ</i><mathml id="125"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>s</mi><mi>m</mi></mrow><mrow><mtext>r</mtext><mo>,</mo><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msubsup></mrow></math></mathml>分别表示左右视图的尺度平滑权重因子,为了平衡每个尺度视差图的平滑效果,本文设置<mathml id="126"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>λ</mi><msubsup><mrow></mrow><mrow><mi>s</mi><mi>m</mi></mrow><mrow><mtext>l</mtext><mo>,</mo><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msubsup><mo>=</mo><mi>λ</mi><msubsup><mrow></mrow><mrow><mi>s</mi><mi>m</mi></mrow><mrow><mtext>r</mtext><mo>,</mo><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msubsup><mo>=</mo><mfrac><mrow><mn>0</mn><mo>.</mo><mn>1</mn></mrow><mrow><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></mfrac></mrow></math></mathml>;<i>s</i><sub><i>i</i></sub>表示当前的尺度因子;<i>L</i><sub><i>sm</i></sub>是由不同尺度左右视图对应的损失子函数<i>L</i><mathml id="127"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>s</mi><mi>m</mi></mrow><mrow><mtext>l</mtext><mo>,</mo><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msubsup></mrow></math></mathml>、<i>L</i><mathml id="128"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>s</mi><mi>m</mi></mrow><mrow><mtext>r</mtext><mo>,</mo><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msubsup></mrow></math></mathml>组成:</p>
                </div>
                <div class="p1">
                    <p id="84" class="code-formula">
                        <mathml id="84"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>L</mi><msubsup><mrow></mrow><mrow><mi>s</mi><mi>m</mi></mrow><mrow><mtext>l</mtext><mo>,</mo><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msubsup><mo>=</mo><mfrac><mn>1</mn><mrow><mi>Ν</mi><msubsup><mrow></mrow><mi>i</mi><mtext>l</mtext></msubsup></mrow></mfrac><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>x</mi><mo>,</mo><mi>y</mi></mrow></munder><mtext>e</mtext></mstyle><msup><mrow></mrow><mrow><mo>-</mo><mo stretchy="false">∥</mo><mfrac><mrow><mo>∂</mo><mi>G</mi><mi>Τ</mi><msubsup><mrow></mrow><mrow><mi>x</mi><mi>y</mi></mrow><mrow><mtext>l</mtext><mo>,</mo><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msubsup></mrow><mrow><mo>∂</mo><mi>x</mi></mrow></mfrac><mo stretchy="false">∥</mo></mrow></msup><mo>×</mo><mrow><mo>|</mo><mrow><mfrac><mrow><mo>∂</mo><mi mathvariant="bold-italic">D</mi><msubsup><mrow></mrow><mrow><mi>x</mi><mi>y</mi></mrow><mrow><mtext>l</mtext><mo>,</mo><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msubsup></mrow><mrow><mo>∂</mo><mi>x</mi></mrow></mfrac></mrow><mo>|</mo></mrow><mo>+</mo><mtext>e</mtext><msup><mrow></mrow><mrow><mo>-</mo><mo stretchy="false">∥</mo><mfrac><mrow><mo>∂</mo><mi>G</mi><mi>Τ</mi><msubsup><mrow></mrow><mrow><mi>x</mi><mi>y</mi></mrow><mrow><mtext>l</mtext><mo>,</mo><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msubsup></mrow><mrow><mo>∂</mo><mi>y</mi></mrow></mfrac><mo stretchy="false">∥</mo></mrow></msup><mo>×</mo><mrow><mo>|</mo><mrow><mfrac><mrow><mo>∂</mo><mi mathvariant="bold-italic">D</mi><msubsup><mrow></mrow><mrow><mi>x</mi><mi>y</mi></mrow><mrow><mtext>l</mtext><mo>,</mo><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msubsup></mrow><mrow><mo>∂</mo><mi>y</mi></mrow></mfrac></mrow><mo>|</mo></mrow><mo>,</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>1</mn><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mi>L</mi><msubsup><mrow></mrow><mrow><mi>s</mi><mi>m</mi></mrow><mrow><mtext>r</mtext><mo>,</mo><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msubsup><mo>=</mo><mfrac><mn>1</mn><mrow><mi>Ν</mi><msubsup><mrow></mrow><mi>i</mi><mtext>r</mtext></msubsup></mrow></mfrac><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>x</mi><mo>,</mo><mi>y</mi></mrow></munder><mtext>e</mtext></mstyle><msup><mrow></mrow><mrow><mo>-</mo><mo stretchy="false">∥</mo><mfrac><mrow><mo>∂</mo><mi>G</mi><mi>Τ</mi><msubsup><mrow></mrow><mrow><mi>x</mi><mi>y</mi></mrow><mrow><mtext>r</mtext><mo>,</mo><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msubsup></mrow><mrow><mo>∂</mo><mi>x</mi></mrow></mfrac><mo stretchy="false">∥</mo></mrow></msup><mo>×</mo><mrow><mo>|</mo><mrow><mfrac><mrow><mo>∂</mo><mi mathvariant="bold-italic">D</mi><msubsup><mrow></mrow><mrow><mi>x</mi><mi>y</mi></mrow><mrow><mtext>r</mtext><mo>,</mo><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msubsup></mrow><mrow><mo>∂</mo><mi>x</mi></mrow></mfrac></mrow><mo>|</mo></mrow><mo>+</mo><mtext>e</mtext><msup><mrow></mrow><mrow><mo>-</mo><mo stretchy="false">∥</mo><mfrac><mrow><mo>∂</mo><mi>G</mi><mi>Τ</mi><msubsup><mrow></mrow><mrow><mi>x</mi><mi>y</mi></mrow><mrow><mtext>r</mtext><mo>,</mo><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msubsup></mrow><mrow><mo>∂</mo><mi>y</mi></mrow></mfrac><mo stretchy="false">∥</mo></mrow></msup><mo>×</mo><mrow><mo>|</mo><mrow><mfrac><mrow><mo>∂</mo><mi mathvariant="bold-italic">D</mi><msubsup><mrow></mrow><mrow><mi>x</mi><mi>y</mi></mrow><mrow><mtext>r</mtext><mo>,</mo><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msubsup></mrow><mrow><mo>∂</mo><mi>y</mi></mrow></mfrac></mrow><mo>|</mo></mrow><mo>.</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>2</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="85">式中:<i>L</i><mathml id="129"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>s</mi><mi>m</mi></mrow><mrow><mtext>l</mtext><mo>,</mo><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msubsup></mrow></math></mathml>、<i>L</i><mathml id="130"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>s</mi><mi>m</mi></mrow><mrow><mtext>r</mtext><mo>,</mo><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msubsup></mrow></math></mathml>分别为<i>s</i><sub><i>i</i></sub>尺度下左右视图的平滑损失函数;<i>N</i><sup>l</sup><sub><i>i</i></sub>、<i>N</i><sup>r</sup><sub><i>i</i></sub>分别表示左右视图在<i>s</i><sub><i>i</i></sub>尺度下的像素数量;x、y为像素坐标;<mathml id="131"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mo>∂</mo><mi mathvariant="bold-italic">G</mi><mi mathvariant="bold-italic">Τ</mi><msubsup><mrow></mrow><mrow><mi>x</mi><mi>y</mi></mrow><mrow><mtext>l</mtext><mo>,</mo><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msubsup></mrow><mrow><mo>∂</mo><mi>x</mi></mrow></mfrac></mrow></math></mathml>、<mathml id="132"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mo>∂</mo><mi mathvariant="bold-italic">G</mi><mi mathvariant="bold-italic">Τ</mi><msubsup><mrow></mrow><mrow><mi>x</mi><mi>y</mi></mrow><mrow><mtext>l</mtext><mo>,</mo><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msubsup></mrow><mrow><mo>∂</mo><mi>y</mi></mrow></mfrac></mrow></math></mathml>、<mathml id="133"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mo>∂</mo><mi mathvariant="bold-italic">G</mi><mi mathvariant="bold-italic">Τ</mi><msubsup><mrow></mrow><mrow><mi>x</mi><mi>y</mi></mrow><mrow><mtext>r</mtext><mo>,</mo><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msubsup></mrow><mrow><mo>∂</mo><mi>x</mi></mrow></mfrac></mrow></math></mathml>、<mathml id="134"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mo>∂</mo><mi mathvariant="bold-italic">G</mi><mi mathvariant="bold-italic">Τ</mi><msubsup><mrow></mrow><mrow><mi>x</mi><mi>y</mi></mrow><mrow><mtext>r</mtext><mo>,</mo><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msubsup></mrow><mrow><mo>∂</mo><mi>y</mi></mrow></mfrac></mrow></math></mathml>为左右视图真值图像在x、y方向上的梯度;<mathml id="135"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mo>∂</mo><mi mathvariant="bold-italic">D</mi><msubsup><mrow></mrow><mrow><mi>x</mi><mi>y</mi></mrow><mrow><mtext>l</mtext><mo>,</mo><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msubsup></mrow><mrow><mo>∂</mo><mi>x</mi></mrow></mfrac></mrow></math></mathml>、<mathml id="136"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mo>∂</mo><mi mathvariant="bold-italic">D</mi><msubsup><mrow></mrow><mrow><mi>x</mi><mi>y</mi></mrow><mrow><mtext>l</mtext><mo>,</mo><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msubsup></mrow><mrow><mo>∂</mo><mi>y</mi></mrow></mfrac></mrow></math></mathml>、<mathml id="137"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mo>∂</mo><mi mathvariant="bold-italic">D</mi><msubsup><mrow></mrow><mrow><mi>x</mi><mi>y</mi></mrow><mrow><mtext>r</mtext><mo>,</mo><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msubsup></mrow><mrow><mo>∂</mo><mi>x</mi></mrow></mfrac></mrow></math></mathml>、<mathml id="138"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mo>∂</mo><mi mathvariant="bold-italic">D</mi><msubsup><mrow></mrow><mrow><mi>x</mi><mi>y</mi></mrow><mrow><mtext>r</mtext><mo>,</mo><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msubsup></mrow><mrow><mo>∂</mo><mi>y</mi></mrow></mfrac></mrow></math></mathml>为左右视图视差图像在x、y方向上的梯度。</p>
                </div>
                <div class="p1">
                    <p id="86">为了生成更加精确的视差图像,本文提出了左右视图一致性损失函数,用于解决生成视差图过程中存在的纹理复制问题,使网络生成的左视差图匹配于网络生成的右视差图。利用训练<i>LRSDR</i>-<i>Net</i>网络过程中生成的各尺度左视差图与右视差图,构建左右视图一致性损失函数L<sub><i>lr</i></sub>:</p>
                </div>
                <div class="p1">
                    <p id="87" class="code-formula">
                        <mathml id="87"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><msub><mrow></mrow><mrow><mtext>l</mtext><mtext>r</mtext></mrow></msub><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>L</mi></mstyle><msubsup><mrow></mrow><mrow><mtext>l</mtext><mtext>r</mtext></mrow><mrow><mtext>l</mtext><mo>,</mo><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msubsup><mo>+</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>L</mi></mstyle><msubsup><mrow></mrow><mrow><mtext>l</mtext><mtext>r</mtext></mrow><mrow><mtext>r</mtext><mo>,</mo><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msubsup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>3</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="88">式中:n为尺度因子的数量;<i>L</i><mathml id="139"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>s</mi><mi>m</mi></mrow><mrow><mtext>l</mtext><mo>,</mo><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msubsup></mrow></math></mathml>,<i>L</i><mathml id="140"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>s</mi><mi>m</mi></mrow><mrow><mtext>r</mtext><mo>,</mo><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msubsup></mrow></math></mathml>分别为在<i>s</i><sub><i>i</i></sub>尺度下的左右,右左视图一致性损失函数:</p>
                </div>
                <div class="p1">
                    <p id="89" class="code-formula">
                        <mathml id="89"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>L</mi><msubsup><mrow></mrow><mrow><mtext>l</mtext><mtext>r</mtext></mrow><mrow><mtext>l</mtext><mo>,</mo><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msubsup><mo>=</mo><mfrac><mn>1</mn><mrow><mi>Ν</mi><msubsup><mrow></mrow><mi>i</mi><mtext>l</mtext></msubsup></mrow></mfrac><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>x</mi><mo>,</mo><mi>y</mi></mrow></munder><mrow><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">D</mi><msubsup><mrow></mrow><mrow><mi>x</mi><mi>y</mi></mrow><mrow><mtext>l</mtext><mo>,</mo><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msubsup><mo>-</mo><mi mathvariant="bold-italic">D</mi><msubsup><mrow></mrow><mrow><mi>x</mi><mi>y</mi><mo>+</mo><mi>D</mi><msubsup><mrow></mrow><mrow><mi>x</mi><mi>y</mi></mrow><mrow><mtext>l</mtext><mo>,</mo><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msubsup></mrow><mrow><mtext>r</mtext><mo>,</mo><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msubsup></mrow><mo>|</mo></mrow></mrow></mstyle><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>4</mn><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mi>L</mi><msubsup><mrow></mrow><mrow><mtext>l</mtext><mtext>r</mtext></mrow><mrow><mtext>r</mtext><mo>,</mo><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msubsup><mo>=</mo><mfrac><mn>1</mn><mrow><mi>Ν</mi><msubsup><mrow></mrow><mi>i</mi><mtext>r</mtext></msubsup></mrow></mfrac><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>x</mi><mo>,</mo><mi>y</mi></mrow></munder><mrow><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">D</mi><msubsup><mrow></mrow><mrow><mi>x</mi><mi>y</mi></mrow><mrow><mtext>r</mtext><mo>,</mo><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msubsup><mo>-</mo><mi mathvariant="bold-italic">D</mi><msubsup><mrow></mrow><mrow><mi>x</mi><mi>y</mi><mo>+</mo><mi>D</mi><msubsup><mrow></mrow><mrow><mi>x</mi><mi>y</mi></mrow><mrow><mtext>r</mtext><mo>,</mo><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msubsup></mrow><mrow><mtext>l</mtext><mo>,</mo><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msubsup></mrow><mo>|</mo></mrow></mrow></mstyle><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>5</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="90">式中:<i>N</i><sup>l</sup><sub><i>i</i></sub>、<i>N</i><sup>r</sup><sub><i>i</i></sub>分别表示左右视差图在<i>s</i><sub><i>i</i></sub>尺度下的像素数量;x、y为像素坐标;<i><b>D</b></i><sup>l,</sup><sup><i>s</i></sup><sub><sup><i>i</i></sup></sub>、<i><b>D</b></i><sup>r,</sup><sup><i>s</i></sup><sub><sup><i>i</i></sup></sub>分别表示在<i>s</i><sub><i>i</i></sub>尺度下估计得到的视差图像。</p>
                </div>
                <div class="p1">
                    <p id="91">综上,用于训练<i>LRSDR</i>-<i>Net</i>中各个子网络的损失函数定义如下:</p>
                </div>
                <div class="p1">
                    <p id="92"><i>Loss</i><sub>deblur</sub>=<i>β</i><sub><i>d</i></sub><i>L</i><sub><i>d</i></sub>+<i>β</i><sub><i>sm</i></sub><i>L</i><sub><i>sm</i></sub>      (16)</p>
                </div>
                <div class="p1">
                    <p id="93"><i>Loss</i><sub>disparity</sub>=<i>α</i><sub><i>m</i></sub><i>L</i><sub><i>m</i></sub>+<i>α</i><sub><i>sm</i></sub><i>L</i><sub><i>sm</i></sub>+<i>α</i><sub>lr</sub><i>L</i><sub>lr</sub>      (17)</p>
                </div>
                <div class="p1">
                    <p id="94">式中:<i>β</i><sub><i>d</i></sub>为去糊网络中多尺度去糊损失函数的权重因子;<i>β</i><sub><i>sm</i></sub>为去糊网络中光度匹配损失函数的权重因子;<i>α</i><sub><i>m</i></sub>为视差网络中光度匹配损失函数的权重因子;<i>α</i><sub><i>sm</i></sub>为视差网络中平滑损失函数的权重因子;<i>α</i><sub>lr</sub>为视差网络中左右视图一致性损失函数的权重因子。</p>
                </div>
                <h3 id="95" name="95" class="anchor-tag"><b>3 实验结果与分析</b></h3>
                <div class="p1">
                    <p id="96">本文实验软件运行环境为Ubuntu16.04,平台为Python3.6,Opencv3.2视觉库,Tensorflow1.4神经网络框架,硬件配置为Intel<sup>®</sup>Core<sup>TM</sup> i7-6700HQ 2.60 GHz CPU,32 G RAM内存+NVIDIA GEFORCE GTX 1070显卡,8 G显存。利用CUDA并行加速技术,本文方法可以以8帧/s的速率生成深度图像。</p>
                </div>
                <div class="p1">
                    <p id="97">为了验证本文所提基于LRSDR-Net单目深度估计方法的有效性,本文选取多个公开数据集进行对比实验,分别是KITTI 2015<citation id="149" type="reference"><link href="25" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>,GOPRO<citation id="150" type="reference"><link href="27" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>与Make3D。训练过程使用KITTI<citation id="151" type="reference"><link href="29" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>数据集与GOPRO数据集。</p>
                </div>
                <div class="p1">
                    <p id="98">在网络训练过程中,本文设置<i>β</i><sub><i>d</i></sub>=1,<i>β</i><sub><i>m</i></sub>=0.01,<i>α</i><sub><i>m</i></sub>=1,<i>α</i><sub><i>sm</i></sub>=1,<i>α</i><sub>lr</sub>=1。训练迭代50次,每个批次同时输入8个训练样本,初始学习率<i>η</i>=0.000 1。经30次迭代之后,每迭代10次学习率减半。选择Adam optimizer优化器,设置<i>β</i><sub>1</sub>=0.9,<i>β</i><sub>2</sub>=0.999,<i>ε</i>=10<sup>-8</sup>。</p>
                </div>
                <h4 class="anchor-tag" id="99" name="99"><b>3.1 定性实验对比分析</b></h4>
                <div class="p1">
                    <p id="100">为了验证本文所提方法LRSDR-Net在估计模糊采集图像的视差图问题上准确性优于目前已存方法,本文使用GROPRO数据集中的模糊采集图像作为输入,文献<citation id="152" type="reference">[<a class="sup">15</a>]</citation>所提方法作为对比方法进行模糊图像视差估计准确性实验。图6为文献<citation id="153" type="reference">[<a class="sup">15</a>]</citation>方法与本文LRSDR-Net网络在GOPRO数据集上的实验结果对比图。第1列为原始采集图像,来自于GOPRO数据集,均带有不同程度的运动模糊。第2列为文献<citation id="154" type="reference">[<a class="sup">15</a>]</citation>方法根据采集图像生成的视差图。第3列为本文LRSDR-Net根据采集图像生成的视差图。</p>
                </div>
                <div class="area_img" id="101">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/DZCL201919027_101.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 文献[15]与本文LRSDR-Net在GOPRO
数据集上的实验结果对比" src="Detail/GetImg?filename=images/DZCL201919027_101.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 文献<citation id="155" type="reference">[<a class="sup">15</a>]</citation>与本文LRSDR-Net在GOPRO
数据集上的实验结果对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/DZCL201919027_101.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="102">从图6中可以看出,基于文献<citation id="156" type="reference">[<a class="sup">15</a>]</citation>生成的视差图不仅存在估计边缘模糊的问题,而且估计的场景存在一定的视差缺失问题,而本文LRSDR-Net生成的视差图不仅在估计边缘上更加清晰准确,而且不存在场景视差缺失的问题。从两组生成的视差图效果对比可以明显的看出本文LRSDR-Net方法生成的视差图优于现有的文献[145]方法,证明本文方法在对模糊采集图像进行视差估计时准确性比现有方法更高。究其原因为现有的方法只是简单的对图像进行视差估计,本文为了提高视差估计质量,在进行视差估计之前对输入数据进行了优化重建,在网络中添加了多尺度递归子网络对输入图像进行多尺度去糊重建,提高了输入的质量,进而使得在处理带有运动模糊等质量不佳的输入图像时,本文方法能够比现有方法更加准确地估计其视差信息。</p>
                </div>
                <div class="p1">
                    <p id="103">为了进一步测试基于本文方法重建的视差图的质量,本文使用KITTI 2015数据集作为输入,对比基于本文方法估计的视差图与数据集中提供的真值视差图。图7所示为基于LRSDR-Net生成的视差图与真值视差图的部分实验对比。第1行为原始采集图像,来自于KITTI 2015数据集。第2行为KITTI 2015中提供的真实视差图,由于数据集中采集的视差数据十分稀疏,为了显示的美观性,本文对真值视差图进行双线性差值运算。第3行为基于LRSDR-Net根据采集图像生成的视差图像。</p>
                </div>
                <div class="area_img" id="104">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/DZCL201919027_104.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 基于LRSDR-Net生成的视差图与真值视差图的
实验结果对比" src="Detail/GetImg?filename=images/DZCL201919027_104.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 基于LRSDR-Net生成的视差图与真值视差图的
实验结果对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/DZCL201919027_104.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="105">从图7中可以看出,相比于真值视差图像,本文方法估计的视差图像有着范围更大的有效视差估计距离,但是在视差估计精度方面依旧不如真值视差图,不少物体边缘的视差估计存在过于平滑的问题。究其原因为,为了填补网络估计视差图中存在的孔洞,本文在网络的损失函数之中加入了平滑因子,平滑因子有效的填补了视差图像中的孔洞,但是导致估计的视差图中物体边缘存在模糊平滑的问题。</p>
                </div>
                <h4 class="anchor-tag" id="106" name="106"><b>3.2 客观定量对比分析</b></h4>
                <div class="p1">
                    <p id="107">为了进一步评价本文所提LRSDR-Net在视差图估计方面的准确性,本文分别将文献<citation id="159" type="reference">[<a class="sup">5</a>,<a class="sup">6</a>]</citation>、文献<citation id="157" type="reference">[<a class="sup">4</a>]</citation>、文献<citation id="158" type="reference">[<a class="sup">15</a>]</citation>方法与本文方法基于KITTI 2015数据集进行5折交叉验证对比试验,实验结果如表1所示。</p>
                </div>
                <div class="area_img" id="108">
                    <p class="img_tit"><b>表1 基于KITTI 2015不同方法的视差图估计性能对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="108" border="1"><tr><td>方法</td><td>Abs Rel</td><td>Square Rel</td><td>RMSE</td><td>RMSE Log</td></tr><tr><td><br />文献[5-6]</td><td>0.214</td><td>1.605</td><td>6.563</td><td>0.292</td></tr><tr><td><br />文献[5-6]</td><td>0.203</td><td>1.548</td><td>6.307</td><td>0.282</td></tr><tr><td><br />文献[4]</td><td>0.201</td><td>1.584</td><td>6.471</td><td>0.273</td></tr><tr><td><br />文献[15]</td><td>0.169</td><td>1.080</td><td>5.104</td><td>0.273</td></tr><tr><td><br />LRSDR-Net</td><td>0.116</td><td>0.897</td><td>4.934</td><td>0.207</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="109">由表1可知,本文方法相较于其他现有方法得到了最小的视差估计误差,究其原因为文献<citation id="162" type="reference">[<a class="sup">5</a>,<a class="sup">6</a>]</citation>只利用了估计的像素深度作为训练损失函数的依据,没有考虑到图像本身的几何结构信息,文献<citation id="160" type="reference">[<a class="sup">4</a>]</citation>利用条件随机场结合CNN网络把问题转变为了一个分类问题,其势能函数中的关联像素部分只考虑了像素间的色差、颜色直方图差异及纹理差异,没有考虑到像素间存在的几何约束问题。文献<citation id="161" type="reference">[<a class="sup">15</a>]</citation>使用自编码网络左视图构建深度图,然后通过右视图与生成的深度图采样重建左视图,利用重建的左视图与真实左视图的误差进行网络训练,但是由于没有引入左右视图一致性约束,导致估计的值存在一定的偏移。而本文根据上面3种方法存在的不足,在网络的损失函数中引入了光度匹配损失函数与左右视图一致性损失函数,用于在方法中引入图像几何结构与左右视图一致性约束,图像的几何结构约束保证网络估计的视差信息更加符合现实的场景结构,而左右视图一致性约束则解决了传统方法中视差偏移的问题,在此作用下,使得基于本文方法生成的视差图比其他现有方法生成的视差图误差更小。</p>
                </div>
                <h3 id="110" name="110" class="anchor-tag"><b>4 结  论</b></h3>
                <div class="p1">
                    <p id="111">对实时采集的场景图像进行深度估计时,由于相机抖动等因素会导致图像带有运动模糊,现有方法对此情况的处理能力较差,无法有效地估计深度信息。此外,现有方法训练网络需要使用带有深度信息的数据集,成本高昂。针对这些问题,本文提出了基于左右视图一致性约束的多尺度去糊递归网络LRSDR-Net,用于对此类场景的单目图像进行有效的实时深度估计。该方法首先对输入的图像进行多尺度去糊重建,生成去除运动模糊后的清晰图像。在此基础上,本文把直接估计图像深度问题转变为估计图像视差问题,利用自编码网络得到去糊后图像对应的视差图。最后,利用基于平行光轴的双目对极几何模型对视差图进行深度估计生成深度图像。LRSDR-Net使用基于左右视图一致性的多尺度LRSDR-Net深度训练网络进行端到端训练,训练集替换为双目图像数据集。通过实验验证本文所提基于左右视图一致性约束的多尺度去糊递归网络有效、可行。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JZCK201902051&amp;v=MDU0MzZxQnRHRnJDVVI3cWZadVp0RnlubVZMdlBMemZJWmJHNEg5ak1yWTlBWllRS0RIODR2UjRUNmo1NE8zenE=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> 何通能,尤加庚,陈德富.基于DenseNet的单目图像深度估计[J].计算机测量与控制,2019,27(2):233-236.
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JQRR201706006&amp;v=MzIzOTF0RnlubVZMdlBMenpaZkxHNEg5Yk1xWTlGWW9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVo=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> 李耀宇,王宏民,张一帆,等.基于结构化深度学习的单目图像深度估计[J].机器人,2017,39(6):812-819.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=WJFZ201610012&amp;v=MTQ2MDR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVp0RnlubVZMdlBNaWZOZExHNEg5Zk5yNDlFWm9RS0RIODQ=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> 冯春,吴小锋,尹飞鸿,等.基于局部特征匹配的双焦单目立体视觉深度估计[J].计算机技术与发展,2016,26(10):55-59.
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning depth from single monocular images using deep convolutional neural fields">

                                <b>[4]</b> LIU F,SHEN C,LIN G,et al.Learning depth from single monocular images using deep convolutional neural fields[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2016,38(10):2024-2039.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Predicting depth,surface normals and semantic labels with a common multi-scale convolutional architecture">

                                <b>[5]</b> EIGEN D,FERGUS R.Predicting depth,surface normals and semantic labels with a common multi-scale convolutional architecture[C].Proceedings of the IEEE International Conference on Computer Vision,2015:2650-2658.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Depth map prediction from a single image using a multi-scale deep network">

                                <b>[6]</b> EIGEN D,PUHRSCH C,FERGUS R.Depth map prediction from a single image using a multi-scale deep network[C].Advances in Neural Information Processing Systems,2014:2366-2374.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZGTB201903009&amp;v=MDM5MDJyZmJMRzRIOWpNckk5RmJZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVadEZ5bm1WTHZQUHk=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b> 应自炉,龙祥.多尺度密集残差网络的单幅图像超分辨率重建[J].中国图象图形学报,2019,24(3):410-419.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Convolutional LSTM network A machine learning approach for precipitation nowcasting">

                                <b>[8]</b> SHI X,CHEN Z,WANG H,et al.Convolutional LSTM network:A machine learning approach for precipitation nowcasting[C].Annual conference on Neural Information Processing Systems,2015:802-810.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZCL201306019&amp;v=MDAxNTMzenFxQnRHRnJDVVI3cWZadVp0RnlubVZMdlBJVGZJWXJHNEg5TE1xWTlFYllRS0RIODR2UjRUNmo1NE8=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b> 范群贞.运动模糊图像复原方法的研究[J].电子测量技术,2013,36(6):73-76.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_10" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZCL201504013&amp;v=MDAyOTJGeW5tVkx2UElUZklZckc0SDlUTXE0OUVaNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnQ=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[10]</b> 沈彤,刘文波,王京.基于双目立体视觉的目标测距系统[J].电子测量技术,2015,38(4):52-54.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=PM-Huber Patchmatch with huber regularization for stereo matching">

                                <b>[11]</b> HEISE P,KLOSE S,JENSEN B,et al.PM-Huber:Patchmatch with huber regularization for stereo matching[C].Proceedings of the IEEE International Conference on Computer Vision,2013:2360-2367.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Are we ready for autonomous driving?The KITTI vision benchmark suite">

                                <b>[12]</b> GEIGER A,LENZ P,URTASUN R.Are we ready for autonomous driving?The KITTI vision benchmark suite[C].2012 IEEE Conference on Computer Vision and Pattern Recognition,IEEE,2012:3354-3361.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep multi-scale convolutional neural network for dynamic scene deblurring">

                                <b>[13]</b> NAH S,KIM T H,LEE K M.Deep multi-scale convolutional neural network for dynamic scene deblurring[C].Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,2017:3883-3891.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Vision meets robotics:the KITTI dataset">

                                <b>[14]</b> GEIGER A,LENZ P,STILLER C,et al.Vision meets robotics:The KITTI dataset[J].The International Journal of Robotics Research,2013,32(11):1231-1237.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Unsupervised CNN for single view depth estimation:Geometry to the rescue">

                                <b>[15]</b> GARG R,BG V K,CARNEIRO G,et al.Unsupervised CNN for single view depth estimation:Geometry to the rescue[C].European Conference on Computer Vision,Springer,Cham,2016:740-756.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="DZCL201919027" />
        <input id="dpi" type="hidden" value="600" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZCL201919027&amp;v=MjMyODFUZklZckc0SDlqTnBvOUhZNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnRGeW5tVkx2TUk=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>


    <link href="/kxreader/Content/css/LeftDetail?v=NLcKG8I1SJUaVFrQ0iGpF2klAT0OsmHRaVSZ1rKb5xg1" rel="stylesheet"/>

</body>
</html>

