

<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>

</head>

<body>

    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637135613406975000%26DBCODE%3dCJFD%26TABLEName%3dCJFDTEMP%26FileName%3dDZCL201916023%26RESULT%3d1%26SIGN%3d%252bNfcyZD9nRPNif1V1HtItOd84IY%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=DZCL201916023&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=DZCL201916023&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>


    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZCL201916023&amp;v=MjI3MjRSN3FmWnVadEZ5am1XNzdLSVRmSVlyRzRIOWpOcVk5SFo0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1U=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dEt5YUNMQWdGcnQya2lvTlRtND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#33" data-title="&lt;b&gt;0 引  言&lt;/b&gt; "><b>0 引  言</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#38" data-title="&lt;b&gt;1 重复性结构下的三维场景重建方法&lt;/b&gt; "><b>1 重复性结构下的三维场景重建方法</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#39" data-title="&lt;b&gt;1.1 SFM重建方法的介绍及相关问题的描述&lt;/b&gt;"><b>1.1 SFM重建方法的介绍及相关问题的描述</b></a></li>
                                                <li><a href="#45" data-title="&lt;b&gt;1.2 结合视觉词袋和背景信息的三维场景重建方法&lt;/b&gt;"><b>1.2 结合视觉词袋和背景信息的三维场景重建方法</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#59" data-title="&lt;b&gt;2 实验结果与分析&lt;/b&gt; "><b>2 实验结果与分析</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#68" data-title="&lt;b&gt;3 结  论&lt;/b&gt; "><b>3 结  论</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#41" data-title="图1 SFM系统流程">图1 SFM系统流程</a></li>
                                                <li><a href="#42" data-title="图2 传统方法使用cereal数据集生成3D模型">图2 传统方法使用cereal数据集生成3D模型</a></li>
                                                <li><a href="#44" data-title="图3 cereal数据集产生的图像匹配路径(部分)">图3 cereal数据集产生的图像匹配路径(部分)</a></li>
                                                <li><a href="#53" data-title="图4 算法流程">图4 算法流程</a></li>
                                                <li><a href="#65" data-title="图5 各数据集下本文方法和传统方法的对比">图5 各数据集下本文方法和传统方法的对比</a></li>
                                                <li><a href="#65" data-title="图5 各数据集下本文方法和传统方法的对比">图5 各数据集下本文方法和传统方法的对比</a></li>
                                                <li><a href="#67" data-title="&lt;b&gt;表1 各数据集下算法用时统计&lt;/b&gt;"><b>表1 各数据集下算法用时统计</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" title=" 李丹,朱玲玲,胡迎松.基于最小生成树的多视图特征点快速匹配算法[J].华中科技大学学报(自然科学版),2017,45(1):41-45." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=HZLG201701008&amp;v=MjI0NDRhYkc0SDliTXJvOUZiSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnRGeWptVzc3TkxUZkg=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dEt5YUNMQWdGcnQya2lvTlRtND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         李丹,朱玲玲,胡迎松.基于最小生成树的多视图特征点快速匹配算法[J].华中科技大学学报(自然科学版),2017,45(1):41-45.
                                    </a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" title=" CARREIRA J,KAR A,TULSIANI S,et al.Virtual view networks for object reconstruction[J].IEEE Conference on Computer Vision and Pattern Recognition,2015:2937-2946." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Virtual view networks for object reconstruction">
                                        <b>[2]</b>
                                         CARREIRA J,KAR A,TULSIANI S,et al.Virtual view networks for object reconstruction[J].IEEE Conference on Computer Vision and Pattern Recognition,2015:2937-2946.
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title=" SNAVELY N,SEITZ S M,SZELISKI R.Photo tourism:Exploring photo collections in 3D[J].Acm Transactions on Graphics,2006,25(3):835-846." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000098386&amp;v=MTgyNDlCTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaW5sVXJ6SUlGMGNheGM9TmlmSVk3SzdIdGpOcjQ5RlpPSUhEM1Evbw==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dEt5YUNMQWdGcnQya2lvTlRtND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         SNAVELY N,SEITZ S M,SZELISKI R.Photo tourism:Exploring photo collections in 3D[J].Acm Transactions on Graphics,2006,25(3):835-846.
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" title=" WU C C.VisualSFM:A visual structure from motion system[EB/OL].2011.http://homes.cs.washington.edu/ccwu/vsfm." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=VisualSFM:A visual structure from motion system">
                                        <b>[4]</b>
                                         WU C C.VisualSFM:A visual structure from motion system[EB/OL].2011.http://homes.cs.washington.edu/ccwu/vsfm.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" title=" SCH&#214;NBERGER J L,FRAHM J M.Structure-from-motion revisited[C].IEEE Conference on Computer Vision &amp;amp; Pattern Recognition.2016:4104-4113." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Structure-from-Motion Revisited">
                                        <b>[5]</b>
                                         SCH&#214;NBERGER J L,FRAHM J M.Structure-from-motion revisited[C].IEEE Conference on Computer Vision &amp;amp; Pattern Recognition.2016:4104-4113.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" title=" HEINLY J,DUNN E,FRAHM J M.Correcting for duplicate scene structure in sparse 3D reconstruction[C].European Conference on Computer Vision.2014:780-795." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Correcting for Duplicate Scene Structure in Sparse 3D Reconstruction">
                                        <b>[6]</b>
                                         HEINLY J,DUNN E,FRAHM J M.Correcting for duplicate scene structure in sparse 3D reconstruction[C].European Conference on Computer Vision.2014:780-795.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" title=" HEINLY J,DUNN E,FRAHM J M.Recovering correct reconstructions from indistinguishable geometry[C].International Conference on 3d Vision.IEEE,2014:377-384." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Recovering correct reconstructions from indistinguishable geometry">
                                        <b>[7]</b>
                                         HEINLY J,DUNN E,FRAHM J M.Recovering correct reconstructions from indistinguishable geometry[C].International Conference on 3d Vision.IEEE,2014:377-384.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" title=" WILSON K,SNAVELY N.Network principles for SfM:disambiguating repeated structures with local context[C].Proceedings of the 2013 IEEE International Conference on Computer Vision,2013:513-520." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Network Principles for SfM: Disambiguating Repeated Structures with Local Context">
                                        <b>[8]</b>
                                         WILSON K,SNAVELY N.Network principles for SfM:disambiguating repeated structures with local context[C].Proceedings of the 2013 IEEE International Conference on Computer Vision,2013:513-520.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" title=" YAN Q A,YANG L,ZHANG L,et al.Distinguishing the indistinguishable:Exploring structural ambiguities via geodesic context[C].Computer Vision &amp;amp; Pattern Recognition.2017:152-160." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Distinguishing the indistinguishable:Exploring structural ambiguities via geodesic context">
                                        <b>[9]</b>
                                         YAN Q A,YANG L,ZHANG L,et al.Distinguishing the indistinguishable:Exploring structural ambiguities via geodesic context[C].Computer Vision &amp;amp; Pattern Recognition.2017:152-160.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_10" title=" LIN W Y,LIU S Y,JIANG N J,et al.RepMatch:Robust feature matching and pose for reconstructing modern cities[C].European Conference on Computer Vision.Springer,Cham,2016:562-579." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Rep Match:Robust Feature Matching and Pose for Reconstructing Modern Cities">
                                        <b>[10]</b>
                                         LIN W Y,LIU S Y,JIANG N J,et al.RepMatch:Robust feature matching and pose for reconstructing modern cities[C].European Conference on Computer Vision.Springer,Cham,2016:562-579.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_11" title=" ZACH C,KLOPSCHITZ M,POLLEFEYS M.Disambiguating Visual Relations Using Loop Constraints[C].Computer Vision &amp;amp; Pattern Recognition,2010:1426-1433." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Disambiguating visual relations using loop constraints">
                                        <b>[11]</b>
                                         ZACH C,KLOPSCHITZ M,POLLEFEYS M.Disambiguating Visual Relations Using Loop Constraints[C].Computer Vision &amp;amp; Pattern Recognition,2010:1426-1433.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_12" title=" 胡敏,齐梅,王晓华,等.基于显著区域词袋模型的物体识别方法[J].电子测量与仪器学报,2013,27(7):647-652." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZIY201307014&amp;v=MDIyNzF0RnlqbVc3N05JVGZDZDdHNEg5TE1xSTlFWUlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVo=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dEt5YUNMQWdGcnQya2lvTlRtND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[12]</b>
                                         胡敏,齐梅,王晓华,等.基于显著区域词袋模型的物体识别方法[J].电子测量与仪器学报,2013,27(7):647-652.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_13" >
                                        <b>[13]</b>
                                     MUR-ARTAL R,TARD&#211;S J D.ORB-SLAM2:An open-source SLAM system for monocular,stereo,and RGB-D cameras[J].IEEE Transactions on Robotics,2017,33(5):1255-1262.</a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_14" title=" ACHANTA R,SHAJI A,SMITH K,et al.SLIC superpixels compared to state-of-the-art superpixel methods[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2012,34(11):2274-2282." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=SLIC Superpixels Compared to State-of-the-Art Superpixel Methods">
                                        <b>[14]</b>
                                         ACHANTA R,SHAJI A,SMITH K,et al.SLIC superpixels compared to state-of-the-art superpixel methods[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2012,34(11):2274-2282.
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_15" title=" FURUKAWA Y,PONCE J.Accurate,dense,and robust multiview stereopsis[J].IEEE Transactions on Pattern Analysis &amp;amp; Machine Intelligence,2010,32(8):1362-1376." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Accurate, dense, and robust multiview stereopsis">
                                        <b>[15]</b>
                                         FURUKAWA Y,PONCE J.Accurate,dense,and robust multiview stereopsis[J].IEEE Transactions on Pattern Analysis &amp;amp; Machine Intelligence,2010,32(8):1362-1376.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=DZCL" target="_blank">电子测量技术</a>
                2019,42(16),128-132 DOI:10.19651/j.cnki.emt.1902712            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>针对重复性结构下的三维场景重建方法研究</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%8F%B6%E6%98%A5%E5%87%AF&amp;code=43246325&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dEt5YUNMQWdGcnQya2lvTlRtND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">叶春凯</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E4%B8%87%E6%97%BA%E6%A0%B9&amp;code=08537469&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dEt5YUNMQWdGcnQya2lvTlRtND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">万旺根</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%8A%E6%B5%B7%E5%A4%A7%E5%AD%A6%E9%80%9A%E4%BF%A1%E4%B8%8E%E4%BF%A1%E6%81%AF%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0017580&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dEt5YUNMQWdGcnQya2lvTlRtND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">上海大学通信与信息工程学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%8A%E6%B5%B7%E5%A4%A7%E5%AD%A6%E6%99%BA%E6%85%A7%E5%9F%8E%E5%B8%82%E7%A0%94%E7%A9%B6%E9%99%A2&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dEt5YUNMQWdGcnQya2lvTlRtND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">上海大学智慧城市研究院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>运动推断结构(SFM)被广泛的应用在三维重建领域,该系统可以用无序的图片进行三维重建。但是在现实生活中,很多物体都是重复性的结构,又因缺乏先验信息,如果仅依靠特征匹配,极易导致错误的图像匹配,最终重建出错误的三维结构。虽然某些物体的结构较为对称,各表面也十分相似,但是仍然包含有大量的背景信息可以利用,并且重建结果的错误往往只是个别图像误匹配所导致的。对此提出一种结合视觉词袋模型和背景信息的三维重建方法,通过实验发现,相较于传统方法,所提方法不仅降低了计算开销,更能有效的纠正错误的三维重建结果。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dEt5YUNMQWdGcnQya2lvTlRtND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">三维重建;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%87%8D%E5%A4%8D%E6%80%A7%E7%BB%93%E6%9E%84&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dEt5YUNMQWdGcnQya2lvTlRtND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">重复性结构;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%A7%86%E8%A7%89%E8%AF%8D%E8%A2%8B%E6%A8%A1%E5%9E%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dEt5YUNMQWdGcnQya2lvTlRtND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">视觉词袋模型;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%83%8C%E6%99%AF%E4%BF%A1%E6%81%AF&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dEt5YUNMQWdGcnQya2lvTlRtND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">背景信息;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    叶春凯,硕士研究生,主要研究方向基于图像的三维重建技术,机器视觉。E-mail:jackye@shu.edu.cn;
                                </span>
                                <span>
                                    万旺根,教授,博士生导师,主要研究方向为计算机图形学、信号处理和数据挖掘等。E-mail:wanwg@staff.shu.edu.cn;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-02-19</p>

                    <p>

                            <b>基金：</b>
                                                        <span>上海市科学技术委员会港澳台科技合作项目(18510760300)资助;</span>
                    </p>
            </div>
                    <h1><b>Research on 3D reconstruction method under repetitive structure</b></h1>
                    <h2>
                    <span>Ye Chunkai</span>
                    <span>Wan Wanggen</span>
            </h2>
                    <h2>
                    <span>School of Communication and Information Engineering, Shanghai University</span>
                    <span>Institute of Smart City, Shanghai University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Structure from motion(SFM)is widely used in 3 D reconstruction, which can perform 3 D reconstruction with disordered pictures. However, in our world, many objects are repetitive, symmetric structures, and due to the lack of prior information, If SFM method only depends on feature matching, it is easy to cause wrong image matches and finally reconstruct incorrect 3 D structures. In this paper, we analyze that although the structure of the objects is relatively symmetrical, it contains a lot of other background information available, and only a few erroneous image matches result in the wrong 3 D structures in SFM pipelines. Based on this, we propose a method based on the combination of the background information and the bag of words(BoW) model to correct the wrong 3 D structures, compared with traditional methods,the experimental results show that the method we proposed can not only reduce the amount of calculation, but also effectively correct the wrong 3 D reconstruction results.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=3D%20reconstruction&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dEt5YUNMQWdGcnQya2lvTlRtND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">3D reconstruction;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=repetitive%20structure&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dEt5YUNMQWdGcnQya2lvTlRtND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">repetitive structure;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=bag%20of%20words&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dEt5YUNMQWdGcnQya2lvTlRtND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">bag of words;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=background%20information&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dEt5YUNMQWdGcnQya2lvTlRtND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">background information;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2019-02-19</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="33" name="33" class="anchor-tag"><b>0 引  言</b></h3>
                <div class="p1">
                    <p id="34">近年来,随着互联网技术的快速发展,人们开始逐渐探索一些新的领域来满足人们的日常生活需求,比如无人驾驶和AR/VR领域,然而这些领域的蓬勃发展都离不开三维重建技术的支持,特别是随着计算机硬件性能的日益提升,使得基于图像的三维重建技术得到了快速发展。</p>
                </div>
                <div class="p1">
                    <p id="35">三维重建的方法大体可以分为两类,一种是实时状态的下的,比如即时定位与构图(simultaneous localization and mapping,SLAM)系统;另外一种是离线状态下的,如运动推断结构(structure from motion, SFM)系统。本文的研究方法主要着重于SFM系统的三维重建技术,SFM方法不同于实时SLAM系统,SFM方法作用于离线状态,更偏向于建模,可以从无序图片中重建出三维场景。然而随着一些相关的应用逐渐走向日常,挑战也随之而来。人们的日常生活中很多东西都是重复性结构,比如一些杯子和房子等。人们可以通过结合目标物体本身细小的差别和周围的环境来进行区分,然而计算机却不具有人这样的感知力。一般的图像匹配方法都只是通过比较两张图像中相似特征点的数目最终构建图像匹配路径<citation id="73" type="reference"><link href="3" rel="bibliography" /><link href="5" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>]</sup></citation>,如果是带有重复性结构的物体,将会存在大量的相似特征,从而造成误匹配,最终重建出错误的模型,目前的一些主流重建方法比如Bundler<citation id="70" type="reference"><link href="7" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>、Visualsfm<citation id="71" type="reference"><link href="9" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>、Colmap<citation id="72" type="reference"><link href="11" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>等都无法克服该问题。</p>
                </div>
                <div class="p1">
                    <p id="36">针对重复性结构下的三维场景重建这一问题已经引起了很多科研人员的重视,目前解决该问题的方法大体上分为两种,一种是聚类的方法,一种是基于几何约束的方法。其中基于聚类的方法,比如Heinly等<citation id="78" type="reference"><link href="13" rel="bibliography" /><link href="15" rel="bibliography" /><sup>[<a class="sup">6</a>,<a class="sup">7</a>]</sup></citation>认为如果A物体和B物体能够同时被一张照片所观察到,B物体和C物体也能够同时被一张照片所观察到,然而没有一张照片能够同时观察到A物体和C物体,那么就推测出B物体有可能是重建出的错误的结构,基于这样的假设,他们提出通过计算点云模型中节点的局部聚类系数<citation id="74" type="reference"><link href="17" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation> (local clustering coefficient, LCC)去表示它和周围节点聚集的紧密程度,如果某些节点的局部聚类系数小于某一阈值,则将其移除,再重新审视两张图片中所共同观察到的点的数目以此来修改图像匹配路径,但是该方法在某些情况下容易造成模型的过分割。Yan等<citation id="75" type="reference"><link href="19" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>认为虽然两张照片非常的相似,但是仍然存在一定量的关键点可供区分,通过聚类的思想挖掘一些关键图像中的关键点,通过与关键点的匹配去生成图像匹配路径,但是该方法需要照片间具有较多的重合,否则将影响整体的精确度。另外一种基于几何约束的方法<citation id="76" type="reference"><link href="21" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>,如Zach<citation id="77" type="reference"><link href="23" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>从回环的几何约束中去检测匹配路径中的误匹配,从而修改图像匹配路径,但是重建过程本身就携带了大量的累积误差,这对该算法的精度产生了不可忽视的影响。</p>
                </div>
                <div class="p1">
                    <p id="37">在日常生活中,虽然场景中包含着重复性结构的物体,但是在图像中仍然会难免的引入其他大量的背景信息,利用这些真实的背景信息作为计算图像匹配路径的参考标准,使得本文提出的算法较于以上算法具有良好的鲁棒性和高效性。</p>
                </div>
                <h3 id="38" name="38" class="anchor-tag"><b>1 重复性结构下的三维场景重建方法</b></h3>
                <h4 class="anchor-tag" id="39" name="39"><b>1.1 SFM重建方法的介绍及相关问题的描述</b></h4>
                <div class="p1">
                    <p id="40">SFM的方法是一种非实时的三维重建方法,主要是将一些日常生活中拍摄的无序照片,重建出三维场景,该方法的主要流程如图1所示,输入多视角的图像,图像特征的提取、匹配,之后优化得到稀疏重建的结果,最后进行稠密重建。但是如果场景中包含重复性结构的物体,在提取sift等特征之后,图像中将会包含大量相同或相似的特征,从而给图像的匹配造成大量的干扰,影响图像匹配路径的准确性。如图2所示,从(a)～(c)3张图片可以观察到图像中一共包含两个杯子,但是通过主流方法Visualsfm<citation id="79" type="reference"><link href="9" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>重建出来的模型结果如图2(d)所示,图像中却只包含一个杯子,这是因为该cereal<citation id="80" type="reference"><link href="13" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>数据集中包含了大量的重复型结构的物体,比如纸盒和杯子,所以在图像的匹配过程中造成了误匹配,最终导致了错误的三维重建结果。</p>
                </div>
                <div class="area_img" id="41">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/DZCL201916023_041.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dEt5YUNMQWdGcnQya2lvTlRtND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 SFM系统流程" src="Detail/GetImg?filename=images/DZCL201916023_041.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dEt5YUNMQWdGcnQya2lvTlRtND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 SFM系统流程  <a class="btn-zoomin" href="Detail/GetImg?filename=images/DZCL201916023_041.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dEt5YUNMQWdGcnQya2lvTlRtND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="42">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/DZCL201916023_042.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dEt5YUNMQWdGcnQya2lvTlRtND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 传统方法使用cereal数据集生成3D模型" src="Detail/GetImg?filename=images/DZCL201916023_042.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dEt5YUNMQWdGcnQya2lvTlRtND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 传统方法使用cereal数据集生成3D模型  <a class="btn-zoomin" href="Detail/GetImg?filename=images/DZCL201916023_042.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dEt5YUNMQWdGcnQya2lvTlRtND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="43">日常生活中的场景包含有大量可以作为参考的背景信息,这是非常可信的参考标准。通过实验发现,依靠传统方法,大多数场景重建的错误,都是由于个别的图像匹配错误所导致的,如图3所示,这是将cereal数据集输入进Visualsfm之后处理得到的部分图像匹配路径,可以发现传统方法计算得到的图像匹配路径中的,大部分还是正确的,只有少数非常相似的图片,由于包含大量的相同特征点,所以误匹配在了一起(红色标记部分)。针对于这种情况,本文提出结合词袋(bas of words, BOW)结构和背景信息的三维重建方法,相较于同类方法,具有一定的靶向性,恢复正确结构的同时,可以大幅度的降低计算量。</p>
                </div>
                <div class="area_img" id="44">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/DZCL201916023_044.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dEt5YUNMQWdGcnQya2lvTlRtND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 cereal数据集产生的图像匹配路径(部分)" src="Detail/GetImg?filename=images/DZCL201916023_044.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dEt5YUNMQWdGcnQya2lvTlRtND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 cereal数据集产生的图像匹配路径(部分)  <a class="btn-zoomin" href="Detail/GetImg?filename=images/DZCL201916023_044.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dEt5YUNMQWdGcnQya2lvTlRtND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="45" name="45"><b>1.2 结合视觉词袋和背景信息的三维场景重建方法</b></h4>
                <div class="p1">
                    <p id="46">本文方法以Visualsfm或者其他建模软件生成的点云模型作为输入。</p>
                </div>
                <div class="p1">
                    <p id="47">1)先将点云数据进行预处理,如果一个点能够同时被3幅图像所观察到,则保留下来以提高算法的鲁棒性。接下来我们根据点云数据采用最小生成树的方法构建Camera Graph,先计算每两张照片之间的权重w,假设<i>o</i><sub><i>A</i></sub>、<i>o</i><sub><i>B</i></sub>分别为照片A、B所观察到的点云数据,那么采用Jaccard距离式(1)可以得到两张照片的相似度,其中<i>w</i><sub>1</sub>越小,两张照片中所共有的点越多,则相似性越大,反之<i>w</i><sub>1</sub>越大,两张照片中所共有的点越少,则相似性越小。</p>
                </div>
                <div class="p1">
                    <p id="48" class="code-formula">
                        <mathml id="48"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>w</mi><msub><mrow></mrow><mn>1</mn></msub><mo>=</mo><mn>1</mn><mo>-</mo><mfrac><mrow><mi>o</mi><msub><mrow></mrow><mi>A</mi></msub><mstyle displaystyle="true"><mo>∩</mo><mi>o</mi></mstyle><msub><mrow></mrow><mi>B</mi></msub></mrow><mrow><mi>o</mi><msub><mrow></mrow><mi>A</mi></msub><mstyle displaystyle="true"><mo>∪</mo><mi>o</mi></mstyle><msub><mrow></mrow><mi>B</mi></msub></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="49">根据权重<i>w</i><sub>1</sub>可以计算图片之间的最小生成树(minimum spanning tree, MST),具体如图4(a)所示的Camera Graph。</p>
                </div>
                <div class="p1">
                    <p id="50">2)将Visualsfm或者其他建模软件生成的关于图片的sift文件进行读取,采用基于视觉词袋模型<citation id="81" type="reference"><link href="25" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>的方法训练模型,词袋模型(bags of binary words for fast place recognition in image sequence, DBOW3)以前是运用在自然语言处理(natural language processing, NLP)中,近几年在计算机视觉领域中应用非常广泛,在开源系统ORBSLAM2<citation id="82" type="reference"><link href="27" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>中被用做回环检测,效果十分出色。它的基本思想就是采用聚类的方法将特征进行聚类成<i>K</i>个质心,也就是<i>K</i>个单词,然后每张照片中就由这<i>K</i>个单词所组成的向量去表示,从而将底层特征抽象成更为复杂的特征。在这里我们根据BOW模型计算出图像间的相似度<i>weigth</i>,再用式(2)计算出<i>w</i><sub>2</sub>,最后利用最小生成树算法(MST)生成Visual Graph,具体如图4(b)所示。通过将Camera Graph和Visual Graph进行对比,具体如式3所示,若存在一条边<i>e</i><sub><i>i</i></sub>属于Camera Graph(CG)但不属于Visual Graph(VG),则将边<i>e</i><sub><i>i</i></sub>列为待检测的对象。因为实际生活中的照片都具有非常多的背景信息,传统的方法得出来的结果也只是少部分误匹配,如果将Camera Graph中的所有边都列为嫌疑对象并进行接下来的步骤,会消耗巨大的计算量,所以在这里通过结合视觉词袋模型所得出的最小生成树的结果,使得整体的算法更具有靶向性,而不是盲目的去检测所有的边。</p>
                </div>
                <div class="p1">
                    <p id="51" class="code-formula">
                        <mathml id="51"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>w</mi><msub><mrow></mrow><mn>2</mn></msub><mo>=</mo><mn>1</mn><mo>-</mo><mfrac><mrow><mi>w</mi><mi>e</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><mo>-</mo><mi>min</mi><mo stretchy="false">(</mo><mi>w</mi><mi>e</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><mi>s</mi><mo stretchy="false">)</mo></mrow><mrow><mi>max</mi><mo stretchy="false">(</mo><mi>w</mi><mi>e</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><mi>s</mi><mo stretchy="false">)</mo><mo>-</mo><mi>min</mi><mo stretchy="false">(</mo><mi>w</mi><mi>e</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><mi>s</mi><mo stretchy="false">)</mo></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="52"><i>E</i>={<i>e</i><sub><i>i</i></sub>:<i>e</i><sub><i>i</i></sub>∈<i>CG</i>∩<i>e</i><sub><i>i</i></sub>∉<i>VG</i>}      (3)</p>
                </div>
                <div class="area_img" id="53">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/DZCL201916023_053.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dEt5YUNMQWdGcnQya2lvTlRtND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 算法流程" src="Detail/GetImg?filename=images/DZCL201916023_053.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dEt5YUNMQWdGcnQya2lvTlRtND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 算法流程  <a class="btn-zoomin" href="Detail/GetImg?filename=images/DZCL201916023_053.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dEt5YUNMQWdGcnQya2lvTlRtND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="54">3)使用超像素<citation id="83" type="reference"><link href="29" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>分割方法对图片进行超像素分割,使得整张照片被分为很多小块,不同的物体之间有较好的区分,这样每一块都是一个物体的部分结构,不会包含其他的物体。如图4(c)所示,将待检测的边切分后形成<i>c</i><sub>1</sub>、<i>c</i><sub>2</sub>两个图像集合(用红色和蓝色分别表示)。将点云模型中的点分为3类,分别是共有的点<i>P</i><sub><i>c</i></sub>,仅被集合<i>c</i><sub>1</sub>所观察到的点<i>P</i><sub><i>u</i></sub><sub>1</sub>,仅被集合<i>c</i><sub>2</sub>所观察到的点 <i>P</i><sub><i>u</i></sub><sub>2</sub>。将这些独特点<i>P</i><sub><i>u</i></sub><sub>1</sub>、<i>P</i><sub><i>u</i></sub><sub>2</sub>反投影至对方的图像集合中后,理论上这些分属于不同集合的独特点反投影在照片上不会位于同一个物体表面,但如果两个分属于不同集合的独特点投影在了同一个结构之上,即投影在了同一个超像素分割块,那么就形成了冲突,其中将冲突量Conflict的定义如式(4)～(7)所示,<i>N</i>为真正的冲突区域的点的集合。其中两个图像集合的共同点<i>P</i><sub><i>c</i></sub>往往是引起误匹配的重复性结构区域,所以如果将<i>P</i><sub><i>u</i></sub><sub>1</sub>、<i>P</i><sub><i>u</i></sub><sub>2</sub>反投影至对方的图像集合中后和<i>p</i><sub><i>c</i></sub>落在相同的超像素分割块中则不纳入到冲突量计算之中。如果Conflict大于阈值<i>T</i>, 则将图像匹配对彻底分开,最后采用随机采样一致算法(random sample consensus, RANSAC)重新匹配到正确的匹配路径节点上,并继续重复以上步骤,保证所有的conflict小于阈值<i>T</i>。</p>
                </div>
                <div class="p1">
                    <p id="55"><i>N</i>=<i>near</i>(<i>P</i><sub><i>u</i></sub><sub>1</sub>,<i>project</i>(<i>P</i><sub><i>u</i></sub><sub>2</sub>))∩<i>near</i>(<i>P</i><sub><i>u</i></sub><sub>2</sub>,<i>project</i>(<i>P</i><sub><i>u</i></sub><sub>1</sub>))      (4)</p>
                </div>
                <div class="p1">
                    <p id="56"><i>Conflict</i><sub>1</sub>={<i>p</i><sub><i>u</i></sub><sub>1</sub>:<i>p</i><sub><i>u</i></sub><sub>1</sub>∈<i>p</i><sub><i>u</i></sub><sub>1</sub>∩<i>p</i><sub><i>u</i></sub><sub>1</sub>∈<i>N</i>}      (5)</p>
                </div>
                <div class="p1">
                    <p id="57"><i>Conflict</i><sub>2</sub>={<i>p</i><sub><i>u</i></sub><sub>2</sub>:<i>p</i><sub><i>u</i></sub><sub>2</sub>∈<i>p</i><sub><i>u</i></sub><sub>2</sub>∩<i>p</i><sub><i>u</i></sub><sub>2</sub>∈<i>N</i>}      (6)</p>
                </div>
                <div class="p1">
                    <p id="58"><i>Conflict</i>=min(<i>Conflict</i><sub>1</sub>,<i>Conflict</i><sub>2</sub>)      (7)</p>
                </div>
                <h3 id="59" name="59" class="anchor-tag"><b>2 实验结果与分析</b></h3>
                <div class="p1">
                    <p id="60">本文所使用的计算平台的主要参数如下:8 G内存、4核8线程、3.41 GHz主频CPU。运行环境为Windows系统,使用MATLAB2016作为算法的主要运行平台。</p>
                </div>
                <div class="p1">
                    <p id="61">本文使用的主要数据集为cereal、street、indoor<citation id="84" type="reference"><link href="13" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>,将以上3个数据集分别用本文方法与目前运用较广的开源软件Visualsfm<citation id="85" type="reference"><link href="9" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>先进行稀疏重建,然后再用经典多视图稠密重建方法PMVS/CMVS<citation id="86" type="reference"><link href="31" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>得到稠密重建的结果,结果如图5所示,图5中由上至下依次对应着cereal、street、indoor数据集的重建结果,左列为本文方法的结果,右列为Visualsfm方法的结果。由图5可以发现,现有的增量式建模方法Visualsfm在带有重复性结构的场景进行建模时,会出现很大的偏差,例如在street数据中,场景是包含了3套房子,但是Visualsfm方法最终重建出了大概2套房子,结构明显不完整,而本文的方法重建出了3套房子的结构,模型较为完整。在cereal数据集中现有方法只是重建出了一只杯子和盒子,模型结构同样发生错乱导致不完整,而本文的方法重建出两只杯子和两个盒子,较为完整的恢复了场景的三维结构。在Indoor数据集中,现有方法中大楼结构也出现了残损,本文方法重建的大楼则较为完整,无明显残缺。</p>
                </div>
                <div class="p1">
                    <p id="62">这是因为目前主流的三维重建方法,如Visualsfm<citation id="89" type="reference"><link href="7" rel="bibliography" /><link href="9" rel="bibliography" /><link href="11" rel="bibliography" /><sup>[<a class="sup">3</a>,<a class="sup">4</a>,<a class="sup">5</a>]</sup></citation>等在进行三维场景的过程中,虽然对每两张图片进行特征匹配,并通过RANSAC方法估计基础矩阵,虽然能够避免一些外点的干扰,但是由于特征点集中包含大量重复性结构所产生的相似特征,使图片之间出现误匹配,从而对基础矩阵产生了错误的估计,即错误的几何关系。一些学者<citation id="87" type="reference"><link href="3" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>为了提高该算法的效率,在计算图像匹配路径的过程中,避免了图片两两之间进行特征匹配,基础矩阵的估计,而是通过比较图像间特征点的匹配个数的多少进行匹配代价的估计,利用MST生成图像匹配路径,虽然性能提升了,但是更容易忽视重复性结构所造成的影响。以上方法如果应用在存在重复性结构场景中,该方法就会造成图像误匹配,对几何关系产生错误的估计,最终造成图5中右列所示的残缺的三维重建结果。而本文的方法利用背景信息作为参考,在现有的图像匹配路径的情况下,通过将生成的3D点云重投影的方式计算两图片间的Conflict值来决定那两张图片进行最终匹配,参考标准更加真实可信,最终对点云数据进行修正。相较于目前的主流重建方法Visualsfm<citation id="88" type="reference"><link href="9" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>,从图5中很容易观察到本文提出的方法能够明显改善重复性结构对三维重建结果所造成的影响,对恢复场景的正确三维结构具有良好的效果。</p>
                </div>
                <div class="p1">
                    <p id="63">将本文提到的方法和现有的针对重复性结构的三维重建方法Heinly<citation id="90" type="reference"><link href="13" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>作比较,由于目前还没有关于带有重复性结构的三维场景重建标准数据集以及对应的结果评价标准,所以针对两种方法的重建结果,在重建结果视觉效果相同的情况下,给出关于算法耗时的数据对比,该数据为多次实验后的统计均值,具体结果如表1所示。由表1可知在同样3个数据集的情况下,本文提出的方法较Heinly<citation id="91" type="reference"><link href="13" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>方法在算法效率上有明显提高,并且随着数据集的增大,优势会逐渐体现。这是因为Heinly<citation id="92" type="reference"><link href="13" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>的算法虽然也利用了背景信息,但是在检测现有匹配路径是否有误的过程,选择遍历整个匹配路径进行检测,没有针对性,计算量大,并且容易因为几何误差造成过度分分割,而本文的方法如图4所示,采用视觉词袋模型生成待定的图像匹配路径,再与原始的匹配路径进行比较,选取其中不一致的匹配对进行反投影,计算Conflict值。这就使得本文的算法具有一定的靶向性,更能够快速准确的列出图像匹配中发生误匹配的图像对,从而进行修正。</p>
                </div>
                <div class="area_img" id="65">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/DZCL201916023_06500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dEt5YUNMQWdGcnQya2lvTlRtND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 各数据集下本文方法和传统方法的对比" src="Detail/GetImg?filename=images/DZCL201916023_06500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dEt5YUNMQWdGcnQya2lvTlRtND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 各数据集下本文方法和传统方法的对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/DZCL201916023_06500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dEt5YUNMQWdGcnQya2lvTlRtND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="65">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/DZCL201916023_06501.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dEt5YUNMQWdGcnQya2lvTlRtND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 各数据集下本文方法和传统方法的对比" src="Detail/GetImg?filename=images/DZCL201916023_06501.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dEt5YUNMQWdGcnQya2lvTlRtND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 各数据集下本文方法和传统方法的对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/DZCL201916023_06501.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dEt5YUNMQWdGcnQya2lvTlRtND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="67">
                    <p class="img_tit"><b>表1 各数据集下算法用时统计</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="67" border="1"><tr><td rowspan="2"><br />Dataset</td><td rowspan="2">Image</td><td rowspan="2">Points</td><td colspan="2"><br />Time/s</td></tr><tr><td><br />Ours</td><td>Heinly<sup>[6]</sup></td></tr><tr><td><br />Street</td><td>19</td><td>7 607</td><td>57</td><td>60</td></tr><tr><td><br />cereal</td><td>25</td><td>12 194</td><td>83</td><td>88</td></tr><tr><td><br />Indoor</td><td>152</td><td>69 632</td><td>255</td><td>269</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h3 id="68" name="68" class="anchor-tag"><b>3 结  论</b></h3>
                <div class="p1">
                    <p id="69">本文提出的结合视觉词袋模型和背景信息的三维重建方法在基于传统增量式重建方法的基础上对带有重复性结构的场景能够取得非常好的效果,本文的方法与传统方法Visualsfm,Colmap等方法相比,能够保证三维场景重建结果的完整性,与同类方法相比较,在有效恢复场景重建结构的同时,能够降低一定的计算量。不光在SFM领域,在SLAM领域中回环检测问题也可以利用本文的算法思想。但是本文的方法是基于特征点的方法,在一些弱纹理区域,或者在一些动态场景中,没有稳定的背景信息做支撑的情况下,本文提出的方法还有待提升,还可以进一步的选用其他稳定性更好的特征,比如可以用神经网络提取更加抽象的特征来替代词袋模型所产生的特征等。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=HZLG201701008&amp;v=MjExMzBiTXJvOUZiSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnRGeWptVzc3TkxUZkhhYkc0SDk=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dEt5YUNMQWdGcnQya2lvTlRtND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> 李丹,朱玲玲,胡迎松.基于最小生成树的多视图特征点快速匹配算法[J].华中科技大学学报(自然科学版),2017,45(1):41-45.
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Virtual view networks for object reconstruction">

                                <b>[2]</b> CARREIRA J,KAR A,TULSIANI S,et al.Virtual view networks for object reconstruction[J].IEEE Conference on Computer Vision and Pattern Recognition,2015:2937-2946.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000098386&amp;v=MTkyNTN0ak5yNDlGWk9JSEQzUS9vQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp0RmlubFVyeklJRjBjYXhjPU5pZklZN0s3SA==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dEt5YUNMQWdGcnQya2lvTlRtND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> SNAVELY N,SEITZ S M,SZELISKI R.Photo tourism:Exploring photo collections in 3D[J].Acm Transactions on Graphics,2006,25(3):835-846.
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=VisualSFM:A visual structure from motion system">

                                <b>[4]</b> WU C C.VisualSFM:A visual structure from motion system[EB/OL].2011.http://homes.cs.washington.edu/ccwu/vsfm.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Structure-from-Motion Revisited">

                                <b>[5]</b> SCHÖNBERGER J L,FRAHM J M.Structure-from-motion revisited[C].IEEE Conference on Computer Vision &amp; Pattern Recognition.2016:4104-4113.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Correcting for Duplicate Scene Structure in Sparse 3D Reconstruction">

                                <b>[6]</b> HEINLY J,DUNN E,FRAHM J M.Correcting for duplicate scene structure in sparse 3D reconstruction[C].European Conference on Computer Vision.2014:780-795.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Recovering correct reconstructions from indistinguishable geometry">

                                <b>[7]</b> HEINLY J,DUNN E,FRAHM J M.Recovering correct reconstructions from indistinguishable geometry[C].International Conference on 3d Vision.IEEE,2014:377-384.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Network Principles for SfM: Disambiguating Repeated Structures with Local Context">

                                <b>[8]</b> WILSON K,SNAVELY N.Network principles for SfM:disambiguating repeated structures with local context[C].Proceedings of the 2013 IEEE International Conference on Computer Vision,2013:513-520.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Distinguishing the indistinguishable:Exploring structural ambiguities via geodesic context">

                                <b>[9]</b> YAN Q A,YANG L,ZHANG L,et al.Distinguishing the indistinguishable:Exploring structural ambiguities via geodesic context[C].Computer Vision &amp; Pattern Recognition.2017:152-160.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Rep Match:Robust Feature Matching and Pose for Reconstructing Modern Cities">

                                <b>[10]</b> LIN W Y,LIU S Y,JIANG N J,et al.RepMatch:Robust feature matching and pose for reconstructing modern cities[C].European Conference on Computer Vision.Springer,Cham,2016:562-579.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Disambiguating visual relations using loop constraints">

                                <b>[11]</b> ZACH C,KLOPSCHITZ M,POLLEFEYS M.Disambiguating Visual Relations Using Loop Constraints[C].Computer Vision &amp; Pattern Recognition,2010:1426-1433.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_12" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZIY201307014&amp;v=MDc1MTZZSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnRGeWptVzc3TklUZkNkN0c0SDlMTXFJOUU=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dEt5YUNMQWdGcnQya2lvTlRtND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[12]</b> 胡敏,齐梅,王晓华,等.基于显著区域词袋模型的物体识别方法[J].电子测量与仪器学报,2013,27(7):647-652.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_13" >
                                    <b>[13]</b>
                                 MUR-ARTAL R,TARDÓS J D.ORB-SLAM2:An open-source SLAM system for monocular,stereo,and RGB-D cameras[J].IEEE Transactions on Robotics,2017,33(5):1255-1262.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=SLIC Superpixels Compared to State-of-the-Art Superpixel Methods">

                                <b>[14]</b> ACHANTA R,SHAJI A,SMITH K,et al.SLIC superpixels compared to state-of-the-art superpixel methods[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2012,34(11):2274-2282.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Accurate, dense, and robust multiview stereopsis">

                                <b>[15]</b> FURUKAWA Y,PONCE J.Accurate,dense,and robust multiview stereopsis[J].IEEE Transactions on Pattern Analysis &amp; Machine Intelligence,2010,32(8):1362-1376.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="DZCL201916023" />
        <input id="dpi" type="hidden" value="96" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dEt5YUNMQWdGcnQya2lvTlRtND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZCL201916023&amp;v=MjI3MjRSN3FmWnVadEZ5am1XNzdLSVRmSVlyRzRIOWpOcVk5SFo0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1U=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN6dEt5YUNMQWdGcnQya2lvTlRtND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="0" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>


    <link href="/kxreader/Content/css/LeftDetail?v=NLcKG8I1SJUaVFrQ0iGpF2klAT0OsmHRaVSZ1rKb5xg1" rel="stylesheet"/>

</body>
</html>

