

<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>

</head>

<body>

    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637135666744162500%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dDZCL201914015%26RESULT%3d1%26SIGN%3dSwBMZ4L%252fkqQKxU2ejvPtlNvBXME%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=DZCL201914015&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=DZCL201914015&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>


    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZCL201914015&amp;v=MTI4NTZZWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnRGeXZoVmJ2T0lUZklZckc0SDlqTnE0OUU=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#35" data-title="&lt;b&gt;0 引  言&lt;/b&gt; "><b>0 引  言</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#40" data-title="&lt;b&gt;1 相关算法&lt;/b&gt; "><b>1 相关算法</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#43" data-title="&lt;b&gt;1.1 最大间距准则&lt;/b&gt;"><b>1.1 最大间距准则</b></a></li>
                                                <li><a href="#56" data-title="&lt;b&gt;1.2 局部敏感判别分析&lt;/b&gt;"><b>1.2 局部敏感判别分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#72" data-title="&lt;b&gt;2 相似性与多样性判别投影&lt;/b&gt; "><b>2 相似性与多样性判别投影</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#73" data-title="&lt;b&gt;2.1 基本思想&lt;/b&gt;"><b>2.1 基本思想</b></a></li>
                                                <li><a href="#122" data-title="&lt;b&gt;2.2 算法描述&lt;/b&gt;"><b>2.2 算法描述</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#140" data-title="&lt;b&gt;3 实  验&lt;/b&gt; "><b>3 实  验</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#142" data-title="&lt;b&gt;3.1 Yale数据集上的实验&lt;/b&gt;"><b>3.1 Yale数据集上的实验</b></a></li>
                                                <li><a href="#150" data-title="&lt;b&gt;3.2 ORL数据集上的实验&lt;/b&gt;"><b>3.2 ORL数据集上的实验</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#158" data-title="&lt;b&gt;4 结  论&lt;/b&gt; "><b>4 结  论</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#124" data-title="图1 SDDP算法流程">图1 SDDP算法流程</a></li>
                                                <li><a href="#144" data-title="图2 Yale数据集上一个人的11幅图像">图2 Yale数据集上一个人的11幅图像</a></li>
                                                <li><a href="#148" data-title="图3 Yale数据集上的准确率曲线">图3 Yale数据集上的准确率曲线</a></li>
                                                <li><a href="#148" data-title="图3 Yale数据集上的准确率曲线">图3 Yale数据集上的准确率曲线</a></li>
                                                <li><a href="#149" data-title="&lt;b&gt;表1 Yale数据集上的最高准确率 (%) 及对应的维数&lt;/b&gt;"><b>表1 Yale数据集上的最高准确率 (%) 及对应的维数</b></a></li>
                                                <li><a href="#152" data-title="图4 ORL数据集中一个人的10幅图像">图4 ORL数据集中一个人的10幅图像</a></li>
                                                <li><a href="#155" data-title="图5 ORL数据集上的准确率曲线">图5 ORL数据集上的准确率曲线</a></li>
                                                <li><a href="#156" data-title="&lt;b&gt;表2 ORL数据集上的最高准确率 (%) 及对应的维数&lt;/b&gt;"><b>表2 ORL数据集上的最高准确率 (%) 及对应的维数</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" title=" 甘炎灵, 金聪.间距判别投影及其在表情识别中的应用[J].计算机应用, 2017, 37 (5) :1413-1418." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201705035&amp;v=MDIwNTlLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnRGeXZoVmJ2T0x6N0JkN0c0SDliTXFvOUdZWVE=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         甘炎灵, 金聪.间距判别投影及其在表情识别中的应用[J].计算机应用, 2017, 37 (5) :1413-1418.
                                    </a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" title=" 张仕霞, 金聪.基于 Lp 范数的二维最大间距准则及其在人脸识别中的应用[J].电子测量技术, 2017, 40 (12) :196-202." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZCL201712039&amp;v=MTMyNzVHYllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVp0Rnl2aFZidk9JVGZJWXJHNEg5Yk5yWTk=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         张仕霞, 金聪.基于 Lp 范数的二维最大间距准则及其在人脸识别中的应用[J].电子测量技术, 2017, 40 (12) :196-202.
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title=" JAIN A K, DUIN R P W, MAO J.Statistical pattern recognition:A review[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2000, 22 (1) :4-37." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Statistical pattern recognition: a review">
                                        <b>[3]</b>
                                         JAIN A K, DUIN R P W, MAO J.Statistical pattern recognition:A review[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2000, 22 (1) :4-37.
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" title=" BELHUMEUR P N, HESPANHA J P, KRIEGMAN D J.Eigenfaces vs.Fisherfaces:Recognition using class specific linear projection[C].European Conference on Computer Vision.Springer, Berlin, Heidelberg, 1996:43-58." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Eigenfaces vs. Fisherfaces: Recognition using class specific linear projection">
                                        <b>[4]</b>
                                         BELHUMEUR P N, HESPANHA J P, KRIEGMAN D J.Eigenfaces vs.Fisherfaces:Recognition using class specific linear projection[C].European Conference on Computer Vision.Springer, Berlin, Heidelberg, 1996:43-58.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" title=" CHEN L F, LIAO H Y M, KO M T, et al.A new LDA-based face recognition system which can solve the small sample size problem[J].Pattern Recognition, 2000, 33 (10) :1713-1726." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011600741248&amp;v=MTE5MDViSzdIdEROcVk5RlkrOE9Ebmd4b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcnpJSTFvU2JoUT1OaWZPZg==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                         CHEN L F, LIAO H Y M, KO M T, et al.A new LDA-based face recognition system which can solve the small sample size problem[J].Pattern Recognition, 2000, 33 (10) :1713-1726.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" title=" LI H, JIANG T, ZHANG K.Efficient and robust feature extraction by maximum margin criterion[C].Advances in Neural Information Processing Systems, 2004:97-104." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Efficient and robust feature extraction by maximum margin criterion">
                                        <b>[6]</b>
                                         LI H, JIANG T, ZHANG K.Efficient and robust feature extraction by maximum margin criterion[C].Advances in Neural Information Processing Systems, 2004:97-104.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" title=" BALASUBRAMANIAN M, SCHWARTZ E L.The isomap algorithm and topological stability[J].Science, 2002, 295 (5552) :7-7." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The isomap algorithm and topological stability">
                                        <b>[7]</b>
                                         BALASUBRAMANIAN M, SCHWARTZ E L.The isomap algorithm and topological stability[J].Science, 2002, 295 (5552) :7-7.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" >
                                        <b>[8]</b>
                                     ROWEIS S T, SAUL L K.Nonlinear dimensionality reduction by locally linear embedding[J].Science, 2000, 290 (5500) :2323-2326.</a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" title=" BELKIN M, NIYOGI P.Laplacian eigenmaps for dimensionality reduction and data representation[J].Neural Computation, 2003, 15 (6) :1373-1396." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJBK&amp;filename=SJBK15090500012194&amp;v=MjkzMTFKWmJLOUh0ak1xbzlGWk9vTkRYVTlvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp0RmlubFVyeklJMW9TYmhRPU5pZg==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                         BELKIN M, NIYOGI P.Laplacian eigenmaps for dimensionality reduction and data representation[J].Neural Computation, 2003, 15 (6) :1373-1396.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_10" title=" BENGIO Y, PAIEMENT J, VINCENT P, et al.Out-of-sample extensions for lle, isomap, mds, eigenmaps, and spectral clustering[C].Advances in Neural Information Processing Systems, 2004:177-184." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Out-of-Sample Extensions for LLE, Isomap, MDS, Eigenmaps and Spectral Clustering">
                                        <b>[10]</b>
                                         BENGIO Y, PAIEMENT J, VINCENT P, et al.Out-of-sample extensions for lle, isomap, mds, eigenmaps, and spectral clustering[C].Advances in Neural Information Processing Systems, 2004:177-184.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_11" title=" HE X, YAN S, HU Y, et al.Face recognition using laplacianfaces[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2005, 27 (3) :328-340." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Face recognition using laplacianfaces">
                                        <b>[11]</b>
                                         HE X, YAN S, HU Y, et al.Face recognition using laplacianfaces[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2005, 27 (3) :328-340.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_12" title=" CAI D, HE X, ZHOU K, et al.Locality sensitive discriminant analysis[C].Proceedings of the 20th International Joint Conference on Artifical Intelligence, 2007:708-713." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Locality sensitive discriminant analysis">
                                        <b>[12]</b>
                                         CAI D, HE X, ZHOU K, et al.Locality sensitive discriminant analysis[C].Proceedings of the 20th International Joint Conference on Artifical Intelligence, 2007:708-713.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_13" title=" YAN S, XU D, ZHANG B, et al.Graph embedding and extensions:A general framework for dimensionality reduction[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2007, 29 (1) :40-51." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Graph embedding and extensions:a general framework for dimensionality reduction">
                                        <b>[13]</b>
                                         YAN S, XU D, ZHANG B, et al.Graph embedding and extensions:A general framework for dimensionality reduction[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2007, 29 (1) :40-51.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_14" title=" GAO Q, LIU J, CUI K, et al.Stable locality sensitive discriminant analysis for image recognition[J].Neural Networks, 2014, 54:49-56." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14032300094688&amp;v=MTY0MjVSZEdlcnFRVE1ud1plWnRGaW5sVXJ6SUkxb1NiaFE9TmlmT2ZiSzhIdExPckk5RlpPSUxDblF4b0JNVDZUNFBRSC9pcg==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[14]</b>
                                         GAO Q, LIU J, CUI K, et al.Stable locality sensitive discriminant analysis for image recognition[J].Neural Networks, 2014, 54:49-56.
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_15" title=" YANG J, ZHANG D, YANG J, et al.Globally maximizing, locally minimizing:unsupervised discriminant projection with applications to face and palm biometrics[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2007, 29 (4) :650-664." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Globally Maximizing, Locally Minimizing: Unsupervised Discriminant Projection with Applications to Face and Palm Biometrics">
                                        <b>[15]</b>
                                         YANG J, ZHANG D, YANG J, et al.Globally maximizing, locally minimizing:unsupervised discriminant projection with applications to face and palm biometrics[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2007, 29 (4) :650-664.
                                    </a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_16" title=" CHEN H T, CHANG H W, LIU T L.Local discriminant embedding and its variants[C].IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2005.CVPR 2005.IEEE, 2005 (2) :846-853." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Local discriminant embedding and its variants">
                                        <b>[16]</b>
                                         CHEN H T, CHANG H W, LIU T L.Local discriminant embedding and its variants[C].IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2005.CVPR 2005.IEEE, 2005 (2) :846-853.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=DZCL" target="_blank">电子测量技术</a>
                2019,42(14),91-97 DOI:10.19651/j.cnki.emt.1902699            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>结合局部与非局部结构的相似性与多样性判别投影在人脸识别上的应用</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%AD%8F%E4%B9%89%E5%BA%B7&amp;code=41193633&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">魏义康</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%87%91%E8%81%AA&amp;code=07645393&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">金聪</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%8D%8E%E4%B8%AD%E5%B8%88%E8%8C%83%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%AD%A6%E9%99%A2&amp;code=0200298&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">华中师范大学计算机学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>为了保持同类样本之间的相似性信息与异类样本之间的多样性信息, 提出一种结合样本的局部近邻结构与非局部近邻结构的判别投影降维算法——相似性与多样性判别投影 (SDDP) 。SDDP利用同类样本表示样本之间的相似性信息, 利用异类样本表示样本之间的多样性信息, 最小化同类样本之间的相似性、最大化异类样本之间的多样性来增强算法的判别性能。为了避免仅使用局部信息而忽略非局部信息的作用, 在表示同类样本之间相似性与异类样本之间多样性时同时考虑样本之间的局部近邻结构与非局部结构。采用最大间距准则 (MMC) , 最小化异类样本之间的多样性与同类样本之间的相似性之差来避免小样本问题。在人脸数据集上的实验表明了SDDP算法提取的低维特征能够提升分类的准确率。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">人脸识别;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%99%8D%E7%BB%B4&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">降维;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">特征提取;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%88%A4%E5%88%AB%E6%8A%95%E5%BD%B1&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">判别投影;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    魏义康, 硕士研究生, 主要研究方向为数字图像处理。;
                                </span>
                                <span>
                                    金聪, 教授, 主要研究方向为数字图像处理、图像水印、图像语义等。E-mail:jinc26@aliyun.com;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-02-15</p>

                    <p>

                            <b>基金：</b>
                                                        <span>中央高校基本科研业务费 (创新资助项目) (2018CXZZ040) 资助;</span>
                    </p>
            </div>
                    <h1><b>Similarity and diversity discriminant projection combining local and non-local structure for face recognition</b></h1>
                    <h2>
                    <span>Wei Yikang</span>
                    <span>Jin Cong</span>
            </h2>
                    <h2>
                    <span>School of Computer, Central China Normal University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>In order to maintain the similarity of within-class samples and the diversity of between-class samples, a dimensionality reduction algorithm named similarity and diversity discriminant projection (SDDP) is proposed, which combines the local neighbor structure and non-local structure of samples. SDDP uses within-class samples to represent the similarity information of samples, and uses between-class samples to represent the diversity information of samples. It minimizes the similarity of within-class samples and maximizes the diversity of between-class samples to enhance the discrimination performance of the algorithm. In order to avoid the problem of only using local information and ignoring the role of non-local information, the local neighbor structure and non-local structure of samples are all taken into account when expressing the similarity of within-class samples and the diversity of between-class samples. In order to avoid the small sample problem, the Maximum Margin Criterion (MMC) is adopted to minimize the difference between the diversity of between-class samples and the similarity of within-class samples. Experiments on face datasets show that the low-dimensional features extracted by SDDP algorithm can improve the classification accuracy.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=face%20recognition&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">face recognition;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=dimensionality%20reduction&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">dimensionality reduction;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=feature%20extraction&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">feature extraction;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=discriminant%20projection&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">discriminant projection;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2019-02-15</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="35" name="35" class="anchor-tag"><b>0 引  言</b></h3>
                <div class="p1">
                    <p id="36">在计算机视觉与模式识别等领域, 经常遇到由于数据维数过高而导致的计算代价与存储空间过大的问题, 甚至会导致维度灾难的问题。对高维数据进行降维处理能够有效地避免由于数据冗余而造成的计算代价增大与存储空间增加的问题, 同时按照不同的目的从高维数据中提取有效的低维特征能够显著地提升任务的效果。在人脸识别等分类任务中, 希望获得适合分类的低维数据表示来提升分类的准确率。已有大量的研究工作通过不同的目的对人脸数据进行降维<citation id="161" type="reference"><link href="3" rel="bibliography" /><link href="5" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>]</sup></citation>, 获得的低维数据表示显著地提升了分类的准确率。</p>
                </div>
                <div class="p1">
                    <p id="37">按照降维过程中是否利用数据的类别信息可将降维算法分为无监督降维算法有有监督降维算法。其中无监督降维算法没有利用数据的类别信息, 在人脸识别领域, 主成分分析 (principle component analysis, PCA) <citation id="162" type="reference"><link href="7" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation> 是一种应用广泛的降维算法。PCA通过最大化投影后样本之间的协方差来保留样本之间的多样性, 求解一个投影矩阵, 利用线性变换将原始空间中的数据投影到低维子空间中。然而PCA算法未利用样本的类别信息, 减弱了PCA在分类任务中的效果。线性判别分析 (linear discriminant analysis, LDA) <citation id="163" type="reference"><link href="9" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation> 作为一种有监督降维算法利用数据的类别信息寻求一个投影方向, 最大化异类样本之间投影后的距离、最小化同类样本之间投影后的距离获得了更强的判别性能, 在分类任务中取得良好的效果。但是LDA算法的求解需要满足类内离散度矩阵非奇异的条件以避免小样本问题<citation id="164" type="reference"><link href="11" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>, 这在实际应用中通常难以满足。解决LDA的小样本问题可以先用PCA对数据进行预处理来保证类内离散度的非奇异来避免, 也有学者针对LDA的小样本问题提出了最大间距准则 (maximum margin criterion, MMC) <citation id="165" type="reference"><link href="13" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>, 最大化异类样本离散度与同类样本离散度之差来避免小样本问题。</p>
                </div>
                <div class="p1">
                    <p id="38">近年来有研究表明人脸数据在原始空间中具有低维流形结构, 基于流形学习的算法利用数据的局部几何结构寻求能反映数据本质流形的低维表示, 已在人脸识别领域取得了显著的效果。按照低维表示与高维数据之间的映射类型可将降维算法分为非线性降维算法与线性降维算法。经典的非线性流形学习算法包括等距映射 (isometric feature mapping, ISOMAP) <citation id="166" type="reference"><link href="15" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>、局部线性嵌入 (locality linear embedding, LLE) <citation id="167" type="reference"><link href="17" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>、拉普拉斯特征映射 (laplacian eigenmaps, LE) <citation id="168" type="reference"><link href="19" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation> 等。这类非线性降维算法的高维数据与低维表示之间的映射关系是隐式的, 不能直接得出新数据的低维表示, 遭受外样本问题<citation id="169" type="reference"><link href="21" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>。有学者研究线性降维算法, 最小化近邻样本投影后的距离, 求解高维数据与低维数据之间的显示映射关系, 改进LLE, 提出局部判别投影 (locality preserving projection, LPP) <citation id="170" type="reference"><link href="23" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation> 来提升算法在新数据上的泛化能力。由于LPP未利用样本的类别信息, 影响了在分类任务中的性能, 有大量研究利用样本的类别信息改进LPP, 其中局部敏感判别分析 (locality sensitive discriminant analysis, LSDA) <citation id="171" type="reference"><link href="25" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation> 最小化同类近邻样本投影后的距离, 最大化异类近邻样本投影后的距离获得了较强的判别性能。颜水成等提出了边界Fisher分析 (marginal Fisher analysis, MFA) <citation id="172" type="reference"><link href="27" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation> 为基于图嵌入的流形学习算法提供了一个统一的框架。</p>
                </div>
                <div class="p1">
                    <p id="39">有研究表明<citation id="173" type="reference"><link href="29" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation> 样本的多样性信息也具有重要的作用, 可保留原始空间中数据的潜在模式, 提升算法的泛化能力。然而LSDA最大化异类近邻样本之间投影后的距离虽然增强了判别性能, 却破坏了数据的局部近邻结构, 不利于算法在新样本上的泛化能力。同时, 仅依赖数据局部结构的流形学习算法更倾向于发现数据的聚类信息, 样本的非局部信息同样重要<citation id="174" type="reference"><link href="31" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>。为了改进LSDA算法, 本文提出结合局部与非局部信息的相似性与多样性判别投影 (similarity and diversity discriminant projection, SDDP) , 利用同类样本表示样本之间的相似性, 增强同类样本投影后的相似性来增强判别性能。利用异类样本表示样本之间的多样性, 增强异类样本投影后的多样性来保证算法的泛化能力与判别性能。充分利用同类样本与异类样本的局部与非局部信息来提升算法的性能。</p>
                </div>
                <h3 id="40" name="40" class="anchor-tag"><b>1 相关算法</b></h3>
                <div class="p1">
                    <p id="41">给定<i>n</i>个样本的矩阵<b><i>X</i></b>=[<b><i>x</i></b><sub>1</sub>, <b><i>x</i></b><sub>2</sub>, …, <b><i>x</i></b><sub><i>i</i></sub>, …, <b><i>x</i></b><sub><i>n</i></sub>]∈<b><i>R</i></b><sup><i>d</i>×<i>n</i></sup>, 其中每一个样本<b><i>x</i></b><sub><i>i</i></sub>∈<b><i>R</i></b><sup><i>d</i></sup>为一个<i>d</i>维列向量。每个样本<i>x</i><sub><i>i</i></sub>所属的类别记作<i>l</i> (<b><i>x</i></b><sub><i>i</i></sub>) , 共有<i>c</i>类样本, 即<i>l</i> (<b><i>x</i></b><sub><i>i</i></sub>) ∈{1, 2, …, <i>c</i>}, 第<i>i</i>类样本的个数为<i>n</i><sub><i>i</i></sub>, 样本总个数为<mathml id="42"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>n</mi><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>c</mi></munderover><mi>n</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub></mrow></math></mathml>。定义投影矩阵<b><i>V</i></b>∈<b>R</b><sup>d×r</sup>, 经过线性变换<b><i>y</i></b><sub><i>i</i></sub>=<b><i>V</i></b><sup><i>T</i></sup><b><i>x</i></b><sub><i>i</i></sub>可将原始空间中的<i>d</i>维数据<i>x</i><sub><i>i</i></sub>转换为<i>r</i>维数据<b><i>y</i></b><sub><i>i</i></sub>∈<b><i>R</i></b><sup><i>r</i></sup>, 其中<i>r</i>≪<i>d</i>。</p>
                </div>
                <h4 class="anchor-tag" id="43" name="43"><b>1.1 最大间距准则</b></h4>
                <div class="p1">
                    <p id="44">MMC最大化投影后类间离散度与类内离散度之差, 使投影后的同类样本距离更近、异类样本距离更远, 从而使投影后的数据更适合分类任务。MMC的优化目标为:</p>
                </div>
                <div class="p1">
                    <p id="45">max<i>tr</i> (<b><i>V</i></b><sup>T</sup> (<b><i>S</i></b><sub><i>b</i></sub>-<b><i>S</i></b><sub><i>w</i></sub>) <b><i>V</i></b>) , <i>s</i>.<i>t</i>.<b><i>V</i></b><sup>T</sup><b><i>V</i></b>=<b><i>I</i></b> (1) </p>
                </div>
                <div class="p1">
                    <p id="46">式中:<b><i>S</i></b><sub><i>b</i></sub>为类间的离散度矩阵, 定义为:</p>
                </div>
                <div class="p1">
                    <p id="47"><mathml id="48"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">S</mi><msub><mrow></mrow><mi>b</mi></msub><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>c</mi></munderover><mrow><mfrac><mn>1</mn><mrow><mi>n</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></mfrac></mrow></mstyle><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">m</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mi mathvariant="bold-italic">m</mi><mo stretchy="false">) </mo><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">m</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mi mathvariant="bold-italic">m</mi><mo stretchy="false">) </mo><msup><mrow></mrow><mtext>Τ</mtext></msup></mrow></math></mathml>      (2) </p>
                </div>
                <div class="p1">
                    <p id="49">式中:<mathml id="50"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">m</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>n</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></munderover><mrow><mfrac><mn>1</mn><mrow><mi>n</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></mfrac></mrow></mstyle><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></math></mathml>, 表示第<i>i</i>类样本的均值, <mathml id="51"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">m</mi><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><mfrac><mn>1</mn><mi>n</mi></mfrac></mrow></mstyle><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></math></mathml>, 表示所有样本的均值。</p>
                </div>
                <div class="p1">
                    <p id="52">式 (1) 中, <b><i>S</i></b><sub><i>w</i></sub>为类内离散度矩阵, 定义为:</p>
                </div>
                <div class="p1">
                    <p id="53"><mathml id="54"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">S</mi><msub><mrow></mrow><mi>w</mi></msub><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>c</mi></munderover><mrow><mfrac><mn>1</mn><mrow><mi>n</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></mfrac></mrow></mstyle><mstyle displaystyle="true"><msub><mo>∑</mo><mrow><mi>l</mi><mi>x</mi><msub><mrow></mrow><mi>j</mi></msub><mo>=</mo><mi>i</mi></mrow></msub><mo stretchy="false"> (</mo></mstyle><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>j</mi></msub><mo>-</mo><mi mathvariant="bold-italic">m</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>j</mi></msub><mo>-</mo><mi mathvariant="bold-italic">m</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><msup><mrow></mrow><mtext>Τ</mtext></msup></mrow></math></mathml>      (3) </p>
                </div>
                <div class="p1">
                    <p id="55">MMC根据数据的全局几何结构对数据进行降维处理, 最大化矩阵差值来避免由于矩阵奇异带来的小样本问题。然而MMC未考虑样本的局部信息, 导致算法不能有效的发现数据的本质流形结构。</p>
                </div>
                <h4 class="anchor-tag" id="56" name="56"><b>1.2 局部敏感判别分析</b></h4>
                <div class="p1">
                    <p id="57">LSDA利用数据的局部近邻结构, 求距离每一个样本<i>x</i><sub><i>i</i></sub>欧氏距离最近的<i>k</i>个样本, 组成<b><i>x</i></b><sub><i>i</i></sub>的近邻样本集合<i>N</i><sub><i>k</i></sub> (<b><i>x</i></b><sub><i>i</i></sub>) ={<b><i>x</i></b><mathml id="58"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mn>1</mn></msubsup></mrow></math></mathml>, <b><i>x</i></b><mathml id="59"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mn>2</mn></msubsup></mrow></math></mathml>, …, <b><i>x</i></b><mathml id="60"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>k</mi></msubsup></mrow></math></mathml>}, 其中<b><i>x</i></b><mathml id="61"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>k</mi></msubsup></mrow></math></mathml>表示所有样本中距离<b><i>x</i></b><sub><i>i</i></sub>最近的第<i>k</i>个样本。LSDA最小化投影后同类近邻样本之间的距离:</p>
                </div>
                <div class="p1">
                    <p id="62">min∑<sub><i>ij</i></sub>‖<b><i>y</i></b><sub><i>i</i></sub>-<b><i>y</i></b><sub><i>j</i></sub>‖<sup>2</sup><b><i>W</i></b><sub><i>w</i></sub> (4) </p>
                </div>
                <div class="p1">
                    <p id="63">式中:<b><i>W</i></b><sub><i>w</i></sub>表示同类近邻样本之间的权重, 定义为:</p>
                </div>
                <div class="area_img" id="64">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/DZCL201914015_06400.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="66">最大化投影后异类近邻样本之间的距离:</p>
                </div>
                <div class="p1">
                    <p id="67">max∑<sub><i>ij</i></sub>‖<b><i>y</i></b><sub><i>i</i></sub>-<b><i>y</i></b><sub><i>j</i></sub>‖<sup>2</sup><b><i>W</i></b><sub><i>b</i></sub> (6) </p>
                </div>
                <div class="p1">
                    <p id="68">式中:<b><i>W</i></b><sub><i>b</i></sub>表示异类近邻样本之间的权重, 定义为:</p>
                </div>
                <div class="area_img" id="69">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/DZCL201914015_06900.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="71">LSDA算法利用数据的局部结构最小化同类近邻样本投影后的距离使原始空间中距离较近的同类样本在子空间中距离更近, 最大化异类近邻样本投影后的距离使原始空间中距离较近的异类样本在子空间中距离更远, 增强投影后数据的可分离性。然而LSDA仅利用数据的局部结构忽略了非局部结构的作用, 同时最大化异类近邻样本投影后的距离虽增强了判别信息破坏了数据的近邻结构, 损失了隐含的信息。</p>
                </div>
                <h3 id="72" name="72" class="anchor-tag"><b>2 相似性与多样性判别投影</b></h3>
                <h4 class="anchor-tag" id="73" name="73"><b>2.1 基本思想</b></h4>
                <div class="p1">
                    <p id="74">为了解决LSDA算法破坏了异类样本之间的近邻关系, 同时仅利用样本局部结构所带来的问题, 本文定义新的算法SDDP。根据分类任务中的目的, 使投影后的数据具有可分性, 同时尽可能的保留样本之间的多样性信息加强算法的泛化能力, 希望使同类样本投影后距离更近, 异类样本投影后距离更远。</p>
                </div>
                <h4 class="anchor-tag" id="75" name="75">1) 类内相似性</h4>
                <div class="p1">
                    <p id="76">与LSDA算法相似, SDDP最小化同类样本投影后的距离增加同类样本投影后的相似性:</p>
                </div>
                <div class="p1">
                    <p id="77">min∑<sub><i>ij</i></sub>‖<b><i>y</i></b><sub><i>i</i></sub>-<b><i>y</i></b><sub><i>j</i></sub>‖<sup>2</sup><b><i>S</i></b><sub><i>ij</i></sub> (8) </p>
                </div>
                <div class="p1">
                    <p id="78">定义同类样本之间的相似性权重<i>S</i>如 (9) 所示:</p>
                </div>
                <div class="area_img" id="79">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/DZCL201914015_07900.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="81">式中:<i>N</i><sub><i>k</i></sub> (<b><i>x</i></b><sub><i>i</i></sub>) 表示<b><i>x</i></b><sub><i>i</i></sub>的<i>k</i>个最近邻样本的集合, 和1.2中LSDA算法的定义一样。<mathml id="82"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Κ</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mtext>e</mtext><msup><mrow></mrow><mrow><mo>-</mo><mfrac><mrow><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">∥</mo><msup><mrow></mrow><mn>2</mn></msup></mrow><mi>t</mi></mfrac></mrow></msup></mrow></math></mathml>为关于<b><i>x</i></b><sub><i>i</i></sub>与<b><i>x</i></b><sub><i>j</i></sub>的函数, <i>K</i> (<b><i>x</i></b><sub><i>i</i></sub>, <b><i>x</i></b><sub><i>j</i></sub>) 随‖<b><i>x</i></b><sub><i>i</i></sub>-<b><i>x</i></b><sub><i>j</i></sub>‖<sup>2</sup>减小而增大, 表明在原始空间中样本之间距离越小, 相似性权值越大。</p>
                </div>
                <div class="p1">
                    <p id="83">同类近邻样本的相似性权重在<i>K</i> (<b><i>x</i></b><sub><i>i</i></sub>, <b><i>x</i></b><sub><i>j</i></sub>) 上乘以一项 (1+<i>K</i> (<b><i>x</i></b><sub><i>i</i></sub>, <b><i>x</i></b><sub><i>j</i></sub>) ) , 同类非近邻样本的相似性权重在<i>K</i> (<b><i>x</i></b><sub><i>i</i></sub>, <b><i>x</i></b><sub><i>j</i></sub>) 上乘以 (1-<i>K</i> (<b><i>x</i></b><sub><i>i</i></sub>, <b><i>x</i></b><sub><i>j</i></sub>) ) 。</p>
                </div>
                <div class="p1">
                    <p id="84">性质1:0≤<i>K</i> (<b><i>x</i></b><sub><i>i</i></sub>, <b><i>x</i></b><sub><i>j</i></sub>) ≤1, 1≤1+<i>K</i> (<b><i>x</i></b><sub><i>i</i></sub>, <b><i>x</i></b><sub><i>j</i></sub>) ≤2, 0≤1-<i>K</i> (<b><i>x</i></b><sub><i>i</i></sub>, <b><i>x</i></b><sub><i>j</i></sub>) ≤1。</p>
                </div>
                <div class="p1">
                    <p id="85">由性质1可知, 同类近邻样本的相似性权重始终大于同类非近邻样本的相似性权重, 与实际的数据分布特点一致, 近邻样本之间比非近邻样本之间有更高的相似性。在表示样本相似性信息时, 同时利用局部与非局部信息, 既维持数据的局部流行结构, 同时使用样本类别信息这一先验知识增加同类非近邻样本投影后的密集程度。</p>
                </div>
                <h4 class="anchor-tag" id="86" name="86">2) 类间多样性</h4>
                <div class="p1">
                    <p id="87">为了使投影后异类样本之间距离更远, 同时保证异类样本之间的近邻关系, 在原始空间中距离较远的仍保持较远的距离来增加样本之间的多样性信息, 最大化异类样本投影后的多样性:</p>
                </div>
                <div class="p1">
                    <p id="88">max∑<sub><i>ij</i></sub>‖<b><i>y</i></b><sub><i>i</i></sub>-<b><i>y</i></b><sub><i>j</i></sub>‖<sup>2</sup><b><i>B</i></b><sub><i>ij</i></sub> (10) </p>
                </div>
                <div class="p1">
                    <p id="89">定义异类样本之间的多样性权重<b><i>B</i></b>如式 (11) :</p>
                </div>
                <div class="area_img" id="90">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/DZCL201914015_09000.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="92">式 (11) 中<mathml id="93"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msup><mi>Κ</mi><mo>′</mo></msup><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mtext>e</mtext><msup><mrow></mrow><mrow><mo>-</mo><mfrac><mi>t</mi><mrow><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">∥</mo><msup><mrow></mrow><mn>2</mn></msup></mrow></mfrac></mrow></msup></mrow></math></mathml>, 为随‖<b><i>x</i></b><sub><i>i</i></sub>-<b><i>x</i></b><sub><i>j</i></sub>‖<sup>2</sup>增大而增大的函数, 表明在原始空间中, 样本之间的距离越远, 多样性权值越大。考虑到异类非近邻样本之间比异类近邻样本之间具有更大的多样性, 如 (11) 中第二项所示在<i>K</i>′ (<b><i>x</i></b><sub><i>i</i></sub>, <b><i>x</i></b><sub><i>j</i></sub>) 上乘以 (1+<i>K</i>′ (<b><i>x</i></b><sub><i>i</i></sub>, <b><i>x</i></b><sub><i>j</i></sub>) ) 来表示异类非近邻样本之间的多样性权重, 如 (11) 中第一项所示在<i>K</i>′ (<b><i>x</i></b><sub><i>i</i></sub>, <b><i>x</i></b><sub><i>j</i></sub>) 上乘以 (1-<i>K</i>′ (<b><i>x</i></b><sub><i>i</i></sub>, <b><i>x</i></b><sub><i>j</i></sub>) ) 来表示异类近邻样本之间的多样性。</p>
                </div>
                <div class="p1">
                    <p id="94">性质2:0≤<i>K</i>′ (<b><i>x</i></b><sub><i>i</i></sub>, <b><i>x</i></b><sub><i>j</i></sub>) ≤1, 0≤1-<i>K</i>′ (<b><i>x</i></b><sub><i>i</i></sub>, <b><i>x</i></b><sub><i>j</i></sub>) ≤1, 1≤1+<i>K</i>′ (<b><i>x</i></b><sub><i>i</i></sub>, <b><i>x</i></b><sub><i>j</i></sub>) ≤2。</p>
                </div>
                <div class="p1">
                    <p id="95">由性质2可知, 异类非近邻样本之间的多样性权重始终大于异类近邻样本之间的多样性权重, 这与实际的数据分布情况一致, 非近邻样本之间比近邻样本之间有更高的可分离性, 在本文中称为多样性。式 (10) 最大化异类样本之间的多样性, 同时考虑局部与非局部信息, 来保证投影后异类样本之间维持在原始空间中的关系。原始空间中距离较近的异类样本之间多样性权重小。距离较远的异类样本之间多样性权重较大, 最大化投影后样本之间的多样性保持在原始空间中距离较远的异类样本在子空间中仍保持较远的距离。</p>
                </div>
                <h4 class="anchor-tag" id="96" name="96">3) 算法目标与求解</h4>
                <div class="p1">
                    <p id="97">最后, 需要将目标 (8) 与目标 (10) 集成在一起方便算法的求解, 根据<b><i>y</i></b><sub><i>i</i></sub>=<b><i>V</i></b><sup>T</sup><b><i>x</i></b><sub><i>i</i></sub>, 对目标 (8) 与目标 (10) 进行线性变换分别得到目标 (12) 与 (13) :</p>
                </div>
                <div class="p1">
                    <p id="98">min∑<sub><i>ij</i></sub>‖<b><i>y</i></b><sub><i>i</i></sub>-<b><i>y</i></b><sub><i>j</i></sub>‖<sup>2</sup><b><i>S</i></b><sub><i>ij</i></sub>=min∑<sub><i>ij</i></sub> (<b><i>V</i></b><sup>T</sup><b><i>x</i></b><sub><i>i</i></sub>-<b><i>V</i></b><sup>T</sup><b><i>x</i></b><sub><i>j</i></sub>) <sup>T</sup> (<b><i>V</i></b><sup>T</sup><b><i>x</i></b><sub><i>i</i></sub>-<b><i>V</i></b><sup>T</sup><b><i>x</i></b><sub><i>j</i></sub>) <b><i>S</i></b><sub><i>ij</i></sub>=2min<i>tr</i> (∑<sub><i>i</i></sub><b><i>V</i></b><sup>T</sup><b><i>x</i></b><sub><i>i</i></sub><b><i>D</i></b><mathml id="99"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mi>i</mi></mrow><mi>S</mi></msubsup></mrow></math></mathml><b><i>x</i></b><sup>T</sup><sub><i>i</i></sub><b><i>V</i></b>-∑<sub><i>ij</i></sub><b><i>V</i></b><sup>T</sup><b><i>x</i></b><sub><i>i</i></sub><b><i>S</i></b><sub><i>ij</i></sub><b><i>x</i></b><sup>T</sup><sub><i>j</i></sub><b><i>V</i></b>) =2min<i>tr</i> (<b><i>V</i></b><sup>T</sup><b><i>X</i></b> (<b><i>D</i></b><sup><i>S</i></sup>-<b><i>S</i></b>) <b><i>X</i></b><sup>T</sup><b><i>V</i></b>) (12) </p>
                </div>
                <div class="p1">
                    <p id="100">式 (12) 中, <b><i>D</i></b><sup><i>S</i></sup>为对角矩阵, 主对角线上的元素<b><i>D</i></b><mathml id="101"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mi>i</mi></mrow><mi>S</mi></msubsup></mrow></math></mathml>=∑<mathml id="102"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup></mrow></math></mathml><b><i>S</i></b><sub><i>ij</i></sub>或<b><i>D</i></b><mathml id="103"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mi>i</mi></mrow><mi>S</mi></msubsup></mrow></math></mathml>=∑<mathml id="104"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup></mrow></math></mathml><b><i>S</i></b><sub><i>ji</i></sub>, 因为<b><i>S</i></b>为对称矩阵, <i>tr</i>表示矩阵的迹。</p>
                </div>
                <div class="p1">
                    <p id="105">max∑<sub><i>ij</i></sub>‖<b><i>y</i></b><sub><i>i</i></sub>-<b><i>y</i></b><sub><i>j</i></sub>‖<sup>2</sup><b><i>B</i></b><sub><i>ij</i></sub>=max∑<sub><i>ij</i></sub> (<b><i>V</i></b><sup>T</sup><b><i>x</i></b><sub><i>i</i></sub>-<b><i>V</i></b><sup>T</sup><b><i>x</i></b><sub><i>j</i></sub>) <sup>T</sup> (<b><i>V</i></b><sup>T</sup><b><i>x</i></b><sub><i>i</i></sub>-<b><i>V</i></b><sup>T</sup><b><i>x</i></b><sub><i>j</i></sub>) <b><i>B</i></b><sub><i>ij</i></sub>=2max<i>tr</i> (∑<sub><i>i</i></sub><b><i>V</i></b><sup>T</sup><b><i>x</i></b><sub><i>i</i></sub><b><i>D</i></b><mathml id="106"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mi>i</mi></mrow><mi>B</mi></msubsup></mrow></math></mathml><b><i>x</i></b><sup>T</sup><sub><i>i</i></sub><b><i>V</i></b>-∑<sub><i>ij</i></sub><b><i>V</i></b><sup>T</sup><b><i>x</i></b><sub><i>i</i></sub><b><i>B</i></b><sub><i>ij</i></sub><b><i>x</i></b><sup>T</sup><sub><i>j</i></sub><b><i>V</i></b>) =2max<i>tr</i> (<b><i>V</i></b><sup>T</sup><b><i>X</i> (<i>D</i></b><sup><i>B</i></sup>-<b><i>B</i></b>) <b><i>X</i></b><sup>T</sup><b><i>V</i></b>) =2max<i>tr</i> (<b><i>V</i></b><sup>T</sup><b><i>XL</i></b><sup><i>B</i></sup><b><i>X</i></b><sup>T</sup><b><i>V</i></b>) (13) </p>
                </div>
                <div class="p1">
                    <p id="107">式 (13) 中<b><i>L</i></b><sup><i>B</i></sup>=<b><i>D</i></b><sup><i>B</i></sup>-<b><i>B</i></b>为拉普拉斯矩阵, 其中<b><i>D</i></b><sup><i>B</i></sup>为对角矩阵, 主对角线上的元素为<mathml id="108"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">D</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mi>i</mi></mrow><mi>B</mi></msubsup><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi mathvariant="bold-italic">B</mi></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow></math></mathml>或<mathml id="109"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">D</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mi>i</mi></mrow><mi>B</mi></msubsup><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi mathvariant="bold-italic">B</mi></mstyle><msub><mrow></mrow><mrow><mi>j</mi><mi>i</mi></mrow></msub></mrow></math></mathml>, 因为<b><i>B</i></b>为对称矩阵。</p>
                </div>
                <div class="p1">
                    <p id="110">根据文献<citation id="175" type="reference">[<a class="sup">12</a>]</citation>, <b><i>D</i></b><mathml id="111"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mi>i</mi></mrow><mi>S</mi></msubsup></mrow></math></mathml>表示的是<b><i>x</i></b><sub><i>i</i></sub>的重要性, 引入约束条件 (14) :</p>
                </div>
                <div class="p1">
                    <p id="112"><b><i>V</i></b><sup>T</sup><b><i>XD</i></b><sup><i>S</i></sup><b><i>X</i></b><sup>T</sup><b><i>V</i></b>=<b><i>I</i></b> (14) </p>
                </div>
                <div class="p1">
                    <p id="113">将约束条件代入式 (12) 可得:</p>
                </div>
                <div class="p1">
                    <p id="114">min<i>tr</i> (<b><i>V</i></b><sup>T</sup><b><i>X</i></b> (<b><i>D</i></b><sup><i>S</i></sup>-<b><i>S</i></b>) <b><i>X</i></b><sup>T</sup><b><i>V</i></b>) =min<i>tr</i> (<b><i>I</i></b>-<b><i>V</i></b><sup>T</sup><b><i>XSX</i></b><sup>T</sup><b><i>V</i></b>) ⇒</p>
                </div>
                <div class="p1">
                    <p id="115">max<i>tr</i> (<b><i>V</i></b><sup>T</sup><b><i>XSX</i></b><sup>T</sup><b><i>V</i></b>) (15) </p>
                </div>
                <div class="p1">
                    <p id="116">利用最大间距准则将目标 (15) 与目标 (13) 集成在一起得出SDDP的优化目标:</p>
                </div>
                <div class="p1">
                    <p id="117">max<i>tr</i> (<b><i>V</i></b><sup>T</sup><b><i>X</i></b> (<i>μ</i><b><i>L</i></b><sup><i>B</i></sup>+ (1-<i>μ</i>) <b><i>S</i></b>) <b><i>X</i></b><sup>T</sup><b><i>V</i></b>) , <i>s</i>.<i>t</i>.<b><i>V</i></b><sup>T</sup><b><i>XD</i></b><sup><i>S</i></sup><b><i>X</i></b><sup>T</sup><b><i>V</i></b>=<b><i>I</i></b> (16) </p>
                </div>
                <div class="p1">
                    <p id="118">使用拉格朗日乘数法求解优化目标 (16) :</p>
                </div>
                <div class="p1">
                    <p id="119"><mathml id="120"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mo>∂</mo><mi mathvariant="bold-italic">V</mi></mfrac><mo stretchy="false">{</mo><mi mathvariant="bold-italic">V</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">X</mi><mo stretchy="false"> (</mo><mi>μ</mi><mi mathvariant="bold-italic">L</mi><msup><mrow></mrow><mi>B</mi></msup><mo>+</mo><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>μ</mi><mo stretchy="false">) </mo><mi mathvariant="bold-italic">S</mi><mo stretchy="false">) </mo><mi mathvariant="bold-italic">X</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">V</mi><mo>-</mo><mi>λ</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">V</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">X</mi><mi mathvariant="bold-italic">D</mi><msup><mrow></mrow><mi>S</mi></msup><mi mathvariant="bold-italic">X</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">V</mi><mo>-</mo><mi mathvariant="bold-italic">Ι</mi><mo stretchy="false">) </mo><mo stretchy="false">}</mo><mo>=</mo><mn>0</mn><mo>⇒</mo><mi mathvariant="bold-italic">X</mi><mo stretchy="false"> (</mo><mi>μ</mi><mi mathvariant="bold-italic">L</mi><msup><mrow></mrow><mi>B</mi></msup><mo>+</mo><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>μ</mi><mo stretchy="false">) </mo><mi mathvariant="bold-italic">S</mi><mo stretchy="false">) </mo><mi mathvariant="bold-italic">X</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">V</mi><mo>=</mo><mi>λ</mi><mi mathvariant="bold-italic">X</mi><mi mathvariant="bold-italic">D</mi><msup><mrow></mrow><mi>S</mi></msup><mi mathvariant="bold-italic">X</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">V</mi></mrow></math></mathml>      (17) </p>
                </div>
                <div class="p1">
                    <p id="121">按照式 (17) , 求优化目标 (16) 的问题转为矩阵的特征分解问题, 对 (<b><i>XD</i></b><sup><i>S</i></sup><b><i>X</i></b><sup>T</sup>) <sup>-1</sup><b><i>X</i></b> (<i>μ</i><b><i>L</i></b><sup><i>B</i></sup>+ (1-<i>μ</i>) <b><i>S</i></b>) <b><i>X</i></b><sup>T</sup>进行特征分解, 最大的特征值<i>λ</i><sub>1</sub>&gt;<i>λ</i><sub>2</sub>&gt;…&gt;<i>λ</i><sub><i>r</i></sub>对应的特征向量组成的矩阵<b><i>V</i></b>=[<b><i>v</i></b><sub>1</sub>, <b><i>v</i></b><sub>2</sub>, …, <b><i>v</i></b><sub><i>r</i></sub>]∈<b><i>R</i></b><sup><i>d</i>×<i>r</i></sup>即为所求的投影矩阵。</p>
                </div>
                <h4 class="anchor-tag" id="122" name="122"><b>2.2 算法描述</b></h4>
                <div class="p1">
                    <p id="123">SDDP算法求解投影矩阵的流程如图1所示。</p>
                </div>
                <div class="area_img" id="124">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/DZCL201914015_124.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 SDDP算法流程" src="Detail/GetImg?filename=images/DZCL201914015_124.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 SDDP算法流程  <a class="btn-zoomin" href="Detail/GetImg?filename=images/DZCL201914015_124.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="125">SDDP算法的步骤如下:</p>
                </div>
                <div class="p1">
                    <p id="126">输入:训练集<b><i>X</i></b>=[<b><i>x</i></b><sub>1</sub>, <b><i>x</i></b><sub>2</sub>, …, <b><i>x</i></b><sub><i>i</i></sub>, …<b><i>x</i></b><sub><i>m</i></sub>]∈<b><i>R</i></b><sup><i>d</i>×<i>m</i></sup>, 对应的样本类别<i>l</i> (<b><i>x</i></b><sub><i>i</i></sub>) ∈{1, 2, …, <i>c</i>}。</p>
                </div>
                <div class="p1">
                    <p id="127">输出:投影矩阵<b><i>V</i></b><sup><i>d</i>×<i>r</i></sup>, 将任意一个样本<b><i>x</i></b><sub><i>i</i></sub>∈<b><i>R</i></b><sup><i>d</i>×1</sup>经过线性变换转为<b><i>y</i></b><sub><i>i</i></sub>=<b><i>V</i></b><sup>T</sup><b><i>x</i></b><sub><i>i</i></sub>, <b><i>y</i></b><sub><i>i</i></sub>∈<b><i>R</i></b><sup><i>r</i>×1</sup>并且<i>r</i>≪<i>d</i>。</p>
                </div>
                <div class="p1">
                    <p id="128">步骤1:计算每一个样本<b><i>x</i></b><sub><i>i</i></sub>与其他所有样本的欧氏距离, 选出距离<b><i>x</i></b><sub><i>i</i></sub>最近的<i>k</i>个样本, 组成<b><i>x</i></b><sub><i>i</i></sub>的近邻样本集合<i>N</i><sub><i>k</i></sub> (<b><i>x</i></b><sub><i>i</i></sub>) ;</p>
                </div>
                <div class="p1">
                    <p id="129">步骤2:按照公式 (9) 与公式 (11) 计算样本之间的相似性权重矩阵<b><i>S</i></b>与多样性权重矩阵<b><i>B</i></b>;</p>
                </div>
                <div class="p1">
                    <p id="130">步骤3:根据<b><i>D</i></b><mathml id="131"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mi>i</mi></mrow><mi>S</mi></msubsup></mrow></math></mathml>=∑<mathml id="132"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></msubsup></mrow></math></mathml><b><i>S</i></b><sub><i>ij</i></sub>或者<b><i>D</i></b><mathml id="133"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mi>i</mi></mrow><mi>S</mi></msubsup></mrow></math></mathml>=∑<mathml id="134"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></msubsup></mrow></math></mathml><b><i>S</i></b><sub><i>ji</i></sub>计算对角矩阵<b><i>D</i></b><sup><i>S</i></sup>, 根据<b><i>D</i></b><mathml id="135"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mi>i</mi></mrow><mi>B</mi></msubsup></mrow></math></mathml>=∑<mathml id="136"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></msubsup></mrow></math></mathml><b><i>B</i></b><sub><i>ij</i></sub>或者<b><i>D</i></b><mathml id="137"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mi>i</mi></mrow><mi>B</mi></msubsup></mrow></math></mathml>=∑<mathml id="138"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></msubsup></mrow></math></mathml><b><i>B</i></b><sub><i>ji</i></sub>计算对角矩阵<b><i>D</i></b><sup><i>B</i></sup>, 计算拉普拉斯矩阵<b><i>L</i></b><sup><i>B</i></sup>=<b><i>D</i></b><sup><i>B</i></sup>-<b><i>B</i></b>;</p>
                </div>
                <div class="p1">
                    <p id="139">步骤4:对 (<b><i>XD</i></b><sup><i>S</i></sup><b><i>X</i></b><sup>T</sup>) <sup>-1</sup><b><i>X</i></b> (<b><i>L</i></b><sup><i>B</i></sup>-<i>μ</i><b><i>S</i></b>) <b><i>X</i></b><sup>T</sup>进行特征分解, 投影矩阵为最大特征值<i>λ</i><sub>1</sub>&gt;<i>λ</i><sub>2</sub>&gt;…&gt;<i>λ</i><sub><i>r</i></sub>对应的特征向量组成的矩阵<b><i>V</i></b>=[<b><i>v</i></b><sub>1</sub>, <b><i>v</i></b><sub>2</sub>, …, <b><i>v</i></b><sub><i>r</i></sub>]∈<b><i>R</i></b><sup><i>d</i>×<i>r</i></sup>。</p>
                </div>
                <h3 id="140" name="140" class="anchor-tag"><b>3 实  验</b></h3>
                <div class="p1">
                    <p id="141">为了验证SDDP算法的有效性, 在Yale与ORL人脸数据集上进行试验, 与使用样本全局几何结构的PCA、MMC算法、仅使用样本局部结构的流形学习算法LSDA、MFA进行对比。实验中先用PCA算法对数据进行预处理以去除冗余信息, 同时避免MFA的小样本问题。虽然SDDP算法不受小样本问题影响, 为了公平对比, 同样采用PCA预处理操作。实验中按照每类样本取<i>p</i>=4, 5, 6个样本作为训练集, 剩余样本为测试集的方式随机划分训练集与测试集。先在训练集上求投影矩阵<b><i>V</i></b>, 再对所有样本进行降维处理, 用最近邻分类器对降维后的数据进行分类, 得出准确率。对于固定的<i>p</i>值, 采用10次随机划分上准确率的均值作为算法最终的准确率。</p>
                </div>
                <h4 class="anchor-tag" id="142" name="142"><b>3.1 Yale数据集上的实验</b></h4>
                <div class="p1">
                    <p id="143">Yale数据集包含15个人, 每个人11幅图像, 包括光照、表情、是否戴眼镜等变化。对Yale数据集中的图像进行裁剪和缩放得到尺寸为32×32像素的灰度图像, 部分图像如图2所示。</p>
                </div>
                <div class="area_img" id="144">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/DZCL201914015_144.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 Yale数据集上一个人的11幅图像" src="Detail/GetImg?filename=images/DZCL201914015_144.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 Yale数据集上一个人的11幅图像  <a class="btn-zoomin" href="Detail/GetImg?filename=images/DZCL201914015_144.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="145">在使用SDDP算法之前, 先用PCA对数据进行预处理以减少冗余信息。PCA预处理的能量比设为0.86。SDDP算法涉及到超参数选择的问题, 需要人为设置近邻参数<i>k</i>, 权重函数的参数<i>t</i>, 以及优化目标 (16) 中调节相似性与多样性在优化中比重的超参数<i>μ</i>。本文经过详细的超参数搜索工作确定一组能够使SDDP算法取得较好结果的值, 在Yale数据集上的实验, 设置近邻参数<i>k</i>=18, 权重函数参数<i>t</i>设置为文献<citation id="176" type="reference"><link href="33" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>代码中的值, 超参数<i>μ</i>=0.03。</p>
                </div>
                <div class="p1">
                    <p id="146">在Yale数据集上PCA、MMC、MFA、LSDA、SDDP算法在<i>p</i>=4, 5, 6时的准确率与降维维数的曲线如图3所示。由图3可知, PCA在<i>p</i>=4, 5, 6时的准确率总是最低的, 这是因为PCA的无监督特性导致的在分类任务中效果较差。其中MMC、MFA、LSDA算法在<i>p</i>=4, 5, 6时取得的最高准确率相近, <i>p</i>=4时基于样本全局结构的MMC算法的最高准确率高于基于样本局部结构的MFA、LSDA算法, 随着训练集的增大, <i>p</i>=6时, 基于局部结构的流形学习算法MFA取得了更高的准确率, 表明了样本的局部结构与全局结构同样重要。表1为PCA、MMC、MFA、LSDA、SDDP算法在Yale数据集上的最高准确率与对应的维数, 括号内为取得最高准确率时的降维维数。由表1可知, <i>p</i>=4, 5, 6时SDDP算法的最高准确率始终高于MMC、MFA、LSDA算法, 这表明了SDDP算法考虑同类样本之间的相似性, 异类样本之间的多样性, 同时兼顾局部与非局部信息对算法在分类任务中效果的提升。</p>
                </div>
                <div class="area_img" id="148">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/DZCL201914015_14800.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 Yale数据集上的准确率曲线" src="Detail/GetImg?filename=images/DZCL201914015_14800.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 Yale数据集上的准确率曲线  <a class="btn-zoomin" href="Detail/GetImg?filename=images/DZCL201914015_14800.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="148">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/DZCL201914015_14801.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 Yale数据集上的准确率曲线" src="Detail/GetImg?filename=images/DZCL201914015_14801.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 Yale数据集上的准确率曲线  <a class="btn-zoomin" href="Detail/GetImg?filename=images/DZCL201914015_14801.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="149">
                    <p class="img_tit"><b>表1 Yale数据集上的最高准确率 (%) 及对应的维数</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="149" border="1"><tr><td><br /></td><td><i>p</i>=4</td><td><i>p</i>=5</td><td><i>p</i>=6</td></tr><tr><td><br />PCA</td><td>54.76 (29) </td><td>55.78 (16) </td><td>62.27 (25) </td></tr><tr><td><br />MMC</td><td>64.85 (12) </td><td>68.22 (15) </td><td>72.80 (14) </td></tr><tr><td><br />MFA</td><td>62.57 (14) </td><td>66.56 (11) </td><td>75.60 (14) </td></tr><tr><td><br />LSDA</td><td>63.14 (17) </td><td>66.11 (25) </td><td>71.87 (17) </td></tr><tr><td><br />SDDP</td><td>69.24 (15) </td><td>73.00 (15) </td><td>77.47 (16) </td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="150" name="150"><b>3.2 ORL数据集上的实验</b></h4>
                <div class="p1">
                    <p id="151">ORL数据集包括40个, 每人10幅图像, 包括面部表情、光照方向、面部朝向的方向、睁眼或者闭眼、是否戴眼镜等多种变化。对ORL数据集的图像进行裁剪和缩放得到尺寸为32×32像素的灰度图像, 部分图像如图4所示。</p>
                </div>
                <div class="area_img" id="152">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/DZCL201914015_152.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 ORL数据集中一个人的10幅图像" src="Detail/GetImg?filename=images/DZCL201914015_152.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 ORL数据集中一个人的10幅图像  <a class="btn-zoomin" href="Detail/GetImg?filename=images/DZCL201914015_152.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="153">使用SDDP算法之前, 先用PCA进行预处理, 能量比设为0.86。经过详细的超参数搜索, 将SDDP的近邻参数<i>k</i>设置为20, 调节相似性与多样性比重的超参数<i>μ</i>设置为0.000 6。</p>
                </div>
                <div class="p1">
                    <p id="154">ORL数据集上的PCA、MMC、MFA、LSDA、SDDP算法在<i>p</i>=4, 5, 6时对应的准确率与降维维数的曲线如图5所示, 各个算法取得的最高准确率及降维维数如表2所示, 括号内为取得最高准确率时对应的降维维数。</p>
                </div>
                <div class="area_img" id="155">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/DZCL201914015_155.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 ORL数据集上的准确率曲线" src="Detail/GetImg?filename=images/DZCL201914015_155.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 ORL数据集上的准确率曲线  <a class="btn-zoomin" href="Detail/GetImg?filename=images/DZCL201914015_155.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="156">
                    <p class="img_tit"><b>表2 ORL数据集上的最高准确率 (%) 及对应的维数</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="156" border="1"><tr><td><br /></td><td><i>p</i>=4</td><td><i>p</i>=5</td><td><i>p</i>=6</td></tr><tr><td><br />PCA</td><td>83.08 (45) </td><td>85.90 (31) </td><td>89.75 (33) </td></tr><tr><td><br />MMC</td><td>88.96 (32) </td><td>91.95 (32) </td><td>94.75 (43) </td></tr><tr><td><br />MFA</td><td>92.25 (19) </td><td>95.25 (18) </td><td>96.44 (20) </td></tr><tr><td><br />LSDA</td><td>86.25 (39) </td><td>91.00 (36) </td><td>93.88 (29) </td></tr><tr><td><br />SDDP</td><td>92.37 (32) </td><td>95.50 (34) </td><td>96.88 (33) </td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="157">由图5可知, 在<i>p</i>=4, 5, 6时无监督的PCA算法总是取得最低的准确率, 这表明在分类任务中样本的类别信息具有重要的作用。有监督的MMC、LSDA、MFA、SDDP算法对应的准确率曲线相近, 这与ORL数据集本身的特点有关。ORL数据集的样本数目远大于Yale数据集, 由于样本数目增加可提升算法的性能, 尤其是基于样本局部结构的流形学习算法在样本数目较多时能发挥更好的性能。根据表2可知, 在<i>p</i>=4, 5, 6时, MMC、LSDA算法的最高准确率相近, MFA、SDDP算法的最高准确率明显高于MMC与LSDA, 这与MFA、SDDP算法采用与样本距离相关的权重有关, 能够更精确的反映样本之间的关系。SDDP算法的类内相似性权重与类间多样性权重更加灵活, 既考虑近邻样本又考虑非近邻样本之间的距离, 更能增强算法的泛化能力, 在ORL数据集上取得了最高的准确率, 表明SDDP算法改进的有效性。</p>
                </div>
                <h3 id="158" name="158" class="anchor-tag"><b>4 结  论</b></h3>
                <div class="p1">
                    <p id="159">本文提出了结合局部与非局部结构的相似性与多样性判别投影SDDP算法, 有效的利用了样本的类别信息, 同时维持数据的流形结构, 利用非局部信息来增强算法的判别能力、提升算法的泛化能力。在Yale数据集与ORL数据集上的实验及分析表明了SDDP算法在多种情况下能够取得较好的分类效果, 适合分类任务。</p>
                </div>
                <div class="p1">
                    <p id="160">虽然SDDP算法获得了良好的改进效果, 但在实际实验中发现SDDP算法需要经过详细搜索设置超参数, 这导致算法在实际应用中的困难。如何自适应的确定超参数的数值是未来需要进一步研究的问题。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201705035&amp;v=MTA3MzN5dmhWYnZPTHo3QmQ3RzRIOWJNcW85R1lZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVadEY=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> 甘炎灵, 金聪.间距判别投影及其在表情识别中的应用[J].计算机应用, 2017, 37 (5) :1413-1418.
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZCL201712039&amp;v=MjE4MzF0Rnl2aFZidk9JVGZJWXJHNEg5Yk5yWTlHYllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVo=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> 张仕霞, 金聪.基于 Lp 范数的二维最大间距准则及其在人脸识别中的应用[J].电子测量技术, 2017, 40 (12) :196-202.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Statistical pattern recognition: a review">

                                <b>[3]</b> JAIN A K, DUIN R P W, MAO J.Statistical pattern recognition:A review[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2000, 22 (1) :4-37.
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Eigenfaces vs. Fisherfaces: Recognition using class specific linear projection">

                                <b>[4]</b> BELHUMEUR P N, HESPANHA J P, KRIEGMAN D J.Eigenfaces vs.Fisherfaces:Recognition using class specific linear projection[C].European Conference on Computer Vision.Springer, Berlin, Heidelberg, 1996:43-58.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011600741248&amp;v=MTM5NDViSzdIdEROcVk5RlkrOE9Ebmd4b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcnpJSTFvU2JoUT1OaWZPZg==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b> CHEN L F, LIAO H Y M, KO M T, et al.A new LDA-based face recognition system which can solve the small sample size problem[J].Pattern Recognition, 2000, 33 (10) :1713-1726.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Efficient and robust feature extraction by maximum margin criterion">

                                <b>[6]</b> LI H, JIANG T, ZHANG K.Efficient and robust feature extraction by maximum margin criterion[C].Advances in Neural Information Processing Systems, 2004:97-104.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The isomap algorithm and topological stability">

                                <b>[7]</b> BALASUBRAMANIAN M, SCHWARTZ E L.The isomap algorithm and topological stability[J].Science, 2002, 295 (5552) :7-7.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" >
                                    <b>[8]</b>
                                 ROWEIS S T, SAUL L K.Nonlinear dimensionality reduction by locally linear embedding[J].Science, 2000, 290 (5500) :2323-2326.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJBK&amp;filename=SJBK15090500012194&amp;v=MjA4ODRIdGpNcW85RlpPb05EWFU5b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcnpJSTFvU2JoUT1OaWZKWmJLOQ==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b> BELKIN M, NIYOGI P.Laplacian eigenmaps for dimensionality reduction and data representation[J].Neural Computation, 2003, 15 (6) :1373-1396.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Out-of-Sample Extensions for LLE, Isomap, MDS, Eigenmaps and Spectral Clustering">

                                <b>[10]</b> BENGIO Y, PAIEMENT J, VINCENT P, et al.Out-of-sample extensions for lle, isomap, mds, eigenmaps, and spectral clustering[C].Advances in Neural Information Processing Systems, 2004:177-184.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Face recognition using laplacianfaces">

                                <b>[11]</b> HE X, YAN S, HU Y, et al.Face recognition using laplacianfaces[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2005, 27 (3) :328-340.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Locality sensitive discriminant analysis">

                                <b>[12]</b> CAI D, HE X, ZHOU K, et al.Locality sensitive discriminant analysis[C].Proceedings of the 20th International Joint Conference on Artifical Intelligence, 2007:708-713.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Graph embedding and extensions:a general framework for dimensionality reduction">

                                <b>[13]</b> YAN S, XU D, ZHANG B, et al.Graph embedding and extensions:A general framework for dimensionality reduction[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2007, 29 (1) :40-51.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_14" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14032300094688&amp;v=MTkxNjVyeklJMW9TYmhRPU5pZk9mYks4SHRMT3JJOUZaT0lMQ25ReG9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaW5sVQ==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[14]</b> GAO Q, LIU J, CUI K, et al.Stable locality sensitive discriminant analysis for image recognition[J].Neural Networks, 2014, 54:49-56.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Globally Maximizing, Locally Minimizing: Unsupervised Discriminant Projection with Applications to Face and Palm Biometrics">

                                <b>[15]</b> YANG J, ZHANG D, YANG J, et al.Globally maximizing, locally minimizing:unsupervised discriminant projection with applications to face and palm biometrics[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2007, 29 (4) :650-664.
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Local discriminant embedding and its variants">

                                <b>[16]</b> CHEN H T, CHANG H W, LIU T L.Local discriminant embedding and its variants[C].IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2005.CVPR 2005.IEEE, 2005 (2) :846-853.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="DZCL201914015" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZCL201914015&amp;v=MTI4NTZZWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnRGeXZoVmJ2T0lUZklZckc0SDlqTnE0OUU=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekd3QkxsbWVobFYrZnFaTmhSdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>


    <link href="/kxreader/Content/css/LeftDetail?v=NLcKG8I1SJUaVFrQ0iGpF2klAT0OsmHRaVSZ1rKb5xg1" rel="stylesheet"/>

</body>
</html>

