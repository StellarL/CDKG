

<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>

</head>

<body>

    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637135584354787500%26DBCODE%3dCJFD%26TABLEName%3dCJFDTEMP%26FileName%3dDZCL201918016%26RESULT%3d1%26SIGN%3dJFR6mrZlZaRz9VRtGOE4Db6bt68%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=DZCL201918016&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=DZCL201918016&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>


    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZCL201918016&amp;v=MDkwMjg2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnRGeW5nVXIzQklUZklZckc0SDlqTnA0OUVZb1FLREg4NHZSNFQ=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#45" data-title="&lt;b&gt;0 引  言&lt;/b&gt; "><b>0 引  言</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#48" data-title="&lt;b&gt;1 系统介绍&lt;/b&gt; "><b>1 系统介绍</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#54" data-title="&lt;b&gt;1.1 联合频谱和IPD特征估计时间频率掩蔽值&lt;/b&gt;"><b>1.1 联合频谱和IPD特征估计时间频率掩蔽值</b></a></li>
                                                <li><a href="#70" data-title="&lt;b&gt;1.2 基于时间频率掩蔽值的MVDR波束成形&lt;/b&gt;"><b>1.2 基于时间频率掩蔽值的MVDR波束成形</b></a></li>
                                                <li><a href="#85" data-title="&lt;b&gt;1.3 联合频谱和方向特征再次估计掩蔽值&lt;/b&gt;"><b>1.3 联合频谱和方向特征再次估计掩蔽值</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#91" data-title="&lt;b&gt;2 实验设置及实验结果&lt;/b&gt; "><b>2 实验设置及实验结果</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#98" data-title="&lt;b&gt;3 结  论&lt;/b&gt; "><b>3 结  论</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#50" data-title="图1 本文算法的流程">图1 本文算法的流程</a></li>
                                                <li><a href="#60" data-title="图2 IPD和cos&lt;i&gt;IPD&lt;/i&gt;的分布">图2 IPD和cos<i>IPD</i>的分布</a></li>
                                                <li><a href="#69" data-title="图3 &lt;i&gt;BLSTM&lt;/i&gt;&lt;sub&gt;1&lt;/sub&gt;网络结构">图3 <i>BLSTM</i><sub>1</sub>网络结构</a></li>
                                                <li><a href="#95" data-title="&lt;b&gt;表1 不同系统在验证集和测试集的WER&lt;/b&gt; (%)"><b>表1 不同系统在验证集和测试集的WER</b> (%)</a></li>
                                                <li><a href="#97" data-title="&lt;b&gt;表2 不同系统在测试集仿真数据的PESQ&lt;/b&gt;"><b>表2 不同系统在测试集仿真数据的PESQ</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="146">


                                    <a id="bibliography_1" title=" GANNOT S,BURSHTEIN D,WEINSTEIN E.Signal enhancement using beamforming and nonstationarity with applications to speech[J].IEEE Transactions on Signal Processing,2001,49(8):1614-1626." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Signal enhancement using beamforming and nonstationarity with applications to speech">
                                        <b>[1]</b>
                                         GANNOT S,BURSHTEIN D,WEINSTEIN E.Signal enhancement using beamforming and nonstationarity with applications to speech[J].IEEE Transactions on Signal Processing,2001,49(8):1614-1626.
                                    </a>
                                </li>
                                <li id="148">


                                    <a id="bibliography_2" title=" DAI,X Z,YU B X,DAI X H.An improved signal Subspace algorithm for speech enhancement[J].Conference on e-Business,e-Services and e-Society,2014:104-114." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=An improved signal subspace algorithm for speech enhancement">
                                        <b>[2]</b>
                                         DAI,X Z,YU B X,DAI X H.An improved signal Subspace algorithm for speech enhancement[J].Conference on e-Business,e-Services and e-Society,2014:104-114.
                                    </a>
                                </li>
                                <li id="150">


                                    <a id="bibliography_3" title=" YOSHIOKA T,ITO N,DELCROIX M,et al.The NTT CHiME-3 system:Advances in speech enhancement and recognition for mobile multi-microphone devices[C].2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU),2015:436-443." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The NTT CHiME-3 system:Advances in speech enhancement and recognition for mobile multi-microphone devices">
                                        <b>[3]</b>
                                         YOSHIOKA T,ITO N,DELCROIX M,et al.The NTT CHiME-3 system:Advances in speech enhancement and recognition for mobile multi-microphone devices[C].2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU),2015:436-443.
                                    </a>
                                </li>
                                <li id="152">


                                    <a id="bibliography_4" title=" 刘文举,聂帅,梁山,等.基于深度学习语音分离技术的研究现状与进展[J].自动化学报,2016,42(6):819-833." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201606003&amp;v=MDQ0MjJDVVI3cWZadVp0RnluZ1VyM0JLQ0xmWWJHNEg5Zk1xWTlGWjRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnI=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                         刘文举,聂帅,梁山,等.基于深度学习语音分离技术的研究现状与进展[J].自动化学报,2016,42(6):819-833.
                                    </a>
                                </li>
                                <li id="154">


                                    <a id="bibliography_5" title=" DU J,TU Y H,SUN L,et al.The USTC-iFlytek system for CHiME-4 challenge[C].Proc.CHiME,2016:36-38." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The USTC-iFlytek system for CHiME-4 challenge">
                                        <b>[5]</b>
                                         DU J,TU Y H,SUN L,et al.The USTC-iFlytek system for CHiME-4 challenge[C].Proc.CHiME,2016:36-38.
                                    </a>
                                </li>
                                <li id="156">


                                    <a id="bibliography_6" title=" HEYMANN J,DRUDE L,CHINAEV A,et al.BLSTM supported GEV beamformer front-end for the 3rd CHiME challenge[C].2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU).IEEE,2015:444-451." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=BLSTM supported GEV beamformer front-end for the 3rd CHiME challenge">
                                        <b>[6]</b>
                                         HEYMANN J,DRUDE L,CHINAEV A,et al.BLSTM supported GEV beamformer front-end for the 3rd CHiME challenge[C].2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU).IEEE,2015:444-451.
                                    </a>
                                </li>
                                <li id="158">


                                    <a id="bibliography_7" title=" ZHANG X L,WANG Z Q,WANG D L.A speech enhancement algorithm by iterating single-and multi-microphone processing and its application to robust ASR[C].2017 IEEE International Conference on Acoustics,Speech and Signal Processing (ICASSP),2017:276-280." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A speech enhancement algorithm by iterating single-and multi-microphone processing and its application to robust ASR">
                                        <b>[7]</b>
                                         ZHANG X L,WANG Z Q,WANG D L.A speech enhancement algorithm by iterating single-and multi-microphone processing and its application to robust ASR[C].2017 IEEE International Conference on Acoustics,Speech and Signal Processing (ICASSP),2017:276-280.
                                    </a>
                                </li>
                                <li id="160">


                                    <a id="bibliography_8" title=" JIANG Y,WANG D L,LIU R S,et al.Binaural classification for reverberant speech segregation using deep neural networks[J].IEEE/ACM Transactions on Audio,Speech and Language Processing (TASLP),2014,22(12):2112-2121." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCMBAF48FEDAF7D6EDA80E930D2A5EA1704&amp;v=MTI3NjlsZkJyTFUwNXR0aHhiMit3cUU9TmlmSVk4SEphTlhFMmZveEZaMEllSHBNdTJjYjZrcDBTMytXcm1Nd0RNT1ZRcnFiQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUQ==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                         JIANG Y,WANG D L,LIU R S,et al.Binaural classification for reverberant speech segregation using deep neural networks[J].IEEE/ACM Transactions on Audio,Speech and Language Processing (TASLP),2014,22(12):2112-2121.
                                    </a>
                                </li>
                                <li id="162">


                                    <a id="bibliography_9" title=" ARAKI S,HAYASHI T,DELCROIX M,et al.Exploring multi-channel features for denoising-autoencoder-based speech enhancement[C].2015 IEEE International Conference on Acoustics,Speech and Signal Processing (ICASSP),2015:116-120." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Exploring multi-channel features for denoising-autoencoder-based speech enhancement">
                                        <b>[9]</b>
                                         ARAKI S,HAYASHI T,DELCROIX M,et al.Exploring multi-channel features for denoising-autoencoder-based speech enhancement[C].2015 IEEE International Conference on Acoustics,Speech and Signal Processing (ICASSP),2015:116-120.
                                    </a>
                                </li>
                                <li id="164">


                                    <a id="bibliography_10" title=" WANG Z Q,WANG D L.On spatial features for supervised speech separation and its application to beamforming and robust ASR[C].2018 IEEE International Conference on Acoustics,Speech and Signal Processing (ICASSP).IEEE,2018:5709-5713." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=On spatial features for supervised speech separation and its application to beamforming and robust ASR">
                                        <b>[10]</b>
                                         WANG Z Q,WANG D L.On spatial features for supervised speech separation and its application to beamforming and robust ASR[C].2018 IEEE International Conference on Acoustics,Speech and Signal Processing (ICASSP).IEEE,2018:5709-5713.
                                    </a>
                                </li>
                                <li id="166">


                                    <a id="bibliography_11" title=" WANG Z Q,WANG D L.All-neural multi-channel speech enhancement[C].Proc.Interspeech,2018:3234-3238." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=All-neural multi-channel speech enhancement">
                                        <b>[11]</b>
                                         WANG Z Q,WANG D L.All-neural multi-channel speech enhancement[C].Proc.Interspeech,2018:3234-3238.
                                    </a>
                                </li>
                                <li id="168">


                                    <a id="bibliography_12" title=" LI J,STOICA P.Robust adaptive beamforming[M].John Wiley &amp;amp; Sons,2005." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Robust adaptive beamforming">
                                        <b>[12]</b>
                                         LI J,STOICA P.Robust adaptive beamforming[M].John Wiley &amp;amp; Sons,2005.
                                    </a>
                                </li>
                                <li id="170">


                                    <a id="bibliography_13" title=" WANG Z Q,WANG D L.Integrating spectral and spatial features for multi-channel speaker separation[C].Proc.Interspeech,2018:2718-2722." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Integrating spectral and spatial features for multi-channel speaker separation">
                                        <b>[13]</b>
                                         WANG Z Q,WANG D L.Integrating spectral and spatial features for multi-channel speaker separation[C].Proc.Interspeech,2018:2718-2722.
                                    </a>
                                </li>
                                <li id="172">


                                    <a id="bibliography_14" title=" YOSHIOKA T,ERDOGAN H,CHEN Z,et al.Multi-microphone neural speech separation for far-field multi-talker speech recognition[C].2018 IEEE International Conference on Acoustics,Speech and Signal Processing (ICASSP),2018:5739-5743." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multi-microphone neural speech separation for far-field multi-talker speech recognition">
                                        <b>[14]</b>
                                         YOSHIOKA T,ERDOGAN H,CHEN Z,et al.Multi-microphone neural speech separation for far-field multi-talker speech recognition[C].2018 IEEE International Conference on Acoustics,Speech and Signal Processing (ICASSP),2018:5739-5743.
                                    </a>
                                </li>
                                <li id="174">


                                    <a id="bibliography_15" title=" ERDOGAN H,HERSHEY J R,WATANABE S,et al.Phase-sensitive and recognition-boosted speech separation using deep recurrent neural networks[C].2015 IEEE International Conference on Acoustics,Speech and Signal Processing (ICASSP),2015:708-712." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Phase-sensitive and recognition-boosted speech separation using deep recurrent neural networks">
                                        <b>[15]</b>
                                         ERDOGAN H,HERSHEY J R,WATANABE S,et al.Phase-sensitive and recognition-boosted speech separation using deep recurrent neural networks[C].2015 IEEE International Conference on Acoustics,Speech and Signal Processing (ICASSP),2015:708-712.
                                    </a>
                                </li>
                                <li id="176">


                                    <a id="bibliography_16" title=" 姚远,王秋菊,周伟,等.改进谱减法结合神经网络的语音增强研究[J].电子测量技术,2017,40(7):75-79" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZCL201707017&amp;v=MDY4NzE0TzN6cXFCdEdGckNVUjdxZlp1WnRGeW5nVXIzQklUZklZckc0SDliTXFJOUVZNFFLREg4NHZSNFQ2ajU=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[16]</b>
                                         姚远,王秋菊,周伟,等.改进谱减法结合神经网络的语音增强研究[J].电子测量技术,2017,40(7):75-79
                                    </a>
                                </li>
                                <li id="178">


                                    <a id="bibliography_17" title=" WANG Z Q,WANG D L.Mask weighted STFT ratios for relative transfer function estimation and its application to robust ASR[C].2018 IEEE International Conference on Acoustics,Speech and Signal Processing (ICASSP),2018:5619-5623." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Mask weighted STFT ratios for relative transfer function estimation and its application to robust ASR">
                                        <b>[17]</b>
                                         WANG Z Q,WANG D L.Mask weighted STFT ratios for relative transfer function estimation and its application to robust ASR[C].2018 IEEE International Conference on Acoustics,Speech and Signal Processing (ICASSP),2018:5619-5623.
                                    </a>
                                </li>
                                <li id="180">


                                    <a id="bibliography_18" title=" VINCENG E,WATANABE S,NUGRAHA A A,et al.An analysis of environment,microphone and data simulation mismatches in robust speech recognition[J].Computer Speech &amp;amp; Language,2017,46:535-557." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESF0BA43456210468C8291D24415A16665&amp;v=MDgwODByTFUwNXR0aHhiMit3cUU9TmlmT2ZjVzRiS0RJckl0QVl1a09ESGcveDJVYjZEWjhQSDNtcUJNd0NMT1NRN3lhQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQg==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[18]</b>
                                         VINCENG E,WATANABE S,NUGRAHA A A,et al.An analysis of environment,microphone and data simulation mismatches in robust speech recognition[J].Computer Speech &amp;amp; Language,2017,46:535-557.
                                    </a>
                                </li>
                                <li id="182">


                                    <a id="bibliography_19" title=" GAROFALO J,GRAFF D,PAUL D,et al.CSR-I (WSJ0) Complete[CP].Linguistic Data Consortium,2007." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=CSR-I  (WSJ0) Complete[CP]">
                                        <b>[19]</b>
                                         GAROFALO J,GRAFF D,PAUL D,et al.CSR-I (WSJ0) Complete[CP].Linguistic Data Consortium,2007.
                                    </a>
                                </li>
                                <li id="184">


                                    <a id="bibliography_20" title=" VINCENT E,GRIBONVAL R,PLUMBLEY M D.Oracle estimators for the benchmarking of source separation algorithms[J].Signal Processing,2007,87(8):1933-1950." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012300650740&amp;v=MTI5OThET3JJOUZZdTRQQzNnNW9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaW5sVXJ6SUlWc1ZhQnM9TmlmT2ZiSzdIdA==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[20]</b>
                                         VINCENT E,GRIBONVAL R,PLUMBLEY M D.Oracle estimators for the benchmarking of source separation algorithms[J].Signal Processing,2007,87(8):1933-1950.
                                    </a>
                                </li>
                                <li id="186">


                                    <a id="bibliography_21" title=" ANGUERA X,WOOTERS C,HERNANDO J.Acoustic beamforming for speaker diarization of meetings[J].IEEE Transactions on Audio,Speech,and Language Processing,2007,15(7):2011-2022." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Acoustic Beamforming for Speaker Diarization of Meetings">
                                        <b>[21]</b>
                                         ANGUERA X,WOOTERS C,HERNANDO J.Acoustic beamforming for speaker diarization of meetings[J].IEEE Transactions on Audio,Speech,and Language Processing,2007,15(7):2011-2022.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=DZCL" target="_blank">电子测量技术</a>
                2019,42(18),90-94 DOI:10.19651/j.cnki.emt.1902730            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>一种联合频谱和空间特征的深度学习多通道语音增强算法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%82%93%E8%B4%BA%E5%85%83&amp;code=43281598&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">邓贺元</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%88%98%E5%8A%A0&amp;code=08175975&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">刘加</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%A4%8F%E5%96%84%E7%BA%A2&amp;code=05969195&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">夏善红</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BD%AD%E6%98%A5%E8%8D%A3&amp;code=09577104&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">彭春荣</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E7%A7%91%E5%AD%A6%E9%99%A2%E7%94%B5%E5%AD%90%E5%AD%A6%E7%A0%94%E7%A9%B6%E6%89%80%E4%BC%A0%E6%84%9F%E6%8A%80%E6%9C%AF%E5%9B%BD%E5%AE%B6%E9%87%8D%E7%82%B9%E5%AE%9E%E9%AA%8C%E5%AE%A4&amp;code=0227399&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国科学院电子学研究所传感技术国家重点实验室</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E7%A7%91%E5%AD%A6%E9%99%A2%E5%A4%A7%E5%AD%A6&amp;code=1698842&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国科学院大学</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%B8%85%E5%8D%8E%E5%A4%A7%E5%AD%A6%E7%94%B5%E5%AD%90%E5%B7%A5%E7%A8%8B%E7%B3%BB%E6%B8%85%E5%8D%8E%E4%BF%A1%E6%81%AF%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF%E5%9B%BD%E5%AE%B6%E5%AE%9E%E9%AA%8C%E5%AE%A4&amp;code=0187103&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">清华大学电子工程系清华信息科学与技术国家实验室</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>近年来,越来越多的电子产品使用麦克风阵列,而且与传统多通道语音增强算法相比,基于深度学习的算法效果更好,为了进一步提高增强效果,提出一种联合频谱特征和空间特征的深度学习算法。该算法包括两个部分,第一部分,使用频谱和通道间相位差特征估计时间频率掩蔽值,然后进行基于掩蔽值的波束成形;第二部分,使用方向特征和频谱特征进行进一步的增强。在CHiME4数据集上的实验证明了该算法的有效性,与仅使用频谱特征的方法相比,在真实数据上的词错误率相对降低27.6%,在仿真数据上的主观语音质量评估得分从2.46提高到2.81。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%97%B6%E9%97%B4%E9%A2%91%E7%8E%87%E6%8E%A9%E8%94%BD%E5%80%BC&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">时间频率掩蔽值;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%A9%BA%E9%97%B4%E7%89%B9%E5%BE%81&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">空间特征;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B3%A2%E6%9D%9F%E6%88%90%E5%BD%A2&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">波束成形;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    邓贺元,硕士研究生,主要研究方向为多通道语音增强,深度学习等。E-mail:heyuandeng@163.com;
                                </span>
                                <span>
                                    刘加,教授,博导,主要研究方向为语音识别、语音增强及关键词检测。;
                                </span>
                                <span>
                                    夏善红,研究员,博导,主要研究方向为电场传感器、MEMS技术。;
                                </span>
                                <span>
                                    彭春荣,研究员,硕导,主要研究方向为MEMS电场传感器及系统。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-02-25</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金(U1836219)资助项目;</span>
                    </p>
            </div>
                    <h1><b>Combining spectral and spatial features for deep learning based multi-channel speech enhancement</b></h1>
                    <h2>
                    <span>Deng Heyuan</span>
                    <span>Liu Jia</span>
                    <span>Xia Shanhong</span>
                    <span>Peng Chunrong</span>
            </h2>
                    <h2>
                    <span>State Key Laboratory of Transducer Technology, Institute of Electronics, Chinese Academy of Sciences</span>
                    <span>University of Chinese Academy of Sciences</span>
                    <span>Tsinghua National Laboratory for Information Science &amp;Technology, Dept.of Electronic Engineering, Tsinghua University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>In recent years, more and more electronic devices use microphone arrays, and compared with traditional multi-channel speech enhancement methods, deep learning-based methods are more efficient. In order to further improve the enhancement effect, this paper proposed an algorithm which combines spectral and spatial features for deep learning. The algorithm consists of two parts, the first part uses the spectral feature and inter-channel phase difference feature to estimate the time-frequency masks, then performs mask-based beamforming. The second part uses the spectral and direction features for further enhancement. Experiments on the CHiME4 dataset demonstrate the effectiveness of the proposed method. Compared to spectral feature-based methods, the word error rate on real data is relatively reduced by 27.6% and the scores of perceptual evaluation of speech quality on simulated dataset are improved from 2.46 to 2.81.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=deep%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">deep learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=time-frequency%20masks&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">time-frequency masks;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=spatial%20feature&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">spatial feature;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=beamforming&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">beamforming;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2019-02-25</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="45" name="45" class="anchor-tag"><b>0 引  言</b></h3>
                <div class="p1">
                    <p id="46">多通道语音增强算法在语音交互、会议系统、助听器等产品得到广泛的使用。传统多通道语音增强算法经过几十年的发展已经相对成熟,主要有波束成形算法<citation id="188" type="reference"><link href="146" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>,基于子空间的算法<citation id="189" type="reference"><link href="148" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>等。波束成形算法需要设计一个线性滤波器,对不同通道的信号施加不同的权重累加,其中权重的计算需要准确的估计到达方向(direction of arrival, DOA)和语音或噪声的协方差矩阵。然而这些估计需要已知麦克风阵列的几何结构、假设信号是平面波传播、假设噪声是平稳的,在实际环境中这些假设条件通常并不满足。</p>
                </div>
                <div class="p1">
                    <p id="47">CHiME 3挑战第一名<citation id="190" type="reference"><link href="150" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>的系统使用复数域的高斯混合模型估计时间频率掩蔽值,由掩蔽值计算波束成形器的导向矢量然后进行波束成形的多通道语音增强算法比传统的算法在词错误率上降低5%,获得很大成功。与此同时,使用神经网络估计时间频率掩蔽值的单通道语音增强算法取得了快速的发展<citation id="191" type="reference"><link href="152" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>。因而使用神经网络估计时间频率掩蔽值与基于掩蔽值的波束成形结合的多通道增强算法成为研究热点<citation id="197" type="reference"><link href="154" rel="bibliography" /><link href="156" rel="bibliography" /><link href="158" rel="bibliography" /><link href="160" rel="bibliography" /><link href="162" rel="bibliography" /><link href="164" rel="bibliography" /><link href="166" rel="bibliography" /><sup>[<a class="sup">5</a>,<a class="sup">6</a>,<a class="sup">7</a>,<a class="sup">8</a>,<a class="sup">9</a>,<a class="sup">10</a>,<a class="sup">11</a>]</sup></citation>。文献<citation id="192" type="reference">[<a class="sup">5</a>]</citation>使用深度神经网络(deep neural network, DNN)估计掩蔽值后与波束成形结合。文献<citation id="193" type="reference">[<a class="sup">6</a>]</citation>使用双向长短时记忆(bi-directional long short term memory, BLSTM)网络估计掩蔽值后与波束成形结合。文献<citation id="194" type="reference">[<a class="sup">7</a>]</citation> 迭代进行基于神经网络的单通道语音增强和多通道语音增强。这几种算法仅使用了信号的频谱信息,没有使用空间信息。文献<citation id="198" type="reference">[<a class="sup">8</a>,<a class="sup">9</a>]</citation>使用了通道间相位差(inter-channel phase difference, IPD)和幅度差特征这两种空间特征,但是它们针对的是增强固定方向的目标语音信号。文献<citation id="195" type="reference">[<a class="sup">10</a>]</citation>中使用相干性特征和方向特征与频谱特征结合估计两次掩蔽值,进行两次波束成形。文献<citation id="196" type="reference">[<a class="sup">11</a>]</citation>使用方向特征估计掩蔽值。本文提出的算法使用IPD特征与频谱特征初次估计掩蔽值进行波束成形,使用方向特征和频谱特征再次估计掩蔽值,更充分利用了空间特征来估计掩蔽值,与波束成形信号的相位结合,改进语音增强的效果。</p>
                </div>
                <h3 id="48" name="48" class="anchor-tag"><b>1 系统介绍</b></h3>
                <div class="p1">
                    <p id="49">本文算法的流程如图1所示,计算多通道信号的频谱和IPD特征,输入到BLSTM<sub>1</sub>网络得到初次估计的时间频率掩蔽值,然后进行基于时间频率掩蔽值的最小方差无失真响应(minimum variance distortionless response, MVDR)波束成形<citation id="199" type="reference"><link href="168" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>,计算方向特征和频谱特征,输入到BLSTM<sub>2</sub>网络得到第二次估计的掩蔽值,与波束成形后的信号相位结合得到增强后的信号。</p>
                </div>
                <div class="area_img" id="50">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/DZCL201918016_050.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 本文算法的流程" src="Detail/GetImg?filename=images/DZCL201918016_050.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 本文算法的流程  <a class="btn-zoomin" href="Detail/GetImg?filename=images/DZCL201918016_050.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="51">麦克风阵列接收到的信号的物理模型如式(1)所示。</p>
                </div>
                <div class="p1">
                    <p id="52"><i><b>Y</b></i>(<i>t</i>,<i>f</i>)=<i><b>a</b></i>(<i>f</i>)<i>s</i>(<i>t</i>,<i>f</i>)+<i><b>N</b></i>(<i>t</i>,<i>f</i>)=<i><b>X</b></i>(<i>t</i>,<i>f</i>)+<i><b>N</b></i>(<i>t</i>,<i>f</i>)      (1)</p>
                </div>
                <div class="p1">
                    <p id="53">式中:*(<i>t</i>,<i>f</i>) 表示信号*在时间<i>t</i>,频率<i>f</i>点的短时傅里叶变换(short time Fourier transform, STFT)的值;<i><b>Y</b></i>(<i>t</i>,<i>f</i>)表示麦克风阵列接收到的多通道信号;<i><b>N</b></i>(<i>t</i>,<i>f</i>)表示多通道的噪声信号;<i>s</i>(<i>t</i>,<i>f</i>)表示目标声源信号;<i><b>a</b></i>(<i>f</i>)表示声源到麦克风阵列的声学传递函数,也是波束成形器的导向矢量;<i><b>X</b></i>(<i>t</i>,<i>f</i>)表示阵列接收的目标信号。</p>
                </div>
                <h4 class="anchor-tag" id="54" name="54"><b>1.1 联合频谱和IPD特征估计时间频率掩蔽值</b></h4>
                <h4 class="anchor-tag" id="55" name="55">1)IPD特征</h4>
                <div class="p1">
                    <p id="56">与单通道语音信号相比,多通道的语音信号不仅包含时间、频率域的信息,还包含空间域信息。一种常用的空间特征为IPD特征。假设有<i>m</i>个麦克风,选择一个参考麦克风<i>r</i>,由于语音信号的稀疏性以及声源到阵列的时间延迟,其他麦克风<i>n</i>与参考麦克风之间STFT比值<i>Y</i><sub><i>n</i></sub>/<i>Y</i><sub><i>r</i></sub>如式(2)所示,在频域上能够聚为一类,这是多通道语音分离的基本依据<citation id="200" type="reference"><link href="170" rel="bibliography" /><link href="172" rel="bibliography" /><sup>[<a class="sup">13</a>,<a class="sup">14</a>]</sup></citation>,其中|*|表示*的幅度。</p>
                </div>
                <div class="p1">
                    <p id="57"><mathml id="100"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mi>Y</mi><msub><mrow></mrow><mi>n</mi></msub></mrow><mrow><mi>Y</mi><msub><mrow></mrow><mi>r</mi></msub></mrow></mfrac><mo>=</mo><mfrac><mrow><mrow><mo>|</mo><mrow><mi>Y</mi><msub><mrow></mrow><mi>n</mi></msub></mrow><mo>|</mo></mrow></mrow><mrow><mrow><mo>|</mo><mrow><mi>Y</mi><msub><mrow></mrow><mi>r</mi></msub></mrow><mo>|</mo></mrow></mrow></mfrac><mtext>e</mtext><msup><mrow></mrow><mrow><mi>j</mi><mo stretchy="false">(</mo><mo>∠</mo><mi>Y</mi><msub><mrow></mrow><mi>n</mi></msub><mo>-</mo><mo>∠</mo><mi>Y</mi><msub><mrow></mrow><mi>r</mi></msub><mo stretchy="false">)</mo></mrow></msup></mrow></math></mathml>      (2)</p>
                </div>
                <div class="p1">
                    <p id="58"><i>Y</i><sub><i>n</i></sub>/<i>Y</i><sub><i>r</i></sub><i>Y</i><sub><i>n</i></sub>/<i>Y</i><sub><i>r</i></sub>包含的空间信息可以由IPD特征表示,由公式(3)计算。实际情况中IPD分布在[-π,π],如图2(a)所示,在频域上可能会不连续,影响对掩蔽值的估计准确度。尝试使用cos函数对其约束,改善其连续性,cos<i>IPD</i>的分布如图2(b)所示。对于通道<i>r</i>和通道<i>n</i>的组合(<i>r</i>,<i>n</i>),神经网络的输入为通道<i>r</i>的频谱特征和<i>IPD</i><sub><i>rn</i></sub>的联合。</p>
                </div>
                <div class="p1">
                    <p id="59"><mathml id="101"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ι</mi><mi>Ρ</mi><mi>D</mi><msub><mrow></mrow><mrow><mi>r</mi><mi>n</mi></mrow></msub><mo>=</mo><mo>∠</mo><mrow><mo>(</mo><mrow><mfrac><mrow><mi>Y</mi><msub><mrow></mrow><mi>n</mi></msub></mrow><mrow><mi>Y</mi><msub><mrow></mrow><mi>r</mi></msub></mrow></mfrac></mrow><mo>)</mo></mrow></mrow></math></mathml>      (3)</p>
                </div>
                <div class="area_img" id="60">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/DZCL201918016_060.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 IPD和cosIPD的分布" src="Detail/GetImg?filename=images/DZCL201918016_060.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 IPD和cos<i>IPD</i>的分布  <a class="btn-zoomin" href="Detail/GetImg?filename=images/DZCL201918016_060.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="61" name="61">2)时间频率掩蔽值及损失函数</h4>
                <div class="p1">
                    <p id="62">神经网络的训练目标是时间频率掩蔽值,常用的有理想二进制掩蔽(ideal binary mask, IBM)、理想比率掩蔽(ideal ratio mask, IRM)以及相敏掩蔽(phase sensitive mask, PSM)。IRM和PSM的计算分别如式(4)、(5)所示。选取IRM作为训练目标时使用幅度谱估计损失函数<i>L</i><sub><i>M</i></sub>如式(6)所示,其中<mathml id="102"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>Μ</mi><mo>^</mo></mover></math></mathml>是估计的掩蔽值,<i>N</i>是时频点个数。选取PSM作为训练目标时,使用相敏频谱估计损失函数<i>L</i><sub><i>P</i></sub>如式(7)所示,使用截断的PSM(tPSM)将PSM截断在[0,1]之间,这两个损失函数都是信号域的损失函数,以最小化估计信号幅度与目标信号幅度的均方误差为训练目标,是对掩蔽值域损失函数的加权改进,效果更好<citation id="201" type="reference"><link href="174" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="63"><mathml id="103"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ι</mi><mi>R</mi><mi>Μ</mi><mo>=</mo><mfrac><mrow><mrow><mo>|</mo><mi mathvariant="bold-italic">X</mi><mo>|</mo></mrow></mrow><mrow><mrow><mo>|</mo><mi mathvariant="bold-italic">X</mi><mo>|</mo></mrow><mo>+</mo><mrow><mo>|</mo><mi mathvariant="bold-italic">Ν</mi><mo>|</mo></mrow></mrow></mfrac></mrow></math></mathml>      (4)</p>
                </div>
                <div class="p1">
                    <p id="64"><mathml id="104"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><mi>S</mi><mi>Μ</mi><mo>=</mo><mfrac><mrow><mrow><mo>|</mo><mi mathvariant="bold-italic">X</mi><mo>|</mo></mrow></mrow><mrow><mrow><mo>|</mo><mi mathvariant="bold-italic">Y</mi><mo>|</mo></mrow></mrow></mfrac><mrow><mi>cos</mi></mrow><mo stretchy="false">(</mo><mo>∠</mo><mi mathvariant="bold-italic">X</mi><mo>-</mo><mo>∠</mo><mi mathvariant="bold-italic">Y</mi><mo stretchy="false">)</mo></mrow></math></mathml>      (5)</p>
                </div>
                <div class="p1">
                    <p id="65"><mathml id="105"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><msub><mrow></mrow><mi>Μ</mi></msub><mo>=</mo><mfrac><mn>1</mn><mi>Ν</mi></mfrac><mstyle displaystyle="true"><msub><mo>∑</mo><mrow><mi>t</mi><mo>,</mo><mi>f</mi></mrow></msub><mrow><mo stretchy="false">(</mo><mover accent="true"><mi>Μ</mi><mo>^</mo></mover><msub><mrow></mrow><mrow><mi>t</mi><mo>,</mo><mi>f</mi></mrow></msub><mrow><mo>|</mo><mi mathvariant="bold-italic">Y</mi><mo>|</mo></mrow><mo>-</mo><mrow><mo>|</mo><mi mathvariant="bold-italic">X</mi><mo>|</mo></mrow><mo stretchy="false">)</mo></mrow></mstyle><msup><mrow></mrow><mn>2</mn></msup></mrow></math></mathml>      (6)</p>
                </div>
                <div class="p1">
                    <p id="66"><mathml id="106"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><msub><mrow></mrow><mi>Ρ</mi></msub><mo>=</mo><mfrac><mn>1</mn><mi>Ν</mi></mfrac><mstyle displaystyle="true"><msub><mo>∑</mo><mrow><mi>t</mi><mo>,</mo><mi>f</mi></mrow></msub><mrow><mo stretchy="false">(</mo><mover accent="true"><mi>Μ</mi><mo>^</mo></mover><msub><mrow></mrow><mrow><mi>t</mi><mo>,</mo><mi>f</mi></mrow></msub><mrow><mo>|</mo><mi mathvariant="bold-italic">Y</mi><mo>|</mo></mrow><mo>-</mo><mi>t</mi><mi>Ρ</mi><mi>S</mi><mi>Μ</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>,</mo><mi>f</mi></mrow></msub><mrow><mo>|</mo><mi mathvariant="bold-italic">X</mi><mo>|</mo></mrow><mo stretchy="false">)</mo></mrow></mstyle><msup><mrow></mrow><mn>2</mn></msup></mrow></math></mathml>      (7)</p>
                </div>
                <h4 class="anchor-tag" id="67" name="67">3)神经网络结构</h4>
                <div class="p1">
                    <p id="68">神经网络使用BLSTM网络。BLSTM网络和DNN相比能够更有效地从过去和未来的帧建模上下文中信息,使用BLSTM网络比使用DNN网络<citation id="202" type="reference"><link href="176" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>能够更准确地估计时间频率掩蔽值。网络结构如图3所示,具体参数在实验部分介绍。</p>
                </div>
                <div class="area_img" id="69">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/DZCL201918016_069.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 BLSTM1网络结构" src="Detail/GetImg?filename=images/DZCL201918016_069.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 <i>BLSTM</i><sub>1</sub>网络结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/DZCL201918016_069.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="70" name="70"><b>1.2 基于时间频率掩蔽值的MVDR波束成形</b></h4>
                <div class="p1">
                    <p id="71">在文献<citation id="203" type="reference">[<a class="sup">5</a>,<a class="sup">6</a>]</citation>中的研究表明与传统的波束成形算法相比,基于神经网络估计时间频率掩蔽值的波束成形算法能够更准确地估计目标语音和噪声的统计信息,实现更好的波束成形效果。这里使用MVDR波束成形,也就是在信号无失真的约束条件下,最小化波束成形器的输出功率。MVDR波束成形器的权向量<i><b>w</b></i>由导向矢量<i><b>a</b></i>和噪声信号的协方差矩阵<i>Φ</i><sub><i>NN</i></sub>决定, 如式(8)所示,其中(*)<sup>H</sup>表示共轭转置。</p>
                </div>
                <div class="p1">
                    <p id="72"><mathml id="107"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">w</mi><mo>=</mo><mfrac><mrow><mi mathvariant="bold-italic">Φ</mi><msubsup><mrow></mrow><mrow><mi>Ν</mi><mi>Ν</mi></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msubsup><mi mathvariant="bold-italic">a</mi></mrow><mrow><mi mathvariant="bold-italic">a</mi><msup><mrow></mrow><mtext>Η</mtext></msup><mi mathvariant="bold-italic">Φ</mi><msubsup><mrow></mrow><mrow><mi>Ν</mi><mi>Ν</mi></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msubsup><mi mathvariant="bold-italic">a</mi></mrow></mfrac></mrow></math></mathml>      (8)</p>
                </div>
                <div class="p1">
                    <p id="73">导向矢量<i><b>a</b></i>是目标信号的协方差矩阵<i>Φ</i><sub><i>XX</i></sub>最大特征值对应的特征向量,如式(9)所示。</p>
                </div>
                <div class="p1">
                    <p id="74"><i><b>a</b></i>=<i>P</i>{<i>Φ</i><sub><i>XX</i></sub>}(9)</p>
                </div>
                <div class="p1">
                    <p id="75">噪声信号的协方差矩阵的计算如式(10)所示<citation id="204" type="reference"><link href="178" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>,</p>
                </div>
                <div class="p1">
                    <p id="76"><mathml id="108"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">Φ</mi><msub><mrow></mrow><mrow><mi>Ν</mi><mi>Ν</mi></mrow></msub><mo>=</mo><mfrac><mn>1</mn><mrow><mstyle displaystyle="true"><msub><mo>∑</mo><mi>t</mi></msub><mi>α</mi></mstyle><msub><mrow></mrow><mrow><mi>t</mi><mo>,</mo><mi>f</mi></mrow></msub></mrow></mfrac><mstyle displaystyle="true"><msub><mo>∑</mo><mi>t</mi></msub><mi>α</mi></mstyle><msub><mrow></mrow><mrow><mi>t</mi><mo>,</mo><mi>f</mi></mrow></msub><mi mathvariant="bold-italic">Y</mi><mi mathvariant="bold-italic">Y</mi><msup><mrow></mrow><mtext>Η</mtext></msup></mrow></math></mathml>      (10)</p>
                </div>
                <div class="p1">
                    <p id="77">式中:<i>α</i><sub><i>t</i></sub><sub>,</sub><sub><i>f</i></sub>每个(<i>t</i>,<i>f</i>)点在噪声信号协方差矩阵计算的权重,由式(11)计算,其中<mathml id="109"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>Μ</mi><mo>^</mo></mover><msub><mrow></mrow><mrow><mi>i</mi><mo>,</mo><mi>t</mi><mo>,</mo><mi>f</mi></mrow></msub></mrow></math></mathml>表示使用BLSTM<sub>1</sub>网络估计对通道<i>i</i>估计的掩蔽值,一共有<i>m</i>个。</p>
                </div>
                <div class="p1">
                    <p id="78"><mathml id="110"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>α</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>,</mo><mi>f</mi></mrow></msub><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∏</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mo stretchy="false">(</mo></mstyle><mn>1</mn><mo>-</mo><mover accent="true"><mi>Μ</mi><mo>^</mo></mover><msub><mrow></mrow><mrow><mi>i</mi><mo>,</mo><mi>t</mi><mo>,</mo><mi>f</mi></mrow></msub><mo stretchy="false">)</mo></mrow></math></mathml>      (11)</p>
                </div>
                <div class="p1">
                    <p id="79">目标信号的协方差矩阵的计算如公式(12)所示。</p>
                </div>
                <div class="p1">
                    <p id="80"><mathml id="111"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">Φ</mi><msub><mrow></mrow><mrow><mi>X</mi><mi>X</mi></mrow></msub><mo>=</mo><mfrac><mn>1</mn><mi>Τ</mi></mfrac><mstyle displaystyle="true"><msub><mo>∑</mo><mi>t</mi></msub><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>^</mo></mover></mstyle><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>^</mo></mover><msup><mrow></mrow><mtext>Η</mtext></msup></mrow></math></mathml>      (12)</p>
                </div>
                <div class="p1">
                    <p id="81">式中:<i>T</i>表示帧数;<mathml id="112"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">X</mi><mo>^</mo></mover></math></mathml>表示估计的多通道目标信号向量,每一维对应每个通道的估计目标信号<mathml id="113"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>X</mi><mo>^</mo></mover><msub><mrow></mrow><mrow><mi>i</mi><mo>,</mo><mi>t</mi><mo>,</mo><mi>f</mi></mrow></msub></mrow></math></mathml>,如式(13)所示。</p>
                </div>
                <div class="p1">
                    <p id="82"><mathml id="114"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>X</mi><mo>^</mo></mover><msub><mrow></mrow><mrow><mi>i</mi><mo>,</mo><mi>t</mi><mo>,</mo><mi>f</mi></mrow></msub><mo>=</mo><mi>Μ</mi><msub><mrow></mrow><mrow><mi>i</mi><mo>,</mo><mi>t</mi><mo>,</mo><mi>f</mi></mrow></msub><mi>Y</mi><msub><mrow></mrow><mrow><mi>i</mi><mo>,</mo><mi>t</mi><mo>,</mo><mi>f</mi></mrow></msub></mrow></math></mathml>      (13)</p>
                </div>
                <div class="p1">
                    <p id="83">在使用BLSTM1网络获得每个通道掩蔽值后,进行波束成形得到的波束成形信号<i>O</i><sub>1</sub>,由式(14)所示。</p>
                </div>
                <div class="p1">
                    <p id="84"><i>O</i><sub>1</sub>(<i>t</i>,<i>f</i>)=<i><b>w</b></i>(<i>f</i>)<sup>H</sup><i><b>Y</b></i>(<i>t</i>,<i>f</i>)(14)</p>
                </div>
                <h4 class="anchor-tag" id="85" name="85"><b>1.3 联合频谱和方向特征再次估计掩蔽值</b></h4>
                <div class="p1">
                    <p id="86">BLSTM<sub>2</sub>网络使用方向特征这种更复杂的空间特征,与频谱特征联合估计掩蔽值,BLSTM<sub>2</sub>网络结构和BLSTM<sub>1</sub>的相同。使用方向特征<i>DF</i>(<i>t</i>,<i>f</i>)这种高级空间特征作为网络的输入,方向特征由式(15)计算<citation id="205" type="reference"><link href="164" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="87"><mathml id="115"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>D</mi><mi>F</mi><mo stretchy="false">(</mo><mi>t</mi><mo>,</mo><mi>f</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><mi>Ρ</mi></mfrac><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">|</mo><mi>i</mi><mo>≠</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></munder><mrow><mi>cos</mi></mrow></mstyle><mo stretchy="false">{</mo><mo>∠</mo><mi>Y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mo>∠</mo><mi>Y</mi><msub><mrow></mrow><mi>j</mi></msub><mo>-</mo><mo stretchy="false">(</mo><mo>∠</mo><mover accent="true"><mi>a</mi><mo>^</mo></mover><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mo>∠</mo><mover accent="true"><mi>a</mi><mo>^</mo></mover><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">}</mo></mrow></math></mathml>      (15)</p>
                </div>
                <div class="p1">
                    <p id="88">式中:<i>P</i>=<i>m</i>-1是所有的麦克风对数;∠<i>Y</i><sub><i>i</i></sub>-∠<i>Y</i><sub><i>j</i></sub>是两个麦克风信号之间的相位差;<mathml id="116"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo>∠</mo><mover accent="true"><mi>a</mi><mo>^</mo></mover><msub><mrow></mrow><mi>i</mi></msub></mrow></math></mathml>是从估计的导向矢量<mathml id="117"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">a</mi><mo>^</mo></mover></math></mathml>中提取的相位部分;<mathml id="118"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo>∠</mo><mover accent="true"><mi>a</mi><mo>^</mo></mover><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mo>∠</mo><mover accent="true"><mi>a</mi><mo>^</mo></mover><msub><mrow></mrow><mi>j</mi></msub></mrow></math></mathml>表示估计的通道<i>i</i>,<i>j</i>语音信号在频率<i>f</i>处估计的相位差。方向特征能够衡量信号是否来自估计的目标方向,在目标信号主导的时频点,<i>DF</i>(<i>t</i>,<i>f</i>)接近1,在噪声主导的时频点,<i>DF</i>(<i>t</i>,<i>f</i>)接近0。波束成形的信号也可以作为一种方向特征。</p>
                </div>
                <div class="p1">
                    <p id="89">对于参考麦克风<i>r</i>,使用BLSTM2网络再一次估计掩蔽值<mathml id="119"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>Μ</mi><mo>^</mo></mover><msub><mrow></mrow><mi>r</mi></msub><mo stretchy="false">(</mo><mi>t</mi><mo>,</mo><mi>f</mi><mo stretchy="false">)</mo></mrow></math></mathml>,然后与波束成形信号相位结合,最终增强的信号<i>O</i><sub>2</sub>(<i>t</i>,<i>f</i>),如式(16)所示。</p>
                </div>
                <div class="p1">
                    <p id="90"><mathml id="120"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ο</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">(</mo><mi>t</mi><mo>,</mo><mi>f</mi><mo stretchy="false">)</mo><mo>=</mo><mover accent="true"><mi>Μ</mi><mo>^</mo></mover><msub><mrow></mrow><mi>r</mi></msub><mo stretchy="false">(</mo><mi>t</mi><mo>,</mo><mi>f</mi><mo stretchy="false">)</mo><mrow><mo>|</mo><mrow><mi>Y</mi><msub><mrow></mrow><mi>r</mi></msub><mo stretchy="false">(</mo><mi>t</mi><mo>,</mo><mi>f</mi><mo stretchy="false">)</mo></mrow><mo>|</mo></mrow><mtext>e</mtext><msup><mrow></mrow><mrow><mi>j</mi><mo>∠</mo><mi>Ο</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">(</mo><mi>t</mi><mo>,</mo><mi>f</mi><mo stretchy="false">)</mo></mrow></msup></mrow></math></mathml>      (16)</p>
                </div>
                <h3 id="91" name="91" class="anchor-tag"><b>2 实验设置及实验结果</b></h3>
                <div class="p1">
                    <p id="92">在CHiME-4挑战<citation id="206" type="reference"><link href="180" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>数据集上进行实验,对本文算法进行评估。该数据集包含训练集,验证集,测试集,每部分都包含真实数据和仿真数据。真实数据的是使用包含6个麦克风的麦克风阵列在公交车(BUS)、咖啡厅(CAF)、步行区(PED)、 街道(STR)这4种场景采集得到,采样率为16 kHz。说话的语句来自WSJ0数据集<citation id="207" type="reference"><link href="182" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>,仿真数据由文献<citation id="208" type="reference">[<a class="sup">20</a>]</citation>提供的方法仿真得到。</p>
                </div>
                <div class="p1">
                    <p id="93">对每个通道的语音信号进行分帧,其中帧长是512个采样点,帧移是128个采样点,使用Hamming窗加窗,做512点的FFT得到信号的STFT形式。BLSTM<sub>1</sub>和BLSTM<sub>2</sub>网络的结构相同,都是512×2-257,由2个包含512个节点、激活函数为ReLU的BLSTM层,和一个包含257个节点、激活函数为Sigmoid的输出层构成。BLSTM<sub>1</sub>的输入是频谱特征和IPD特征一共514维,BLSTM<sub>2</sub>的输入是DF特征,输入信号的频谱特征,波束成形信号的频谱特征,一共771维。在训练阶段,使用训练集的仿真数据对两个网络进行训练。对于BLSTM<sub>1</sub>网络,每两个通道的语音数据为一组,计算第一个通道的对数幅度谱(257维)和IPD特征(257维)共514维作为网络的输入,同时计算对应的掩蔽值为训练目标。对于BLSTM<sub>2</sub>网络,仿真数据按照式(8)计算波束成形器的权向量<i><b>w</b></i>,然后计算波束成形的信号,对参考麦克风,计算方向特征,波束成形信号频谱特征,参考信号频谱特征作为网络的输入,进行训练。在神经网络的训练过程中,使用Adam算法优化训练,使用交叉验证的方法防止过拟合。CHiME4挑战的评价指标是词错误率(word error rate, WER),在进行多通道语音增强后进行语音识别,获得识别结果的词错误率,WER越小代表效果越好。同时使用语音质量感知评估(perceptual evaluation of speech quality, PESQ)评价语音的质量,数值越大代表效果越好。</p>
                </div>
                <div class="p1">
                    <p id="94">使用数据增强后的DNN声学模型和RNN语言模型进行语音识别。不同的系统在验证集和测试集的语音识别结果如表1所示。BeamformIt!<citation id="209" type="reference"><link href="186" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>是官方使用的增强方法,是一种典型的传统波束成形方法。BLSTM+GEV是文献<citation id="210" type="reference">[<a class="sup">6</a>]</citation>中使用的方法,使用神经网络估计掩蔽值与波束成形结合,可以看出与传统方法相比WER从8.98%降低到5.72%,效果提升明显。使用IPD特征估计PSM,WER降低到4.97%。文献<citation id="211" type="reference">[<a class="sup">10</a>]</citation>中使用空间特征与频谱特征结合估计掩蔽值,进行二次波束成形,WER为4.54%,而本文算法使用初级空间特征IPD和波束成形初步增强语音,接着使用基于方向特征的神经网络再次估计掩蔽值与波束成形信号相位结合,WER降低到4.23%,与仅使用幅度谱的系统相比,WER相对降低27.6%,与文献<citation id="212" type="reference">[<a class="sup">10</a>]</citation>相比,WER相对降低6.3%。可以看出实验结果验证了本文算法的有效性。</p>
                </div>
                <div class="area_img" id="95">
                    <p class="img_tit"><b>表1 不同系统在验证集和测试集的WER</b> (%) <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="95" border="1"><tr><td rowspan="2"><br />系统</td><td colspan="2"><br />验证集</td><td colspan="2">测试集</td></tr><tr><td><br />仿真</td><td>真实</td><td>仿真</td><td>真实</td></tr><tr><td><br />BeamformIt!</td><td>4.55</td><td>5.48</td><td>8.49</td><td>8.98</td></tr><tr><td><br />BLSTM+GEV</td><td>3.93</td><td>3.95</td><td>4.43</td><td>5.72</td></tr><tr><td><br />BLSTM<sub>1</sub>+MVDR(PSM)</td><td>4.02</td><td>3.62</td><td>4.63</td><td>4.97</td></tr><tr><td><br />BLSTM<sub>1</sub>+MVDR+BLSTM<sub>2</sub></td><td>3.79</td><td>3.28</td><td>4.24</td><td>4.23</td></tr><tr><td><br />文献[10]</td><td>3.90</td><td>3.11</td><td>4.33</td><td>4.54</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="96">不同系统在测试集仿真数据上的PESQ分数如表2所示,可以看出联合空间特征和频谱特征估计时间频率掩蔽值以及使用神经网络再次估计掩蔽值然后与波束成形信号相位结合,能够提升PESQ分数,提高语音的质量。</p>
                </div>
                <div class="area_img" id="97">
                    <p class="img_tit"><b>表2 不同系统在测试集仿真数据的PESQ</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="97" border="1"><tr><td><br />系统</td><td>BUS</td><td>CAF</td><td>PED</td><td>STR</td><td>平均</td></tr><tr><td><br />BeamformIt!</td><td>2.34</td><td>2.10</td><td>2.15</td><td>2.21</td><td>2.20</td></tr><tr><td><br />BLSTM+GEV</td><td>2.62</td><td>2.33</td><td>2.41</td><td>2.48</td><td>2.46</td></tr><tr><td><br />BLSTM1+MVDR(PSM)</td><td>2.82</td><td>2.61</td><td>2.75</td><td>2.71</td><td>2.72</td></tr><tr><td><br />BLSTM1+MVDR+BLSTM2</td><td>2.89</td><td>2.67</td><td>2.85</td><td>2.83</td><td>2.81</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h3 id="98" name="98" class="anchor-tag"><b>3 结  论</b></h3>
                <div class="p1">
                    <p id="99">本文提出了一种联合频谱特征和空间特征的深度学习多通道语音增强算法,充分利用多通道信号包含的时间、频率、空间信息。通过在CHiME4的6通道数据集上进行的实验表明,本文算法能带来更好的增强效果,降低语音识别的词错误率,提高语音的质量。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="146">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Signal enhancement using beamforming and nonstationarity with applications to speech">

                                <b>[1]</b> GANNOT S,BURSHTEIN D,WEINSTEIN E.Signal enhancement using beamforming and nonstationarity with applications to speech[J].IEEE Transactions on Signal Processing,2001,49(8):1614-1626.
                            </a>
                        </p>
                        <p id="148">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=An improved signal subspace algorithm for speech enhancement">

                                <b>[2]</b> DAI,X Z,YU B X,DAI X H.An improved signal Subspace algorithm for speech enhancement[J].Conference on e-Business,e-Services and e-Society,2014:104-114.
                            </a>
                        </p>
                        <p id="150">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The NTT CHiME-3 system:Advances in speech enhancement and recognition for mobile multi-microphone devices">

                                <b>[3]</b> YOSHIOKA T,ITO N,DELCROIX M,et al.The NTT CHiME-3 system:Advances in speech enhancement and recognition for mobile multi-microphone devices[C].2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU),2015:436-443.
                            </a>
                        </p>
                        <p id="152">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201606003&amp;v=MTM0MjdmWnVadEZ5bmdVcjNCS0NMZlliRzRIOWZNcVk5Rlo0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3E=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b> 刘文举,聂帅,梁山,等.基于深度学习语音分离技术的研究现状与进展[J].自动化学报,2016,42(6):819-833.
                            </a>
                        </p>
                        <p id="154">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The USTC-iFlytek system for CHiME-4 challenge">

                                <b>[5]</b> DU J,TU Y H,SUN L,et al.The USTC-iFlytek system for CHiME-4 challenge[C].Proc.CHiME,2016:36-38.
                            </a>
                        </p>
                        <p id="156">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=BLSTM supported GEV beamformer front-end for the 3rd CHiME challenge">

                                <b>[6]</b> HEYMANN J,DRUDE L,CHINAEV A,et al.BLSTM supported GEV beamformer front-end for the 3rd CHiME challenge[C].2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU).IEEE,2015:444-451.
                            </a>
                        </p>
                        <p id="158">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A speech enhancement algorithm by iterating single-and multi-microphone processing and its application to robust ASR">

                                <b>[7]</b> ZHANG X L,WANG Z Q,WANG D L.A speech enhancement algorithm by iterating single-and multi-microphone processing and its application to robust ASR[C].2017 IEEE International Conference on Acoustics,Speech and Signal Processing (ICASSP),2017:276-280.
                            </a>
                        </p>
                        <p id="160">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCMBAF48FEDAF7D6EDA80E930D2A5EA1704&amp;v=MjQzNzNiMit3cUU9TmlmSVk4SEphTlhFMmZveEZaMEllSHBNdTJjYjZrcDBTMytXcm1Nd0RNT1ZRcnFiQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQnJMVTA1dHRoeA==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b> JIANG Y,WANG D L,LIU R S,et al.Binaural classification for reverberant speech segregation using deep neural networks[J].IEEE/ACM Transactions on Audio,Speech and Language Processing (TASLP),2014,22(12):2112-2121.
                            </a>
                        </p>
                        <p id="162">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Exploring multi-channel features for denoising-autoencoder-based speech enhancement">

                                <b>[9]</b> ARAKI S,HAYASHI T,DELCROIX M,et al.Exploring multi-channel features for denoising-autoencoder-based speech enhancement[C].2015 IEEE International Conference on Acoustics,Speech and Signal Processing (ICASSP),2015:116-120.
                            </a>
                        </p>
                        <p id="164">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=On spatial features for supervised speech separation and its application to beamforming and robust ASR">

                                <b>[10]</b> WANG Z Q,WANG D L.On spatial features for supervised speech separation and its application to beamforming and robust ASR[C].2018 IEEE International Conference on Acoustics,Speech and Signal Processing (ICASSP).IEEE,2018:5709-5713.
                            </a>
                        </p>
                        <p id="166">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=All-neural multi-channel speech enhancement">

                                <b>[11]</b> WANG Z Q,WANG D L.All-neural multi-channel speech enhancement[C].Proc.Interspeech,2018:3234-3238.
                            </a>
                        </p>
                        <p id="168">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Robust adaptive beamforming">

                                <b>[12]</b> LI J,STOICA P.Robust adaptive beamforming[M].John Wiley &amp; Sons,2005.
                            </a>
                        </p>
                        <p id="170">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Integrating spectral and spatial features for multi-channel speaker separation">

                                <b>[13]</b> WANG Z Q,WANG D L.Integrating spectral and spatial features for multi-channel speaker separation[C].Proc.Interspeech,2018:2718-2722.
                            </a>
                        </p>
                        <p id="172">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multi-microphone neural speech separation for far-field multi-talker speech recognition">

                                <b>[14]</b> YOSHIOKA T,ERDOGAN H,CHEN Z,et al.Multi-microphone neural speech separation for far-field multi-talker speech recognition[C].2018 IEEE International Conference on Acoustics,Speech and Signal Processing (ICASSP),2018:5739-5743.
                            </a>
                        </p>
                        <p id="174">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Phase-sensitive and recognition-boosted speech separation using deep recurrent neural networks">

                                <b>[15]</b> ERDOGAN H,HERSHEY J R,WATANABE S,et al.Phase-sensitive and recognition-boosted speech separation using deep recurrent neural networks[C].2015 IEEE International Conference on Acoustics,Speech and Signal Processing (ICASSP),2015:708-712.
                            </a>
                        </p>
                        <p id="176">
                            <a id="bibliography_16" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZCL201707017&amp;v=MjcyNjM0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVadEZ5bmdVcjNCSVRmSVlyRzRIOWJNcUk5RVk0UUtESDg=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[16]</b> 姚远,王秋菊,周伟,等.改进谱减法结合神经网络的语音增强研究[J].电子测量技术,2017,40(7):75-79
                            </a>
                        </p>
                        <p id="178">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Mask weighted STFT ratios for relative transfer function estimation and its application to robust ASR">

                                <b>[17]</b> WANG Z Q,WANG D L.Mask weighted STFT ratios for relative transfer function estimation and its application to robust ASR[C].2018 IEEE International Conference on Acoustics,Speech and Signal Processing (ICASSP),2018:5619-5623.
                            </a>
                        </p>
                        <p id="180">
                            <a id="bibliography_18" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESF0BA43456210468C8291D24415A16665&amp;v=MDU0MDRZZk9HUWxmQnJMVTA1dHRoeGIyK3dxRT1OaWZPZmNXNGJLRElySXRBWXVrT0RIZy94MlViNkRaOFBIM21xQk13Q0xPU1E3eWFDT052RlNpV1dyN0pJRnBtYUJ1SA==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[18]</b> VINCENG E,WATANABE S,NUGRAHA A A,et al.An analysis of environment,microphone and data simulation mismatches in robust speech recognition[J].Computer Speech &amp; Language,2017,46:535-557.
                            </a>
                        </p>
                        <p id="182">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=CSR-I  (WSJ0) Complete[CP]">

                                <b>[19]</b> GAROFALO J,GRAFF D,PAUL D,et al.CSR-I (WSJ0) Complete[CP].Linguistic Data Consortium,2007.
                            </a>
                        </p>
                        <p id="184">
                            <a id="bibliography_20" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012300650740&amp;v=MDg3NzZmYks3SHRET3JJOUZZdTRQQzNnNW9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaW5sVXJ6SUlWc1ZhQnM9TmlmTw==&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[20]</b> VINCENT E,GRIBONVAL R,PLUMBLEY M D.Oracle estimators for the benchmarking of source separation algorithms[J].Signal Processing,2007,87(8):1933-1950.
                            </a>
                        </p>
                        <p id="186">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Acoustic Beamforming for Speaker Diarization of Meetings">

                                <b>[21]</b> ANGUERA X,WOOTERS C,HERNANDO J.Acoustic beamforming for speaker diarization of meetings[J].IEEE Transactions on Audio,Speech,and Language Processing,2007,15(7):2011-2022.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="DZCL201918016" />
        <input id="dpi" type="hidden" value="600" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZCL201918016&amp;v=MDkwMjg2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnRGeW5nVXIzQklUZklZckc0SDlqTnA0OUVZb1FLREg4NHZSNFQ=&amp;uid=WEEvREcwSlJHSldRa1Fhb09pSnNveUxRekc1UDhybXA2TzF5RHYzd3RWOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="0" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>


    <link href="/kxreader/Content/css/LeftDetail?v=NLcKG8I1SJUaVFrQ0iGpF2klAT0OsmHRaVSZ1rKb5xg1" rel="stylesheet"/>

</body>
</html>

