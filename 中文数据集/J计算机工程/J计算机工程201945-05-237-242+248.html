<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637130533309873750%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJC201905039%26RESULT%3d1%26SIGN%3dsdhtYo%252fXCJJs6pwMqWgoU4nxIic%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201905039&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201905039&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201905039&amp;v=MjE1MDY0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSb0Z5M21WcnpMTHo3QmJiRzRIOWpNcW85R2JZUUtESDg0dlI=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#33" data-title="0 概述 ">0 概述</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#38" data-title="1 问题描述 ">1 问题描述</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#48" data-title="2 基于深度强化学习的会话调度策略 ">2 基于深度强化学习的会话调度策略</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#52" data-title="2.1 会话调度模型">2.1 会话调度模型</a></li>
                                                <li><a href="#58" data-title="2.2 会话调度的强化学习模型">2.2 会话调度的强化学习模型</a></li>
                                                <li><a href="#79" data-title="2.3 强化学习的调度算法">2.3 强化学习的调度算法</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#113" data-title="3 实验结果与分析 ">3 实验结果与分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#114" data-title="3.1 参数设定">3.1 参数设定</a></li>
                                                <li><a href="#117" data-title="3.2 结果分析">3.2 结果分析</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#129" data-title="4 结束语 ">4 结束语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#55" data-title="&lt;b&gt;图1 基于深度强化学习的会话调度模型&lt;/b&gt;"><b>图1 基于深度强化学习的会话调度模型</b></a></li>
                                                <li><a href="#88" data-title="&lt;b&gt;图2 强化学习策略网络模型&lt;/b&gt;"><b>图2 强化学习策略网络模型</b></a></li>
                                                <li><a href="#92" data-title="&lt;b&gt;图3 强化学习&lt;i&gt;Q&lt;/i&gt;网络模型&lt;/b&gt;"><b>图3 强化学习<i>Q</i>网络模型</b></a></li>
                                                <li><a href="#120" data-title="&lt;b&gt;图4 Zipf分布下用户请求接收率与系统负载间的关系&lt;/b&gt;"><b>图4 Zipf分布下用户请求接收率与系统负载间的关系</b></a></li>
                                                <li><a href="#121" data-title="&lt;b&gt;图5 Zipf分布下总迁移会话数与系统负载间的关系&lt;/b&gt;"><b>图5 Zipf分布下总迁移会话数与系统负载间的关系</b></a></li>
                                                <li><a href="#122" data-title="&lt;b&gt;图6 Zipf分布下运行时间与系统负载间的关系&lt;/b&gt;"><b>图6 Zipf分布下运行时间与系统负载间的关系</b></a></li>
                                                <li><a href="#125" data-title="&lt;b&gt;图7 随机均匀分布下用户请求接收率与系统负载间的关系&lt;/b&gt;"><b>图7 随机均匀分布下用户请求接收率与系统负载间的关系</b></a></li>
                                                <li><a href="#126" data-title="&lt;b&gt;图8 随机均匀分布下总迁移会话数与系统负载间的关系&lt;/b&gt;"><b>图8 随机均匀分布下总迁移会话数与系统负载间的关系</b></a></li>
                                                <li><a href="#127" data-title="&lt;b&gt;图9 随机均匀分布下运行时间与系统负载间的关系&lt;/b&gt;"><b>图9 随机均匀分布下运行时间与系统负载间的关系</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" title=" 姜同全, 王子磊, 奚宏生, 等.基于动态阈值分配的流媒体边缘云会话迁移策略[J].计算机工程, 2017, 43 (1) :55-60." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201701010&amp;v=MzE3NjY0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSb0Z5M21WcnpMTHo3QmJiRzRIOWJNcm85RVpJUUtESDg0dlI=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         姜同全, 王子磊, 奚宏生, 等.基于动态阈值分配的流媒体边缘云会话迁移策略[J].计算机工程, 2017, 43 (1) :55-60.
                                    </a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" title=" WANG Feng, LIU Jiangchuan, CHEN Minghua.CALMS:cloud-assisted live media streaming for globalized demands with time/region diversities[C]//Proceedings of IEEE International Conference on Computer Communications.Washington D.C., USA:IEEE Press, 2012:199-207." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=CALMS:Cloud-assisted live media streaming for globalized demands with time/region diversities">
                                        <b>[2]</b>
                                         WANG Feng, LIU Jiangchuan, CHEN Minghua.CALMS:cloud-assisted live media streaming for globalized demands with time/region diversities[C]//Proceedings of IEEE International Conference on Computer Communications.Washington D.C., USA:IEEE Press, 2012:199-207.
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title=" WOLF J L, YU P S, SHACHNAI H.Disk load balancing for video-on-demand systems[J].Multimedia Systems, 1997, 5 (6) :358-370." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002578319&amp;v=MDAxMTN1ZHRGQzdsVnJ6TkkxMD1OajdCYXJPNEh0SE9xb2hOWitvR1kzazV6QmRoNGo5OVNYcVJyeG94Y01IN1I3cWVi&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         WOLF J L, YU P S, SHACHNAI H.Disk load balancing for video-on-demand systems[J].Multimedia Systems, 1997, 5 (6) :358-370.
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" title=" SUTTON R S, PRECUP D, SINGH S.Between MDPs and semi-MDPs:a framework for temporal abstraction in reinforcement learning[J].Artificial Intelligence, 1999, 112 (1/2) :181-211." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011702007226&amp;v=Mjg2MThuSUpWMFJhUkU9TmlmT2ZiSzdIdEROcUk5SFpPc0lEbjQvb0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VTA==&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                         SUTTON R S, PRECUP D, SINGH S.Between MDPs and semi-MDPs:a framework for temporal abstraction in reinforcement learning[J].Artificial Intelligence, 1999, 112 (1/2) :181-211.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" title=" MIYAZAWA T, KAFLE V P, HARAI H.Reinforcement learning based dynamic resource migration for virtual networks[C]//Proceedings of Symposium on Integrated Network and Service Management.Washington D.C., USA:IEEE Press, 2017:428-434." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Reinforcement learning based dynamic resource migration for virtual networks">
                                        <b>[5]</b>
                                         MIYAZAWA T, KAFLE V P, HARAI H.Reinforcement learning based dynamic resource migration for virtual networks[C]//Proceedings of Symposium on Integrated Network and Service Management.Washington D.C., USA:IEEE Press, 2017:428-434.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" title=" WANG Jinzhi, QU Shuhui, WANG Jie, et al.Real-time decision support with reinforcement learning for dynamic flowshop scheduling[C]//Proceedings of European Conference on Smart Objects, Systems and Technologies.Munich, Germany:[s.n.], 2017:1-9." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Real-time decision support with reinforcement learning for dynamic flowshop scheduling">
                                        <b>[6]</b>
                                         WANG Jinzhi, QU Shuhui, WANG Jie, et al.Real-time decision support with reinforcement learning for dynamic flowshop scheduling[C]//Proceedings of European Conference on Smart Objects, Systems and Technologies.Munich, Germany:[s.n.], 2017:1-9.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" title=" PENG Zhiping, CUI Delong, MA Yuanjia, et al.A reinforcement learning-based mixed job scheduler scheme for cloud computing under SLA constraint[C]// Proceedings of the 3rd International Conference on Cyber Security and Cloud Computing.Washington D.C., USA:IEEE Press, 2016:142-147." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A reinforcement learning-based mixed job scheduler scheme for cloud computing under SLA constraint">
                                        <b>[7]</b>
                                         PENG Zhiping, CUI Delong, MA Yuanjia, et al.A reinforcement learning-based mixed job scheduler scheme for cloud computing under SLA constraint[C]// Proceedings of the 3rd International Conference on Cyber Security and Cloud Computing.Washington D.C., USA:IEEE Press, 2016:142-147.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" title=" ZHAO Yang, XIAO Mingqing, GE Yawei.Dynamic resource scheduling of cloud-based automatic test system using reinforcement learning[C]//Proceedings of the 13th IEEE International Conference on Electronic Measurement and Instruments.Washington D.C., USA:IEEE Press, 2017:159-165." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Dynamic resource scheduling of cloud-based automatic test system using reinforcement learning">
                                        <b>[8]</b>
                                         ZHAO Yang, XIAO Mingqing, GE Yawei.Dynamic resource scheduling of cloud-based automatic test system using reinforcement learning[C]//Proceedings of the 13th IEEE International Conference on Electronic Measurement and Instruments.Washington D.C., USA:IEEE Press, 2017:159-165.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" title=" WANG Y C, USHER J M.Application of reinforcement learning for agent-based production scheduling[J].Engineering Applications of Artificial Intelligence, 2005, 18 (1) :73-82." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501714417&amp;v=MDU2MTNSZEdlcnFRVE1ud1plWnVIeWptVUxuSUpWMFJhUkU9TmlmT2ZiSzdIdEROcW85RVkrb0xDSDArb0JNVDZUNFBRSC9pcg==&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                         WANG Y C, USHER J M.Application of reinforcement learning for agent-based production scheduling[J].Engineering Applications of Artificial Intelligence, 2005, 18 (1) :73-82.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_10" title=" MNIH V, KAVUKCUOGLU K, SILVER D, et al.Human-level control through deep reinforcement learning[J].Nature, 2015, 518 (7540) :529-533." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Human-Level control through deep reinforcement learning">
                                        <b>[10]</b>
                                         MNIH V, KAVUKCUOGLU K, SILVER D, et al.Human-level control through deep reinforcement learning[J].Nature, 2015, 518 (7540) :529-533.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_11" title=" SILVER D, HUANG A, MADDISON C J, et al.Mastering the game of go with deep neural networks and tree search[J].Nature, 2016, 529 (7587) :484-489." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Mastering the game of Go with deep neural networks and tree search">
                                        <b>[11]</b>
                                         SILVER D, HUANG A, MADDISON C J, et al.Mastering the game of go with deep neural networks and tree search[J].Nature, 2016, 529 (7587) :484-489.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_12" title=" LILLICRAP T P, HUNT J J, PRITZEL A, et al.Continuous control with deep reinforcement learning[EB/OL].[2018-02-08].https://arxiv.org/pdf/1509.02971.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Continuous Control with Deep Reinforcement Learning[C/OL]">
                                        <b>[12]</b>
                                         LILLICRAP T P, HUNT J J, PRITZEL A, et al.Continuous control with deep reinforcement learning[EB/OL].[2018-02-08].https://arxiv.org/pdf/1509.02971.pdf.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_13" title=" 李军, 倪宏, 王玲芳, 等.流媒体系统中基于请求迁移的任务调度算法[J].吉林大学学报 (工学版) , 2015, 45 (3) :938-945." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JLGY201503037&amp;v=MTkxMTh0R0ZyQ1VSTE9lWmVSb0Z5M21WcnpMTHlITWQ3RzRIOVRNckk5R1k0UUtESDg0dlI0VDZqNTRPM3pxcUI=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[13]</b>
                                         李军, 倪宏, 王玲芳, 等.流媒体系统中基于请求迁移的任务调度算法[J].吉林大学学报 (工学版) , 2015, 45 (3) :938-945.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_14" title=" 温暖, 刘正华, 祝令谱, 等.深度强化学习在变体飞行器自主外形优化中的应用[J].宇航学报, 2017, 38 (11) :1153-1159." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=YHXB201711003&amp;v=Mjc2MTZaNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm9GeTNtVnJ6TFBDWFRiTEc0SDliTnJvOUY=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[14]</b>
                                         温暖, 刘正华, 祝令谱, 等.深度强化学习在变体飞行器自主外形优化中的应用[J].宇航学报, 2017, 38 (11) :1153-1159.
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_15" title=" CHEN Liang, ZHOU Yipeng, CHIU D M.Smart streaming for online video services[J].IEEE Transactions on Multimedia, 2015, 17 (4) :485-497." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Smart streaming for online video services">
                                        <b>[15]</b>
                                         CHEN Liang, ZHOU Yipeng, CHIU D M.Smart streaming for online video services[J].IEEE Transactions on Multimedia, 2015, 17 (4) :485-497.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJC" target="_blank">计算机工程</a>
                2019,45(05),237-242+248 DOI:10.19678/j.issn.1000-3428.0050875            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于深度强化学习的流媒体边缘云会话调度策略</b></span>
 <span class="shoufa"></span>                                     </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BE%90%E8%A5%BF%E5%BB%BA&amp;code=39426791&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">徐西建</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E5%AD%90%E7%A3%8A&amp;code=09509616&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王子磊</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%A5%9A%E5%AE%8F%E7%94%9F&amp;code=05968468&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">奚宏生</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E7%A7%91%E5%AD%A6%E6%8A%80%E6%9C%AF%E5%A4%A7%E5%AD%A6%E8%87%AA%E5%8A%A8%E5%8C%96%E7%B3%BB&amp;code=0002522&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国科学技术大学自动化系</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>在流媒体边缘云资源调度中, 传统启发式方法或规划方法多数存在自适应性不足、时间复杂度高等问题。基于迁移代价、负载均衡等约束, 提出一种流媒体边缘云会话调度策略。以流媒体边缘云系统的状态信息作为属性特征, 结合深度学习与确定性策略进行梯度强化学习, 以解决用户请求接入问题。实验结果表明, 该策略具有较好的请求接入效果, 且能够降低迁移代价, 同时缩短了运行时间。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B5%81%E5%AA%92%E4%BD%93%E8%BE%B9%E7%BC%98%E4%BA%91&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">流媒体边缘云;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%BC%9A%E8%AF%9D%E8%B0%83%E5%BA%A6&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">会话调度;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%BC%9A%E8%AF%9D%E8%BF%81%E7%A7%BB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">会话迁移;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">强化学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%A1%AE%E5%AE%9A%E6%80%A7%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">确定性策略梯度;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    徐西建 (1992—) , 男, 硕士研究生, 主研方向为网络多媒体;;
                                </span>
                                <span>
                                    王子磊, 副教授;E-mail: zlwang@ ustc. edu. cn;
                                </span>
                                <span>
                                    奚宏生, 教授;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-03-20</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金 (61673362);</span>
                                <span>中央高校基本科研业务费专项资金 (WK3500000002);</span>
                    </p>
            </div>
                    <h1><b>Session Scheduling Strategy for Streaming Media Edge Cloud Based on Deep Reinforcement Learning</b></h1>
                    <h2>
                    <span>XU Xijian</span>
                    <span>WANG Zilei</span>
                    <span>XI Hongsheng</span>
            </h2>
                    <h2>
                    <span>Department of Automation, University of Science and Technology of China</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>In the streaming cloud edge resource scheduling, traditional heuristic methods or planning methods mostly have problems such as insufficient adaptability and high time complexity.Based on the constraints of migration cost and load balancing, a session scheduling strategy for streaming media edge cloud is proposed.The state information of the streaming media edge cloud system is used as the attribute feature, and the deep learning and the deterministic strategy are combined to carry out the gradient reinforcement learning to solve the problem of user request access.Experimental results show that the strategy has better request access effect, and can reduce the migration cost and shorten the running time.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=streaming%20Media%20Edge%20Cloud%20(MEC)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">streaming Media Edge Cloud (MEC) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=session%20scheduling&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">session scheduling;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=session%20migration&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">session migration;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=deep%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">deep learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=reinforcement%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">reinforcement learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=deterministic%20strategy%20gradient&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">deterministic strategy gradient;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-03-20</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="33" name="33" class="anchor-tag">0 概述</h3>
                <div class="p1">
                    <p id="34">近年来, 随着云计算技术的成熟, 流媒体服务正逐步向云形态转变, 即流媒体云。流媒体云通过在不同地理位置放置媒体边缘云 (Media Edge Cloud, MEC) , 将用户所请求的内容推到网络边缘, 以降低用户响应延迟并减轻骨干网络的流量负载<citation id="131" type="reference"><link href="3" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>, 同时子云内部的各个服务器节点按照云计算的方式组织起来, 能够适应系统负载变化以及用户端请求规模变化等情形, 从而有效地解决传统流媒体服务的问题。</p>
                </div>
                <div class="p1">
                    <p id="35">在流媒体边缘云内, 将系统资源虚拟化为资源池来保证服务透明性。云资源的分配是根据实际需求的规模而由云平台自动调整, 即云资源的弹性分配<citation id="132" type="reference"><link href="3" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>。因此, 如何实时地分配系统资源以满足用户需求, 是流媒体边缘云资源调度的难点<citation id="133" type="reference"><link href="5" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>。在分配资源有限的条件下, 用户请求模式的倾斜性及波动性, 会导致“会话分布不合理”问题:由于用户请求到来的随机性, 系统负载变得不均衡, 影响用户请求的接入效果。</p>
                </div>
                <div class="p1">
                    <p id="36">为解决上述问题, 国内外学者提出基于迁移的流媒体任务调度方法<citation id="134" type="reference"><link href="7" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>。文献<citation id="135" type="reference">[<a class="sup">1</a>]</citation>提出一种基于动态阈值分配的会话迁移策略, 根据流行度分布, 确定每台服务器上各类视频的会话分配阈值, 通过分配阈值来指导用户请求接入。文献<citation id="136" type="reference">[<a class="sup">4</a>]</citation>提出直接存取存储设备 (Direct Access Storage Device, DASD) 跳跃算法, 对负载相差较大的节点进行会话迁移, 以维护硬盘负载均衡。但自适应性不足, 难以根据系统运行场景进行策略调节, 且数学模型相对复杂, 计算量较大, 不能较好地解决大规模资源分配问题。强化学习以试错的机制与环境进行交互, 通过累积奖赏最大化方式学习最优策略<citation id="137" type="reference"><link href="11" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>, 广泛应用于调度领域<citation id="141" type="reference"><link href="13" rel="bibliography" /><link href="15" rel="bibliography" /><link href="17" rel="bibliography" /><link href="19" rel="bibliography" /><link href="21" rel="bibliography" /><sup>[<a class="sup">6</a>,<a class="sup">7</a>,<a class="sup">8</a>,<a class="sup">9</a>,<a class="sup">10</a>]</sup></citation>。DeepMind团队研究了深度Q网络 (Deep Q Network, DQN) 在Atari游戏中的应用<citation id="138" type="reference"><link href="23" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>和AlphaGo在围棋中的对弈<citation id="139" type="reference"><link href="25" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>, 取得了较好的效果。文献<citation id="140" type="reference">[<a class="sup">13</a>]</citation>结合确定性策略与DQN, 提出深度确定性策略梯度 (Deep Deterministic Policy Gradient, DDPG) 方法, 该方法能够解决高维输入问题, 且收敛速度较快。</p>
                </div>
                <div class="p1">
                    <p id="37">根据流媒体边缘云资源分配情况, 考虑迁移代价、负载均衡等约束, 结合深度学习与确定性策略梯度强化学习, 本文提出一种流媒体边缘云会话调度算法。</p>
                </div>
                <h3 id="38" name="38" class="anchor-tag">1 问题描述</h3>
                <div class="p1">
                    <p id="39">流媒体边缘云资源分配的关键是如何对用户请求进行有效调度。用户会话调度通过请求接入、会话迁移以及请求拒绝来实现。本文主要研究如何有效利用MEC系统的状态信息, 选取最优的会话调度策略来提高资源利用率。</p>
                </div>
                <div class="p1">
                    <p id="40">假设MEC系统所提供的视频内容有<i>I</i>种, 第<i>i</i>种视频用<i>v</i><sub><i>i</i></sub>表示, 每种视频均为恒定码率编码, 以相同码率提供服务。假设MEC流媒体服务器总数为<i>J</i>, 第<i>j</i>台服务器用<i>M</i><sub><i>j</i></sub>表示, 定义<b><i>D</i></b>作为视频部署矩阵, 大小为<i>I</i>×<i>J</i>, 元素<i>d</i><sub><i>ij</i></sub>∈{0, 1}表示<i>M</i><sub><i>j</i></sub>上是否部署了<i>v</i><sub><i>i</i></sub>的副本。假设所有服务器均同构, 并且单个服务器最多可同时提供<i>C</i>个流媒体会话, 以及最多可存储<i>S</i>份视频。定义<b><i>K</i></b>作为会话分布矩阵, 大小为<i>I</i>×<i>J</i>, 单个元素<i>k</i><sub><i>ij</i></sub>∈[0, 1]表示视频<i>v</i><sub><i>i</i></sub>在服务器<i>M</i><sub><i>j</i></sub>上的所有会话占用系统总服务能力 (<i>JC</i>) 的比值, 定义<b><i>G</i></b>为服务器邻接矩阵, 大小为<i>J</i>×<i>J</i>, 元素<mathml id="41"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>g</mi><msub><mrow></mrow><mrow><mi>n</mi><mi>j</mi></mrow></msub><mo>∈</mo><mrow><mo>{</mo><mrow><mn>0</mn><mo>, </mo><mn>1</mn></mrow><mo>}</mo></mrow></mrow></math></mathml>表示M<sub>n</sub>服务器上是否有会话可以迁移到M<sub>j</sub>服务器, 其中n=1, 2, …, J。</p>
                </div>
                <div class="p1">
                    <p id="42">定义l<sub>j</sub>为流媒体服务器M<sub>j</sub>的负载, 即总的接入会话数量, 则:</p>
                </div>
                <div class="p1">
                    <p id="43" class="code-formula">
                        <mathml id="43"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>l</mi><msub><mrow></mrow><mi>j</mi></msub><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ι</mi></munderover><mo stretchy="false"> (</mo></mstyle><mi>k</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>×</mo><mi>J</mi><mi>C</mi><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="44">定义<mathml id="45"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi>L</mi></mstyle><mrow><mspace width="0.25em" /><mo>-</mo></mrow></mover></mrow></math></mathml>为系统所有流媒体服务器的平均负载, 则:</p>
                </div>
                <div class="p1">
                    <p id="46" class="code-formula">
                        <mathml id="46"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi>L</mi></mstyle><mrow><mspace width="0.25em" /><mo>-</mo></mrow></mover><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>J</mi></munderover><mi>l</mi></mstyle><msub><mrow></mrow><mi>j</mi></msub><mo>/</mo><mi>Ν</mi><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="47">本文根据<i>MEC</i>系统的状态信息, 寻找一个智能的会话调度策略, 合理处理用户请求, 最大化用户请求接入率, 并且对迁移代价进行适当的约束控制, 同时尽可能使系统达到负载均衡。</p>
                </div>
                <h3 id="48" name="48" class="anchor-tag">2 基于深度强化学习的会话调度策略</h3>
                <div class="p1">
                    <p id="49">本文以<i>MEC</i>系统的状态信息作为属性特征, 结合深度神经网络, 通过确定性策略梯度强化学习算法接入新用户请求。</p>
                </div>
                <div class="p1">
                    <p id="50">本文会话调度过程为:在当前考虑周期内的各个时刻, 对于新的用户视频请求, 决策器根据当前<i>MEC</i>系统状态作出调度决策, 将视频请求接入到最合适的服务器。</p>
                </div>
                <div class="p1">
                    <p id="51">为使决策器的调度策略达到最优, 本文基于强化学习方法建立<i>MEC</i>系统的会话调度模型, 并且设计了状态空间、动作集合、回报函数等强化学习要素, 使用深度卷积神经网络拟合决策器和价值函数。为提升算法的效率, 本文采用确定性策略梯度进行神经网络训练。</p>
                </div>
                <h4 class="anchor-tag" id="52" name="52">2.1 会话调度模型</h4>
                <div class="p1">
                    <p id="53">对于流媒体边缘云系统, 强化学习的目标是通过大量的学习训练, 决策器根据当前<i>MEC</i>系统的状态和视频请求, 按照经验策略自主将该视频请求接入到最合适的服务器。然后根据该服务器的负载状态, 通过迁出视频策略来执行请求接入或单步会话迁移动作, 从而得到对当前新到来的用户请求的最优调度策略。</p>
                </div>
                <div class="p1">
                    <p id="54">本文将深度强化学习方法应用于流媒体边缘云会话调度, 其会话调度模型如图1所示。</p>
                </div>
                <div class="area_img" id="55">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201905039_055.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 基于深度强化学习的会话调度模型" src="Detail/GetImg?filename=images/JSJC201905039_055.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图1 基于深度强化学习的会话调度模型</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201905039_055.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="56">在图1中, 对于当前步视频请求<i>v</i><sub><i>i</i></sub>, 决策器的决策动作为将该视频请求接入到某一台服务器, 假设接入的服务器为<i>M</i><sub><i>j</i></sub>, 那么迁出视频的策略为:如果<i>M</i><sub><i>j</i></sub>未满载, 则不需要迁出视频, 设迁出视频编号为0, 对应请求接入;如果<i>M</i><sub><i>j</i></sub>处于满载状态, 则需要迁出视频, 迁出视频编号的集合为{<i>v</i><sub><i>m</i></sub>|<i>k</i><sub><i>mj</i></sub>≥1, <i>m</i>≠<i>i</i>, <i>m</i>≤<i>I</i>}, 对应单步会话迁移。</p>
                </div>
                <div class="p1">
                    <p id="57">对于该会话调度模型, 本文目标是在得到尽可能大的累积回报值的情况下, 得到对当前新到来的用户请求的最优调度方案。MEC系统将在整个周期内的各个时刻根据该调度方案处理用户请求。</p>
                </div>
                <h4 class="anchor-tag" id="58" name="58">2.2 会话调度的强化学习模型</h4>
                <div class="p1">
                    <p id="59">根据问题特点, MEC系统在<i>t</i>时间步的状态定义如下:</p>
                </div>
                <div class="p1">
                    <p id="60" class="code-formula">
                        <mathml id="60"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>s</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mrow><mo>[</mo><mrow><mi mathvariant="bold-italic">G</mi><msub><mrow></mrow><mi>t</mi></msub><mo>;</mo><mi mathvariant="bold-italic">R</mi><msub><mrow></mrow><mi>t</mi></msub><mo>;</mo><mi mathvariant="bold-italic">D</mi><msub><mrow></mrow><mi>t</mi></msub><mo>;</mo><mi mathvariant="bold-italic">Κ</mi><msub><mrow></mrow><mi>t</mi></msub></mrow><mo>]</mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="61">其中, <b><i>G</i></b><sub><i>t</i></sub>为服务器邻接矩阵, 大小为<i>J</i>×<i>J</i>, 表示各个服务器之间是否可以进行会话迁移, <b><i>R</i></b><sub><i>t</i></sub>为<i>t</i>时间步的视频请求矩阵, 大小为<i>I</i>×<i>J</i>, 视频请求矩阵中有一行的元素全为1, 其他元素全为0, 元素全为1的那一行对应的视频编号即为当前视频请求, <b><i>D</i></b><sub><i>t</i></sub>为视频部署矩阵, 大小为<i>I</i>×<i>J</i>, 反映MEC系统中视频副本的部署情况, <b><i>K</i></b><sub><i>t</i></sub>为会话分布矩阵, 大小也为<i>I</i>×<i>J</i>, 反映MEC系统中视频会话的分布情况。每处理完一个新的视频请求, 系统会发生一次状态转移。</p>
                </div>
                <div class="p1">
                    <p id="62">由于任务是根据当前步的MEC系统状态和视频请求决定将请求接入到哪一台服务器或者拒绝该请求, 因此动作定义为视频请求<i>v</i><sub><i>i</i></sub>接入到的服务器编号<i>M</i><sub><i>j</i></sub>, 其中, <i>i</i>=1, 2, …, <i>I</i>。针对当前步视频请求<i>v</i><sub><i>i</i></sub>, 可选的动作集合如式 (4) 所示。当<i>v</i><sub><i>i</i></sub>直接接入或通过会话迁移接入到MEC系统时, 可选动作集合为部署了视频<i>v</i><sub><i>i</i></sub>的服务器集合;当拒绝视频请求<i>v</i><sub><i>i</i></sub>时, 对应的动作为0。</p>
                </div>
                <div class="p1">
                    <p id="63" class="code-formula">
                        <mathml id="63"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>a</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mo stretchy="false">{</mo><mi>Μ</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">|</mo><mi>d</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mn>1</mn><mo>, </mo><mi>j</mi><mo>≤</mo><mi>J</mi><mo stretchy="false">}</mo><mo>, </mo><mtext>接</mtext><mtext>入</mtext></mtd></mtr><mtr><mtd><mn>0</mn><mo>, </mo><mtext>拒</mtext><mtext>绝</mtext></mtd></mtr></mtable></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="64">如果视频请求<i>v</i><sub><i>i</i></sub>根据决策动作接入到<i>M</i><sub><i>j</i></sub>服务器, 那么本文选择<i>M</i><sub><i>j</i></sub>服务器上视频<i>v</i><sub><i>i</i></sub>的部署情况、<i>M</i><sub><i>j</i></sub>服务器的负载以及执行该动作后MEC系统的负载均衡方差的值作为即时回报函数。服务器上视频部署情况、服务器的负载值和系统的负载均衡方差的取值范围不一样, 因此将系统的负载均衡方差进行归一化。定义负载均衡方差函数<citation id="142" type="reference"><link href="29" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>为:</p>
                </div>
                <div class="p1">
                    <p id="65" class="code-formula">
                        <mathml id="65"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>σ</mi><mo>=</mo><mrow><mi>arctan</mi></mrow><mrow><mo> (</mo><mrow><mfrac><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>J</mi></munderover><mrow><mo stretchy="false"> (</mo><mi>l</mi><msub><mrow></mrow><mi>j</mi></msub><mo>-</mo><mover><mstyle mathsize="140%" displaystyle="true"><mi>L</mi></mstyle><mrow><mspace width="0.25em" /><mo>-</mo></mrow></mover><mo stretchy="false">) </mo></mrow></mstyle><msup><mrow></mrow><mn>2</mn></msup></mrow><mi>J</mi></mfrac></mrow><mo>) </mo></mrow><mo>×</mo><mn>2</mn><mo>/</mo><mtext>π</mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>5</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="66">其中, <mathml id="67"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi>L</mi></mstyle><mrow><mspace width="0.25em" /><mo>-</mo></mrow></mover></mrow></math></mathml>为<i>MEC</i>系统所有流媒体服务器的平均负载, <mathml id="68"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi>L</mi></mstyle><mrow><mspace width="0.25em" /><mo>-</mo></mrow></mover><mo>=</mo><mfrac><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mi>l</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub></mrow><mi>Ν</mi></mfrac><mo>, </mo><mi>l</mi><msub><mrow></mrow><mi>j</mi></msub></mrow></math></mathml>为流媒体服务器<i>M</i><sub><i>j</i></sub>的负载, <mathml id="69"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>l</mi><msub><mrow></mrow><mi>j</mi></msub><mo>=</mo><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ι</mi></munderover><mo stretchy="false"> (</mo></mstyle><mi>k</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>×</mo><mo stretchy="false"> (</mo><mi>J</mi><mi>C</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo></mrow></mrow></math></mathml>。该函数将负载均衡方差的值映射到[0, 1]范围之间。由于负载均衡方差是方差的相反数, 式 (5) 表明负载均衡方差σ越大, 系统的负载越均衡。</p>
                </div>
                <div class="p1">
                    <p id="70">如果t时间步请求的视频为v<sub>i</sub>, 那么该步的动作a<sub>t</sub>带来的回报价值如式 (6) 所示。</p>
                </div>
                <div class="p1">
                    <p id="71" class="code-formula">
                        <mathml id="71"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>r</mi><mo stretchy="false"> (</mo><mi>s</mi><msub><mrow></mrow><mi>t</mi></msub><mo>, </mo><mi>a</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mi>ω</mi><msub><mrow></mrow><mn>1</mn></msub><mo>×</mo><mi>d</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>+</mo><mi>ω</mi><msub><mrow></mrow><mn>2</mn></msub><mo>×</mo><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>l</mi><msub><mrow></mrow><mi>j</mi></msub><mo>/</mo><mi>C</mi><mo stretchy="false">) </mo><mo>+</mo><mi>ω</mi><msub><mrow></mrow><mn>3</mn></msub><mo>×</mo><mi>σ</mi><mo>, </mo><mtext>接</mtext><mtext>入</mtext></mtd></mtr><mtr><mtd><mo>-</mo><mn>1</mn><mo>+</mo><mi>ω</mi><msub><mrow></mrow><mn>1</mn></msub><mo>×</mo><mi>d</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>+</mo><mi>ω</mi><msub><mrow></mrow><mn>3</mn></msub><mo>×</mo><mi>σ</mi><mo>, </mo><mtext>迁</mtext><mtext>移</mtext></mtd></mtr><mtr><mtd><mo>-</mo><mn>1</mn><mo>, </mo><mtext>拒</mtext><mtext>绝</mtext></mtd></mtr></mtable></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>6</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="72">其中, i=1, 2, …, I, j=1, 2, …, J, 第1行为接入情况, 当决策动作a<sub>t</sub>对应的M<sub>j</sub>服务器上部署有视频v<sub>i</sub>时, 会有相应的奖励值ω<sub>1</sub>, 如果M<sub>j</sub>服务器上未部署视频v<sub>i</sub>, 则该动作不是合理的接入动作, 不在可选的动作集合中, 奖励值为0, (1-l<sub>j</sub>/C) 表示服务器剩余的服务能力。回报函数的第2项表示回报值和服务器剩余服务能力的大小有关, 剩余服务能力越大, 相应的奖励值越大, 当服务器满载, 即剩余服务能力为0时, 奖励值为0。最后一项表示负载均衡方差的值越大, 相应的回报奖励值就会越高。第2行是迁移情况, 由于会话迁移有一定的代价, 因此回报值减1作为相应的惩罚。当动作为拒绝视频请求时, 回报值设为-1。</p>
                </div>
                <div class="p1">
                    <p id="73">M<sub>j</sub>服务器上视频v<sub>i</sub>的部署情况d<sub>ij</sub>、M<sub>j</sub>服务器的剩余服务能力 (1-l<sub>j</sub>/C) 以及执行该动作后<i>MEC</i>系统的负载均衡方差的加权平均值作为即时回报。ω<sub>1</sub>、ω<sub>2</sub>、ω<sub>3</sub>分别表示3个优化目标所带来的回报值所占的权重, 权重的值可以根据优化目标的重要程度分别进行设置, 但是3个权重的和必须满足<mathml id="74"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mn>3</mn></munderover><mi>ω</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mn>1</mn></mrow></math></mathml>。</p>
                </div>
                <div class="p1">
                    <p id="75">定义在<i>MEC</i>系统状态s<sub>t</sub>下, 采取动作a<sub>t</sub>后, 如果持续执行策略μ, 所获得即时回报的期望值为动作-价值函数, 定义<i>Bellman</i>等式如下:</p>
                </div>
                <div class="p1">
                    <p id="76" class="code-formula">
                        <mathml id="76"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Q</mi><msup><mrow></mrow><mi>μ</mi></msup><mrow><mo> (</mo><mrow><mi>s</mi><msub><mrow></mrow><mi>t</mi></msub><mo>, </mo><mi>a</mi><msub><mrow></mrow><mi>t</mi></msub></mrow><mo>) </mo></mrow><mo>=</mo><mi>E</mi><mrow><mo>[</mo><mrow><mi>r</mi><mrow><mo> (</mo><mrow><mi>s</mi><msub><mrow></mrow><mi>t</mi></msub><mo>, </mo><mi>a</mi><msub><mrow></mrow><mi>t</mi></msub></mrow><mo>) </mo></mrow><mo>+</mo><mi>γ</mi><mi>Q</mi><msup><mrow></mrow><mi>μ</mi></msup><mrow><mo> (</mo><mrow><mi>s</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>, </mo><mi>μ</mi><mrow><mo> (</mo><mrow><mi>s</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><mo>) </mo></mrow></mrow><mo>) </mo></mrow></mrow><mo>]</mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>7</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="77">其中, <mathml id="78"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>r</mi><mrow><mo> (</mo><mrow><mi>s</mi><msub><mrow></mrow><mi>t</mi></msub><mo>, </mo><mi>a</mi><msub><mrow></mrow><mi>t</mi></msub></mrow><mo>) </mo></mrow></mrow></math></mathml>为在MEC系统状态<i>s</i><sub><i>t</i></sub>下, 采取动作<i>a</i><sub><i>t</i></sub>后得到的即时回报值。在整个会话调度过程中, 上述等式即为最终求解方程, 通过求解该方程得到最优的调度策略。</p>
                </div>
                <h4 class="anchor-tag" id="79" name="79">2.3 强化学习的调度算法</h4>
                <h4 class="anchor-tag" id="80" name="80">2.3.1 行为策略选择</h4>
                <div class="p1">
                    <p id="81">本文选择确定性的行为策略, 定义一个函数μ, 其表示为:</p>
                </div>
                <div class="p1">
                    <p id="82">a<sub>t</sub>=μ (s<sub>t</sub>)      (8) </p>
                </div>
                <div class="p1">
                    <p id="83">由式 (8) 可知, 每一步的行为均可以通过μ函数计算获得, 使用卷积神经网络对μ函数进行模拟, 该网络即为策略网络, 参数为θ<sup>μ</sup>。用一个函数J (μ) 来衡量策略μ的表现, 定义如下:</p>
                </div>
                <div class="p1">
                    <p id="84">J (μ) =E[Q<sup>μ</sup> (s<sub>t</sub>, μ (s<sub>t</sub>) ) ]      (9) </p>
                </div>
                <div class="p1">
                    <p id="85">其中, s<sub>t</sub>为系统的状态, 且状态由基于行为选择策略产生, Q<sup>μ</sup> (s<sub>t</sub>, μ (s<sub>t</sub>) ) 是在每个状态下, 如果都按照策略μ选择动作a<sub>t</sub>时, 能够产生的Q值, 即J (μ) 是在策略为μ时, Q<sup>μ</sup> (s<sub>t</sub>, μ (s<sub>t</sub>) ) 的期望值。因此, 最优行为策略是使J (μ) 最大化时的策略μ, 有:</p>
                </div>
                <div class="p1">
                    <p id="86">μ=<i>argmax</i> (J (μ) )      (10) </p>
                </div>
                <div class="p1">
                    <p id="87">本文策略网络为卷积神经网络, 如图2所示。网络输入为<i>MEC</i>系统状态, 即大小均为I×J的视频请求矩阵、视频部署矩阵、会话分布矩阵和大小为J×J的服务器邻接矩阵。服务器邻接矩阵经过3个卷积层, 第1层包含8个尺寸为1×J且步长为1的长方形卷积核, 第2层包含8个尺寸为1×1且步长为1的卷积核, 第3层包含1个尺寸为1×1且步长为1的卷积核, 得到一个通道的1×J维特征向量。会话分布矩阵也经过3个卷积层, 第1层包含8个尺寸为I×1且步长为1的长方形卷积核, 第2层包含8个尺寸为1×1且步长为1的卷积核, 第3层包含1个尺寸为1×1且步长为1的卷积核, 得到一个通道的1×J维的特征向量。视频请求矩阵和视频部署矩阵的特征向量表示服务器上当前视频的部署信息, 大小为1×J。3个特征向量通过<i>concat</i>层连接起来, 通过2个卷积层和1个全连接层, 第1个卷积层包含8个尺寸为1×1且步长为1的卷积核, 第2个卷积层包含1个尺寸为1×1且步长为1的卷积核, 最后通过<i>Softmax</i>分类器得到服务器编号的概率分布, 维度为J+1。决策动作为概率最大值对应的服务器编号。</p>
                </div>
                <div class="area_img" id="88">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201905039_088.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 强化学习策略网络模型" src="Detail/GetImg?filename=images/JSJC201905039_088.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图2 强化学习策略网络模型</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201905039_088.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="89">为使探索性更强, 在该确定性策略的基础上, 加入行为搜索, 即有20%的比例是在可选动作空间随机选取动作, 剩余动作为策略网络输出的结果。</p>
                </div>
                <h4 class="anchor-tag" id="90" name="90">2.3.2 值迭代算法</h4>
                <div class="p1">
                    <p id="91">本文使用卷积神经网络对<i>Q</i>函数进行模拟, 该网络即为<i>Q</i>网络, 其参数为<i>θ</i><sup><i>Q</i></sup>, <i>Q</i>网络的模型如图3所示。</p>
                </div>
                <div class="area_img" id="92">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201905039_092.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 强化学习Q网络模型" src="Detail/GetImg?filename=images/JSJC201905039_092.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图3 强化学习<i>Q</i>网络模型</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201905039_092.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="93"><i>Q</i>网络的输入为MEC系统状态和动作向量, 动作向量为将策略网络输出的概率分布向量转化为one-hot向量的结果, 大小为1×<i>J</i>。在<i>Q</i>网络中, MEC系统状态特征提取的网络结构和策略网络中对应部分的网络结构相同, 得到大小为1×<i>J</i>的特征向量, 然后再和动作向量通过concat层连接起来, 经过一个全连接层, 得到<i>Q</i>值。</p>
                </div>
                <div class="p1">
                    <p id="94">在网络训练时输入样本数据在时间上高度关联<citation id="143" type="reference"><link href="31" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>, 直接训练不易收敛。为打破数据之间的关联性, 采用“经验回放”方法, 将生成的样本数据保存到缓存区中, 训练时采用的样本数据从该缓存区中随机抽取。</p>
                </div>
                <div class="p1">
                    <p id="95">在网络训练过程中, 本文采用目标网络方法, 建立策略网络和<i>Q</i>网络的拷贝<i>θ</i><sup><i>μ</i>′</sup>和<i>θ</i><sup><i>Q</i>′</sup>来计算目标值, 然后以<i>τ</i>的比例缓慢更新原网络<citation id="144" type="reference"><link href="31" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>。通过这种网络学习方法, 学习过程会更加稳定, 收敛更有保障。基于确定性策略梯度的强化学习算法的伪代码如算法1所示。</p>
                </div>
                <div class="p1">
                    <p id="96"><b>算法1</b> 基于确定性策略梯度的强化学习算法</p>
                </div>
                <div class="area_img" id="152">
                                <img alt="" src="Detail/GetImg?filename=images/JSJC201905039_15200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                            <p class="img_tit"></p>

                </div>
                <h3 id="113" name="113" class="anchor-tag">3 实验结果与分析</h3>
                <h4 class="anchor-tag" id="114" name="114">3.1 参数设定</h4>
                <div class="p1">
                    <p id="115">在MEC系统仿真环境下, 环境参数设置如下:流媒体服务器总数<i>J</i>=20, 每台流媒体服务器容量<i>S</i>=30, 最大服务会话数量<i>C</i>=90, MEC的视频种类数<i>I</i>=300。同时, 假设用户的请求到达率服从 <i>ξ</i>个请求每分钟的泊松分布, <i>ξ</i>的取值范围为57～63。文献[16]给出了会话播放时长的具体分布, 平均播放时长设定为30 min, 在一个影片播放时间内该系统可以并发支持1 800 (<i>JC</i>) 个用户视频请求, 因此, 当<i>ξ</i>=60 (1 800/30) 时, 系统达到满载。用户请求的视频内容则分别服从Zipf分布和随机均匀分布。</p>
                </div>
                <div class="p1">
                    <p id="116">为分析深度强化学习算法的有效性和实用性, 本文在tensorflow平台下进行编程, 并且应用在MEC系统会话调度策略中。算法参数设计如下:策略网络学习率取0.000 1, <i>Q</i>网络学习率设为0.001, 折扣系数设为0.99, 缓存区容量为1 000 000, 缓存区预热系数为1 000, 迭代次数为200 000, 时间步长上限为50步, 每个迭代的样本数量为500, 回报函数中的权重系数为<i>ω</i><sub>1</sub>=0.8、<i>ω</i><sub>2</sub>=0.1、<i>ω</i><sub>3</sub>=0.1。</p>
                </div>
                <h4 class="anchor-tag" id="117" name="117">3.2 结果分析</h4>
                <div class="p1">
                    <p id="118">本文根据第3.1节中参数设置进行深度神经网络训练, 将训练好的网络模型用于MEC系统仿真实验, 仿真时长设定为200 min。同时, 为更好地反映算法优化的效果, 在相同实验条件下, 采用文献<citation id="145" type="reference">[<a class="sup">1</a>]</citation>提出的基于动态阈值分配的流媒体边缘云会话迁移策略进行对比。本文分别将用户请求接收率、总迁移会话数和运行时间作为性能评价指标。</p>
                </div>
                <div class="p1">
                    <p id="119">设定用户请求的视频内容服从Zipf分布。图4～图6分别给出该条件下用户请求接收率、总迁移会话数量和仿真算法运行时间与系统负载之间的变化关系。</p>
                </div>
                <div class="area_img" id="120">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201905039_120.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 Zipf分布下用户请求接收率与系统负载间的关系" src="Detail/GetImg?filename=images/JSJC201905039_120.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图4 Zipf分布下用户请求接收率与系统负载间的关系</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201905039_120.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="121">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201905039_121.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 Zipf分布下总迁移会话数与系统负载间的关系" src="Detail/GetImg?filename=images/JSJC201905039_121.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图5 Zipf分布下总迁移会话数与系统负载间的关系</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201905039_121.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="122">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201905039_122.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 Zipf分布下运行时间与系统负载间的关系" src="Detail/GetImg?filename=images/JSJC201905039_122.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图6 Zipf分布下运行时间与系统负载间的关系</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201905039_122.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="123">从图4和图5可以看出, 在低负载 (<i>ξ</i>≤60) 情况下, 本文算法用户请求接收率和总迁移会话数和文献<citation id="146" type="reference">[<a class="sup">1</a>]</citation>算法基本相同;在高负载 (<i>ξ</i>&gt;60) 情况下, 本文算法用户请求接收率和总迁移会话数略低于文献<citation id="147" type="reference">[<a class="sup">1</a>]</citation>算法。与文献<citation id="148" type="reference">[<a class="sup">1</a>]</citation>算法相比, 本文算法的用户请求接收率平均下降0.144%, 总迁移会话数平均减少1.29%。该结果体现出了强化学习的优势:由于会话迁移具有代价性, 为获取更大的回报值, 决策器不断调整决策动作, 最终降低迁移代价, 同时保证了较高的请求接收率。从图6可以看出, 无论是低负载还是高负载, 对于运行时间, 本文算法均优于文献<citation id="149" type="reference">[<a class="sup">1</a>]</citation>算法, 且运行时间平均缩短了75.17%, 这是因为:在用户请求接入过程中, 文献<citation id="150" type="reference">[<a class="sup">1</a>]</citation>算法需要不断更新会话分配阈值, 导致大量的计算, 而本文使用的深度强化学习方法, 只需要通过训练好的策略网络作出调度决策, 计算量比较小, 提高了调度的效率。</p>
                </div>
                <div class="p1">
                    <p id="124">为评估本文算法的适应性, 设定用户请求的视频内容服从随机均匀分布。图7～图9分别给出了该条件下请求接收率、总迁移会话数和仿真算法运行时间与系统负载之间的变化关系。与文献<citation id="151" type="reference">[<a class="sup">1</a>]</citation>算法相比, 本文算法的用户请求接收率平均下降了0.22%, 总迁移会话数平均减少了0.66%, 且运行时间平均缩短了74.64%。实验结果表明, 本文算法具有一定的自适应性, 在用户请求的分布发生变化情况下, 仍然可以在训练过程中对调度策略作出调整, 最终获得较低的迁移代价, 较高的用户请求接收率。</p>
                </div>
                <div class="area_img" id="125">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201905039_125.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 随机均匀分布下用户请求接收率与系统负载间的关系" src="Detail/GetImg?filename=images/JSJC201905039_125.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图7 随机均匀分布下用户请求接收率与系统负载间的关系</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201905039_125.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="126">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201905039_126.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 随机均匀分布下总迁移会话数与系统负载间的关系" src="Detail/GetImg?filename=images/JSJC201905039_126.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图8 随机均匀分布下总迁移会话数与系统负载间的关系</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201905039_126.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="127">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201905039_127.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图9 随机均匀分布下运行时间与系统负载间的关系" src="Detail/GetImg?filename=images/JSJC201905039_127.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图9 随机均匀分布下运行时间与系统负载间的关系</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201905039_127.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="128">综上所述, 本文所提出的基于深度强化学习的流媒体边缘云会话调度策略不仅取得了较好的请求接入效果, 而且迁移代价较低, 更重要的是有很大的速度优势, 即运行时间较短。同时, 在不确定的MEC系统环境中具有较强的适应性。</p>
                </div>
                <h3 id="129" name="129" class="anchor-tag">4 结束语</h3>
                <div class="p1">
                    <p id="130">本文提出一种基于深度强化学习的流媒体边缘云会话调度策略。该策略将会话调度问题转化为强化学习问题, 定义状态空间、动作集合和回报函数, 利用卷积神经网络来拟合行为选择策略函数和动作-价值函数。实验结果表明, 与基于动态阈值分配的流媒体边缘云会话迁移策略相比, 该策略能够降低迁移代价, 同时缩短了运行时间。下一步将视频请求接入服务器和从该服务器上迁出视频作为强化学习策略网络的输出, 以改进流媒体边缘云会话调度策略。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201701010&amp;v=Mjc0MTQzbVZyekxMejdCYmJHNEg5Yk1ybzlFWklRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJvRnk=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> 姜同全, 王子磊, 奚宏生, 等.基于动态阈值分配的流媒体边缘云会话迁移策略[J].计算机工程, 2017, 43 (1) :55-60.
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=CALMS:Cloud-assisted live media streaming for globalized demands with time/region diversities">

                                <b>[2]</b> WANG Feng, LIU Jiangchuan, CHEN Minghua.CALMS:cloud-assisted live media streaming for globalized demands with time/region diversities[C]//Proceedings of IEEE International Conference on Computer Communications.Washington D.C., USA:IEEE Press, 2012:199-207.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002578319&amp;v=MDg1NzZOajdCYXJPNEh0SE9xb2hOWitvR1kzazV6QmRoNGo5OVNYcVJyeG94Y01IN1I3cWVidWR0RkM3bFZyek5JMTA9&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> WOLF J L, YU P S, SHACHNAI H.Disk load balancing for video-on-demand systems[J].Multimedia Systems, 1997, 5 (6) :358-370.
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011702007226&amp;v=MDMyODJzSURuNC9vQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVVMbklKVjBSYVJFPU5pZk9mYks3SHRETnFJOUhaTw==&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b> SUTTON R S, PRECUP D, SINGH S.Between MDPs and semi-MDPs:a framework for temporal abstraction in reinforcement learning[J].Artificial Intelligence, 1999, 112 (1/2) :181-211.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Reinforcement learning based dynamic resource migration for virtual networks">

                                <b>[5]</b> MIYAZAWA T, KAFLE V P, HARAI H.Reinforcement learning based dynamic resource migration for virtual networks[C]//Proceedings of Symposium on Integrated Network and Service Management.Washington D.C., USA:IEEE Press, 2017:428-434.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Real-time decision support with reinforcement learning for dynamic flowshop scheduling">

                                <b>[6]</b> WANG Jinzhi, QU Shuhui, WANG Jie, et al.Real-time decision support with reinforcement learning for dynamic flowshop scheduling[C]//Proceedings of European Conference on Smart Objects, Systems and Technologies.Munich, Germany:[s.n.], 2017:1-9.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A reinforcement learning-based mixed job scheduler scheme for cloud computing under SLA constraint">

                                <b>[7]</b> PENG Zhiping, CUI Delong, MA Yuanjia, et al.A reinforcement learning-based mixed job scheduler scheme for cloud computing under SLA constraint[C]// Proceedings of the 3rd International Conference on Cyber Security and Cloud Computing.Washington D.C., USA:IEEE Press, 2016:142-147.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Dynamic resource scheduling of cloud-based automatic test system using reinforcement learning">

                                <b>[8]</b> ZHAO Yang, XIAO Mingqing, GE Yawei.Dynamic resource scheduling of cloud-based automatic test system using reinforcement learning[C]//Proceedings of the 13th IEEE International Conference on Electronic Measurement and Instruments.Washington D.C., USA:IEEE Press, 2017:159-165.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501714417&amp;v=MjQzNDliSzdIdEROcW85RVkrb0xDSDArb0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VTG5JSlYwUmFSRT1OaWZPZg==&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b> WANG Y C, USHER J M.Application of reinforcement learning for agent-based production scheduling[J].Engineering Applications of Artificial Intelligence, 2005, 18 (1) :73-82.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Human-Level control through deep reinforcement learning">

                                <b>[10]</b> MNIH V, KAVUKCUOGLU K, SILVER D, et al.Human-level control through deep reinforcement learning[J].Nature, 2015, 518 (7540) :529-533.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Mastering the game of Go with deep neural networks and tree search">

                                <b>[11]</b> SILVER D, HUANG A, MADDISON C J, et al.Mastering the game of go with deep neural networks and tree search[J].Nature, 2016, 529 (7587) :484-489.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Continuous Control with Deep Reinforcement Learning[C/OL]">

                                <b>[12]</b> LILLICRAP T P, HUNT J J, PRITZEL A, et al.Continuous control with deep reinforcement learning[EB/OL].[2018-02-08].https://arxiv.org/pdf/1509.02971.pdf.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_13" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JLGY201503037&amp;v=MTExNzVHWTRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJvRnkzbVZyekxMeUhNZDdHNEg5VE1ySTk=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[13]</b> 李军, 倪宏, 王玲芳, 等.流媒体系统中基于请求迁移的任务调度算法[J].吉林大学学报 (工学版) , 2015, 45 (3) :938-945.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_14" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=YHXB201711003&amp;v=MTUxNjRSTE9lWmVSb0Z5M21WcnpMUENYVGJMRzRIOWJOcm85Rlo0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1U=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[14]</b> 温暖, 刘正华, 祝令谱, 等.深度强化学习在变体飞行器自主外形优化中的应用[J].宇航学报, 2017, 38 (11) :1153-1159.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Smart streaming for online video services">

                                <b>[15]</b> CHEN Liang, ZHOU Yipeng, CHIU D M.Smart streaming for online video services[J].IEEE Transactions on Multimedia, 2015, 17 (4) :485-497.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJC201905039" />
        <input id="dpi" type="hidden" value="600" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201905039&amp;v=MjE1MDY0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSb0Z5M21WcnpMTHo3QmJiRzRIOWpNcW85R2JZUUtESDg0dlI=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
