<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637129054920275000%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJC201908038%26RESULT%3d1%26SIGN%3dmVp0UzWD6C4CFAemAr%252b6yLq779U%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201908038&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201908038&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201908038&amp;v=MTY1MTJwNDlHYklRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJxRkNqblViM0xMejdCYmJHNEg5ak0=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#59" data-title="0 概述 ">0 概述</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#61" data-title="1 AR系统三维注册 ">1 AR系统三维注册</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#66" data-title="2 实时特征点检测与匹配 ">2 实时特征点检测与匹配</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#68" data-title="2.1 FAST算法实时特征检测">2.1 FAST算法实时特征检测</a></li>
                                                <li><a href="#72" data-title="2.2 特征点匹配">2.2 特征点匹配</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#78" data-title="3 SIFT-ORB-MRANSAC融合算法 ">3 SIFT-ORB-MRANSAC融合算法</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#90" data-title="3.1 局部区域快速特征点选取">3.1 局部区域快速特征点选取</a></li>
                                                <li><a href="#95" data-title="3.2 特征点局部极值的快速算法">3.2 特征点局部极值的快速算法</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#132" data-title="4 KCF目标跟踪算法 ">4 KCF目标跟踪算法</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#136" data-title="5 实时特征检测与目标跟踪的AR三维注册 ">5 实时特征检测与目标跟踪的AR三维注册</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#140" data-title="6 实验结果与分析 ">6 实验结果与分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#141" data-title="6.1 实验平台">6.1 实验平台</a></li>
                                                <li><a href="#143" data-title="6.2 对比算法">6.2 对比算法</a></li>
                                                <li><a href="#145" data-title="6.3 实验结果">6.3 实验结果</a></li>
                                                <li><a href="#155" data-title="6.4 结果分析">6.4 结果分析</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#159" data-title="7 结束语 ">7 结束语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#131" data-title="&lt;b&gt;图1 不同窗口条件下各算法平均计算次数对比&lt;/b&gt;"><b>图1 不同窗口条件下各算法平均计算次数对比</b></a></li>
                                                <li><a href="#147" data-title="&lt;b&gt;表1 4种算法匹配性能对比&lt;/b&gt;"><b>表1 4种算法匹配性能对比</b></a></li>
                                                <li><a href="#149" data-title="&lt;b&gt;图2 摄像头初始捕捉图像&lt;/b&gt;"><b>图2 摄像头初始捕捉图像</b></a></li>
                                                <li><a href="#150" data-title="&lt;b&gt;图3 目标跟踪初始场景&lt;/b&gt;"><b>图3 目标跟踪初始场景</b></a></li>
                                                <li><a href="#151" data-title="&lt;b&gt;图4 本文方法的三维注册结果&lt;/b&gt;"><b>图4 本文方法的三维注册结果</b></a></li>
                                                <li><a href="#152" data-title="图5 TLD+CT目标跟踪算法与SURF、SIFT、FAST算法结合的三维注册结果">图5 TLD+CT目标跟踪算法与SURF、SIFT、FAST算法结合的三维注册结果</a></li>
                                                <li><a href="#157" data-title="&lt;b&gt;表2 平均运算时间对比&lt;/b&gt;"><b>表2 平均运算时间对比</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="193">


                                    <a id="bibliography_1" title=" BILLINGHURST M, CLARK A, LEE G.A survey of augmented reality[J].Foundations and Trends in Human-Computer Interaction, 2015, 8 (2/3) :73-272." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A survey of augmented reality">
                                        <b>[1]</b>
                                         BILLINGHURST M, CLARK A, LEE G.A survey of augmented reality[J].Foundations and Trends in Human-Computer Interaction, 2015, 8 (2/3) :73-272.
                                    </a>
                                </li>
                                <li id="195">


                                    <a id="bibliography_2" title=" 王月, 张树生, 何卫平, 等.基于模型的增强现实无标识三维注册追踪方法[J].上海交通大学学报, 2018, 52 (1) :83-89." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=SHJT201801015&amp;v=MjM5ODZPZVplUnFGQ2puVWIzTE5pWEJlckc0SDluTXJvOUVZWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkw=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         王月, 张树生, 何卫平, 等.基于模型的增强现实无标识三维注册追踪方法[J].上海交通大学学报, 2018, 52 (1) :83-89.
                                    </a>
                                </li>
                                <li id="197">


                                    <a id="bibliography_3" title=" 李乾, 高尚兵, 潘志庚, 等.基于无标记识别的增强现实方法研究[J].系统仿真学报, 2018, 30 (7) :191-197." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XTFZ201807023&amp;v=MTMyOTA1NE8zenFxQnRHRnJDVVJMT2VaZVJxRkNqblViM0xQVG5OZExHNEg5bk1xSTlIWjRRS0RIODR2UjRUNmo=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         李乾, 高尚兵, 潘志庚, 等.基于无标记识别的增强现实方法研究[J].系统仿真学报, 2018, 30 (7) :191-197.
                                    </a>
                                </li>
                                <li id="199">


                                    <a id="bibliography_4" title=" LOWE D G.Distinctive image features from scale-invariant keypoints[J].International Journal of Computer Vision, 2004, 60 (2) :91-110." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002830901&amp;v=MDkxNjF0SE9wNHhGYmVzT1kzazV6QmRoNGo5OVNYcVJyeG94Y01IN1I3cWVidWR0RkN6bVU3M0tJbDA9Tmo3QmFyTzRI&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                         LOWE D G.Distinctive image features from scale-invariant keypoints[J].International Journal of Computer Vision, 2004, 60 (2) :91-110.
                                    </a>
                                </li>
                                <li id="201">


                                    <a id="bibliography_5" title=" 欧阳能钧, 李伟彤, 韦蔚, 等.基于SIFT与Contourlee变换分辨遥感图像配准[J].遥感技术与应用, 2013, 28 (1) :58-64." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=YGJS201301009&amp;v=MTg2MzZxQnRHRnJDVVJMT2VaZVJxRkNqblViM0xQQ3JCZmJHNEg5TE1ybzlGYllRS0RIODR2UjRUNmo1NE8zenE=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                         欧阳能钧, 李伟彤, 韦蔚, 等.基于SIFT与Contourlee变换分辨遥感图像配准[J].遥感技术与应用, 2013, 28 (1) :58-64.
                                    </a>
                                </li>
                                <li id="203">


                                    <a id="bibliography_6" title=" BAY H, WALKER J J, HARRIS J K.et al.Error-correcting barcoded primers for pyrosequencing hundreds of samples in multiplex[J].Nature Methods, 2008, 5 (3) :235-237." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Error-correcting barcoded primers for pyrosequencing hundreds of samples in multiplex">
                                        <b>[6]</b>
                                         BAY H, WALKER J J, HARRIS J K.et al.Error-correcting barcoded primers for pyrosequencing hundreds of samples in multiplex[J].Nature Methods, 2008, 5 (3) :235-237.
                                    </a>
                                </li>
                                <li id="205">


                                    <a id="bibliography_7" title=" RUBLEE E, RABAUD V, KONOLIGE K, et al.ORB:an efficient alternative to SIFT or SURF[C]//Proceedings of IEEE International Conference on Computer Vision.Washington D.C., USA:IEEE Press, 2012:2564-2571." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=ORB:an efficient alternative to SIFT or SURF">
                                        <b>[7]</b>
                                         RUBLEE E, RABAUD V, KONOLIGE K, et al.ORB:an efficient alternative to SIFT or SURF[C]//Proceedings of IEEE International Conference on Computer Vision.Washington D.C., USA:IEEE Press, 2012:2564-2571.
                                    </a>
                                </li>
                                <li id="207">


                                    <a id="bibliography_8" title=" WU Yi, LIM J, YANG M H.Online object tracking:a benchmark[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2013:2411-2418." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Online object tracking:A benchmark">
                                        <b>[8]</b>
                                         WU Yi, LIM J, YANG M H.Online object tracking:a benchmark[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2013:2411-2418.
                                    </a>
                                </li>
                                <li id="209">


                                    <a id="bibliography_9" title=" KALAL Z, MIKOLAJCZYK K, MATAS J.Tracking-learning-detection[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2012, 34 (7) :1409-1422." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Tracking-Learning-Detection">
                                        <b>[9]</b>
                                         KALAL Z, MIKOLAJCZYK K, MATAS J.Tracking-learning-detection[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2012, 34 (7) :1409-1422.
                                    </a>
                                </li>
                                <li id="211">


                                    <a id="bibliography_10" title=" HARE S, SAFFARI A, TORR P H S.Struck:structured output tracking with kernels[C]//Proceedings of IEEE International Conference on Computer Vision.Washington D.C., USA:IEEE Press, 2011:263-270." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Struck:structured output tracking with kernels">
                                        <b>[10]</b>
                                         HARE S, SAFFARI A, TORR P H S.Struck:structured output tracking with kernels[C]//Proceedings of IEEE International Conference on Computer Vision.Washington D.C., USA:IEEE Press, 2011:263-270.
                                    </a>
                                </li>
                                <li id="213">


                                    <a id="bibliography_11" title=" ZHANG Kaihua, ZHANG Lei, YANG M H.Real-time compressive tracking[C]//Proceedings of European Conference on Computer Vision.Washington D.C., USA:IEEE Press, 2012:864-877." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Real-time compressive tracking">
                                        <b>[11]</b>
                                         ZHANG Kaihua, ZHANG Lei, YANG M H.Real-time compressive tracking[C]//Proceedings of European Conference on Computer Vision.Washington D.C., USA:IEEE Press, 2012:864-877.
                                    </a>
                                </li>
                                <li id="215">


                                    <a id="bibliography_12" title=" 李炎, 尹东.基于TLD的增强现实跟踪注册方法[J].系统仿真学报, 2014, 26 (9) :2062-2067." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XTFZ201409034&amp;v=MjIzNTVxcUJ0R0ZyQ1VSTE9lWmVScUZDam5VYjNMUFRuTmRMRzRIOVhNcG85R1lJUUtESDg0dlI0VDZqNTRPM3o=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[12]</b>
                                         李炎, 尹东.基于TLD的增强现实跟踪注册方法[J].系统仿真学报, 2014, 26 (9) :2062-2067.
                                    </a>
                                </li>
                                <li id="217">


                                    <a id="bibliography_13" title=" 钟权, 周进, 吴钦章, 等.一种改进的实时压缩跟踪算法[J].光电工程, 2014, 41 (4) :1-8." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GDGC201404002&amp;v=MjQ3NjdlWmVScUZDam5VYjNMSWluTWJiRzRIOVhNcTQ5RlpvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE8=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[13]</b>
                                         钟权, 周进, 吴钦章, 等.一种改进的实时压缩跟踪算法[J].光电工程, 2014, 41 (4) :1-8.
                                    </a>
                                </li>
                                <li id="219">


                                    <a id="bibliography_14" title=" 荆圣媛, 韩勇, 孟学文, 等.移动AR+VR支持下旅游GIS系统的设计与实现[J].测绘通报, 2019 (1) :79-84." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=CHTB201901017&amp;v=MjM2NTVFWTRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJxRkNqblViM0xKaVhmYkxHNEg5ak1ybzk=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[14]</b>
                                         荆圣媛, 韩勇, 孟学文, 等.移动AR+VR支持下旅游GIS系统的设计与实现[J].测绘通报, 2019 (1) :79-84.
                                    </a>
                                </li>
                                <li id="221">


                                    <a id="bibliography_15" >
                                        <b>[15]</b>
                                     HENRIQUES J F, CASEIRO R, PEDRO M, et al.High-speed tracking with kernelized correlation filters[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2015, 37 (3) :583-596.</a>
                                </li>
                                <li id="223">


                                    <a id="bibliography_16" title=" 王任华, 沈剑宇, 蒋敏.基于自适应多模型联合的目标跟踪算法[J/OL].计算机工程:1-9[2019-06-21].https://doi.org/10.19678/j.issn.1000-3428.0052593." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201908044&amp;v=MTU1NzR6cXFCdEdGckNVUkxPZVplUnFGQ2puVWIzTEx6N0JiYkc0SDlqTXA0OUJZSVFLREg4NHZSNFQ2ajU0TzM=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[16]</b>
                                         王任华, 沈剑宇, 蒋敏.基于自适应多模型联合的目标跟踪算法[J/OL].计算机工程:1-9[2019-06-21].https://doi.org/10.19678/j.issn.1000-3428.0052593.
                                    </a>
                                </li>
                                <li id="225">


                                    <a id="bibliography_17" title=" RABIN J, DELON J, GOUSSEAU Y, et al.MAC-RANSAC:a robust algorithm for the recognition of multiple objects[C]//Proceedings of the 5th International Symposium on 3D Data Processing, Visualization and Transmission.Paris, France:[s.n.], 2010:51." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=MAC-RANSAC:a robust algorithm for the recognition of multiple objects">
                                        <b>[17]</b>
                                         RABIN J, DELON J, GOUSSEAU Y, et al.MAC-RANSAC:a robust algorithm for the recognition of multiple objects[C]//Proceedings of the 5th International Symposium on 3D Data Processing, Visualization and Transmission.Paris, France:[s.n.], 2010:51.
                                    </a>
                                </li>
                                <li id="227">


                                    <a id="bibliography_18" title=" ROSTEN E, DRUMMOND T.Machine learning for highspeed corner detection[C]//Proceedings of European Conference on Computer Vision.Graz, Austria:[s.n.], 2006:430-443." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Machine learning for highspeed corner detection">
                                        <b>[18]</b>
                                         ROSTEN E, DRUMMOND T.Machine learning for highspeed corner detection[C]//Proceedings of European Conference on Computer Vision.Graz, Austria:[s.n.], 2006:430-443.
                                    </a>
                                </li>
                                <li id="229">


                                    <a id="bibliography_19" title=" CALONDER M, LEPETIT V, STRECHA C, et al.Brief:binary robust ipendent elementary features[C]//Proceedings of European Conference on Computer Vision.Heraklion, Greece:[s.n.], 2010:778-792." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Brief:binary robust independent elementary features">
                                        <b>[19]</b>
                                         CALONDER M, LEPETIT V, STRECHA C, et al.Brief:binary robust ipendent elementary features[C]//Proceedings of European Conference on Computer Vision.Heraklion, Greece:[s.n.], 2010:778-792.
                                    </a>
                                </li>
                                <li id="231">


                                    <a id="bibliography_20" title=" ROSIN P L.Measuring corner properties[J].Computer Vision and Image Understanding, 1999, 73 (2) :291-307." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501084512&amp;v=MDQwMzFMQ1gwN29CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVUx2TElGd1dhQkU9TmlmT2ZiSzdIdEROcW85RVpPTQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[20]</b>
                                         ROSIN P L.Measuring corner properties[J].Computer Vision and Image Understanding, 1999, 73 (2) :291-307.
                                    </a>
                                </li>
                                <li id="233">


                                    <a id="bibliography_21" title=" 周见光, 石刚, 马小虎.增强现实系统中的虚拟交互方法[J].计算机工程, 2012, 38 (1) :251-252." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201201083&amp;v=MTIwNTJGQ2puVWIzTEx6N0JiYkc0SDlQTXJvOU5aNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUnE=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[21]</b>
                                         周见光, 石刚, 马小虎.增强现实系统中的虚拟交互方法[J].计算机工程, 2012, 38 (1) :251-252.
                                    </a>
                                </li>
                                <li id="235">


                                    <a id="bibliography_22" title=" 施琦, 王涌天, 陈靖.一种基于视觉的增强现实三维注册算法[J].中国图象图形学报, 2018, 7 (7) :679-683." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZGTB200207010&amp;v=MjczNTBScUZDam5VYjNMUHlyZmJMRzRIdFBNcUk5RVpJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmU=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[22]</b>
                                         施琦, 王涌天, 陈靖.一种基于视觉的增强现实三维注册算法[J].中国图象图形学报, 2018, 7 (7) :679-683.
                                    </a>
                                </li>
                                <li id="237">


                                    <a id="bibliography_23" title=" 顾庆传, 姜娜.基于FAST特征点检测的实时虚实注册方法[J].哈尔滨师范大学自然科学学报, 2017, 33 (2) :58-62." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=HEBY201702015&amp;v=MDExNTBiTXJZOUVZWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUnFGQ2puVWIzTExTakpkN0c0SDk=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[23]</b>
                                         顾庆传, 姜娜.基于FAST特征点检测的实时虚实注册方法[J].哈尔滨师范大学自然科学学报, 2017, 33 (2) :58-62.
                                    </a>
                                </li>
                                <li id="239">


                                    <a id="bibliography_24" title=" 张亚玲, 朱望纯.基于物体识别的增强现实装配指导[J].桂林电子科技大学学报, 2018, 38 (5) :29-32." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GLDZ201805006&amp;v=MTY5NDFIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJxRkNqblViM0xJaUhQZExHNEg5bk1xbzlGWW9RS0Q=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[24]</b>
                                         张亚玲, 朱望纯.基于物体识别的增强现实装配指导[J].桂林电子科技大学学报, 2018, 38 (5) :29-32.
                                    </a>
                                </li>
                                <li id="241">


                                    <a id="bibliography_25" title=" 谭暑秋.面部动态增强现实的特征点检测方法研究[D].杭州:电子科技大学, 2018." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CDFD&amp;filename=1018055155.nh&amp;v=MjE3MjRGck85RzlESnFwRWJQSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUnFGQ2puVWIzTFZGMjY=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[25]</b>
                                         谭暑秋.面部动态增强现实的特征点检测方法研究[D].杭州:电子科技大学, 2018.
                                    </a>
                                </li>
                                <li id="243">


                                    <a id="bibliography_26" title=" 张浩华, 吴艳敏, 程立英, 等.基于移动端平台古生物博物馆增强现实系统的设计与应用[J].沈阳师范大学学报 (自然科学版) , 2018, 36 (2) :61-69." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=SYSX201802010&amp;v=MTM5NjZPZVplUnFGQ2puVWIzTE5qVFlkckc0SDluTXJZOUVaSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkw=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[26]</b>
                                         张浩华, 吴艳敏, 程立英, 等.基于移动端平台古生物博物馆增强现实系统的设计与应用[J].沈阳师范大学学报 (自然科学版) , 2018, 36 (2) :61-69.
                                    </a>
                                </li>
                                <li id="245">


                                    <a id="bibliography_27" title=" 王小红.基于TLD与SIFT的增强现实三维注册方法[J].哈尔滨师范大学自然科学学报, 2017, 33 (2) :71-74." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=HEBY201702018&amp;v=MDM0MzBiTXJZOUViSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUnFGQ2puVWIzTExTakpkN0c0SDk=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[27]</b>
                                         王小红.基于TLD与SIFT的增强现实三维注册方法[J].哈尔滨师范大学自然科学学报, 2017, 33 (2) :71-74.
                                    </a>
                                </li>
                                <li id="247">


                                    <a id="bibliography_28" title=" 邢藏菊, 温兰兰, 何苏勤.TLD视频目标跟踪器快速匹配的研究[J].小型微型计算机系统, 2015, 36 (5) :1113-1116." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XXWX201505048&amp;v=MzExMDdUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJxRkNqblViM0xQVFhjZHJHNEg5VE1xbzlCYklRS0RIODR2UjQ=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[28]</b>
                                         邢藏菊, 温兰兰, 何苏勤.TLD视频目标跟踪器快速匹配的研究[J].小型微型计算机系统, 2015, 36 (5) :1113-1116.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJC" target="_blank">计算机工程</a>
                2019,45(08),230-235 DOI:10.19678/j.issn.1000-3428.0053924            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于KCF与改进ORB的增强现实方法</b></span>
 <span class="shoufa"></span>                                     </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%8E%E4%B8%80&amp;code=23367109&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">李一</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%86%AF%E6%A5%A0&amp;code=22661324&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">冯楠</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%B0%AD%E9%A1%BA%E6%88%90&amp;code=41465614&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">谭顺成</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E4%BA%BA%E6%B0%91%E8%A7%A3%E6%94%BE%E5%86%9B92493%E9%83%A8%E9%98%9F13%E5%88%86%E9%98%9F&amp;code=0050899&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国人民解放军92493部队13分队</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E4%BA%BA%E6%B0%91%E8%A7%A3%E6%94%BE%E5%86%9B92941%E9%83%A8%E9%98%9F41%E5%88%86%E9%98%9F&amp;code=1017920&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国人民解放军92941部队41分队</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%B5%B7%E5%86%9B%E8%88%AA%E7%A9%BA%E5%A4%A7%E5%AD%A6%E4%BF%A1%E6%81%AF%E8%9E%8D%E5%90%88%E7%A0%94%E7%A9%B6%E6%89%80&amp;code=1701745&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">海军航空大学信息融合研究所</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>针对增强现实 (AR) 系统在进行虚拟信息叠加时待注册区域被遮挡的情况, 提出一种基于实时特征检测与目标跟踪的AR系统三维注册方法。使用KCF算法对待注册目标区域与模板进行实时FAST特征匹配, 通过改进ORB算法, 提出SIFT-ORB-MRANSAC融合算法, 完成特征点的提取、匹配以及误匹配去除。在此基础上, 根据特征点之间的匹配关系计算注册矩阵, 通过注册矩阵叠加虚拟信息实现对现实世界的增强。实验结果表明, 该方法在光照变化、遮挡以及旋转变换的情况下, 可将虚拟信息快速、准确地叠加到待注册区域, 提高AR系统的运算效率。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=ORB%E7%AE%97%E6%B3%95&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">ORB算法;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=KCF%E7%AE%97%E6%B3%95&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">KCF算法;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%A2%9E%E5%BC%BA%E7%8E%B0%E5%AE%9E&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">增强现实;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%B8%89%E7%BB%B4%E6%B3%A8%E5%86%8C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">三维注册;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%89%B9%E5%BE%81%E7%82%B9&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">特征点;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    李一 (1980—) , 男, 高级工程师, 主研方向为装备数据工程;;
                                </span>
                                <span>
                                    冯楠, 工程师、硕士;;
                                </span>
                                <span>
                                    谭顺成, 工程师、博士。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-02-16</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金 (61671462);</span>
                    </p>
            </div>
                    <h1><b>Augmented Reality Method Based on KCF and Improved ORB</b></h1>
                    <h2>
                    <span>LI Yi</span>
                    <span>FENG Nan</span>
                    <span>TAN Shuncheng</span>
            </h2>
                    <h2>
                    <span>The 13th Unit of 92493 PLA Troops</span>
                    <span>The 41st Unit of 92491 PLA Troops</span>
                    <span>Research Institute of Information Fusion, Naval Aeronautical University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Aiming at the situation that the area to be resigistered in the Augmented Reality (AR) system is occluded when the virtual information is superimposed, a three-dimensional registration method for AR system based on real-time feature detection and target tracking is proposed.KCF algorithm is used to perform real-time FAST feature matching on the target area and the template.By improving the ORB algorithm, SIFT-ORB-MRANSAC fusion algorithm is proposed to extract and match feature remove mismatches.On this basis, the registration matrix is calculated by the matching relationship between the feature points, and virtual information is overlaid to the registration matrix to augment the real world.Experimental results show that the proposed method can quickly and accurately overlay the virtual information to the registered area under the conditions of illumination change, occlusion and rotation transformation, and improve the points as well as computation efficiency of AR systems.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=ORB%20algorithm&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">ORB algorithm;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=KCF%20algorithm&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">KCF algorithm;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Augmented%20Reality%20(AR)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Augmented Reality (AR) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=three-dimensional%20registration&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">three-dimensional registration;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=feature%20point&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">feature point;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2019-02-16</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="59" name="59" class="anchor-tag">0 概述</h3>
                <div class="p1">
                    <p id="60">增强现实 (Augmented Reality, AR) 技术是将虚拟信息叠加到现实世界真实场景中的一种可交互的三维可视化技术<citation id="249" type="reference"><link href="193" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>, 而如何将虚拟信息正确无误地叠加到真实世界中的某个区域, 是构建增强现实系统的关键, 即三维注册技术。三维注册是构建AR系统的核心技术, 但是在虚拟信息的叠加过程中, 待注册区域会出现被遮挡的情况。针对该问题, 本文提出一种基于实时特征检测与目标跟踪的三维注册方法。</p>
                </div>
                <h3 id="61" name="61" class="anchor-tag">1 AR系统三维注册</h3>
                <div class="p1">
                    <p id="62">目前AR系统三维注册技术可以分为基于标识的注册、基于自然特征的注册<citation id="250" type="reference"><link href="195" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>以及基于特殊标识与自然纹理的三维注册。但外界复杂环境对需注册区域的干扰, 是基于标识的注册方法在虚拟信息的叠加过程中难以避免的问题<citation id="251" type="reference"><link href="197" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>。随着新一代人工智能技术的发展, 基于自然特征的注册方法可以满足很多应用需求, 其不受所处场合的限制, 已成为AR系统注册的主流技术, 是增强现实技术发展的重要研究方向。</p>
                </div>
                <div class="p1">
                    <p id="63">研究者面向图像的特征点提取与匹配提出很多针对性的算法, 目前主流的特征检测算法有SIFT特征检测算法<citation id="252" type="reference"><link href="199" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>、实时FAST特征检测算法<citation id="253" type="reference"><link href="201" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>、SURF特征检测算法<citation id="254" type="reference"><link href="203" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>等。文献<citation id="255" type="reference">[<a class="sup">4</a>]</citation>提出的SIFT算法, 在特征点、描述子的旋转不变性和尺度不变性上具有优势, 同时对光照变化的适应能力强, 得到的特征点比较稳定。然而由于该算法计算信息较多, 复杂度较高, 运算量较大, 因此其运行效率不高, 不适用于实时性要求较高的应用场景。相对而言, 文献<citation id="256" type="reference">[<a class="sup">5</a>]</citation>提出的实时FAST算法, 其优势在于检测实时性较高, 能满足实时增强现实系统的要求。文献<citation id="257" type="reference">[<a class="sup">6</a>]</citation>提出的SURF算法是SIFT算法的改进, 其通过Hessian矩阵和Hear小波的结合, 以牺牲一定的特征点稳定性为代价, 换取了运算效率的有效提升。ORB算法<citation id="258" type="reference"><link href="205" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>的运算效率较高, 同时具有旋转不变性。目前, 绝大部分的图像处理过程仍利用ORB算法处理相关的图像问题。但ORB算法在尺度不变性方面存在的缺陷, 其在图片放大、缩小情况下会频繁出现错误匹配问题, 从而影响特征点匹配的准确率。在对准确性和实时性要求较高的应用场景, 必须对ORB算法进行改进。</p>
                </div>
                <div class="p1">
                    <p id="64">在视觉跟踪领域中, 目标跟踪算法种类较多。文献<citation id="259" type="reference">[<a class="sup">8</a>]</citation>对该领域流行的29个算法在50个视频的样本集上进行了性能测试评估。其中TLD<sup></sup><citation id="260" type="reference"><link href="209" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>、Struck<sup></sup><citation id="261" type="reference"><link href="211" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>和CT<sup></sup><citation id="262" type="reference"><link href="213" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>的运行效率较高, 相同硬件环境下Struck的处理速度能达到20帧/s左右;而TLD能达到约28帧/s的处理速度。文献<citation id="263" type="reference">[<a class="sup">12</a>]</citation>提出的基于TLD的增强现实跟踪注册方法, 以及文献<citation id="264" type="reference">[<a class="sup">13</a>]</citation>提出的CT改进算法, 在实时性和正确性上相对于原算法的提升有限<citation id="265" type="reference"><link href="219" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>。文献<citation id="266" type="reference">[<a class="sup">15</a>]</citation>提出的KCF (Kernerlized Correlation Filter) 算法, 可以得到有良好的跟踪效果, 抗复杂环境干扰能力强, 且运算效率提升明显。文献<citation id="267" type="reference">[<a class="sup">16</a>]</citation>提出的跟踪算法将 KCF 跟踪模型和基于颜色直方图的跟踪模型进行联合, 同时采用自适应权重联合的策略取代固定权重方法进行目标位置评估, 充分利用2种模型的互补特性, 提升了目标跟踪可靠性。</p>
                </div>
                <div class="p1">
                    <p id="65">在对以上特征匹配算法分析的基础上, 本文将三维注册方法联合实时特征检测与目标跟踪技术, 使用KCF算法对待注册区域进行目标跟踪。同时对ORB算法进行改进, 利用SIFT算法提升特征点提取效率, 通过MRANSAC<citation id="268" type="reference"><link href="225" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>算法消除特征点错误匹配, 提升匹配准确率, 最终利用三维注册矩阵求解完成注册。</p>
                </div>
                <h3 id="66" name="66" class="anchor-tag">2 实时特征点检测与匹配</h3>
                <div class="p1">
                    <p id="67">考虑到增强现实系统对实时性要求的日益提高, 本文采用FAST算法对待注册跟踪目标图像的特征点进行提取与匹配, 充分发挥其特征检测速度快的特性。</p>
                </div>
                <h4 class="anchor-tag" id="68" name="68">2.1 FAST算法实时特征检测</h4>
                <div class="p1">
                    <p id="69">实时FAST<sup></sup><citation id="269" type="reference"><link href="227" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>算法在特征点提取过程中加入了实时方向信息, 同时利用BRIEF<citation id="270" type="reference"><link href="229" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>进行特征点描述, 以改善特征点对图像噪声敏感以及不具备旋转不变性的问题。在实时FAST特征点监测过程中, 对于每一个像素点<i>p</i>, 在判断<i>p</i>是否为角点时, 将以像素点<i>p</i>为中心, 以为<i>r</i>半径, 在圆周上选取存在联系的<i>n</i>个像素, 判断公式如式 (1) 所示。</p>
                </div>
                <div class="p1">
                    <p id="70" class="code-formula">
                        <mathml id="70"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>C</mi><mi>R</mi><mi>F</mi><mo>=</mo><mrow><mo>{</mo><mrow><mtable><mtr><mtd columnalign="left"><mn>1</mn><mo>, </mo><mrow><mo>|</mo><mrow><mi>Ι</mi><msub><mrow></mrow><mi>p</mi></msub><mo>-</mo><mi>Ι</mi><msub><mrow></mrow><mi>k</mi></msub></mrow><mo>|</mo></mrow><mo>&gt;</mo><mi>t</mi></mtd></mtr><mtr><mtd columnalign="left"><mn>0</mn><mo>, </mo><mtext>其</mtext><mtext>他</mtext></mtd></mtr></mtable></mrow></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="71">其中:<i>k</i>=1, 2, …, <i>n</i>;<i>I</i><sub><i>p</i></sub>为像素点<i>p</i>的灰度值;<i>I</i><sub><i>k</i></sub>是以<i>r</i>为半径的圆周上的像素点<i>k</i>的灰度值;<i>t</i>是根据经验选取的阈值, 若<i>CRF</i>=1的个数多于阈值<i>t</i>′ (<i>t</i>′ ≥12) , 则将<i>p</i>视为候选点。FAST算法还将建立图像金字塔同时通过灰度质心法<citation id="271" type="reference"><link href="231" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>来获得方向信息并提取BRIEF算子, 以此引入尺度特性, 从而解决旋转不变性的问题。</p>
                </div>
                <h4 class="anchor-tag" id="72" name="72">2.2 特征点匹配</h4>
                <div class="p1">
                    <p id="73">在注册区域拥有足够数量的高质量特征点<citation id="272" type="reference"><link href="233" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>对实时FAST特征匹配的效果有显著影响, 判断特征点是否足够的计算公式如式 (2) 所示。</p>
                </div>
                <div class="p1">
                    <p id="74"><i>h</i><sub><i>i</i></sub>&gt;<i>H</i>      (2) </p>
                </div>
                <div class="p1">
                    <p id="75">其中, 海塞矩阵值为<i>h</i><sub><i>i</i></sub>, 阈值<i>H</i>=1 000。对提取的特征与上一帧特征进行匹配<citation id="273" type="reference"><link href="235" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>, 由于受到复杂因素的影响, 会导致误匹配的特征点<citation id="274" type="reference"><link href="237" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>, 因此需要采用RANSAC<citation id="275" type="reference"><link href="225" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>算法去除特征点匹配错误。</p>
                </div>
                <div class="p1">
                    <p id="76">完成匹配并计算出AR系统的注册矩阵<citation id="277" type="reference"><link href="239" rel="bibliography" /><link href="241" rel="bibliography" /><link href="243" rel="bibliography" /><sup>[<a class="sup">24</a>,<a class="sup">25</a>,<a class="sup">26</a>]</sup></citation>后, 即可在系统待注册区域中进行虚拟信息叠加工作, 从而实现对现实世界的增强。在此过程中, 为了能够快速地进行目标跟踪、特征检测以及特征匹配, 考虑到多数图像处理操作都是基于RGB格式, 因此, 还需将视频图像换成RGB格式, 以便对图像进行后期处理显示。本文通过对输入的视频图像按照式 (3) 进行色彩转换<citation id="276" type="reference"><link href="245" rel="bibliography" /><sup>[<a class="sup">27</a>]</sup></citation>, 将图像从彩色转换为灰度, 从而提高运算效率, 降低计算维度。</p>
                </div>
                <div class="p1">
                    <p id="77"><i>I</i>=0.299×<i>R</i>+0.587×<i>G</i>+0.114×<i>B</i>      (3) </p>
                </div>
                <h3 id="78" name="78" class="anchor-tag">3 SIFT-ORB-MRANSAC融合算法</h3>
                <div class="p1">
                    <p id="79">本文设计SIFT-ORB-MRANSAC融合算法的目的在于保留ORB算法运行效率高和匹配效果好的优点, 同时针对其尺度不变性方面的缺陷进行改进。对于图像特征点的分析过程主要有关键点提取、描述子生成以及特征点匹配处理3个重要部分。关键点的提取部分主要是利用SIFT算法, 以确保关键点选取的质量;描述子通过对关键点使用ORB算法进行处理得到;特征点匹配部分使用Hamming算法进行特征点匹配, 同时通过MRANSAC方法对匹配错误进行清除以确保匹配正确性。</p>
                </div>
                <div class="p1">
                    <p id="80">在利用Harris算法进行多尺度特征点提取的过程中, 首先需要对灰度图像<i>I</i> (<i>x</i>, <i>y</i>) 进行高斯平滑卷积处理以生成多层图像金字塔。记金字塔的最底层为<i>P</i><sub>0</sub> (<i>x</i>, <i>y</i>) =<i>I</i> (<i>x</i>, <i>y</i>) , 则任意一层的图像金字塔可表示为:</p>
                </div>
                <div class="p1">
                    <p id="81" class="code-formula">
                        <mathml id="81"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mover><mstyle mathsize="140%" displaystyle="true"><mi>Ρ</mi></mstyle><mrow><mspace width="0.25em" /><mo>∼</mo></mrow></mover><msub><mrow></mrow><mi>l</mi></msub><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo><mo>=</mo><mi>Ρ</mi><msub><mrow></mrow><mi>l</mi></msub><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo><mo>⊗</mo><mi>g</mi><msub><mrow></mrow><mrow><mi>σ</mi><msub><mrow></mrow><mi>p</mi></msub></mrow></msub><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi>Ρ</mi><msub><mrow></mrow><mrow><mi>l</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo><mo>=</mo><mover><mstyle mathsize="140%" displaystyle="true"><mi>Ρ</mi></mstyle><mrow><mspace width="0.25em" /><mo>∼</mo></mrow></mover><msub><mrow></mrow><mi>l</mi></msub><mo stretchy="false"> (</mo><mi>s</mi><mi>x</mi><mo>, </mo><mi>s</mi><mi>y</mi><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>5</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="82">设<i>g</i><sub><i>σ</i></sub> (<i>x</i>, <i>y</i>) 为标准差为<i>σ</i>的高斯平滑窗口, <i>s</i>为采样间隔 (一般取2) , 则对于金字塔中任意点的Harris特征点检测矩阵为:</p>
                </div>
                <div class="p1">
                    <p id="83" class="code-formula">
                        <mathml id="83"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">Η</mi><msub><mrow></mrow><mi>l</mi></msub><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo><mo>=</mo><mo>∇</mo><mrow><mi>σ</mi><msub><mrow></mrow><mi>d</mi></msub></mrow><mi>Ρ</mi><msub><mrow></mrow><mi>l</mi></msub><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo><mo>∇</mo><msub><mrow></mrow><mrow><mi>σ</mi><msub><mrow></mrow><mtext>d</mtext></msub></mrow></msub><mi>Ρ</mi><msub><mrow></mrow><mi>l</mi></msub><mrow><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo></mrow><msup><mrow></mrow><mtext>Τ</mtext></msup><mo>⊗</mo><mi>g</mi><msub><mrow></mrow><mrow><mi>σ</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msub><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>6</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="84">其中, <i>l</i>表示金字塔层数, (<i>x</i>, <i>y</i>) 为坐标, <mathml id="85"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo>∇</mo><msub><mrow></mrow><mi>σ</mi></msub></mrow></math></mathml>为在尺度σ上的梯度, 即</p>
                </div>
                <div class="p1">
                    <p id="86" class="code-formula">
                        <mathml id="86"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo>∇</mo><msub><mrow></mrow><mi>σ</mi></msub><mi>f</mi><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo><mo>≜</mo><mo>∇</mo><mspace width="0.25em" /><mi>f</mi><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo><mo>⊗</mo><mi>g</mi><msub><mrow></mrow><mi>σ</mi></msub><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>7</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="87">文献<citation id="278" type="reference">[<a class="sup">4</a>]</citation>设置积分尺度σ<sub>i</sub>=1.5, 微分尺度σ<sub><i>d</i></sub>=1.0, 同时通过函数f<sub><i>HM</i></sub>:</p>
                </div>
                <div class="p1">
                    <p id="88" class="code-formula">
                        <mathml id="88"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>f</mi><msub><mrow></mrow><mrow><mtext>Η</mtext><mtext>Μ</mtext></mrow></msub><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mrow><mi>det</mi><mtext> </mtext><mi mathvariant="bold-italic">Η</mi><msub><mrow></mrow><mi>l</mi></msub><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo></mrow><mrow><mtext>t</mtext><mtext>r</mtext><mtext> </mtext><mi mathvariant="bold-italic">Η</mi><msub><mrow></mrow><mi>l</mi></msub><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo></mrow></mfrac><mo>=</mo><mfrac><mrow><mi>λ</mi><msub><mrow></mrow><mn>1</mn></msub><mi>λ</mi><msub><mrow></mrow><mn>2</mn></msub></mrow><mrow><mi>λ</mi><msub><mrow></mrow><mn>1</mn></msub><mo>+</mo><mi>λ</mi><msub><mrow></mrow><mn>2</mn></msub></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>8</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="89">来计算矩阵<b><i>H</i></b>特征值 (<i>λ</i><sub>1</sub>, <i>λ</i><sub>2</sub>) 的调和平均值。当<i>f</i><sub>HM</sub>大于某个阈值 (通常取10) , 则点 (<i>x</i>, <i>y</i>) 为特征点。</p>
                </div>
                <h4 class="anchor-tag" id="90" name="90">3.1 局部区域快速特征点选取</h4>
                <div class="p1">
                    <p id="91">由于特征点数量与特征点匹配速度呈正相关性, 因此控制特征点的数量是提升匹配速度的有效途径, 而Harris算法采用的全局特征点选取思路, 特征点分布情况受图像内容影响较大, 若简单按照<i>f</i><sub>HM</sub>值排序选取有限数量特征点进行匹配, 虽然可以有效控制特征点数量, 但可能出现特征点分布区域不均匀的情况, 如果图像重合区域较小则会导致匹配效果较差。</p>
                </div>
                <div class="p1">
                    <p id="92">针对上述问题, 本文采用局部极值选取的思路进行特征点选取, 即在不同尺度金字塔图像中选取<i>f</i><sub>HM</sub>在一定半径<i>r</i>范围内的局部极值。半径<i>r</i>的按照以下公式确定:</p>
                </div>
                <div class="p1">
                    <p id="93"><i>r</i>=-7+2×lb (max (<i>W</i><sub><i>l</i></sub>, <i>H</i><sub><i>l</i></sub>) /2)      (9) </p>
                </div>
                <div class="p1">
                    <p id="94">其中, <i>W</i><sub><i>l</i></sub>、<i>H</i><sub><i>l</i></sub>分别表示第<i>l</i>层金字塔图像的宽度和高度。通过以上方法可有效解决特征点分布不均匀的问题。</p>
                </div>
                <h4 class="anchor-tag" id="95" name="95">3.2 特征点局部极值的快速算法</h4>
                <div class="p1">
                    <p id="96">在传统的Harris算法中, 如果使用采用穷举法来进行局部特征点搜索, 则在<i>W</i>×<i>H</i>的图像、<i>w</i>×<i>w</i>的窗口大小条件下, 根据条件概率计算, 平均要进行<mathml id="97"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false"> (</mo><mi>W</mi><mo>-</mo><mi>w</mi><mo>+</mo><mn>1</mn><mo stretchy="false">) </mo><mo>×</mo><mo stretchy="false"> (</mo><mi>Η</mi><mo>-</mo><mi>w</mi><mo>+</mo><mn>1</mn><mo stretchy="false">) </mo><mo>×</mo><mi>k</mi><mo stretchy="false"> (</mo><mi>k</mi><mo>=</mo><mn>1</mn><mo>+</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mo>+</mo><mo>⋯</mo><mfrac><mn>1</mn><mrow><mi>w</mi><msup><mrow></mrow><mn>2</mn></msup><mo>-</mo><mn>1</mn></mrow></mfrac><mo stretchy="false">) </mo></mrow></math></mathml>次比较, 计算量较大。</p>
                </div>
                <div class="p1">
                    <p id="98">为提升局部特征点选取效率, 本文采用改进的<i>HGW</i> (<i>Herk</i>-<i>Gil</i>-<i>Werman</i>) 算法, 利用极大值滤波的思想<citation id="279" type="reference"><link href="201" rel="bibliography" /><link href="215" rel="bibliography" /><link href="219" rel="bibliography" /><sup>[<a class="sup">5</a>,<a class="sup">12</a>,<a class="sup">14</a>]</sup></citation>, 将最大值滤波的求取过程转化为二维窗口局部极值的求取, 以提升特征点局部极值的计算效率。具体步骤如下:</p>
                </div>
                <div class="p1">
                    <p id="99">1) 设矩阵<b><i>X</i></b>为图像的像素表示, <b><i>A</i></b>为<i>H</i>× (<i>w</i>-mod (<i>W</i>, <i>w</i>) ) 的全零矩阵 (mod为取余运算符) , 为了达到将图像均分为多个<i>w</i>×<i>w</i>矩阵的目的, 将图像扩充为<mathml id="100"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi mathvariant="bold-italic">X</mi></mstyle><mrow><mspace width="0.25em" /><mo>∼</mo></mrow></mover><mo>=</mo><mrow><mo>[</mo><mrow><mtable><mtr><mtd><mi mathvariant="bold-italic">X</mi></mtd><mtd><mi mathvariant="bold-italic">A</mi></mtd></mtr></mtable></mrow><mo>]</mo></mrow></mrow></math></mathml>。</p>
                </div>
                <div class="p1">
                    <p id="101">2) 将二维窗口极值计算转换为水平与垂直2个一维窗口极值计算过程:</p>
                </div>
                <div class="p1">
                    <p id="102" class="code-formula">
                        <mathml id="102"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mrow><mtable><mtr><mtd columnalign="left"><mo>-</mo><mi>w</mi><mo>≤</mo><mi>k</mi><mo>≤</mo><mi>w</mi></mtd></mtr><mtr><mtd columnalign="left"><mo>-</mo><mi>w</mi><mo>≤</mo><mi>l</mi><mo>≤</mo><mi>w</mi></mtd></mtr></mtable></mrow></munder><mo stretchy="false">{</mo><mi>f</mi><mo stretchy="false"> (</mo><mi>x</mi><mo>+</mo><mi>k</mi><mo>, </mo><mi>y</mi><mo>+</mo><mi>l</mi><mo stretchy="false">) </mo><mo stretchy="false">}</mo><mo>=</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mrow><mo>-</mo><mi>w</mi><mo>≤</mo><mi>l</mi><mo>≤</mo><mi>w</mi></mrow></munder><mo stretchy="false">{</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mrow><mo>-</mo><mi>w</mi><mo>≤</mo><mi>k</mi><mo>≤</mo><mi>w</mi></mrow></munder><mo stretchy="false">{</mo><mi>f</mi><mo stretchy="false"> (</mo><mi>x</mi><mo>+</mo><mi>k</mi><mo>, </mo><mi>y</mi><mo>+</mo><mi>l</mi><mo stretchy="false">) </mo><mo stretchy="false">}</mo><mo stretchy="false">}</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>0</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="103">记<mathml id="104"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi mathvariant="bold-italic">X</mi></mstyle><mrow><mspace width="0.25em" /><mo>∼</mo></mrow></mover></mrow></math></mathml>某一行的元素为x<sub>0</sub>, x<sub>1</sub>, …, x<sub>n-1</sub>, 考虑一维数组的局部最大值<mathml id="105"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mrow><mn>0</mn><mo>≤</mo><mi>j</mi><mo>&lt;</mo><mi>p</mi></mrow></munder><mi>x</mi><msub><mrow></mrow><mrow><mi>i</mi><mo>+</mo><mi>j</mi></mrow></msub><mspace width="0.25em" /><mo stretchy="false"> (</mo><mi>i</mi><mo>=</mo><mn>0</mn><mo>, </mo><mn>1</mn><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>n</mi><mo>-</mo><mi>w</mi><mo stretchy="false">) </mo></mrow></math></mathml>, 则用序列x<sub>w-1</sub>, x<sub>2w-1</sub>, x<sub>3w-1</sub>, …将该行元素分为多个数组, 记数组元素的下标为t。对于包括x<sub>t</sub>的每个小数组, 元素R<sub>k</sub>和Q<sub>k</sub> (k=0, 1, …, w-1) 定义如下:</p>
                </div>
                <div class="p1">
                    <p id="106">R<sub>k</sub>=R<sub>k</sub> (t) =<i>max</i> (x<sub>t</sub>, x<sub>t-1</sub>, …, x<sub>t-k</sub>) =<i>max</i> (R<sub>k-1</sub>, x<sub>t-k</sub>) </p>
                </div>
                <div class="p1">
                    <p id="107">Q<sub>k</sub>=Q<sub>k</sub> (t) =<i>max</i> (x<sub>t</sub>, x<sub>t+1</sub>, …, x<sub>t+k</sub>) =<i>max</i> (Q<sub>k-1</sub>, x<sub>t+k</sub>)      (11) </p>
                </div>
                <div class="p1">
                    <p id="108">通过以上步骤处理每个数组需要进行2 (w-1) 次比较。因此, 为提高提高R<sub>k</sub>和Q<sub>k</sub>的计算速度, 本文采用以下的改进公式:</p>
                </div>
                <div class="p1">
                    <p id="109">q=<image href="images/JSJC201908038_110.jpg" type="" display="inline" placement="inline"><alt></alt></image><mathml id="111"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mi>w</mi><mo>+</mo><mn>1</mn></mrow><mn>2</mn></mfrac></mrow></math></mathml><image href="images/JSJC201908038_112.jpg" type="" display="inline" placement="inline"><alt></alt></image><mathml id="113"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo>=</mo><mfrac><mi>w</mi><mn>2</mn></mfrac><mo>+</mo><mfrac><mrow><mi>w</mi><mtext> </mtext><mtext>m</mtext><mtext>o</mtext><mtext>d</mtext><mtext> </mtext><mn>2</mn></mrow><mn>2</mn></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml></p>
                </div>
                <div class="p1">
                    <p id="114"> (1) 计算<i>Q</i><sub><i>k</i></sub> (<i>k</i>=0, 1, …, <i>q</i>-1) 和<i>R</i><sub><i>k</i></sub> (<i>t</i>+1) (<i>k</i>=<i>q</i>, <i>q</i>+1, …, <i>w</i>) , 比较次数为<i>q</i>-1+<i>w</i>-<i>q</i>=<i>w</i>-1。</p>
                </div>
                <div class="p1">
                    <p id="115"> (2) 比较<i>Q</i><sub><i>q</i>-1</sub>和<i>R</i><sub><i>q</i></sub> (<i>t</i>+1) 。因为<i>Q</i><sub><i>k</i></sub>是非递减数列, <i>R</i><sub><i>q</i></sub> (<i>t</i>+1) 是非递增数列, 所以当<i>R</i><sub><i>q</i></sub>≥<i>Q</i><sub><i>q</i>-1</sub>时, <i>R</i><sub><i>q</i>-1</sub>=<i>R</i><sub><i>q</i>-2</sub>=…=<i>R</i><sub>1</sub>=<i>R</i><sub>0</sub>。</p>
                </div>
                <div class="p1">
                    <p id="116"> (3) 计算<i>Q</i><sub><i>q</i></sub>, <i>Q</i><sub><i>q</i>+1</sub>…, <i>Q</i><sub><i>w</i>-1</sub>, 比较次数为<i>w</i>-<i>q</i>。同第2步, 若<i>R</i><sub><i>q</i></sub>≤<i>Q</i><sub><i>q</i>-1</sub>, 则无须计算<i>Q</i><sub><i>q</i></sub>, <i>Q</i><sub><i>q</i>+1</sub>…<i>Q</i><sub><i>w</i>-1</sub>, 只要进行<i>q</i>-1次比较, 即可得到<i>R</i><sub>1</sub>, <i>R</i><sub>2</sub>…, <i>R</i><sub><i>q</i>-1</sub>。</p>
                </div>
                <div class="p1">
                    <p id="117">通过以上计算步骤, 计算<i>R</i><sub><i>k</i></sub>和<i>Q</i><sub><i>k</i></sub>平均需要进行 (<i>w</i>-1) +1+max (<i>w</i>-<i>q</i>, <i>q</i>-1) =1.5<i>w</i>- (<i>w</i> mod 2) /2次比较。</p>
                </div>
                <div class="p1">
                    <p id="118">3) 合并该行元素的<i>R</i><sub><i>k</i></sub>和<i>Q</i><sub><i>k</i></sub>, 即可得到以下窗口最大值的函数:</p>
                </div>
                <div class="p1">
                    <p id="119"><i>t</i><sub><i>k</i></sub>=max (<i>x</i><sub><i>j</i>-<i>k</i></sub>, <i>x</i><sub><i>j</i>-<i>k</i>+1</sub>, …, <i>x</i><sub><i>j</i></sub>, …, <i>x</i><sub><i>j</i>+<i>w</i>-<i>k</i>-1</sub>) =</p>
                </div>
                <div class="p1">
                    <p id="120">max (<i>R</i><sub><i>k</i></sub>, <i>Q</i><sub><i>w</i>-<i>k</i>-1</sub>)      (13) </p>
                </div>
                <div class="p1">
                    <p id="121">因为<i>R</i><sub><i>w</i>-2</sub>≥<i>Rw</i>-1≥…≥<i>R</i><sub>1</sub>并且<i>Q</i><sub><i>w</i>-2</sub>≥<i>Q</i><sub><i>w</i>-1</sub>≥…≥<i>Q</i><sub>1</sub>, 如果<i>R</i><sub><i>i</i></sub>≥<i>Q</i><sub><i>w</i>-<i>i</i>-1</sub>, 则对任意<i>k</i>&gt;<i>i</i>, 有<i>R</i><sub><i>k</i></sub>≥<i>R</i><sub><i>i</i></sub>≥<i>S</i><sub><i>w</i>-<i>i</i>-1</sub>≥<i>S</i><sub><i>w</i>-<i>k</i>-1</sub>, 所以当<i>k</i>&gt;<i>i</i>时无需进行比较。同理, 在<i>R</i><sub><i>i</i></sub>≤<i>S</i><sub><i>w</i>-<i>i</i>-1</sub>条件下, 若<i>k</i>&lt;<i>i</i>则进行无需比较。结合二叉树检索的时间复杂度, 可知本步骤的元素搜索平均次数为「lb (<i>w</i>-1) ⎤/<i>w</i>。</p>
                </div>
                <div class="p1">
                    <p id="122">使用一维窗口的最大值<i>t</i><sub><i>k</i></sub>代替<mathml id="123"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi mathvariant="bold-italic">X</mi></mstyle><mrow><mspace width="0.25em" /><mo>∼</mo></mrow></mover></mrow></math></mathml>的相应元素, 可得到矩阵<b><i>Y</i></b>。本步骤中每个元素的平均比较次数为:</p>
                </div>
                <div class="p1">
                    <p id="124" class="code-formula">
                        <mathml id="124"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>C</mi><mi>o</mi><mi>m</mi><mo>=</mo><mn>1</mn><mo>.</mo><mn>5</mn><mo>+</mo><mfrac><mrow><mo>⌈</mo><mtext>l</mtext><mtext>b</mtext><mo stretchy="false"> (</mo><mi>w</mi><mo>-</mo><mn>1</mn><mo stretchy="false">) </mo><mo>⌉</mo></mrow><mi>w</mi></mfrac><mo>-</mo><mfrac><mrow><mo stretchy="false"> (</mo><mi>w</mi><mtext> </mtext><mtext>m</mtext><mtext>o</mtext><mtext>d</mtext><mtext> </mtext><mn>2</mn><mo stretchy="false">) </mo></mrow><mrow><mn>2</mn><mi>w</mi></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>4</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="125">矩阵<b><i>Y</i></b>的列元素局部极值可通过重复第1个步骤和第2个步骤求取。通过本步骤获得的矩阵<mathml id="126"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi mathvariant="bold-italic">Y</mi></mstyle><mrow><mspace width="0.25em" /><mo>∼</mo></mrow></mover></mrow></math></mathml>即为2维窗口局部极值分布矩阵, 用以表征某元素w半径区域内的极值分布。</p>
                </div>
                <div class="p1">
                    <p id="127">去除<mathml id="128"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi mathvariant="bold-italic">Y</mi></mstyle><mrow><mspace width="0.25em" /><mo>∼</mo></mrow></mover></mrow></math></mathml>中的扩充矩阵, 并将其与<b><i>X</i></b>矩阵进行减运算, 得到的结果矩阵中0值表示对应的像素点在<i>w</i>×<i>w</i>窗口范围内没有比它更大的值, 即特征点。综上可知全部比较次数平均值为:</p>
                </div>
                <div class="p1">
                    <p id="129" class="code-formula">
                        <mathml id="129"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>C</mi><mi>o</mi><mi>m</mi><msub><mrow></mrow><mrow><mtext>t</mtext><mtext>o</mtext><mtext>t</mtext><mtext>a</mtext><mtext>l</mtext></mrow></msub><mo>=</mo><mn>3</mn><mo>+</mo><mfrac><mrow><mn>2</mn><mo>⌈</mo><mtext>l</mtext><mtext>b</mtext><mo stretchy="false"> (</mo><mi>w</mi><mo>-</mo><mn>1</mn><mo stretchy="false">) </mo><mo>⌉</mo></mrow><mi>w</mi></mfrac><mo>-</mo><mfrac><mrow><mo stretchy="false"> (</mo><mi>w</mi><mtext> </mtext><mtext>m</mtext><mtext>o</mtext><mtext>d</mtext><mtext> </mtext><mn>2</mn><mo stretchy="false">) </mo></mrow><mi>w</mi></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>5</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="130">各算法每个元素比较运算次数如图1所示。从中可以看出窗口大小对于常规算法、HGW算法及改进的HGW算法的影响。随着窗口的增大, 常规算法单个元素比较次数随之增加, 而HGW算法及改进的HGW算法比较次数逐渐收敛到固定数值。进一步分析可知, 改进的HGW算法的单元素比较次数趋向于3, 明显少于传统HGW的6次。因此, 改进的HGW算法伴随窗口增大其优势更加明显。</p>
                </div>
                <div class="area_img" id="131">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201908038_131.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 不同窗口条件下各算法平均计算次数对比" src="Detail/GetImg?filename=images/JSJC201908038_131.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图1 不同窗口条件下各算法平均计算次数对比</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201908038_131.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h3 id="132" name="132" class="anchor-tag">4 KCF目标跟踪算法</h3>
                <div class="p1">
                    <p id="133">KCF目标跟踪算法将目标的检测跟踪任务分解成了3个部分:跟踪, 学习, 检测。这3个子任务相互独立并且同时运行。跟踪器中使用传统NCC算法进行图像匹配以滤除匹配效果差的像素点。设<i>T</i>为模板图像 (<i>M</i><sub>2</sub>×<i>N</i><sub>2</sub>大小) , <i>F</i>为待匹配图像 (<i>M</i><sub>1</sub>×<i>N</i><sub>1</sub>大小) , 选取待匹配图像<i>F</i>的左上角为基准点, 在待匹配图像<i>F</i>中移动模板<i>T</i>后所得的搜索子图及模板位于 (<i>m</i>, <i>n</i>) 处的像素分别为<i>F</i><sup><i>ij</i></sup> (<i>i</i>+<i>m</i>, <i>j</i>+<i>n</i>) (其中0≤<i>i</i>≤<i>M</i><sub>1</sub>-<i>M</i><sub>2</sub>, 0≤<i>i</i>≤<i>N</i><sub>1</sub>-<i>N</i><sub>2</sub>) 以及<i>T</i> (<i>m</i>, <i>n</i>) 。找出模板图像以及待匹配图像中所有匹配点的最大相关值即NCC算法的主要任务<citation id="280" type="reference"><link href="247" rel="bibliography" /><sup>[<a class="sup">28</a>]</sup></citation>。经典去均值NCC算法的计算公式如式 (16) ～式 (18) 所示。</p>
                </div>
                <div class="p1">
                    <p id="134" class="code-formula">
                        <mathml id="134"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtext> </mtext><mtext> </mtext><mi>R</mi><mo stretchy="false"> (</mo><mi>i</mi><mo>, </mo><mi>j</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>m</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>Μ</mi><msub><mrow></mrow><mn>2</mn></msub><mo>-</mo><mn>1</mn></mrow></munderover><mrow></mrow></mstyle></mrow><mtext> </mtext><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>n</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>Ν</mi><msub><mrow></mrow><mn>2</mn></msub><mo>-</mo><mn>1</mn></mrow></munderover><mrow></mrow></mstyle></mrow><mo stretchy="false"> (</mo><mi>F</mi><msup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msup><mo stretchy="false"> (</mo><mi>i</mi><mo>+</mo><mi>m</mi><mo>, </mo><mi>j</mi><mo>+</mo><mi>n</mi><mo stretchy="false">) </mo><mo>-</mo><mover><mstyle mathsize="140%" displaystyle="true"><mi>F</mi></mstyle><mrow><mspace width="0.25em" /><mo>—</mo></mrow></mover><mo stretchy="false"> (</mo><mi>i</mi><mo>, </mo><mi>j</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo stretchy="false"> (</mo><mi>Τ</mi><mo stretchy="false"> (</mo><mi>m</mi><mo>, </mo><mi>n</mi><mo stretchy="false">) </mo><mo>-</mo><mover><mstyle mathsize="140%" displaystyle="true"><mi>Τ</mi></mstyle><mrow><mspace width="0.25em" /><mo>—</mo></mrow></mover><mo stretchy="false">) </mo></mrow><mrow><msqrt><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>m</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>Μ</mi><msub><mrow></mrow><mn>2</mn></msub><mo>-</mo><mn>1</mn></mrow></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>n</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>Ν</mi><msub><mrow></mrow><mn>2</mn></msub><mo>-</mo><mn>1</mn></mrow></munderover><mrow><mo stretchy="false"> (</mo><mi>F</mi><msup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msup><mo stretchy="false"> (</mo><mi>i</mi><mo>+</mo><mi>m</mi><mo>, </mo><mi>j</mi><mo>+</mo><mi>n</mi><mo stretchy="false">) </mo><mo>-</mo><mover><mstyle mathsize="140%" displaystyle="true"><mi>F</mi></mstyle><mrow><mspace width="0.25em" /><mo>—</mo></mrow></mover><mo stretchy="false"> (</mo><mi>i</mi><mo>, </mo><mi>j</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo></mrow></mstyle></mrow></mstyle><msup><mrow></mrow><mn>2</mn></msup><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>m</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>Μ</mi><msub><mrow></mrow><mn>2</mn></msub><mo>-</mo><mn>1</mn></mrow></munderover><mrow></mrow></mstyle></mrow><mtext> </mtext><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>n</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>Ν</mi><msub><mrow></mrow><mn>2</mn></msub><mo>-</mo><mn>1</mn></mrow></munderover><mrow></mrow></mstyle></mrow><mo stretchy="false"> (</mo><mi>Τ</mi><mo stretchy="false"> (</mo><mi>m</mi><mo>, </mo><mi>n</mi><mo stretchy="false">) </mo><mo>-</mo><mover><mstyle mathsize="140%" displaystyle="true"><mi>Τ</mi></mstyle><mrow><mspace width="0.25em" /><mo>—</mo></mrow></mover><mo stretchy="false">) </mo><msup><mrow></mrow><mn>2</mn></msup></mrow></msqrt></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>6</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="135" class="code-formula">
                        <mathml id="135"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mover><mstyle mathsize="140%" displaystyle="true"><mi>F</mi></mstyle><mrow><mspace width="0.25em" /><mo>—</mo></mrow></mover><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mfrac><mn>1</mn><mrow><mi>Μ</mi><msub><mrow></mrow><mn>2</mn></msub><mo>×</mo><mi>Ν</mi><msub><mrow></mrow><mn>2</mn></msub></mrow></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>m</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>Μ</mi><msub><mrow></mrow><mn>2</mn></msub><mo>-</mo><mn>1</mn></mrow></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>n</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>Ν</mi><msub><mrow></mrow><mn>2</mn></msub><mo>-</mo><mn>1</mn></mrow></munderover><mi>F</mi></mstyle></mrow></mstyle><msup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msup><mo stretchy="false"> (</mo><mi>i</mi><mo>+</mo><mi>m</mi><mo>, </mo><mi>j</mi><mo>+</mo><mi>n</mi><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>7</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mover><mstyle mathsize="140%" displaystyle="true"><mi>Τ</mi></mstyle><mrow><mspace width="0.25em" /><mo>—</mo></mrow></mover><mo>=</mo><mfrac><mn>1</mn><mrow><mi>Μ</mi><msub><mrow></mrow><mn>2</mn></msub><mo>×</mo><mi>Ν</mi><msub><mrow></mrow><mn>2</mn></msub></mrow></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>m</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>Μ</mi><msub><mrow></mrow><mn>2</mn></msub><mo>-</mo><mn>1</mn></mrow></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>n</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>Ν</mi><msub><mrow></mrow><mn>2</mn></msub><mo>-</mo><mn>1</mn></mrow></munderover><mi>Τ</mi></mstyle></mrow></mstyle><mo stretchy="false"> (</mo><mi>m</mi><mo>, </mo><mi>n</mi><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>8</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <h3 id="136" name="136" class="anchor-tag">5 实时特征检测与目标跟踪的AR三维注册</h3>
                <div class="p1">
                    <p id="137">设投影点<i>X</i><sub><i>P</i></sub> (<i>u</i>, <i>v</i>) 与待注册图形上任意点<i>P</i> (<i>X</i>, <i>Y</i>, <i>Z</i>) 之间坐标变换关系如式 (19) 所示。</p>
                </div>
                <div class="p1">
                    <p id="138" class="code-formula">
                        <mathml id="138"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>h</mi><mrow><mo>[</mo><mrow><mtable><mtr><mtd><mi>u</mi></mtd></mtr><mtr><mtd><mi>v</mi></mtd></mtr><mtr><mtd><mn>1</mn></mtd></mtr></mtable></mrow><mo>]</mo></mrow><mo>=</mo><mrow><mo>[</mo><mrow><mtable><mtr><mtd><mi>s</mi><msub><mrow></mrow><mi>x</mi></msub><mi>f</mi></mtd><mtd><mn>0</mn></mtd><mtd><mi>u</mi><msub><mrow></mrow><mn>0</mn></msub></mtd><mtd><mn>0</mn></mtd></mtr><mtr><mtd><mn>0</mn></mtd><mtd><mi>s</mi><msub><mrow></mrow><mi>y</mi></msub><mi>f</mi></mtd><mtd><mi>v</mi><msub><mrow></mrow><mn>0</mn></msub></mtd><mtd><mn>0</mn></mtd></mtr><mtr><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd><mtd><mn>1</mn></mtd><mtd><mn>0</mn></mtd></mtr></mtable></mrow><mo>]</mo></mrow><mrow><mo>[</mo><mrow><mtable><mtr><mtd><mi>X</mi><msub><mrow></mrow><mi>C</mi></msub></mtd></mtr><mtr><mtd><mi>Y</mi><msub><mrow></mrow><mi>C</mi></msub></mtd></mtr><mtr><mtd><mi>Ζ</mi><msub><mrow></mrow><mi>C</mi></msub></mtd></mtr><mtr><mtd><mn>1</mn></mtd></mtr></mtable></mrow><mo>]</mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>9</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="139">其中:尺度因子为<i>s</i><sub><i>x</i></sub>, <i>s</i><sub><i>y</i></sub>;<i>f</i>是摄像机焦距;图像重心的坐标为<i>u</i><sub>0</sub>、<i>v</i><sub>0</sub>。通过获取摄像头的参数矩阵<b><i>C</i></b>, 并且利用算法第一步可得到初始三维注册矩阵<b><i>T</i></b>′<sub><i>m</i></sub>。AR系统的场景中不存在待注册图形时, 将进行特征模板的匹配。具体步骤分为以下5个部分:1) 获取现实图像;2) 提取现实图像特征点;3) 对于所有特征点, 利用特征模板进行运算;4) 选出相关度较大的特征点作为当前图像特征对应的点;5) 进行增强现实三维注册。</p>
                </div>
                <h3 id="140" name="140" class="anchor-tag">6 实验结果与分析</h3>
                <h4 class="anchor-tag" id="141" name="141">6.1 实验平台</h4>
                <div class="p1">
                    <p id="142">对本文方法进行实验, 具体实验平台描述如下:硬件平台包括摄像头和PC机 (CPU为I5, 内存为8 GB) ;软件平台为Windows7操作系统、VS2013+OpenCV2.4.10和OpenGL图形库。</p>
                </div>
                <h4 class="anchor-tag" id="143" name="143">6.2 对比算法</h4>
                <div class="p1">
                    <p id="144">为检验本文方法在特征点提取、匹配效果等方面的表现, 以SIFT算法及Harris算法作为对比对象进行实验。图像样本分辨率为640 像素×480 像素。实验结果表明, Harris算法提取的特征点更集中在边缘和角点处。相对于SIFT和Harris算法, 本文提出的快速MOPS算法其优势在于更均匀的特征点分布。由于快速MOPS算法只是对MOPS算法的具体计算比较过程进行了优化, 对计算精度没有改变, 因此对特征点匹配的准确率没有影响, 反映在实验结果中, 则表现为特征点匹配准确/错误的数量保持一致。</p>
                </div>
                <h4 class="anchor-tag" id="145" name="145">6.3 实验结果</h4>
                <div class="p1">
                    <p id="146">4种算法的匹配性能比较如表1所示。可以看出, Harris算法运行速度最快, 但误匹配率较高。SIFT算法效果最好, 但运算最慢。本文提出快速MOPS算法在运行时间上与传统Harris算法比较接近, 匹配正确数量方面相对于表现最好的SIFT算法相差不大, 但是运行时间远少于SIFT算法, 同时在匹配错误方面表现最好, 匹配错误数量最少。综合实验结果可以看出, 本文提出的快速MOPS算法兼具SIFT和Harris算法的优点, 在获得较高处理效率的同时, 能够取得较好的匹配效果, 可以满足实时增强现实系统的速度与效果需求。</p>
                </div>
                <div class="area_img" id="147">
                    <p class="img_tit"><b>表1 4种算法匹配性能对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="147" border="1"><tr><td><br />算法</td><td>平均运行时间/s</td><td>特征点匹配<br />正确数量</td><td>特征点匹配<br />错误数量</td></tr><tr><td><br />SIFT算法</td><td>3.74</td><td>246</td><td>21</td></tr><tr><td><br />MOPS算法</td><td>0.92</td><td>215</td><td>24</td></tr><tr><td><br />快速MOPS算法</td><td>0.70</td><td>215</td><td>24</td></tr><tr><td><br />Harris算法</td><td>0.49</td><td>227</td><td>152</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="148">对本文提出的三维注册方法进行实验, 图2所示为摄像头初始捕捉图像, 图3所示为目标跟踪的初始场景, 图4所示为本文算法进行三维注册的结果。</p>
                </div>
                <div class="area_img" id="149">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201908038_149.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 摄像头初始捕捉图像" src="Detail/GetImg?filename=images/JSJC201908038_149.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图2 摄像头初始捕捉图像</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201908038_149.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="150">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201908038_150.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 目标跟踪初始场景" src="Detail/GetImg?filename=images/JSJC201908038_150.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图3 目标跟踪初始场景</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201908038_150.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="151">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201908038_151.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 本文方法的三维注册结果" src="Detail/GetImg?filename=images/JSJC201908038_151.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图4 本文方法的三维注册结果</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201908038_151.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="152">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201908038_152.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 TLD+CT目标跟踪算法与SURF、SIFT、FAST算法结合的三维注册结果" src="Detail/GetImg?filename=images/JSJC201908038_152.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 TLD+CT目标跟踪算法与SURF、SIFT、FAST算法结合的三维注册结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201908038_152.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="154">可以看出, 本文注册方法在上述2种情况下均可成功完成AR系统注册, 而TLD+CT目标跟踪算法与SURF、SIFT、FAST算法结合都无法进行三维注册, 如图5所示, 这主要是由于目标跟踪有延迟以及特征检测算法的精度不高。</p>
                </div>
                <h4 class="anchor-tag" id="155" name="155">6.4 结果分析</h4>
                <div class="p1">
                    <p id="156">以SURF算法作为参照对象进行三维注册对比实验。实验内容包括注册区域平移、旋转和缩放等操作, 分别以50帧、150帧、250帧耗时进行计算时间统计, 通过1 000次对比实验结果统计平均, 最终注册运算时间对比结果如表2所示。</p>
                </div>
                <div class="area_img" id="157">
                    <p class="img_tit"><b>表2 平均运算时间对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"> s</p>
                    <table id="157" border="1"><tr><td><br />帧数</td><td>基于SURF的方法</td><td>本文方法</td></tr><tr><td><br />50</td><td>0.208</td><td>0.102</td></tr><tr><td><br />150</td><td>0.214</td><td>0.121</td></tr><tr><td><br />250</td><td>0.239</td><td>0.147</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="158">可以看出, 本文三维注册方法在不同帧数条件下平均耗时相比于基于SURF特征检测的三维注册方法运算时间均缩短了约1/3。由此可见基于实时特征检测与目标跟踪的增强现实三维注册方法有效提高了AR系统的实时性、稳定性以及鲁棒性。</p>
                </div>
                <h3 id="159" name="159" class="anchor-tag">7 结束语</h3>
                <div class="p1">
                    <p id="160">本文提出基于KCF与改进ORB算法的增强现实系统三维注册方法, 通过设计SIFT-ORB-MRANSAC融合算法, 完成特征点的提取、匹配以及去除误匹配。针对增强现实系统在虚拟信息叠加过程中待注册区域被遮挡的情况, 对待注册的目标区域采用KCF算法进行跟踪, 改进ORB算法并将其应用到跟踪目标区域中, 在包含尺度、旋转变化等情况的复杂环境下, 使注册的成功率以及运算效率均得到提升。通过对比执行时间, 验证了本文方法在保证注册效果的同时, 仍有较好的运算效率, 可满足实时增强现实系统的需求。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="193">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A survey of augmented reality">

                                <b>[1]</b> BILLINGHURST M, CLARK A, LEE G.A survey of augmented reality[J].Foundations and Trends in Human-Computer Interaction, 2015, 8 (2/3) :73-272.
                            </a>
                        </p>
                        <p id="195">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=SHJT201801015&amp;v=MzAxOTFNcm85RVlZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVScUZDam5VYjNMTmlYQmVyRzRIOW4=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> 王月, 张树生, 何卫平, 等.基于模型的增强现实无标识三维注册追踪方法[J].上海交通大学学报, 2018, 52 (1) :83-89.
                            </a>
                        </p>
                        <p id="197">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XTFZ201807023&amp;v=MTgwMDc0SDluTXFJOUhaNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUnFGQ2puVWIzTFBUbk5kTEc=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> 李乾, 高尚兵, 潘志庚, 等.基于无标记识别的增强现实方法研究[J].系统仿真学报, 2018, 30 (7) :191-197.
                            </a>
                        </p>
                        <p id="199">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002830901&amp;v=MTEzMzd0SE9wNHhGYmVzT1kzazV6QmRoNGo5OVNYcVJyeG94Y01IN1I3cWVidWR0RkN6bVU3M0tJbDA9Tmo3QmFyTzRI&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b> LOWE D G.Distinctive image features from scale-invariant keypoints[J].International Journal of Computer Vision, 2004, 60 (2) :91-110.
                            </a>
                        </p>
                        <p id="201">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=YGJS201301009&amp;v=MTU0NTdCdEdGckNVUkxPZVplUnFGQ2puVWIzTFBDckJmYkc0SDlMTXJvOUZiWVFLREg4NHZSNFQ2ajU0TzN6cXE=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b> 欧阳能钧, 李伟彤, 韦蔚, 等.基于SIFT与Contourlee变换分辨遥感图像配准[J].遥感技术与应用, 2013, 28 (1) :58-64.
                            </a>
                        </p>
                        <p id="203">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Error-correcting barcoded primers for pyrosequencing hundreds of samples in multiplex">

                                <b>[6]</b> BAY H, WALKER J J, HARRIS J K.et al.Error-correcting barcoded primers for pyrosequencing hundreds of samples in multiplex[J].Nature Methods, 2008, 5 (3) :235-237.
                            </a>
                        </p>
                        <p id="205">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=ORB:an efficient alternative to SIFT or SURF">

                                <b>[7]</b> RUBLEE E, RABAUD V, KONOLIGE K, et al.ORB:an efficient alternative to SIFT or SURF[C]//Proceedings of IEEE International Conference on Computer Vision.Washington D.C., USA:IEEE Press, 2012:2564-2571.
                            </a>
                        </p>
                        <p id="207">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Online object tracking:A benchmark">

                                <b>[8]</b> WU Yi, LIM J, YANG M H.Online object tracking:a benchmark[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2013:2411-2418.
                            </a>
                        </p>
                        <p id="209">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Tracking-Learning-Detection">

                                <b>[9]</b> KALAL Z, MIKOLAJCZYK K, MATAS J.Tracking-learning-detection[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2012, 34 (7) :1409-1422.
                            </a>
                        </p>
                        <p id="211">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Struck:structured output tracking with kernels">

                                <b>[10]</b> HARE S, SAFFARI A, TORR P H S.Struck:structured output tracking with kernels[C]//Proceedings of IEEE International Conference on Computer Vision.Washington D.C., USA:IEEE Press, 2011:263-270.
                            </a>
                        </p>
                        <p id="213">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Real-time compressive tracking">

                                <b>[11]</b> ZHANG Kaihua, ZHANG Lei, YANG M H.Real-time compressive tracking[C]//Proceedings of European Conference on Computer Vision.Washington D.C., USA:IEEE Press, 2012:864-877.
                            </a>
                        </p>
                        <p id="215">
                            <a id="bibliography_12" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XTFZ201409034&amp;v=MTIyODdlWmVScUZDam5VYjNMUFRuTmRMRzRIOVhNcG85R1lJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE8=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[12]</b> 李炎, 尹东.基于TLD的增强现实跟踪注册方法[J].系统仿真学报, 2014, 26 (9) :2062-2067.
                            </a>
                        </p>
                        <p id="217">
                            <a id="bibliography_13" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GDGC201404002&amp;v=Mjc3NTMzenFxQnRHRnJDVVJMT2VaZVJxRkNqblViM0xJaW5NYmJHNEg5WE1xNDlGWm9RS0RIODR2UjRUNmo1NE8=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[13]</b> 钟权, 周进, 吴钦章, 等.一种改进的实时压缩跟踪算法[J].光电工程, 2014, 41 (4) :1-8.
                            </a>
                        </p>
                        <p id="219">
                            <a id="bibliography_14" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=CHTB201901017&amp;v=MTAzODNmYkxHNEg5ak1ybzlFWTRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJxRkNqblViM0xKaVg=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[14]</b> 荆圣媛, 韩勇, 孟学文, 等.移动AR+VR支持下旅游GIS系统的设计与实现[J].测绘通报, 2019 (1) :79-84.
                            </a>
                        </p>
                        <p id="221">
                            <a id="bibliography_15" >
                                    <b>[15]</b>
                                 HENRIQUES J F, CASEIRO R, PEDRO M, et al.High-speed tracking with kernelized correlation filters[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2015, 37 (3) :583-596.
                            </a>
                        </p>
                        <p id="223">
                            <a id="bibliography_16" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201908044&amp;v=MTQ5NDViRzRIOWpNcDQ5QllJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVScUZDam5VYjNMTHo3QmI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[16]</b> 王任华, 沈剑宇, 蒋敏.基于自适应多模型联合的目标跟踪算法[J/OL].计算机工程:1-9[2019-06-21].https://doi.org/10.19678/j.issn.1000-3428.0052593.
                            </a>
                        </p>
                        <p id="225">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=MAC-RANSAC:a robust algorithm for the recognition of multiple objects">

                                <b>[17]</b> RABIN J, DELON J, GOUSSEAU Y, et al.MAC-RANSAC:a robust algorithm for the recognition of multiple objects[C]//Proceedings of the 5th International Symposium on 3D Data Processing, Visualization and Transmission.Paris, France:[s.n.], 2010:51.
                            </a>
                        </p>
                        <p id="227">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Machine learning for highspeed corner detection">

                                <b>[18]</b> ROSTEN E, DRUMMOND T.Machine learning for highspeed corner detection[C]//Proceedings of European Conference on Computer Vision.Graz, Austria:[s.n.], 2006:430-443.
                            </a>
                        </p>
                        <p id="229">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Brief:binary robust independent elementary features">

                                <b>[19]</b> CALONDER M, LEPETIT V, STRECHA C, et al.Brief:binary robust ipendent elementary features[C]//Proceedings of European Conference on Computer Vision.Heraklion, Greece:[s.n.], 2010:778-792.
                            </a>
                        </p>
                        <p id="231">
                            <a id="bibliography_20" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501084512&amp;v=MDg5MDlpZk9mYks3SHRETnFvOUVaT01MQ1gwN29CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVUx2TElGd1dhQkU9Tg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[20]</b> ROSIN P L.Measuring corner properties[J].Computer Vision and Image Understanding, 1999, 73 (2) :291-307.
                            </a>
                        </p>
                        <p id="233">
                            <a id="bibliography_21" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201201083&amp;v=MzIyMDllUnFGQ2puVWIzTEx6N0JiYkc0SDlQTXJvOU5aNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVo=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[21]</b> 周见光, 石刚, 马小虎.增强现实系统中的虚拟交互方法[J].计算机工程, 2012, 38 (1) :251-252.
                            </a>
                        </p>
                        <p id="235">
                            <a id="bibliography_22" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZGTB200207010&amp;v=MDk3NTh0R0ZyQ1VSTE9lWmVScUZDam5VYjNMUHlyZmJMRzRIdFBNcUk5RVpJUUtESDg0dlI0VDZqNTRPM3pxcUI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[22]</b> 施琦, 王涌天, 陈靖.一种基于视觉的增强现实三维注册算法[J].中国图象图形学报, 2018, 7 (7) :679-683.
                            </a>
                        </p>
                        <p id="237">
                            <a id="bibliography_23" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=HEBY201702015&amp;v=MjQyMjFyQ1VSTE9lWmVScUZDam5VYjNMTFNqSmQ3RzRIOWJNclk5RVlZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0Y=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[23]</b> 顾庆传, 姜娜.基于FAST特征点检测的实时虚实注册方法[J].哈尔滨师范大学自然科学学报, 2017, 33 (2) :58-62.
                            </a>
                        </p>
                        <p id="239">
                            <a id="bibliography_24" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GLDZ201805006&amp;v=MTg1ODhaZVJxRkNqblViM0xJaUhQZExHNEg5bk1xbzlGWW9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2U=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[24]</b> 张亚玲, 朱望纯.基于物体识别的增强现实装配指导[J].桂林电子科技大学学报, 2018, 38 (5) :29-32.
                            </a>
                        </p>
                        <p id="241">
                            <a id="bibliography_25" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CDFD&amp;filename=1018055155.nh&amp;v=MDY0MjIyNkZyTzlHOURKcXBFYlBJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVScUZDam5VYjNMVkY=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[25]</b> 谭暑秋.面部动态增强现实的特征点检测方法研究[D].杭州:电子科技大学, 2018.
                            </a>
                        </p>
                        <p id="243">
                            <a id="bibliography_26" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=SYSX201802010&amp;v=MjQ5NDZPZVplUnFGQ2puVWIzTE5qVFlkckc0SDluTXJZOUVaSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkw=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[26]</b> 张浩华, 吴艳敏, 程立英, 等.基于移动端平台古生物博物馆增强现实系统的设计与应用[J].沈阳师范大学学报 (自然科学版) , 2018, 36 (2) :61-69.
                            </a>
                        </p>
                        <p id="245">
                            <a id="bibliography_27" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=HEBY201702018&amp;v=Mjc1MDY0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVScUZDam5VYjNMTFNqSmQ3RzRIOWJNclk5RWJJUUtESDg0dlI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[27]</b> 王小红.基于TLD与SIFT的增强现实三维注册方法[J].哈尔滨师范大学自然科学学报, 2017, 33 (2) :71-74.
                            </a>
                        </p>
                        <p id="247">
                            <a id="bibliography_28" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XXWX201505048&amp;v=MTMwMzBUTXFvOUJiSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUnFGQ2puVWIzTFBUWGNkckc0SDk=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[28]</b> 邢藏菊, 温兰兰, 何苏勤.TLD视频目标跟踪器快速匹配的研究[J].小型微型计算机系统, 2015, 36 (5) :1113-1116.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJC201908038" />
        <input id="dpi" type="hidden" value="600" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201908038&amp;v=MTY1MTJwNDlHYklRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJxRkNqblViM0xMejdCYmJHNEg5ak0=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
