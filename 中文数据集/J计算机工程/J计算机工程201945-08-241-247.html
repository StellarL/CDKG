<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637129055063556250%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJC201908040%26RESULT%3d1%26SIGN%3dsJsVMDONOIA6D8sUkB8Fy%252b59wjM%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201908040&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201908040&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201908040&amp;v=MTI2MjM0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVScUZDam5VYjdOTHo3QmJiRzRIOWpNcDQ5QlpJUUtESDg=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#39" data-title="0 概述 ">0 概述</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#44" data-title="1 LCT跟踪算法 ">1 LCT跟踪算法</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#46" data-title="1.1 核相关滤波">1.1 核相关滤波</a></li>
                                                <li><a href="#60" data-title="1.2 模型更新与重检测">1.2 模型更新与重检测</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#66" data-title="2 改进的LCT跟踪算法 ">2 改进的LCT跟踪算法</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#68" data-title="2.1 综合颜色直方图">2.1 综合颜色直方图</a></li>
                                                <li><a href="#81" data-title="2.2 特征算法自适应融合">2.2 特征算法自适应融合</a></li>
                                                <li><a href="#95" data-title="2.3 改进算法流程">2.3 改进算法流程</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#106" data-title="3 实验结果与分析 ">3 实验结果与分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#108" data-title="3.1 跟踪性能指标对比">3.1 跟踪性能指标对比</a></li>
                                                <li><a href="#117" data-title="3.2 跟踪效果对比">3.2 跟踪效果对比</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#121" data-title="4 结束语 ">4 结束语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#80" data-title="&lt;b&gt;图1 综合颜色直方图效果&lt;/b&gt;"><b>图1 综合颜色直方图效果</b></a></li>
                                                <li><a href="#111" data-title="&lt;b&gt;表1 4种算法特征描述&lt;/b&gt;"><b>表1 4种算法特征描述</b></a></li>
                                                <li><a href="#114" data-title="&lt;b&gt;图2 距离精度与重叠成功率曲线&lt;/b&gt;"><b>图2 距离精度与重叠成功率曲线</b></a></li>
                                                <li><a href="#115" data-title="&lt;b&gt;表2 不同算法中心位置误差结果对比&lt;/b&gt;"><b>表2 不同算法中心位置误差结果对比</b></a></li>
                                                <li><a href="#116" data-title="&lt;b&gt;表3 不同算法帧率结果对比&lt;/b&gt;"><b>表3 不同算法帧率结果对比</b></a></li>
                                                <li><a href="#120" data-title="&lt;b&gt;图3 4种算法跟踪效果&lt;/b&gt;"><b>图3 4种算法跟踪效果</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="142">


                                    <a id="bibliography_1" title=" 黄凯奇, 陈晓棠, 康运锋, 等.智能视频监控技术综述[J].计算机学报, 2015, 38 (6) :1093-1118." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJX201506001&amp;v=MjQwODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJxRkNqblViN05MejdCZHJHNEg5VE1xWTlGWllRS0RIODQ=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         黄凯奇, 陈晓棠, 康运锋, 等.智能视频监控技术综述[J].计算机学报, 2015, 38 (6) :1093-1118.
                                    </a>
                                </li>
                                <li id="144">


                                    <a id="bibliography_2" title=" 尹宏鹏, 陈波, 柴毅, 等.基于视觉的目标检测与跟踪综述[J].自动化学报, 2016, 42 (10) :1466-1489." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201610002&amp;v=MTk5NTR6cXFCdEdGckNVUkxPZVplUnFGQ2puVWI3TktDTGZZYkc0SDlmTnI0OUZab1FLREg4NHZSNFQ2ajU0TzM=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         尹宏鹏, 陈波, 柴毅, 等.基于视觉的目标检测与跟踪综述[J].自动化学报, 2016, 42 (10) :1466-1489.
                                    </a>
                                </li>
                                <li id="146">


                                    <a id="bibliography_3" title=" CHEN Zhe, HONG Zhibin, TAO Dacheng.An experimental survey on correlation filter-based tracking[EB/OL].[2018-04-20].https://arxiv.org/pdf/1509.05520.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=An experimental survey on correlation filter-based tracking">
                                        <b>[3]</b>
                                         CHEN Zhe, HONG Zhibin, TAO Dacheng.An experimental survey on correlation filter-based tracking[EB/OL].[2018-04-20].https://arxiv.org/pdf/1509.05520.pdf.
                                    </a>
                                </li>
                                <li id="148">


                                    <a id="bibliography_4" title=" BOLME D S, BEVERIDGE J R, DRAPER B A, et al.Visual object tracking using adaptive correlation filters[C]//Proceedings of Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2010:2544-2550." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Visual object tracking using adaptive correlation filters">
                                        <b>[4]</b>
                                         BOLME D S, BEVERIDGE J R, DRAPER B A, et al.Visual object tracking using adaptive correlation filters[C]//Proceedings of Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2010:2544-2550.
                                    </a>
                                </li>
                                <li id="150">


                                    <a id="bibliography_5" title=" HENRIQUES J F, CASEIRO R, MARTINS P, et al.Exploiting the circulant structure of tracking-by-detection with kernels[C]//Proceedings of European Conference on Computer Vision.Berlin, Germany:Springer, 2012:702-715." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Exploiting the circulant structure of trackingby-detection with kernels">
                                        <b>[5]</b>
                                         HENRIQUES J F, CASEIRO R, MARTINS P, et al.Exploiting the circulant structure of tracking-by-detection with kernels[C]//Proceedings of European Conference on Computer Vision.Berlin, Germany:Springer, 2012:702-715.
                                    </a>
                                </li>
                                <li id="152">


                                    <a id="bibliography_6" title=" HENRIQUES J F, CASEIRO R, MARTINS P, et al.High-speed tracking with kernelized correlation filters[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2015, 37 (3) :583-596." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=High-speed tracking with kernelized correlation filters">
                                        <b>[6]</b>
                                         HENRIQUES J F, CASEIRO R, MARTINS P, et al.High-speed tracking with kernelized correlation filters[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2015, 37 (3) :583-596.
                                    </a>
                                </li>
                                <li id="154">


                                    <a id="bibliography_7" title=" DANELLJAN M, KHAN F S, FELSBERG M, et al.Adaptive color attributes for real-time visual tracking[C]//Proceedings of Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2014:1090-1097." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Adaptive color attributes for real-time visual tracking">
                                        <b>[7]</b>
                                         DANELLJAN M, KHAN F S, FELSBERG M, et al.Adaptive color attributes for real-time visual tracking[C]//Proceedings of Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2014:1090-1097.
                                    </a>
                                </li>
                                <li id="156">


                                    <a id="bibliography_8" title=" DANELLJAN M, BHAT G, KHAN F S, et al.ECO:efficient convolution operators for tracking[C]//Proceedings of Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2017:6931-6939." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=ECO:Efficient Convolution Operators for Tracking">
                                        <b>[8]</b>
                                         DANELLJAN M, BHAT G, KHAN F S, et al.ECO:efficient convolution operators for tracking[C]//Proceedings of Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2017:6931-6939.
                                    </a>
                                </li>
                                <li id="158">


                                    <a id="bibliography_9" title=" 沈秋, 严小乐, 刘霖枫, 等.基于自适应特征选择的多尺度相关滤波跟踪[J].光学学报, 2017, 37 (5) :166-175." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201705021&amp;v=MTg3NTZxQnRHRnJDVVJMT2VaZVJxRkNqblViN05JalhUYkxHNEg5Yk1xbzlIWllRS0RIODR2UjRUNmo1NE8zenE=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                         沈秋, 严小乐, 刘霖枫, 等.基于自适应特征选择的多尺度相关滤波跟踪[J].光学学报, 2017, 37 (5) :166-175.
                                    </a>
                                </li>
                                <li id="160">


                                    <a id="bibliography_10" title=" SANTNER J, LEISTNER C, SAFFARI A, et al.PROST:parallel robust online simple tracking[C]//Proceedings of Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2010:723-730." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=PROST:Parallel robust online simple tracking">
                                        <b>[10]</b>
                                         SANTNER J, LEISTNER C, SAFFARI A, et al.PROST:parallel robust online simple tracking[C]//Proceedings of Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2010:723-730.
                                    </a>
                                </li>
                                <li id="162">


                                    <a id="bibliography_11" title=" KALAL Z, MIKOLAJCZYK K, MATAS J.Tracking-learning-detection[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2012, 34 (7) :1409-1422." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Tracking-Learning-Detection">
                                        <b>[11]</b>
                                         KALAL Z, MIKOLAJCZYK K, MATAS J.Tracking-learning-detection[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2012, 34 (7) :1409-1422.
                                    </a>
                                </li>
                                <li id="164">


                                    <a id="bibliography_12" title=" 张雷, 于凤芹.基于置信图特性的改进时空上下文目标跟踪[J].计算机工程, 2016, 42 (8) :277-281, 288." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201608050&amp;v=MTE4NDBESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVScUZDam5VYjdOTHo3QmJiRzRIOWZNcDQ5QVpJUUs=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[12]</b>
                                         张雷, 于凤芹.基于置信图特性的改进时空上下文目标跟踪[J].计算机工程, 2016, 42 (8) :277-281, 288.
                                    </a>
                                </li>
                                <li id="166">


                                    <a id="bibliography_13" title=" MA Chao, YANG Xiaokang, ZHANG Chongyang, et al.Long-term correlation tracking[C]//Proceedings of Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2015:5388-5396." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Long-term Correlation Tracking">
                                        <b>[13]</b>
                                         MA Chao, YANG Xiaokang, ZHANG Chongyang, et al.Long-term correlation tracking[C]//Proceedings of Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2015:5388-5396.
                                    </a>
                                </li>
                                <li id="168">


                                    <a id="bibliography_14" >
                                        <b>[14]</b>
                                     WANG Naiyan, SHI Jianping, YEUNG D Y, et al.Understanding and diagnosing visual tracking systems[C]//Proceedings of International Conference on Computer Vision.Washington D.C., USA:IEEE Press, 2015:3101-3109.</a>
                                </li>
                                <li id="170">


                                    <a id="bibliography_15" >
                                        <b>[15]</b>
                                     POSSEGGER H, MAUTHNER T, BISCHOF H.In defense of color-based model-free tracking[C]//Proceedings of Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2015:2113-2120.</a>
                                </li>
                                <li id="172">


                                    <a id="bibliography_16" title=" BERTINETTO L, VALMADRE J, GOLODETZ S, et al.Staple:complementary learners for real-time tracking[C]//Proceedings of Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2016:1401-1409." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Staple:Complementary learners for real-time tracking">
                                        <b>[16]</b>
                                         BERTINETTO L, VALMADRE J, GOLODETZ S, et al.Staple:complementary learners for real-time tracking[C]//Proceedings of Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2016:1401-1409.
                                    </a>
                                </li>
                                <li id="174">


                                    <a id="bibliography_17" title=" WANG Mengmeng, LIU Yong, HUANG Zeyi.Large margin object tracking with circulant feature maps[C]//Proceedings of Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2017:4800-4808." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Large Margin Object Tracking with Circulant Feature Maps">
                                        <b>[17]</b>
                                         WANG Mengmeng, LIU Yong, HUANG Zeyi.Large margin object tracking with circulant feature maps[C]//Proceedings of Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2017:4800-4808.
                                    </a>
                                </li>
                                <li id="176">


                                    <a id="bibliography_18" title=" WU Yi, LIM J, YANG M H.Online object tracking:a benchmark[C]//Proceedings of Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2013:2411-2418." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Online object tracking:a benchmark">
                                        <b>[18]</b>
                                         WU Yi, LIM J, YANG M H.Online object tracking:a benchmark[C]//Proceedings of Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2013:2411-2418.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJC" target="_blank">计算机工程</a>
                2019,45(08),241-247 DOI:10.19678/j.issn.1000-3428.0051612            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于特征融合的改进LCT跟踪算法</b></span>
 <span class="shoufa"></span>                                     </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%AE%98%E6%B4%AA%E8%BF%90&amp;code=06598948&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">官洪运</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%AC%A7%E9%98%B3%E6%B1%9F%E5%9D%A4&amp;code=37807292&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">欧阳江坤</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%A8%E7%9B%8A%E4%BC%9F&amp;code=37807291&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">杨益伟</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%90%B4%E7%82%9C&amp;code=37807293&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">吴炜</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%9C%E5%8D%8E%E5%A4%A7%E5%AD%A6%E4%BF%A1%E6%81%AF%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF%E5%AD%A6%E9%99%A2&amp;code=0265717&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">东华大学信息科学与技术学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%9C%E5%8D%8E%E5%A4%A7%E5%AD%A6%E6%95%B0%E5%AD%97%E5%8C%96%E7%BA%BA%E7%BB%87%E6%9C%8D%E8%A3%85%E6%8A%80%E6%9C%AF%E6%95%99%E8%82%B2%E9%83%A8%E5%B7%A5%E7%A8%8B%E7%A0%94%E7%A9%B6%E4%B8%AD%E5%BF%83&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">东华大学数字化纺织服装技术教育部工程研究中心</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>为解决LCT算法在目标形变与快速移动情况下跟踪效果差的问题, 提出一种基于特征融合的跟踪算法。在梯度方向直方图特征相关滤波的基础上, 提取目标与背景颜色直方图特征, 得到颜色特征的目标预测位置。在此基础上, 根据跟踪置信度确定特征融合权重, 综合考虑梯度特征与颜色特征得到跟踪结果。实验结果证明, 与LCT算法相比, 该算法的距离精度和重叠精度分别提高了11.5%和21.2%, 平均中心位置误差减少了15.3像素。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">机器视觉;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%95%BF%E6%9C%9F%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">长期目标跟踪;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%9B%B8%E5%85%B3%E6%BB%A4%E6%B3%A2&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">相关滤波;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%A2%9C%E8%89%B2%E7%9B%B4%E6%96%B9%E5%9B%BE&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">颜色直方图;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%9B%AE%E6%A0%87%E5%BD%A2%E5%8F%98&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">目标形变;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%89%B9%E5%BE%81%E8%9E%8D%E5%90%88&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">特征融合;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    官洪运 (1960—) , 男, 副教授, 主研方向为视觉目标跟踪、图像通信;;
                                </span>
                                <span>
                                    *欧阳江坤 (通信作者) , 硕士研究生。E-mail: oyjkun@ 163. com;
                                </span>
                                <span>
                                    杨益伟, 硕士研究生。;
                                </span>
                                <span>
                                    吴炜, 硕士研究生。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-05-21</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金 (71171045);</span>
                                <span>上海市教科委创新项目 (14YZ130);</span>
                    </p>
            </div>
                    <h1><b>Improved LCT Tracking Algorithm Based on Feature Fusion</b></h1>
                    <h2>
                    <span>GUAN Hongyun</span>
                    <span>OUYANG Jiangkun</span>
                    <span>YANG Yiwei</span>
                    <span>WU Wei</span>
            </h2>
                    <h2>
                    <span>College of Information Science and Technology, Donghua University</span>
                    <span>Engineering Research Center of Digitalized Textile and Fashion Technology, Ministry of Education, Donghua University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>To address the poor tracking performance of the LCT algorithm when the target is in deformation or fast moving, a tracking algorithm based on feature fusion is proposed.Based on the Histogram of Oriented Gradient (HOG) feature correlation filtering, the target and background color histogram features are extracted to obtain the target prediction position of the color feature.On the basis, the feature fusion weight is determined according to the tracking confidence.Tracking results are obtained by considering gradient features and color features.Experimental results show that compared with the LCT algorithm, the distance accuracy and overlap accuracy of the algorithm are increased by 11.5% and 21.2%, respectively, and the average center position error is reduced by 15.3 pixels.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=machine%20vision&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">machine vision;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=long-term%20object%20tracking&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">long-term object tracking;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=correlation%20filtering&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">correlation filtering;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=color%20histograms&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">color histograms;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=target%20deformation&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">target deformation;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=feature%20fusion&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">feature fusion;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-05-21</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="39" name="39" class="anchor-tag">0 概述</h3>
                <div class="p1">
                    <p id="40">目标跟踪是计算机视觉领域中热门的研究课题之一, 被广泛应用于视频监控、车辆导航和人机交互等方面。在目标变化、快速移动或遮挡等复杂情况下, 如何保证目标跟踪算法的鲁棒性及运行速度是一个极具挑战的问题<citation id="178" type="reference"><link href="142" rel="bibliography" /><link href="144" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="41">近年来, 针对目标跟踪, 国内外学者研究了相关滤波方法<citation id="179" type="reference"><link href="146" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>。相关滤波是一种信号处理方法, 通过输入信号经过滤波器后的输出响应, 判断输入信号的相关性。在目标跟踪上, 寻找最大响应来确定跟踪目标的中心位置。文献<citation id="180" type="reference">[<a class="sup">4</a>]</citation>提出最小输出平方误差和 (Minimum Output Sum of Squared Error, MOSSE) 滤波器, 利用输出误差平方和损失函数构建滤波器模板, 从而实现目标跟踪。文献<citation id="181" type="reference">[<a class="sup">5</a>]</citation>提出CSK算法, 将相关滤波引入核空间并利用循环移位进行密集采样。文献<citation id="182" type="reference">[<a class="sup">6</a>]</citation>提出KCF (Kernelized Correlation Filter) 算法并结合梯度方向直方图 (Histogram of Oriented Gradient, HOG) 特征。文献<citation id="183" type="reference">[<a class="sup">7</a>]</citation>利用颜色特征设计CN算法。文献<citation id="184" type="reference">[<a class="sup">8</a>]</citation>提出ECO (Efﬁcient Convolution Operators) 算法, 将深度特征融入到相关滤波中, 取得了较好的效果。文献<citation id="185" type="reference">[<a class="sup">9</a>]</citation>提出一种自适应特征选择的相关滤波跟踪方法, 研究表明, 特征选择对跟踪结果影响较大。</p>
                </div>
                <div class="p1">
                    <p id="42">对于无人机追踪或视频监控类的应用场景, 需对目标进行长期跟踪。跟踪模型在线更新能增强算法可塑性, 适应目标新的外观, 但同时也会导致背景信息的不断累积, 影响跟踪准确性<citation id="186" type="reference"><link href="160" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>。针对此问题, 文献<citation id="187" type="reference">[<a class="sup">11</a>]</citation>提出TLD (Tracking Learning Detection) 算法, 将跟踪与检测结合, 利用重检测器防止跟踪失败, 同时用检测器的结果训练跟踪器, 此方法适合长期跟踪, 但缺点是跟踪速度较慢, 且目标变化较大时效果不佳。文献<citation id="188" type="reference">[<a class="sup">12</a>]</citation>根据置信图特性来解决遮挡后易发生漂移的问题。文献<citation id="189" type="reference">[<a class="sup">13</a>]</citation>提出LCT算法, 综合考虑了目标的上下文关系及尺度变换, 在相关滤波基础上结合基于随机蕨 (Random Ferns, RF) 的重检测器, 使其在长期跟踪上的效果得到了进一步提高。</p>
                </div>
                <div class="p1">
                    <p id="43">虽然LCT算法在长期跟踪方面取得了较好的效果, 但只使用了HOG特征, 且对目标形变及快速运动较为敏感。本文针对LCT算法的不足, 引入颜色直方图统计方法, 根据跟踪置信度确定自适应融合权值, 将2种方法相结合得到最终结果。</p>
                </div>
                <h3 id="44" name="44" class="anchor-tag">1 LCT跟踪算法</h3>
                <div class="p1">
                    <p id="45">针对长期目标跟踪, LCT跟踪算法在HOG特征的核相关滤波基础上添加了尺度估计, 并用随机蕨算法进行重检测, 确保发生遮挡后仍然能准确跟踪目标, 使算法能够长时间维持跟踪准确性即不发生跟踪漂移。</p>
                </div>
                <h4 class="anchor-tag" id="46" name="46">1.1 核相关滤波</h4>
                <div class="p1">
                    <p id="47">LCT算法的跟踪器分为3个部分:位置滤波器<i>R</i><sub>c</sub>, 置信度滤波器<i>R</i><sub>t</sub>以及尺度滤波器<i>R</i><sub>s</sub>, 分别用于跟踪目标定位、置信度确认以及目标尺度估计, 均为相关滤波类方法。</p>
                </div>
                <div class="p1">
                    <p id="48">选择包含目标与部分背景区域的图像块<i>x</i>, 大小为<i>M</i>×<i>N</i>, 在范围内通过循环移位创造样本<i>x</i><sub><i>m</i>, <i>n</i></sub> (<i>m</i>, <i>n</i>) ∈{0, 1, …, <i>M</i>-1}×{0, 1, …, <i>N</i>-1}, 建立目标函数岭回归模型, 有:</p>
                </div>
                <div class="p1">
                    <p id="49" class="code-formula">
                        <mathml id="49"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>min</mi></mrow><mspace width="0.25em" /><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>m</mi><mo>, </mo><mi>n</mi></mrow></munder><mrow><mo stretchy="false">∥</mo><mi>φ</mi><mrow><mo> (</mo><mrow><mi>x</mi><msub><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mi>n</mi></mrow></msub></mrow><mo>) </mo></mrow><mo>⋅</mo><mi>w</mi><mo>-</mo><mi>y</mi><mo stretchy="false"> (</mo><mi>m</mi><mo>, </mo><mi>n</mi><mo stretchy="false">) </mo><mo stretchy="false">∥</mo></mrow></mstyle><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mi>λ</mi><mrow><mo stretchy="false">∥</mo><mi>w</mi><mo stretchy="false">∥</mo></mrow><msup><mrow></mrow><mn>2</mn></msup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="50">其中, <i>λ</i>为正则化项参数, <i>y</i> (<i>m</i>, <i>n</i>) 为通过高斯函数设置的标签, 且目标中心点处值最大, <i>φ</i> (<i>x</i><sub><i>m</i>, <i>n</i></sub>) 是核空间下的映射, 核函数选择高斯核。</p>
                </div>
                <div class="p1">
                    <p id="51">利用快速傅里叶变换计算相关性, 可将目标解表示为<mathml id="52"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>w</mi><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>m</mi><mo>, </mo><mi>n</mi></mrow></munder><mi>a</mi></mstyle><mo stretchy="false"> (</mo><mi>m</mi><mo>, </mo><mi>n</mi><mo stretchy="false">) </mo><mi>φ</mi><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mi>n</mi></mrow></msub><mo stretchy="false">) </mo></mrow></math></mathml>, 相关系数<i>a</i>定义如下:</p>
                </div>
                <div class="p1">
                    <p id="53" class="code-formula">
                        <mathml id="53"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>A</mi><mo>=</mo><mi>F</mi><mo stretchy="false"> (</mo><mi>a</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mrow><mi>F</mi><mo stretchy="false"> (</mo><mi>y</mi><mo stretchy="false">) </mo></mrow><mrow><mi>F</mi><mo stretchy="false"> (</mo><mi>φ</mi><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mo>⋅</mo><mi>φ</mi><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="54">其中, <i>F</i>表示离散傅里叶变换。</p>
                </div>
                <div class="p1">
                    <p id="55">在新的一帧下, 搜索窗口<i>z</i>内响应输出为:</p>
                </div>
                <div class="p1">
                    <p id="56" class="code-formula">
                        <mathml id="56"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi>y</mi></mstyle><mrow><mspace width="0.25em" /><mo>^</mo></mrow></mover><mo>=</mo><mi>F</mi><msup><mrow></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mrow><mo> (</mo><mrow><mi>A</mi><mo>⊙</mo><mi>F</mi><mrow><mo> (</mo><mrow><mi>φ</mi><mrow><mo> (</mo><mi>z</mi><mo>) </mo></mrow><mo>⋅</mo><mi>φ</mi><mrow><mo> (</mo><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi>x</mi></mstyle><mrow><mspace width="0.25em" /><mo>^</mo></mrow></mover></mrow><mo>) </mo></mrow></mrow><mo>) </mo></mrow></mrow><mo>) </mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="57">其中, <mathml id="58"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi>x</mi></mstyle><mrow><mspace width="0.25em" /><mo>^</mo></mrow></mover></mrow></math></mathml>表示学习到的目标外观模型, ⊙表示元素点积, <mathml id="59"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi>y</mi></mstyle><mrow><mspace width="0.25em" /><mo>^</mo></mrow></mover></mrow></math></mathml>响应的最大值即为目标的新位置。</p>
                </div>
                <h4 class="anchor-tag" id="60" name="60">1.2 模型更新与重检测</h4>
                <div class="p1">
                    <p id="61">为保证模型对目标新外观的适应性, 需要对模型进行更新, 其计算公式为:</p>
                </div>
                <div class="p1">
                    <p id="62" class="code-formula">
                        <mathml id="62"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi>x</mi></mstyle><mrow><mspace width="0.25em" /><mo>^</mo></mrow></mover></mrow><msup><mrow></mrow><mi>t</mi></msup><mo>=</mo><mrow><mo> (</mo><mrow><mn>1</mn><mo>-</mo><mi>α</mi></mrow><mo>) </mo></mrow><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi>x</mi></mstyle><mrow><mspace width="0.25em" /><mo>^</mo></mrow></mover></mrow><msup><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msup><mo>+</mo><mi>α</mi><mi>x</mi><msup><mrow></mrow><mi>t</mi></msup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi>A</mi></mstyle><mrow><mspace width="0.25em" /><mo>^</mo></mrow></mover></mrow><msup><mrow></mrow><mi>t</mi></msup><mo>=</mo><mrow><mo> (</mo><mrow><mn>1</mn><mo>-</mo><mi>α</mi></mrow><mo>) </mo></mrow><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi>A</mi></mstyle><mrow><mspace width="0.25em" /><mo>^</mo></mrow></mover></mrow><msup><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msup><mo>+</mo><mi>α</mi><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>5</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="63">其中, <i>t</i>代表当前帧, <i>α</i>是学习速率。</p>
                </div>
                <div class="p1">
                    <p id="64">目标在发生遮挡或离开视野后, 容易造成跟踪失败, 并且过多的背景信息引入会导致模型准确性降低。为降低该情况产生的不利影响, LCT算法对跟踪器中采取了置信度分析与重检测2种控制方法。</p>
                </div>
                <div class="p1">
                    <p id="65">LCT算法在获得目标位置后, 通过置信度滤波器<i>R</i><sub>t</sub>在所得目标位置周围计算最大响应值, 作为跟踪置信度, 如果置信度低于规定阈值, 则停止更新模型, 并启动重检测器。重检测器采用随机蕨分类器<citation id="190" type="reference"><link href="162" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>, 利用每一帧的跟踪结果对蕨进行训练, 所有蕨的后验概率均值即为得到的目标响应。同时, 用重检测器获得的跟踪结果代替置信度不高的结果。</p>
                </div>
                <h3 id="66" name="66" class="anchor-tag">2 改进的LCT跟踪算法</h3>
                <div class="p1">
                    <p id="67">目标特征的选择对跟踪结果十分重要, 选择恰当特征能够有效表达目标, 在一定程度上改善跟踪性能<citation id="191" type="reference"><link href="168" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>。基于HOG特征的相关滤波器, 依赖跟踪目标的空间布局, 对颜色变化不敏感, 但易受到形变影响。颜色直方图不考虑像素位置, 使其对目标形变表现出较高的鲁棒性, 也不会受到快速运动时的边界影响。对于此问题, 本文算法在LCT算法的基础上对融合颜色特征与权值自适应调整2个方面进行改进。</p>
                </div>
                <h4 class="anchor-tag" id="68" name="68">2.1 综合颜色直方图</h4>
                <div class="p1">
                    <p id="69">综合颜色直方图方法通过目标与周围背景颜色信息定位目标位置<citation id="192" type="reference"><link href="170" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>。文献<citation id="193" type="reference">[<a class="sup">16</a>]</citation>将多尺度相关滤波方法DSST与综合颜色特征方法DAT (Distractor Aware Tracker) 结合, 证明了对目标边界不敏感的颜色特征可以对HOG特征方法进行补充。本文将相关滤波方法LCT与综合颜色直方图方法结合, 使其在保持LCT算法优点的同时, 消除对形变与快速移动敏感的劣势。</p>
                </div>
                <div class="p1">
                    <p id="70">颜色直方图方法通过得分矩阵的方式从搜索区域中确定前景目标位置, 得分矩阵公式如下:</p>
                </div>
                <div class="p1">
                    <p id="71" class="code-formula">
                        <mathml id="71"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>R</mi><msub><mrow></mrow><mrow><mtext>h</mtext><mtext>i</mtext><mtext>s</mtext><mtext>t</mtext></mrow></msub><mrow><mo> (</mo><mrow><mi>x</mi><mo>;</mo><mi mathvariant="bold-italic">β</mi></mrow><mo>) </mo></mrow><mo>=</mo><mi>g</mi><mrow><mo> (</mo><mrow><mi>ψ</mi><msub><mrow></mrow><mi>x</mi></msub><mo>;</mo><mi mathvariant="bold-italic">β</mi></mrow><mo>) </mo></mrow><mo>=</mo><mi mathvariant="bold-italic">β</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mrow><mo> (</mo><mrow><mfrac><mn>1</mn><mrow><mrow><mo>|</mo><mi>Η</mi><mo>|</mo></mrow></mrow></mfrac><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>u</mi><mo>∈</mo><mi>Η</mi></mrow></munder><mi>ψ</mi></mstyle><mrow><mo>[</mo><mi>u</mi><mo>]</mo></mrow></mrow><mo>) </mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>6</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="72">其中, <i>ψ</i>[<i>u</i>]表示像素<i>u</i>的特征值, <i>β</i>表示直方图权重向量。</p>
                </div>
                <div class="p1">
                    <p id="73">通过岭回归可获取最小偏差的<i>β</i>值, 为计算简便, 将前景区域与背景区域的情况分开, 最终得到的模型解如下:</p>
                </div>
                <div class="p1">
                    <p id="74" class="code-formula">
                        <mathml id="74"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>β</mi><msubsup><mrow></mrow><mi>t</mi><mi>j</mi></msubsup><mo>=</mo><mfrac><mrow><mi>ρ</mi><msup><mrow></mrow><mi>j</mi></msup><mo stretchy="false"> (</mo><mi>Ο</mi><mo stretchy="false">) </mo></mrow><mrow><mi>ρ</mi><msup><mrow></mrow><mi>j</mi></msup><mo stretchy="false"> (</mo><mi>Ο</mi><mo stretchy="false">) </mo><mo>+</mo><mi>ρ</mi><msup><mrow></mrow><mi>j</mi></msup><mo stretchy="false"> (</mo><mi>B</mi><mo stretchy="false">) </mo><mo>+</mo><mi>λ</mi></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>7</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="75">其中, <i>j</i>表示特征矩阵的第<i>j</i>维, <i>O</i>与<i>B</i>分别表示前景区域与背景区域, <i>ρ</i><sup><i>j</i></sup> (<i>A</i>) =<i>N</i><sup><i>j</i></sup> (<i>A</i>) /|<i>A</i>|表示<i>j</i>通道下区域A内各像素对应直方图得分<mathml id="76"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ν</mi><msup><mrow></mrow><mi>j</mi></msup><mrow><mo> (</mo><mi>A</mi><mo>) </mo></mrow></mrow></math></mathml>所占比例, 可理解为像素点属于前景或背景的概率。</p>
                </div>
                <div class="p1">
                    <p id="77">图1为颜色直方图方法中的关键部分。其中, 原图像中的虚线框是前景和背景区域, 实线框是目标位置。可以看出, 根据前景直方图与背景直方图求得区域概率图, 分别在前景区域与背景区域有较大值, 而综合结果目标与环境边界明显, 通过区分前景与背景可确定目标位置。为增加颜色方法的适应性, ρ (O) 与ρ (B) 在新一帧时的更新策略与相关滤波器采用线性插值法, 且只在响应达到设定阈值时进行更新, 其计算公式为:</p>
                </div>
                <div class="p1">
                    <p id="78" class="code-formula">
                        <mathml id="78"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>ρ</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false"> (</mo><mi>A</mi><mo stretchy="false">) </mo><mo>=</mo><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>η</mi><mo stretchy="false"> (</mo><mi>A</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mi>ρ</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo stretchy="false"> (</mo><mi>A</mi><mo stretchy="false">) </mo><mo>+</mo><mi>η</mi><msup><mi>ρ</mi><mo>′</mo></msup><msub><mrow></mrow><mi>t</mi></msub><mo>, </mo><mi>r</mi><msub><mrow></mrow><mrow><mtext>a</mtext><mtext>l</mtext><mtext>l</mtext></mrow></msub><mo>&gt;</mo><mi>Τ</mi></mtd></mtr><mtr><mtd><mi>ρ</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo stretchy="false"> (</mo><mi>A</mi><mo stretchy="false">) </mo><mo>, </mo><mi>r</mi><msub><mrow></mrow><mrow><mtext>a</mtext><mtext>l</mtext><mtext>l</mtext></mrow></msub><mo>≤</mo><mi>Τ</mi></mtd></mtr></mtable></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>8</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="79">其中, T为响应阈值。</p>
                </div>
                <div class="area_img" id="80">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201908040_080.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 综合颜色直方图效果" src="Detail/GetImg?filename=images/JSJC201908040_080.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图1 综合颜色直方图效果</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201908040_080.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="81" name="81">2.2 特征算法自适应融合</h4>
                <div class="p1">
                    <p id="82">在目标跟踪过程中, 相关滤波方法与综合颜色直方图方法同时执行, 分别获得响应矩阵<b><i>R</i></b><sub>cf</sub>与<b><i>R</i></b><sub>hist</sub>, 通过加权融合方式得到输出响应矩阵, 可表示为:</p>
                </div>
                <div class="p1">
                    <p id="83"><b><i>R</i></b><sub>all</sub>= (1-<i>α</i>) ·<b><i>R</i></b><sub>cf</sub>+<i>α</i>·<b><i>R</i></b><sub>hist</sub>      (9) </p>
                </div>
                <div class="p1">
                    <p id="84">其中, <mathml id="85"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>α</mi><mo>∈</mo><mrow><mo>[</mo><mrow><mn>0</mn><mo>, </mo><mn>1</mn></mrow><mo>]</mo></mrow></mrow></math></mathml>是权值因子, <b><i>R</i></b><sub>all</sub>中最大值所在位置选定为新一帧的目标位置, 有:</p>
                </div>
                <div class="p1">
                    <p id="86" class="code-formula">
                        <mathml id="86"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>p</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mrow><mi>arg</mi></mrow><mrow><mi>max</mi></mrow><mrow><mo> (</mo><mrow><mi mathvariant="bold-italic">R</mi><msub><mrow></mrow><mrow><mtext>a</mtext><mtext>l</mtext><mtext>l</mtext></mrow></msub><mrow><mo> (</mo><mrow><mi>p</mi><msub><mrow></mrow><mi>t</mi></msub></mrow><mo>) </mo></mrow></mrow><mo>) </mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>0</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="87">现有特征融合方法一般使用固定权值, 但存在如下不足:在跟踪期间如果某个特征信息变化, 其权值也会相应调整, 否则会影响跟踪效果, 甚至导致跟踪失败。</p>
                </div>
                <div class="p1">
                    <p id="88">为能根据特征的重要性改变权值, 本文算法在算法融合时对权值因子<i>α</i>进行自适应调整, 根据当前帧下2种特征所得结果跟踪置信度, 调整下一帧<i>α</i>的大小, 在某种特征占主导地位时增大其权值。LCT算法中根据响应矩阵最大响应值的大小判断跟踪效果的好坏, 可以在一定程度上维持模型的稳定性, 但仅依靠响应峰值无法准确判断目标大范围遮挡的情况, 进而导致跟踪模型污染。为保证长期跟踪时模型的可靠性, 本文利用平均峰值相关能量 (Average Peak-to Correlation Energy, APCE) <citation id="194" type="reference"><link href="174" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>来反映跟踪的置信度, 表示为:</p>
                </div>
                <div class="p1">
                    <p id="89" class="code-formula">
                        <mathml id="89"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>A</mi><msub><mrow></mrow><mrow><mtext>A</mtext><mtext>Ρ</mtext><mtext>C</mtext><mtext>E</mtext></mrow></msub><mo>=</mo><mfrac><mrow><mrow><mrow><mo>|</mo><mrow><mi>R</mi><msub><mrow></mrow><mrow><mi>max</mi></mrow></msub><mo>-</mo><mi>R</mi><msub><mrow></mrow><mrow><mi>min</mi></mrow></msub></mrow><mo>|</mo></mrow></mrow><msup><mrow></mrow><mn>2</mn></msup></mrow><mrow><mi>m</mi><mi>e</mi><mi>a</mi><mi>n</mi><mrow><mo> (</mo><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>w</mi><mo>, </mo><mi>h</mi></mrow></munder><mrow><mrow><mo> (</mo><mrow><mi>R</mi><msub><mrow></mrow><mrow><mi>w</mi><mo>, </mo><mi>h</mi></mrow></msub><mo>-</mo><mi>R</mi><msub><mrow></mrow><mrow><mi>min</mi></mrow></msub></mrow><mo>) </mo></mrow></mrow></mstyle><msup><mrow></mrow><mn>2</mn></msup></mrow><mo>) </mo></mrow></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="90">其中, <i>R</i><sub>max</sub>与<i>R</i><sub>min</sub>分别表示响应图中响应值的最大值与最小值, APCE值反映了响应图的波动程度和检测到的目标置信度。在响应图峰值尖锐噪声较少时, APCE值将会变大, 表明属于目标的可信度较高。而在目标被遮挡或者丢失时, APCE值将会显著降低。</p>
                </div>
                <div class="p1">
                    <p id="91">本文将APCE值引入模型的更新策略, 同时求取算法融合时的权值因子。在获得2种算法的响应后, 由式 (11) 求得对应的置信度值<i>APCE</i><sub>cf</sub>、<i>APCE</i><sub>hist</sub>, 再计算出下一帧的权值因子, 即:</p>
                </div>
                <div class="p1">
                    <p id="92" class="code-formula">
                        <mathml id="92"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>α</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mfrac><mrow><mi>A</mi><mi>Ρ</mi><mi>C</mi><mi>E</mi><msub><mrow></mrow><mrow><mtext>h</mtext><mtext>i</mtext><mtext>s</mtext><mtext>t</mtext></mrow></msub></mrow><mrow><mi>A</mi><mi>Ρ</mi><mi>C</mi><mi>E</mi><msub><mrow></mrow><mrow><mtext>h</mtext><mtext>i</mtext><mtext>s</mtext><mtext>t</mtext></mrow></msub><mo>+</mo><mi>A</mi><mi>Ρ</mi><mi>C</mi><mi>E</mi><msub><mrow></mrow><mrow><mtext>c</mtext><mtext>f</mtext></mrow></msub></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="93">由式 (12) 可知:当<i>APCE</i><sub>cf</sub>&gt;<i>APCE</i><sub>hist</sub>时, 相关滤波置信度较高, 减小<i>α</i>值;当<i>APCE</i><sub>cf</sub>&lt;<i>APCE</i><sub>hist</sub>时, 颜色直方图算法置信度较高, 增大<i>α</i>值。</p>
                </div>
                <div class="p1">
                    <p id="94">颜色直方图方法根据目标区域整体进行搜索, 其目标范围响应较高, 权值过大会造成检索到的最大响应位置偏移, 降低算法的准确性。另外, 当目标发生遮挡时, 2种方法的置信度值均会显著下降, 此时判断权值没有意义。考虑上述原因, 程序中对<i>α</i>设置上限, 只在一定范围内调整权值因子, 经多组实验测试, 设定上限值为0.7。</p>
                </div>
                <h4 class="anchor-tag" id="95" name="95">2.3 改进算法流程</h4>
                <div class="p1">
                    <p id="96">在LCT算法的基础上, 本文构造的改进LCT跟踪算法具体步骤描述如下:</p>
                </div>
                <div class="p1">
                    <p id="97"><b>输入</b> 首帧图像, 目标初始化位置<i>pos</i><sub>1</sub>与尺度<i>t</i><sub>1</sub></p>
                </div>
                <div class="p1">
                    <p id="98"><b>输出</b> 整个序列每帧内目标框的中心点位置<i>pos</i><sub><i>t</i></sub>与尺度<i>s</i><sub><i>t</i></sub></p>
                </div>
                <div class="p1">
                    <p id="99"><b>步骤1</b> 根据<i>pos</i><sub>1</sub>与<i>t</i><sub>1</sub>确定目标区域 (前景区域) 与搜索区域 (背景区域) 。</p>
                </div>
                <div class="p1">
                    <p id="100"><b>步骤2</b> 提取搜索区域HOG特征, 训练位置滤波器<i>R</i><sub><i>c</i></sub>与尺度滤波器模型, 提取前景与背景区域颜色直方图特征确定颜色模型。</p>
                </div>
                <div class="p1">
                    <p id="101"><b>步骤3</b> 在第<i>t</i>帧, 根据第<i>t</i>-1帧确定的目标位置<i>pos</i><sub><i>t</i>-1</sub>确定搜索区域, 提取HOG特征与颜色直方图特征, 模型获取相关滤波得分矩阵与颜色得分矩阵, 根据式 (9) 进行融合得到综合响应, 找到最大响应点所在位置作为中心点位置<i>pos</i><sub><i>t</i></sub>。</p>
                </div>
                <div class="p1">
                    <p id="102"><b>步骤4</b> 计算最佳尺度<i>s</i><sub><i>t</i></sub>, 并通过置信度滤波器<i>R</i><sub>t</sub>计算置信度APCE值, 在<i>A</i><sub>APCE</sub>&gt;<i>T</i><sub><i>τ</i></sub>时, 执行步骤6;否则启动重检测模块。</p>
                </div>
                <div class="p1">
                    <p id="103"><b>步骤5</b> 随机蕨检测器搜索目标位置, 如果获得新位置<i>pos</i>′的置信度高于上一步所得值, 则<i>pos</i><sub><i>t</i></sub>=<i>pos</i>′。</p>
                </div>
                <div class="p1">
                    <p id="104"><b>步骤6</b> 如果<i>A</i><sub>APCE</sub>&gt;<i>T</i><sub><i>α</i></sub>, 则根据新位置<i>pos</i><sub><i>t</i></sub>与式 (4) 、式 (5) 、式 (8) 更新模型。</p>
                </div>
                <div class="p1">
                    <p id="105"><b>步骤7</b> 如果序列结束, 则停止算法;否则重复执行步骤3～步骤6。</p>
                </div>
                <h3 id="106" name="106" class="anchor-tag">3 实验结果与分析</h3>
                <div class="p1">
                    <p id="107">本文实验使用Matlab R2016b平台, 在Intel Core i7-7700HQ CPU、主频2.80 GHz、内存8 GB的计算机上调试运行, 评测方式选择一遍成功率 (One-Pass Evaluation, OPE) , 即第一帧给定目标位置开始跟踪, 跟踪失败后不会重新初始化。跟踪性能通过距离精度 (Distance Precision, DP) 、重叠成功率 (Overlap Success Rate, OSR) 、中心位置误差 (Center Location Error, CLE) 以及帧率 (Frames Per Second, FPS) 4种指标来评判<citation id="195" type="reference"><link href="176" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>。</p>
                </div>
                <h4 class="anchor-tag" id="108" name="108">3.1 跟踪性能指标对比</h4>
                <div class="p1">
                    <p id="109">为验证算法有效性, 在OTB<citation id="196" type="reference"><link href="176" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>提供的数据集中选出存在目标遮挡、形变、快速移动、长期跟踪等情况的视频序列6组 (Board、Jogging1、Gym、Lemming、Girl2、Tiger2) 进行测试, 评测时与KCF、CN以及LCT这3种算法进行对比, 并对测试结果进行分析。</p>
                </div>
                <div class="p1">
                    <p id="110">在视频序列中, Broad、Gym、Tiger2这3个序列存在快速移动或形变, Jogging1、Lemming、Girl2这 3个序列中存在完全遮挡。表1给出了4种算法特征描述。</p>
                </div>
                <div class="area_img" id="111">
                    <p class="img_tit"><b>表1 4种算法特征描述</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="111" border="1"><tr><td>算法</td><td>特征</td><td>尺度变换</td><td>重检测模块</td></tr><tr><td>KCF算法</td><td>HOG</td><td>无</td><td>无</td></tr><tr><td><br />CN算法</td><td>CN</td><td>无</td><td>无</td></tr><tr><td><br />LCT算法</td><td>HOG</td><td>有</td><td>有</td></tr><tr><td><br />本文算法</td><td>HOG、DAT</td><td>有</td><td>有</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="112">图2为6组序列的DP与OSR曲线, 图像纵轴为百分比, 数字分别表示阈值为20像素与阈值0.5时DP指标数值与OSR指标数值。从图2可以看出, 本文算法DP与OSR指标较好, 均达到最优或者次优。在Girl2序列中, 本文算法效果与LCT算法相比, DP与OSR上升了11.5%和21.2%, 相比HOG特征方法KCF上升了46.8%和48.2%, 相比颜色特征方法CN上升了34.7%和39.4%。在存在完全遮挡的3个序列中, 无重检测模块的KCF算法与CN算法效果较差, 本文算法与LCT算法相比提升了1.2%和21.5%。HOG特征与颜色特征在不同序列中具有不同的准确度, 对于存在持续形变的Gym与频繁遮挡的Tiger2序列, KCF算法与LCT算法受异常扰动影响较大, CN算法相对稳定。相比CN算法, 本文算法在Gym序列中的DP和OSR分别提高了16.9%和25.7%, 在序列Tiger2中提高了17.5%和19.8%。</p>
                </div>
                <div class="p1">
                    <p id="113">表2给出了中心位置误差对比结果。可以看出, 本文算法在6个序列中的平均CLE值为15.3像素, 相比LCT算法, 减少了15.3像素。表3给出不同算法的FPS对比结果。可以看出, 只包含一个相关滤波器的KCF以及CN算法的处理速度高达214.00和77.10。LCT算法添加了置信度、尺度估计和重检测, 其计算复杂度较高, 但帧率只有11.7。本文算法在其基础上添加了综合颜色直方图算法, 但由于搜索区域参考的是较小的背景区域, 减少了运算量, 因此平均帧率相对LCT算法反而增加了4.4, 达到了更快的速度。</p>
                </div>
                <div class="area_img" id="114">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201908040_114.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 距离精度与重叠成功率曲线" src="Detail/GetImg?filename=images/JSJC201908040_114.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图2 距离精度与重叠成功率曲线</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201908040_114.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="115">
                    <p class="img_tit"><b>表2 不同算法中心位置误差结果对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="115" border="1"><tr><td><br />算法</td><td>Board</td><td>Jogging1</td><td>Gym</td><td>Lemming</td><td>Tiger2</td><td>Girl2</td><td>平均值</td></tr><tr><td><br />KCF算法</td><td>35.40</td><td>88.30</td><td>16.30</td><td>77.90</td><td>47.40</td><td>265.00</td><td>88.40</td></tr><tr><td><br />CN算法</td><td>25.70</td><td>101.00</td><td>19.30</td><td>90.70</td><td>18.30</td><td>42.40</td><td>49.60</td></tr><tr><td><br />LCT算法</td><td>33.30</td><td>5.82</td><td>9.45</td><td>15.60</td><td>17.40</td><td>102.00</td><td>30.60</td></tr><tr><td><br />本文算法</td><td>15.90</td><td>14.70</td><td>9.33</td><td>7.64</td><td>11.20</td><td>32.80</td><td>15.30</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="116">
                    <p class="img_tit"><b>表3 不同算法帧率结果对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note">frame·s<sup>-1</sup></p>
                    <table id="116" border="1"><tr><td>算法</td><td>Board</td><td>Jogging1</td><td>Gym</td><td>Lemming</td><td>Tiger2</td><td>Girl2</td><td>平均值</td></tr><tr><td><br />KCF算法</td><td>86.90</td><td>329.80</td><td>270.00</td><td>215.00</td><td>224.90</td><td>157.00</td><td>214.00</td></tr><tr><td><br />CN算法</td><td>13.40</td><td>101.00</td><td>123.00</td><td>73.40</td><td>88.00</td><td>63.90</td><td>77.10</td></tr><tr><td><br />LCT算法</td><td>5.76</td><td>19.70</td><td>20.00</td><td>8.03</td><td>6.18</td><td>10.40</td><td>11.70</td></tr><tr><td><br />本文算法</td><td>11.90</td><td>20.60</td><td>21.10</td><td>16.40</td><td>12.20</td><td>14.10</td><td>16.10</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="117" name="117">3.2 跟踪效果对比</h4>
                <div class="p1">
                    <p id="118">图3为4种算法的实际跟踪效果, 其中, 括号中数字表示视频对应帧数。以Lemming、Gym和tiger2序列为例具体分析。在Lemming的第380帧, 目标在被完全遮挡后重新出现在视野中, 此时KCF算法与CN算法都跟丢目标, LCT算法与本文算法还能继续跟踪。之后目标发生快速移动, LCT算法准确性逐渐降低, 并在1 200帧后丢失目标, 相比本文算法未发生漂移。在Gym序列中, 目标位置变化较小, 但全程持续形变, KCF算法与CN算法无法准确跟踪, 并在680帧发生漂移。LCT算法虽然能够持续跟踪目标, 但目标框尺寸迅速缩小, 仅能对目标中变化较小的部分进行跟踪, 从图1与表2的CLE值可以看出, 虽然LCT算法中心位置误差不大且只有9.45像素, 但由于尺寸不准确, 重叠率达到54.1%, 比本文算法降低了36.7%。在Tiger2序列中, 目标一直保持快速移动且被部分遮挡, KCF算法在120帧就丢失目标, CN算法与LCT算法随着帧数增加偏差也渐渐增大, 在340帧CN算法已经发生漂移, 只有本文算法在全程保持了较高的准确性。</p>
                </div>
                <div class="p1">
                    <p id="119">上述结果表明, 本文算法保持了LCT算法的鲁棒性, 在发生局部遮挡或完全遮挡后依然能够持续跟踪。同时对于一些LCT算法无法解决的问题, 如快速形变和快速运动, 本文算法依旧有效, 提高了跟踪性能。</p>
                </div>
                <div class="area_img" id="120">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201908040_120.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 4种算法跟踪效果" src="Detail/GetImg?filename=images/JSJC201908040_120.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图3 4种算法跟踪效果</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201908040_120.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h3 id="121" name="121" class="anchor-tag">4 结束语</h3>
                <div class="p1">
                    <p id="122">本文提出一种基于特征融合的改进LCT跟踪算法。引入统计颜色特征方法共同获取目标响应矩阵, 根据跟踪置信度确定自适应融合因子, 综合目标模板与颜色跟踪方法得到最终结果。实验结果表明, 该算法的跟踪精度与成功率优于LCT算法, 在保持LCT算法鲁棒性与跟踪速度的同时, 对目标形变和快速移动情况具有较好的跟踪效果。本文在HOG特征的基础上融入了颜色直方图, 下一步将考虑融入其他特征, 研究不同场景下特征选取对跟踪效果的影响, 以进一步提升跟踪精度。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="142">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJX201506001&amp;v=MDQ5NjZPZVplUnFGQ2puVWI3Tkx6N0Jkckc0SDlUTXFZOUZaWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkw=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> 黄凯奇, 陈晓棠, 康运锋, 等.智能视频监控技术综述[J].计算机学报, 2015, 38 (6) :1093-1118.
                            </a>
                        </p>
                        <p id="144">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201610002&amp;v=MDczNzZVYjdOS0NMZlliRzRIOWZOcjQ5RlpvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVScUZDam4=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> 尹宏鹏, 陈波, 柴毅, 等.基于视觉的目标检测与跟踪综述[J].自动化学报, 2016, 42 (10) :1466-1489.
                            </a>
                        </p>
                        <p id="146">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=An experimental survey on correlation filter-based tracking">

                                <b>[3]</b> CHEN Zhe, HONG Zhibin, TAO Dacheng.An experimental survey on correlation filter-based tracking[EB/OL].[2018-04-20].https://arxiv.org/pdf/1509.05520.pdf.
                            </a>
                        </p>
                        <p id="148">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Visual object tracking using adaptive correlation filters">

                                <b>[4]</b> BOLME D S, BEVERIDGE J R, DRAPER B A, et al.Visual object tracking using adaptive correlation filters[C]//Proceedings of Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2010:2544-2550.
                            </a>
                        </p>
                        <p id="150">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Exploiting the circulant structure of trackingby-detection with kernels">

                                <b>[5]</b> HENRIQUES J F, CASEIRO R, MARTINS P, et al.Exploiting the circulant structure of tracking-by-detection with kernels[C]//Proceedings of European Conference on Computer Vision.Berlin, Germany:Springer, 2012:702-715.
                            </a>
                        </p>
                        <p id="152">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=High-speed tracking with kernelized correlation filters">

                                <b>[6]</b> HENRIQUES J F, CASEIRO R, MARTINS P, et al.High-speed tracking with kernelized correlation filters[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2015, 37 (3) :583-596.
                            </a>
                        </p>
                        <p id="154">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Adaptive color attributes for real-time visual tracking">

                                <b>[7]</b> DANELLJAN M, KHAN F S, FELSBERG M, et al.Adaptive color attributes for real-time visual tracking[C]//Proceedings of Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2014:1090-1097.
                            </a>
                        </p>
                        <p id="156">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=ECO:Efficient Convolution Operators for Tracking">

                                <b>[8]</b> DANELLJAN M, BHAT G, KHAN F S, et al.ECO:efficient convolution operators for tracking[C]//Proceedings of Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2017:6931-6939.
                            </a>
                        </p>
                        <p id="158">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201705021&amp;v=MTM5MzFNcW85SFpZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVScUZDam5VYjdOSWpYVGJMRzRIOWI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b> 沈秋, 严小乐, 刘霖枫, 等.基于自适应特征选择的多尺度相关滤波跟踪[J].光学学报, 2017, 37 (5) :166-175.
                            </a>
                        </p>
                        <p id="160">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=PROST:Parallel robust online simple tracking">

                                <b>[10]</b> SANTNER J, LEISTNER C, SAFFARI A, et al.PROST:parallel robust online simple tracking[C]//Proceedings of Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2010:723-730.
                            </a>
                        </p>
                        <p id="162">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Tracking-Learning-Detection">

                                <b>[11]</b> KALAL Z, MIKOLAJCZYK K, MATAS J.Tracking-learning-detection[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2012, 34 (7) :1409-1422.
                            </a>
                        </p>
                        <p id="164">
                            <a id="bibliography_12" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201608050&amp;v=MjYzMDg2ajU0TzN6cXFCdEdGckNVUkxPZVplUnFGQ2puVWI3Tkx6N0JiYkc0SDlmTXA0OUFaSVFLREg4NHZSNFQ=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[12]</b> 张雷, 于凤芹.基于置信图特性的改进时空上下文目标跟踪[J].计算机工程, 2016, 42 (8) :277-281, 288.
                            </a>
                        </p>
                        <p id="166">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Long-term Correlation Tracking">

                                <b>[13]</b> MA Chao, YANG Xiaokang, ZHANG Chongyang, et al.Long-term correlation tracking[C]//Proceedings of Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2015:5388-5396.
                            </a>
                        </p>
                        <p id="168">
                            <a id="bibliography_14" >
                                    <b>[14]</b>
                                 WANG Naiyan, SHI Jianping, YEUNG D Y, et al.Understanding and diagnosing visual tracking systems[C]//Proceedings of International Conference on Computer Vision.Washington D.C., USA:IEEE Press, 2015:3101-3109.
                            </a>
                        </p>
                        <p id="170">
                            <a id="bibliography_15" >
                                    <b>[15]</b>
                                 POSSEGGER H, MAUTHNER T, BISCHOF H.In defense of color-based model-free tracking[C]//Proceedings of Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2015:2113-2120.
                            </a>
                        </p>
                        <p id="172">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Staple:Complementary learners for real-time tracking">

                                <b>[16]</b> BERTINETTO L, VALMADRE J, GOLODETZ S, et al.Staple:complementary learners for real-time tracking[C]//Proceedings of Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2016:1401-1409.
                            </a>
                        </p>
                        <p id="174">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Large Margin Object Tracking with Circulant Feature Maps">

                                <b>[17]</b> WANG Mengmeng, LIU Yong, HUANG Zeyi.Large margin object tracking with circulant feature maps[C]//Proceedings of Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2017:4800-4808.
                            </a>
                        </p>
                        <p id="176">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Online object tracking:a benchmark">

                                <b>[18]</b> WU Yi, LIM J, YANG M H.Online object tracking:a benchmark[C]//Proceedings of Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2013:2411-2418.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJC201908040" />
        <input id="dpi" type="hidden" value="600" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201908040&amp;v=MTI2MjM0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVScUZDam5VYjdOTHo3QmJiRzRIOWpNcDQ5QlpJUUtESDg=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
