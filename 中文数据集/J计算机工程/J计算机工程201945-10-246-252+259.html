<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637126203785083750%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJC201910041%26RESULT%3d1%26SIGN%3d%252fUbHwn8mfhsmxe1vr6EZAsyXkYU%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201910041&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201910041&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201910041&amp;v=MDk3MjhIOWpOcjQ5QlpZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSdEZ5M2tWTHpKTHo3QmJiRzQ=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qem1VK3YrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#37" data-title="0 概述 ">0 概述</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#42" data-title="1 深度生成模型 ">1 深度生成模型</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#43" data-title="1.1 生成对抗网络">1.1 生成对抗网络</a></li>
                                                <li><a href="#47" data-title="1.2 深度卷积神经网络">1.2 深度卷积神经网络</a></li>
                                                <li><a href="#54" data-title="1.3 辅助分类器生成对抗网络">1.3 辅助分类器生成对抗网络</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#61" data-title="2 CP-ACGAN分类算法 ">2 CP-ACGAN分类算法</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#62" data-title="2.1 ACGAN分类算法">2.1 ACGAN分类算法</a></li>
                                                <li><a href="#68" data-title="2.2 CP-ACGAN算法">2.2 CP-ACGAN算法</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#102" data-title="3 实验结果与分析 ">3 实验结果与分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#104" data-title="3.1 MNIST数据集">3.1 MNIST数据集</a></li>
                                                <li><a href="#116" data-title="3.2 CIFAR10数据集">3.2 CIFAR10数据集</a></li>
                                                <li><a href="#122" data-title="3.3 CIFAR100数据集">3.3 CIFAR100数据集</a></li>
                                                <li><a href="#129" data-title="3.4 算法应用">3.4 算法应用</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#139" data-title="4 结束语 ">4 结束语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#56" data-title="&lt;b&gt;图1 ACGAN网络结构&lt;/b&gt;"><b>图1 ACGAN网络结构</b></a></li>
                                                <li><a href="#65" data-title="&lt;b&gt;图2 ACGAN生成器结构&lt;/b&gt;"><b>图2 ACGAN生成器结构</b></a></li>
                                                <li><a href="#67" data-title="&lt;b&gt;图3 ACGAN判别器结构&lt;/b&gt;"><b>图3 ACGAN判别器结构</b></a></li>
                                                <li><a href="#71" data-title="&lt;b&gt;图4 CP-ACGAN网络结构&lt;/b&gt;"><b>图4 CP-ACGAN网络结构</b></a></li>
                                                <li><a href="#92" data-title="&lt;b&gt;图5 CP-ACGAN判别网络结构&lt;/b&gt;"><b>图5 CP-ACGAN判别网络结构</b></a></li>
                                                <li><a href="#107" data-title="&lt;b&gt;图6 不同算法在MNIST数据集上的准确率对比&lt;/b&gt;"><b>图6 不同算法在MNIST数据集上的准确率对比</b></a></li>
                                                <li><a href="#109" data-title="&lt;b&gt;表1 不同算法在3种数据集上的最高预测准确率对比&lt;/b&gt;"><b>表1 不同算法在3种数据集上的最高预测准确率对比</b></a></li>
                                                <li><a href="#111" data-title="&lt;b&gt;表2 不同算法的平均预测均值与方差情况&lt;/b&gt;"><b>表2 不同算法的平均预测均值与方差情况</b></a></li>
                                                <li><a href="#114" data-title="&lt;b&gt;图7 ACGAN算法与CP-ACGAN算法的生成图像&lt;/b&gt;"><b>图7 ACGAN算法与CP-ACGAN算法的生成图像</b></a></li>
                                                <li><a href="#118" data-title="&lt;b&gt;图8 不同算法在CIFAR10数据集上的准确率对比&lt;/b&gt;"><b>图8 不同算法在CIFAR10数据集上的准确率对比</b></a></li>
                                                <li><a href="#120" data-title="&lt;b&gt;表3 CIFAR10数据集上不同算法的均值与方差&lt;/b&gt;"><b>表3 CIFAR10数据集上不同算法的均值与方差</b></a></li>
                                                <li><a href="#124" data-title="&lt;b&gt;图9 不同算法在CIFAR100数据集上的准确率对比&lt;/b&gt;"><b>图9 不同算法在CIFAR100数据集上的准确率对比</b></a></li>
                                                <li><a href="#126" data-title="&lt;b&gt;表4 不同算法在CIFAR100数据集上的均值与方差&lt;/b&gt;"><b>表4 不同算法在CIFAR100数据集上的均值与方差</b></a></li>
                                                <li><a href="#132" data-title="&lt;b&gt;图10 HTRU正样本侯选体图像&lt;/b&gt;"><b>图10 HTRU正样本侯选体图像</b></a></li>
                                                <li><a href="#133" data-title="&lt;b&gt;图11 HTRU负样本侯选体图像&lt;/b&gt;"><b>图11 HTRU负样本侯选体图像</b></a></li>
                                                <li><a href="#137" data-title="&lt;b&gt;表5 不同算法在HTRU上的参数计算值&lt;/b&gt;"><b>表5 不同算法在HTRU上的参数计算值</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" title=" KRIZHEVSKY A,SUTSKEVER I,HINTON G E.ImageNet classification with deep convolutional neural networks[C]//Proceedings of International Conference on Neural Information Processing Systems.[S.l.]:Curran Associates Inc.,2012:1097-1105." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image Net Classification with Deep Convolutional Neural Networks">
                                        <b>[1]</b>
                                         KRIZHEVSKY A,SUTSKEVER I,HINTON G E.ImageNet classification with deep convolutional neural networks[C]//Proceedings of International Conference on Neural Information Processing Systems.[S.l.]:Curran Associates Inc.,2012:1097-1105.
                                    </a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" >
                                        <b>[2]</b>
                                     HE Kaiming,ZHANG Xiangyu,REN Shaoqing,et al.Deep residual learning for image recognition[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Press,2016:770-778.</a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title=" 王万良,李卓蓉.生成式对抗网络研究进展[J].通信学报,2018,39(2):135-148." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=TXXB201802014&amp;v=MTY2ODFUWFRiTEc0SDluTXJZOUVZSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUnRGeTNrVkx6Sk0=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qem1VK3YrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         王万良,李卓蓉.生成式对抗网络研究进展[J].通信学报,2018,39(2):135-148.
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" title=" GOODFELLOW I J,POUGET-ABADIE J,MIRZA M,et al.Generative adversarial nets[C]//Proceedings of International Conference on Neural Information Processing Systems.Cambridge,USA:MIT Press,2014:2672-2680." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Generative adversarial nets">
                                        <b>[4]</b>
                                         GOODFELLOW I J,POUGET-ABADIE J,MIRZA M,et al.Generative adversarial nets[C]//Proceedings of International Conference on Neural Information Processing Systems.Cambridge,USA:MIT Press,2014:2672-2680.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" title=" RADFORD A,METZ L,CHINTALA S.Unsupervised representation learning with deep convolutional generative adversarial networks[EB/OL].[2018-09-08].https://arxiv.org/pdf/1511.06434.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks">
                                        <b>[5]</b>
                                         RADFORD A,METZ L,CHINTALA S.Unsupervised representation learning with deep convolutional generative adversarial networks[EB/OL].[2018-09-08].https://arxiv.org/pdf/1511.06434.pdf.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" title=" MIRZA M,OSINDERO S.Conditional generative adversarial nets[EB/OL].[2018-09-08].https://arxiv.org/pdf/1411.1784.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Conditional generative adversarial nets">
                                        <b>[6]</b>
                                         MIRZA M,OSINDERO S.Conditional generative adversarial nets[EB/OL].[2018-09-08].https://arxiv.org/pdf/1411.1784.pdf.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" title=" ODENA A,OLAH C,SHLENS J.Conditional image synthesis with auxiliary classifier GANs[EB/OL].[2018-09-08].https://arxiv.org/pdf/1610.09585.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Conditional image synthesis with auxiliary classifier GANs">
                                        <b>[7]</b>
                                         ODENA A,OLAH C,SHLENS J.Conditional image synthesis with auxiliary classifier GANs[EB/OL].[2018-09-08].https://arxiv.org/pdf/1610.09585.pdf.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" title=" KINGMA D P,REZENDE D J,MOHAMED S,et al.Semi-supervised learning with deep generative models[C]//Proceedings of International Conference on Neural Information Processing Systems.Cambridge,USA:MIT Press,2014:3581-3589." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Semisupervised learning with deep generative models">
                                        <b>[8]</b>
                                         KINGMA D P,REZENDE D J,MOHAMED S,et al.Semi-supervised learning with deep generative models[C]//Proceedings of International Conference on Neural Information Processing Systems.Cambridge,USA:MIT Press,2014:3581-3589.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" title=" DENG Zhijie,ZHANG Hao,LIANG Xiaodan,et al.Structured generative adversarial networks[C]//Proceedings of International Conference on Neural Information Processing Systems.Cambridge,USA:MIT Press,2017:3899-3909." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Structured generative adversarial networks">
                                        <b>[9]</b>
                                         DENG Zhijie,ZHANG Hao,LIANG Xiaodan,et al.Structured generative adversarial networks[C]//Proceedings of International Conference on Neural Information Processing Systems.Cambridge,USA:MIT Press,2017:3899-3909.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_10" title=" DAI Zihang,YANG Zhilin,YANG Fan,et al.Good semi-supervised learning that requires a bad GAN[C]//Proceedings of International Conference on Neural Information Processing Systems.Cambridge,USA:MIT Press,2017:1-14." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Good semi-supervised learning that requires a bad GAN">
                                        <b>[10]</b>
                                         DAI Zihang,YANG Zhilin,YANG Fan,et al.Good semi-supervised learning that requires a bad GAN[C]//Proceedings of International Conference on Neural Information Processing Systems.Cambridge,USA:MIT Press,2017:1-14.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_11" title=" ODENA A.Semi-supervised learning with generative adversarial networks[EB/OL].[2018-09-08].https://arxiv.org/pdf/1606.01583.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Semi-supervised learning with generative adversarial networks">
                                        <b>[11]</b>
                                         ODENA A.Semi-supervised learning with generative adversarial networks[EB/OL].[2018-09-08].https://arxiv.org/pdf/1606.01583.pdf.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_12" title=" 唐贤伦,杜一铭,刘雨微,等.基于条件深度卷积生成对抗网络的图像识别方法[J].自动化学报,2018,44(5):855-865." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201805009&amp;v=MjA5OTZWTHpKS0NMZlliRzRIOW5NcW85RmJZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSdEZ5M2s=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qem1VK3YrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[12]</b>
                                         唐贤伦,杜一铭,刘雨微,等.基于条件深度卷积生成对抗网络的图像识别方法[J].自动化学报,2018,44(5):855-865.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_13" title=" SALIMANS T,GOODFELLOW I,ZAREMBA W,et al.Improved techniques for training GANs[C]//Proceedings of International Conference on Neural Information Processing Systems.[S.l.]:Curran Associates Inc.,2016:1-10." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Improved techniques for training GANs">
                                        <b>[13]</b>
                                         SALIMANS T,GOODFELLOW I,ZAREMBA W,et al.Improved techniques for training GANs[C]//Proceedings of International Conference on Neural Information Processing Systems.[S.l.]:Curran Associates Inc.,2016:1-10.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_14" title=" CHEN Xi,DUAN Yan,HOUTHOOFT R,et al.InfoGAN:interpretable representation learning by information maximizing generative adversarial nets[C]//Proceedings of International Conference on Neural Information Processing Systems.[S.l.]:Curran Associates Inc.,2016:1-9." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=InfoGAN:interpretable representation learning by information maximizing generative adversarial nets">
                                        <b>[14]</b>
                                         CHEN Xi,DUAN Yan,HOUTHOOFT R,et al.InfoGAN:interpretable representation learning by information maximizing generative adversarial nets[C]//Proceedings of International Conference on Neural Information Processing Systems.[S.l.]:Curran Associates Inc.,2016:1-9.
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_15" title=" ULYANOV D,VEDALDI A,LEMPITSKY V.Adversarial generator-encoder networks[EB/OL].[2018-09-08].https://arxiv.org/pdf/1704.02304.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Adversarial generator-encoder networks">
                                        <b>[15]</b>
                                         ULYANOV D,VEDALDI A,LEMPITSKY V.Adversarial generator-encoder networks[EB/OL].[2018-09-08].https://arxiv.org/pdf/1704.02304.pdf.
                                    </a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_16" title=" SCHERER D,MULLER A,BEHNKE S.Evaluation of pooling operation in convolutional architecture for object recognition[C]//Proceedings of International Conference on Artificial Neural Networks.Berlin,Germany:Springer,2010:92-101." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Evaluation of pooling operations in convolutional architectures for object recognition">
                                        <b>[16]</b>
                                         SCHERER D,MULLER A,BEHNKE S.Evaluation of pooling operation in convolutional architecture for object recognition[C]//Proceedings of International Conference on Artificial Neural Networks.Berlin,Germany:Springer,2010:92-101.
                                    </a>
                                </li>
                                <li id="35">


                                    <a id="bibliography_17" title=" BOUREAU Y L,PONCE J,LECUN Y.A theoretical analysis of feature pooling in visual recognition[C]//Proceedings of International Conference on Machine Learning.Berlin,Germany:Springer,2010:111-118." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A theoretical analysis of feature pooling in visual recognition">
                                        <b>[17]</b>
                                         BOUREAU Y L,PONCE J,LECUN Y.A theoretical analysis of feature pooling in visual recognition[C]//Proceedings of International Conference on Machine Learning.Berlin,Germany:Springer,2010:111-118.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJC" target="_blank">计算机工程</a>
                2019,45(10),246-252+259 DOI:10.19678/j.issn.1000-3428.0052774            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于ACGAN的图像识别算法</b></span>
 <span class="shoufa"></span>                                     </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%91%A8%E6%9E%97%E5%8B%87&amp;code=28710035&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qem1VK3YrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">周林勇</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%B0%A2%E6%99%93%E5%B0%A7&amp;code=06962154&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qem1VK3YrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">谢晓尧</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%88%98%E5%BF%97%E6%9D%B0&amp;code=11023486&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qem1VK3YrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">刘志杰</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%B0%AD%E5%AE%8F%E5%8D%AB&amp;code=40515294&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qem1VK3YrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">谭宏卫</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%B8%B8%E5%96%84%E5%B9%B3&amp;code=23460367&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qem1VK3YrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">游善平</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E8%B4%B5%E5%B7%9E%E5%B8%88%E8%8C%83%E5%A4%A7%E5%AD%A6%E4%BF%A1%E6%81%AF%E4%B8%8E%E8%AE%A1%E7%AE%97%E7%A7%91%E5%AD%A6%E9%87%8D%E7%82%B9%E5%AE%9E%E9%AA%8C%E5%AE%A4&amp;code=0186961&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qem1VK3YrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">贵州师范大学信息与计算科学重点实验室</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E8%B4%B5%E5%B7%9E%E8%B4%A2%E7%BB%8F%E5%A4%A7%E5%AD%A6%E6%95%B0%E5%AD%A6%E4%B8%8E%E7%BB%9F%E8%AE%A1%E5%AD%A6%E9%99%A2&amp;code=1694100&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qem1VK3YrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">贵州财经大学数学与统计学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>针对基于辅助分类器生成对抗网络(ACGAN)的图像分类算法在训练过程中稳定性低且分类效果差的问题,提出一种改进的图像识别算法CP-ACGAN。对于网络结构,在判别网络的输出层取消样本的真假判别,只输出样本标签的后验估计并引入池化层。对于损失函数,除真实样本的交叉熵损失外,在判别网络中增加生成样本的条件控制标签及后验估计间的交叉熵损失。在此基础上,利用真假样本的交叉熵损失及属性重构生成器和判别器的损失函数。在MNSIT、CIFAR10、CIFAR100数据集上的实验结果表明,与ACGAN算法、CNN算法相比,该算法具有较好的分类效果与稳定性,且分类准确率分别高达99.62%、79.07%、48.03%。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qem1VK3YrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">生成对抗网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%BE%85%E5%8A%A9%E5%88%86%E7%B1%BB%E5%99%A8%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qem1VK3YrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">辅助分类器生成对抗网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qem1VK3YrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">特征提取;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qem1VK3YrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">图像分类;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%89%B9%E5%BE%81%E5%8C%B9%E9%85%8D&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qem1VK3YrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">特征匹配;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    周林勇(1987—),男,博士研究生,主研方向为图像处理、深度学习;E-mail:gzmtzly@126.com;
                                </span>
                                <span>
                                    *谢晓尧(通信作者),教授、博士生导师;;
                                </span>
                                <span>
                                    刘志杰,教授、博士;;
                                </span>
                                <span>
                                    谭宏卫,博士研究生。;
                                </span>
                                <span>
                                    游善平,博士研究生。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-09-28</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金(U1631132);</span>
                    </p>
            </div>
                    <h1><b>Image Identification Algorithm Based on ACGAN</b></h1>
                    <h2>
                    <span>ZHOU Linyong</span>
                    <span>XIE Xiaoyao</span>
                    <span>LIU Zhijie</span>
                    <span>TAN Hongwei</span>
                    <span>YOU Shanping</span>
            </h2>
                    <h2>
                    <span>Key Laboratory of Information and Computing Science,Guizhou Normal University</span>
                    <span>School of Mathematics and Statistics,Guizhou University of Finance and Economics</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>To address the problem that the image classification algorithm based on Auxiliary Classifier Generative Adversarial Net(ACGAN) is unstable and the classification effect is poor,an improved image recognition algorithm CP-ACGAN is proposed.In the network structure,the authenticity discrimination of the output layer samples is cancelled.The posterior estimation of the sample label is outputted and introduced into the pooling layer.For the loss function,in addition to the cross entropy loss of real samples the cross entropy loss between the conditional control label of the generated sample and its posterior estimation is added to the discriminant network.The loss functions of the generator and discriminator are reconstructed based on the cross entropy loss and attributes of true and false samples.Experiments on MNSIT,CIFAR10 and CIFAR100 datasets show that compared with ACGAN algorithm and CNN algorithm,the algorithm has better classification effect and stability,and the classification accuracy rate is 99.62%,79.07% and 48.03% respectively.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Generative%20Adversarial%20Network(GAN)&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qem1VK3YrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Generative Adversarial Network(GAN);</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Auxiliary%20Classifier%20Generative%20Adversarial%20Network(ACGAN)&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qem1VK3YrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Auxiliary Classifier Generative Adversarial Network(ACGAN);</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=feature%20extraction&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qem1VK3YrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">feature extraction;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=image%20classification&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qem1VK3YrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">image classification;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=feature%20matching&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qem1VK3YrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">feature matching;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-09-28</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="37" name="37" class="anchor-tag">0 概述</h3>
                <div class="p1">
                    <p id="38">图像识别是计算机视觉领域研究的热点问题之一。近年来,随着基于深度学习在图像特征提取等方面的发展,越来越多的研究人员开始利用深度学习的方法来解决图像分类问题。2012年,Krizhevsky A等人提出基于深度学习的AlexNet 网络,该网络在ImageNet图像分类大赛中,将Top5错误率控制在15.4%以内,超过第2名的非深度学习方法10个百分点<citation id="142" type="reference"><link href="3" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>。2015年,He Kaiming等人利用ResNet将Top5错误率控制在3.57%以内<citation id="143" type="reference"><link href="5" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>。深度网络的成功应用使得深度卷积神经网络(Convolutional Neural Network,CNN)成为图像分类中最重要的方法之一。深度卷积神经网络本质上是一种高效的特征提取方法,该方法将提取的特征作为分类器的输入从而实现分类。然而,此类网络的缺点是只能利用人工提供的样本,且不能学习到样本的空间分布及更深层次理解样本的内在结构,影响最终分类效果。</p>
                </div>
                <div class="p1">
                    <p id="39">生成模型是一种能够学习到数据的潜在分布并生成新样本的模型。传统的生成模型有贝叶斯网络、高斯混合和隐马尔科夫随机场等<citation id="144" type="reference"><link href="7" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>。文献<citation id="145" type="reference">[<a class="sup">4</a>]</citation>在传统生成网络的基础上,提出生成对抗网络(Generative Adversarial Network,GAN)。GAN的核心思想是同时训练生成和判别2个相互对抗的网络。训练判别网络使其可区分真实样本和生成网络生成的样本。因此,判别网络本质上是一个二分类模型。训练生成网络使其生成尽可能真实的样本,让判别器错误地将其判别为真样本,达到以假乱真的效果。</p>
                </div>
                <div class="p1">
                    <p id="40">文献<citation id="146" type="reference">[<a class="sup">5</a>]</citation>提出一种将GAN与CNN相结合的深度卷积生成对抗网络(Deep Convolutional Generative Adversarial Network,DCGAN)模型,使生成网络训练更加稳定,且图像更加清晰。文献<citation id="147" type="reference">[<a class="sup">6</a>]</citation>提出一种条件生成对抗网络(Conditional Generative Adversarial Network,CGAN),与原始GAN不同,CGAN训练时在生成器和判别器中加入了图像的类别标签,从而实现图像的可控性生成。文献<citation id="148" type="reference">[<a class="sup">7</a>]</citation>提出一种辅助分类器生成对抗网络(Auxiliary Classifier Generative Adversarial Network,ACGAN)。与CGAN一样,ACGAN也利用图像的标签信息进行训练,但此时只在生成器中加入标签信息便可实现图像的可控性生成。研究表明,与CGAN网络相比,ACGAN具有更好的图像生成能力。传统的GAN属于无监督学习范畴,文献<citation id="150" type="reference">[<a class="sup">8</a>,<a class="sup">9</a>,<a class="sup">10</a>,<a class="sup">11</a>]</citation>将GAN方法成功应用于半监督学习中,而CGAN、ACGAN的应用又将其引入到监督学习的范畴。文献<citation id="149" type="reference">[<a class="sup">12</a>]</citation>提出一种基于CGAN的图像识别方法,将生成对抗网络应用于有监督图像分类中。然而,传统的ACGAN网络的判别器用于图像分类时,存在训练不稳定、判别效果差等问题。</p>
                </div>
                <div class="p1">
                    <p id="41">本文提出一种改进的基于ACGAN的监督图像分类算法CP-ACGAN。该算法对ACGAN的网络结构和损失函数进行改进,同时,在判别器中引入特征匹配(Feature Matching,FM)<citation id="151" type="reference"><link href="27" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>以提高生成样本的多样性。</p>
                </div>
                <h3 id="42" name="42" class="anchor-tag">1 深度生成模型</h3>
                <h4 class="anchor-tag" id="43" name="43">1.1 生成对抗网络</h4>
                <div class="p1">
                    <p id="44">GAN模型基于最小最大二人博弈问题,其对抗训练的方式如式(1)所示。</p>
                </div>
                <div class="p1">
                    <p id="45" class="code-formula">
                        <mathml id="45"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mi>G</mi></munder></mrow><mtext> </mtext><mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mi>D</mi></munder></mrow><mtext> </mtext><mi>V</mi><mo stretchy="false">(</mo><mi>G</mi><mo>,</mo><mi>D</mi><mo stretchy="false">)</mo><mo>=</mo><mi>E</mi><msub><mrow></mrow><mrow><mi>x</mi><mo>∼</mo><mi>p</mi><msub><mrow></mrow><mrow><mtext>d</mtext><mtext>a</mtext><mtext>t</mtext><mtext>a</mtext></mrow></msub></mrow></msub><mo stretchy="false">[</mo><mrow><mi>ln</mi></mrow><mtext> </mtext><mi>D</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mo>+</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mi>E</mi><msub><mrow></mrow><mrow><mi>z</mi><mo>∼</mo><mi>p</mi><msub><mrow></mrow><mi>z</mi></msub><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo></mrow></msub><mo stretchy="false">[</mo><mrow><mi>ln</mi></mrow><mo stretchy="false">(</mo><mn>1</mn><mo>-</mo><mi>D</mi><mo stretchy="false">(</mo><mi>G</mi><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="46">其中,<i>E</i>表示期望。GAN网络模型包含一个生成器<i>G</i>和一个判别器<i>D</i>,生成器是噪声<i>z</i>～<i>p</i><sub><i>z</i></sub>(<i>z</i>)到生成样本空间<i>G</i>(<i>z</i>;<i>θ</i><sub><i>g</i></sub>)的一个映射,而判别器<i>D</i>(<i>x</i>;<i>θ</i><sub><i>d</i></sub>)则判断输入样本<i>x</i>是来自真实分布还是生成分布,因此判别器本质上是一个二分类问题。在<i>G</i>与<i>D</i>的对抗中,生成分布<i>p</i><sub><i>g</i></sub>(<i>x</i>)不断靠近真实分布<i>p</i>(<i>x</i>),最终达到Nash均衡。此时生成器可以完全拟合真实数据分布,即<i>p</i><sub>g</sub>(<i>x</i>)=<i>p</i>(<i>x</i>),从而实现生成样本的分布与真实样本分布完全一致,达到生成真实样本的目的。GAN的2个神经网络<i>G</i>和<i>D</i>均利用传统的反向传播原理,且计算过程不需要复杂的马尔科夫链、极大似然估计及复杂的变分下限,因此,可大幅降低网络的训练难度,使模型更容易收敛。</p>
                </div>
                <h4 class="anchor-tag" id="47" name="47">1.2 深度卷积神经网络</h4>
                <div class="p1">
                    <p id="48">原始GAN的生成器和判别器都采用全连接神经网络,训练过程简单且计算量小,但训练后生成的图片比较模糊,视觉效果较差。而CNN具有强大的特征提取能力及更好的空间信息感知能力,文献<citation id="152" type="reference">[<a class="sup">5</a>]</citation>提出DCGAN网络模型,用卷积层和转置卷积层分别代替<i>D</i>和<i>G</i>中的全连接层,使生成图像的清晰度更高。</p>
                </div>
                <div class="p1">
                    <p id="49">DCGAN在网络结构上有如下改变:</p>
                </div>
                <div class="p1">
                    <p id="50">1)取消了CNN中的池化层,在判别器中用步幅卷积代替池化,在生成器中用fractional-strided卷积代替池化。</p>
                </div>
                <div class="p1">
                    <p id="51">2)除生成器的输出层和判别器的输入层外,其他网络层都加入批量归一化(Batch Normalization,BN)。BN有助于降低网络对初始参数的过度依赖,防止梯度消失及生成器收敛到同一点,从而提高生成样本的多样性及训练网络的稳定性,降低网络的震荡。</p>
                </div>
                <div class="p1">
                    <p id="52">3)取消全连接层。</p>
                </div>
                <div class="p1">
                    <p id="53">4)在生成器中,除最后输出层使用Tanh激活函数外,其他层均采用ReLU函数,判别器中所有层均使用LeakyReLU激活函数。</p>
                </div>
                <h4 class="anchor-tag" id="54" name="54">1.3 辅助分类器生成对抗网络</h4>
                <div class="p1">
                    <p id="55">传统的生成对抗网络都是无监督模型,CGAN将生成对抗网络应用到监督型学习方法中,可以使标签与生成图像相对应。ACGAN在CGAN的基础上进行改进,并结合InfoGAN<citation id="153" type="reference"><link href="29" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>中最大互信息的思想。图1所示为ACGAN的网络结构。</p>
                </div>
                <div class="area_img" id="56">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201910041_056.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qem1VK3YrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 ACGAN网络结构" src="Detail/GetImg?filename=images/JSJC201910041_056.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qem1VK3YrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图1 ACGAN网络结构</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201910041_056.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qem1VK3YrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="57">ACGAN训练的目标函数如式(2)和式(3)所示。</p>
                </div>
                <div class="p1">
                    <p id="58"><i>L</i><sub><i>s</i></sub>=<i>E</i>[ln <i>p</i>(<i>s</i>=real|<i>x</i><sub>data</sub>)]+<i>E</i>[ln <i>p</i>(<i>s</i>=fake)|<i>x</i><sub>fake</sub>]      (2)</p>
                </div>
                <div class="p1">
                    <p id="59"><i>L</i><sub><i>c</i></sub>=<i>E</i>[ln <i>p</i>(<i>C</i>=<i>c</i>|<i>x</i><sub>data</sub>)]+<i>E</i>[ln <i>p</i>(<i>C</i>=<i>c</i>)|<i>x</i><sub>fake</sub>]      (3)</p>
                </div>
                <div class="p1">
                    <p id="60">训练<i>D</i>可使<i>L</i><sub><i>s</i></sub>+<i>L</i><sub><i>c</i></sub>最大化,训练<i>G</i>可使<i>L</i><sub><i>c</i></sub>-<i>L</i><sub><i>s</i></sub>最大化。从网络结构或训练目标函数可以看出,ACGAN损失函数在GAN的基础上增加了输入样本标签信息与标签后验估计值之间的交叉熵。</p>
                </div>
                <h3 id="61" name="61" class="anchor-tag">2 CP-ACGAN分类算法</h3>
                <h4 class="anchor-tag" id="62" name="62">2.1 ACGAN分类算法</h4>
                <div class="p1">
                    <p id="63">ACGAN中判别器除输出样本真假判别外,还输出输入标签的后验估计。在网络训练完成后,输入一个样本<i>x</i>,判别器就可以输出其对应于每一类的概率<i>p</i>(<i>y</i>|<i>x</i>),选择使得<i>p</i>(<i>y</i>|<i>x</i>)最大的类别<i>k</i>作为输入样本<i>x</i>的标签,从而对图像进行分类。</p>
                </div>
                <div class="p1">
                    <p id="64">基于ACGAN的图像分类模型的生成器结构如图2所示(以MNIST数据集为例)。生成器包括4个全连接层和5个转置卷积层,其中,1、3转置卷积层结构相同,且<i>kernel</i>_<i>size</i>为4,<i>stride</i>为2,<i>padding</i>为1;2、4、5转置卷积层结构相同,且<i>kernel</i>_<i>size</i>为5,<i>stride</i>为1, <i>padding</i>为1。</p>
                </div>
                <div class="area_img" id="65">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201910041_065.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qem1VK3YrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 ACGAN生成器结构" src="Detail/GetImg?filename=images/JSJC201910041_065.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qem1VK3YrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图2 ACGAN生成器结构</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201910041_065.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qem1VK3YrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="66">图3所示为ACGAN模型判别器结构。判别器与生成器结构刚好相反,但同样包括5个卷积层和4个全连接层,其中,1、2、4卷积层结构相同,且<i>kernel</i>_<i>size</i>为5,<i>stride</i>为1,<i>padding</i>为1,3、5卷积层结构相同,且<i>kernel</i>_<i>size</i>为4,<i>stride</i>为2,<i>padding</i>为1。判别网络的输出层除输出样本真假判别外,还输出样本标签的后验估计,在测试集中,即为样本标签的预测值。</p>
                </div>
                <div class="area_img" id="67">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201910041_067.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qem1VK3YrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 ACGAN判别器结构" src="Detail/GetImg?filename=images/JSJC201910041_067.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qem1VK3YrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图3 ACGAN判别器结构</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201910041_067.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qem1VK3YrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="68" name="68">2.2 CP-ACGAN算法</h4>
                <h4 class="anchor-tag" id="69" name="69">2.2.1 网络结构与损失函数的改进</h4>
                <div class="p1">
                    <p id="70">在利用ACGAN的判别网络<i>D</i>进行分类时,存在收敛速度慢、训练不稳定等问题。因此,本文将原始的ACGAN网络结构进行改进,其结构如图4所示。</p>
                </div>
                <div class="area_img" id="71">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201910041_071.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qem1VK3YrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 CP-ACGAN网络结构" src="Detail/GetImg?filename=images/JSJC201910041_071.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qem1VK3YrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图4 CP-ACGAN网络结构</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201910041_071.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qem1VK3YrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="72">从图4可以看出,改进后的网络取消了判别器中的真假样本判别项输出,同时在判别器中引入了特征匹配。为保证网络有效利用真假样本各自特征,对生成器和判别器的损失函数进行改进,将真实样本看成带标签的监督数据,将生成样本看成有标签的假数据,然后在判别网络的输出层连接softmax分类器,得到真实样本的监督损失函数为:</p>
                </div>
                <div class="p1">
                    <p id="73" class="code-formula">
                        <mathml id="73"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>L</mi><msub><mrow></mrow><mrow><mtext>s</mtext><mtext>u</mtext><mtext>p</mtext><mtext>e</mtext><mtext>r</mtext><mtext>v</mtext><mtext>i</mtext><mtext>s</mtext><mtext>e</mtext><mtext>d</mtext></mrow></msub><mo>=</mo><mo>-</mo><mi>E</mi><msub><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>∼</mo><mi>p</mi><msub><mrow></mrow><mrow><mtext>d</mtext><mtext>a</mtext><mtext>t</mtext><mtext>a</mtext></mrow></msub></mrow></msub><mo stretchy="false">[</mo><mrow><mi>ln</mi></mrow><mtext> </mtext><mi>p</mi><mo stretchy="false">(</mo><msup><mi>y</mi><mo>′</mo></msup><mo stretchy="false">|</mo><mi>x</mi><mo>,</mo><msup><mi>y</mi><mo>′</mo></msup><mo>&lt;</mo><mi>Κ</mi><mo>+</mo><mn>1</mn><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mo>=</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mo>-</mo><mfrac><mn>1</mn><mi>Ν</mi></mfrac><mrow><mi>ln</mi></mrow><mtext> </mtext><mi>p</mi><mo stretchy="false">(</mo><msup><mi>y</mi><mo>′</mo></msup><mo stretchy="false">|</mo><mi>x</mi><mo>,</mo><msup><mi>y</mi><mo>′</mo></msup><mo>&lt;</mo><mi>Κ</mi><mo>+</mo><mn>1</mn><mo stretchy="false">)</mo><mo>=</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mo>-</mo><mfrac><mn>1</mn><mi>Ν</mi></mfrac><mrow><mi>ln</mi></mrow><mfrac><mrow><mi>exp</mi><mo stretchy="false">(</mo><mo>〈</mo><mi>y</mi><mo>,</mo><msup><mi>y</mi><mo>′</mo></msup><mo>〉</mo><mo stretchy="false">)</mo></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Κ</mi></munderover><mtext>e</mtext></mstyle><mtext>x</mtext><mtext>p</mtext><mtext> </mtext><msup><mi>y</mi><mo>′</mo></msup><msub><mrow></mrow><mi>i</mi></msub></mrow></mfrac><mo>=</mo><mo>-</mo><mfrac><mn>1</mn><mi>Ν</mi></mfrac><mo stretchy="false">(</mo><mo>〈</mo><mi>y</mi><mo>,</mo><msup><mi>y</mi><mo>′</mo></msup><mo>〉</mo><mo>-</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mrow><mi>ln</mi></mrow><mo stretchy="false">(</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Κ</mi></munderover><mrow><mi>exp</mi></mrow></mstyle><mtext> </mtext><msup><mi>y</mi><mo>′</mo></msup><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>=</mo><mi>C</mi><mi>E</mi><mo stretchy="false">(</mo><mi>y</mi><mo>,</mo><msup><mi>y</mi><mo>′</mo></msup><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>4</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="74">其中,<i>N</i>为训练中一个batch内的样本数,〈·〉表示内积,<i>y</i>为样本标签,<i>y</i>′为样本标签预测值。因此,真实数据的损失函数为:</p>
                </div>
                <div class="p1">
                    <p id="75"><i>L</i><sub>real</sub>=<i>L</i><sub>supervised</sub>      (5)</p>
                </div>
                <div class="p1">
                    <p id="76">对生成数据,其误差包括2个部分:一部分为判别生成样本为第<i>K</i>+1类,即假样本类的概率损失值;另一部分为生成样本的输出标签<i>y</i>′<sub>fake</sub>与输入标签<i>y</i>之间的交叉熵损失值。令<i>L</i><sub>unsupervised</sub>表示假样本类的概率期望损失,由softmax分类器定义,有:</p>
                </div>
                <div class="p1">
                    <p id="77">softmax(<i>x</i>)=softmax(<i>x</i>-<i>c</i>)      (6)</p>
                </div>
                <div class="p1">
                    <p id="78">令<i>y</i>′<sub><i>K</i></sub><sub>+1</sub>=0,可得:</p>
                </div>
                <div class="p1">
                    <p id="79" class="code-formula">
                        <mathml id="79"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>L</mi><msub><mrow></mrow><mrow><mtext>u</mtext><mtext>n</mtext><mtext>s</mtext><mtext>u</mtext><mtext>p</mtext><mtext>e</mtext><mtext>r</mtext><mtext>v</mtext><mtext>i</mtext><mtext>s</mtext><mtext>e</mtext><mtext>d</mtext></mrow></msub><mo>=</mo><mo>-</mo><mi>E</mi><msub><mrow></mrow><mrow><mi>x</mi><mo>∼</mo><mi>p</mi><msub><mrow></mrow><mi>g</mi></msub></mrow></msub><mo stretchy="false">[</mo><mrow><mi>ln</mi></mrow><mtext> </mtext><mi>p</mi><mo stretchy="false">(</mo><msup><mi>y</mi><mo>′</mo></msup><mo>=</mo><mi>Κ</mi><mo>+</mo><mn>1</mn><mo stretchy="false">|</mo><mi>x</mi><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mo>=</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mo>-</mo><mfrac><mn>1</mn><mi>Ν</mi></mfrac><mrow><mi>ln</mi></mrow><mtext> </mtext><mi>p</mi><mo stretchy="false">(</mo><msup><mi>y</mi><mo>′</mo></msup><mo>=</mo><mi>Κ</mi><mo>+</mo><mn>1</mn><mo stretchy="false">|</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mo>-</mo><mfrac><mn>1</mn><mi>Ν</mi></mfrac><mrow><mi>ln</mi></mrow><mfrac><mrow><mi>exp</mi><mtext> </mtext><msup><mi>y</mi><mo>′</mo></msup><msub><mrow></mrow><mrow><mi>Κ</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>Κ</mi><mo>+</mo><mn>1</mn></mrow></munderover><mtext>e</mtext></mstyle><mtext>x</mtext><mtext>p</mtext><mtext> </mtext><msup><mi>y</mi><mo>′</mo></msup><msub><mrow></mrow><mi>i</mi></msub></mrow></mfrac><mo>=</mo><mfrac><mn>1</mn><mi>Ν</mi></mfrac><mrow><mi>ln</mi></mrow><mo stretchy="false">(</mo><mn>1</mn><mo>+</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Κ</mi></munderover><mrow><mi>exp</mi></mrow></mstyle><mtext> </mtext><msup><mi>y</mi><mo>′</mo></msup><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>7</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="80">在网络中,每个batch内生成样本的输入标签与真实样本的标签一致,因此生成样本的生成标签<i>y</i>′<sub>fake</sub>与输入标签<i>y</i>之间的交叉熵损失值为<i>CE</i>(<i>y</i>,<i>y</i>′<sub>fake</sub>)。综上可知,生成样本的损失为:</p>
                </div>
                <div class="p1">
                    <p id="81"><i>L</i><sub>fake</sub>=0.5×(<i>L</i><sub>unsupervised</sub>+<i>CE</i>(<i>y</i>,<i>y</i>′<sub>fake</sub>))      (8)</p>
                </div>
                <div class="p1">
                    <p id="82">训练过程中交替更新生成器与判别器参数,因此,需要分别构建生成器和判别器的误差。</p>
                </div>
                <div class="p1">
                    <p id="83">判别器<i>D</i>的损失函数为:</p>
                </div>
                <div class="p1">
                    <p id="84"><i>L</i><sub><i>D</i></sub>=0.5×(<i>L</i><sub>real</sub>+<i>L</i><sub>fake</sub>)      (9)</p>
                </div>
                <div class="p1">
                    <p id="85">生成器<i>G</i>的损失函数为:</p>
                </div>
                <div class="p1">
                    <p id="86"><i>L</i><sub><i>G</i></sub>=0.5×(<i>L</i><sub>FM</sub>+<i>L</i><sub>unsupervised</sub>)      (10)</p>
                </div>
                <div class="p1">
                    <p id="87">其中,<i>L</i><sub>FM</sub>=‖<i>E</i><sub><i>x</i></sub><sub>～</sub><sub><i>p</i></sub><sub>data</sub>[<i>f</i>(<i>x</i>)]-<i>E</i><sub><i>z</i></sub><sub>～</sub><sub><i>pz</i></sub>[<i>f</i>(<i>G</i>(<i>z</i>))]‖<mathml id="141"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></math></mathml>,表示特征匹配的二范数损失项。</p>
                </div>
                <h4 class="anchor-tag" id="88" name="88">2.2.2 池化方法</h4>
                <div class="p1">
                    <p id="89">池化方法作为卷积神经网络的重要步骤,不仅可以有效提取特征,还可以实现数据降维并防止过拟合。池化是卷积神经网络特征提取的关键步骤,具有保持平移、旋转、伸缩不变性等特点<citation id="154" type="reference"><link href="33" rel="bibliography" /><link href="35" rel="bibliography" /><sup>[<a class="sup">16</a>,<a class="sup">17</a>]</sup></citation>。常用的池化方法包括均值池化、最大池化和随机池化等。</p>
                </div>
                <div class="p1">
                    <p id="90">在DCGAN的应用中,为使生成的图片更加高清,在判别网络中使用转置卷积来代替池化。然而,池化方法在分类问题上具有重要的作用,因此,将生成对抗网络与池化方法相结合,使带池化的生成对抗网络可用于解决分类问题,不仅可以使生成样本具有多样性,还可以有效地提取特征。</p>
                </div>
                <div class="p1">
                    <p id="91">基于此,本文提出一种用于分类的生成对抗网络算法CP-ACGAN。该算法在特征匹配和重构损失函数的基础上进行改进,将ACGAN中判别器的部分卷积层改为池化层,并将原判别网络中的第3、5卷积层改为池化层,而生成器结构保持不变。图5所示为改进后的CP-ACGAN判别网络结构。</p>
                </div>
                <div class="area_img" id="92">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201910041_092.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qem1VK3YrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 CP-ACGAN判别网络结构" src="Detail/GetImg?filename=images/JSJC201910041_092.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qem1VK3YrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图5 CP-ACGAN判别网络结构</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201910041_092.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qem1VK3YrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="93" name="93">2.2.3 可行性分析</h4>
                <div class="p1">
                    <p id="94">在原始GAN、ACGAN的基础上,本文提出CP-ACGAN分类算法。GAN作为一种生成模型,由于其较强的数据生成能力而被应用到半监督学习中。与基于GAN的半监督学习方法相比,本文CP-ACGAN算法有以下2点改进:1)在生成数据时,潜空间中加入了真实数据标签,利用判别模型对样本标签做后验估计,当最小化判别器损失函数时,可以使生成样本的伪标签与后验估计一致,同时也可使生成模型产生的图片与输入标签相对应,从而进一步扩充训练样本的多样性,提高预测准确率;2)本文在结构模型上引入池化方法,在GAN中,为使生成的图片更加逼真,通常采用步幅卷积来代替池化,但步幅卷积分类效果较差,池化因具有较强的特征提取功能,更加适合分类问题,因此本文选用池化方法。</p>
                </div>
                <div class="p1">
                    <p id="95">与ACGAN相比,本文CP-ACGAN算法进行如下2点改进:</p>
                </div>
                <div class="p1">
                    <p id="96">1)调整判别网络的输出层。基于GAN的半监督学习,取消样本的真假判别,只输出样本标签的后验估计。同时与ACGAN不同,本文算法只适用于图片识别,因此,在生成网络中采用了池化层而取消卷积层。</p>
                </div>
                <div class="p1">
                    <p id="97">2)改变损失函数。判别网络输出层会改变损失函数,进而更有利于提高图像识别能力。</p>
                </div>
                <div class="p1">
                    <p id="98">与传统的CNN算法相比,本文CP-ACGAN算法具有较强的图像识别能力,主要有以下2个原因:</p>
                </div>
                <div class="p1">
                    <p id="99">1)与GAN一样,本文CP-ACGAN算法具有拟合样本分布的能力,可以学习到样本数据的内在分布,能从更高层次理解样本数据的空间分布。</p>
                </div>
                <div class="p1">
                    <p id="100">2)利用GAN的强大生成能力,不断生成新的、更加多样化的样本,这本质上是对样本的一种有效扩充,而CNN仅对给定的样本进行特征提取,且依赖人为提供的样本特征。</p>
                </div>
                <div class="p1">
                    <p id="101">综上,本文CP-ACGAN算法与GAN半监督学习算法、ACGAN算法和CNN算法都有相似之处,但又有所不同。CP-ACGAN算法分别利用ACGAN的模型特征、GAN半监督学习中判别网络输出层结构以及CNN池化算法来重构模型结构,再重构生成器和判别器的损失函数,从而实现基于生成模型的图像分类。</p>
                </div>
                <h3 id="102" name="102" class="anchor-tag">3 实验结果与分析</h3>
                <div class="p1">
                    <p id="103">为进一步验证本文算法的有效性,拟在MNIST、CIFAR10、CIFAR100及HTRU这4种数据集上分别进行实验。其中,CP-ACGAN算法的池化层均采用均值池化。</p>
                </div>
                <h4 class="anchor-tag" id="104" name="104">3.1 MNIST数据集</h4>
                <div class="p1">
                    <p id="105">MNIST为手写字体数据集,共60 000个训练样本以及10 000个测试样本,每个样本对应0～9中的一个数字,且均为28像素×28像素的二维图像数据,展开成向量后为784维。每次训练的batchsize为100,每组实验共训练100个epoch,生成器和判别器均采用Adam优化,学习率均为0.000 2。实验基于深度学习框架Pytorch实现。</p>
                </div>
                <div class="p1">
                    <p id="106">对于图像分类问题,目前最优的方法是深度卷积神经网络算法,本文将CP-ACGAN算法与CNN算法进行比较。与ACGAN判别器相比,CNN算法的卷积层后面加入池化层,与本文CP-ACGAN算法的判别器网络结构是一样的。因此,本文使用CNN的均值池化和最大池化作为对比算法。图6所示为不同算法在MNIST测试集上的准确率对比结果。</p>
                </div>
                <div class="area_img" id="107">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201910041_107.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qem1VK3YrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 不同算法在MNIST数据集上的准确率对比" src="Detail/GetImg?filename=images/JSJC201910041_107.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qem1VK3YrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图6 不同算法在MNIST数据集上的准确率对比</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201910041_107.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qem1VK3YrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="108">表1所示为训练完成后不同算法在3种数据集上的最高预测准确率对比。</p>
                </div>
                <div class="area_img" id="109">
                    <p class="img_tit"><b>表1 不同算法在3种数据集上的最高预测准确率对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="109" border="1"><tr><td>算法</td><td>MNIST</td><td>CIFAR10</td><td>CIFAR100</td></tr><tr><td><br />均值池化CNN算法</td><td>0.995 1</td><td>0.779 6</td><td>0.459 4</td></tr><tr><td><br />最大池化CNN算法</td><td>0.994 3</td><td>0.763 9</td><td>0.428 3</td></tr><tr><td><br />ACGAN算法</td><td>0.995 0</td><td>0.730 6</td><td>0.398 9</td></tr><tr><td><br />CP-ACGAN算法</td><td>0.996 2</td><td>0.790 7</td><td>0.480 3</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="110">表2所示为训练50个epoch后,网络训练趋于平稳时,不同算法的平均预测均值与方差情况。</p>
                </div>
                <div class="area_img" id="111">
                    <p class="img_tit"><b>表2 不同算法的平均预测均值与方差情况</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="111" border="1"><tr><td><br />算法</td><td>均值</td><td>方差</td></tr><tr><td><br />均值池化CNN算法</td><td>0.994 980</td><td>6.000 000e-09</td></tr><tr><td><br />最大池化CNN算法</td><td>0.994 144</td><td>3.264 000e-09</td></tr><tr><td><br />ACGAN算法</td><td>0.994 116</td><td>4.069 440e-07</td></tr><tr><td><br />CP-ACGAN算法</td><td>0.995 604</td><td>1.923 840e-07</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="112">从图6、表1、表2可以看出,与ACGAN算法相比,CP-ACGAN算法的方差较小,即具有较好的稳定性。同时,CP-ACGAN算法的最高预测准确率为99.62%,高于ACGAN算法的99.50%,50个epoch后的平均预测准确率较高;与CNN算法相比,CP-ACGAN算法具有较高的最高预测准确率和平均预测准确率。但相比CNN算法,CP-ACGAN算法的方差较大,即稳定性较差。</p>
                </div>
                <div class="p1">
                    <p id="113">图7所示为100个epoch训练完成后,ACGAN算法与CP-ACGAN算法生成的图像。</p>
                </div>
                <div class="area_img" id="114">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201910041_114.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qem1VK3YrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 ACGAN算法与CP-ACGAN算法的生成图像" src="Detail/GetImg?filename=images/JSJC201910041_114.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qem1VK3YrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图7 ACGAN算法与CP-ACGAN算法的生成图像</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201910041_114.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qem1VK3YrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="115">从图7可以看出,虽然CP-ACGAN算法具有较好的分类效果,但生成的图像质量比ACGAN算法差,这与用池化来代替步幅卷积有关。</p>
                </div>
                <h4 class="anchor-tag" id="116" name="116">3.2 CIFAR10数据集</h4>
                <div class="p1">
                    <p id="117">CIFAR10是比MNIST更加复杂的数据集,每张图像是32像素×32像素的彩色图像,即图像大小为3×32×32,共包含10个类别,每个类别5 000张图像,即共50 000张训练图像,另有10 000张测试图片。本文实验的网络结构与MNIST实验结构类似,只是生成器最后的输出层输出特征数和判别器的输入层特征数均为3。图8所示为训练完100个epoch后,不同算法在CIFAR10数据集上的测试准确率对比结果。</p>
                </div>
                <div class="area_img" id="118">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201910041_118.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qem1VK3YrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 不同算法在CIFAR10数据集上的准确率对比" src="Detail/GetImg?filename=images/JSJC201910041_118.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qem1VK3YrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图8 不同算法在CIFAR10数据集上的准确率对比</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201910041_118.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qem1VK3YrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="119">表3所示为训练50个epoch后,不同算法的均值与方差对比结果。</p>
                </div>
                <div class="area_img" id="120">
                    <p class="img_tit"><b>表3 CIFAR10数据集上不同算法的均值与方差</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="120" border="1"><tr><td><br />算法</td><td>均值</td><td>方差</td></tr><tr><td><br />均值池化CNN算法</td><td>0.775 686</td><td>3.935 204 0e-06</td></tr><tr><td><br />最大池化CNN算法</td><td>0.755 746</td><td>3.878 884 0e-06</td></tr><tr><td><br />ACGAN算法</td><td>0.719 572</td><td>5.750 046 4e-05</td></tr><tr><td><br />CP-ACGAN算法</td><td>0.782 244</td><td>3.151 721 6e-05</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="121">从图8、表1、表3可以看出,ACGAN算法在MNIST数据集上具有较好的识别效果,但在处理复杂的CIFAR10数据集时,识别效果较差。改进后的CP-ACGAN算法表现出极强的适应能力,在处理复杂的CIFAR10数据集时,识别效果优于CNN算法,但稳定性较差。</p>
                </div>
                <h4 class="anchor-tag" id="122" name="122">3.3 CIFAR100数据集</h4>
                <div class="p1">
                    <p id="123">CIFAR100数据集与CIFAR10数据集类似,都是三通道彩色图像。但CIFAR100数据集共有100个类别,每个类别共500张训练图像,即共有50 000张训练图像,另有10 000张测试图像。CIFAR100数据集实验中不同算法的网络结构与CIFAR10数据集相同。图9所示为不同算法在CIFAR100数据集上的准确率对比结果。</p>
                </div>
                <div class="area_img" id="124">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201910041_124.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qem1VK3YrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图9 不同算法在CIFAR100数据集上的准确率对比" src="Detail/GetImg?filename=images/JSJC201910041_124.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qem1VK3YrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图9 不同算法在CIFAR100数据集上的准确率对比</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201910041_124.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qem1VK3YrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="125">表4所示为当训练50个epoch,网络逐渐趋于稳定后,不同算法的均值与方差。</p>
                </div>
                <div class="area_img" id="126">
                    <p class="img_tit"><b>表4 不同算法在CIFAR100数据集上的均值与方差</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="126" border="1"><tr><td><br />算法</td><td>均值</td><td>方差</td></tr><tr><td><br />均值池化CNN算法</td><td>0.452 368</td><td>4.916 176 0e-06</td></tr><tr><td><br />最大池化CNN算法</td><td>0.409 430</td><td>5.255 700 0e-06</td></tr><tr><td><br />ACGAN算法</td><td>0.388 450</td><td>3.049 890 0e-05</td></tr><tr><td><br />CP-ACGAN算法</td><td>0.462 822</td><td>2.751 691 6e-05</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="127">从图9、表1、表4可以看出,与CNN算法相比,ACGAN算法在复杂的CIFAR100数据集中识别效果较差。与CNN算法相比,本文提出的CP-ACGAN算法在CIFAR100数据集上具有较强的适应能力,但稳定性较差,同时与ACGAN算法相比具有更小的方差,因此,在测试集中的稳定性较好。</p>
                </div>
                <div class="p1">
                    <p id="128">综上可知,当处理较简单的MNIST数据集时,ACGAN、CP-ACGAN算法均具有较好的分类效果;当处理复杂的高维数据时,ACGAN表现不如CNN算法,但本文提出的CP-ACGAN算法具有较好的分类效果。因此,CP-ACGAN算法增强了网络对复杂数据的适应能力,且识别效果优于CNN算法。</p>
                </div>
                <h4 class="anchor-tag" id="129" name="129">3.4 算法应用</h4>
                <div class="p1">
                    <p id="130">世界上最大口径射电望远镜(FAST)500米射电天文望远镜于2016年在贵州省平塘县建成,寻找脉冲星是FAST的主要科学目标之一。脉冲星的识别本质上是一个二分类问题,为确定脉冲星,在脉冲星候选体HTRU数据集上进行实验并验证本文算法的有效性。该数据集包含1 195个正样本(脉冲星样本)和89 999个负样本(噪声信号样本),正负样本不平衡是脉冲星侯选体的主要特征之一。</p>
                </div>
                <div class="p1">
                    <p id="131">图10、图11是正、负侯选体样本的时间频率图。</p>
                </div>
                <div class="area_img" id="132">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201910041_132.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qem1VK3YrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图10 HTRU正样本侯选体图像" src="Detail/GetImg?filename=images/JSJC201910041_132.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qem1VK3YrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图10 HTRU正样本侯选体图像</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201910041_132.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qem1VK3YrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="133">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201910041_133.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qem1VK3YrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图11 HTRU负样本侯选体图像" src="Detail/GetImg?filename=images/JSJC201910041_133.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qem1VK3YrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图11 HTRU负样本侯选体图像</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201910041_133.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qem1VK3YrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="134">本文实验选取两类样本的Sub-Integrations图并采用自样本叠加,在不改变样本基本轮廓的情况下将Sub-Integrations图的维度转化为64×64,将总样本的80%作为训练集,20%作为测试集。由于正、负样本数量差别较大,因此本文选择精确率(<i>P</i><sub><i>r</i></sub>)、召回率(<i>R</i>)和<i>F</i>1值作为评价指标,其计算公式如下:</p>
                </div>
                <div class="p1">
                    <p id="135" class="code-formula">
                        <mathml id="135"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>Ρ</mi><msub><mrow></mrow><mi>r</mi></msub><mo>=</mo><mfrac><mrow><mi>Τ</mi><mi>Ρ</mi></mrow><mrow><mi>Τ</mi><mi>Ρ</mi><mo>+</mo><mi>F</mi><mi>Ρ</mi></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>1</mn><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mi>R</mi><mo>=</mo><mfrac><mrow><mi>Τ</mi><mi>Ρ</mi></mrow><mrow><mi>Τ</mi><mi>Ρ</mi><mo>+</mo><mi>F</mi><mi>Ν</mi></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>2</mn><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mi>F</mi><mn>1</mn><mo>=</mo><mfrac><mrow><mn>2</mn><mo>×</mo><mi>Ρ</mi><msub><mrow></mrow><mi>r</mi></msub><mo>×</mo><mi>R</mi></mrow><mrow><mi>Ρ</mi><msub><mrow></mrow><mi>r</mi></msub><mo>+</mo><mi>R</mi></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>3</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="136">其中,<i>TP</i>、<i>FP</i>、<i>FN</i>由混淆矩阵计算而来,<i>TP</i>表示将正类预测为正类数,<i>FP</i>表示将负类预测为正类数,即误报数,<i>FN</i>表示将正类预测为负类数,即漏报数。表5所示为不同算法在HTRU上的参数计算值。</p>
                </div>
                <div class="area_img" id="137">
                    <p class="img_tit"><b>表5 不同算法在HTRU上的参数计算值</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="137" border="1"><tr><td><br />算法</td><td>精确率</td><td>召回率</td><td><i>F</i>1</td><td>错误数</td></tr><tr><td><br />均值池化CNN算法</td><td>0.958</td><td>0.903</td><td>0.910</td><td>43</td></tr><tr><td><br />最大池化CNN算法</td><td>0.957</td><td>0.864</td><td>0.804</td><td>41</td></tr><tr><td><br />ACGAN算法</td><td>0.979</td><td>0.915</td><td>0.922</td><td>35</td></tr><tr><td><br />CP-ACGAN算法</td><td>0.986</td><td>0.920</td><td>0.929</td><td>33</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="138">从表5可以看出,本文CP-ACGAN算法的各个参数均能显示更优的性能。因此,本文算法可以对正负样本分布不均衡的脉冲星候选体数据集表现出更好的适应性,可用于筛选脉冲星候选体。</p>
                </div>
                <h3 id="139" name="139" class="anchor-tag">4 结束语</h3>
                <div class="p1">
                    <p id="140">本文通过分析ACGAN高清图像的合成原理及其判别器性能,提出一种基于ACGAN的图像分类算法CP-ACGAN。在原始ACGAN算法的基础上,CP-ACGAN算法通过特征匹配以改变判别器的输出层结构,引入softmax分类器并采用半监督学习思想重构生成器和判别器的损失函数,并在判别器中加入池化方法。实验结果表明,与ACGAN算法相比,CP-ACGAN算法的分类效果较好,稳定性较高,且具有较好的扩展性。通过实验结果还可以看出,与ACGAN相比,CP-ACGAN算法虽然分类效果更好,但生成的图像样本却较差,这与池化的使用相关,但并非绝对的关系,因此,下一步将分析造成这种现象的原因并提高CP-ACGAN算法生成图像的样本质量。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image Net Classification with Deep Convolutional Neural Networks">

                                <b>[1]</b> KRIZHEVSKY A,SUTSKEVER I,HINTON G E.ImageNet classification with deep convolutional neural networks[C]//Proceedings of International Conference on Neural Information Processing Systems.[S.l.]:Curran Associates Inc.,2012:1097-1105.
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" >
                                    <b>[2]</b>
                                 HE Kaiming,ZHANG Xiangyu,REN Shaoqing,et al.Deep residual learning for image recognition[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Press,2016:770-778.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=TXXB201802014&amp;v=MjUzMDhaZVJ0Rnkza1ZMekpNVFhUYkxHNEg5bk1yWTlFWUlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2U=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qem1VK3YrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> 王万良,李卓蓉.生成式对抗网络研究进展[J].通信学报,2018,39(2):135-148.
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Generative adversarial nets">

                                <b>[4]</b> GOODFELLOW I J,POUGET-ABADIE J,MIRZA M,et al.Generative adversarial nets[C]//Proceedings of International Conference on Neural Information Processing Systems.Cambridge,USA:MIT Press,2014:2672-2680.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks">

                                <b>[5]</b> RADFORD A,METZ L,CHINTALA S.Unsupervised representation learning with deep convolutional generative adversarial networks[EB/OL].[2018-09-08].https://arxiv.org/pdf/1511.06434.pdf.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Conditional generative adversarial nets">

                                <b>[6]</b> MIRZA M,OSINDERO S.Conditional generative adversarial nets[EB/OL].[2018-09-08].https://arxiv.org/pdf/1411.1784.pdf.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Conditional image synthesis with auxiliary classifier GANs">

                                <b>[7]</b> ODENA A,OLAH C,SHLENS J.Conditional image synthesis with auxiliary classifier GANs[EB/OL].[2018-09-08].https://arxiv.org/pdf/1610.09585.pdf.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Semisupervised learning with deep generative models">

                                <b>[8]</b> KINGMA D P,REZENDE D J,MOHAMED S,et al.Semi-supervised learning with deep generative models[C]//Proceedings of International Conference on Neural Information Processing Systems.Cambridge,USA:MIT Press,2014:3581-3589.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Structured generative adversarial networks">

                                <b>[9]</b> DENG Zhijie,ZHANG Hao,LIANG Xiaodan,et al.Structured generative adversarial networks[C]//Proceedings of International Conference on Neural Information Processing Systems.Cambridge,USA:MIT Press,2017:3899-3909.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Good semi-supervised learning that requires a bad GAN">

                                <b>[10]</b> DAI Zihang,YANG Zhilin,YANG Fan,et al.Good semi-supervised learning that requires a bad GAN[C]//Proceedings of International Conference on Neural Information Processing Systems.Cambridge,USA:MIT Press,2017:1-14.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Semi-supervised learning with generative adversarial networks">

                                <b>[11]</b> ODENA A.Semi-supervised learning with generative adversarial networks[EB/OL].[2018-09-08].https://arxiv.org/pdf/1606.01583.pdf.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_12" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201805009&amp;v=MjYzMTZxQnRHRnJDVVJMT2VaZVJ0Rnkza1ZMekpLQ0xmWWJHNEg5bk1xbzlGYllRS0RIODR2UjRUNmo1NE8zenE=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qem1VK3YrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[12]</b> 唐贤伦,杜一铭,刘雨微,等.基于条件深度卷积生成对抗网络的图像识别方法[J].自动化学报,2018,44(5):855-865.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Improved techniques for training GANs">

                                <b>[13]</b> SALIMANS T,GOODFELLOW I,ZAREMBA W,et al.Improved techniques for training GANs[C]//Proceedings of International Conference on Neural Information Processing Systems.[S.l.]:Curran Associates Inc.,2016:1-10.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=InfoGAN:interpretable representation learning by information maximizing generative adversarial nets">

                                <b>[14]</b> CHEN Xi,DUAN Yan,HOUTHOOFT R,et al.InfoGAN:interpretable representation learning by information maximizing generative adversarial nets[C]//Proceedings of International Conference on Neural Information Processing Systems.[S.l.]:Curran Associates Inc.,2016:1-9.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Adversarial generator-encoder networks">

                                <b>[15]</b> ULYANOV D,VEDALDI A,LEMPITSKY V.Adversarial generator-encoder networks[EB/OL].[2018-09-08].https://arxiv.org/pdf/1704.02304.pdf.
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Evaluation of pooling operations in convolutional architectures for object recognition">

                                <b>[16]</b> SCHERER D,MULLER A,BEHNKE S.Evaluation of pooling operation in convolutional architecture for object recognition[C]//Proceedings of International Conference on Artificial Neural Networks.Berlin,Germany:Springer,2010:92-101.
                            </a>
                        </p>
                        <p id="35">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A theoretical analysis of feature pooling in visual recognition">

                                <b>[17]</b> BOUREAU Y L,PONCE J,LECUN Y.A theoretical analysis of feature pooling in visual recognition[C]//Proceedings of International Conference on Machine Learning.Berlin,Germany:Springer,2010:111-118.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJC201910041" />
        <input id="dpi" type="hidden" value="600" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qem1VK3YrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201910041&amp;v=MDk3MjhIOWpOcjQ5QlpZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSdEZ5M2tWTHpKTHo3QmJiRzQ=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qem1VK3YrND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
