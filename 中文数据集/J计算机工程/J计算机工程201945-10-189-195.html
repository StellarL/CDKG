<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637126202924146250%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJC201910032%26RESULT%3d1%26SIGN%3dBWBU3B%252bL%252bX7a5YBcPAYBVR3y9iE%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201910032&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201910032&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201910032&amp;v=MjAwOTQzaFVydkpMejdCYmJHNEg5ak5yNDlHWm9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJ0Rnk=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZKU3U5a3pnekRzZjlxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#35" data-title="0 概述 ">0 概述</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#39" data-title="1 多核学习相关理论 ">1 多核学习相关理论</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#40" data-title="1.1 相关符号和概念">1.1 相关符号和概念</a></li>
                                                <li><a href="#46" data-title="1.2 正则化多核学习框架">1.2 正则化多核学习框架</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#55" data-title="2 算法设计 ">2 算法设计</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#56" data-title="2.1 弹性网型正则化">2.1 弹性网型正则化</a></li>
                                                <li><a href="#83" data-title="2.2 AdaBoost-EMKL算法">2.2 AdaBoost-EMKL算法</a></li>
                                                <li><a href="#108" data-title="2.3 时间复杂度">2.3 时间复杂度</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#110" data-title="3 实验结果与分析 ">3 实验结果与分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#111" data-title="3.1 实验数据与环境">3.1 实验数据与环境</a></li>
                                                <li><a href="#114" data-title="3.2 实验参数设置">3.2 实验参数设置</a></li>
                                                <li><a href="#116" data-title="3.3 性能比较">3.3 性能比较</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#143" data-title="4 结束语 ">4 结束语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#113" data-title="&lt;b&gt;表1 数据集信息&lt;/b&gt;"><b>表1 数据集信息</b></a></li>
                                                <li><a href="#119" data-title="&lt;b&gt;图1 6种算法的分类精度与训练样本数之间的关系&lt;/b&gt;"><b>图1 6种算法的分类精度与训练样本数之间的关系</b></a></li>
                                                <li><a href="#121" data-title="&lt;b&gt;表2 6种算法的分类精度与标准差比较&lt;/b&gt;"><b>表2 6种算法的分类精度与标准差比较</b></a></li>
                                                <li><a href="#130" data-title="&lt;b&gt;图2 本文算法的收敛性比较&lt;/b&gt;"><b>图2 本文算法的收敛性比较</b></a></li>
                                                <li><a href="#134" data-title="&lt;b&gt;表3 不同范数正则约束情况下Ionosphere数据集的测试结果&lt;/b&gt;"><b>表3 不同范数正则约束情况下Ionosphere数据集的测试结果</b></a></li>
                                                <li><a href="#135" data-title="&lt;b&gt;表4 不同范数正则约束情况下Segment数据集的测试结果&lt;/b&gt;"><b>表4 不同范数正则约束情况下Segment数据集的测试结果</b></a></li>
                                                <li><a href="#136" data-title="&lt;b&gt;表5 不同范数正则约束情况下Wdbc数据集的测试结果&lt;/b&gt;"><b>表5 不同范数正则约束情况下Wdbc数据集的测试结果</b></a></li>
                                                <li><a href="#139" data-title="&lt;b&gt;图3 Adaboost-EMKL算法精度与&lt;/b&gt;&lt;i&gt;&lt;b&gt;v&lt;/b&gt;&lt;/i&gt;&lt;b&gt;取值的关系&lt;/b&gt;"><b>图3 Adaboost-EMKL算法精度与</b><i><b>v</b></i><b>取值的关系</b></a></li>
                                                <li><a href="#142" data-title="&lt;b&gt;图4 Adaboost-EMKL算法中&lt;/b&gt;&lt;i&gt;&lt;b&gt;v&lt;/b&gt;&lt;/i&gt;&lt;b&gt;取值与选择核数量的关系&lt;/b&gt;"><b>图4 Adaboost-EMKL算法中</b><i><b>v</b></i><b>取值与选择核数量的关系</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="5">


                                    <a id="bibliography_1" title=" CHAPELLE O,VAPNIK V,BOUSQUET O,et al.Choosing multiple parameters for support vector machines[J].Machine Learning,2002,46(1/2/3):131-159." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00001340333&amp;v=MDkwNjk3cWVidWR0RkN2bFZydkpKRjg9Tmo3QmFyTzRIdEhOckl0RlorZ01ZM2s1ekJkaDRqOTlTWHFScnhveGNNSDdS&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZKU3U5a3pnekRzZjlxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         CHAPELLE O,VAPNIK V,BOUSQUET O,et al.Choosing multiple parameters for support vector machines[J].Machine Learning,2002,46(1/2/3):131-159.
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_2" title=" KLOFT M,BREFELD U,SONNENBURG S.Efficient and accurate Lp-norm multiple kernel learning[C]//Prcoeedings of Advances in Neural Information Processing Systems.[S.1.]:NIPS Foundation,Inc.,2009:997-1005." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Efficient and accurate lp-normmultiple kernel learning">
                                        <b>[2]</b>
                                         KLOFT M,BREFELD U,SONNENBURG S.Efficient and accurate Lp-norm multiple kernel learning[C]//Prcoeedings of Advances in Neural Information Processing Systems.[S.1.]:NIPS Foundation,Inc.,2009:997-1005.
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_3" title=" SUN Tao,JIAO Licheng,LIU Fang,et al.Selective multiple kernel learning for classification with ensemble strategy[J].Pattern Recognition,2013,46(11):3081-3090." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13080200145820&amp;v=MjAxNDRaZThLQkg0NW9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVUx6SUpWb1ZiaE09TmlmT2ZiSzdIdG5Nclk5Rg==&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZKU3U5a3pnekRzZjlxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         SUN Tao,JIAO Licheng,LIU Fang,et al.Selective multiple kernel learning for classification with ensemble strategy[J].Pattern Recognition,2013,46(11):3081-3090.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_4" title=" YANG Haiqin,XU Zenglin,YE Jieping,et al.Efficient sparse generalized multiple kernel learning[J].IEEE Transactions on Neural Networks,2011,22(3):433-446." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Efficient Sparse Generalized Multiple Kernel Learning">
                                        <b>[4]</b>
                                         YANG Haiqin,XU Zenglin,YE Jieping,et al.Efficient sparse generalized multiple kernel learning[J].IEEE Transactions on Neural Networks,2011,22(3):433-446.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_5" title=" KLOFT M,BREFELD U,SONNENBURG S,et al.Non-sparse regularization and efficient training with multiple kernels[EB/OL].[2018-03-01].https://arxiv.org/abs/1003.0079." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Non-sparse regularization and efficient training with multiple kernels">
                                        <b>[5]</b>
                                         KLOFT M,BREFELD U,SONNENBURG S,et al.Non-sparse regularization and efficient training with multiple kernels[EB/OL].[2018-03-01].https://arxiv.org/abs/1003.0079.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_6" title=" XIA Hao,HOI S C H.MKBoost:a framework of multiple kernel boosting[J].IEEE Transactions on Knowledge and Data Engineering,2013,25(7):1574-1586." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Hoi MKBoost:A Framework of Multiple Kernel Boosting">
                                        <b>[6]</b>
                                         XIA Hao,HOI S C H.MKBoost:a framework of multiple kernel boosting[J].IEEE Transactions on Knowledge and Data Engineering,2013,25(7):1574-1586.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_7" title=" CITI L.Elastic-net constrained multiple kernel learning using a majorization-minimization approach[C]//Proceedings of Computer Science and Electronic Engineering Conference.Washington D.C.,USA:IEEE Press,2015:29-34." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Elastic-net constrained multiple kernel learning using a majorization-minimization approach">
                                        <b>[7]</b>
                                         CITI L.Elastic-net constrained multiple kernel learning using a majorization-minimization approach[C]//Proceedings of Computer Science and Electronic Engineering Conference.Washington D.C.,USA:IEEE Press,2015:29-34.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_8" title=" 武征鹏,张学工.弹性多核学习[J].自动化学报,2011,37(6):693-699." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201106007&amp;v=MDc3ODJDVVJMT2VaZVJ0RnkzaFVydkpLQ0xmWWJHNEg5RE1xWTlGWTRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnI=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZKU3U5a3pnekRzZjlxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                         武征鹏,张学工.弹性多核学习[J].自动化学报,2011,37(6):693-699.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_9" title=" 王庆超,付光远,汪洪桥,等.基于局部空间变稀疏约束的多核学习方法[J].电子学报,2018,46(4):930-937." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZXU201804022&amp;v=MDI5OTZVcnZKSVRmVGU3RzRIOW5NcTQ5SFpvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSdEZ5M2g=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZKU3U5a3pnekRzZjlxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                         王庆超,付光远,汪洪桥,等.基于局部空间变稀疏约束的多核学习方法[J].电子学报,2018,46(4):930-937.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_10" title=" 何佳佳,陈秀宏,田进,等.基于牛顿梯度优化的弹性多核学习[J].传感器与微系统,2018(2):136-139." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=CGQJ201802038&amp;v=MzE2NTMzenFxQnRHRnJDVVJMT2VaZVJ0RnkzaFVydkpKaXJhWkxHNEg5bk1yWTlHYklRS0RIODR2UjRUNmo1NE8=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZKU3U5a3pnekRzZjlxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[10]</b>
                                         何佳佳,陈秀宏,田进,等.基于牛顿梯度优化的弹性多核学习[J].传感器与微系统,2018(2):136-139.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_11" title=" 张仁峰,吴小俊,陈素根.通用稀疏多核学习[J].计算机应用研究,2016,33(1):21-27." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSYJ201601006&amp;v=MjM3Mjg2ajU0TzN6cXFCdEdGckNVUkxPZVplUnRGeTNoVXJ2Skx6N1NaTEc0SDlmTXJvOUZZb1FLREg4NHZSNFQ=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZKU3U5a3pnekRzZjlxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[11]</b>
                                         张仁峰,吴小俊,陈素根.通用稀疏多核学习[J].计算机应用研究,2016,33(1):21-27.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_12" title=" 梁军,李世浩,张飞云,等.基于半无限规划的弹性多核学习算法[J].华中科技大学学报(自然科学版),2015,43(8):103-106." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=HZLG201508022&amp;v=MjM5NzMzenFxQnRHRnJDVVJMT2VaZVJ0RnkzaFVydkpMVGZIYWJHNEg5VE1wNDlIWm9RS0RIODR2UjRUNmo1NE8=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZKU3U5a3pnekRzZjlxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[12]</b>
                                         梁军,李世浩,张飞云,等.基于半无限规划的弹性多核学习算法[J].华中科技大学学报(自然科学版),2015,43(8):103-106.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_13" title=" 曹莹,苗启广,刘家辰,等.AdaBoost算法研究进展与展望[J].自动化学报,2013,39(6):745-758." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201306008&amp;v=MDI2NzdCdEdGckNVUkxPZVplUnRGeTNoVXJ2SktDTGZZYkc0SDlMTXFZOUZiSVFLREg4NHZSNFQ2ajU0TzN6cXE=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZKU3U5a3pnekRzZjlxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[13]</b>
                                         曹莹,苗启广,刘家辰,等.AdaBoost算法研究进展与展望[J].自动化学报,2013,39(6):745-758.
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_14" title=" SCHOLKOPF B,SMOLA A J.Learning with kernels:support vector machines,regularization,optimization,and beyond[M].Cambridge,USA:MIT Press,2001." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning with kernels:support vector machines,regularization,optimization,and beyond">
                                        <b>[14]</b>
                                         SCHOLKOPF B,SMOLA A J.Learning with kernels:support vector machines,regularization,optimization,and beyond[M].Cambridge,USA:MIT Press,2001.
                                    </a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_15" title=" RAKOTOMAMONJY A,BACH F R,CANU S,et al.SimpleMKL[J].Journal of Machine Learning Research,2008,9(3):2491-2521." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=SimpleMKL">
                                        <b>[15]</b>
                                         RAKOTOMAMONJY A,BACH F R,CANU S,et al.SimpleMKL[J].Journal of Machine Learning Research,2008,9(3):2491-2521.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJC" target="_blank">计算机工程</a>
                2019,45(10),189-195 DOI:10.19678/j.issn.1000-3428.0050909            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于AdaBoost的弹性网型正则化多核学习算法</b></span>
 <span class="shoufa"></span>                                     </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E4%BB%BB%E8%83%9C%E5%85%B5&amp;code=10157281&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZKU3U5a3pnekRzZjlxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">任胜兵</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%B0%A2%E5%A6%82%E8%89%AF&amp;code=39553211&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZKU3U5a3pnekRzZjlxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">谢如良</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%8D%97%E5%A4%A7%E5%AD%A6%E8%BD%AF%E4%BB%B6%E5%AD%A6%E9%99%A2&amp;code=0193746&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZKU3U5a3pnekRzZjlxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中南大学软件学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>在正则化多核学习中,稀疏的核函数权值会导致有用信息丢失和泛化性能退化,而通过非稀疏模型选取所有核函数则会产生较多的冗余信息并对噪声敏感。针对上述问题,基于AdaBoost框架提出一种弹性网型正则化多核学习算法。在迭代选取基本分类器时对核函数的权值进行弹性网型正则化约束,即混合<i>L</i><sub>1</sub>范数和<i>L</i><sub><i>p</i></sub>范数约束,构造基于多个基本核最优凸组合的基本分类器,并将其集成到最终的强分类器中。实验结果表明,该算法在保留集成算法优势的同时,能够实现核函数权值稀疏性和非稀疏性的平衡,与<i>L</i><sub>1</sub>-MKL和<i>L</i><sub><i>p</i></sub>-MKL算法相比,能够以较少的迭代次数获得分类精度较高的分类器。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZKU3U5a3pnekRzZjlxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">集成学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%A4%9A%E6%A0%B8%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZKU3U5a3pnekRzZjlxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">多核学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%BC%B9%E6%80%A7%E7%BD%91%E5%9E%8B%E6%AD%A3%E5%88%99%E5%8C%96&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZKU3U5a3pnekRzZjlxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">弹性网型正则化;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%BC%B1%E5%88%86%E7%B1%BB%E5%99%A8&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZKU3U5a3pnekRzZjlxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">弱分类器;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%A8%80%E7%96%8F%E6%80%A7&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZKU3U5a3pnekRzZjlxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">稀疏性;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    任胜兵(1969—),男,副教授、博士,主研方向为模式识别、图像处理、可信软件;E-mail:rsb@csu.edu.cn;
                                </span>
                                <span>
                                    谢如良,硕士研究生。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-03-22</p>

                    <p>

                            <b>基金：</b>
                                                        <span>中南大学研究生自主探索创新项目(1053320170432);</span>
                    </p>
            </div>
                    <h1><b>Elastic-net Regularization Multi Kernel Learning Algorithm Based on AdaBoost</b></h1>
                    <h2>
                    <span>REN Shengbing</span>
                    <span>XIE Ruliang</span>
            </h2>
                    <h2>
                    <span>School of Software,Central South University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>In regularization multi kernel learning,the sparse kernel function weight leads to the loss of useful information and the degradation of generalization performance,while selecting all kernel functions through non-sparse models generates more redundant information and is sensitivity to noise.Aiming at these problems,an elastic-net regularization multi kernel learning algorithm based on AdaBoost architecture is proposed.When the basic classifier is selected at each iteration,the weight of the kernel function is added with the elastic-net regularization,that is,mixed <i>L</i><sub>1</sub> norm and <i>L</i><sub><i>p</i></sub> norm constraints.The basic classifier are constructed based on multi basic kernel optimal convex combinations,which are integrated into the final strong classifier.Experimental results show that the proposed algorithm can balance the sparsity and non-sparsity of the weight in kernel function while preserving the advantages of the integrated algorithm.Compared with <i>L</i><sub>1</sub>-MKL and <i>L</i><sub><i>p</i></sub>-MKL algorithms,it can obtain the classifier with higher classification accuracy in fewer iterations.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=ensemble%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZKU3U5a3pnekRzZjlxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">ensemble learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=multi%20kernel%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZKU3U5a3pnekRzZjlxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">multi kernel learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=elastic-net%20regularization&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZKU3U5a3pnekRzZjlxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">elastic-net regularization;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=weak%20classifier&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZKU3U5a3pnekRzZjlxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">weak classifier;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=sparsity&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZKU3U5a3pnekRzZjlxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">sparsity;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-03-22</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="35" name="35" class="anchor-tag">0 概述</h3>
                <div class="p1">
                    <p id="36">目前多核学习<citation id="172" type="reference"><link href="5" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>已成为机器学习领域的热门研究方向,各种基于支持向量机(Support Vector Machine,SVM)的多核学习方法相继出现。文献<citation id="173" type="reference">[<a class="sup">2</a>]</citation>在核权值<i>L</i><sub>2</sub>范数约束的基础上,提出<i>L</i><sub><i>p</i></sub>(<i>p</i>&gt;1)范数约束的多核学习(<i>L</i><sub><i>p</i></sub>-norm Multiple Kernel Learning,<i>L</i><sub><i>p</i></sub>-MKL)方法。文献<citation id="174" type="reference">[<a class="sup">3</a>]</citation>提出基于集成策略的多核选择方法,优化了核函数的选择过程。在上述基于正则化的多核学习中,当采用<i>L</i><sub>1</sub>范数约束核权值时,得到的核权值是稀疏解,会导致信息丢失和模型泛化能力减弱<citation id="175" type="reference"><link href="11" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>;而采用<i>L</i><sub><i>p</i></sub>(<i>p</i>&gt;1)范数约束时,得到的核权值为非稀疏解,核函数将被全部选择,导致模型对噪声敏感,会出现冗余信息并且降低可解释性<citation id="176" type="reference"><link href="13" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>,同时也会增加时间和空间复杂度。文献<citation id="177" type="reference">[<a class="sup">6</a>]</citation>提出MKBoost-D2方法。该方法每次迭代得到的最优基本分类器为所有核函数的加权组合,其中也包含性能较差的核函数,因此与采用<i>L</i><sub><i>p</i></sub>范数约束存在同样的问题。</p>
                </div>
                <div class="p1">
                    <p id="37">文献<citation id="180" type="reference">[<a class="sup">7</a>,<a class="sup">8</a>]</citation>提出采用弹性网络对核函数权值进行约束的多核学习方法,文献<citation id="178" type="reference">[<a class="sup">9</a>]</citation>采用一种变稀疏的方式选取核权值。上述方法基于弹性正则化选择核权值,平衡了核权值的稀疏性与非稀疏性,提高了分类精度。文献<citation id="181" type="reference">[<a class="sup">10</a>,<a class="sup">11</a>]</citation>对弹性多核学习算法进行优化,文献<citation id="179" type="reference">[<a class="sup">12</a>]</citation>提出基于半无限规划的弹性多核学习算法。但上述弹性正则化多核学习没有对最终的分类器进行加权集成,分类性能不稳定。</p>
                </div>
                <div class="p1">
                    <p id="38">本文基于AdaBoost<citation id="182" type="reference"><link href="29" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>框架提出一种弹性网型正则化多核学习算法(Elastic-net Regularization Multi Kernel Learning Algorithm Based on AdaBoost,AdaBoost-EMKL)。在获取基本分类器时,采用弹性网类型正则化进行约束,即对核权值同时加人<i>L</i><sub>1</sub>和<i>L</i><sub><i>p</i></sub>范数约束,以保留性能优异的核函数和大量有用的特征,通过自适应多核学习动态平衡权值的稀疏性和非稀疏性。</p>
                </div>
                <h3 id="39" name="39" class="anchor-tag">1 多核学习相关理论</h3>
                <h4 class="anchor-tag" id="40" name="40">1.1 相关符号和概念</h4>
                <div class="p1">
                    <p id="41">假设数据集为<mathml id="145"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>D</mi><mo>=</mo><mo stretchy="false">{</mo><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mrow><mo>|</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mo>⋯</mo><mo>,</mo><mi>m</mi></mrow></mrow><mo stretchy="false">}</mo></mrow></math></mathml>,<i><b>x</b></i><sub><i>i</i></sub>∈<image href="images/JSJC201910032_146.jpg" type="" display="inline" placement="inline"><alt></alt></image><sup><i>d</i></sup>。如果学习算法求解的是二分类问题,则<i>y</i><sub><i>i</i></sub>=±1;如果学习算法求解的是回归问题,则<i>y</i><sub><i>i</i></sub>∈<image href="images/JSJC201910032_147.jpg" type="" display="inline" placement="inline"><alt></alt></image>。机器学习的目标是找到一个函数<i>f</i>,该函数能够对未知数据进行准确预测。设<i>f</i>是通过最小正则化风险得到的函数,如式(1)所示。</p>
                </div>
                <div class="p1">
                    <p id="42" class="code-formula">
                        <mathml id="42"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>f</mi><msup><mrow></mrow><mo>*</mo></msup><mo>=</mo><mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mtext>a</mtext><mtext>r</mtext><mtext>g</mtext><mtext>m</mtext><mtext>i</mtext><mtext>n</mtext></mrow></mstyle><mi>f</mi></munder></mrow><mtext> </mtext><mi>C</mi><mi>R</mi><msub><mrow></mrow><mrow><mtext>e</mtext><mtext>m</mtext><mtext>p</mtext></mrow></msub><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">)</mo><mo>+</mo><mi>Ω</mi><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="43">其中,<mathml id="148"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>R</mi><msub><mrow></mrow><mrow><mtext>e</mtext><mtext>m</mtext><mtext>p</mtext></mrow></msub><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><mi>n</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>G</mi></mstyle><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo>,</mo><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></math></mathml>为f关于损失的经验风险,Ω:<i><b>H</b></i>→<image href="images/JSJC201910032_149.jpg" type="" display="inline" placement="inline"><alt></alt></image>为正则化项,正参数<i>C</i>用于平衡正则化项和经验风险项。本文使用的正则化项为<mathml id="150"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>Ω</mi><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">)</mo><mo>=</mo></mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mrow><mo>|</mo><mi mathvariant="bold-italic">w</mi><mo>|</mo></mrow><msubsup><mrow></mrow><mi>p</mi><mi>p</mi></msubsup></mrow></math></mathml>,函数<i>f</i>采用线性模型<citation id="183" type="reference"><link href="31" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>,如式(2)所示。</p>
                </div>
                <div class="p1">
                    <p id="44"><i>f</i><sub><i>w</i></sub><sub>,</sub><sub><i>b</i></sub>(<i><b>x</b></i>)=〈<i><b>w</b></i>,ϕ(<i><b>x</b></i>)〉+<i>b</i>      (2)</p>
                </div>
                <div class="p1">
                    <p id="45">其中,ϕ(<i><b>x</b></i>):<i><b>X</b></i>→<i><b>H</b></i>为样本从输入空间<image href="images/JSJC201910032_151.jpg" type="" display="inline" placement="inline"><alt></alt></image><sup><i>n</i></sup>到特征空间<i><b>H</b></i>(希尔伯特空间)的映射,其完成映射过程的核函数<i>K</i>(<i>x</i>,<i>z</i>)=ϕ(<i>x</i>)ϕ(<i>z</i>)并不需要定义映射函数ϕ的具体形式。因此,在输入空间<image href="images/JSJC201910032_152.jpg" type="" display="inline" placement="inline"><alt></alt></image><sup><i>n</i></sup>中的的超曲面模型对应于特征空间<i><b>H</b></i>中超平面模型,则原空间中的非线性可分问题转变成新空间中的线性可分问题。</p>
                </div>
                <h4 class="anchor-tag" id="46" name="46">1.2 正则化多核学习框架</h4>
                <div class="p1">
                    <p id="47">在多核学习框架中,假设最优核<i>K</i>是基核{<i>K</i><sub>1</sub>,<i>K</i><sub>2</sub>,…,<i>K</i><sub><i>M</i></sub>}的线性组合,其核权值为{<i>θ</i><sub>1</sub>,<i>θ</i><sub>2</sub>,…,<i>θ</i><sub><i>M</i></sub>},通常可以通过正则化风险最小来求解最优核组合,如式(3)和式(4)所示。</p>
                </div>
                <div class="p1">
                    <p id="48" class="code-formula">
                        <mathml id="48"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mrow><mi mathvariant="bold-italic">w</mi><mo>,</mo><mi>b</mi><mo>,</mo><mi mathvariant="bold-italic">θ</mi><mo>≥</mo><mn>0</mn></mrow></munder><mi>C</mi><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>G</mi></mstyle><mo stretchy="false">(</mo><mi>f</mi><msub><mrow></mrow><mrow><mi mathvariant="bold-italic">w</mi><mo>,</mo><mi>b</mi><mo>,</mo><mi mathvariant="bold-italic">θ</mi></mrow></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi mathvariant="bold-italic">i</mi></msub><mo stretchy="false">)</mo><mo>,</mo><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo>+</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mrow><mo>|</mo><mi mathvariant="bold-italic">w</mi><mo>|</mo></mrow><msubsup><mrow></mrow><mi>p</mi><mi>p</mi></msubsup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>3</mn><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mtext>s</mtext><mo>.</mo><mtext>t</mtext><mo>.</mo><mtext>ϕ</mtext><mo stretchy="false">(</mo><mi mathvariant="bold-italic">θ</mi><mo stretchy="false">)</mo><mo>≤</mo><mn>1</mn><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>4</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="49">其中,<i>G</i>:<image href="images/JSJC201910032_153.jpg" type="" display="inline" placement="inline"><alt></alt></image>×<i>y</i><sub><i>i</i></sub>→<image href="images/JSJC201910032_154.jpg" type="" display="inline" placement="inline"><alt></alt></image>为损失函数,<mathml id="155"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mrow><mo>|</mo><mi mathvariant="bold-italic">w</mi><mo>|</mo></mrow><msubsup><mrow></mrow><mi>p</mi><mi>p</mi></msubsup></mrow></math></mathml>为正则化项,ϕ(<i>θ</i>)为核权值<i>θ</i>的正则化定义。由于式(3)为非凸函数,因此根据文献<citation id="184" type="reference">[<a class="sup">2</a>]</citation>进行变换可得到式(5)。</p>
                </div>
                <div class="p1">
                    <p id="50" class="code-formula">
                        <mathml id="50"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mrow><mi mathvariant="bold-italic">w</mi><mo>,</mo><mi>b</mi><mo>,</mo><mi mathvariant="bold-italic">θ</mi><mo>≥</mo><mn>0</mn></mrow></munder><mi>C</mi><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>G</mi></mstyle><mo stretchy="false">(</mo><mi>f</mi><msub><mrow></mrow><mrow><mi mathvariant="bold-italic">w</mi><mo>,</mo><mi>b</mi><mo>,</mo><mi mathvariant="bold-italic">θ</mi></mrow></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi mathvariant="bold-italic">i</mi></msub><mo stretchy="false">)</mo><mo>,</mo><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo>+</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>m</mi><mo>=</mo><mn>1</mn></mrow><mi>Μ</mi></munderover><mrow><mfrac><mrow><mrow><mo>|</mo><mi mathvariant="bold-italic">w</mi><mo>|</mo></mrow><msubsup><mrow></mrow><mi>p</mi><mi>p</mi></msubsup></mrow><mrow><mi>θ</mi><msub><mrow></mrow><mi>m</mi></msub></mrow></mfrac></mrow></mstyle><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>5</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="51">如果式(5)中的损失函数<i>G</i>与式(4)都为凸函数,则式(5)也为凸函数,因此,求解的最优值为全局最优值。本文分类问题中采用的损失函数为合页损失函数<i>L</i>[<i>y</i>(<i><b>w</b></i><sup>T</sup>·<i><b>x</b></i>+<i>b</i>)]=[1-<i>y</i>(<i><b>w</b></i><sup>T</sup>·<i><b>x</b></i>+<i>b</i>)]<sub>+</sub>,同时在公式中引入松弛变量<i>ξ</i><sub><i>i</i></sub>,则式(5)可改写成式(6)。</p>
                </div>
                <div class="p1">
                    <p id="52" class="code-formula">
                        <mathml id="52"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mrow><mi mathvariant="bold-italic">w</mi><mo>,</mo><mi>b</mi><mo>,</mo><mi mathvariant="bold-italic">θ</mi><mo>,</mo><mi mathvariant="bold-italic">ξ</mi></mrow></munder><mi>C</mi><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>ξ</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mo>+</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>m</mi><mo>=</mo><mn>1</mn></mrow><mi>Μ</mi></munderover><mrow><mfrac><mrow><mrow><mo>|</mo><mi mathvariant="bold-italic">w</mi><mo>|</mo></mrow><msubsup><mrow></mrow><mi>p</mi><mi>p</mi></msubsup></mrow><mrow><mi>θ</mi><msub><mrow></mrow><mi>m</mi></msub></mrow></mfrac></mrow></mstyle></mtd></mtr><mtr><mtd><mtext>s</mtext><mo>.</mo><mtext>t</mtext><mo>.</mo><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">(</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>m</mi><mo>=</mo><mn>1</mn></mrow><mi>Μ</mi></munderover><mi mathvariant="bold-italic">w</mi></mstyle><msubsup><mrow></mrow><mi>m</mi><mtext>Τ</mtext></msubsup><mi>φ</mi><msub><mrow></mrow><mi>m</mi></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo>+</mo><mi>b</mi><mo stretchy="false">)</mo><mo>≥</mo><mn>1</mn><mo>-</mo><mi>ξ</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo></mtd></mtr><mtr><mtd><mi>ξ</mi><msub><mrow></mrow><mi>i</mi></msub><mo>≥</mo><mn>0</mn><mo>,</mo><mtext>ϕ</mtext><mo stretchy="false">(</mo><mi mathvariant="bold-italic">θ</mi><mo stretchy="false">)</mo><mo>≤</mo><mn>1</mn><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>6</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="53">在多核学习过程中,如果在选择核权值时采用<i>L</i><sub>1</sub>范数进行约束,即在式(6)中<mathml id="156"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtext>ϕ</mtext><mo stretchy="false">(</mo><mi mathvariant="bold-italic">θ</mi><mo stretchy="false">)</mo><mo>=</mo><mrow><mo>|</mo><mi mathvariant="bold-italic">θ</mi><mo>|</mo></mrow><msub><mrow></mrow><mn>1</mn></msub></mrow></math></mathml>,则该方法被称为<i>L</i><sub>1</sub>-MKL。目前有大量关于<i>L</i><sub>1</sub>-MKL加速的算法,例如半定规划(Semi-Definite Programming,SDP)、半无限线性规划(Semi-Infinite Linear Programming,SILP)以及梯度下降等方法都可以减少求取最优核权值的时间复杂度,但是使用<i>L</i><sub>1</sub>-MKL可能会丢失信息,导致模型的泛化性能下降。</p>
                </div>
                <div class="p1">
                    <p id="54">如果在选择核权值时采用<i>L</i><sub><i>p</i></sub>范数进行约束,即在式(6)中<mathml id="157"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtext>ϕ</mtext><mo stretchy="false">(</mo><mi mathvariant="bold-italic">θ</mi><mo stretchy="false">)</mo><mo>=</mo><mrow><mo>|</mo><mi mathvariant="bold-italic">θ</mi><mo>|</mo></mrow><msubsup><mrow></mrow><mi>p</mi><mi>p</mi></msubsup></mrow></math></mathml>,则该方法被称为<i>L</i><sub><i>p</i></sub>-MKL。在通用性上<i>L</i><sub><i>p</i></sub>-MKL较<i>L</i><sub>1</sub>-MKL有明显提高。但是<i>L</i><sub><i>p</i></sub>-MKL在核权值的选择上会产生非稀疏解,即所有的核都会参与到模型中,而非稀疏模型可解释性较差,并且对噪声敏感,同时也会产生冗余信息。因此,本文在选择核权值时,采用的是混合<i>L</i><sub>1</sub>范数和<i>L</i><sub><i>p</i></sub>范数进行约束,即弹性正则化约束。基于弹性正则化约束的核权值选择是通过调节参数<i>v</i>平衡其稀疏性与非稀疏性,该方法能够增强模型的泛化能力,同时也能降低其经验风险,即降低模型对噪声的敏感性,使模型更具鲁棒性。</p>
                </div>
                <h3 id="55" name="55" class="anchor-tag">2 算法设计</h3>
                <h4 class="anchor-tag" id="56" name="56">2.1 弹性网型正则化</h4>
                <div class="p1">
                    <p id="57">针对传统多核学习中核函数权值稀疏性或非稀疏性的问题,本文在选择基本分类器时,基于<i>L</i><sub>1</sub>范数和<i>L</i><sub><i>p</i></sub> 范数(<i>p</i>&gt;1)混合网型正则化对基本分类器的核函数进行约束选择,以平衡核权值的稀疏性与非稀疏性,减少因稀疏性造成模型泛化能力退化与信息丢失的情况,避免非稀疏模型的解释性差、噪声敏感以及产生冗余信息的缺点。优化目标如式(7)和式(8)所示。</p>
                </div>
                <div class="p1">
                    <p id="58" class="code-formula">
                        <mathml id="58"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mrow><mi mathvariant="bold-italic">θ</mi><mo>∈</mo><mi>Θ</mi><mo>,</mo><mi mathvariant="bold-italic">w</mi><mo>,</mo><mi>b</mi></mrow></munder><mi>C</mi><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>G</mi></mstyle><mo stretchy="false">(</mo><mi>f</mi><msub><mrow></mrow><mrow><mi mathvariant="bold-italic">w</mi><mo>,</mo><mi>b</mi><mo>,</mo><mi mathvariant="bold-italic">θ</mi></mrow></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo>,</mo><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo>+</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>m</mi><mo>=</mo><mn>1</mn></mrow><mi>Μ</mi></munderover><mrow><mfrac><mrow><mrow><mo>|</mo><mi mathvariant="bold-italic">w</mi><mo>|</mo></mrow><msubsup><mrow></mrow><mi>p</mi><mi>p</mi></msubsup></mrow><mrow><mi>θ</mi><msub><mrow></mrow><mi>m</mi></msub></mrow></mfrac></mrow></mstyle><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>7</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="59"><i>Θ</i>={<i>θ</i>∈<image href="images/JSJC201910032_158.jpg" type="" display="inline" placement="inline"><alt></alt></image><sup><i>m</i></sup><sub>+</sub>:<mathml id="159"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false">(</mo><mi>v</mi><mrow><mo>|</mo><mi mathvariant="bold-italic">θ</mi><mo>|</mo></mrow><msub><mrow></mrow><mn>1</mn></msub><mo>+</mo><mo stretchy="false">(</mo><mn>1</mn><mo>-</mo><mi>v</mi><mo stretchy="false">)</mo><mrow><mo>|</mo><mi mathvariant="bold-italic">θ</mi><mo>|</mo></mrow><msubsup><mrow></mrow><mi>p</mi><mi>p</mi></msubsup><mo stretchy="false">)</mo><mo>≤</mo><mn>1</mn><mo stretchy="false">}</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>8</mn><mo stretchy="false">)</mo></mrow></math></mathml></p>
                </div>
                <div class="p1">
                    <p id="60">其中,参数<i>v</i>(0≤<i>v</i>≤1)的主要作用是平衡2项约束,即平衡核权值的稀疏性与非稀疏性。当<i>v</i>=0时,约束简化为对核权值的<i>L</i><sub><i>p</i></sub>范数约束,即为<i>L</i><sub><i>p</i></sub> -MKL方法;当<i>v</i>=1时,约束简化为<i>L</i><sub>1</sub>范数约束,即为<i>L</i><sub>1</sub>-MKL方法。当<i>v</i>∈(0,1)时,式(8)关于<i>θ</i>是严格的凸函数,并且同时具有<i>L</i><sub>1</sub>范数约束与<i>L</i><sub><i>p</i></sub>范数约束的相关特性。当<i>v</i>的值较大时,<mathml id="160"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>|</mo><mi mathvariant="bold-italic">θ</mi><mo>|</mo></mrow><msub><mrow></mrow><mn>1</mn></msub></mrow></math></mathml>占比更大,因此核权值更趋向于稀疏解;反之,会降低核权值的稀疏性。关于优化目标式(7),如果使用凸损失函数,则优化目标为凸优化问题,求得的最优值为全局最优值。与式(6)的处理方式相同,如果采用合页损失函数,则优化目标式(7)的原始问题等价于式(9)。</p>
                </div>
                <div class="p1">
                    <p id="61" class="code-formula">
                        <mathml id="61"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mrow><mi mathvariant="bold-italic">θ</mi><mo>∈</mo><mi>Θ</mi><mo>,</mo><mi mathvariant="bold-italic">w</mi><mo>,</mo><mi>b</mi></mrow></munder><mi>C</mi><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>ξ</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mo>+</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>m</mi><mo>=</mo><mn>1</mn></mrow><mi>Μ</mi></munderover><mrow><mfrac><mrow><mrow><mo>|</mo><mi mathvariant="bold-italic">w</mi><mo>|</mo></mrow><msubsup><mrow></mrow><mi>p</mi><mi>p</mi></msubsup></mrow><mrow><mi>θ</mi><msub><mrow></mrow><mi>m</mi></msub></mrow></mfrac></mrow></mstyle></mtd></mtr><mtr><mtd><mtext>s</mtext><mo>.</mo><mtext>t</mtext><mo>.</mo><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">(</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>m</mi><mo>=</mo><mn>1</mn></mrow><mi>Μ</mi></munderover><mi mathvariant="bold-italic">w</mi></mstyle><msubsup><mrow></mrow><mi>m</mi><mtext>Τ</mtext></msubsup><mi>φ</mi><msub><mrow></mrow><mi>m</mi></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo>+</mo><mi>b</mi><mo stretchy="false">)</mo><mo>≥</mo><mn>1</mn><mo>-</mo><mi>ξ</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mi>ξ</mi><msub><mrow></mrow><mi>i</mi></msub><mo>≥</mo><mn>0</mn><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>9</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="62">采用拉格朗日乘子法,可得到L(<i><b>w</b></i>,<i>b</i>,<i>ξ</i>,<i>α</i>,<i>β</i>)计算公式如下:</p>
                </div>
                <div class="p1">
                    <p id="63" class="code-formula">
                        <mathml id="63"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>L</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">w</mi><mo>,</mo><mi>b</mi><mo>,</mo><mi mathvariant="bold-italic">ξ</mi><mo>,</mo><mi mathvariant="bold-italic">α</mi><mo>,</mo><mi mathvariant="bold-italic">β</mi><mo stretchy="false">)</mo><mo>=</mo><mi>C</mi><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>ξ</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mo>+</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>m</mi><mo>=</mo><mn>1</mn></mrow><mi>Μ</mi></munderover><mrow><mfrac><mrow><mrow><mo>|</mo><mi mathvariant="bold-italic">w</mi><mo>|</mo></mrow><msubsup><mrow></mrow><mi>p</mi><mi>p</mi></msubsup></mrow><mrow><mi>θ</mi><msub><mrow></mrow><mi>m</mi></msub></mrow></mfrac></mrow></mstyle><mo>-</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>β</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mi>ξ</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>α</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">(</mo><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">(</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>m</mi><mo>=</mo><mn>1</mn></mrow><mi>Μ</mi></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>m</mi><mo>=</mo><mn>1</mn></mrow><mi>Μ</mi></munderover><mi mathvariant="bold-italic">w</mi></mstyle></mrow></mstyle><msubsup><mrow></mrow><mi>m</mi><mtext>Τ</mtext></msubsup><mi>φ</mi><msub><mrow></mrow><mi>m</mi></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo>+</mo><mi>b</mi><mo stretchy="false">)</mo><mo>-</mo><mn>1</mn><mo>+</mo><mi>ξ</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>0</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="64">其中,<i>α</i>&gt;0、<i>β</i>&gt;0为拉格朗日乘子。对拉格朗日函数对应的原始变量<i><b>w</b></i>、<i>b</i>、<i>ξ</i>求偏导数,得到式(9)的对偶形式:</p>
                </div>
                <div class="p1">
                    <p id="65" class="code-formula">
                        <mathml id="65"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mrow><mi mathvariant="bold-italic">θ</mi><mo>∈</mo><mi>Θ</mi></mrow></munder></mrow><mtext> </mtext><mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mi mathvariant="bold-italic">α</mi></munder></mrow><mtext> </mtext><mi>D</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">θ</mi><mo>,</mo><mi mathvariant="bold-italic">α</mi><mo stretchy="false">)</mo><mo>,</mo><mtext>s</mtext><mo>.</mo><mtext>t</mtext><mo>.</mo><mn>0</mn><mo>≤</mo><mi mathvariant="bold-italic">α</mi><mo>≤</mo><mi>C</mi><mo>,</mo><mi mathvariant="bold-italic">y</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">α</mi><mo>=</mo><mn>0</mn><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>1</mn><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mi>D</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">θ</mi><mo>,</mo><mi mathvariant="bold-italic">α</mi><mo stretchy="false">)</mo><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mi>α</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>,</mo><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mi>α</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mi>α</mi><msub><mrow></mrow><mi>j</mi></msub><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mi>y</mi><msub><mrow></mrow><mi>j</mi></msub><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>m</mi><mo>=</mo><mn>1</mn></mrow><mi>Μ</mi></munderover><mi>θ</mi></mstyle><msub><mrow></mrow><mi>m</mi></msub><mi>Κ</mi><msub><mrow></mrow><mi>m</mi></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo>=</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mi>α</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mrow><mo stretchy="false">(</mo><mi mathvariant="bold-italic">α</mi><mo>˚</mo><mspace width="0.25em" /><mi mathvariant="bold-italic">y</mi><mo stretchy="false">)</mo></mrow><msup><mrow></mrow><mtext>Τ</mtext></msup><mo stretchy="false">(</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>m</mi><mo>=</mo><mn>1</mn></mrow><mi>Μ</mi></munderover><mi>θ</mi></mstyle><msub><mrow></mrow><mi>m</mi></msub><mi>Κ</mi><msub><mrow></mrow><mi>m</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">(</mo><mi mathvariant="bold-italic">α</mi><mo>˚</mo><mspace width="0.25em" /><mi mathvariant="bold-italic">y</mi><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>2</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="66">其中,。 表示2个向量对应位置元素的乘积。式(11)为凸问题,其最优解能够保证为全局最优。并且式(11)为SVM的标准对偶形式,通常在算法中直接使用SVM解决器<citation id="185" type="reference"><link href="33" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>进行求解。优化式(11)的鞍点问题,通常采用交替优化<i>α</i>和<i>θ</i>进行求解。当<i>θ</i>固定时,可以执行SVM求解变量<i>α</i>;当求取核权值<i>θ</i>时,可以将式(11)转化成半无限规划问题再求解。假设<i>u</i>(<i>α</i>,<i>θ</i>)为目标函数值,并且<i>α</i><sup>*</sup>为最优解,则对于任何<i>α</i>和<i>θ</i>,有<i>u</i>(<i>α</i><sup>*</sup>,<i>θ</i>)≥<i>u</i>(<i>α</i>,<i>θ</i>)。因此,式(11)等价于最小化最优值的上界<i>η</i>,式(13)为半无限规划问题。</p>
                </div>
                <div class="area_img" id="68">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJC201910032_06800.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZKU3U5a3pnekRzZjlxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="69">其中,<i>A</i>={<i>α</i>∈<image href="images/JSJC201910032_161.jpg" type="" display="inline" placement="inline"><alt></alt></image><mathml id="162"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>|</mo><mrow><mn>0</mn><mo>≤</mo></mrow></mrow><mi mathvariant="bold-italic">α</mi><mo>≤</mo><mi>C</mi><mo>,</mo><mi mathvariant="bold-italic">y</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">α</mi><mo>=</mo><mn>0</mn><mo stretchy="false">}</mo></mrow></math></mathml>,当<i>p</i>&gt;1时,<mathml id="163"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>v</mi><mrow><mo>|</mo><mi mathvariant="bold-italic">θ</mi><mo>|</mo></mrow><msub><mrow></mrow><mn>1</mn></msub><mo>+</mo></mrow><mo stretchy="false">(</mo><mn>1</mn><mo>-</mo><mi>v</mi><mo stretchy="false">)</mo><mrow><mo>|</mo><mi mathvariant="bold-italic">θ</mi><mo>|</mo></mrow><msubsup><mrow></mrow><mi>p</mi><mi>p</mi></msubsup><mo>≤</mo><mn>1</mn></mrow></math></mathml>为非线性约束,可以使用二阶泰勒展开式进行近似处理。在二阶泰勒展开式中,二次项的<i>Hessian</i>矩阵为正对角阵,因此能有效地求解二次约束问题。为保证算法的有效运行,首先赋予<i>θ</i>一个初始值,假设所有的核函数<i>K</i><sub>1</sub>,<i>K</i><sub>2</sub>,…,<i>K</i><sub><i>M</i></sub>都是半正定的,则式(14)成立。</p>
                </div>
                <div class="p1">
                    <p id="70" class="code-formula">
                        <mathml id="70"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>v</mi><mrow><mo>|</mo><mi mathvariant="bold-italic">θ</mi><mo>|</mo></mrow><msub><mrow></mrow><mn>1</mn></msub><mo>+</mo><mo stretchy="false">(</mo><mn>1</mn><mo>-</mo><mi>v</mi><mo stretchy="false">)</mo><mrow><mo>|</mo><mi mathvariant="bold-italic">θ</mi><mo>|</mo></mrow><msubsup><mrow></mrow><mi>p</mi><mi>p</mi></msubsup><mo>=</mo><mn>1</mn><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>4</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="71">假设所有<i>θ</i><mathml id="164"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>t</mi><mn>0</mn></msubsup></mrow></math></mathml>都相同,结合式(13)和式(14),可求解初始值θ<mathml id="165"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>t</mi><mn>0</mn></msubsup></mrow></math></mathml>为:</p>
                </div>
                <div class="p1">
                    <p id="72" class="code-formula">
                        <mathml id="72"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>θ</mi><msubsup><mrow></mrow><mi>t</mi><mn>0</mn></msubsup><mo>=</mo><mfrac><mrow><msqrt><mrow><mi>v</mi><msup><mrow></mrow><mn>2</mn></msup><mi>p</mi><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mn>4</mn><mo stretchy="false">(</mo><mn>1</mn><mo>-</mo><mi>v</mi><mo stretchy="false">)</mo><mi>p</mi></mrow></msqrt><mo>-</mo><mi>v</mi><mi>p</mi></mrow><mrow><mn>2</mn><mo stretchy="false">(</mo><mn>1</mn><mo>-</mo><mi>v</mi><mo stretchy="false">)</mo><mi>p</mi></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>5</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="73">算法1为求取基本分类器的基本过程。在算法1中,首先进行相关参数的初始化,第1步采用<i>SVM</i>的标准解法更新<i>α</i>;然后计算优化问题,第3步～第6步通过混合网型约束求取核函数的权值,当违反规范化最大约束时,算法就会停止,即已经达到收敛判定准则;最后对集合分类器进行集成,即可得到一个基本分类器。</p>
                </div>
                <div class="p1">
                    <p id="74"><b>算法1</b> 弹性网型正则算法</p>
                </div>
                <div class="area_img" id="188">
                                <img alt="" src="Detail/GetImg?filename=images/JSJC201910032_18800.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZKU3U5a3pnekRzZjlxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="83" name="83">2.2 AdaBoost-EMKL算法</h4>
                <div class="p1">
                    <p id="84">在采用基于弹性网型正则化选择出核函数后,即可得到基本分类器。AdaBoost-EMKL的总体思路是在选择出基本分类器后,应用提升算法学习一个多核分类器。训练样本的权系数<i>D</i><sub><i>t</i></sub>表明其对于学习的重要性,在每一轮的迭代中,首先根据样本分布<i>D</i><sub><i>t</i></sub>抽取<i>n</i>个子样本,得到训练子集并给定一定数量的核函数。在得到训练子集和核函数以后,基于弹性网型正则化进行基本核函数的核权值的选取,如算法1所示。选择出基本分类器<i>f</i><sub><i>t</i></sub>(<i><b>x</b></i>)以后,根据在AdaBoost相似过程中训练数据的分布<i>D</i><sub><i>t</i></sub>计算每个基本分类器的错分率,依据错分率进一步度量每个基本分类器的性能,如式(16)所示。</p>
                </div>
                <div class="p1">
                    <p id="85" class="code-formula">
                        <mathml id="85"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>ε</mi><msup><mrow></mrow><mi>t</mi></msup><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mi>D</mi></mstyle><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo><mo stretchy="false">(</mo><mi>f</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo>≠</mo><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>6</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="86">根据分类器的错分率<i>ε</i><sup><i>t</i></sup>进行每个基本分类器权重<i>α</i><sub><i>t</i></sub>的计算,同时对训练样本的权重分布进行更新,如式(17)和式(18)所示。</p>
                </div>
                <div class="p1">
                    <p id="87" class="code-formula">
                        <mathml id="87"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>α</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mrow><mi>ln</mi></mrow><mrow><mo>(</mo><mrow><mfrac><mrow><mn>1</mn><mo>-</mo><mi>ε</mi><msup><mrow></mrow><mi>t</mi></msup></mrow><mrow><mi>ε</mi><msup><mrow></mrow><mi>t</mi></msup></mrow></mfrac></mrow><mo>)</mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>7</mn><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mi>D</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>→</mo><mfrac><mrow><mi>D</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow><mrow><mi>Ζ</mi><msup><mrow></mrow><mi>t</mi></msup></mrow></mfrac><mrow><mi>exp</mi></mrow><mo stretchy="false">(</mo><mo>-</mo><mi>α</mi><msub><mrow></mrow><mi>t</mi></msub><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mi>f</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>8</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="88">其中,<i>Z</i><sup><i>t</i></sup>是归一化因子。通过反复学习基本核分类器,迭代完成后根据权重系数对基本分类器进行集成,得到的最终分类器如式(19)所示。</p>
                </div>
                <div class="p1">
                    <p id="89"><i>f</i>(<i><b>x</b></i>)=sign<mathml id="169"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false">(</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>Τ</mi></munderover><mi>α</mi></mstyle><msub><mrow></mrow><mi>t</mi></msub><mtext> </mtext><mi>f</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>9</mn><mo stretchy="false">)</mo></mrow></math></mathml></p>
                </div>
                <div class="p1">
                    <p id="90">本文算法伪代码如算法2所示。</p>
                </div>
                <div class="p1">
                    <p id="91"><b>算法2</b> AdaBoost-EMKL算法</p>
                </div>
                <div class="area_img" id="189">
                                <img alt="" src="Detail/GetImg?filename=images/JSJC201910032_18900.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZKU3U5a3pnekRzZjlxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="107">算法2的输入为相关数据集和初始化的参数,输出为强分类器,第1步～第8步是进行基本分类器的选择。其中,第2步根据样本的分布抽取<i>n</i>个样本子集,第3步采用算法1进行基本分类器核权值的选择,第4步计算训练错分率,第5步根据错分率计算当前基本分类器的权值,第6步更新训练样本的分布同时进行归一化操作。进行<i>T</i>轮迭代后,结束整个迭代的过程,将得到的基本分类器进行集成为最终的强分类器。AdaBoost-EMKL算法由于采用了弹性网型正则化选取核函数的权值,避免了核权值稀疏性与非稀疏性所带来的问题。同时该算法具有集成算法的优势,能够通过不断迭代增强分类器的性能。</p>
                </div>
                <h4 class="anchor-tag" id="108" name="108">2.3 时间复杂度</h4>
                <div class="p1">
                    <p id="109">在算法2中,每次进行迭代时,首先都要选择出一个基本分类器,如算法2的第3步所示。基本分类器的求解采用算法1中的基本过程。该过程包含了标准SVM问题的求解(如算法1的第1步所示)以及二次约束二次规划问题的求解,如算法1的第4步所示。本文采用的是SMO算法求解SVM标准问题,其时间复杂度为<i>O</i>(<i>n</i><sup>2</sup>),二次约束二次规划问题的收敛速度为<i>O</i>(<i>n</i><sup>2</sup>),因此,算法1总的时间复杂度为<i>O</i>(<i>n</i><sup>2</sup>)。而算法2的时间复杂度主要取决于其自身迭代次数与算法1的时间复杂度,因此,算法2的时间复杂度为<i>O</i>(<i>T</i>)×<i>O</i>(<i>n</i><sup>2</sup>)。</p>
                </div>
                <h3 id="110" name="110" class="anchor-tag">3 实验结果与分析</h3>
                <h4 class="anchor-tag" id="111" name="111">3.1 实验数据与环境</h4>
                <div class="p1">
                    <p id="112">本文采用标准SVM、SimpleMKL、<i>L</i><sub>1</sub>-MKL、<i>L</i><sub><i>p</i></sub>-MKL以及MKBoost-D2<citation id="186" type="reference"><link href="15" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>算法作为对比算法,验证本文算法的性能以及可行性。SimpleMKL与<i>L</i><sub>1</sub>-MKL基于<i>L</i><sub>1</sub>范数约束,本文算法与其对比核权值的稀疏性。<i>L</i><sub><i>p</i></sub>-MKL基于<i>L</i><sub><i>p</i></sub> 范数约束,MKBoost-D2基于核权值非稀疏的Adaboost集成多核方法。上述2种方法在选取核函数的权值时都是非稀疏的,因此能与本文算法形成鲜明的对比。本文实验采用的数据集信息如表1所示,整个实验将在Microsoft Windows 8(64位),2.7 GHz Intel CPU,6 GB RAM,Matlab7.8.0 R2009a平台上进行。</p>
                </div>
                <div class="area_img" id="113">
                    <p class="img_tit"><b>表1 数据集信息</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="113" border="1"><tr><td>数据集名称</td><td>数据集来源</td><td>特征数量</td><td>特征维数</td><td>类别</td></tr><tr><td>Ionosphere</td><td>UCI</td><td>351</td><td>34</td><td>2</td></tr><tr><td><br />Wdbc</td><td>UCI</td><td>569</td><td>30</td><td>2</td></tr><tr><td><br />German.number</td><td>Statlog</td><td>1 000</td><td>24</td><td>2</td></tr><tr><td><br />Monks2</td><td>UCI</td><td>1 033</td><td>6</td><td>2</td></tr><tr><td><br />Svmguide2</td><td>CWH03a</td><td>1 243</td><td>21</td><td>2</td></tr><tr><td><br />Balance-scale</td><td>UCI</td><td>576</td><td>4</td><td>4</td></tr><tr><td><br />Segment</td><td>Statlog</td><td>2 310</td><td>19</td><td>7</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="114" name="114">3.2 实验参数设置</h4>
                <div class="p1">
                    <p id="115">本文实验选用的高斯核<i>σ</i>的取值分别为1.5<sup>-5</sup>,1.5<sup>-4</sup>,…,1.5<sup>6</sup>,2<sup>-5</sup>,2<sup>-4</sup>,…,2<sup>6</sup>,多项式指数分别为1、2、3。SVM以及其他算法正则化参数采用5-折交叉验证选择最优参数。设置<i>L</i><sub><i>p</i></sub> -MKL与本文算法的<i>p</i>值为2、4、6。平衡参数<i>v</i>的值从集合{0.0,0.1,…,1.0}中选取。对于SimpleMKL算法,采用SimpleMKL工具箱进行求解<citation id="187" type="reference"><link href="33" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>,当对偶间隙小于0.001或者迭代次数超过500时停止算法。本文算法采用迭代次数超过500次并且核权值相差在0.001以内作为停止标准。</p>
                </div>
                <h4 class="anchor-tag" id="116" name="116">3.3 性能比较</h4>
                <h4 class="anchor-tag" id="117" name="117">3.3.1 分类精度对比</h4>
                <div class="p1">
                    <p id="118">本文对每个算法进行10次重复实验,每次实验均从数据集中随机选择一定数量的样本作为训练集,剩余样本作为测试集,各算法的分类精度与训练样本的关系如图1所示。从图1可以看出,随着训练样本数量增多,各算法的分类精度都有所提升,并且在各个数据集中,无论训练样本数量多少,本文算法的分类精度均较高,表明其泛化性能较优。</p>
                </div>
                <div class="area_img" id="119">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201910032_119.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZKU3U5a3pnekRzZjlxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 6种算法的分类精度与训练样本数之间的关系" src="Detail/GetImg?filename=images/JSJC201910032_119.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZKU3U5a3pnekRzZjlxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图1 6种算法的分类精度与训练样本数之间的关系</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201910032_119.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZKU3U5a3pnekRzZjlxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="120">随机从7个数据集中选择80%的样本作为训练集,剩余样本为测试集。各算法均执行10次,每次参数相同,在测试集上求出其平均分类精度与标准差,结果如表2所示。</p>
                </div>
                <div class="area_img" id="121">
                    <p class="img_tit"><b>表2 6种算法的分类精度与标准差比较</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="121" border="1"><tr><td><br />数据集</td><td>SVM</td><td>SimpleMKL</td><td><i>L</i><sub>1</sub>-MKL</td><td><i>L</i><sub><i>p</i></sub>-MKL</td><td>AdaBoost-D2</td><td>AdaBoost-FMKL</td></tr><tr><td>Ionosphere</td><td>0.910 2±0.013 3</td><td>0.932 5±0.008 9</td><td>0.920 2±0.100 6</td><td>0.939 8±0.007 6</td><td>0.941 8±0.004 8</td><td>0.945 3±0.003 5</td></tr><tr><td><br />Wdbc</td><td>0.932 1±0.009 1</td><td>0.945 2±0.001 3</td><td>0.943 5±0.012 6</td><td>0.942 5±0.002 2</td><td>0.946 2±0.001 3</td><td>0.948 2±0.013 5</td></tr><tr><td><br />German.number</td><td>0.692 1±0.014 3</td><td>0.692 5±0.002 9</td><td>0.701 2±0.130 2</td><td>0.712 1±0.003 6</td><td>0.713 8±0.003 8</td><td>0.715 3±0.073 2</td></tr><tr><td><br />Monks2</td><td>0.870 2±0.001 6</td><td>0.882 5±0.012 9</td><td>0.892 8±0.205 6</td><td>0.907 4±0.014 5</td><td>0.908 5±0.034 2</td><td>0.911 2±0.000 6</td></tr><tr><td><br />Svmguide2</td><td>0.785 6±0.145 1</td><td>0.789 2±0.006 9</td><td>0.804 1±0.070 3</td><td>0.809 3±0.007 3</td><td>0.811 4±0.002 1</td><td>0.815 3±0.016 8</td></tr><tr><td><br />Balance-scale</td><td>0.940 2±0.043 1</td><td>0.952 8±0.003 7</td><td>0.951 2±0.060 6</td><td>0.951 2±0.002 1</td><td>0.975 6±0.001 1</td><td>0.976 7±0.000 8</td></tr><tr><td><br />Segment</td><td>0.900 2±0.006 3</td><td>0.912 8±0.007 4</td><td>0.920 0±0.032 7</td><td>0.926 5±0.001 2</td><td>0.928 8±0.004 2</td><td>0.932 3±0.003 1</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="122">从表2得到以下结论:</p>
                </div>
                <div class="p1">
                    <p id="123">1)在所有的数据集中SVM表现的分类性能较差。</p>
                </div>
                <div class="p1">
                    <p id="124">2)SimpleMKL基于<i>L</i><sub>1</sub>范数约束,因此,其与<i>L</i><sub>1</sub>-MKL的分类性能相差较小。</p>
                </div>
                <div class="p1">
                    <p id="125">3)<i>L</i><sub><i>p</i></sub>-MKL作为非稀疏类模型,其泛化性能优于稀疏模型,但是由于其对噪声敏感并且有冗余信息,分类性能并没有明显优于<i>L</i><sub>1</sub>-MKL,且相对于AdaBoost-D2分类精度较差。</p>
                </div>
                <div class="p1">
                    <p id="126">4)AdaBoost-D2是在非稀疏性核权值构造的基本分类器基础上集成的强分类器,其分类性能优于<i>L</i><sub><i>p</i></sub>-MKL。</p>
                </div>
                <div class="p1">
                    <p id="127">5)Adaboost-EMKL在选择基本分类器时采用弹性网型正则约束核权值,平衡了核权值的稀疏性与非稀疏性,通过集成学习得到最终的强分类器,因此本文算法在分类性能上优于AdaBosot-D2。</p>
                </div>
                <h4 class="anchor-tag" id="128" name="128">3.3.2 收敛性对比</h4>
                <div class="p1">
                    <p id="129">在Ionosphere、Monks2、Segement、Svmguide2数据集随机选取80%样本作为训练集,剩余的为测试集,对比本文算法和MKBoost-D2算法的收敛性。算法迭代次数为30次,实验次数为10次,2种集成多核学习算法的精度与迭代关系如图2所示。</p>
                </div>
                <div class="area_img" id="130">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201910032_130.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZKU3U5a3pnekRzZjlxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 本文算法的收敛性比较" src="Detail/GetImg?filename=images/JSJC201910032_130.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZKU3U5a3pnekRzZjlxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图2 本文算法的收敛性比较</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201910032_130.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZKU3U5a3pnekRzZjlxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="131">从图2可以看出,算法的分类精度随着迭代次数的增加而提高。在刚开始迭代时,MKBoost-D2分类精度高于本文算法,当迭代超过10次后,本文算法的分类精度较优,且算法开始收敛,即分类精度逐渐稳定。因此,本文算法的收敛性较好,能在有效的迭代次数内收敛到最优值。</p>
                </div>
                <h4 class="anchor-tag" id="132" name="132">3.3.3 参数<i>p</i>对比</h4>
                <div class="p1">
                    <p id="133">为了验证不同<i>p</i>值,即在不同范数正则约束情况下,不同算法的时间性能以及核函数数量选择的变化情况,本文在Ionosphere、Segment、Wdbc数据集中进行算法评估。随机选择80%样本作为训练集,剩余样本为测试集。各算法均执行10次,实验结果如表3～表5所示。从表3～表5可以看出,<i>p</i>值不同本文算法分类精度也不同,因此,选择合适的<i>p</i>值对算法至关重要,针对不同的数据集可以采用交叉验证的方法选择出合适的<i>p</i>值。当<i>p</i>=2时,本文算法比<i>L</i><sub>1</sub>-MKL选择了更多的核函数。这是因为<i>L</i><sub>1</sub>-MKL的核权值是稀疏的,所以丢弃了许多核函数,忽略一些有用的信息,导致其缺少泛化能力。而<i>L</i><sub><i>p</i></sub>-MKL选择了所有的核函数,但是其对噪声敏感,因此与<i>L</i><sub>1</sub>-MKL对比,其分类精度并不存在优势,由表4可知<i>L</i><sub><i>p</i></sub>-MKL的分类性能比稀疏的<i>L</i><sub>1</sub>-MKL差。而本文算法采用是弹性网型正则,选择了适量的核函数,所取得的核权值是处于稀疏与非稀疏之间。同时算法的运行时间随着<i>p</i>值的变化而不同,当<i>p</i>值较小时,算法的运行时间较少,并且在多数数据集中,本文算法运行效率都高于<i>L</i><sub>1</sub>-MKL,这是因为当<i>L</i><sub>1</sub>-MKL更新下降方向时,需要进行大量的计算。</p>
                </div>
                <div class="area_img" id="134">
                    <p class="img_tit"><b>表3 不同范数正则约束情况下Ionosphere数据集的测试结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="134" border="1"><tr><td>算法</td><td><i>p</i></td><td>分类精度</td><td>核函数数量</td><td>运行时间/s</td></tr><tr><td rowspan="3"><br /><i>L</i><sub><i>p</i></sub>-MKL</td><td>2</td><td>0.942 1±0.012 5</td><td>410</td><td>4.5±0.6</td></tr><tr><td><br />4</td><td>0.941 1±0.015 6</td><td>410</td><td>3.6±0.9</td></tr><tr><td><br />6</td><td>0.938 6±0.069 1</td><td>410</td><td>5.6±1.2</td></tr><tr><td rowspan="3"><br />AdaBoost-EMKL</td><td>2</td><td>0.946 5±0.008 5</td><td>42.5±1.3</td><td>7.9±1.5</td></tr><tr><td><br />4</td><td>0.943 2±0.013 6</td><td>30.2±1.6</td><td>9.6±0.8</td></tr><tr><td><br />6</td><td>0.941 0±0.096 4</td><td>21.5±3.1</td><td>13.6±2.1</td></tr><tr><td><i>L</i><sub>1</sub>-MKL</td><td>—</td><td>0.942 0±0.016 4</td><td>35.6±2.9</td><td>10.2±2.6</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="135">
                    <p class="img_tit"><b>表4 不同范数正则约束情况下Segment数据集的测试结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="135" border="1"><tr><td>算法</td><td><i>p</i></td><td>分类精度</td><td>核函数数量</td><td>运行时间/s</td></tr><tr><td rowspan="3"><br /><i>L</i><sub><i>p</i></sub>-MKL</td><td>2</td><td>0.921 6±0.014 3</td><td>251</td><td>1.5±0.4</td></tr><tr><td><br />4</td><td>0.920 1±0.012 7</td><td>251</td><td>1.6±0.2</td></tr><tr><td><br />6</td><td>0.918 4±0.067 7</td><td>251</td><td>2.4±0.2</td></tr><tr><td rowspan="3"><br />AdaBoost-EMKL</td><td>2</td><td>0.928 9±0.018 5</td><td>21.5±1.1</td><td>4.1±0.3</td></tr><tr><td><br />4</td><td>0.923 2±0.013 1</td><td>17.3±0.6</td><td>4.6±0.2</td></tr><tr><td><br />6</td><td>0.919 2±0.015 8</td><td>14.5±0.5</td><td>7.1±0.1</td></tr><tr><td><i>L</i><sub>1</sub>-MKL</td><td>—</td><td>0.924 5±0.014 7</td><td>19.6±0.9</td><td>3.2±0.6</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="136">
                    <p class="img_tit"><b>表5 不同范数正则约束情况下Wdbc数据集的测试结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="136" border="1"><tr><td>算法</td><td><i>p</i></td><td>分类精度</td><td>核函数数量</td><td>运行时间/s</td></tr><tr><td rowspan="3"><br /><i>L</i><sub><i>p</i></sub>-MKL</td><td>2</td><td>0.943 5±0.012 2</td><td>396</td><td>2.7±0.2</td></tr><tr><td><br />4</td><td>0.941 1±0.027 5</td><td>396</td><td>3.6±0.9</td></tr><tr><td><br />6</td><td>0.9436 ±0.061 2</td><td>396</td><td>5.6±1.2</td></tr><tr><td rowspan="3"><br />AdaBoost-EMKL</td><td>2</td><td>0.946 1±0.003 5</td><td>32.7±1.1</td><td>8.9±1.5</td></tr><tr><td><br />4</td><td>0.943 2±0.013 1</td><td>17.8±1.4</td><td>12.4±1.8</td></tr><tr><td><br />6</td><td>0.941 0±0.018 6</td><td>16.5±1.2</td><td>18.4±2.6</td></tr><tr><td><i>L</i><sub>1</sub>-MKL</td><td>—</td><td>0.942 0±0.014 5</td><td>25.4±2.7</td><td>28.2±2.1</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="137" name="137">3.3.4 参数<i>v</i>对比</h4>
                <div class="p1">
                    <p id="138">本文选择Segment与Wdbc数据集测试平衡参数<i>v</i>对于分类精度的影响,实验结果如图3所示。</p>
                </div>
                <div class="area_img" id="139">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201910032_139.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZKU3U5a3pnekRzZjlxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 Adaboost-EMKL算法精度与v取值的关系" src="Detail/GetImg?filename=images/JSJC201910032_139.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZKU3U5a3pnekRzZjlxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图3 Adaboost-EMKL算法精度与</b><i><b>v</b></i><b>取值的关系</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201910032_139.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZKU3U5a3pnekRzZjlxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="140">当<i>v</i>=0时,由式(12)可知,弹性网型正则退化为<i>L</i><sub><i>p</i></sub>正则,此时其分类精度是<i>L</i><sub><i>P</i></sub>-MKL的结果。同理可得,当<i>v</i>=1时,弹性网型正则为<i>L</i><sub>1</sub>正则。当<i>v</i>∈(0,1)时,对于不同的数据集,最优分类精度随着<i>v</i>值取值不同而变化,因此本文采用交叉验证来对<i>v</i>和<i>p</i>进行调整,以获得更好的分类精度。</p>
                </div>
                <div class="p1">
                    <p id="141">本文在Segment数据集中测试Adaboost-EMKL算法的 <i>v</i>与核函数数量的关系,结果如图4所示。从图4可以看出,随着<i>v</i>值的增大,即核权值更稀疏,所选择的核函数数量逐渐变小,并且该规律不受其他因素影响,即核函数数量只与<i>v</i>相关。</p>
                </div>
                <div class="area_img" id="142">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201910032_142.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZKU3U5a3pnekRzZjlxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 Adaboost-EMKL算法中v取值与选择核数量的关系" src="Detail/GetImg?filename=images/JSJC201910032_142.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZKU3U5a3pnekRzZjlxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图4 Adaboost-EMKL算法中</b><i><b>v</b></i><b>取值与选择核数量的关系</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201910032_142.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZKU3U5a3pnekRzZjlxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h3 id="143" name="143" class="anchor-tag">4 结束语</h3>
                <div class="p1">
                    <p id="144">本文提出AdaBoost-EMKL算法,利用AdaBoost集成学习的设计思想,将基本分类器集成为强分类器。在迭代求取基本分类器时,采用弹性网型正则化约束选择核权值,以提高模型的泛化能力并降低其对噪声敏感性。实验结果表明,该算法不仅具有AdaBoost集成算法的优点,而且能够平衡<i>L</i><sub>1</sub>正则的稀疏性与<i>L</i><sub><i>p</i></sub>正则的非稀疏性,在较少的迭代次数内,获得分类精度高、运行速度快且性能稳定的分类器。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="5">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00001340333&amp;v=MjQ4NzhxZWJ1ZHRGQ3ZsVnJ2SkpGOD1OajdCYXJPNEh0SE5ySXRGWitnTVkzazV6QmRoNGo5OVNYcVJyeG94Y01IN1I3&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZKU3U5a3pnekRzZjlxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> CHAPELLE O,VAPNIK V,BOUSQUET O,et al.Choosing multiple parameters for support vector machines[J].Machine Learning,2002,46(1/2/3):131-159.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Efficient and accurate lp-normmultiple kernel learning">

                                <b>[2]</b> KLOFT M,BREFELD U,SONNENBURG S.Efficient and accurate Lp-norm multiple kernel learning[C]//Prcoeedings of Advances in Neural Information Processing Systems.[S.1.]:NIPS Foundation,Inc.,2009:997-1005.
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13080200145820&amp;v=MTY2MzRxUVRNbndaZVp1SHlqbVVMeklKVm9WYmhNPU5pZk9mYks3SHRuTXJZOUZaZThLQkg0NW9CTVQ2VDRQUUgvaXJSZEdlcg==&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZKU3U5a3pnekRzZjlxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> SUN Tao,JIAO Licheng,LIU Fang,et al.Selective multiple kernel learning for classification with ensemble strategy[J].Pattern Recognition,2013,46(11):3081-3090.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Efficient Sparse Generalized Multiple Kernel Learning">

                                <b>[4]</b> YANG Haiqin,XU Zenglin,YE Jieping,et al.Efficient sparse generalized multiple kernel learning[J].IEEE Transactions on Neural Networks,2011,22(3):433-446.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Non-sparse regularization and efficient training with multiple kernels">

                                <b>[5]</b> KLOFT M,BREFELD U,SONNENBURG S,et al.Non-sparse regularization and efficient training with multiple kernels[EB/OL].[2018-03-01].https://arxiv.org/abs/1003.0079.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Hoi MKBoost:A Framework of Multiple Kernel Boosting">

                                <b>[6]</b> XIA Hao,HOI S C H.MKBoost:a framework of multiple kernel boosting[J].IEEE Transactions on Knowledge and Data Engineering,2013,25(7):1574-1586.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Elastic-net constrained multiple kernel learning using a majorization-minimization approach">

                                <b>[7]</b> CITI L.Elastic-net constrained multiple kernel learning using a majorization-minimization approach[C]//Proceedings of Computer Science and Electronic Engineering Conference.Washington D.C.,USA:IEEE Press,2015:29-34.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201106007&amp;v=MTcyNDg2ajU0TzN6cXFCdEdGckNVUkxPZVplUnRGeTNoVXJ2SktDTGZZYkc0SDlETXFZOUZZNFFLREg4NHZSNFQ=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZKU3U5a3pnekRzZjlxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b> 武征鹏,张学工.弹性多核学习[J].自动化学报,2011,37(6):693-699.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZXU201804022&amp;v=MTE3MTQ5SFpvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSdEZ5M2hVcnZKSVRmVGU3RzRIOW5NcTQ=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZKU3U5a3pnekRzZjlxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b> 王庆超,付光远,汪洪桥,等.基于局部空间变稀疏约束的多核学习方法[J].电子学报,2018,46(4):930-937.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_10" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=CGQJ201802038&amp;v=MjE3NzN5M2hVcnZKSmlyYVpMRzRIOW5Nclk5R2JJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSdEY=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZKU3U5a3pnekRzZjlxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[10]</b> 何佳佳,陈秀宏,田进,等.基于牛顿梯度优化的弹性多核学习[J].传感器与微系统,2018(2):136-139.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_11" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSYJ201601006&amp;v=MTQ5MjBGckNVUkxPZVplUnRGeTNoVXJ2Skx6N1NaTEc0SDlmTXJvOUZZb1FLREg4NHZSNFQ2ajU0TzN6cXFCdEc=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZKU3U5a3pnekRzZjlxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[11]</b> 张仁峰,吴小俊,陈素根.通用稀疏多核学习[J].计算机应用研究,2016,33(1):21-27.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_12" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=HZLG201508022&amp;v=MDM1NjdUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJ0RnkzaFVydkpMVGZIYWJHNEg5VE1wNDlIWm9RS0RIODR2UjQ=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZKU3U5a3pnekRzZjlxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[12]</b> 梁军,李世浩,张飞云,等.基于半无限规划的弹性多核学习算法[J].华中科技大学学报(自然科学版),2015,43(8):103-106.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_13" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201306008&amp;v=MDI3ODg2ajU0TzN6cXFCdEdGckNVUkxPZVplUnRGeTNoVXJ2SktDTGZZYkc0SDlMTXFZOUZiSVFLREg4NHZSNFQ=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZKU3U5a3pnekRzZjlxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[13]</b> 曹莹,苗启广,刘家辰,等.AdaBoost算法研究进展与展望[J].自动化学报,2013,39(6):745-758.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning with kernels:support vector machines,regularization,optimization,and beyond">

                                <b>[14]</b> SCHOLKOPF B,SMOLA A J.Learning with kernels:support vector machines,regularization,optimization,and beyond[M].Cambridge,USA:MIT Press,2001.
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=SimpleMKL">

                                <b>[15]</b> RAKOTOMAMONJY A,BACH F R,CANU S,et al.SimpleMKL[J].Journal of Machine Learning Research,2008,9(3):2491-2521.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJC201910032" />
        <input id="dpi" type="hidden" value="600" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZKU3U5a3pnekRzZjlxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201910032&amp;v=MjAwOTQzaFVydkpMejdCYmJHNEg5ak5yNDlHWm9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJ0Rnk=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZKU3U5a3pnekRzZjlxOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="1" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
