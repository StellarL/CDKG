<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637130530281748750%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJC201905024%26RESULT%3d1%26SIGN%3dwBAaKBNfaZExPK65hhe18zKqpDU%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201905024&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201905024&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201905024&amp;v=MTM3NzE0TzN6cXFCdEdGckNVUkxPZVplUm9GeTNsVzd6TUx6N0JiYkc0SDlqTXFvOUhZSVFLREg4NHZSNFQ2ajU=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#33" data-title="0 概述 ">0 概述</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#37" data-title="1 相关工作 ">1 相关工作</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#39" data-title="1.1 Bagging算法">1.1 Bagging算法</a></li>
                                                <li><a href="#41" data-title="1.2 Adaboost算法">1.2 Adaboost算法</a></li>
                                                <li><a href="#45" data-title="1.3 Random Subspace算法">1.3 Random Subspace算法</a></li>
                                                <li><a href="#47" data-title="1.4 随机森林算法">1.4 随机森林算法</a></li>
                                                <li><a href="#49" data-title="1.5 旋转森林算法">1.5 旋转森林算法</a></li>
                                                <li><a href="#51" data-title="1.6 动态随机森林算法">1.6 动态随机森林算法</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#53" data-title="2 PRF算法 ">2 PRF算法</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#65" data-title="3 实验结果与分析 ">3 实验结果与分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#66" data-title="3.1 UCI机器学习数据库">3.1 UCI机器学习数据库</a></li>
                                                <li><a href="#92" data-title="3.2 CASAS智能家居数据库">3.2 CASAS智能家居数据库</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#108" data-title="4 结束语 ">4 结束语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#60" data-title="&lt;b&gt;表1 不同深度下权值因子的取值范围&lt;/b&gt;"><b>表1 不同深度下权值因子的取值范围</b></a></li>
                                                <li><a href="#68" data-title="&lt;b&gt;表2 UCI数据库信息&lt;/b&gt;"><b>表2 UCI数据库信息</b></a></li>
                                                <li><a href="#72" data-title="&lt;b&gt;表3 各集成算法的分类预测错误率比较&lt;/b&gt;"><b>表3 各集成算法的分类预测错误率比较</b></a></li>
                                                <li><a href="#82" data-title="&lt;b&gt;图1 不同集成分类算法的Kappa值对比&lt;/b&gt;"><b>图1 不同集成分类算法的Kappa值对比</b></a></li>
                                                <li><a href="#85" data-title="&lt;b&gt;表4 DNA数据集信息&lt;/b&gt;"><b>表4 DNA数据集信息</b></a></li>
                                                <li><a href="#86" data-title="&lt;b&gt;图2 不同算法在噪声环境下的预测误差对比&lt;/b&gt;"><b>图2 不同算法在噪声环境下的预测误差对比</b></a></li>
                                                <li><a href="#90" data-title="&lt;b&gt;图3 不同算法运行时间对比&lt;/b&gt;"><b>图3 不同算法运行时间对比</b></a></li>
                                                <li><a href="#94" data-title="&lt;b&gt;表5 传感器网络数据结构&lt;/b&gt;"><b>表5 传感器网络数据结构</b></a></li>
                                                <li><a href="#95" data-title="&lt;b&gt;图4 传感器网络1&lt;/b&gt;"><b>图4 传感器网络1</b></a></li>
                                                <li><a href="#100" data-title="&lt;b&gt;图5 决策树数目对算法预测误差的影响&lt;/b&gt;"><b>图5 决策树数目对算法预测误差的影响</b></a></li>
                                                <li><a href="#104" data-title="&lt;b&gt;图6 传感器网络2&lt;/b&gt;"><b>图6 传感器网络2</b></a></li>
                                                <li><a href="#105" data-title="&lt;b&gt;图7 传感器网络3&lt;/b&gt;"><b>图7 传感器网络3</b></a></li>
                                                <li><a href="#106" data-title="&lt;b&gt;图8 不同传感器网络下算法预测误差对比&lt;/b&gt;"><b>图8 不同传感器网络下算法预测误差对比</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" title=" 宋朋涛, 李超, 徐莉婷, 等.基于个人计算机的智能家居边缘计算系统[J].计算机工程, 2017, 43 (11) :1-7." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201711002&amp;v=MTM0MjZPZVplUm9GeTNsVzd6TUx6N0JiYkc0SDliTnJvOUZab1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkw=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         宋朋涛, 李超, 徐莉婷, 等.基于个人计算机的智能家居边缘计算系统[J].计算机工程, 2017, 43 (11) :1-7.
                                    </a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" title=" ZHANG Le, SUGANTHAN P N.Random forests with ensemble of feature spaces[J].Pattern Recognition, 2014, 47 (10) :3429-3437." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14061600061881&amp;v=MTM3NTA5RlpPME9CSFE0b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VTG5JSlY0Y2FSWT1OaWZPZmJLOEh0Zk5xWQ==&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         ZHANG Le, SUGANTHAN P N.Random forests with ensemble of feature spaces[J].Pattern Recognition, 2014, 47 (10) :3429-3437.
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title=" GOPIKA D, AZHAGUSUNDARI B.A novel approach on ensemble classifiers with fast rotation forest algorithm[J].International Journal of Innovative Research in Computer and Communication Engineering, 2014, 2 (8) :25-30." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A novel approach on ensemble classifiers with fast rotation forest algorithm">
                                        <b>[3]</b>
                                         GOPIKA D, AZHAGUSUNDARI B.A novel approach on ensemble classifiers with fast rotation forest algorithm[J].International Journal of Innovative Research in Computer and Communication Engineering, 2014, 2 (8) :25-30.
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" title=" 周文谊, 王吉源.一种模糊森林学习方法及其行人检测应用[J].计算机工程, 2017, 43 (3) :304-308, 315." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201703051&amp;v=MjAwODRSTE9lWmVSb0Z5M2xXN3pNTHo3QmJiRzRIOWJNckk5QVpZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1U=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                         周文谊, 王吉源.一种模糊森林学习方法及其行人检测应用[J].计算机工程, 2017, 43 (3) :304-308, 315.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" title=" BREIMAN L.Bagging predictors[J].Machine Learning, 1996, 24 (2) :123-140." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00001339482&amp;v=MTI4MTFOckl4TVlPTU5ZM2s1ekJkaDRqOTlTWHFScnhveGNNSDdSN3FlYnVkdEZDN2xWci9BSTFvPU5qN0Jhck80SHRI&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                         BREIMAN L.Bagging predictors[J].Machine Learning, 1996, 24 (2) :123-140.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" title=" HAN Jiawei.Data mining:concepts and techniques[M].San Francisco, USA:Morgan Kaufmann Publishers Inc., 2005." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Data mining:concepts and techniques">
                                        <b>[6]</b>
                                         HAN Jiawei.Data mining:concepts and techniques[M].San Francisco, USA:Morgan Kaufmann Publishers Inc., 2005.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" title=" LOH W Y.Classification and regression trees[J].Wiley Interdisciplinary Reviews Data Mining and Knowledge Discovery, 2011, 1 (1) :14-23." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJWD&amp;filename=SJWD00001347916&amp;v=MTMxODROaWZjYXJPNEh0SE5ySXRDYmVvSlkzazV6QmRoNGo5OVNYcVJyeG94Y01IN1I3cWVidWR0RkM3bFZyL0FJMW89&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                         LOH W Y.Classification and regression trees[J].Wiley Interdisciplinary Reviews Data Mining and Knowledge Discovery, 2011, 1 (1) :14-23.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" title=" FREUND Y, SCHAPIRE R E.Experiments with a new boosting algorithm[EB/OL].[2017-12-15].https://cseweb.ucsd.edu/～yfreund/papers/boostingexperiments.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Experiments with a new boosting algorithm">
                                        <b>[8]</b>
                                         FREUND Y, SCHAPIRE R E.Experiments with a new boosting algorithm[EB/OL].[2017-12-15].https://cseweb.ucsd.edu/～yfreund/papers/boostingexperiments.pdf.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" title=" HO T K.The random subspace method for constructing decision forests[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 1998, 20 (8) :832-844." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The random subspace method for constructing decision forests">
                                        <b>[9]</b>
                                         HO T K.The random subspace method for constructing decision forests[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 1998, 20 (8) :832-844.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_10" title=" BREIMAN L.Random forests[J].Machine Learning, 2001, 45 (1) :5-32." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00001340271&amp;v=MTg3ODVYcVJyeG94Y01IN1I3cWVidWR0RkM3bFZyL0FJMW89Tmo3QmFyTzRIdEhOckl0Rlp1d09ZM2s1ekJkaDRqOTlT&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[10]</b>
                                         BREIMAN L.Random forests[J].Machine Learning, 2001, 45 (1) :5-32.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_11" title=" FAWAGREH K, GABER M M, ELYAN E.Random forests:from early developments to recent advancements[J].Systems Science and Control Engineering, 2014, 2 (1) :602-609." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=STJD&amp;filename=STJD14121800001217&amp;v=MDcyMTZIOVBOcDQ5RlpPc09EbjArb0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VTG5JSlY0Y2FSWT1Oam5CYXJLOA==&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[11]</b>
                                         FAWAGREH K, GABER M M, ELYAN E.Random forests:from early developments to recent advancements[J].Systems Science and Control Engineering, 2014, 2 (1) :602-609.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_12" title=" BERNARD S, ADAM S, HEUTTE L.Dynamic random forests[J].Pattern Recognition Letters, 2012, 33 (12) :1580-1586." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012300413207&amp;v=MzE5NjNPZmJLN0h0RE9ySTlGWU9vTURudytvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVVMbklKVjRjYVJZPU5pZg==&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[12]</b>
                                         BERNARD S, ADAM S, HEUTTE L.Dynamic random forests[J].Pattern Recognition Letters, 2012, 33 (12) :1580-1586.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_13" title=" MART&#205;NEZ-MU&#209;OZ G, SU&#193;REZ A.Out-of-bag estima-tion of the optimal sample size in Bagging[J].Pattern Recognition, 2010, 43 (1) :143-152." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011600738682&amp;v=MTk1MjM3SHRETnFZOUZZK2dIQ25RN29CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVUxuSUpWNGNhUlk9TmlmT2ZiSw==&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[13]</b>
                                         MART&#205;NEZ-MU&#209;OZ G, SU&#193;REZ A.Out-of-bag estima-tion of the optimal sample size in Bagging[J].Pattern Recognition, 2010, 43 (1) :143-152.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_14" title=" QUINLAN J R.C4.5:programs for machine learning[M].San Francisco, USA:Morgan Kaufmann Publishers Inc., 1992." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=C4.5:programs for machine learning">
                                        <b>[14]</b>
                                         QUINLAN J R.C4.5:programs for machine learning[M].San Francisco, USA:Morgan Kaufmann Publishers Inc., 1992.
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_15" title=" MARGINEANTU D D, DIETTERICH T G.Pruning adaptive boosting[C]//Proceedings of the 14th Interna-tional Conference on Machine Learning.San Francisco, USA:Morgan Kaufmann Publishers Inc., 1997:211-218." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Pruning adaptive boosting">
                                        <b>[15]</b>
                                         MARGINEANTU D D, DIETTERICH T G.Pruning adaptive boosting[C]//Proceedings of the 14th Interna-tional Conference on Machine Learning.San Francisco, USA:Morgan Kaufmann Publishers Inc., 1997:211-218.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJC" target="_blank">计算机工程</a>
                2019,45(05),149-154 DOI:10.19678/j.issn.1000-3428.0049940            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于改进随机森林算法的智能环境活动识别</b></span>
 <span class="shoufa"></span>                                     </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%96%9B%E9%93%AD%E9%BE%99&amp;code=39000643&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">薛铭龙</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%8E%E4%B8%80%E5%8D%9A&amp;code=08913298&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">李一博</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%A4%A9%E6%B4%A5%E5%A4%A7%E5%AD%A6%E7%B2%BE%E5%AF%86%E4%BB%AA%E5%99%A8%E4%B8%8E%E5%85%89%E7%94%B5%E5%AD%90%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0246359&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">天津大学精密仪器与光电子工程学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>为使智能家居系统从传感器网络返回的数据中自动识别用户行为并生成个性化服务策略, 提出一种引入惩罚项的随机森林算法。对每次迭代过程中使用的属性集设置不同的惩罚项因子, 生成尽可能不同的决策树, 从而兼顾集成算法的多样性与分类精度。在UCI、CASAS数据集上的实验结果表明, 与传统集成分类算法Bagging、Adaboost相比, 该算法具有更高的分类精度与噪声鲁棒性。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%87%AA%E5%8A%A8%E8%AF%86%E5%88%AB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">自动识别;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">集成学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%83%A9%E7%BD%9A%E9%A1%B9%E5%9B%A0%E5%AD%90&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">惩罚项因子;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%88%86%E7%B1%BB%E7%B2%BE%E5%BA%A6&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">分类精度;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%99%AA%E5%A3%B0%E9%B2%81%E6%A3%92%E6%80%A7&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">噪声鲁棒性;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    薛铭龙 (1994—) , 男, 硕士研究生, 主研方向为模式识别、人工智能;;
                                </span>
                                <span>
                                    李一博, 副教授、博士生导师。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-01-02</p>

                    <p>

                            <b>基金：</b>
                                                        <span>天津市自然科学基金 (17JCYBJC19300);</span>
                    </p>
            </div>
                    <h1><b>Intelligent Environmental Activity Recognition Based on Improved Random Forest Algorithm</b></h1>
                    <h2>
                    <span>XUE Minglong</span>
                    <span>LI Yibo</span>
            </h2>
                    <h2>
                    <span>School of Precision Instruments and Opto-Electronics Engineering, Tianjin University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>In order to enable smart home systems to automatically identify user behavior and generate personalized service strategies from data returned by sensor networks, a random forest algorithm with penalty items is proposed.Different penalty factors are set to the attribute set used in each iteration to generate as different decision trees as possible, keeping the diversity and classification accuracy of the ensemble algorithm.Experimental results on UCI and CASAS datasets show that compared with the traditional integrated classification algorithms, Bagging and Adaboost, this algorithm has higher classification accuracy and noise robustness.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=automatic%20recognition&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">automatic recognition;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=ensemble%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">ensemble learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=penalty%20factor&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">penalty factor;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=classification%20accuracy&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">classification accuracy;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=noise%20robustness&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">noise robustness;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-01-02</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="33" name="33" class="anchor-tag">0 概述</h3>
                <div class="p1">
                    <p id="34">随着智能家居系统的发展, 家庭环境中出现了越来越多的智能化设备<citation id="110" type="reference"><link href="3" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>。使用合适的识别算法来处理由传感器网络返回的大量数据, 已成为当今模式识别领域的研究热点之一。</p>
                </div>
                <div class="p1">
                    <p id="35">近年来, 机器学习算法中的集成学习算法受到人们的广泛关注, 如基于特征空间的随机森林 (Random Forest) 算法<citation id="111" type="reference"><link href="5" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>、快速旋转森林算法<citation id="112" type="reference"><link href="7" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>等。集成学习算法较符合人们长久以来解决问题所采用的哲学思想, 即汇总多专家的意见并采取多数投票的方式来确定最终结果。而对于集成学习算法本身而言, “专家”的角色由各自独立的分类器所扮演。每个分类器自身可能不具有很高的分类精度, 且容易受到噪音的影响, 但将这些个体分类器进行集成并使用其投票结果作为最终的运行结果, 往往能得到一个精度较高、噪声鲁棒性较强的分类器, 在上述过程中, 本文称其中的各分类器为基分类器。如果“专家”能从多个角度分析问题, 其投票结果相对而言更可信, 因此, 多样性也是评价集成学习算法性能优劣的标准之一。</p>
                </div>
                <div class="p1">
                    <p id="36">使用决策树算法作为基分类器的集成算法又称为决策森林算法。本文在随机森林算法的基础上, 引入惩罚项因子, 提出一种PRF (Penalized Random Forest) 算法, 以对智能环境中的用户活动进行分类识别。</p>
                </div>
                <h3 id="37" name="37" class="anchor-tag">1 相关工作</h3>
                <div class="p1">
                    <p id="38">决策森林是一种集成算法<citation id="113" type="reference"><link href="9" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>, 其目的是在每次迭代中建立精确度高、多样性强 (互相关指数低) 的决策树算法集合。</p>
                </div>
                <h4 class="anchor-tag" id="39" name="39">1.1 Bagging算法</h4>
                <div class="p1">
                    <p id="40">Bagging算法<citation id="114" type="reference"><link href="11" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>于1996年被提出。在Bagging算法中, 每个基分类器由从原始训练数据集<i>D</i>中随机抽样的训练数据集<i>D</i><sub><i>i</i></sub>训练而得, 新的训练数据集<i>D</i><sub><i>i</i></sub>包含与原始训练数据集<i>D</i>等量的数据实例。因此, 原始数据集<i>D</i>中的有些数据可能会被多次选择, 而有些可能从未被选择。文献<citation id="115" type="reference">[<a class="sup">6</a>]</citation>研究结果表明, 在每次采样过程中, 有32.8%的数据为重复数据。在实验前, 需预定义采样次数<i>T</i>, 从而生成<i>T</i>个新的训练数据集<i>D</i><sub>1</sub>, <i>D</i><sub>2</sub>, …, <i>D</i><sub><i>T</i></sub>。对每个数据集<i>D</i><sub><i>i</i></sub> (<i>i</i>=1, 2, …, <i>T</i>) 使用决策树CART<citation id="116" type="reference"><link href="15" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>算法建立模型并作为集成算法的基分类器, 因此, 在决策森林中共有<i>T</i>棵决策树为其决策提供信息。Bagging算法实现简单、效率较高, 并且易于与其他算法相结合以提高系统性能, 因此, 该算法在其他决策森林中也被广泛引用。</p>
                </div>
                <h4 class="anchor-tag" id="41" name="41">1.2 Adaboost算法</h4>
                <div class="p1">
                    <p id="42">Adaboost算法<citation id="117" type="reference"><link href="17" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>是Boost算法中的一种经典算法。在Boost算法中, 基分类器由加入权值因子的训练数据集<i>D</i>训练得出。Adaboost算法将权值因子加在训练实例上, 在算法开始时, 将训练数据集中每个数据的权值因子均设为<mathml id="43"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mn>1</mn><mrow><mrow><mo>|</mo><mi>D</mi><mo>|</mo></mrow></mrow></mfrac></mrow></math></mathml>, 即认为每个数据对结果的贡献均相等, 而在之后的迭代过程中, 逐渐增加每次分类错误的数据的权值, 使算法随着迭代次数的增加更加关注易被分类错的数据, 从而提高算法的精确度。</p>
                </div>
                <div class="p1">
                    <p id="44">Adaboost/Adaboost.M1算法假设基分类器的分类精度比随机预测精度更高 (即假设基分类器分类精度高于50%) , 因此, Adaboost.M1算法只适用于预测分类结果只有2种可能的情况 (如+1与-1) 。使用<i>X</i>×<i>Y</i>映射的Adaboost.M2算法能够解决该问题。但是, 上述2种算法均易受到噪音信号的干扰, 并且由于每次关注易被错误分类的训练数据, 导致算法容易出现过拟合现象。</p>
                </div>
                <h4 class="anchor-tag" id="45" name="45">1.3 Random Subspace算法</h4>
                <div class="p1">
                    <p id="46">Random Subspace算法<citation id="118" type="reference"><link href="19" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>与Bagging算法相反, Bagging算法在每次迭代过程中对数据进行重新采样, 而Random Subspace算法则重新选择属性子集。在通常情况下, 训练数据由属性与类别信息构成, 分类模型的建立过程即找到由训练数据的属性信息到相应类别的映射关系。在每次迭代中, 该算法从全部特征空间<i>m</i>中随机选择属性子集<i>f</i>从而构成<i>m</i>.<i>f</i>, 决策树由新的训练数据集训练得出。实验结果表明, Random Subspace算法的精度介于Bagging算法与Adaboost算法之间, 但多样性不及以上2种算法。</p>
                </div>
                <h4 class="anchor-tag" id="47" name="47">1.4 随机森林算法</h4>
                <div class="p1">
                    <p id="48">随机森林算法<citation id="119" type="reference"><link href="21" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>于2001年被提出, 至今, 其仍是集成学习领域被广泛应用与研究的算法之一。随机森林算法将Bagging算法与Random Subspace算法进行融合<citation id="120" type="reference"><link href="23" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>。在文献<citation id="121" type="reference">[<a class="sup">10</a>]</citation>中, 主要讨论|<i>f</i>|=1、|<i>f</i>|=int (lb|<i>m</i>|) +1这2种情况下随机森林与其他算法的性能对比。实验结果表明, 在大部分情况下, |<i>f</i>|=int (lb|<i>m</i>|) +1能获得更高的分类精度。但是, 由于算法所取特征子空间<i>f</i>的长度并不能随特征空间<i>m</i>成比例变化, 假设选择低维数据实例组成训练数据集, 属性全集<i>m</i>中共有4个属性 (int (lb 4) +1=3) , 则在决策树建立过程中会选择包含3个属性的子集作为分裂属性, 其包含全部属性的75%, 即得到的决策森林中基分类器间的互相关指数很高, 每个“专家”分析问题的角度都很相似, 即使每个“专家”的水平都很高, 该分类器所得结果的可信度也较低。因此, 基分类器之间的差异也会作为集成分类器的评判标准之一, 即前文提到的多样性。当|<i>m</i>|=4时决策森林的多样性较差, 同时, 若属性全集<i>m</i>中含有150个属性, 则int (lb 150) +1=8, 子集中只包含整体特征空间中约5%的属性, 该集成分类器的多样性很强, 但可能会漏掉某些分类能力很强的属性。本文引入属性惩罚项, 提出一种改进的随机森林算法。该算法降低<i>f</i>中出现重复属性的概率, 以提升集成分类器的多样性。</p>
                </div>
                <h4 class="anchor-tag" id="49" name="49">1.5 旋转森林算法</h4>
                <div class="p1">
                    <p id="50">旋转森林算法与随机森林算法的实现过程很相似。不同之处在于, 旋转森林使用PCA (Principal Component Analysis) 为每次迭代选取特征子空间。PCA算法可以将高维数据投影到低维, 因为不能向Gain Ratio与Gini Index提供有关熵的信息, 所以其并不适合作为分裂过程所使用的算法。此外, 旋转森林算法还易受到噪声信号的干扰。</p>
                </div>
                <h4 class="anchor-tag" id="51" name="51">1.6 动态随机森林算法</h4>
                <div class="p1">
                    <p id="52">动态随机森林算法<citation id="122" type="reference"><link href="25" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>是一种改进的随机森林算法, 其基于一种改进的Boosting算法。动态随机森林与Adaboost相比, 后者在每次迭代中只关注上次迭代<i>i</i>-1中被分类错误的训练数据, 而前者中用到的改进Boosting算法根据之前每次迭代的分类结果为训练数据集实例重新分配权值。</p>
                </div>
                <h3 id="53" name="53" class="anchor-tag">2 PRF算法</h3>
                <div class="p1">
                    <p id="54">PRF算法引入惩罚项因子, 使同一属性被重复选择的概率大幅降低。在决策树模型中, 深度更低的节点所用到的属性能获得更高的惩罚度 (其权值<i>w</i><sub><i>i</i></sub>更低) 。这样设计的原因是, 深度越低的节点所使用的分裂属性对整个决策树模型结构变化的影响越大, 本文设置更高的惩罚度能降低其在下次迭代过程中出现的概率, 以此保证集成算法的多样性, 最终提高分类结果的可信度。PRF算法具体步骤如下:</p>
                </div>
                <div class="p1">
                    <p id="55"><b>步骤1</b> 对原始数据集<i>D</i>进行Bootstrap采样, 得到新的训练子集<i>D</i><sub><i>i</i></sub>。该步骤有利于提高集成学习算法的多样性<citation id="123" type="reference"><link href="27" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="56"><b>步骤2</b> 根据Bootstrap采样结果<i>D</i><sub><i>i</i></sub>建立决策树模型, 此处选择C4.5算法<citation id="124" type="reference"><link href="29" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>作为基分类器。与C4.5不同, PRF算法在生成每个节点时, 使用<i>w</i><sub><i>i</i></sub><i>f</i><sub><i>i</i></sub>替代<i>f</i><sub><i>i</i></sub>来选择分裂属性。该算法与Adaboost算法的不同之处在于其在每次迭代中对属性值的权值进行重新分配。<i>w</i><sub><i>i</i></sub>初始值设为1, 即算法在初次迭代时等同于Bagging算法。</p>
                </div>
                <div class="p1">
                    <p id="57"><b>步骤3</b> 更新属性权值<i>w</i><sub><i>i</i></sub>, 该值由属性所在节点的深度<i>λ</i>决定。若该节点为根节点, 则<i>λ</i>=1, 以此类推。通过<i>λ</i>可得到<i>w</i><sub><i>i</i></sub>的取值范围<i>WR</i><sup><i>λ</i></sup>为:</p>
                </div>
                <div class="p1">
                    <p id="58" class="code-formula">
                        <mathml id="58"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>W</mi><mi>R</mi><msup><mrow></mrow><mi>λ</mi></msup><mo>=</mo><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mo stretchy="false">[</mo><mn>0</mn><mo>, </mo><mtext>e</mtext><msup><mrow></mrow><mrow><mo>-</mo><mfrac><mn>1</mn><mi>λ</mi></mfrac></mrow></msup><mo stretchy="false">]</mo><mo>, </mo><mi>λ</mi><mo>=</mo><mn>1</mn></mtd></mtr><mtr><mtd><mo stretchy="false">[</mo><mtext>e</mtext><msup><mrow></mrow><mrow><mo>-</mo><mfrac><mn>1</mn><mrow><mi>λ</mi><mo>-</mo><mn>1</mn></mrow></mfrac></mrow></msup><mo>+</mo><mi>ρ</mi><mo>, </mo><mtext>e</mtext><msup><mrow></mrow><mrow><mo>-</mo><mfrac><mn>1</mn><mi>λ</mi></mfrac></mrow></msup><mo stretchy="false">]</mo><mo>, </mo><mi>λ</mi><mo>&gt;</mo><mn>1</mn></mtd></mtr></mtable></mrow></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="59">对于函数e<sup>-<i>x</i></sup> (<i>x</i>&gt;0) 而言, 函数值在开始时会迅速降低, 随后降低速度逐渐放缓, 这也符合本文对算法的要求, 即对深度<i>λ</i>较低的节点所使用的分裂属性设置较高的惩罚度。<i>ρ</i>的设置是为防止相邻深度所得到的<i>WR</i>值存在交叉, 本文中<i>ρ</i>的值设为0.000 01。不同深度的<i>WR</i>值如表1所示。</p>
                </div>
                <div class="area_img" id="60">
                    <p class="img_tit"><b>表1 不同深度下权值因子的取值范围</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="60" border="1"><tr><td><br />深度<i>λ</i></td><td><i>WR</i><sup><i>λ</i></sup><sub>low</sub></td><td><i>WR</i><sup><i>λ</i></sup><sub>high</sub></td></tr><tr><td><br />1</td><td>0.000</td><td>0.368</td></tr><tr><td><br />2</td><td>0.368</td><td>0.607</td></tr><tr><td><br />3</td><td>0.607</td><td>0.717</td></tr><tr><td><br />︙</td><td>︙</td><td>︙</td></tr><tr><td><br />10</td><td>0.895</td><td>0.904</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="61">本文通过<i>t</i>次Bootstrap采样得到<i>t</i>个训练数据集<i>D</i><sub><i>i</i></sub> (<i>i</i>=1, 2, …, <i>t</i>) , 在每个训练数据集上建立C4.5决策树模型<i>T</i><sub><i>i</i></sub> (<i>i</i>=1, 2, …, <i>t</i>) , 在<i>T</i><sub><i>i</i>-1</sub>属性上出现的权值会被惩罚, 权值<i>w</i><sub><i>i</i></sub>从<i>WR</i><sup><i>λ</i></sup>中随机抽样取得。在建立下个决策树<i>T</i><sub><i>i</i>+1</sub>时, 未在<i>T</i><sub><i>i</i>-1</sub>中出现的属性权值<i>w</i><sub><i>i</i></sub>会保持不变。这一特性使得决策森林模型中出现2棵相似决策树的概率大幅降低, 从而提高了算法的多样性。</p>
                </div>
                <div class="p1">
                    <p id="62"><b>步骤4</b> 对未在上个决策树中出现的属性, 逐渐增加其属性权值, 这样能够有效防止具有很强分类能力的属性一旦被一个决策树模型所选择, 其在之后的决策树中便几乎不可能出现的现象。保持未在<i>T</i><sub><i>i</i>-1</sub>中出现的属性权值不变具有很重要的意义, 但是同样会带来负面影响。例如, 如果一个属性出现在<i>T</i><sub><i>i</i>-1</sub>的根节点中, 它会被步骤3分配一个很低的权值 (可能是0) 。在之后决策树的建立过程中, 该属性几乎不可能再出现。解决该问题的方法是, 对于已经被惩罚过的属性 (<i>w</i><sub><i>i</i></sub>&lt;1) , 若其在<i>T</i><sub><i>i</i>-1</sub>中未出现, 便在迭代后逐渐增加其权值<i>w</i><sub><i>i</i></sub>。具体来说, 如果属性<i>A</i><sub><i>i</i></sub>在决策树<i>T</i><sub><i>i</i>-1</sub>的深度为<i>λ</i>的节点中, 在<i>T</i><sub><i>i</i>-1</sub>建立后其权值为<i>w</i><sub><i>i</i></sub>, 决策树<i>T</i><sub><i>i</i>-1</sub>的高度为<i>η</i> (高度与决策树中节点深度最大值相等) , 则令属性权值<i>w</i><sub><i>i</i></sub>=<i>w</i><sub><i>i</i></sub>+<i>σ</i><sub><i>i</i></sub>。权值<i>w</i><sub><i>i</i></sub>的增加量<i>σ</i><sub><i>i</i></sub>定义如下:</p>
                </div>
                <div class="p1">
                    <p id="63" class="code-formula">
                        <mathml id="63"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>σ</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mfrac><mrow><mn>1</mn><mo>-</mo><mi>w</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mrow><mi>η</mi><mo>+</mo><mn>1</mn><mo>-</mo><mi>λ</mi></mrow></mfrac></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="64">在步骤4中, 如果更新后的<i>w</i><sub><i>i</i></sub>满足<i>w</i><sub><i>i</i></sub>&gt;1, 则令<i>w</i><sub><i>i</i></sub>=1。更新后的<i>w</i><sub><i>i</i></sub>会被应用到决策树<i>T</i><sub><i>i</i>+1</sub>的建立过程中。</p>
                </div>
                <h3 id="65" name="65" class="anchor-tag">3 实验结果与分析</h3>
                <h4 class="anchor-tag" id="66" name="66">3.1 UCI机器学习数据库</h4>
                <div class="p1">
                    <p id="67">UCI机器学习数据库是加州理工大学欧文分校整理、由各行各业的大数据信息组成并提供维护的公共机器学习数据库, 其涵盖领域广泛, 包括航空、航天、医疗、材料等行业, 供高校学者科研使用。本文使用UCI数据库以验证算法的分类性能, 表2所示为UCI数据库中数据集的具体信息。</p>
                </div>
                <div class="area_img" id="68">
                    <p class="img_tit"><b>表2 UCI数据库信息</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="68" border="1"><tr><td>数据集</td><td>样本数</td><td>属性数</td><td>类别数</td></tr><tr><td>Heart</td><td>270</td><td>13</td><td>2</td></tr><tr><td><br />Breast cancer</td><td>699</td><td>9</td><td>2</td></tr><tr><td><br />Ionosphere</td><td>351</td><td>34</td><td>7</td></tr><tr><td><br />Diabetes</td><td>768</td><td>9</td><td>2</td></tr><tr><td><br />Glass</td><td>214</td><td>10</td><td>6</td></tr><tr><td><br />Soybean</td><td>683</td><td>36</td><td>19</td></tr><tr><td><br />Waveform</td><td>5 000</td><td>21</td><td>3</td></tr><tr><td><br />Letters</td><td>20 000</td><td>17</td><td>26</td></tr><tr><td><br />Satellite</td><td>4 435</td><td>37</td><td>6</td></tr><tr><td><br />Shuttle</td><td>43 500</td><td>10</td><td>7</td></tr><tr><td><br />DNA</td><td>3 190</td><td>61</td><td>3</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="69">本次实验共用到11个UCI数据集, 其中, 表2中前6个数据集训练样本数相对较少, 后5个数据集训练样本数较多。属性数是算法进行分类操作的依据, 类别数代表预测结果的种类数。例如, 在就医时医生会通过患者身体的各项指标判断其是否患病, 其中, 身体的各项指标便是属性, 而是否患病则是预测类, 在该例中, 预测类共有2种:患病与不患病。</p>
                </div>
                <h4 class="anchor-tag" id="70" name="70">3.1.1 预测精度</h4>
                <div class="p1">
                    <p id="71">预测精度是评价分类器模型性能优劣的重要指标之一, 由于本文所用的实验数据集训练样本数远小于现有机器学习领域所用到的大数据量, 因此本文使用10-fold交叉验证方法得到模型的预测精度。在下文实验中的智能家居数据集CASAS, 其训练样本数大于100万, 可以对其进行直接切分后作为训练数据集与测试数据集。10-fold交叉验证把数据集平均分为10份, 依次选取其中一份作为测试数据集, 其余作为训练数据集, 取10次实验的平均值作为实验结果, 如表3所示。</p>
                </div>
                <div class="area_img" id="72">
                    <p class="img_tit"><b>表3 各集成算法的分类预测错误率比较</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note">%</p>
                    <table id="72" border="1"><tr><td>数据集</td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi>e</mi></mstyle><mrow><mspace width="0.25em" /><mo>-</mo></mrow></mover></mrow><msub><mrow></mrow><mtext>s</mtext></msub></mrow></math></td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi>e</mi></mstyle><mrow><mspace width="0.25em" /><mo>-</mo></mrow></mover></mrow><msub><mrow></mrow><mrow><mtext>B</mtext><mtext>a</mtext><mtext>g</mtext><mtext>g</mtext><mtext>i</mtext><mtext>n</mtext><mtext>g</mtext></mrow></msub></mrow></math></td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi>e</mi></mstyle><mrow><mspace width="0.25em" /><mo>-</mo></mrow></mover></mrow><msub><mrow></mrow><mrow><mtext>A</mtext><mtext>d</mtext><mtext>a</mtext><mtext>b</mtext><mtext>o</mtext><mtext>o</mtext><mtext>s</mtext><mtext>t</mtext></mrow></msub></mrow></math></td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi>e</mi></mstyle><mrow><mspace width="0.25em" /><mo>-</mo></mrow></mover></mrow><msub><mrow></mrow><mrow><mtext>R</mtext><mtext>a</mtext><mtext>n</mtext><mtext>d</mtext><mtext>o</mtext><mtext>m</mtext><mspace width="0.25em" /><mtext>F</mtext><mtext>o</mtext><mtext>r</mtext><mtext>e</mtext><mtext>s</mtext><mtext>t</mtext></mrow></msub></mrow></math></td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi>e</mi></mstyle><mrow><mspace width="0.25em" /><mo>-</mo></mrow></mover></mrow><msub><mrow></mrow><mrow><mtext>Ρ</mtext><mtext>R</mtext><mtext>F</mtext></mrow></msub></mrow></math></td></tr><tr><td>Heart</td><td>23.330</td><td>20.74</td><td>18.89</td><td>19.26</td><td><b>17.41</b></td></tr><tr><td><br />Breast cancer</td><td>6.160</td><td>3.58</td><td>5.01</td><td>4.15</td><td><b>3.43</b></td></tr><tr><td><br />Ionosphere</td><td>10.510</td><td>8.83</td><td>7.69</td><td>7.12</td><td><b>6.84</b></td></tr><tr><td><br />Diabetes</td><td>24.740</td><td><b>24.22</b></td><td>26.82</td><td>24.61</td><td>26.95</td></tr><tr><td><br />Glass</td><td>33.640</td><td>27.57</td><td>28.97</td><td>28.97</td><td><b>26.64</b></td></tr><tr><td><br />Soybean</td><td>15.230</td><td>14.35</td><td>9.66</td><td>8.64</td><td><b>6.59</b></td></tr><tr><td><br />Waveform</td><td>23.580</td><td>17.96</td><td>18.42</td><td>17.62</td><td><b>17.26</b></td></tr><tr><td><br />Letters</td><td>15.785</td><td>10.01</td><td><b>6.02</b></td><td>8.32</td><td>6.75</td></tr><tr><td><br />Satellite</td><td>14.270</td><td>11.36</td><td>11.21</td><td>11.03</td><td><b>10.35</b></td></tr><tr><td><br />Shuttle</td><td>0.060</td><td>0.06</td><td><b>0.01</b></td><td>0.12</td><td>0.03</td></tr><tr><td><br />DNA</td><td>7.020</td><td>6.18</td><td>5.89</td><td>5.24</td><td><b>4.42</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="73">在表3中, <mathml id="74"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi>e</mi></mstyle><mrow><mspace width="0.25em" /><mo>-</mo></mrow></mover></mrow><msub><mrow></mrow><mtext>s</mtext></msub></mrow></math></mathml>代表决策树模型的分类错误率, 每个数据集中集成算法分类错误率最低的数值用粗体标识。由表3可以看出, 在11个数据集中, 8个数据集用本文PRF算法分类时具有最低的分类错误率, 1个数据集用Bagging算法达到最低分类错误率, 2个数据集用Adaboost算法达到最低分类错误率。该结果表明, PRF算法能够取得比经典集成算法更好的分类效果。</p>
                </div>
                <h4 class="anchor-tag" id="75" name="75">3.1.2 Kappa统计量</h4>
                <div class="p1">
                    <p id="76">Kappa统计量<citation id="125" type="reference"><link href="31" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>是评价集成算法性能的一个经典指标。Kappa值用于计算2个分类器间关于预测结果的吻合率, 其计算公式如下:</p>
                </div>
                <div class="p1">
                    <p id="77" class="code-formula">
                        <mathml id="77"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Κ</mi><mi>a</mi><mi>p</mi><mi>p</mi><mi>a</mi><mo>=</mo><mfrac><mrow><mi>Θ</mi><msub><mrow></mrow><mn>1</mn></msub><mo>-</mo><mi>Θ</mi><msub><mrow></mrow><mn>2</mn></msub></mrow><mrow><mn>1</mn><mo>-</mo><mi>Θ</mi><msub><mrow></mrow><mn>2</mn></msub></mrow></mfrac></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="78">其中, <i>Θ</i><sub>1</sub>代表2个分类器实际的吻合率, <i>Θ</i><sub>2</sub>代表2个分类器理论上的吻合率。<i>Θ</i><sub>1</sub>、<i>Θ</i><sub>2</sub>定义如下:</p>
                </div>
                <div class="p1">
                    <p id="79" class="code-formula">
                        <mathml id="79"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>Θ</mi><msub><mrow></mrow><mn>1</mn></msub><mo>=</mo><mfrac><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></munderover><mi>C</mi></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mi>i</mi></mrow></msub></mrow><mi>m</mi></mfrac></mtd></mtr><mtr><mtd><mi>Θ</mi><msub><mrow></mrow><mn>2</mn></msub><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></munderover><mo stretchy="false"> (</mo></mstyle><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></munderover><mrow><mfrac><mrow><mi>C</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow><mi>m</mi></mfrac></mrow></mstyle><mo>×</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></munderover><mrow><mfrac><mrow><mi>C</mi><msub><mrow></mrow><mrow><mi>j</mi><mi>i</mi></mrow></msub></mrow><mi>m</mi></mfrac></mrow></mstyle><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="80">其中, <i>L</i>代表训练集的类别数, <i>m</i>代表训练集的样本数, <i>C</i><sub><i>ij</i></sub>代表训练集中被第1个分类器标记为<i>i</i>类同时被第2个分类器标记为<i>j</i>类的样本数。</p>
                </div>
                <div class="p1">
                    <p id="81">Kappa值介于-1与+1之间。由于在算法的实现过程中, 集成学习算法的多样性与精确度往往无法兼顾, Kappa统计量即应运而生。如果某一算法在精确度和多样性中只取其一, 则其Kappa值较低。只有当算法同时兼顾精确度与多样性, 其Kappa值才能接近1。为简单起见, 本文将集成算法与随机分类器进行Kappa检验, 在Letters、Satellite、Shuttle、DNA 4个数据量较大的数据集上进行实验, 结果如图1所示。由图1可以看出, PRF算法与Adaboost算法都具有较高的Kappa值, 且PRF的Kappa平均值比Adaboost更高, 即PRF算法具有最优的分类性能。</p>
                </div>
                <div class="area_img" id="82">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201905024_082.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 不同集成分类算法的Kappa值对比" src="Detail/GetImg?filename=images/JSJC201905024_082.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图1 不同集成分类算法的Kappa值对比</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201905024_082.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="83" name="83">3.1.3 噪声鲁棒性</h4>
                <div class="p1">
                    <p id="84">在现实生活中, 从自然界获得的数据往往存在噪声, 因此, 需要算法在噪声环境下依然具有较高的预测精度。本文在DNA数据集上进行噪声实验, 表4所示为DNA数据集的构成信息。DNA只有A、C、G、T 4种密码子, 因此, 可以通过随机改变基因序列的密码子类型来改变DNA序列, 从而达到添加噪声的目的。各算法的噪声实验结果如图2所示。</p>
                </div>
                <div class="area_img" id="85">
                    <p class="img_tit"><b>表4 DNA数据集信息</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="85" border="1"><tr><td>序号</td><td>密码子1</td><td>密码子2</td><td>…</td><td>密码子60</td><td>类</td></tr><tr><td>1</td><td>C</td><td>C</td><td>…</td><td>G</td><td>EI</td></tr><tr><td><br />2</td><td>A</td><td>0</td><td>…</td><td>C</td><td>EI</td></tr><tr><td><br />︙</td><td>︙</td><td>︙</td><td></td><td>︙</td><td>︙</td></tr><tr><td><br />3 190</td><td>A</td><td>G</td><td>…</td><td>T</td><td>N</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="86">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201905024_086.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 不同算法在噪声环境下的预测误差对比" src="Detail/GetImg?filename=images/JSJC201905024_086.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图2 不同算法在噪声环境下的预测误差对比</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201905024_086.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="87">由图2可以看出, PRF算法在不同噪声环境下的预测精度均优于其他集成学习算法。因此, PRF算法具有良好的噪声鲁棒性。</p>
                </div>
                <h4 class="anchor-tag" id="88" name="88">3.1.4 运行时间</h4>
                <div class="p1">
                    <p id="89">算法运行时间也是评价其性能优劣的重要指标之一。本文在4个数据量较大的数据集上分别进行实验, 10-fold交叉验证运行时间结果如图3所示。</p>
                </div>
                <div class="area_img" id="90">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201905024_090.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 不同算法运行时间对比" src="Detail/GetImg?filename=images/JSJC201905024_090.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图3 不同算法运行时间对比</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201905024_090.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="91">由图3可以看出, 本文PRF算法虽然具有较高的精度, 但是其模型建立时间以及算法预测阶段的执行时间远高于其他算法。</p>
                </div>
                <h4 class="anchor-tag" id="92" name="92">3.2 CASAS智能家居数据库</h4>
                <div class="p1">
                    <p id="93">CASAS是美国华盛顿大学建立的智能家居项目, 该项目所建立的智能家居系统借助传感器网络以及计算机处理技术观测用户当前正在进行的活动, 从而为其制定个性化服务策略或者检测住户的健康水平。CASAS是一个公共数据平台, 其数据集格式如表5所示, 数据集中所使用的传感器网络布局如图4所示。</p>
                </div>
                <div class="area_img" id="94">
                    <p class="img_tit"><b>表5 传感器网络数据结构</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="94" border="1"><tr><td>年</td><td>月</td><td>日</td><td>时</td><td>分</td><td>秒</td><td>毫秒</td><td>传感器编号</td><td>用户状态</td></tr><tr><td>2009</td><td>9</td><td>25</td><td>12</td><td>49</td><td>51</td><td>47 872</td><td>T001_22</td><td>Idle</td></tr><tr><td><br />2009</td><td>9</td><td>25</td><td>12</td><td>49</td><td>51</td><td>75</td><td>T003_22</td><td>Personal_<br />Hygiene_end</td></tr><tr><td><br />2009</td><td>9</td><td>25</td><td>12</td><td>49</td><td>51</td><td>7 914</td><td>T005_23</td><td>Idle</td></tr><tr><td><br />2009</td><td>9</td><td>25</td><td>12</td><td>49</td><td>51</td><td>83 278</td><td>T002_22</td><td>Idle</td></tr><tr><td><br />2009</td><td>9</td><td>25</td><td>12</td><td>49</td><td>51</td><td>87 213</td><td>T004_24</td><td>Idle</td></tr><tr><td><br />2009</td><td>9</td><td>25</td><td>12</td><td>49</td><td>51</td><td>9 297</td><td>M025_OFF</td><td>Personal_<br />Hygiene_end</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="95">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201905024_095.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 传感器网络1" src="Detail/GetImg?filename=images/JSJC201905024_095.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图4 传感器网络1</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201905024_095.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="96" name="96">3.2.1 决策树数目</h4>
                <div class="p1">
                    <p id="97">PRF算法中的参数<mathml id="98"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>|</mo><mi>Τ</mi><mo>|</mo></mrow></mrow></math></mathml>代表其决策树数目。本节讨论<i>PRF</i>在不同<mathml id="99"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>|</mo><mi>Τ</mi><mo>|</mo></mrow></mrow></math></mathml>时的预测误差变化, 实验结果如图5所示。</p>
                </div>
                <div class="area_img" id="100">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201905024_100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 决策树数目对算法预测误差的影响" src="Detail/GetImg?filename=images/JSJC201905024_100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图5 决策树数目对算法预测误差的影响</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201905024_100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="101">从图5可以看出, 决策树数目越大, PRF算法的预测精度越高。但应注意, 由于该算法的运行时间较长, 因此应避免使用过于庞大的决策树数目。在算法实际的使用过程中, 本文推荐决策树数目设置为10, 该情况可以达到较好的分类效果。</p>
                </div>
                <h4 class="anchor-tag" id="102" name="102">3.2.2 传感器网络布局</h4>
                <div class="p1">
                    <p id="103">为探究传感器网络布局对算法性能的影响, 进行2组实验, 图6中的传感器网络2在传感器网络1的基础上增加了几组运动传感器, 如<i>M</i><sub>47</sub>、<i>M</i><sub>48</sub>等, 图7中的传感器网络3在传感器网络1的基础上改变了网络的局部密集程度而不改变传感器的种类及数目, 实验结果如图8所示。</p>
                </div>
                <div class="area_img" id="104">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201905024_104.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 传感器网络2" src="Detail/GetImg?filename=images/JSJC201905024_104.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图6 传感器网络2</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201905024_104.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="105">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201905024_105.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 传感器网络3" src="Detail/GetImg?filename=images/JSJC201905024_105.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图7 传感器网络3</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201905024_105.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="106">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201905024_106.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 不同传感器网络下算法预测误差对比" src="Detail/GetImg?filename=images/JSJC201905024_106.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图8 不同传感器网络下算法预测误差对比</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201905024_106.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="107">由图8可以看出, 在传感器网络均匀分布时, 其具有较高的预测精度, 在此基础上增加传感器密度, 预测精度虽有所提高, 但效果并不明显。但是, 如果场景中的传感器网络分布稀疏不均, 其预测精度会大幅降低, 原因是传感器网络中存在“盲区”, 用户在场景中的部分行为不能被及时准确地捕捉。因此, 在传感器的布置过程中, 应根据其型号与性能, 尽量做到均匀分布, 以发挥PRF算法的性能优势。</p>
                </div>
                <h3 id="108" name="108" class="anchor-tag">4 结束语</h3>
                <div class="p1">
                    <p id="109">本文提出一种集成分类算法PRF, 该算法引入惩罚项因子, 使用训练数据集属性权值再分配的方法重新建立新的决策树, 从而兼顾算法的精确度与多样性。实验结果表明, 与Bagging算法、Adaboost算法相比, 该算法具有较高的分类精确度与噪声鲁棒性。但是, 由于PRF算法需要在每次迭代中重新对属性权值进行分配, 因此其运行时间较长, 且该算法每次迭代都依赖上一次迭代的实验结果, 导致其很难在并行计算机群上运行。因此, 提高算法的运行速度并开发出可以并行运行的集成分类算法, 将是下一步的研究方向。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201711002&amp;v=MjE0NDBESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSb0Z5M2xXN3pNTHo3QmJiRzRIOWJOcm85RlpvUUs=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> 宋朋涛, 李超, 徐莉婷, 等.基于个人计算机的智能家居边缘计算系统[J].计算机工程, 2017, 43 (11) :1-7.
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14061600061881&amp;v=MTU5NjlMbklKVjRjYVJZPU5pZk9mYks4SHRmTnFZOUZaTzBPQkhRNG9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVQ==&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> ZHANG Le, SUGANTHAN P N.Random forests with ensemble of feature spaces[J].Pattern Recognition, 2014, 47 (10) :3429-3437.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A novel approach on ensemble classifiers with fast rotation forest algorithm">

                                <b>[3]</b> GOPIKA D, AZHAGUSUNDARI B.A novel approach on ensemble classifiers with fast rotation forest algorithm[J].International Journal of Innovative Research in Computer and Communication Engineering, 2014, 2 (8) :25-30.
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201703051&amp;v=MDUyODZHNEg5Yk1ySTlBWllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJvRnkzbFc3ek1MejdCYmI=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b> 周文谊, 王吉源.一种模糊森林学习方法及其行人检测应用[J].计算机工程, 2017, 43 (3) :304-308, 315.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00001339482&amp;v=MTcxNjJITnJJeE1ZT01OWTNrNXpCZGg0ajk5U1hxUnJ4b3hjTUg3UjdxZWJ1ZHRGQzdsVnIvQUkxbz1OajdCYXJPNEh0&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b> BREIMAN L.Bagging predictors[J].Machine Learning, 1996, 24 (2) :123-140.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Data mining:concepts and techniques">

                                <b>[6]</b> HAN Jiawei.Data mining:concepts and techniques[M].San Francisco, USA:Morgan Kaufmann Publishers Inc., 2005.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJWD&amp;filename=SJWD00001347916&amp;v=MDIzNTJidWR0RkM3bFZyL0FJMW89TmlmY2FyTzRIdEhOckl0Q2Jlb0pZM2s1ekJkaDRqOTlTWHFScnhveGNNSDdSN3Fl&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b> LOH W Y.Classification and regression trees[J].Wiley Interdisciplinary Reviews Data Mining and Knowledge Discovery, 2011, 1 (1) :14-23.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Experiments with a new boosting algorithm">

                                <b>[8]</b> FREUND Y, SCHAPIRE R E.Experiments with a new boosting algorithm[EB/OL].[2017-12-15].https://cseweb.ucsd.edu/～yfreund/papers/boostingexperiments.pdf.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The random subspace method for constructing decision forests">

                                <b>[9]</b> HO T K.The random subspace method for constructing decision forests[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 1998, 20 (8) :832-844.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_10" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00001340271&amp;v=MjE0OTVsVnIvQUkxbz1OajdCYXJPNEh0SE5ySXRGWnV3T1kzazV6QmRoNGo5OVNYcVJyeG94Y01IN1I3cWVidWR0RkM3&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[10]</b> BREIMAN L.Random forests[J].Machine Learning, 2001, 45 (1) :5-32.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_11" target="_blank" href="/kcms/detail/detail.aspx?dbcode=STJD&amp;filename=STJD14121800001217&amp;v=MTAzNDZQTnA0OUZaT3NPRG4wK29CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVUxuSUpWNGNhUlk9TmpuQmFySzhIOQ==&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[11]</b> FAWAGREH K, GABER M M, ELYAN E.Random forests:from early developments to recent advancements[J].Systems Science and Control Engineering, 2014, 2 (1) :602-609.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_12" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012300413207&amp;v=MTAwOTNUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VTG5JSlY0Y2FSWT1OaWZPZmJLN0h0RE9ySTlGWU9vTURudytvQk1UNg==&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[12]</b> BERNARD S, ADAM S, HEUTTE L.Dynamic random forests[J].Pattern Recognition Letters, 2012, 33 (12) :1580-1586.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_13" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011600738682&amp;v=MDk1MzcrZ0hDblE3b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VTG5JSlY0Y2FSWT1OaWZPZmJLN0h0RE5xWTlGWQ==&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[13]</b> MARTÍNEZ-MUÑOZ G, SUÁREZ A.Out-of-bag estima-tion of the optimal sample size in Bagging[J].Pattern Recognition, 2010, 43 (1) :143-152.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=C4.5:programs for machine learning">

                                <b>[14]</b> QUINLAN J R.C4.5:programs for machine learning[M].San Francisco, USA:Morgan Kaufmann Publishers Inc., 1992.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Pruning adaptive boosting">

                                <b>[15]</b> MARGINEANTU D D, DIETTERICH T G.Pruning adaptive boosting[C]//Proceedings of the 14th Interna-tional Conference on Machine Learning.San Francisco, USA:Morgan Kaufmann Publishers Inc., 1997:211-218.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJC201905024" />
        <input id="dpi" type="hidden" value="600" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201905024&amp;v=MTM3NzE0TzN6cXFCdEdGckNVUkxPZVplUm9GeTNsVzd6TUx6N0JiYkc0SDlqTXFvOUhZSVFLREg4NHZSNFQ2ajU=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
