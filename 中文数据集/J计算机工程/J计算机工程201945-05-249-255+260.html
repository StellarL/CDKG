<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637130533365342500%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJC201905041%26RESULT%3d1%26SIGN%3daj3WD14HINHCI03X6rptS82EFAw%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201905041&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201905041&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201905041&amp;v=MDkyMzBSb0Z5M21WcnJQTHo3QmJiRzRIOWpNcW85QlpZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmU=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#41" data-title="0 概述 ">0 概述</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#46" data-title="1 相关工作 ">1 相关工作</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#51" data-title="2 MIDI文件与复音音乐 ">2 MIDI文件与复音音乐</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#52" data-title="2.1 MIDI文件">2.1 MIDI文件</a></li>
                                                <li><a href="#54" data-title="2.2 复音音乐结构">2.2 复音音乐结构</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#60" data-title="3 基于字符的LSTM网络 ">3 基于字符的LSTM网络</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#61" data-title="3.1 递归神经网络与LSTM">3.1 递归神经网络与LSTM</a></li>
                                                <li><a href="#64" data-title="3.2 文本描述">3.2 文本描述</a></li>
                                                <li><a href="#76" data-title="3.3 输入输出向量表示">3.3 输入输出向量表示</a></li>
                                                <li><a href="#81" data-title="3.4 网络模型训练">3.4 网络模型训练</a></li>
                                                <li><a href="#119" data-title="3.5 音乐序列生成">3.5 音乐序列生成</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#133" data-title="4 实验结果与分析 ">4 实验结果与分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#136" data-title="4.1 实验设置">4.1 实验设置</a></li>
                                                <li><a href="#139" data-title="4.2 LSTM单元数">4.2 LSTM单元数</a></li>
                                                <li><a href="#143" data-title="4.3 LSTM层数">4.3 LSTM层数</a></li>
                                                <li><a href="#147" data-title="4.4 word尺度">4.4 word尺度</a></li>
                                                <li><a href="#151" data-title="4.5 轨道数量">4.5 轨道数量</a></li>
                                                <li><a href="#155" data-title="4.6 统计特性分析">4.6 统计特性分析</a></li>
                                                <li><a href="#172" data-title="4.7 听感评测">4.7 听感评测</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#178" data-title="5 结束语 ">5 结束语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#56" data-title="&lt;b&gt;图1 单一轨道复音音乐&lt;/b&gt;"><b>图1 单一轨道复音音乐</b></a></li>
                                                <li><a href="#58" data-title="&lt;b&gt;图2 2个轨道的复音音乐&lt;/b&gt;"><b>图2 2个轨道的复音音乐</b></a></li>
                                                <li><a href="#63" data-title="&lt;b&gt;图3 LSTM单元结构&lt;/b&gt;"><b>图3 LSTM单元结构</b></a></li>
                                                <li><a href="#74" data-title="&lt;b&gt;图4 word在五线谱上的表示&lt;/b&gt;"><b>图4 word在五线谱上的表示</b></a></li>
                                                <li><a href="#79" data-title="&lt;b&gt;图5 输入输出向量表示方法示例&lt;/b&gt;"><b>图5 输入输出向量表示方法示例</b></a></li>
                                                <li><a href="#83" data-title="&lt;b&gt;图6 网络模型训练&lt;/b&gt;"><b>图6 网络模型训练</b></a></li>
                                                <li><a href="#121" data-title="&lt;b&gt;图7 音乐序列生成过程&lt;/b&gt;"><b>图7 音乐序列生成过程</b></a></li>
                                                <li><a href="#135" data-title="&lt;b&gt;图8 基于charRNN的复音音乐生成方法流程&lt;/b&gt;"><b>图8 基于charRNN的复音音乐生成方法流程</b></a></li>
                                                <li><a href="#141" data-title="&lt;b&gt;图9 不同LSTM单元数损失函数值对比&lt;/b&gt;"><b>图9 不同LSTM单元数损失函数值对比</b></a></li>
                                                <li><a href="#145" data-title="&lt;b&gt;图10 不同LSTM层数损失函数值对比&lt;/b&gt;"><b>图10 不同LSTM层数损失函数值对比</b></a></li>
                                                <li><a href="#149" data-title="&lt;b&gt;图11 不同word尺度损失函数值对比&lt;/b&gt;"><b>图11 不同word尺度损失函数值对比</b></a></li>
                                                <li><a href="#153" data-title="&lt;b&gt;图12 不同轨道数量损失函数值对比&lt;/b&gt;"><b>图12 不同轨道数量损失函数值对比</b></a></li>
                                                <li><a href="#170" data-title="&lt;b&gt;图13 不同迭代次数下的统计特性&lt;/b&gt;"><b>图13 不同迭代次数下的统计特性</b></a></li>
                                                <li><a href="#171" data-title="&lt;b&gt;图14 真实音乐音高统计特性&lt;/b&gt;"><b>图14 真实音乐音高统计特性</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" title=" 余立功, 卜佳俊, 陈纯.基于内外概率算法的音乐节奏自动生成[J].浙江大学学报 (工学版) , 2005, 39 (12) :1969-1972, 1983." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZDZC200512027&amp;v=MDA1MTA1NE8zenFxQnRHRnJDVVJMT2VaZVJvRnkzbVZyck9QeW5SYmJHNEh0VE5yWTlIWTRRS0RIODR2UjRUNmo=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         余立功, 卜佳俊, 陈纯.基于内外概率算法的音乐节奏自动生成[J].浙江大学学报 (工学版) , 2005, 39 (12) :1969-1972, 1983.
                                    </a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" title=" GRAVES A.Generating sequences with recurrent neural networks[EB/OL].[2018-02-01].https://arxiv.org/pdf/1308.0850.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Generating sequences with recurrent neural networks">
                                        <b>[2]</b>
                                         GRAVES A.Generating sequences with recurrent neural networks[EB/OL].[2018-02-01].https://arxiv.org/pdf/1308.0850.pdf.
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title=" BAUM L E, PETRIE T, SOULES G, et al.A maximization technique occurring in the statistical analysis of probabilistic functions of Markov chains[J].The Annals of Mathematical Statistics, 1970, 41 (1) :164-171." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJST&amp;filename=SJST14112600634565&amp;v=MTQ4NjVycVFUTW53WmVadUh5am1VTG5JSlYwUmJ4UT1OaWZZZXJLOEg5RE9xWTlGWXVnTENYbzhvQk1UNlQ0UFFIL2lyUmRHZQ==&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         BAUM L E, PETRIE T, SOULES G, et al.A maximization technique occurring in the statistical analysis of probabilistic functions of Markov chains[J].The Annals of Mathematical Statistics, 1970, 41 (1) :164-171.
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" title=" HILLER L, ISAACSON L M.Experimental music composition with an electronic computer[M].Westport, USA:Greenwood Publishing Group Inc., 1979." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Experimental music composition with an electronic computer">
                                        <b>[4]</b>
                                         HILLER L, ISAACSON L M.Experimental music composition with an electronic computer[M].Westport, USA:Greenwood Publishing Group Inc., 1979.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" title=" ALLAN M, WILLIAMS C K I.Harmonising chorales by probabilistic inference[C]//Proceedings of the 17th International Conference on Neural Information Processing Systems.Cambridge, USA:MIT Press, 2005:25-32." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Harmonising chorales by probabilistic inference">
                                        <b>[5]</b>
                                         ALLAN M, WILLIAMS C K I.Harmonising chorales by probabilistic inference[C]//Proceedings of the 17th International Conference on Neural Information Processing Systems.Cambridge, USA:MIT Press, 2005:25-32.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" title=" SIMON I, MORRIS D, BASU S.MySong:automatic accompaniment generation for vocal melodies[C]//Proceedings of the SIGCHI Conference on Human Factors in Computing Systems.New York, USA:ACM Press, 2008:725-734." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=My Song:Automatic Accompaniment Generation for Vocal Melodies">
                                        <b>[6]</b>
                                         SIMON I, MORRIS D, BASU S.MySong:automatic accompaniment generation for vocal melodies[C]//Proceedings of the SIGCHI Conference on Human Factors in Computing Systems.New York, USA:ACM Press, 2008:725-734.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" title=" JACOB B L.Algorithmic composition as a model of creativity[J].Organised Sound, 1996, 1 (3) :157-165." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Algorithmic composition as a model of creativity">
                                        <b>[7]</b>
                                         JACOB B L.Algorithmic composition as a model of creativity[J].Organised Sound, 1996, 1 (3) :157-165.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" title=" BILOTTA E, PANTANO P, TALARICO V.Synthetic harmonies :an approach to musical semiosis by means of cellular automata[C]//Proceedings of the 7th International Conference on Artificial Life.Cambridge, USA:MIT Press, 2002:153-159." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Synthetic harmonies an approach to musical semiosis by means of cellular automata">
                                        <b>[8]</b>
                                         BILOTTA E, PANTANO P, TALARICO V.Synthetic harmonies :an approach to musical semiosis by means of cellular automata[C]//Proceedings of the 7th International Conference on Artificial Life.Cambridge, USA:MIT Press, 2002:153-159.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" title=" 王存睿, 段晓东, 刘向东, 等.基于Hilbert映射的元胞自动机音乐生成算法[J].微电子学与计算机, 2010, 27 (1) :5-8." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=WXYJ201001003&amp;v=MjI2Mjk5SE1ybzlGWjRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJvRnkzbVZyck9NalhTWkxHNEg=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                         王存睿, 段晓东, 刘向东, 等.基于Hilbert映射的元胞自动机音乐生成算法[J].微电子学与计算机, 2010, 27 (1) :5-8.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_10" title=" DE LA PUENTE A O, ALFONSO R S, MORENO M A.Automatic composition of music by means of grammatical evolution[C]//Proceedings of Conference on APL.New York, USA:ACM Press, 2002:148-155." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Automatic composition of music by means of grammatical evolution">
                                        <b>[10]</b>
                                         DE LA PUENTE A O, ALFONSO R S, MORENO M A.Automatic composition of music by means of grammatical evolution[C]//Proceedings of Conference on APL.New York, USA:ACM Press, 2002:148-155.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_11" title=" LEWIS J.Algorithms for music composition by neural nets Improved CBR paradigms[EB/OL].[2018-02-04].https://quod.lib.umich.edu/cgi/p/pod/dod-idx/algorithms-for-music-composition.pdf?c=icmc;idno=bbp2372.1989.044;format=pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Algorithms for music composition by neural nets Improved CBR paradigms">
                                        <b>[11]</b>
                                         LEWIS J.Algorithms for music composition by neural nets Improved CBR paradigms[EB/OL].[2018-02-04].https://quod.lib.umich.edu/cgi/p/pod/dod-idx/algorithms-for-music-composition.pdf?c=icmc;idno=bbp2372.1989.044;format=pdf.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_12" title=" ECK D, SCHMIDHUBER J A first look at music composition using lstm recurrent neural networks[EB/OL].[2018-02-04].http://people.idsia.ch/～juergen/blues/IDSIA-07-02.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A first look at music composition using lstm recurrent neural networks">
                                        <b>[12]</b>
                                         ECK D, SCHMIDHUBER J A first look at music composition using lstm recurrent neural networks[EB/OL].[2018-02-04].http://people.idsia.ch/～juergen/blues/IDSIA-07-02.pdf.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_13" title=" LAMBERT A J, WEYDE T, ARMSTRONG N.Perceiving and predicting expressive rhythm with recurrent neural networks[EB/OL].[2018-02-01].http://openaccess.city.ac.uk/16489/." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Perceiving and predicting expressive rhythm with recurrent neural networks">
                                        <b>[13]</b>
                                         LAMBERT A J, WEYDE T, ARMSTRONG N.Perceiving and predicting expressive rhythm with recurrent neural networks[EB/OL].[2018-02-01].http://openaccess.city.ac.uk/16489/.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_14" title=" CHOI K, FAZEKAS G, SANDLER M.Text-based LSTM networks for automatic music composition[EB/OL].[2018-02-01].https://arxiv.org/pdf/1604.05358.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Text-based LSTM networks for automatic music composition">
                                        <b>[14]</b>
                                         CHOI K, FAZEKAS G, SANDLER M.Text-based LSTM networks for automatic music composition[EB/OL].[2018-02-01].https://arxiv.org/pdf/1604.05358.pdf.
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_15" title=" 李雄飞, 冯婷婷, 骆实, 等.基于递归神经网络的自动作曲算法[J].吉林大学学报 (工学版) , 2018, 48 (3) :866-873." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JLGY201803029&amp;v=MTM2MjBMeUhNZDdHNEg5bk1ySTlIYllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJvRnkzbVZyck8=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[15]</b>
                                         李雄飞, 冯婷婷, 骆实, 等.基于递归神经网络的自动作曲算法[J].吉林大学学报 (工学版) , 2018, 48 (3) :866-873.
                                    </a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_16" title=" HOCHREITER S, BENGIO Y, FRASCONI P, et al.Gradient flow in recurrent nets:the difficulty of learning long-term dependencies[EB/OL].[2018-02-01].https://www.bioinf.jku.at/publications/older/ch7.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Gradient flow in recurrent nets:the difficulty of learning long-term dependencies">
                                        <b>[16]</b>
                                         HOCHREITER S, BENGIO Y, FRASCONI P, et al.Gradient flow in recurrent nets:the difficulty of learning long-term dependencies[EB/OL].[2018-02-01].https://www.bioinf.jku.at/publications/older/ch7.pdf.
                                    </a>
                                </li>
                                <li id="35">


                                    <a id="bibliography_17" title=" HOCHREITER S, SCHMIDHUBER J.Long short-term memory[J].Neural Computation, 1997, 9 (8) :1735-1780." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJBK&amp;filename=SJBK15090500014198&amp;v=MDUzOTZvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVVMbklKVjBSYnhRPU5pZkpaYks5SHRqTXFvOUZaT29MRFhVeA==&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[17]</b>
                                         HOCHREITER S, SCHMIDHUBER J.Long short-term memory[J].Neural Computation, 1997, 9 (8) :1735-1780.
                                    </a>
                                </li>
                                <li id="37">


                                    <a id="bibliography_18" title=" LI Rongjian, ZHANG Wenlu, SUK H I, et al.Deep learning based imaging data completion for improved brain disease diagnosis[C]//Proceedings of International Conference on Medical Image Computing and Computer-Assisted Intervention.Berlin, Germany:Springer, 2014:305-312." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep learning based imaging data completion for improved brain disease diagnosis">
                                        <b>[18]</b>
                                         LI Rongjian, ZHANG Wenlu, SUK H I, et al.Deep learning based imaging data completion for improved brain disease diagnosis[C]//Proceedings of International Conference on Medical Image Computing and Computer-Assisted Intervention.Berlin, Germany:Springer, 2014:305-312.
                                    </a>
                                </li>
                                <li id="39">


                                    <a id="bibliography_19" title=" KLERK D D.Equal temperament[J].Acta Musicologica, 1979, 51 (1) :140-150." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJST&amp;filename=SJST6CC1AFE70C7D7EE0DEA0865BA1064288&amp;v=MzExMDBlclhMYmRDOTJmcENaSmdJZUh0TXVoWm5uMDU5UUhubjNtTTBlYlNRUjdLWENPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkNwYlEzNU41aHdidTZ4YTQ9TmlmWQ==&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[19]</b>
                                         KLERK D D.Equal temperament[J].Acta Musicologica, 1979, 51 (1) :140-150.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJC" target="_blank">计算机工程</a>
                2019,45(05),249-255+260 DOI:10.19678/j.issn.1000-3428.0050596            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于charRNN的复音音乐生成方法</b></span>
 <span class="shoufa"></span>                                     </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E6%80%9D%E6%BA%90&amp;code=23682813&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王思源</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%91%A8%E5%BB%BA%E5%9B%BD&amp;code=10141373&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">周建国</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%AD%A6%E6%B1%89%E5%A4%A7%E5%AD%A6%E7%94%B5%E5%AD%90%E4%BF%A1%E6%81%AF%E5%AD%A6%E9%99%A2&amp;code=0009404&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">武汉大学电子信息学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>在音乐生成过程中, charRNN方法只能对单音音乐进行训练, 而不适用于多个乐器合奏的复音音乐。为使charRNN能适用于复音音乐, 提出一种将MIDI音乐转换为一种基于一定语法规则的音乐描述语言的方法。利用charRNN完成文本训练, 得到音乐生成模型, 基于十二平均律方法获得音乐的统计特性, 从而比较不同音乐片段间的差异。实验结果表明, 该方法生成的音乐与真实音乐在结构和听感上比较相似, 可用于多轨道复音音乐的自动生成。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%95%BF%E7%9F%AD%E6%9C%9F%E8%AE%B0%E5%BF%86&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">长短期记忆;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%A4%8D%E9%9F%B3%E9%9F%B3%E4%B9%90&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">复音音乐;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%87%AA%E5%8A%A8%E5%88%9B%E4%BD%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">自动创作;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%AE%A1%E7%AE%97%E6%9C%BA%E9%9F%B3%E4%B9%90&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">计算机音乐;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    王思源 (1994—) , 男, 硕士研究生, 主研方向为多媒体处理、智能信息;E-mail: wsy7906@ qq. com;
                                </span>
                                <span>
                                    周建国, 副教授、博士。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-03-05</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家重点研发计划 (2017YFB0504103);</span>
                    </p>
            </div>
                    <h1><b>Polyphonic Music Generation Method Based on charRNN</b></h1>
                    <h2>
                    <span>WANG Siyuan</span>
                    <span>ZHOU Jianguo</span>
            </h2>
                    <h2>
                    <span>School of Electronic Information, Wuhan University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>In the music generation process, the charRNN method can only train monophonic music, and is not suitable for polyphonic music of multiple instrumental ensembles.In order to make charRNN suitable for polyphonic music, a method of converting MIDI music into a music description language based on certain grammatical rules is proposed.The text training is completed by using charRNN, thus obtaining a music generation model.The statistical properties of the music are obtained based on the theory of twelve-tone temperament method to compare the differences between the different pieces of music.Experimental results show that the music generated by this method is similar to the real music in structure and hearing, and can be used for automatic generation of multi-track polyphonic music.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Long%20Short-Term%20Memory%20(LSTM)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Long Short-Term Memory (LSTM) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=polyphonic%20music&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">polyphonic music;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=automatic%20composition&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">automatic composition;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=deep%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">deep learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=computer%20music&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">computer music;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-03-05</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="41" name="41" class="anchor-tag">0 概述</h3>
                <div class="p1">
                    <p id="42">计算机音乐是指运用计算机生成音符序列, 可辅助作曲者进行音乐写作。自21世纪以来, 影视娱乐节目、动画、游戏等需要大量的原创音乐支持<citation id="180" type="reference"><link href="3" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>, 但专业的音乐制作人数量有限, 音乐制作成本较高, 盗版音乐现象猖獗。因此, 使用计算机辅助作曲者、编曲者创作音乐受到国内外研究者的关注。</p>
                </div>
                <div class="p1">
                    <p id="43">近年来, 深度学习技术逐渐成为科技发展的热点, 如通过递归神经网络 (Recurrent Neural Network, RNN) 处理自然语言。其中, charRNN<citation id="181" type="reference"><link href="5" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>是RNN在自然语言处理中的一种典型应用, 被用来学习一些特定序列的内部规律, 并生成类似序列, 如自动作诗、歌词、小说等。而在数字音乐处理中, 音符可以看成是特殊的字符对其进行处理。</p>
                </div>
                <div class="p1">
                    <p id="44">对于单音音乐, 将音符序列转化成字符序列, 按照传统方法进行训练。然而在日常生活中人们所听到的大部分音乐都是由多种乐器混合出来的复音音乐, 如钢琴、吉他、手风琴等复音乐器, 每次可以同时发出多个声音, 很难直接将乐谱按时间顺序转化成对应的文本序列。charRNN可以学习到音乐内在的逻辑关系。</p>
                </div>
                <div class="p1">
                    <p id="45">为充分发挥charRNN计算成本低、结构简单、训练速度快的优点, 本文提出一种复音音乐生成方法。将乐器数字接口 (Musical Instrument Digital Interface, MIDI) 文件转化为一种音乐描述文本, 利用该描述语言的向量表示训练LSTM网络模型, 根据得到的模型参数进行音乐序列的模拟生成。</p>
                </div>
                <h3 id="46" name="46" class="anchor-tag">1 相关工作</h3>
                <div class="p1">
                    <p id="47">目前, 计算机音乐生成方法主要分为3类:基于概率模型的方法, 基于随机选择与组合的方法和基于深度学习的方法。</p>
                </div>
                <div class="p1">
                    <p id="48">基于概率模型的方法是通过大量真实音乐数据, 得到真实音乐的统计规律, 再利用该规律生成新的音乐。如隐马尔科夫模型 (Hidden Markov Model, HMM) <citation id="182" type="reference"><link href="7" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>, 对于复音音乐来说, 可以用隐含状态<i>S</i>表示和声连接, 观测状态<i>O</i>表示旋律, 能方便地为歌曲配上和声或者给和声配上旋律。文献<citation id="183" type="reference">[<a class="sup">4</a>]</citation>利用马尔科夫模型实现自动作曲, 但仅生成一些字符表示的乐谱文件。文献<citation id="184" type="reference">[<a class="sup">5</a>]</citation>使用赞歌训练隐马尔科夫模型, 并生成新的音乐, 首次将其应用于复音音乐中。文献<citation id="185" type="reference">[<a class="sup">6</a>]</citation>根据已有旋律, 使用隐马尔科夫模型生成对应的伴奏。但马尔科夫状态的演化方式是线性的, 较难学习到长期的依赖关系, 且只适用于数据较少的情况。</p>
                </div>
                <div class="p1">
                    <p id="49">除基于马尔科夫模型的方法外, 国内外学者提出基于随机选择与组合的计算机音乐生成方法。文献<citation id="186" type="reference">[<a class="sup">7</a>]</citation>使用创造力模型来模拟作曲家作曲过程, 但只给出相关概念。文献<citation id="187" type="reference">[<a class="sup">8</a>]</citation>使用元胞自动机的方法来模拟音乐的生成, 由于音乐曲谱和元胞自动机的演化有一定的相似性, 因此使用简单的规则即可在一定程度上模拟音乐的生成过程<citation id="188" type="reference"><link href="19" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>。文献<citation id="189" type="reference">[<a class="sup">10</a>]</citation>利用语法进化和遗传算法来生成旋律。上述方法多数采用随机选择和组合的思想, 将音乐划分成一些固定的片段组合, 然后使用各种随机方法对其进行重新排列, 并根据一定的标准对其进行评价, 逐渐筛选出最优结果。上述方法实现手段不同, 但均是随机生成片段后进行排列和重组, 需大量的实验进行筛选, 稳定性较差。</p>
                </div>
                <div class="p1">
                    <p id="50">近年来, 基于深度学习的方法有不少研究, 其通过神经网络来学习音乐特征, 利用音乐特征生成新的音乐片段。文献<citation id="190" type="reference">[<a class="sup">11</a>]</citation>使用RNN网络进行自动作曲, 同时采用改进式创作方法, 但较难学习到音乐片段间的长期依赖性, 生成的音乐容易陷入到个别音符的循环当中。文献<citation id="191" type="reference">[<a class="sup">12</a>]</citation>使用长短期记忆 (Long Short-Term Memory, LSTM) 网络来学习生成12小节布鲁斯片段, 避免RNN中的梯度消失问题, 可学习到音乐序列中音符之间的长期相关性, 使生成音乐的动机明确, 段落感更强。文献<citation id="192" type="reference">[<a class="sup">13</a>]</citation>利用LSTM生成打击乐轨道。文献<citation id="193" type="reference">[<a class="sup">14</a>]</citation>使用基于词组的LSTM来学习生成爵士和弦和打击乐轨道。然而使用基于词组的LSTM来处理复音音乐问题会导致LSTM网络中状态过多、运算量较大, 在轨道数或声部较多时尤为明显。为简化训练过程和缩短训练时间, 本文将多轨MIDI文件通过一定的编码方式转化成文本, 并利用该文本训练基于字符的LSTM网络。</p>
                </div>
                <h3 id="51" name="51" class="anchor-tag">2 MIDI文件与复音音乐</h3>
                <h4 class="anchor-tag" id="52" name="52">2.1 MIDI文件</h4>
                <div class="p1">
                    <p id="53">MIDI是一种电子通信协议, 将音符的时长、音高、时钟等信息以指令形式表示, 音乐也可以用音频的形式储存下来。在音频文件中, 很难在乐谱的层面上对音乐进行直接编辑。相比于音频文件, MIDI文件是抽象化的音乐序列信息, 有体积小、易编辑等特点, 因此非常适合使用机器学习方法进行训练。MIDI和音频文件的关系, 类似于语音和对应文本的关系。</p>
                </div>
                <h4 class="anchor-tag" id="54" name="54">2.2 复音音乐结构</h4>
                <div class="p1">
                    <p id="55">一首音乐由连续多个小节组成, 其小节数是4或8的倍数。节拍是小节的构成单位, 将小节按时间平均地划分成几等份。同一首音乐中每个小节的节拍数固定, 且决定了音乐的节奏特性。比如最常见的4/4拍, 指将一小节平均分成四份, 其中每一份的长度被定义为一个四分音符。如图1所示, 若这一小节以四分音符为一拍, 前面的二分音符占二分之一小节, 为两拍, 后面的2个四分音符各占一拍, 共计4拍, 符合4/4拍的定义。</p>
                </div>
                <div class="area_img" id="56">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201905041_056.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 单一轨道复音音乐" src="Detail/GetImg?filename=images/JSJC201905041_056.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图1 单一轨道复音音乐</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201905041_056.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="57">同一个乐器演奏的所有音符同属一个轨道。同一轨道的复音是由同一个乐器同时演奏多个音符。从图1可以看出, 该乐谱所示的乐器需要在该小节第4拍同时演奏C2和C3这2个音。复音音乐中的复音现象还包括了由不同乐器在某一时刻同时演奏的情况。图2所示为2个轨道的复音音节, 在第二小节的第一拍, 即箭头所指示的位置, 2个轨道同时演奏一个二分音符。</p>
                </div>
                <div class="area_img" id="58">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201905041_058.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 2个轨道的复音音乐" src="Detail/GetImg?filename=images/JSJC201905041_058.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图2 2个轨道的复音音乐</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201905041_058.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="59">音乐往往具有较强的结构特征和循环性, 其和弦有着特定的模式<citation id="194" type="reference"><link href="31" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>, 如著名的ii-V-I进行、流行乐中的卡农进行和IV-V-iii-vi进行等, 表明音乐存在纵向相关性。另外, 由于复音音乐在同一时刻存在多个音符, 3个以上的音符可以构成一个和弦, 同时和弦的构成存在横向相关性。</p>
                </div>
                <h3 id="60" name="60" class="anchor-tag">3 基于字符的LSTM网络</h3>
                <h4 class="anchor-tag" id="61" name="61">3.1 递归神经网络与LSTM</h4>
                <div class="p1">
                    <p id="62">神经网络是根据生物的神经结构设计的计算模型, 应用于模式识别、函数逼近、数据压缩等领域。由于神经网络固有的梯度消失<citation id="195" type="reference"><link href="33" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>问题, RNN较难学习到时间序列的长期依赖关系, 因此LSTM引入一个具有复杂结构的LSTM单元<citation id="196" type="reference"><link href="35" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>, 通过一个单独的内部状态<i>C</i> (<i>t</i>) 保持长期记忆, 可学习到序列内的长期依赖关系, 其结构如图3所示。</p>
                </div>
                <div class="area_img" id="63">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201905041_063.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 LSTM单元结构" src="Detail/GetImg?filename=images/JSJC201905041_063.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图3 LSTM单元结构</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201905041_063.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="64" name="64">3.2 文本描述</h4>
                <div class="p1">
                    <p id="65">本文获得大量真实音乐的MIDI文件后, 由于不同创作者的习惯不同, 曲风不同, 在配器和编排风格上也不同, 因此从节奏、配器和调性方面对样本进行预处理。</p>
                </div>
                <div class="p1">
                    <p id="66">节奏对音乐的微观结构影响较大, 为使训练样本中各个片段的节奏保持一致, 音乐中4/4拍占绝大多数, 本文只选择4/4拍的音乐作为样本, 剔除掉6/8拍或3/4拍的音乐。不同乐器的编写方式不同, 乐器的选择也对音乐的整体结构影响较大。真实音乐往往有许多不同的配器方法, 而人声、钢琴和贝司分别决定了旋律层、声层和低音层, 对人的听感影响最大。因此, 本文使用音乐制作软件将获取到的原始MIDI文件剪切到只保留主旋律、钢琴和贝司3个轨道, 删除掉打击乐、弦乐等多余的轨道。</p>
                </div>
                <div class="p1">
                    <p id="67">多数成年人在童年时期没有经过特殊的训练, 无法在没有参考音的情况下直接听出声音的频率, 因此也无法听出被奏响音的绝对音高。但成年人在一定的训练下, 可以获得辨识相对音高的能力, 即在已知参考音音高时, 可以根据相对音程推算得到它的绝对音高。由于多数人会认为位于不同调上的同一首音乐听感相同, 使得把所有的音乐移调到同一个调上再作处理成为可能, 即将乐谱归一化。因此, 本文将所有音乐统一调整到了C调。</p>
                </div>
                <div class="p1">
                    <p id="68">由于charRNN的输入为字符序列, 因此需要将MIDI文件转化成文本描述形式。音乐具有节奏性, 如果一小节时间长度为<i>t</i>, 音符的起始点和终止点会落在<i>t</i>/<i>n</i>位置上, 其中, <i>n</i>一般是2次幂或3倍的2次幂。基于上述分析, 可以将<i>t</i>/<i>n</i>作为编码的单位时间, 本文将其定义为一个“word”, 用来表示在一个长度为<i>t</i>/<i>n</i>的一段时间内所有轨道中, 所有被奏响的音符。音乐中所有的音符的时长, 都可以用整数倍数个word表示。例如, 当<i>n</i>=32时, 一个四分音符可以用8个word表示, 一个八分音符可以用4个word表示;当<i>n</i>=16时, 一个四分音符可以用4个word表示, 而一个八分音符可以用2个word表示。</p>
                </div>
                <div class="p1">
                    <p id="69">word的构造方法如下:</p>
                </div>
                <div class="p1">
                    <p id="70"><i>m</i>}<i>p</i><sub>1</sub><i>p</i><sub>2</sub>…<i>p</i><sub><i>m</i></sub>}<i>b</i>      (1) </p>
                </div>
                <div class="p1">
                    <p id="71">其中, <i>m</i>表示在单位时间内主旋律轨道所对应的编码, 一般对应人声, <i>b</i>表示贝司对应的编码, <i>p</i>表示钢琴的编码, }为分隔符。由于钢琴单位时间内可能同时有多个音出现, 因此<i>p</i><sub>1</sub>～<i>p</i><sub><i>m</i></sub>表示从低到高排列的钢琴所有同时奏响的音符。如果音符的结束点和word所对应的时间段结束点重合的话, 在该音符的后面加上“|”字符表示音符的结束。图4所示为word构成。在五线谱的第二小节, <i>n</i>=16可以看作由16个word构成。其中, 第一个word的组成音分别为旋律D4, 钢琴为G3、B3、D4, 贝司G1, 对应的ASCII码分别为旋律“J”、钢琴“CGJ”、“+”, 因此该word的编码结果为:</p>
                </div>
                <div class="p1">
                    <p id="72">J}CGJ}+      (2) </p>
                </div>
                <div class="p1">
                    <p id="73">其中, J对应旋律轨, CGJ对应钢琴轨, +对应贝司轨, }为分隔符。</p>
                </div>
                <div class="area_img" id="74">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201905041_074.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 word在五线谱上的表示" src="Detail/GetImg?filename=images/JSJC201905041_074.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图4 word在五线谱上的表示</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201905041_074.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="75">一首音乐一般由<i>m</i>个小节构成, 而小节的长度固定, 如果每个小节包含<i>n</i>个word, 那么<i>n</i>×<i>m</i>个word构成一首音乐, 且每个word之间用空格字符隔开。当一个word的时长内没有音符奏响时, 该word用“}}”表示。</p>
                </div>
                <h4 class="anchor-tag" id="76" name="76">3.3 输入输出向量表示</h4>
                <div class="p1">
                    <p id="77">由于神经网络输入是向量形式, 文本格式的训练数据无法直接输入到网络中, 本文将训练数据用向量来表示, 输入音符<i>x</i><sub><i>n</i></sub>转化为输入向量<b><i>X</i></b><sub><i>n</i></sub>。各个音符之间没有明确的数量关系, 对其采用One-hot编码。MIDI音符的编号共有128个, 如果经过编码后的输入向量<b><i>X</i></b><sub><i>n</i></sub>共有128维, 那么步数为<i>m</i>的输入数据的一个batch可表示为[<i>x</i><sub>1</sub>, <i>x</i><sub>2</sub>, …, <i>x</i><sub><i>m</i></sub>]。</p>
                </div>
                <div class="p1">
                    <p id="78">为使LSTM网络可以学习到音符、小节乃至乐段之间的相关性, 用<b><i>X</i></b><sub><i>n</i></sub>所对应音符在训练数据中相邻的下一个音符的向量<b><i>X</i></b><sub><i>n</i>+1</sub>作为输出向量<b><i>Y</i></b>, 一个步数为<i>m</i>的输出batch可表示为[<i>x</i><sub>2</sub>, <i>x</i><sub>3</sub>, …, <i>x</i><sub><i>m</i></sub><sub>+1</sub>], 其具体示例如图5所示。其中, 所有构成的输入输出样本对已经使用实斜线连接, 当<i>m</i>等于10时, 输出batch截取为输入batch向后偏移一步的等长片段。</p>
                </div>
                <div class="area_img" id="79">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201905041_079.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 输入输出向量表示方法示例" src="Detail/GetImg?filename=images/JSJC201905041_079.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图5 输入输出向量表示方法示例</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201905041_079.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="80">经过向量化表示后, 标注后的描述文本可以表示为若干组128维输入向量<b><i>X</i></b>和128维输出向量<b><i>Y</i></b>构成的样本对。</p>
                </div>
                <h4 class="anchor-tag" id="81" name="81">3.4 网络模型训练</h4>
                <div class="p1">
                    <p id="82">本文网络结构由3个主要部分构成, 分别是输入层、隐藏层 (<i>LSTM</i>层) 、输出层。其中, 隐藏层的层数随着实验变量的设置而改变。从128个向量中选取一个作为输出, 因此使用<i>softmax</i>层作为输出层, 同时将输出层与<i>LSTM</i>层进行全连接。将上述步骤中获得输入输出向量作为训练数据输入到<i>LSTM</i>网络中, 反复迭代并计算误差, 将误差反向传播更新<i>LSTM</i>网络的权值参数, 其过程如图6所示。<i>LSTM</i>网络每次读入一个音符作为输入, 且该音符的下一个音符作为输出。</p>
                </div>
                <div class="area_img" id="83">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201905041_083.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 网络模型训练" src="Detail/GetImg?filename=images/JSJC201905041_083.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图6 网络模型训练</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201905041_083.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="84">网络模型训练具体算法描述如下:</p>
                </div>
                <div class="p1">
                    <p id="85"><b>算法1</b> 网络模型建立</p>
                </div>
                <div class="p1">
                    <p id="86"><b>输入</b> 由ASCII字符构成的训练样本</p>
                </div>
                <div class="p1">
                    <p id="87"><b>输出</b> 音乐序列模型参数 (LSTM的权值参数) </p>
                </div>
                <div class="p1">
                    <p id="88">1.随机初始化LSTM模型参数;</p>
                </div>
                <div class="p1">
                    <p id="89">2.将训练数据进行One-Hot编码, 并送入输出层, 进行前向传播;</p>
                </div>
                <div class="p1">
                    <p id="90">3.根据输出层的结果与目标输出进行比较, 对误差进行反向传播;</p>
                </div>
                <div class="p1">
                    <p id="91">4.完成规定的迭代次数, 最终将网络的模型参数输出。</p>
                </div>
                <div class="p1">
                    <p id="92">在前向传播过程中, 输出门的计算公式为:</p>
                </div>
                <div class="p1">
                    <p id="93"><b><i>w</i></b><sup><i>t</i></sup>=<i>σ</i> (∑<b><i>w</i></b><sub><i>iw</i></sub><b><i>x</i></b><mathml id="94"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>t</mi></msubsup></mrow></math></mathml>+∑<b><i>w</i></b><sub><i>hw</i></sub><b><i>b</i></b><mathml id="95"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>h</mi><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msubsup></mrow></math></mathml>+∑<b><i>w</i></b><sub><i>cw</i></sub><b><i>s</i></b><mathml id="96"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>c</mi><mi>t</mi></msubsup></mrow></math></mathml>)      (3) </p>
                </div>
                <div class="p1">
                    <p id="97">最终输出为:</p>
                </div>
                <div class="p1">
                    <p id="98"><b><i>c</i></b><sup><i>t</i></sup>=<b><i>w</i></b><sup><i>t</i></sup>×<i>σ</i> (<b><i>s</i></b><mathml id="99"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>c</mi><mi>t</mi></msubsup></mrow></math></mathml>)      (4) </p>
                </div>
                <div class="p1">
                    <p id="100">其中, <b><i>w</i></b>表示连接各个部分的权值矩阵, 下标表示连接的输入输出, <b><i>x</i></b><mathml id="101"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>t</mi></msubsup></mrow></math></mathml>表示当前时刻的输入, <b><i>s</i></b><mathml id="102"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>c</mi><mi>t</mi></msubsup></mrow></math></mathml>表示当前时刻的单元状态。</p>
                </div>
                <div class="p1">
                    <p id="103">在后向传播的过程中, 状态量为:</p>
                </div>
                <div class="p1">
                    <p id="104">ε<sup>t</sup><sub><i>s</i></sub>=<b><i>b</i></b><mathml id="105"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>w</mi><mi>t</mi></msubsup></mrow></math></mathml><b><i>h</i></b>′ (<b><i>s</i></b><mathml id="106"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>c</mi><mi>t</mi></msubsup></mrow></math></mathml>) <i>ε</i><mathml id="107"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>c</mi><mi>t</mi></msubsup></mrow></math></mathml>+<b><i>b</i></b><mathml id="108"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>φ</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup></mrow></math></mathml><i>ε</i><mathml id="109"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>s</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup></mrow></math></mathml>+<b><i>w</i></b><sub><i>cl</i></sub><i>δ</i><mathml id="110"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>l</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup></mrow></math></mathml>+</p>
                </div>
                <div class="p1">
                    <p id="111"><b><i>w</i></b><sub><i>cφ</i></sub><i>δ</i><mathml id="112"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>φ</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup></mrow></math></mathml>+<b><i>w</i></b><sub><i>cw</i></sub><i>δ</i><mathml id="113"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>w</mi><mi>t</mi></msubsup></mrow></math></mathml>      (5) </p>
                </div>
                <div class="p1">
                    <p id="114">其中, <mathml id="115"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>ε</mi><msubsup><mrow></mrow><mi>c</mi><mi>t</mi></msubsup><mover><mstyle mathsize="140%" displaystyle="true"><mo>=</mo></mstyle><mrow><mi>d</mi><mi>e</mi><mi>f</mi></mrow></mover><mfrac><mrow><mo>∂</mo><mi>L</mi></mrow><mrow><mo>∂</mo><mi mathvariant="bold-italic">b</mi><msubsup><mrow></mrow><mi>c</mi><mi>t</mi></msubsup></mrow></mfrac><mo>, </mo><mi>ε</mi><msubsup><mrow></mrow><mi>s</mi><mi>t</mi></msubsup><mover><mstyle mathsize="140%" displaystyle="true"><mo>=</mo></mstyle><mrow><mi>d</mi><mi>e</mi><mi>f</mi></mrow></mover><mfrac><mrow><mo>∂</mo><mi>L</mi></mrow><mrow><mo>∂</mo><mi mathvariant="bold-italic">s</mi><msubsup><mrow></mrow><mi>c</mi><mi>t</mi></msubsup></mrow></mfrac><mo>, </mo><mi>L</mi></mrow></math></mathml>表示损失函数。</p>
                </div>
                <div class="p1">
                    <p id="116">为提高学习速度, 本文使用交叉熵作为损失函数, 其表示预测结果和真实结果的偏离程度, 表达式为:</p>
                </div>
                <div class="p1">
                    <p id="117" class="code-formula">
                        <mathml id="117"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Η</mi><mo stretchy="false"> (</mo><mi>p</mi><mo>, </mo><mi>q</mi><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>i</mi></munder><mi>p</mi></mstyle><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo><mo>×</mo><mrow><mi>lg</mi></mrow><mfrac><mn>1</mn><mrow><mi>q</mi><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></mfrac><mo>, </mo><mi>i</mi><mo>=</mo><mn>1</mn><mo>, </mo><mn>2</mn><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>m</mi><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>6</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="118">其中, p (i) 表示样本中的真实概率分布, q (i) 表示实际输出的概率分布, m表示所有音符编码后的种类数, 同时将最终迭代次数设置为1 000, 并记录损失函数的值。另外, 需要记录学习到的权值参数, 以便使用该模型生成新的音乐序列。在训练<i>LSTM</i>网络时, 使用<i>dropout</i>策略<citation id="197" type="reference"><link href="37" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>防止过拟合, 且<i>dropout</i>率设置为50%。</p>
                </div>
                <h4 class="anchor-tag" id="119" name="119">3.5 音乐序列生成</h4>
                <div class="p1">
                    <p id="120">训练完后, 使用算法1中生成的模型来自动生成新的音乐序列。LSTM网络每次读入一个音符, 根据该音符和输入的音符推测下一个可能出现的音符, 如图7所示。</p>
                </div>
                <div class="area_img" id="121">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201905041_121.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 音乐序列生成过程" src="Detail/GetImg?filename=images/JSJC201905041_121.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图7 音乐序列生成过程</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201905041_121.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="122">音乐序列生成过程如算法2所示。</p>
                </div>
                <div class="p1">
                    <p id="123"><b>算法2</b> 音乐序列生成</p>
                </div>
                <div class="p1">
                    <p id="124"><b>输入</b> 音乐序列模型参数, 初始序列</p>
                </div>
                <div class="p1">
                    <p id="125"><b>输出</b> 已生成音乐序列<i>S</i></p>
                </div>
                <div class="p1">
                    <p id="126">1.随机选择一个长度为m预定的初始序列作为网络的初始输入;</p>
                </div>
                <div class="p1">
                    <p id="127">2.按顺序取出初始序列I中的一个音符, 通过One-Hot编码后, 作为输入向量X输入到LSTM网络中;</p>
                </div>
                <div class="p1">
                    <p id="128">3.在初始序列I取完后, 将I作为生成序列S的开头部分, 否则返回第2步;</p>
                </div>
                <div class="p1">
                    <p id="129">4.通过正向传播, 从输出层得到输出向量Y, 将Y作为下一次正向传播的输入向量X;</p>
                </div>
                <div class="p1">
                    <p id="130">5.将Y解码为音符y, 并直接添加在已生成序列S的末尾, 若没有到指定的循环次数, 则返回第4步;</p>
                </div>
                <div class="p1">
                    <p id="131">6.输出所获得的生成音乐序列S, 并通过脚本将S转化成MIDI序列作为最终结果。</p>
                </div>
                <div class="p1">
                    <p id="132">算法2中所提到的初始序列既可手动配置, 也可随机生成。和弦序列为作曲者指定, 生成的文本文件通过脚本转化为MIDI文件后, 即可进行播放。</p>
                </div>
                <h3 id="133" name="133" class="anchor-tag">4 实验结果与分析</h3>
                <div class="p1">
                    <p id="134">本文基于charRNN的复音音乐生成方法流程如图8所示。</p>
                </div>
                <div class="area_img" id="135">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201905041_135.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 基于charRNN的复音音乐生成方法流程" src="Detail/GetImg?filename=images/JSJC201905041_135.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图8 基于charRNN的复音音乐生成方法流程</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201905041_135.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="136" name="136">4.1 实验设置</h4>
                <div class="p1">
                    <p id="137">本文实验运行在处理器为Intel i5-6500 3.20 GHz, 内存8 GB的计算机上, 预处理时的音乐编辑软件为Cubase 5.1.2, 实验所使用编程语言为Python 3.6.1和C#, 并使用基于Python的深度学习库tensorflow来完成程序设计。</p>
                </div>
                <div class="p1">
                    <p id="138">训练数据选自MIDI论坛, 所有MIDI文件均为3个轨道, 4/4拍, 且没有转调的流行音乐。在MIDI文件转化为对应的描述文本后, 其长度大约6 MB。本文使用John Walker的Miditocsv开源工具把MIDI文件转化为csv文件, 使用Python脚本将csv文件转化成txt文件进行处理。</p>
                </div>
                <h4 class="anchor-tag" id="139" name="139">4.2 LSTM单元数</h4>
                <div class="p1">
                    <p id="140">LSTM单元数是每一层LSTM层中单元个数, 即网络的宽度。LSTM单元数越多, 学习能力一般会越强。本文采用宽度分别为512、1 024、2 048这3种规格的LSTM层进行实验, 层数选择为2层, 样本采用旋律、钢琴、贝司3个轨道, 时间划分为32 words/bar, LSTM单元数与训练速度的关系如图9所示。</p>
                </div>
                <div class="area_img" id="141">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201905041_141.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图9 不同LSTM单元数损失函数值对比" src="Detail/GetImg?filename=images/JSJC201905041_141.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图9 不同LSTM单元数损失函数值对比</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201905041_141.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="142">从图9可以看出, 当迭代次数为1 000时, 单元数为1 024时可以获得最好的训练效果。迭代次数越多, 网络可能会获得较好的训练效果, 但其提升效果并不明显, 且训练时间较长。因此, 本文设置单元数为1 024。</p>
                </div>
                <h4 class="anchor-tag" id="143" name="143">4.3 LSTM层数</h4>
                <div class="p1">
                    <p id="144">LSTM层数是LSTM网络中隐层层数, 在前馈神经网络中, 层数越多, 收敛的速度越慢, 学习的效果就越好。本文在每层LSTM单元数为1024的情况下, 选取1、2、3层分别进行实验, 结果如图10所示。</p>
                </div>
                <div class="area_img" id="145">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201905041_145.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图10 不同LSTM层数损失函数值对比" src="Detail/GetImg?filename=images/JSJC201905041_145.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图10 不同LSTM层数损失函数值对比</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201905041_145.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="146">从图10可以看出, LSTM层数对实验结果的影响较大。在每层单元数为1 024的情况下, 当层数为1或者2时, 2层实验结果要略好于1层。但当层数为3时, 损失函数下降速度明显变慢, 效果比其他2种层数差。因此, 在使用RNN进行音乐生成的时候, 最好选择双层LSTM网络。</p>
                </div>
                <h4 class="anchor-tag" id="147" name="147">4.4 word尺度</h4>
                <div class="p1">
                    <p id="148">本文分别将MIDI文件转化为尺度为32 words/bar和16 words/bar, 测试2种情况下的文本文件对训练结果的影响。一个小节分配的word越多, 编码后获得的文本文件会越长, 对原来的MIDI文件的描述也更精确, 结果如图11所示。</p>
                </div>
                <div class="area_img" id="149">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201905041_149.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图11 不同word尺度损失函数值对比" src="Detail/GetImg?filename=images/JSJC201905041_149.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图11 不同word尺度损失函数值对比</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201905041_149.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="150">从图11可以看出, 16 words/bar结果优于32 words/bar。在编码时, 乐词使用更大的时间尺度, 会获得更好的实验结果, 但会失去一些细节信息, 如时值较短的音符等, 使得生成的音乐缺少灵活性。</p>
                </div>
                <h4 class="anchor-tag" id="151" name="151">4.5 轨道数量</h4>
                <div class="p1">
                    <p id="152">由于音乐体裁和类型的多样性, 每首音乐所使用的乐器也各有不同。不同的乐器有着不同的编配方式, 因此乐器的选择对训练样本影响较大。本文选择2个轨道进行对比实验, 如图12所示。</p>
                </div>
                <div class="area_img" id="153">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201905041_153.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图12 不同轨道数量损失函数值对比" src="Detail/GetImg?filename=images/JSJC201905041_153.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图12 不同轨道数量损失函数值对比</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201905041_153.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="154">从图12可以看出, 训练样本中所包含的轨道数越多, 训练效果越差, 主要是因为轨道数越多, 和声织体就会越复杂, 生成的文本语法更复杂, 结果与样本之间会产生较大的偏差。</p>
                </div>
                <h4 class="anchor-tag" id="155" name="155">4.6 统计特性分析</h4>
                <div class="p1">
                    <p id="156">十二平均律<citation id="198" type="reference"><link href="39" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>是音乐中的一种定调方法, 将物理频率呈2∶1的2个单音之间的音程划分为 12个半音, 相隔12个半音的2个音称为一个八度, 这2个音互为八度音, 八度音在人类的耳中听感十分相似。十二平均律可表示为:</p>
                </div>
                <div class="p1">
                    <p id="157" class="code-formula">
                        <mathml id="157"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><msub><mrow></mrow><mi>n</mi></msub><mo>=</mo><mi>Ρ</mi><msub><mrow></mrow><mi>a</mi></msub><mo stretchy="false"> (</mo><mroot><mn>2</mn><mrow><mn>1</mn><mn>2</mn></mrow></mroot><mo stretchy="false">) </mo><mo stretchy="false"> (</mo><mi>n</mi><mo>-</mo><mi>a</mi><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>7</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="158">其中, <i>P</i><sub><i>n</i></sub>表示待计算的音符的绝对音高, <i>P</i><sub><i>a</i></sub>表示参考音高, <i>n</i>和<i>a</i>分别表示待计算音符和参考音符从左数起的位次。</p>
                </div>
                <div class="p1">
                    <p id="159">为衡量不同音乐片段的相似性, 本文将音乐中出现的所有音符映射到同一个八度上, 从而可以得到不同音乐的音高统计特性。根据上述理论, 本文对音乐的音高统计特性分析如算法3所示。</p>
                </div>
                <div class="p1">
                    <p id="160"><b>算法3</b> 音高统计算法</p>
                </div>
                <div class="p1">
                    <p id="161"><b>输入</b> 音乐描述文本</p>
                </div>
                <div class="p1">
                    <p id="162"><b>输出</b> 音高统计特性分布</p>
                </div>
                <div class="p1">
                    <p id="163">1.取第一个音符N;</p>
                </div>
                <div class="p1">
                    <p id="164">2.获得N中的音符长度l, 和N对应的MIDI序号k;</p>
                </div>
                <div class="p1">
                    <p id="165">3.获得N分类序号Xi, 方法为Xi=k/12;</p>
                </div>
                <div class="p1">
                    <p id="166">4.音符类别Xi下的总时长T (Xi) 在原有基础上增加l;</p>
                </div>
                <div class="p1">
                    <p id="167">5.取下一个音符N, 执行算法第2步, 直到取完音乐中所有的音符;</p>
                </div>
                <div class="p1">
                    <p id="168">6.其中X<sub>i</sub>的取值0～11分别映射到C, C#, …, B等12种音符类型。通过T (X<sub>i</sub>) /∑T (X<sub>i</sub>) 可以求出各个音符类型所占的比例。</p>
                </div>
                <div class="p1">
                    <p id="169">在十二个音符类别中, 出现频率最高的是C (唱名do) 、D (唱名re) 、E (唱名mi) 、G (唱名so) 、A (唱名la) 5种音, 分别对应民族调式的五声音阶, 这主要由于选取训练样本时使用中文流行歌曲, 而中文音乐大部分都有一定的民族特色, 这5个音占主导地位。本文使用2层LSTM, 且每层设置为1 024个LSTM单元, 迭代过程中的音乐统计特性变化如图13所示。其中, a、b、c、d分别是迭代200次、400次、600次和1 000次后的结果。当迭代次数为200时, 真实音乐音高统计如图14所示, C、E、G、A音占优势, 但几个非自然音阶 (有“#”号) 的音符还没有出现。当迭代次数为400时, 出现部分非自然音阶音符, 同时各种音符之间的比例符合真实分布。当迭代次数为1 000时, 所有的音高均已经出现, 且最接近于真实样本中的分布。本文使用dropout策略防止过拟合, 最终生成的音乐统计特性和真实样本中不完全相同, 但较接近真实分布。</p>
                </div>
                <div class="area_img" id="170">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201905041_170.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图13 不同迭代次数下的统计特性" src="Detail/GetImg?filename=images/JSJC201905041_170.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图13 不同迭代次数下的统计特性</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201905041_170.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="171">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201905041_171.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图14 真实音乐音高统计特性" src="Detail/GetImg?filename=images/JSJC201905041_171.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图14 真实音乐音高统计特性</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201905041_171.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="172" name="172">4.7 听感评测</h4>
                <div class="p1">
                    <p id="173">本文使用准确度 (<i>P</i>) 、精确度 (<i>A</i>) 、召回率 (<i>R</i>) 评估实验结果, 计算公式如下所示:</p>
                </div>
                <div class="p1">
                    <p id="174" class="code-formula">
                        <mathml id="174"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mspace width="0.25em" /><mi>Ρ</mi><mo>=</mo><mfrac><mrow><mi>Τ</mi><mi>Ρ</mi><mo>+</mo><mi>Τ</mi><mi>Ν</mi></mrow><mrow><mi>Τ</mi><mi>Ρ</mi><mo>+</mo><mi>F</mi><mi>Ν</mi><mo>+</mo><mi>F</mi><mi>Ρ</mi><mo>+</mo><mi>Τ</mi><mi>Ν</mi></mrow></mfrac></mtd></mtr><mtr><mtd><mi>A</mi><mo>=</mo><mfrac><mrow><mi>Τ</mi><mi>Ρ</mi></mrow><mrow><mi>Τ</mi><mi>Ρ</mi><mo>+</mo><mi>F</mi><mi>Ρ</mi></mrow></mfrac></mtd></mtr><mtr><mtd><mi>R</mi><mo>=</mo><mfrac><mrow><mi>Τ</mi><mi>Ρ</mi></mrow><mrow><mi>Τ</mi><mi>Ρ</mi><mo>+</mo><mi>F</mi><mi>Ν</mi></mrow></mfrac></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="175">其中, <i>TP</i>表示判断为真、实际为真, <i>TN</i>表示判断为假、实际为假, <i>FP</i>表示判断为真、实际为假, <i>FN</i>表示判断为假、实际为真。</p>
                </div>
                <div class="p1">
                    <p id="176">生成音乐主要分为旋律、低音、和声3个声部, 且具有明确的旋律性。上述方法生成5个音乐片段, 与5个从真实音乐中截取出的音乐片段混合, 让4个非音乐专业人士作为测试人员进行辨别, 且没有事先告知其真实音乐和合成音乐的比例。</p>
                </div>
                <div class="p1">
                    <p id="177">统计结果可得<i>TP</i>=17、<i>TN</i>=6、<i>FP</i>=14、<i>FN</i>=3, 本文方法的精确度为54.8%, 准确度为57.5%, 召回率为85%, 转移率为30%。可以看出, 精确度和准确度均在50%左右, 表明有判断错误的音乐, 而召回率在80%以上, 表明大部分正例判断正确, 负例大部分都被误判为正例。因此, 多数人将真实音乐判断正例, 同时把大部分合成音乐判断成正例, 验证了本文方法的有效性。</p>
                </div>
                <h3 id="178" name="178" class="anchor-tag">5 结束语</h3>
                <div class="p1">
                    <p id="179">本文提出一种复音音乐的生成方法。该方法将MIDI片段转化为文本文件, 利用LSTM网络对其进行训练, 得到音乐生成模型。基于十二平均律的音高统计方法, 设计人耳听感测试, 结果表明, 本文方法生成的音乐与真实音乐相似度较高, 且音乐片段具有旋律性, 符合和声规则。下一步将研究音乐生成模型的连贯性及节奏性。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZDZC200512027&amp;v=MTkyOTA1NE8zenFxQnRHRnJDVVJMT2VaZVJvRnkzbVZyck9QeW5SYmJHNEh0VE5yWTlIWTRRS0RIODR2UjRUNmo=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> 余立功, 卜佳俊, 陈纯.基于内外概率算法的音乐节奏自动生成[J].浙江大学学报 (工学版) , 2005, 39 (12) :1969-1972, 1983.
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Generating sequences with recurrent neural networks">

                                <b>[2]</b> GRAVES A.Generating sequences with recurrent neural networks[EB/OL].[2018-02-01].https://arxiv.org/pdf/1308.0850.pdf.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJST&amp;filename=SJST14112600634565&amp;v=MzA0NTBqbVVMbklKVjBSYnhRPU5pZlllcks4SDlET3FZOUZZdWdMQ1hvOG9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeQ==&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> BAUM L E, PETRIE T, SOULES G, et al.A maximization technique occurring in the statistical analysis of probabilistic functions of Markov chains[J].The Annals of Mathematical Statistics, 1970, 41 (1) :164-171.
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Experimental music composition with an electronic computer">

                                <b>[4]</b> HILLER L, ISAACSON L M.Experimental music composition with an electronic computer[M].Westport, USA:Greenwood Publishing Group Inc., 1979.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Harmonising chorales by probabilistic inference">

                                <b>[5]</b> ALLAN M, WILLIAMS C K I.Harmonising chorales by probabilistic inference[C]//Proceedings of the 17th International Conference on Neural Information Processing Systems.Cambridge, USA:MIT Press, 2005:25-32.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=My Song:Automatic Accompaniment Generation for Vocal Melodies">

                                <b>[6]</b> SIMON I, MORRIS D, BASU S.MySong:automatic accompaniment generation for vocal melodies[C]//Proceedings of the SIGCHI Conference on Human Factors in Computing Systems.New York, USA:ACM Press, 2008:725-734.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Algorithmic composition as a model of creativity">

                                <b>[7]</b> JACOB B L.Algorithmic composition as a model of creativity[J].Organised Sound, 1996, 1 (3) :157-165.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Synthetic harmonies an approach to musical semiosis by means of cellular automata">

                                <b>[8]</b> BILOTTA E, PANTANO P, TALARICO V.Synthetic harmonies :an approach to musical semiosis by means of cellular automata[C]//Proceedings of the 7th International Conference on Artificial Life.Cambridge, USA:MIT Press, 2002:153-159.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=WXYJ201001003&amp;v=MzAwMjI4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm9GeTNtVnJyT01qWFNaTEc0SDlITXJvOUZaNFFLREg=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b> 王存睿, 段晓东, 刘向东, 等.基于Hilbert映射的元胞自动机音乐生成算法[J].微电子学与计算机, 2010, 27 (1) :5-8.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Automatic composition of music by means of grammatical evolution">

                                <b>[10]</b> DE LA PUENTE A O, ALFONSO R S, MORENO M A.Automatic composition of music by means of grammatical evolution[C]//Proceedings of Conference on APL.New York, USA:ACM Press, 2002:148-155.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Algorithms for music composition by neural nets Improved CBR paradigms">

                                <b>[11]</b> LEWIS J.Algorithms for music composition by neural nets Improved CBR paradigms[EB/OL].[2018-02-04].https://quod.lib.umich.edu/cgi/p/pod/dod-idx/algorithms-for-music-composition.pdf?c=icmc;idno=bbp2372.1989.044;format=pdf.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A first look at music composition using lstm recurrent neural networks">

                                <b>[12]</b> ECK D, SCHMIDHUBER J A first look at music composition using lstm recurrent neural networks[EB/OL].[2018-02-04].http://people.idsia.ch/～juergen/blues/IDSIA-07-02.pdf.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Perceiving and predicting expressive rhythm with recurrent neural networks">

                                <b>[13]</b> LAMBERT A J, WEYDE T, ARMSTRONG N.Perceiving and predicting expressive rhythm with recurrent neural networks[EB/OL].[2018-02-01].http://openaccess.city.ac.uk/16489/.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Text-based LSTM networks for automatic music composition">

                                <b>[14]</b> CHOI K, FAZEKAS G, SANDLER M.Text-based LSTM networks for automatic music composition[EB/OL].[2018-02-01].https://arxiv.org/pdf/1604.05358.pdf.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_15" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JLGY201803029&amp;v=MDIyNzdyck9MeUhNZDdHNEg5bk1ySTlIYllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJvRnkzbVY=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[15]</b> 李雄飞, 冯婷婷, 骆实, 等.基于递归神经网络的自动作曲算法[J].吉林大学学报 (工学版) , 2018, 48 (3) :866-873.
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Gradient flow in recurrent nets:the difficulty of learning long-term dependencies">

                                <b>[16]</b> HOCHREITER S, BENGIO Y, FRASCONI P, et al.Gradient flow in recurrent nets:the difficulty of learning long-term dependencies[EB/OL].[2018-02-01].https://www.bioinf.jku.at/publications/older/ch7.pdf.
                            </a>
                        </p>
                        <p id="35">
                            <a id="bibliography_17" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJBK&amp;filename=SJBK15090500014198&amp;v=MTEzNjFiSzlIdGpNcW85RlpPb0xEWFV4b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VTG5JSlYwUmJ4UT1OaWZKWg==&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[17]</b> HOCHREITER S, SCHMIDHUBER J.Long short-term memory[J].Neural Computation, 1997, 9 (8) :1735-1780.
                            </a>
                        </p>
                        <p id="37">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep learning based imaging data completion for improved brain disease diagnosis">

                                <b>[18]</b> LI Rongjian, ZHANG Wenlu, SUK H I, et al.Deep learning based imaging data completion for improved brain disease diagnosis[C]//Proceedings of International Conference on Medical Image Computing and Computer-Assisted Intervention.Berlin, Germany:Springer, 2014:305-312.
                            </a>
                        </p>
                        <p id="39">
                            <a id="bibliography_19" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJST&amp;filename=SJST6CC1AFE70C7D7EE0DEA0865BA1064288&amp;v=MTg1MTJlSHRNdWhabm4wNTlRSG5uM21NMGViU1FSN0tYQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQ3BiUTM1TjVod2J1NnhhND1OaWZZZXJYTGJkQzkyZnBDWkpnSQ==&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[19]</b> KLERK D D.Equal temperament[J].Acta Musicologica, 1979, 51 (1) :140-150.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJC201905041" />
        <input id="dpi" type="hidden" value="600" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201905041&amp;v=MDkyMzBSb0Z5M21WcnJQTHo3QmJiRzRIOWpNcW85QlpZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmU=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="1" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
