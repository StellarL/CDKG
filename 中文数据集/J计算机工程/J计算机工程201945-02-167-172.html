<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637131276284650000%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJC201902028%26RESULT%3d1%26SIGN%3dcyrOXTm197EhGS3txnKAOyJjNL8%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201902028&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201902028&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201902028&amp;v=MjUxODNVUkxPZVplUm5GeWprVUxyS0x6N0JiYkc0SDlqTXJZOUhiSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckM=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#63" data-title="0概述 ">0概述</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#68" data-title="1基于参考点的k近邻算法 ">1基于参考点的k近邻算法</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#76" data-title="2改进算法 ">2改进算法</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#77" data-title="2.1参考点选取">2.1参考点选取</a></li>
                                                <li><a href="#92" data-title="2.2参考点的自适应权重">2.2参考点的自适应权重</a></li>
                                                <li><a href="#102" data-title="2.3算法设计">2.3算法设计</a></li>
                                                <li><a href="#135" data-title="2.4参数α设置">2.4参数α设置</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#143" data-title="3实验结果与分析 ">3实验结果与分析</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#154" data-title="4结束语 ">4结束语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#70" data-title="图1 参考点O&lt;sub&gt;1&lt;/sub&gt;与不同样本点的距离">图1 参考点O<sub>1</sub>与不同样本点的距离</a></li>
                                                <li><a href="#71" data-title="图2 2个参考点O&lt;sub&gt;1&lt;/sub&gt;和O&lt;sub&gt;2&lt;/sub&gt;样本位置度量">图2 2个参考点O<sub>1</sub>和O<sub>2</sub>样本位置度量</a></li>
                                                <li><a href="#95" data-title="图3 2个参考点根据样本与数据集分布情况的自适应权重">图3 2个参考点根据样本与数据集分布情况的自适应权重</a></li>
                                                <li><a href="#145" data-title="表1 公开数据集的大小与维度">表1 公开数据集的大小与维度</a></li>
                                                <li><a href="#147" data-title="表2 k=1时4种算法分类精度对比结果">表2 k=1时4种算法分类精度对比结果</a></li>
                                                <li><a href="#148" data-title="表3 k=5时4种算法分类精度对比结果">表3 k=5时4种算法分类精度对比结果</a></li>
                                                <li><a href="#150" data-title="图4 人工数据集分类时间随样本数的变化结果">图4 人工数据集分类时间随样本数的变化结果</a></li>
                                                <li><a href="#151" data-title="表4 4种算法分类精度对比结果">表4 4种算法分类精度对比结果</a></li>
                                                <li><a href="#153" data-title="图5 各算法预测性能对比结果">图5 各算法预测性能对比结果</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="33">


                                    <a id="bibliography_1" title="何清, 李宁, 罗文娟, 等.大数据下的机器学习算法综述[J].模式识别与人工智能, 2014, 27 (4) :327-336." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201404007&amp;v=MTU3MTdMck5LRDdZYkxHNEg5WE1xNDlGWTRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJuRnlqa1U=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                        何清, 李宁, 罗文娟, 等.大数据下的机器学习算法综述[J].模式识别与人工智能, 2014, 27 (4) :327-336.
                                    </a>
                                </li>
                                <li id="35">


                                    <a id="bibliography_2" title="王煜, 张明, 王正欧, 等.用于文本分类的改进KNN算法[J].计算机工程与应用, 2007, 43 (13) :159-162." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSGG200713047&amp;v=MTM5NTMzenFxQnRHRnJDVVJMT2VaZVJuRnlqa1VMck5MejdNYWJHNEh0Yk5ySTlCWTRRS0RIODR2UjRUNmo1NE8=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                        王煜, 张明, 王正欧, 等.用于文本分类的改进KNN算法[J].计算机工程与应用, 2007, 43 (13) :159-162.
                                    </a>
                                </li>
                                <li id="37">


                                    <a id="bibliography_3" title=" POLI G, LLAPA E, CECATTO J R, et al. Solar flare detection system based on tolerance near sets in a GPUCUDA framew ork[J]. Know ledge-Based Systems, 2014, 70 (C) :345-360." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Solar flare detection system based on tolerance near sets in a GPUCUDA framew ork">
                                        <b>[3]</b>
                                         POLI G, LLAPA E, CECATTO J R, et al. Solar flare detection system based on tolerance near sets in a GPUCUDA framew ork[J]. Know ledge-Based Systems, 2014, 70 (C) :345-360.
                                    </a>
                                </li>
                                <li id="39">


                                    <a id="bibliography_4" title=" BHATTACHARYA G, GHOSH K, CHOWDHURY A S.Granger causality driven AHP for feature weighted k NN[J]. Pattern Recognition, 2017, 66:425-436." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Granger causality driven AHP for feature weighted k NN">
                                        <b>[4]</b>
                                         BHATTACHARYA G, GHOSH K, CHOWDHURY A S.Granger causality driven AHP for feature weighted k NN[J]. Pattern Recognition, 2017, 66:425-436.
                                    </a>
                                </li>
                                <li id="41">


                                    <a id="bibliography_5" title="吴泽泰, 蔡仁钦, 徐书燕, 等.基于K近邻法的WiFi定位研究与改进[J].计算机工程, 2017, 43 (3) :289-293." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201703048&amp;v=MjUxMDVSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm5GeWprVUxyTkx6N0JiYkc0SDliTXJJOUJiSVFLREg4NHY=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                        吴泽泰, 蔡仁钦, 徐书燕, 等.基于K近邻法的WiFi定位研究与改进[J].计算机工程, 2017, 43 (3) :289-293.
                                    </a>
                                </li>
                                <li id="43">


                                    <a id="bibliography_6" title=" UGUZ H. A two-stage feature selection method for text categorization by using information gain, principal component analysis and genetic algorithm[J].Know ledge-Based Systems, 2011, 12 (5) :1024-1032." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501719222&amp;v=MTQ5NTBiSUlGOFhieGM9TmlmT2ZiSzdIdEROcW85RVkrb0dEbjQ3b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VTA==&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                         UGUZ H. A two-stage feature selection method for text categorization by using information gain, principal component analysis and genetic algorithm[J].Know ledge-Based Systems, 2011, 12 (5) :1024-1032.
                                    </a>
                                </li>
                                <li id="45">


                                    <a id="bibliography_7" title="周红标, 乔俊飞.基于高维k-近邻互信息的特征选择方法[J].智能系统学报, 2017, 12 (5) :595-600." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZNXT201705003&amp;v=MDAzMzMzenFxQnRHRnJDVVJMT2VaZVJuRnlqa1VMck5QeVBUZXJHNEg5Yk1xbzlGWjRRS0RIODR2UjRUNmo1NE8=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                        周红标, 乔俊飞.基于高维k-近邻互信息的特征选择方法[J].智能系统学报, 2017, 12 (5) :595-600.
                                    </a>
                                </li>
                                <li id="47">


                                    <a id="bibliography_8" title=" XIA Shuyin, XIONG Zhongyang, LUO Yueguo, et al.Location difference of multiple distances based k-nearest neighbors algorithm[J]. Know ledge-Based Systems, 2015, 90 (C) :99-110." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES17DB8B9C1DCC15B591A6EFC1FFF4A5EC&amp;v=MjMwNjNtYUJ1SFlmT0dRbGZDcGJRMzVORmh4TG04eGEwPU5pZk9mYksvYXFQRTNZWTJaWjk4ZjMwOHZSTWE2MDU3UFFtUnJXUkREN2JsUU0vc0NPTnZGU2lXV3I3SklGcA==&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                         XIA Shuyin, XIONG Zhongyang, LUO Yueguo, et al.Location difference of multiple distances based k-nearest neighbors algorithm[J]. Know ledge-Based Systems, 2015, 90 (C) :99-110.
                                    </a>
                                </li>
                                <li id="49">


                                    <a id="bibliography_9" title=" GOU J, ZHAN Y, RAO Y, et al. Improved pseudo nearest neighbor classification[J]. Know ledge-Based Systems, 2014, 70 (C) :361-375." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14110700332878&amp;v=MTIyMDRRSC9pclJkR2VycVFUTW53WmVadUh5am1VTGJJSUY4WGJ4Yz1OaWZPZmJLOEg5RE1xSTlGWitnTkJIc3hvQk1UNlQ0UA==&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                         GOU J, ZHAN Y, RAO Y, et al. Improved pseudo nearest neighbor classification[J]. Know ledge-Based Systems, 2014, 70 (C) :361-375.
                                    </a>
                                </li>
                                <li id="51">


                                    <a id="bibliography_10" title=" FRIEDMAN J H, BENTLEY J L, FINKEL R A. An algorithm for finding best matches in logarithmic expected time[J]. ACM Transactions on M athematical Softw are, 1977, 3 (3) :209-226." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000101001&amp;v=MDU1NzU9TmlmSVk3SzdIdGpOcjQ5Rlplc09ESHc0b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VTGJJSUY4WGJ4Yw==&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[10]</b>
                                         FRIEDMAN J H, BENTLEY J L, FINKEL R A. An algorithm for finding best matches in logarithmic expected time[J]. ACM Transactions on M athematical Softw are, 1977, 3 (3) :209-226.
                                    </a>
                                </li>
                                <li id="53">


                                    <a id="bibliography_11" title=" FUKUNAGA K, NARENDRA P M. A branch and bound algorithm for computing k-nearest neighbors[J]. IEEE Transactions on Computers, 1975, 24 (7) :750-753." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A branch and bound algorithm for computing k-nearest neighbors">
                                        <b>[11]</b>
                                         FUKUNAGA K, NARENDRA P M. A branch and bound algorithm for computing k-nearest neighbors[J]. IEEE Transactions on Computers, 1975, 24 (7) :750-753.
                                    </a>
                                </li>
                                <li id="55">


                                    <a id="bibliography_12" title=" CHEN Y S, HUNG Y P, TEN T F, et al. Fast and versatile algorithm for nearest neighbor search based on a low er bound tree[J]. Pattern Recognition, 2007, 40 (2) :360-375." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011600739651&amp;v=MjMyNTU9TmlmT2ZiSzdIdEROcVk5RlkrZ0dDbms0b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VTGJJSUY4WGJ4Yw==&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[12]</b>
                                         CHEN Y S, HUNG Y P, TEN T F, et al. Fast and versatile algorithm for nearest neighbor search based on a low er bound tree[J]. Pattern Recognition, 2007, 40 (2) :360-375.
                                    </a>
                                </li>
                                <li id="57">


                                    <a id="bibliography_13" title=" LIAW Y C, LEOU M L, WU C M. Fast exact k nearest neighbors search using an orthogonal search tree[J].Pattern Recognition, 2010, 43 (6) :2351-2358." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011600738499&amp;v=MjUwMTBqbVVMYklJRjhYYnhjPU5pZk9mYks3SHRETnFZOUZZK2dIQ0hVd29CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeQ==&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[13]</b>
                                         LIAW Y C, LEOU M L, WU C M. Fast exact k nearest neighbors search using an orthogonal search tree[J].Pattern Recognition, 2010, 43 (6) :2351-2358.
                                    </a>
                                </li>
                                <li id="59">


                                    <a id="bibliography_14" title="钱途, 钱江波, 董一鸿, 等. SLSB-forest:高维数据的近似k近邻查询[J].电信科学, 2017, 33 (9) :58-68." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DXKX201709008&amp;v=MjIxMjVMT2VaZVJuRnlqa1VMck5JVFhBZHJHNEg5Yk1wbzlGYklRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[14]</b>
                                        钱途, 钱江波, 董一鸿, 等. SLSB-forest:高维数据的近似k近邻查询[J].电信科学, 2017, 33 (9) :58-68.
                                    </a>
                                </li>
                                <li id="61">


                                    <a id="bibliography_15" title=" YANG Liu, DONG Limei, BI Xiaoru. An improved location difference of multiple distances based nearest neighbors searching algorithm[J]. International Journal for Light and Electron Optics, 2016, 127 (22) :10838-10843." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=An Improved Location Difference of Multiple Distances Based Nearest Neighbors Searching Algorithm">
                                        <b>[15]</b>
                                         YANG Liu, DONG Limei, BI Xiaoru. An improved location difference of multiple distances based nearest neighbors searching algorithm[J]. International Journal for Light and Electron Optics, 2016, 127 (22) :10838-10843.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJC" target="_blank">计算机工程</a>
                2019,45(02),167-172 DOI:10.19678/j.issn.1000-3428.0049901            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm">基于参考点的改进k近邻分类算法</span>
 <span class="shoufa"></span>                                     </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%A2%81%E8%81%AA&amp;code=38707142&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">梁聪</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%A4%8F%E4%B9%A6%E9%93%B6&amp;code=37482827&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">夏书银</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%99%88%E5%AD%90%E5%BF%A0&amp;code=38707143&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">陈子忠</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E9%87%8D%E5%BA%86%E9%82%AE%E7%94%B5%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF%E5%AD%A6%E9%99%A2&amp;code=0174747&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">重庆邮电大学计算机科学与技术学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>基本k近邻 (kNN) 分类算法具有二次方的时间复杂度, 且分类效率和精度较低。针对该问题, 提出一种改进的参考点kNN分类算法。依据点到样本距离的方差选择参考点, 并赋予参考点自适应权重。实验结果表明, 与基本k NN算法及kd-tree近邻算法相比, 该算法具有较高的分类精度及较低的时间复杂度。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=k%E8%BF%91%E9%82%BB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">k近邻;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8F%82%E8%80%83%E7%82%B9&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">参考点;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%87%AA%E9%80%82%E5%BA%94%E6%9D%83%E9%87%8D&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">自适应权重;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%96%B9%E5%B7%AE&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">方差;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%88%86%E7%B1%BB%E6%95%88%E7%8E%87&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">分类效率;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    梁聪 (1991—) , 男, 硕士研究生, 主研方向为数据挖掘、大数据处理;E-mail: lcong2011@ 126. com
;
                                </span>
                                <span>
                                    *夏书银 (通信作者) , 副教授、博士;
;
                                </span>
                                <span>
                                    陈子忠, 教授、博士。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2017-12-28</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家重点研发计划 (2016QY01W0200, 2016YFB1000905);</span>
                                <span>重庆市教委科学技术研究项目 (KJ1600426, KJ1600419);</span>
                    </p>
            </div>
                    <h1>Improvement k-Nearest Neighbor Classification Algorithm Based on Reference Points</h1>
                    <h2>
                    <span>LIANG Cong</span>
                    <span>XIA Shuyin</span>
                    <span>CHEN Zizhong</span>
            </h2>
                    <h2>
                    <span>College of Computer Science and Technology, Chongqing University of Posts and Telecommunications</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>The basic k-Nearest Neighbor ( kNN) classification algorithm has quadratic time complexity, has a low classification efficiency and has a low classification accuracy. Aiming at this problem, an improvement reference points kNN classification algorithm is proposed. The reference point is selected according to the variance of the point-to-sample distance, and the reference point is given an adaptive weight. Experimental results show that compared with the basic kNN algorithm and kd-tree neighbor algorithm, this algorithm has high classification accuracy and has low time complexity.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=k-Nearest%20Neighbor%20(kNN)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">k-Nearest Neighbor (kNN) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=reference%20points&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">reference points;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=self-adaptive%20weight&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">self-adaptive weight;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=variance&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">variance;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=classification%20efficiency&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">classification efficiency;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2017-12-28</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="63" name="63" class="anchor-tag">0概述</h3>
                <div class="p1">
                    <p id="64">分类是数据挖掘领域中一项重要技术, 根据已知的训练数据集建立分类模型, 并以此预测待分类数据的类别。目前常用的分类方法有k近邻 (k-Nearest Neighbors, kNN) 算法、决策树、支持向量机和神经网络等<citation id="161" type="reference"><link href="33" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>。其中, kNN算法是一种基于实例学习的分类算法, 根据待分类样本在特征空间中k个最近邻样本中的多数类别来进行分类<citation id="162" type="reference"><link href="35" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>。kNN算法具有较高分类准确度和易于实现等优点, 广泛应用于模式识别<citation id="164" type="reference"><link href="37" rel="bibliography" /><link href="39" rel="bibliography" /><sup>[<a class="sup">3</a>,<a class="sup">4</a>]</sup></citation>、异常点检测<citation id="163" type="reference"><link href="41" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>、特征选择<citation id="165" type="reference"><link href="43" rel="bibliography" /><link href="45" rel="bibliography" /><sup>[<a class="sup">6</a>,<a class="sup">7</a>]</sup></citation>与分类<citation id="166" type="reference"><link href="47" rel="bibliography" /><link href="49" rel="bibliography" /><sup>[<a class="sup">8</a>,<a class="sup">9</a>]</sup></citation>等领域。</p>
                </div>
                <div class="p1">
                    <p id="65">基本kNN算法具有二次方的时间复杂度, 通过计算待分类样本到所有训练样本的相似度以找到k个近邻样本, 通常称为全搜索算法 (Full Search Algorithm, FSA) 。为提高kNN算法的效率, 目前改进方向之一是为训练数据集建立树结构索引。文献<citation id="168" type="reference">[<a class="sup">10</a>]</citation>提出构建平衡k-d树 (k-dimensional tree, kdtree) 的方法。文献<citation id="169" type="reference">[<a class="sup">11</a>]</citation>使用分层聚类方法构建ball树, 从而提高查找k近邻的效率。文献<citation id="170" type="reference">[<a class="sup">12</a>]</citation>通过建立LB树和使用赢者更新搜索策略来加速查找k近邻。文献<citation id="167" type="reference">[<a class="sup">13</a>]</citation>提出一种基于正交搜索树的快速kNN搜索方法。多数基于树结构的k NN算法具有O (nlb n) ～O (n<sup>2</sup>) 范围内的时间复杂度, 虽然能有效地提高kNN的效率, 但是创建树结构的过程及其查找效率容易受到数据集维度和噪声的影响, 且不同数据集所构建的树结构复杂程度相差较大, 导致该类方法的稳定性较差。</p>
                </div>
                <div class="p1">
                    <p id="66">kNN算法另一改进方向是设法缩小训练样本的搜索范围, 查找近似k近邻<citation id="171" type="reference"><link href="59" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>。文献<citation id="172" type="reference">[<a class="sup">8</a>]</citation>提出一种基于参考点思路的kNN算法, 称为LDMDBA算法 (Location Difference of Multiple Distances Based Algorithm) , 其借助参考点大幅度降低搜索训练样本的数量, 以加快k近邻的查找速度。该算法具有O (nlb n) 时间复杂度, 而且稳定性较好, 对不同维度的数据集均具有较高的分类效率, 但是该算法在选取参考点时按照维度顺序任意选择, 并没有考虑参考点与样本集之间的空间分布关系, 而且忽略不同参考点在查找近邻时所起的不同作用, 从而造成分类精度在一定程度上的损失。文献<citation id="173" type="reference">[<a class="sup">15</a>]</citation>通过增加参考点数量改进该算法, 然而增多参考点会降低精度, 且会导致算法的计算时间增加。</p>
                </div>
                <div class="p1">
                    <p id="67">为提高参考点k近邻算法的分类精度, 本文基于参考点与训练样本之间的空间分布关系, 选取与样本距离方差较大的点作为参考点, 并根据待分类样本动态赋予参考点自适应权重。</p>
                </div>
                <h3 id="68" name="68" class="anchor-tag">1基于参考点的k近邻算法</h3>
                <div class="p1">
                    <p id="69">LDM DBA算法属于近似近邻算法, 利用若干参考点到所有样本点的距离来度量样本彼此间的位置差异, 从而大幅提高搜索k个近邻样本的效率。如图1所示, O<sub>1</sub>为参考点, 由于距离BO<sub>1</sub>相比于CO<sub>1</sub>更接近于AO<sub>1</sub>, B点可被认为是A点的近邻, 因此到参考点的距离可代替样本间欧式距离来判断样本点之间的近邻关系。然而当有多个样本点处于以参考点O<sub>1</sub>为圆心的圆上时, 仅靠单个参考点无法充分判断一个样本点的近邻, 如图2所示, 虽然A'O<sub>1</sub>与AO<sub>1</sub>距离相等, 但是A和A'实际相距较远, 此时需通过增加参考点数目来加强位置判断, 如添加O<sub>2</sub>参考点。由于O<sub>1</sub>和O<sub>2</sub>这2个参考点所找出A的近邻样本近似, 因此仍需计算A点到这些近似近邻点的欧式距离以增强所找近邻点的准确性。</p>
                </div>
                <div class="area_img" id="70">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201902028_07000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 参考点O1与不同样本点的距离" src="Detail/GetImg?filename=images/JSJC201902028_07000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 参考点O<sub>1</sub>与不同样本点的距离  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201902028_07000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="71">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201902028_07100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 2个参考点O1和O2样本位置度量" src="Detail/GetImg?filename=images/JSJC201902028_07100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 2个参考点O<sub>1</sub>和O<sub>2</sub>样本位置度量  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201902028_07100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="156">假定样本x属于d维空间<image id="157" type="formula" href="images/JSJC201902028_15700.jpg" display="inline" placement="inline"><alt></alt></image>, 即x= (x<sup>1</sup>, x<sup>2</sup>, …, x<sup>d</sup>) , 从x到某个参考点O<sub>i</sub>的相似度使用欧式距离衡量, 其中, 1≤i≤lb d, 定义样本x相对于参考点O<sub>i</sub>的位置差异因子 (Location Difference Based Factor, LDBF) 如下:</p>
                </div>
                <div class="area_img" id="74">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJC201902028_07400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="158">该算法主要步骤是计算所有训练样本到参考点O<sub>i</sub>的LDBF<sub>i</sub>值, 并排序产生与参考点相同数量的LDBF<sub>i</sub>有序序列。对于一个待分类样本x, 在LDBF<sub>i</sub>有序序列中查找LDBF<sub>i</sub> (x) 的插入位置, 并在训练样本序列中搜索以该位置为中心的近邻样本子序列, 然后计算若干子序列组成大的序列中所有样本与x的欧式距离, 从而找到x的k个近邻样本并做出分类。</p>
                </div>
                <h3 id="76" name="76" class="anchor-tag">2改进算法</h3>
                <h4 class="anchor-tag" id="77" name="77">2.1参考点选取</h4>
                <div class="p1">
                    <p id="78">参考点作用是度量样本之间的位置差异, 若所有样本到参考点的距离比较接近, 则参考点将失去作用。图2中点A与A'同时位于圆心为参考点O<sub>1</sub>的圆上, 即距离O<sub>1</sub>A和O<sub>1</sub>A'的方差为0, 此时无法借助O<sub>1</sub>判断A与A'之间的相对距离, 因此该参考点O<sub>1</sub>对于区分A与A'无效。LDMDBA算法在选取参考点时没有考虑参考点与训练样本之间的空间分布关系, 而是按照数据集的维度顺序选择。显然, 参考点到各个样本的距离相差越大越好, 以便借助参考点判断样本之间的近邻关系, 本文利用所有样本到参考点距离的方差衡量样本相对于参考点在空间位置上的发散程度。</p>
                </div>
                <div class="p1">
                    <p id="79">参考点O<sub>i</sub>到所有样本距离的方差V<sub>i</sub>可表示为:</p>
                </div>
                <div class="area_img" id="80">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJC201902028_08000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="81">数据集的元素通常缩放至[0, 1]之间, 在笛卡尔坐标系中参考点习惯设置在坐标轴-1或1的位置上。参考点可表示为某个维度的值为1或-1, 其他所有维度值为0的d维向量, 具体可分为值为1的正向量和值为-1的负向量2种形式, 即参考点O<sub>i</sub>的第i个维度值为-1或1, 其中1≤i≤d。例如, 具有3个属性值O<sub>1</sub>的正向量形式为 (1, 0, 0) , 其负向量形式为 (-1, 0, 0) , O<sub>2</sub>的正向量形式为 (0, 1, 0) 。由于单独计算每个参考点到所有样本距离的计算量较大, 且存在大量重复计算项, 因此使用矩阵简化计算过程。</p>
                </div>
                <div class="p1">
                    <p id="82">设在数据集X中, 第j个样本x<sub>j</sub>用一个具有d维属性的行向量 (x<sub>j 1</sub>, x<sub>j 2</sub>, …, x<sub>jd</sub>) 表示, 其中x<sub>jk</sub> (1≤j≤n, 1≤k≤d, n为X的容量大小) 表示样本x<sub>j</sub>的第k个属性值。数据集X可用矩阵表示为:</p>
                </div>
                <div class="area_img" id="83">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJC201902028_08300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="84">对矩阵X中每个元素做平方运算, 可得:</p>
                </div>
                <div class="area_img" id="85">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJC201902028_08500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="86">对正向量和负向量形式的参考点O<sub>i</sub>, 分别定义如下变量C<sub>1</sub>和C<sub>2</sub>, 其中, E<sub>1</sub>是元素为1的n行d列矩阵。</p>
                </div>
                <div class="area_img" id="87">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJC201902028_08700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="88">设E<sub>2</sub>是元素为1的d阶矩阵, 则变量D<sub>1</sub>和D<sub>2</sub>的计算公式为:</p>
                </div>
                <div class="area_img" id="89">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJC201902028_08900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="90">从式 (6) 、式 (7) 可以看出, D<sub>1</sub>的第i个列向量为所有样本到正向量形式参考点O<sub>i</sub>的LDBF<sub>i</sub><sup>2</sup> (x<sub>j</sub>) , D<sub>2</sub>的第i个列向量为所有样本到负向量形式参考点O<sub>i</sub>的LDBF<sub>i</sub><sup>2</sup> (x<sub>j</sub>) 。对D<sub>1</sub>和D<sub>2</sub>中元素求算术平方根后可得式 (8) , 其中p为1或2, 即得到所有样本到各个参考点的距离, 从而可按式 (2) 计算方差V<sub>i</sub>。</p>
                </div>
                <div class="area_img" id="91">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJC201902028_09100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <h4 class="anchor-tag" id="92" name="92">2.2参考点的自适应权重</h4>
                <div class="p1">
                    <p id="93">由于样本分布的原因, 不同参考点对查找k近邻的贡献不尽相同。原始算法忽视参考点与样本之间的位置分布关系, 认为在获取待分类样本的近邻样本子序列时各参考点具有同等重要性, 即每个参考点获得的样本子序列长度一样, 然而在实际中各参考点与样本的位置分布各有差异, 如果在获得各子序列时给予不同参考点同样作用就有可能误导k近邻的查找, 解决方法是赋予参考点差异化的权重。</p>
                </div>
                <div class="p1">
                    <p id="94">参考点的权重取决于它与数据集的空间分布关系。对于一个样本x, 若借助单个参考点所找到一定数量的近邻样本具有不同类别标签, 则说明这些近邻样本可能是不同标签数据集的边界样本, 此时需要赋予该参考点较大的权重以增强它找到正确近邻样本的能力, 即认为该参考点对查找k近邻具有更大影响。如果该参考点找到x的近邻样本均具有相同标签, 此时说明x可能处于相同标签的数据集中, 可给该参考点设置较小权重。从欧式空间来看, 参考点的权重越大, 由该参考点所找到的近邻样本子序列的长度越大。如图3所示, x<sub>1</sub>和x<sub>2</sub>是待分类样本, 由参考点O<sub>1</sub>所找到x<sub>1</sub>的一定数量 (如2k个) 近邻样本均具有相同标签, 说明借助O<sub>1</sub>确定x<sub>1</sub>的位置很可能处于同一类别的样本中, 此时可赋予O<sub>1</sub>较小的权重ω<sub>1</sub>, 以减少样本的搜索范围。对于x<sub>2</sub>由O<sub>2</sub>找到的近邻样本具有不同标签, 可能由于x<sub>2</sub>处于不同类别的边界样本中, 此时需给O<sub>2</sub>设置较大的权重ω<sub>2</sub>, 以增强所找近邻样本的准确性。由于参考点的权重是根据待分类样本一定数量近邻样本的标签决定, 其值因待分类样本不同而具有动态性和差异性, 且能根据近邻样本标签自动确定大小, 因此权重自适应。</p>
                </div>
                <div class="area_img" id="95">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201902028_09500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 2个参考点根据样本与数据集分布情况的自适应权重" src="Detail/GetImg?filename=images/JSJC201902028_09500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 2个参考点根据样本与数据集分布情况的自适应权重  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201902028_09500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="96">对于一个样本x, 假设由参考点O<sub>i</sub>所找到的2k个近邻样本的类别标签为{c<sub>1</sub>, c<sub>2</sub>, …, c<sub>2 k</sub>}, 样本集共有m个类别标签, 即标签集S={s<sub>1</sub>, s<sub>2</sub>, …, s<sub>m</sub>}, 统计x近邻样本中第j个标签的数目为:</p>
                </div>
                <div class="area_img" id="97">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJC201902028_09700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="98">其中, 1≤j≤m, 当a=b时, δ (a, b) =1, 否则, δ (a, b) =0。</p>
                </div>
                <div class="p1">
                    <p id="100">参考点O<sub>i</sub>的自适应权重定义为:</p>
                </div>
                <div class="area_img" id="101">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJC201902028_10100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <h4 class="anchor-tag" id="102" name="102">2.3算法设计</h4>
                <div class="p1">
                    <p id="103">对数据集X中每个样本进行缩放预处理, 即每个样本的特征值除以X中相应维度的最大特征值, 从而将样本特征值缩放至[0, 1]区间。假设k为要查找的近邻数, d为数据集的维度。</p>
                </div>
                <div class="p1">
                    <p id="104">本文算法的处理过程分为准备阶段与分类阶段, 前者按第2.1节计算d个候选参考点的V<sub>i</sub>, 将V<sub>i</sub>降序排列, 取前面lb d个V<sub>i</sub>所对应的候选参考点作为参考点, 同时获取参考点与所有样本的LDBF<sub>i</sub>值, 再将LDBF<sub>i</sub>排序, 并生成相应的训练样本索引和排好序的类别标签, 具体步骤如算法1所示。</p>
                </div>
                <div class="p1">
                    <p id="105">算法1训练样本索引和类别标签算法</p>
                </div>
                <div class="area_img" id="159">
                                <img alt="" src="Detail/GetImg?filename=images/JSJC201902028_15900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="117">分类过程如算法2所示。对一个未知样本y进行分类的步骤是, 在前阶段获得的LDBF<sub>i</sub>有序序列中查找LDBF<sub>i</sub> (y) 的位置, 并在排序标签中获取以y为中心的2k个标签, 按式 (10) 计算自适应权重ω<sub>i</sub>。然后在训练样本索引序列中产生一个以y为中心的长度为2k×ω<sub>i</sub>×α (α的取值见第2.4节) 的近邻样本子序列, 并在由lb d个子序列组成的大序列中计算所有近邻样本到y的欧式距离, 取k个最小距离所对应的样本作为y的k近邻, 以完成分类。</p>
                </div>
                <div class="p1">
                    <p id="118">算法2分类算法</p>
                </div>
                <div class="area_img" id="160">
                                <img alt="" src="Detail/GetImg?filename=images/JSJC201902028_16000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="160">
                                <img alt="" src="Detail/GetImg?filename=images/JSJC201902028_16001.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="134">算法1的时间复杂度取决于第13行, 算法2的时间复杂度取决于第11行, 即本文算法的时间复杂度为两者时间复杂度较大的一个算法的时间复杂度。第13行的时间复杂度由排序算法决定, 因此其平均时间复杂度在O (nlb n) 与O (n<sup>2</sup>) 之间。第11行的时间复杂度受第7行权重ω<sub>i</sub>与子序列长度因子α的取值影响, 通常ω<sub>i</sub>是一个较小的值, 对于二分类, ω<sub>i</sub>最大值为2k+1, α值越小, 子序列包含的样本数越少, 如果α设置为一个足够小的值, 则算法2的时间复杂度将小于算法1的时间复杂度, 整个算法的时间复杂度为O (nlb n) 。该算法不仅小于等于现有的k NN算法, 而且不依赖可能受到数据集影响的树型结构, 因此该算法能有效应用于各种数据集。对于分类阶段, 算法2的第4行使用折半查找算法。</p>
                </div>
                <h4 class="anchor-tag" id="135" name="135">2.4参数α设置</h4>
                <div class="p1">
                    <p id="136">α是子序列长度因子, 其值会影响算法的时间复杂度, 若α取值越大, 则算法2第10行、第11行将包含更多样本和距离值, 虽然有可能使找到的近邻样本更接近查询点, 但会增加算法的时间复杂度。若α取值为n的线性函数, 则算法的时间复杂度将和FSA一样达到二次方。因此, 在算法的整体时间复杂度不大于O (nlb n) 情况下, α取值越大越好。</p>
                </div>
                <div class="p1">
                    <p id="137">α设置为时间复杂度低于O (n) 的表达式, 且α应随着样本n增加而变大, 因此不能设为指数a<sup>n</sup>, 其中0&lt;a&lt;1。当α设为幂函数n<sup>1/m</sup>时, m是大于1的整数, 算法2第11行的距离序列的长度为lb d×2k×ω<sub>i</sub>×n<sup>1/m</sup>, 当对容量为n的数据集进行分类时, 第11行的时间复杂度等于O (n×lb d×2k×ω<sub>i</sub>×n<sup>1/m</sup>) , 利用堆来求最小k个距离, 权重ω<sub>i</sub>设为最大值2k+1。</p>
                </div>
                <div class="area_img" id="138">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJC201902028_13800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="139">当α为n的幂函数时, 整个算法的时间复杂度大于O (n lb n) , 因此α不能为n的幂函数。</p>
                </div>
                <div class="p1">
                    <p id="140">当α取值为n的对数函数log<sub>m</sub>n, 其中m&gt;1, 第11行的时间复杂度等于O (n×lb d×2k× (2k+1) ×log<sub>m</sub>n) 。</p>
                </div>
                <div class="area_img" id="141">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJC201902028_14100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="142">上式的极限为常数, 当α为log<sub>m</sub>n时, 第11行的时间复杂度等于排序算法, 整个算法的时间复杂度为O (nlb n) 。m值越小, 第11行序列的长度越大, 因此m可设为2。</p>
                </div>
                <h3 id="143" name="143" class="anchor-tag">3实验结果与分析</h3>
                <div class="p1">
                    <p id="144">实验使用一台CPU为Intel i7 4710mq, 内存为4 GB的电脑, 算法使用Python实现。为充分比较各个算法的性能, 实验所用数据集分别来自公开数据集与人工数据集, 前者选自UCI Machine Learning Repository网站的7个标准数据集, 分别为Breast cancer、CodRNA、Diabetes、Ijcnn1、Svmguide1、Svmguide3和Sonar, 各数据集均划分训练集和测试集, 大小和维度如表1所示。人工数据集是由均值分别为0和4、方差均为3的高斯分布产生的二分类数据。</p>
                </div>
                <div class="area_img" id="145">
                                            <p class="img_tit">
                                                表1 公开数据集的大小与维度
                                                
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201902028_14500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/JSJC201902028_14500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201902028_14500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表1 公开数据集的大小与维度" src="Detail/GetImg?filename=images/JSJC201902028_14500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>
                <div class="p1">
                    <p id="146">表2和表3所示为在公开数据集上查找1和5近邻时的分类精度, 由于FSA和kd-tree近邻算法均属于精确近邻搜索算法, 具有同样的分类准确性, 因此将两者列在一起。除了表2的Svmguide3数据集所获精度比LDMDBA算法低外, 本文算法在其他数据集上普遍获得较高的分类精度, 尤其在Cod-RNA数据集上获得的精度较原算法提高4%～5%。本文算法改进LDMDBA选择参考点的策略, 所选参考点能更好区分样本间的位置差异。参考点所获近邻样本子序列的长度一样 (即参考点具有同等权重) 或增加子序列长度有时会降低精度, 因此本文算法在查找近邻点时动态地赋予参考点自适应权重。基于以上2处改进, 本文算法取得的分类精度优于原算法。在表3中, 本文算法在前几个数据集已取得与精确k NN算法相等的精度, 与LDMDBA相比精度提升较小。由于查找的近邻样本仍是近似的, 因此本文算法与LDMDBA算法在一些数据集上的精度不如精确k NN算法。</p>
                </div>
                <div class="area_img" id="147">
                                            <p class="img_tit">
                                                表2 k=1时4种算法分类精度对比结果
                                                
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201902028_14700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/JSJC201902028_14700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                <p class="img_note">%</p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201902028_14700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表2 k=1时4种算法分类精度对比结果" src="Detail/GetImg?filename=images/JSJC201902028_14700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>
                <div class="area_img" id="148">
                                            <p class="img_tit">
                                                表3 k=5时4种算法分类精度对比结果
                                                
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201902028_14800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/JSJC201902028_14800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                <p class="img_note">%</p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201902028_14800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表3 k=5时4种算法分类精度对比结果" src="Detail/GetImg?filename=images/JSJC201902028_14800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>
                <div class="p1">
                    <p id="149">由于公开数据集容量和维度较小, 本文使用由高斯分布产生的人工数据集进行实验。图4显示数据集容量的增长对算法性能的影响, 近邻数k和样本的特征数分别设为5与30, 样本数目从2 000增至100 000。由于各算法耗时变化范围过大而不利于显示, 图4中纵坐标显示的是时间 (ms) 取对数 (底数为2) 后的结果。从图4可以看出, 随着样本数的增加, 本文算法的分类效率优于FSA算法和kd-tree近邻算法, 耗时略多于LDMDBA算法的主要原因是选择参考点时需计算方差, 而原算法按照维度顺序选择参考点, 并不需要计算方差。kd-tree算法在查找近邻时不断回溯搜索, 造成其计算时间多于FSA算法。表4显示4种算法分类精度对比结果, 可以看出, 本文算法优于LDMDBA算法。</p>
                </div>
                <div class="area_img" id="150">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201902028_15000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 人工数据集分类时间随样本数的变化结果" src="Detail/GetImg?filename=images/JSJC201902028_15000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 人工数据集分类时间随样本数的变化结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201902028_15000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="151">
                                            <p class="img_tit">
                                                表4 4种算法分类精度对比结果
                                                
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201902028_15100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/JSJC201902028_15100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                <p class="img_note">%</p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201902028_15100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表4 4种算法分类精度对比结果" src="Detail/GetImg?filename=images/JSJC201902028_15100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>
                <div class="p1">
                    <p id="152">图5所示为在数据集维度增大的情况下各算法的预测性能, 即排除训练阶段耗费的时间, 在容量为2 000的数据集中各算法预测400个样本类别所花费的时间。本文算法预测一个样本相当于在一个有序序列中查找距离值并计算较小规模近邻样本的距离, 故其耗费时间远低于精确近邻搜索算法, 如当维度为1 024时, 本文算法耗时约2<sup>10</sup>ms, 而kd-tree耗时超过2<sup>14</sup>ms。由于获取的参考点自适应权重可能会增加所找近邻样本的数量, 因此本文算法花费时间略多于LDMDBA算法。</p>
                </div>
                <div class="area_img" id="153">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201902028_15300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 各算法预测性能对比结果" src="Detail/GetImg?filename=images/JSJC201902028_15300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 各算法预测性能对比结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201902028_15300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h3 id="154" name="154" class="anchor-tag">4结束语</h3>
                <div class="p1">
                    <p id="155">基于参考点的kNN算法不依赖复杂树型结构, 对各种数据集具有较高的分类效率, 但分类精度低于精确近邻算法。为此, 本文提出一种改进的基于参考点的k NN分类算法。该算法将点到样本点距离的方差作为参考点的选取依据, 并在查找近邻样本时动态赋予参考点自适应权重。在数据集上进行实验, 结果表明, 本文算法相对于LDMDBA算法具有较高的分类准确性。与精确近邻搜索算法相比, 该算法时间复杂度较低, 且能提高分类速度。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="33">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201404007&amp;v=MzEyMzlHRnJDVVJMT2VaZVJuRnlqa1VMck5LRDdZYkxHNEg5WE1xNDlGWTRRS0RIODR2UjRUNmo1NE8zenFxQnQ=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b>何清, 李宁, 罗文娟, 等.大数据下的机器学习算法综述[J].模式识别与人工智能, 2014, 27 (4) :327-336.
                            </a>
                        </p>
                        <p id="35">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSGG200713047&amp;v=Mjg4Njc0SHRiTnJJOUJZNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm5GeWprVUxyTkx6N01hYkc=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b>王煜, 张明, 王正欧, 等.用于文本分类的改进KNN算法[J].计算机工程与应用, 2007, 43 (13) :159-162.
                            </a>
                        </p>
                        <p id="37">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Solar flare detection system based on tolerance near sets in a GPUCUDA framew ork">

                                <b>[3]</b> POLI G, LLAPA E, CECATTO J R, et al. Solar flare detection system based on tolerance near sets in a GPUCUDA framew ork[J]. Know ledge-Based Systems, 2014, 70 (C) :345-360.
                            </a>
                        </p>
                        <p id="39">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Granger causality driven AHP for feature weighted k NN">

                                <b>[4]</b> BHATTACHARYA G, GHOSH K, CHOWDHURY A S.Granger causality driven AHP for feature weighted k NN[J]. Pattern Recognition, 2017, 66:425-436.
                            </a>
                        </p>
                        <p id="41">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201703048&amp;v=Mjk0NzdJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSbkZ5amtVTHJOTHo3QmJiRzRIOWJNckk5QmI=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b>吴泽泰, 蔡仁钦, 徐书燕, 等.基于K近邻法的WiFi定位研究与改进[J].计算机工程, 2017, 43 (3) :289-293.
                            </a>
                        </p>
                        <p id="43">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501719222&amp;v=MjIxOTJvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVVMYklJRjhYYnhjPU5pZk9mYks3SHRETnFvOUVZK29HRG40Nw==&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b> UGUZ H. A two-stage feature selection method for text categorization by using information gain, principal component analysis and genetic algorithm[J].Know ledge-Based Systems, 2011, 12 (5) :1024-1032.
                            </a>
                        </p>
                        <p id="45">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZNXT201705003&amp;v=MjA2NTJxbzlGWjRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJuRnlqa1VMck5QeVBUZXJHNEg5Yk0=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b>周红标, 乔俊飞.基于高维k-近邻互信息的特征选择方法[J].智能系统学报, 2017, 12 (5) :595-600.
                            </a>
                        </p>
                        <p id="47">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES17DB8B9C1DCC15B591A6EFC1FFF4A5EC&amp;v=MDE3NDlpZk9mYksvYXFQRTNZWTJaWjk4ZjMwOHZSTWE2MDU3UFFtUnJXUkREN2JsUU0vc0NPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkNwYlEzNU5GaHhMbTh4YTA9Tg==&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b> XIA Shuyin, XIONG Zhongyang, LUO Yueguo, et al.Location difference of multiple distances based k-nearest neighbors algorithm[J]. Know ledge-Based Systems, 2015, 90 (C) :99-110.
                            </a>
                        </p>
                        <p id="49">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14110700332878&amp;v=MDA1NjNOQkhzeG9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVUxiSUlGOFhieGM9TmlmT2ZiSzhIOURNcUk5RlorZw==&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b> GOU J, ZHAN Y, RAO Y, et al. Improved pseudo nearest neighbor classification[J]. Know ledge-Based Systems, 2014, 70 (C) :361-375.
                            </a>
                        </p>
                        <p id="51">
                            <a id="bibliography_10" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000101001&amp;v=MjI1MjNGWmVzT0RIdzRvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVVMYklJRjhYYnhjPU5pZklZN0s3SHRqTnI0OQ==&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[10]</b> FRIEDMAN J H, BENTLEY J L, FINKEL R A. An algorithm for finding best matches in logarithmic expected time[J]. ACM Transactions on M athematical Softw are, 1977, 3 (3) :209-226.
                            </a>
                        </p>
                        <p id="53">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A branch and bound algorithm for computing k-nearest neighbors">

                                <b>[11]</b> FUKUNAGA K, NARENDRA P M. A branch and bound algorithm for computing k-nearest neighbors[J]. IEEE Transactions on Computers, 1975, 24 (7) :750-753.
                            </a>
                        </p>
                        <p id="55">
                            <a id="bibliography_12" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011600739651&amp;v=MjMyMTBrNG9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVUxiSUlGOFhieGM9TmlmT2ZiSzdIdEROcVk5RlkrZ0dDbg==&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[12]</b> CHEN Y S, HUNG Y P, TEN T F, et al. Fast and versatile algorithm for nearest neighbor search based on a low er bound tree[J]. Pattern Recognition, 2007, 40 (2) :360-375.
                            </a>
                        </p>
                        <p id="57">
                            <a id="bibliography_13" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011600738499&amp;v=Mjk1NjJadUh5am1VTGJJSUY4WGJ4Yz1OaWZPZmJLN0h0RE5xWTlGWStnSENIVXdvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZQ==&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[13]</b> LIAW Y C, LEOU M L, WU C M. Fast exact k nearest neighbors search using an orthogonal search tree[J].Pattern Recognition, 2010, 43 (6) :2351-2358.
                            </a>
                        </p>
                        <p id="59">
                            <a id="bibliography_14" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DXKX201709008&amp;v=MzI2NzR6cXFCdEdGckNVUkxPZVplUm5GeWprVUxyTklUWEFkckc0SDliTXBvOUZiSVFLREg4NHZSNFQ2ajU0TzM=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[14]</b>钱途, 钱江波, 董一鸿, 等. SLSB-forest:高维数据的近似k近邻查询[J].电信科学, 2017, 33 (9) :58-68.
                            </a>
                        </p>
                        <p id="61">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=An Improved Location Difference of Multiple Distances Based Nearest Neighbors Searching Algorithm">

                                <b>[15]</b> YANG Liu, DONG Limei, BI Xiaoru. An improved location difference of multiple distances based nearest neighbors searching algorithm[J]. International Journal for Light and Electron Optics, 2016, 127 (22) :10838-10843.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJC201902028" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201902028&amp;v=MjUxODNVUkxPZVplUm5GeWprVUxyS0x6N0JiYkc0SDlqTXJZOUhiSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckM=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="1" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
