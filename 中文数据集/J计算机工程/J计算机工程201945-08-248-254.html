<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637129055092931250%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJC201908041%26RESULT%3d1%26SIGN%3d%252bgel3XTqEnbK4JLk66ebJueJ1qQ%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201908041&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201908041&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201908041&amp;v=MDAyOTJGQ2puVUxyQUx6N0JiYkc0SDlqTXA0OUJaWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUnE=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#57" data-title="0 概述 ">0 概述</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#63" data-title="1 语音情感识别方法 ">1 语音情感识别方法</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#67" data-title="1.1 MFCC特征提取">1.1 MFCC特征提取</a></li>
                                                <li><a href="#88" data-title="1.2 语谱图特征提取">1.2 语谱图特征提取</a></li>
                                                <li><a href="#100" data-title="1.3 特征融合">1.3 特征融合</a></li>
                                                <li><a href="#118" data-title="1.4 样本分类">1.4 样本分类</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#123" data-title="2 实验结果与分析 ">2 实验结果与分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#124" data-title="2.1 实验数据集">2.1 实验数据集</a></li>
                                                <li><a href="#128" data-title="2.2 结果分析">2.2 结果分析</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#148" data-title="3 结束语 ">3 结束语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#66" data-title="&lt;b&gt;图1 基于MKL的MFCC和语谱图特征融合语音情感识别模型&lt;/b&gt;"><b>图1 基于MKL的MFCC和语谱图特征融合语音情感识别模型</b></a></li>
                                                <li><a href="#90" data-title="&lt;b&gt;图2 7种情感语谱图&lt;/b&gt;"><b>图2 7种情感语谱图</b></a></li>
                                                <li><a href="#99" data-title="&lt;b&gt;图3 CNN特征提取&lt;/b&gt;"><b>图3 CNN特征提取</b></a></li>
                                                <li><a href="#116" data-title="&lt;b&gt;图4 多核学习结构&lt;/b&gt;"><b>图4 多核学习结构</b></a></li>
                                                <li><a href="#131" data-title="&lt;b&gt;图5 MFCC+SVM在EMO-DB中的情感识别混淆矩阵&lt;/b&gt;"><b>图5 MFCC+SVM在EMO-DB中的情感识别混淆矩阵</b></a></li>
                                                <li><a href="#133" data-title="&lt;b&gt;图6 CNN+SVM在EMO-DB中的情感识别混淆矩阵&lt;/b&gt;"><b>图6 CNN+SVM在EMO-DB中的情感识别混淆矩阵</b></a></li>
                                                <li><a href="#135" data-title="&lt;b&gt;图7 MFCC+CNN+MKL在EMO-DB中的情感识别混淆矩阵&lt;/b&gt;"><b>图7 MFCC+CNN+MKL在EMO-DB中的情感识别混淆矩阵</b></a></li>
                                                <li><a href="#138" data-title="&lt;b&gt;图8 MFCC+SVM在CASIA中的情感识别混淆矩阵&lt;/b&gt;"><b>图8 MFCC+SVM在CASIA中的情感识别混淆矩阵</b></a></li>
                                                <li><a href="#140" data-title="&lt;b&gt;图9 CNN+SVM在CASIA中的情感识别混淆矩阵&lt;/b&gt;"><b>图9 CNN+SVM在CASIA中的情感识别混淆矩阵</b></a></li>
                                                <li><a href="#142" data-title="&lt;b&gt;图10 MFCC+SVM在CASIA中的情感识别混淆矩阵&lt;/b&gt;"><b>图10 MFCC+SVM在CASIA中的情感识别混淆矩阵</b></a></li>
                                                <li><a href="#146" data-title="&lt;b&gt;表1 不同方法准确率对比&lt;/b&gt;"><b>表1 不同方法准确率对比</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="175">


                                    <a id="bibliography_1" title=" DAHAKE P P, SHAW K, MALATHI P.Speaker dependent speech emotion recognition using MFCC and support vector machine[C]//Proceedings of International Conference on Automatic Control and Dynamic Optimization Techniques.Washington D.C., USA:IEEE Press, 2017:1080-1084." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Speaker dependent speech emotion recognition using MFCC and Support Vector Machine">
                                        <b>[1]</b>
                                         DAHAKE P P, SHAW K, MALATHI P.Speaker dependent speech emotion recognition using MFCC and support vector machine[C]//Proceedings of International Conference on Automatic Control and Dynamic Optimization Techniques.Washington D.C., USA:IEEE Press, 2017:1080-1084.
                                    </a>
                                </li>
                                <li id="177">


                                    <a id="bibliography_2" title=" 韩文静.语音情感识别关键技术研究[D].哈尔滨:哈尔滨工业大学, 2013." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CDFD&amp;filename=1014084817.nh&amp;v=MTIzODI4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUnFGQ2puVUxyQVZGMjZHck93R3RuTnFKRWJQSVFLREg=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         韩文静.语音情感识别关键技术研究[D].哈尔滨:哈尔滨工业大学, 2013.
                                    </a>
                                </li>
                                <li id="179">


                                    <a id="bibliography_3" title=" LI Longfei, ZHAO Yong, JIANG Dongmei, et al.Hybrid deep neural network-hidden markov model based speech emotion recognition[C]//Proceedings of Humaine Association Conference on Affective Computing and Intelligent Interaction.Washington D.C., USA:IEEE Press, 2013:312-317." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Hybrid Deep Neural Network-Hidden Markov Model (DNN-HMM)Based Speech Emotion Recognition">
                                        <b>[3]</b>
                                         LI Longfei, ZHAO Yong, JIANG Dongmei, et al.Hybrid deep neural network-hidden markov model based speech emotion recognition[C]//Proceedings of Humaine Association Conference on Affective Computing and Intelligent Interaction.Washington D.C., USA:IEEE Press, 2013:312-317.
                                    </a>
                                </li>
                                <li id="181">


                                    <a id="bibliography_4" title=" SCHULLER B, M&#220;LLER R, LANG M, et al.Speaker independent emotion recognition by early fusion of acoustic and linguistic features within ensembles[C]//Proceedings of the 9th European Conference on Speech Communication and Technology.[S.l.]:Curran Associates Inc., 2005:805-808." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Speaker independent emotion recognition by early fusion of acoustic and linguistic features within ensembles">
                                        <b>[4]</b>
                                         SCHULLER B, M&#220;LLER R, LANG M, et al.Speaker independent emotion recognition by early fusion of acoustic and linguistic features within ensembles[C]//Proceedings of the 9th European Conference on Speech Communication and Technology.[S.l.]:Curran Associates Inc., 2005:805-808.
                                    </a>
                                </li>
                                <li id="183">


                                    <a id="bibliography_5" title=" &#214;ZSEVEN T.Investigation of the effect of spectrogram images and different texture analysis methods on speech emotion recognition[J].Applied Acoustics, 2018, 142:70-77." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESA509EBE6C47A4F5094A4BCFABB6E03E5&amp;v=MTc0MjFneVUzV0JIZjhlVVJzK2FDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZDcGJRMzVOeGl4THE4eGFBPU5pZk9mY0s5SHRpNTNmcERGKzhJZlhoUHloWWE3azU1Tw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                         &#214;ZSEVEN T.Investigation of the effect of spectrogram images and different texture analysis methods on speech emotion recognition[J].Applied Acoustics, 2018, 142:70-77.
                                    </a>
                                </li>
                                <li id="185">


                                    <a id="bibliography_6" title=" MIRSAMADI S, BARSOUM E, ZHANG Cha.Auto-matic speech emotion recognition using recurrent neural networks with local attention[C]//Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing.Washington D.C., USA:IEEE Press, 2017:2227-2231." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Auto-matic speech emotion recognition using recurrent neural networks with local attention">
                                        <b>[6]</b>
                                         MIRSAMADI S, BARSOUM E, ZHANG Cha.Auto-matic speech emotion recognition using recurrent neural networks with local attention[C]//Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing.Washington D.C., USA:IEEE Press, 2017:2227-2231.
                                    </a>
                                </li>
                                <li id="187">


                                    <a id="bibliography_7" title=" HUANG C W, NARAYANAN S S.Deep convolutional recurrent neural network with attention mechanism for robust speech emotion recognition[C]//Proceedings of IEEE International Conference on Multimedia and Expo.Washington D.C., USA:IEEE Press, 2017:583-588." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep convolutional recurrent neural network with attention mechanism for robust speech emotion recognition">
                                        <b>[7]</b>
                                         HUANG C W, NARAYANAN S S.Deep convolutional recurrent neural network with attention mechanism for robust speech emotion recognition[C]//Proceedings of IEEE International Conference on Multimedia and Expo.Washington D.C., USA:IEEE Press, 2017:583-588.
                                    </a>
                                </li>
                                <li id="189">


                                    <a id="bibliography_8" title=" XIA Rui, LIU Yang.A multi-task learning framework for emotion recognition using 2D continuous space[J].IEEE Transactions on Affective Computing, 2017, 8 (1) :3-14." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A multi-task learning framework for emotion recognition using 2D continuous space">
                                        <b>[8]</b>
                                         XIA Rui, LIU Yang.A multi-task learning framework for emotion recognition using 2D continuous space[J].IEEE Transactions on Affective Computing, 2017, 8 (1) :3-14.
                                    </a>
                                </li>
                                <li id="191">


                                    <a id="bibliography_9" title=" 卢官明, 袁亮, 杨文娟, 等.基于长短期记忆和卷积神经网络的语音情感识别[J].南京邮电大学学报 (自然科学版) , 2018, 38 (5) :63-69." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=NJYD201805014&amp;v=MTg3MTZVTHJBS3lmU2FyRzRIOW5NcW85RVlJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVScUZDam4=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                         卢官明, 袁亮, 杨文娟, 等.基于长短期记忆和卷积神经网络的语音情感识别[J].南京邮电大学学报 (自然科学版) , 2018, 38 (5) :63-69.
                                    </a>
                                </li>
                                <li id="193">


                                    <a id="bibliography_10" title=" 陈晓敏.基于时序深度学习模型的语音情感识别方法研究[D].哈尔滨:哈尔滨工业大学, 2018." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=1018896203.nh&amp;v=MTk3MTh0R0ZyQ1VSTE9lWmVScUZDam5VTHJBVkYyNkZydXhHTlBNckpFYlBJUUtESDg0dlI0VDZqNTRPM3pxcUI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[10]</b>
                                         陈晓敏.基于时序深度学习模型的语音情感识别方法研究[D].哈尔滨:哈尔滨工业大学, 2018.
                                    </a>
                                </li>
                                <li id="195">


                                    <a id="bibliography_11" title=" 曾润华, 张树群.改进卷积神经网络的语音情感识别方法[J].应用科学学报, 2018, 36 (5) :837-844" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=YYKX201805011&amp;v=MTk4OTVxcUJ0R0ZyQ1VSTE9lWmVScUZDam5VTHJBUERUQWRyRzRIOW5NcW85RVpZUUtESDg0dlI0VDZqNTRPM3o=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[11]</b>
                                         曾润华, 张树群.改进卷积神经网络的语音情感识别方法[J].应用科学学报, 2018, 36 (5) :837-844
                                    </a>
                                </li>
                                <li id="197">


                                    <a id="bibliography_12" title=" HAN Kun, YU Dong, TASHEV I.Speech emotion recognition using deep neural network and extreme learning machine[EB/OL].[2018-10-20].http://193.6.4.39/～czap/letoltes/IS14/IS2014/PDF/AUTHOR/IS140441.PDF." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Speech emotion recognition using deep neural network and extreme learning machine">
                                        <b>[12]</b>
                                         HAN Kun, YU Dong, TASHEV I.Speech emotion recognition using deep neural network and extreme learning machine[EB/OL].[2018-10-20].http://193.6.4.39/～czap/letoltes/IS14/IS2014/PDF/AUTHOR/IS140441.PDF.
                                    </a>
                                </li>
                                <li id="199">


                                    <a id="bibliography_13" title=" LIM W, JANG D, LEE T.Speech emotion recognition using convolutional and recurrent neural networks[C]//Proceedings of Signal and Information Processing Asso-ciation Annual Summit and Conference.Washington D.C., USA:IEEE Press, 2016:1-4." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Speech emotion recognition using convolutional and recurrent neural networks">
                                        <b>[13]</b>
                                         LIM W, JANG D, LEE T.Speech emotion recognition using convolutional and recurrent neural networks[C]//Proceedings of Signal and Information Processing Asso-ciation Annual Summit and Conference.Washington D.C., USA:IEEE Press, 2016:1-4.
                                    </a>
                                </li>
                                <li id="201">


                                    <a id="bibliography_14" title=" FAYEK H M, LECH M, CAVEDON L.Evaluating deep learning architectures for speech emotion recognition[J].Neural Networks, 2017, 92 (8) :60-68." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESFF3B6B9DC93B19963C39D06D8B75C21E&amp;v=MDU0NDdNZm4wd3hoQVFtVHgwUEgvazJCcEhmcmZuUjd2cUNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkNwYlEzNU54aXhMcTh4YUE9TmlmT2ZjWE9IYVBLM1lZeEYrSQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[14]</b>
                                         FAYEK H M, LECH M, CAVEDON L.Evaluating deep learning architectures for speech emotion recognition[J].Neural Networks, 2017, 92 (8) :60-68.
                                    </a>
                                </li>
                                <li id="203">


                                    <a id="bibliography_15" title=" 汪洪桥, 孙富春, 蔡艳宁, 等.多核学习方法[J].自动化学报, 2010, 36 (8) :1037-1050." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201008001&amp;v=MjE3MjZHNEg5SE1wNDlGWllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJxRkNqblVMckFLQ0xmWWI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[15]</b>
                                         汪洪桥, 孙富春, 蔡艳宁, 等.多核学习方法[J].自动化学报, 2010, 36 (8) :1037-1050.
                                    </a>
                                </li>
                                <li id="205">


                                    <a id="bibliography_16" title=" RAKOTOMAMONJY A, BACH F R, CANU S, et al.SimpleMKL[J].Journal of Machine Learning Research, 2008, 9 (11) :2491-2521." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=SimpleMKL">
                                        <b>[16]</b>
                                         RAKOTOMAMONJY A, BACH F R, CANU S, et al.SimpleMKL[J].Journal of Machine Learning Research, 2008, 9 (11) :2491-2521.
                                    </a>
                                </li>
                                <li id="207">


                                    <a id="bibliography_17" title=" ONG C S, SMOLA A J, WILLIAMSON R C.Learning the kernel with hyperkernels[J].Journal of Machine Learning Research, 2005, 6 (7) :1043-1071." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning the kernel with hyperkernels">
                                        <b>[17]</b>
                                         ONG C S, SMOLA A J, WILLIAMSON R C.Learning the kernel with hyperkernels[J].Journal of Machine Learning Research, 2005, 6 (7) :1043-1071.
                                    </a>
                                </li>
                                <li id="209">


                                    <a id="bibliography_18" title=" AIOLLI F, DONINI M.EasyMKL:a scalable multiple kernel learning algorithm[J].Neurocomputing, 2015, 169:215-224." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES875C4EAEC89C18E570483F8BB58785F6&amp;v=MTQ3NzF2RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZDcGJRMzVOeGl4THE4eGFBPU5pZk9mYnUvRzZMSTJ2NHdGK01HZjMweHVoTVU2anQxU3ducTNtQXdjYldjUU15WkNPTg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[18]</b>
                                         AIOLLI F, DONINI M.EasyMKL:a scalable multiple kernel learning algorithm[J].Neurocomputing, 2015, 169:215-224.
                                    </a>
                                </li>
                                <li id="211">


                                    <a id="bibliography_19" title=" PORIA S, CHATURVEDI I, CAMBRIA E, et al.Convolutional MKL based multimodal emotion recognition and sentiment analysis[C]//Proceedings of the 16th International Conference on Data Mining.Washington D.C., USA:IEEE Press, 2016:439-448." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Convolutional MKL based multimodal emotion recognition and sentiment analysis">
                                        <b>[19]</b>
                                         PORIA S, CHATURVEDI I, CAMBRIA E, et al.Convolutional MKL based multimodal emotion recognition and sentiment analysis[C]//Proceedings of the 16th International Conference on Data Mining.Washington D.C., USA:IEEE Press, 2016:439-448.
                                    </a>
                                </li>
                                <li id="213">


                                    <a id="bibliography_20" title=" JIN Yun, SONG Peng, ZHENG Wenming, et al.Novel feature fusion method for speech emotion recognition based on multiple kernel learning[J].Journal of Southeast University, 2013, 29 (2) :129-133." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DNDY201302005&amp;v=MTE1MzhRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJxRkNqblVMckFJU1BQZDdHNEg5TE1yWTlGWVk=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[20]</b>
                                         JIN Yun, SONG Peng, ZHENG Wenming, et al.Novel feature fusion method for speech emotion recognition based on multiple kernel learning[J].Journal of Southeast University, 2013, 29 (2) :129-133.
                                    </a>
                                </li>
                                <li id="215">


                                    <a id="bibliography_21" title=" JIN Yun, SONG Peng, ZHENG Wenming, et al.Speaker-independent speech emotion recognition based on two-layer multiple kernel learning[J].IEICE Transactions on Information and Systems, 2013, 96 (10) :2286-2289." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Speakerindependent speech emotion recognition based on two-layer multiple kernel learning">
                                        <b>[21]</b>
                                         JIN Yun, SONG Peng, ZHENG Wenming, et al.Speaker-independent speech emotion recognition based on two-layer multiple kernel learning[J].IEICE Transactions on Information and Systems, 2013, 96 (10) :2286-2289.
                                    </a>
                                </li>
                                <li id="217">


                                    <a id="bibliography_22" title=" XU Xinzhou, DENG Jun, CUMMINS N, et al.A two-dimensional framework of multiple kernel subspace learning for recognizing emotion in speech[J].IEEE/ACM Transactions on Audio, Speech and Language Processing, 2017, 25 (7) :1436-1449." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCMD85A04FDC8E1EF2D425D4BDEEDD65FA1&amp;v=MDE4NjlEb0pUQTJXMldkQkRiU1JNOHVlQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQ3BiUTM1TnhpeExxOHhhQT1OaWZJWThld0c2RE1xL2t4RitONkRRbFB6V0lYNg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[22]</b>
                                         XU Xinzhou, DENG Jun, CUMMINS N, et al.A two-dimensional framework of multiple kernel subspace learning for recognizing emotion in speech[J].IEEE/ACM Transactions on Audio, Speech and Language Processing, 2017, 25 (7) :1436-1449.
                                    </a>
                                </li>
                                <li id="219">


                                    <a id="bibliography_23" title=" SUN Yaxin, WEN Guihua, WANG Jiabing.Weighted spectral features based on local hu moments for speech emotion recognition[J].Biomedical Signal Processing and Control, 2015, 18 (4) :80-90." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14121200104097&amp;v=MTk5MTJVTHZMSUZ3WGJ4bz1OaWZPZmJLOEg5UE5yWTlGWmVzTERIVStvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[23]</b>
                                         SUN Yaxin, WEN Guihua, WANG Jiabing.Weighted spectral features based on local hu moments for speech emotion recognition[J].Biomedical Signal Processing and Control, 2015, 18 (4) :80-90.
                                    </a>
                                </li>
                                <li id="221">


                                    <a id="bibliography_24" title=" MAO Qirong, DONG Ming, HUANG Zhengwei, et al.Learning salient features for speech emotion recognition using convolutional neural networks[J].IEEE Tran-sactions on Multimedia, 2014, 16 (8) :2203-2213." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning Salient Features for Speech Emotion Recognition Using Convolutional Neural Networks">
                                        <b>[24]</b>
                                         MAO Qirong, DONG Ming, HUANG Zhengwei, et al.Learning salient features for speech emotion recognition using convolutional neural networks[J].IEEE Tran-sactions on Multimedia, 2014, 16 (8) :2203-2213.
                                    </a>
                                </li>
                                <li id="223">


                                    <a id="bibliography_25" title=" BURKHARDT F, PAESCHKE A, ROLFES M, et al.A database of German emotional speech[C]//Proceedings of the 9th European Conference on Speech Communication and Technology.[S.l.]:Curran Associates Inc., 2005:1-4." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Database of German Emotional Speech">
                                        <b>[25]</b>
                                         BURKHARDT F, PAESCHKE A, ROLFES M, et al.A database of German emotional speech[C]//Proceedings of the 9th European Conference on Speech Communication and Technology.[S.l.]:Curran Associates Inc., 2005:1-4.
                                    </a>
                                </li>
                                <li id="225">


                                    <a id="bibliography_26" title=" KUCHIBHOTLA S, VANKAYALAPATI H D, ANNE K R.An optimal two stage feature selection for speech emotion recognition using acoustic features[J].Inter-national Journal of Speech Technology, 2016, 19 (4) :657-667." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=An Optimal Two Stage Feature Selection for Speech Emotion Recognition using Acoustic Features">
                                        <b>[26]</b>
                                         KUCHIBHOTLA S, VANKAYALAPATI H D, ANNE K R.An optimal two stage feature selection for speech emotion recognition using acoustic features[J].Inter-national Journal of Speech Technology, 2016, 19 (4) :657-667.
                                    </a>
                                </li>
                                <li id="227">


                                    <a id="bibliography_27" title=" ZHU Bing, ZHOU Wenkai, WANG Yutian, et al.End-to-end speech emotion recognition based on neural network[C]//Proceedings of the 17th International Con-ference on Communication Technology.Washington D.C., USA:IEEE Press, 2017:1634-1638." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=End-to-end speech emotion recognition based on neural network">
                                        <b>[27]</b>
                                         ZHU Bing, ZHOU Wenkai, WANG Yutian, et al.End-to-end speech emotion recognition based on neural network[C]//Proceedings of the 17th International Con-ference on Communication Technology.Washington D.C., USA:IEEE Press, 2017:1634-1638.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJC" target="_blank">计算机工程</a>
                2019,45(08),248-254 DOI:10.19678/j.issn.1000-3428.0053232            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于多核学习特征融合的语音情感识别方法</b></span>
 <span class="shoufa"></span>                                     </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E5%BF%A0%E6%B0%91&amp;code=37496644&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王忠民</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%88%98%E6%88%88&amp;code=40490649&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">刘戈</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%AE%8B%E8%BE%89&amp;code=39108853&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">宋辉</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E8%A5%BF%E5%AE%89%E9%82%AE%E7%94%B5%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%AD%A6%E9%99%A2&amp;code=1698419&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">西安邮电大学计算机学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E8%A5%BF%E5%AE%89%E9%82%AE%E7%94%B5%E5%A4%A7%E5%AD%A6%E9%99%95%E8%A5%BF%E7%9C%81%E7%BD%91%E7%BB%9C%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B8%8E%E6%99%BA%E8%83%BD%E5%A4%84%E7%90%86%E9%87%8D%E7%82%B9%E5%AE%9E%E9%AA%8C%E5%AE%A4&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">西安邮电大学陕西省网络数据分析与智能处理重点实验室</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>在语音情感识别中提取梅尔频率倒谱系数 (MFCC) 会丢失谱特征信息, 导致情感识别准确率较低。为此, 提出一种结合MFCC和语谱图特征的语音情感识别方法。从音频信号中提取MFCC特征, 将信号转换为语谱图, 利用卷积神经网络提取图像特征。在此基础上, 使用多核学习算法融合音频特征, 并将生成的核函数应用于支持向量机进行情感分类。在2种语音情感数据集上的实验结果表明, 与单一特征的分类器相比, 该方法的语音情感识别准确率高达96%。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%AF%AD%E9%9F%B3%E6%83%85%E6%84%9F%E8%AF%86%E5%88%AB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">语音情感识别;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%A4%9A%E6%A0%B8%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">多核学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">卷积神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%A2%85%E5%B0%94%E9%A2%91%E7%8E%87%E5%80%92%E8%B0%B1%E7%B3%BB%E6%95%B0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">梅尔频率倒谱系数;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%AF%AD%E8%B0%B1%E5%9B%BE&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">语谱图;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    王忠民 (1967—) , 男, 教授、博士, 主研方向为语音情感识别、嵌入式系统、智能信息处理;;
                                </span>
                                <span>
                                    刘戈, 硕士研究生;;
                                </span>
                                <span>
                                    宋辉, 讲师、硕士。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-11-23</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金 (61373116);</span>
                                <span>陕西省科技统筹创新工程计划项目 (2016KTZDGY04-01);</span>
                                <span>陕西省教育厅专项科研计划项目 (16JK1706);</span>
                                <span>西安市科技局科技计划项目 (2017084CG/RC047 (XAYD001) );</span>
                                <span>西安邮电大学研究生创新创业基金 (CXJJ2017061);</span>
                    </p>
            </div>
                    <h1><b>Speech Emotion Recognition Method Based on Multiple Kernel Learning Feature Fusion</b></h1>
                    <h2>
                    <span>WANG Zhongmin</span>
                    <span>LIU Ge</span>
                    <span>SONG Hui</span>
            </h2>
                    <h2>
                    <span>School of Computer Science and Technology, Xi'an University of Posts and Telecommunications</span>
                    <span>Shaanxi Key Laboratory of Netw ork Data Analysis and Intelligent Processing, Xi'an University of Posts and Telecommunications</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Extracting the Mel-Frequency Cepstral Coefficients (MFCC) in speech emotion recognition will lose the spectral feature information, resulting in a low accuracy of emotion recognition.Therefore, a speech emotion recognition method combining MFCC and spectrogram features is proposed.The MFCC features are extracted from the audio signal, the signal is converted into a spectral map, and the image features are extracted using a Convolutional Neural Network (CNN) .On this basis, the Multiple Kernel Learning (MKL) algorithm is used to fuse the audio features, and the generated kernel functions are used to support the vector machine for emotion classification.Experimental results in two kinds of speech emotion data sets show that the speech emotion recognition accuracy of this method is as high as 96% compared with the classifier based on single feature.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=speech%20emotion%20recognition&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">speech emotion recognition;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Multiple%20Kernel%20Learning%20(MKL)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Multiple Kernel Learning (MKL) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Convolution%20Neural%20Network%20(CNN)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Convolution Neural Network (CNN) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Mel-Frequency%20Cepstral%20Coefficients%20(MFCC)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Mel-Frequency Cepstral Coefficients (MFCC) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=spectrogram&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">spectrogram;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-11-23</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="57" name="57" class="anchor-tag">0 概述</h3>
                <div class="p1">
                    <p id="58">语言是人类最重要的交流方式之一, 且语音作为语言交流的载体具有丰富的情感信息, 因此语音情感识别成为人机交互中的重要组成部分。语音情感识别通过处理分析语音信号来识别说话人情感状态<citation id="229" type="reference"><link href="175" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>, 人机交互系统利用具有情感属性的计算机检查用户的情绪并做出响应, 以提高用户的满意度<citation id="230" type="reference"><link href="177" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>。随着计算机技术的发展, 对具有情感识别能力的口语对话系统提出更高的要求, 因此, 研究语音情感识别技术具有重要意义。</p>
                </div>
                <div class="p1">
                    <p id="59">语音情感识别的声学特征可归纳为韵律特征、谱相关特征和音质特征这3种类型。常用的韵律特征有基频、能量、时长等。谱特征有LPC (Linear Predictor Coefficient) 、LPCC (Linear Predictor Cepstral Coefficient) 、梅尔频率倒谱系数 (Mel-Frequency Cepstral Coefficients, MFCC) 等。音质特征有频率微扰和振幅微扰、声门参数等, 被广泛应用于语音情感识别研究中。 文献<citation id="231" type="reference">[<a class="sup">2</a>]</citation>提取音频信号中音高、共振峰、过零率、MFCC和其统计参数的混合特征, 在支持向量机 (Support Vector Machine, SVM) 中验证不同核函数的分类准确率, 结果表明, MFCC和共振峰的融合特征使情绪识别具有较好的准确性。文献<citation id="232" type="reference">[<a class="sup">3</a>]</citation>建立梅尔频率倒谱系数训练的混合DNN隐马尔可夫模型 (Hidden Markov Model, HMM) , 并与GMM-HMM进行比较。文献<citation id="233" type="reference">[<a class="sup">4</a>]</citation>使用276种声学和语言特征, 取得了84.84%的准确率。文献<citation id="234" type="reference">[<a class="sup">5</a>]</citation>通过对比语音谱图图像的纹理与信号声学获得的特征在SVM的识别准确率, 证明纹理分析方法可用于语音情感识别。</p>
                </div>
                <div class="p1">
                    <p id="60">随着深度学习的发展, 研究人员使用卷积神经网络 (Convolutional Neural Network, CNN) <citation id="237" type="reference"><link href="185" rel="bibliography" /><link href="187" rel="bibliography" /><sup>[<a class="sup">6</a>,<a class="sup">7</a>]</sup></citation>、循环神经网络 (Recurrent Neural Network, RNN) <citation id="238" type="reference"><link href="189" rel="bibliography" /><link href="191" rel="bibliography" /><sup>[<a class="sup">8</a>,<a class="sup">9</a>]</sup></citation>、深度信念网络 (Deep Belief Network, DBN) <citation id="239" type="reference"><link href="193" rel="bibliography" /><link href="195" rel="bibliography" /><link href="197" rel="bibliography" /><sup>[<a class="sup">10</a>,<a class="sup">11</a>,<a class="sup">12</a>]</sup></citation>等方法进行语音特征提取和情感分类。文献<citation id="235" type="reference">[<a class="sup">13</a>]</citation>结合CNN和RNN提出Time Distributed CNNs, 得到88%的平均识别准确率。文献<citation id="236" type="reference">[<a class="sup">14</a>]</citation>研究了前馈神经网络和循环神经网络在语音情感识别中的优点和局限性, 给出在IEMOCAP数据集上说话人无关的语音情感识定量和定性评估结果。</p>
                </div>
                <div class="p1">
                    <p id="61">多核学习 (Multiple Kernel Learning, MKL) 是一种特征融合方法, 可以处理异质或不规则数据、样本的不均匀分布等问题<citation id="240" type="reference"><link href="203" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>。MKL组织多种特征成组, 使每个组有自己的核函数, 这些不同的核函数组合成一个新的核函数, 并将该函数应用于分类器中。为解决MKL中多核函数组合的问题, 研究人员提出改进MKL方法, 如SimpleMKL<citation id="241" type="reference"><link href="205" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>、基于超核的多核方法<citation id="242" type="reference"><link href="207" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>、EasyMKL<citation id="243" type="reference"><link href="209" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>等。MKL在解决多模态情感识别和多特征融合语音识别方面具有较好的性能。文献<citation id="244" type="reference">[<a class="sup">19</a>]</citation>使用MKL对文本、视频、音频3种情感特征进行特征融合, 得到76.85%的平均情感识别准确率。文献<citation id="245" type="reference">[<a class="sup">20</a>]</citation>使用MKL方法将语音的局部声学特征和全局特征进行融合, 得到81.1%的平均情感识别准确率。文献<citation id="246" type="reference">[<a class="sup">21</a>]</citation>提出一种两层MKL情感识别方法, 使用第1层MKL进行特征提取, 再利用第2层MKL进行情感识别。文献<citation id="247" type="reference">[<a class="sup">22</a>]</citation>提出多核子空间学习二维框架, 以解决MKL通常仅为多个内核提供一个非负映射方向, 但可能导致有价值的信息丢失。</p>
                </div>
                <div class="p1">
                    <p id="62">单一类型特征通常难以全面表达样本信息。因此, 研究人员采用多种特征融合方法, 从特征选择、特征融合、分类模型优化等方面描述样本。MFCC是语音情感识别中最常用的特征类型, 但没有考虑单帧的Mel滤波器的相邻系数之间和相邻帧的Mel滤波器系数之间的关系, 且会失去有用特征<citation id="248" type="reference"><link href="219" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>。另外, 特征融合方法通常是将多个特征向量简单的组合, 但不能充分利用每种特征识别语音情感信息。基于上述问题, 本文提出一种基于多核学习特征融合的语音情感识别方法。利用CNN从语谱图提取特征, 使用多核学习融合特征, 以达到提高语音情感识别准确率的目的。</p>
                </div>
                <h3 id="63" name="63" class="anchor-tag">1 语音情感识别方法</h3>
                <div class="p1">
                    <p id="64">MFCC特征和CNN提取的语谱图特征, 从不同的角度描述样本信息, 这2种特征在情感识别任务中都获得较好的结果, 采用合适的特征融合方法并结合两种特征的优势, 在情感识别任务中能够有效提高识别准确率。</p>
                </div>
                <div class="p1">
                    <p id="65">为提高不同类型的特征表达能力, 采用多核学习方法进行特征融合。在多核学习框架中, 每种特征分别输入对应的核函数进行映射。不同核函数通过组合得到由多个特征空间构建的组合空间。数据在新的特征空间中得到更好的表达, 能显著提高分类正确率或预测精度。基于MKL的MFCC和语谱图特征融合语音情感识别模型如图1所示。</p>
                </div>
                <div class="area_img" id="66">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201908041_066.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 基于MKL的MFCC和语谱图特征融合语音情感识别模型" src="Detail/GetImg?filename=images/JSJC201908041_066.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图1 基于MKL的MFCC和语谱图特征融合语音情感识别模型</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201908041_066.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="67" name="67">1.1 MFCC特征提取</h4>
                <div class="p1">
                    <p id="68">MFCC是基于人类听觉特性的特征, 广泛应用于语音识别等领域。由于人耳感知的声音高低与声音的频率成非线性关系, 而与声音频率的对数成线性相关, 因此构造出与赫兹频率成非线性对应关系的梅尔频率。赫兹频率和梅尔频率转换公式表示为:</p>
                </div>
                <div class="p1">
                    <p id="69" class="code-formula">
                        <mathml id="69"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Μ</mi><mi>e</mi><mi>l</mi><mrow><mo> (</mo><mi>f</mi><mo>) </mo></mrow><mo>=</mo><mn>1</mn><mspace width="0.25em" /><mn>1</mn><mn>2</mn><mn>5</mn><mrow><mi>ln</mi></mrow><mrow><mo> (</mo><mrow><mn>1</mn><mo>+</mo><mfrac><mi>f</mi><mrow><mn>7</mn><mn>0</mn><mn>0</mn></mrow></mfrac></mrow><mo>) </mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="70">其中, <i>f</i>为语音信号实际频率, 单位为Hz。</p>
                </div>
                <div class="p1">
                    <p id="71">在预处理阶段, 将语音信号分为<i>N</i>帧, 得到信号<mathml id="72"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>s</mi><msub><mrow></mrow><mi>n</mi></msub><mrow><mo> (</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn><mo>, </mo><mn>2</mn><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>Ν</mi></mrow><mo>) </mo></mrow></mrow></math></mathml>, 然后进行预加重、加窗等操作, 使信号平滑并对高频部分突显。</p>
                </div>
                <div class="p1">
                    <p id="73" class="code-formula">
                        <mathml id="73"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Η</mi><mrow><mo> (</mo><mi>z</mi><mo>) </mo></mrow><mo>=</mo><mn>1</mn><mo>-</mo><mi>μ</mi><mi>z</mi><msup><mrow></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="74">其中, <i>μ</i>一般取0.97, 使用Hamming窗作为窗函数。加窗方法如下:</p>
                </div>
                <div class="p1">
                    <p id="75" class="code-formula">
                        <mathml id="75"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><msup><mi>s</mi><mo>′</mo></msup><msub><mrow></mrow><mi>n</mi></msub><mo>=</mo><mrow><mo>[</mo><mrow><mn>0</mn><mo>.</mo><mn>5</mn><mn>4</mn><mo>-</mo><mn>0</mn><mo>.</mo><mn>4</mn><mn>6</mn><mi>cos</mi><mrow><mo> (</mo><mrow><mfrac><mrow><mn>2</mn><mtext>π</mtext><mrow><mo> (</mo><mrow><mi>n</mi><mo>-</mo><mn>1</mn></mrow><mo>) </mo></mrow></mrow><mrow><mi>Ν</mi><mo>-</mo><mn>1</mn></mrow></mfrac></mrow><mo>) </mo></mrow></mrow><mo>]</mo></mrow><mo>×</mo><mi>s</mi><msub><mrow></mrow><mi>n</mi></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mtext>s</mtext><mo>.</mo><mtext>t</mtext><mo>.</mo><mspace width="0.25em" /><mn>0</mn><mo>≤</mo><mi>n</mi><mo>≤</mo><mi>Ν</mi></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="76">将处理好的信号由第<i>i</i>帧的时域信号<mathml id="77"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub><mrow><mo> (</mo><mi>n</mi><mo>) </mo></mrow></mrow></math></mathml>转化为频域信号。</p>
                </div>
                <div class="p1">
                    <p id="78" class="code-formula">
                        <mathml id="78"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>S</mi><msub><mrow></mrow><mi>i</mi></msub><mrow><mo> (</mo><mi>k</mi><mo>) </mo></mrow><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mi>s</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mrow><mo> (</mo><mi>n</mi><mo>) </mo></mrow><mtext>e</mtext><msup><mrow></mrow><mrow><mfrac><mrow><mo>-</mo><mtext>j</mtext><mn>2</mn><mtext>π</mtext><mi>k</mi><mi>n</mi></mrow><mi>Ν</mi></mfrac></mrow></msup><mtext> </mtext><mtext>s</mtext><mo>.</mo><mtext>t</mtext><mo>.</mo><mspace width="0.25em" /><mn>1</mn><mo>≤</mo><mi>k</mi><mo>≤</mo><mi>Ν</mi><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="79">其中, N表示傅里叶变换的点数。</p>
                </div>
                <div class="p1">
                    <p id="80">定义一个由M个三角滤波器组成的滤波器组, 每个滤波器中心频率为<mathml id="81"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>f</mi><mrow><mo> (</mo><mi>m</mi><mo>) </mo></mrow><mo>, </mo><mi>m</mi><mo>=</mo><mn>1</mn><mo>, </mo><mn>2</mn><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>Μ</mi></mrow></math></mathml>, 在梅尔频率轴上等间隔分布。使用梅尔滤波器组过滤频域信号, 得到每一帧数据在该滤波器对应频段的能量值。之后对所得能量值取对数, 每个滤波器组输出的对数能量为:</p>
                </div>
                <div class="p1">
                    <p id="82" class="code-formula">
                        <mathml id="82"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>s</mi><mo stretchy="false"> (</mo><mi>m</mi><mo stretchy="false">) </mo><mo>=</mo><mrow><mi>ln</mi></mrow><mrow><mo> (</mo><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>Ν</mi><mo>-</mo><mn>1</mn></mrow></munderover><mrow><mrow><mo>|</mo><mrow><mi>X</mi><mo stretchy="false"> (</mo><mi>k</mi><mo stretchy="false">) </mo></mrow><mo>|</mo></mrow></mrow></mstyle><msup><mrow></mrow><mn>2</mn></msup><mi>Η</mi><mo stretchy="false"> (</mo><mi>k</mi><mo stretchy="false">) </mo></mrow><mo>) </mo></mrow><mspace width="0.25em" /><mtext>s</mtext><mo>.</mo><mtext>t</mtext><mo>.</mo><mspace width="0.25em" /><mn>0</mn><mo>≤</mo><mi>m</mi><mo>≤</mo><mi>Μ</mi><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>5</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="83">使用离散余弦变换, 得到梅尔频率倒谱系数, 有:</p>
                </div>
                <div class="p1">
                    <p id="84" class="code-formula">
                        <mathml id="84"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>c</mi><mo stretchy="false"> (</mo><mi>m</mi><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>Μ</mi></munderover><mi>s</mi></mstyle><mo stretchy="false"> (</mo><mi>m</mi><mo stretchy="false">) </mo><mrow><mi>cos</mi></mrow><mrow><mo> (</mo><mrow><mfrac><mrow><mtext>π</mtext><mi>m</mi></mrow><mi>Μ</mi></mfrac><mo stretchy="false"> (</mo><mi>n</mi><mo>-</mo><mn>0</mn><mo>.</mo><mn>5</mn><mo stretchy="false">) </mo></mrow><mo>) </mo></mrow><mspace width="0.25em" /><mtext>s</mtext><mo>.</mo><mtext>t</mtext><mo>.</mo><mspace width="0.25em" /><mn>1</mn><mo>≤</mo><mi>n</mi><mo>≤</mo><mi>Μ</mi><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>6</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="85">标准的MFCC通常只反映语音静态特性, 为使特征更能体现时域连续性, 可以用差分参数来表示。差分参数的计算方法为:</p>
                </div>
                <div class="p1">
                    <p id="86" class="code-formula">
                        <mathml id="86"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>d</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mi>C</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>-</mo><mi>C</mi><msub><mrow></mrow><mi>t</mi></msub><mo>, </mo><mi>t</mi><mo>&lt;</mo><mi>Κ</mi></mtd></mtr><mtr><mtd><mi>C</mi><msub><mrow></mrow><mi>t</mi></msub><mo>-</mo><mi>C</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>, </mo><mi>t</mi><mo>≥</mo><mi>Q</mi><mo>-</mo><mi>k</mi></mtd></mtr><mtr><mtd><mfrac><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>Κ</mi></munderover><mi>k</mi></mstyle><mo stretchy="false"> (</mo><mi>C</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>+</mo><mi>k</mi></mrow></msub><mo>-</mo><mi>C</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mi>k</mi></mrow></msub><mo stretchy="false">) </mo></mrow><mrow><msqrt><mrow><mn>2</mn><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>Κ</mi></munderover><mi>k</mi></mstyle><msup><mrow></mrow><mn>2</mn></msup></mrow></msqrt></mrow></mfrac><mo>, </mo><mtext>其</mtext><mtext>他</mtext></mtd></mtr></mtable></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>7</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="87">其中, <i>d</i><sub><i>t</i></sub>示第<i>t</i>个一阶差分, <i>C</i><sub><i>t</i></sub>表示第<i>t</i>个倒谱系数, <i>Q</i>为倒谱系数的阶数, <i>t</i>一般取1或2, 将上式结果代入就可得到二阶差分参数。</p>
                </div>
                <h4 class="anchor-tag" id="88" name="88">1.2 语谱图特征提取</h4>
                <div class="p1">
                    <p id="89">语谱图是语音信号经傅里叶分析显示的图形, 其有效结合语音的时域分析和频域分析特性, 能够直观地表示语音信号在时间序列上频谱的变化情况。文献<citation id="249" type="reference">[<a class="sup">24</a>]</citation>采用语谱图进行语音情感识别研究。语谱图使用3维方式表示语音的频谱特性, 纵轴表示频率, 横轴表示时间, 颜色的深浅表示特定频带的能量大小。通过语谱图可以得到大量与语音的语句特性相关的信息。因此可以使用CNN从语音对应的语谱图中提取有效的特征信息。7种情感的语谱图如图2所示。</p>
                </div>
                <div class="area_img" id="90">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201908041_090.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 7种情感语谱图" src="Detail/GetImg?filename=images/JSJC201908041_090.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图2 7种情感语谱图</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201908041_090.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="91">从语音信号转换并绘制语谱图的过程如下:</p>
                </div>
                <div class="p1">
                    <p id="92">1) 读取语音信号数据;</p>
                </div>
                <div class="p1">
                    <p id="93">2) 信号分帧加窗;</p>
                </div>
                <div class="p1">
                    <p id="94">3) 能量密度谱计算;</p>
                </div>
                <div class="p1">
                    <p id="95">4) 功率谱伪彩色显示;</p>
                </div>
                <div class="p1">
                    <p id="96">5) 显示语谱图。</p>
                </div>
                <div class="p1">
                    <p id="97">CNN一般由3部分组成, 第一部分是输入层, 第二部分是若干个卷积层和子采样层组成, 最后一部分是全连接的多层感知分类器, 特征图是卷积层的输出。在训练阶段学习得到各种滤波器, 滤波器对输入进行卷积后获得的一组特征映射。子采样也称为池化, 通常采用平均池化和最大池化的形式。 在卷积层之后引入池化层以减少参数的数量并避免过度拟合。另外, CNN还包含有全连接多层感知器分类器。</p>
                </div>
                <div class="p1">
                    <p id="98">图3所示为提取语谱图特征的CNN结构, 包括输入层、2个卷积层、2个池化层、1个全连接层。该网络的输入是尺寸为128像素×128像素的语谱图。第1个卷积层<i>C</i><sub>1</sub>由64个卷积核组成, 卷积核大小为5×5, 经过卷积得到的特征图用0填充边缘部分保持大小不变, 即128像素×128像素。卷积后连接ReLU激活函数引入非线性单元。在<i>C</i><sub>1</sub>后连接第1个池化层<i>S</i><sub>1</sub>, <i>S</i><sub>1</sub>使用大小为2×2的卷积核进行最大池化。通过池化层对特征图进行压缩, 降低计算复杂度并提取主要特征, 得到大小为64像素×64像素的特征图。第2个卷积层<i>C</i><sub>2</sub>由128个卷积核组成, 卷积核大小为5×5, 通过<i>C</i><sub>2</sub>得到128个尺寸为64像素×64像素的特征图, 使用ReLU作为激活函数。在<i>C</i><sub>2</sub>后连接第2个池化层<i>S</i><sub>2</sub>, <i>S</i><sub>2</sub>使用最大池化方法, 卷积核大小为2×2, <i>S</i><sub>2</sub>输出128个尺寸为32像素×32像素的特征图。最后是一个全连接层, 全连接层由1 024个神经元组成。经过全连接层得到1 024维特征向量。</p>
                </div>
                <div class="area_img" id="99">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201908041_099.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 CNN特征提取" src="Detail/GetImg?filename=images/JSJC201908041_099.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图3 CNN特征提取</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201908041_099.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="100" name="100">1.3 特征融合</h4>
                <div class="p1">
                    <p id="101">在核方法中, 数据由选择核<i>K</i> (<i>x</i>, <i>x</i>′) 表示。由于多种来源的样本特征有不同特性, 使用单核方法进行处理并不理想。研究表明使用多个核可以提高决策函数的可解释性和性能。</p>
                </div>
                <div class="p1">
                    <p id="102">使用多核学习方法进行特征融合, 实质上是将若干单个特征对应的基本核矩阵进行组合。基于多个基本核矩阵合成得到的多核矩阵, 可以实现异构数据融合, 并用于训练分类器。</p>
                </div>
                <div class="p1">
                    <p id="103">多尺度核的学习方法通过多核学习的框架将有多尺度特性的多尺度核函数组合起来。这种方法比通常的合成核方法更具灵活性, 能提供更完备的尺度选择。</p>
                </div>
                <div class="p1">
                    <p id="104">高斯径向基核是应用比较广泛的多尺度核函数, 即:</p>
                </div>
                <div class="p1">
                    <p id="105" class="code-formula">
                        <mathml id="105"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>k</mi><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>z</mi><mo stretchy="false">) </mo><mo>=</mo><mrow><mi>exp</mi></mrow><mrow><mo> (</mo><mrow><mo>-</mo><mfrac><mrow><mrow><mo stretchy="false">∥</mo><mi>x</mi><mo>-</mo><mi>z</mi><mo stretchy="false">∥</mo></mrow><msup><mrow></mrow><mn>2</mn></msup></mrow><mrow><mn>2</mn><mi>σ</mi><msup><mrow></mrow><mn>2</mn></msup></mrow></mfrac></mrow><mo>) </mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>8</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="106">将其多尺度化后可已得到不同尺度的核, 来完成多核学习任务。</p>
                </div>
                <div class="p1">
                    <p id="107" class="code-formula">
                        <mathml id="107"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mrow><mi>exp</mi></mrow><mrow><mo> (</mo><mrow><mo>-</mo><mfrac><mrow><mrow><mo stretchy="false">∥</mo><mi>x</mi><mo>-</mo><mi>z</mi><mo stretchy="false">∥</mo></mrow><msup><mrow></mrow><mn>2</mn></msup></mrow><mrow><mn>2</mn><mi>σ</mi><msubsup><mrow></mrow><mn>1</mn><mn>2</mn></msubsup></mrow></mfrac></mrow><mo>) </mo></mrow><mo>, </mo><mrow><mi>exp</mi></mrow><mrow><mo> (</mo><mrow><mo>-</mo><mfrac><mrow><mrow><mo stretchy="false">∥</mo><mi>x</mi><mo>-</mo><mi>z</mi><mo stretchy="false">∥</mo></mrow><msup><mrow></mrow><mn>2</mn></msup></mrow><mrow><mn>2</mn><mi>σ</mi><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></mfrac></mrow><mo>) </mo></mrow><mo>, </mo><mo>⋯</mo><mo>, </mo></mtd></mtr><mtr><mtd><mrow><mi>exp</mi></mrow><mrow><mo> (</mo><mrow><mo>-</mo><mfrac><mrow><mrow><mo stretchy="false">∥</mo><mi>x</mi><mo>-</mo><mi>z</mi><mo stretchy="false">∥</mo></mrow><msup><mrow></mrow><mn>2</mn></msup></mrow><mrow><mn>2</mn><mi>σ</mi><msubsup><mrow></mrow><mi>m</mi><mn>2</mn></msubsup></mrow></mfrac></mrow><mo>) </mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>9</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="108">其中, <i>σ</i><sub>1</sub>&lt;<i>σ</i><sub>2</sub>&lt;…&lt;<i>σ</i><sub><i>m</i></sub>, 当<i>σ</i>较小时, 高斯核函数可以很好拟合变化剧烈的样本;当<i>σ</i>较大时, 可以拟合平缓变化的样本。</p>
                </div>
                <div class="p1">
                    <p id="109">对MFCC特征和语谱图特征均采用多尺度高斯核函数。每类特征对应一个多尺度核, 该多尺度核由<i>M</i>个不同尺度的高斯核函数加权合成得到。</p>
                </div>
                <div class="p1">
                    <p id="110" class="code-formula">
                        <mathml id="110"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Κ</mi><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>z</mi><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Μ</mi></munderover><mi>β</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mi>k</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>z</mi><mo stretchy="false">) </mo><mspace width="0.25em" /><mtext>s</mtext><mo>.</mo><mtext>t</mtext><mo>.</mo><mi>β</mi><msub><mrow></mrow><mi>i</mi></msub><mo>≥</mo><mn>0</mn><mo>, </mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Μ</mi></munderover><mi>β</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mn>1</mn><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>0</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="111">其中, <i>β</i><sub><i>i</i></sub>是权值, <i>k</i><sub><i>i</i></sub>是基本核, <i>M</i>是基本核的总个数。大尺度的核可以较好地拟合平缓变化的区域, 在其他变化剧烈的区域会存在误差, 但可以通过小尺度的核进行弥补, 使函数有更好的拟合性能。2种特征最终得到2个合成核<i>K</i><sub>1</sub>和<i>K</i><sub>2</sub>。</p>
                </div>
                <div class="p1">
                    <p id="112">采用一种非平稳多核组合方法为2个合成核分配不同的权值。对于测试数据{<i>x</i><sub><i>i</i></sub>, <i>y</i><sub><i>i</i></sub>}<mathml id="113"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></msubsup></mrow></math></mathml>, x<sub>i</sub>是样本数据, y<sub>i</sub>是对应标签。最终数据的决策函数如下:</p>
                </div>
                <div class="p1">
                    <p id="114">f (x) =a<sub>1</sub> (x) K<sub>1</sub> (x<sub>t</sub>, x) +a<sub>2</sub> (x) K<sub>2</sub> (x<sub>t</sub>, x) +b      (11) </p>
                </div>
                <div class="p1">
                    <p id="115">多核学习结构如图4所示。</p>
                </div>
                <div class="area_img" id="116">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201908041_116.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 多核学习结构" src="Detail/GetImg?filename=images/JSJC201908041_116.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图4 多核学习结构</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201908041_116.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="117">在解决异构数据问题时, 将不同特征分量输入对应的核函数进行映射, 使数据在新的特征空间得到更好表达, 从而提高分类准确率。</p>
                </div>
                <h4 class="anchor-tag" id="118" name="118">1.4 样本分类</h4>
                <div class="p1">
                    <p id="119">SVM通过在特征空间中找到最佳的超平面使正类和负类样本的间隔最大, 利用核函数将样本特征映射到希尔伯特空间, 将原特征空间的线性不可分的问题变成线性可分。</p>
                </div>
                <div class="p1">
                    <p id="120">基于单个特征空间的单核方法在多个领域得到广泛应用, 但是对于异构特征采用单核未必能得到最优的映射。因此结合多核学习方法, 采用多核SVM进行分类。</p>
                </div>
                <div class="p1">
                    <p id="121">为将SVM应用于多分类问题, 需要构造合适的多类分类器。采用一对多的策略构造多分类SVM。这种策略为每一类分配一个分类器, 该类与其他类相匹配。对于<i>N</i>类的分类任务只需要构造<i>N</i>个分类器, 这使得它的计算效率很高。由于每种类别仅有一个分类表示, 因此可以通过检查相应分类器来获取有关该类的信息。</p>
                </div>
                <div class="p1">
                    <p id="122">在模型训练时, 为happiness、anger、boredom、disgust、fear、sadness、normal等情感各训练一个分类器, 每个分类器将这几种感情中的一种作为一类, 将其余情感数据作为另一类, 使用相对应的分类器进行训练和测试, 得到分类结果。</p>
                </div>
                <h3 id="123" name="123" class="anchor-tag">2 实验结果与分析</h3>
                <h4 class="anchor-tag" id="124" name="124">2.1 实验数据集</h4>
                <div class="p1">
                    <p id="125">本文实验使用EMO-DB语音情感数据库<citation id="250" type="reference"><link href="223" rel="bibliography" /><sup>[<a class="sup">25</a>]</sup></citation>和CASIA汉语语音情感数据库。</p>
                </div>
                <div class="p1">
                    <p id="126">EMO-DB语音情感数据库是由柏林工业大学录制的德语情感语音库。该数据集由535个语音数据组成, 其采样率为16 kHz。语音数据由演员进行7种情感的模拟得到, 这7种感情分别为happiness、anger、boredom、disgust、fear、sadness、normal。</p>
                </div>
                <div class="p1">
                    <p id="127">CASIA汉语情感语料库由中国科学院自动化所录制, 共包括4个专业发音人, 6种情绪为anger、happiness、fear、sadness、surprise和normal, 共9 600句不同发音, 包括300句相同文本和100句不同文本, 选取300句相同文本的音频作为实验数据。</p>
                </div>
                <h4 class="anchor-tag" id="128" name="128">2.2 结果分析</h4>
                <div class="p1">
                    <p id="129">本文实验分别使用MFCC特征在SVM中进行分类 (MFCC+SVM) 、CNN提取语谱图特征在SVM中分类 (CNN+SVM) 、MKL融合2种特征后进行分类 (MFCC+CNN+MKL) 验证传统MFCC特征、CNN提取的语谱图特征以及两者经MKL方法融合后的特征在情感识别的效果, 实验采用10折交叉验证来评估方法的识别效果。</p>
                </div>
                <div class="p1">
                    <p id="130">MFCC+SVM在EMO-DB语音情感数据集的分类混淆矩阵如图5所示。其中, 颜色越深表示情感识别准确率越高 (下文相同) 。7种情感的识别准确率为46%、64%、47%、54%、51%、66%、52%, 总体识别准确率为55.1%。</p>
                </div>
                <div class="area_img" id="131">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201908041_131.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 MFCC+SVM在EMO-DB中的情感识别混淆矩阵" src="Detail/GetImg?filename=images/JSJC201908041_131.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图5 MFCC+SVM在EMO-DB中的情感识别混淆矩阵</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201908041_131.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="132">CNN+SVM在EMO-DB语音情感数据集的分类混淆矩阵如图6所示。7种情感的识别准确率为58%、83%、63%、52%、62%、82%、59%, 总体识别准确率为67.7%。</p>
                </div>
                <div class="area_img" id="133">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201908041_133.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 CNN+SVM在EMO-DB中的情感识别混淆矩阵" src="Detail/GetImg?filename=images/JSJC201908041_133.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图6 CNN+SVM在EMO-DB中的情感识别混淆矩阵</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201908041_133.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="134">MFCC+CNN+MKL在EMO-DB语音情感数据集的分类混淆矩阵如图7所示。7种情感的识别准确率为85%、84%、86%、82%、86%、89%、85%, 总体识别准确率为86%。</p>
                </div>
                <div class="area_img" id="135">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201908041_135.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 MFCC+CNN+MKL在EMO-DB中的情感识别混淆矩阵" src="Detail/GetImg?filename=images/JSJC201908041_135.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图7 MFCC+CNN+MKL在EMO-DB中的情感识别混淆矩阵</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201908041_135.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="136">使用MKL融合后的特征在7种情感的识别准确率比MFCC特征均有提高。相比于CNN提取的语谱图特征的识别准确率, 只有anger和sadness这2种情绪识别准确率有一定程度的下降, 其余情感都得到提高。融合特征的整体识别准确率比MFCC特征提高了30.9%, 比语谱图特征提高了18.3%。</p>
                </div>
                <div class="p1">
                    <p id="137">MFCC+SVM在CASIA情感数据集的分类混淆矩阵如图8所示。6种情感的识别准确率为58%、54%、56%、54%、71%、60%, 总体识别准确率为58.6%。</p>
                </div>
                <div class="area_img" id="138">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201908041_138.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 MFCC+SVM在CASIA中的情感识别混淆矩阵" src="Detail/GetImg?filename=images/JSJC201908041_138.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图8 MFCC+SVM在CASIA中的情感识别混淆矩阵</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201908041_138.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="139">CNN+SVM在CASIA语音情感数据集的分类混淆矩阵如图9所示。6种情感的识别准确率为58%、51%、59%、87%、72%、63%, 总体识别准确率为64.2%。</p>
                </div>
                <div class="area_img" id="140">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201908041_140.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图9 CNN+SVM在CASIA中的情感识别混淆矩阵" src="Detail/GetImg?filename=images/JSJC201908041_140.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图9 CNN+SVM在CASIA中的情感识别混淆矩阵</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201908041_140.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="141">MFCC+CNN+MKL在CASIA语音情感数据集的分类混淆矩阵如图10所示。6种情感的识别准确率为92%、79%、84%、94%、86%、96%, 总体识别准确率为88%。</p>
                </div>
                <div class="area_img" id="142">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201908041_142.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图10 MFCC+SVM在CASIA中的情感识别混淆矩阵" src="Detail/GetImg?filename=images/JSJC201908041_142.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图10 MFCC+SVM在CASIA中的情感识别混淆矩阵</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201908041_142.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="143">由CASIA数据集的实验结果可以看出, 经过MKL融合后所得特征, 在6个情感上的识别准确率比MFCC和语谱图特征均有所提高, 且模型的整体识别准确率分别提高了29.4%和23.8%。</p>
                </div>
                <div class="p1">
                    <p id="144">在EMO-DB和CASIA数据集的实验结果表明, 使用MKL方法进行特征融合后进行分类的识别结果, 明显优于使用单个特征进行情感识别的识别结果。CASIA数据集的实验结果优于EMO-DB的原因可能是, CASIA的样本分布比EMO-DB更均匀并且CASIA的样本数量更多。</p>
                </div>
                <div class="p1">
                    <p id="145">表1所示为不同方法在EMO-DB数据集上准确率对比结果。文献<citation id="251" type="reference">[<a class="sup">20</a>]</citation>采用MKL融合多种声学特征的局部特征和全局特征对5种情感进行分类。文献<citation id="252" type="reference">[<a class="sup">26</a>]</citation>提出一种2阶段的特征融合方法对6种情感进行分类。文献<citation id="253" type="reference">[<a class="sup">27</a>]</citation>提出一种基于神经网络而无需特征提取的端到端语音情感识别系统对5种情感进行分类。</p>
                </div>
                <div class="area_img" id="146">
                    <p class="img_tit"><b>表1 不同方法准确率对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="146" border="1"><tr><td rowspan="2"><br />方法</td><td colspan="7"><br />准确率</td></tr><tr><td>happiness</td><td>anger</td><td>boredom</td><td>disgust</td><td>fear</td><td>sadness</td><td>normal</td></tr><tr><td>文献<br />[20]<br />方法</td><td>—</td><td>0.81</td><td>—</td><td>0.87</td><td>0.83</td><td>0.95</td><td>0.65</td></tr><tr><td><br />文献<br />[26]<br />方法</td><td>0.90</td><td>0.91</td><td>—</td><td>0.86</td><td>0.84</td><td>0.90</td><td>0.88</td></tr><tr><td><br />文献<br />[27]<br />方法</td><td>0.69</td><td>0.89</td><td>0.53</td><td>—</td><td>—</td><td>0.89</td><td>0.69</td></tr><tr><td><br />本文方法</td><td>0.85</td><td>0.84</td><td>0.86</td><td>0.82</td><td>0.86</td><td>0.89</td><td>0.85</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="147">由表1可以看出, 本文方法在EMO-DB数据集上的7种情感识别准确率相较其他方法有提高也有降低。综上, 本文方法在情感识别中能够得到比较理想的结果。</p>
                </div>
                <h3 id="148" name="148" class="anchor-tag">3 结束语</h3>
                <div class="p1">
                    <p id="149">为提高语音情感识别准确率, 本文提出基于MKL特征融合的语音情感识别方法。提取语音中的MFCC特征和语谱图特征, 使用MKL方法进行特征融合, 并将得到的核函数输入给SVM分类器进行情感识别。在EMO-DB和CASIA数据集上进行实验, 结果表明, 本文方法具有较高的情感识别率。下一步将加入心电信号、脑电信号等人体生理特征, 从多方面识别人类情感, 从而提高情感识别性能。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="175">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Speaker dependent speech emotion recognition using MFCC and Support Vector Machine">

                                <b>[1]</b> DAHAKE P P, SHAW K, MALATHI P.Speaker dependent speech emotion recognition using MFCC and support vector machine[C]//Proceedings of International Conference on Automatic Control and Dynamic Optimization Techniques.Washington D.C., USA:IEEE Press, 2017:1080-1084.
                            </a>
                        </p>
                        <p id="177">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CDFD&amp;filename=1014084817.nh&amp;v=MjU2NjdlWmVScUZDam5VTHJBVkYyNkdyT3dHdG5OcUpFYlBJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE8=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> 韩文静.语音情感识别关键技术研究[D].哈尔滨:哈尔滨工业大学, 2013.
                            </a>
                        </p>
                        <p id="179">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Hybrid Deep Neural Network-Hidden Markov Model (DNN-HMM)Based Speech Emotion Recognition">

                                <b>[3]</b> LI Longfei, ZHAO Yong, JIANG Dongmei, et al.Hybrid deep neural network-hidden markov model based speech emotion recognition[C]//Proceedings of Humaine Association Conference on Affective Computing and Intelligent Interaction.Washington D.C., USA:IEEE Press, 2013:312-317.
                            </a>
                        </p>
                        <p id="181">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Speaker independent emotion recognition by early fusion of acoustic and linguistic features within ensembles">

                                <b>[4]</b> SCHULLER B, MÜLLER R, LANG M, et al.Speaker independent emotion recognition by early fusion of acoustic and linguistic features within ensembles[C]//Proceedings of the 9th European Conference on Speech Communication and Technology.[S.l.]:Curran Associates Inc., 2005:805-808.
                            </a>
                        </p>
                        <p id="183">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESA509EBE6C47A4F5094A4BCFABB6E03E5&amp;v=MjYyOTkzNU54aXhMcTh4YUE9TmlmT2ZjSzlIdGk1M2ZwREYrOElmWGhQeWhZYTdrNTVPZ3lVM1dCSGY4ZVVScythQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQ3BiUQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b> ÖZSEVEN T.Investigation of the effect of spectrogram images and different texture analysis methods on speech emotion recognition[J].Applied Acoustics, 2018, 142:70-77.
                            </a>
                        </p>
                        <p id="185">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Auto-matic speech emotion recognition using recurrent neural networks with local attention">

                                <b>[6]</b> MIRSAMADI S, BARSOUM E, ZHANG Cha.Auto-matic speech emotion recognition using recurrent neural networks with local attention[C]//Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing.Washington D.C., USA:IEEE Press, 2017:2227-2231.
                            </a>
                        </p>
                        <p id="187">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep convolutional recurrent neural network with attention mechanism for robust speech emotion recognition">

                                <b>[7]</b> HUANG C W, NARAYANAN S S.Deep convolutional recurrent neural network with attention mechanism for robust speech emotion recognition[C]//Proceedings of IEEE International Conference on Multimedia and Expo.Washington D.C., USA:IEEE Press, 2017:583-588.
                            </a>
                        </p>
                        <p id="189">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A multi-task learning framework for emotion recognition using 2D continuous space">

                                <b>[8]</b> XIA Rui, LIU Yang.A multi-task learning framework for emotion recognition using 2D continuous space[J].IEEE Transactions on Affective Computing, 2017, 8 (1) :3-14.
                            </a>
                        </p>
                        <p id="191">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=NJYD201805014&amp;v=MTcwMzVuVUxyQUt5ZlNhckc0SDluTXFvOUVZSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUnFGQ2o=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b> 卢官明, 袁亮, 杨文娟, 等.基于长短期记忆和卷积神经网络的语音情感识别[J].南京邮电大学学报 (自然科学版) , 2018, 38 (5) :63-69.
                            </a>
                        </p>
                        <p id="193">
                            <a id="bibliography_10" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=1018896203.nh&amp;v=MDk4OTRqblVMckFWRjI2RnJ1eEdOUE1ySkViUElRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJxRkM=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[10]</b> 陈晓敏.基于时序深度学习模型的语音情感识别方法研究[D].哈尔滨:哈尔滨工业大学, 2018.
                            </a>
                        </p>
                        <p id="195">
                            <a id="bibliography_11" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=YYKX201805011&amp;v=Mjg3MDM0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVScUZDam5VTHJBUERUQWRyRzRIOW5NcW85RVpZUUtESDg=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[11]</b> 曾润华, 张树群.改进卷积神经网络的语音情感识别方法[J].应用科学学报, 2018, 36 (5) :837-844
                            </a>
                        </p>
                        <p id="197">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Speech emotion recognition using deep neural network and extreme learning machine">

                                <b>[12]</b> HAN Kun, YU Dong, TASHEV I.Speech emotion recognition using deep neural network and extreme learning machine[EB/OL].[2018-10-20].http://193.6.4.39/～czap/letoltes/IS14/IS2014/PDF/AUTHOR/IS140441.PDF.
                            </a>
                        </p>
                        <p id="199">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Speech emotion recognition using convolutional and recurrent neural networks">

                                <b>[13]</b> LIM W, JANG D, LEE T.Speech emotion recognition using convolutional and recurrent neural networks[C]//Proceedings of Signal and Information Processing Asso-ciation Annual Summit and Conference.Washington D.C., USA:IEEE Press, 2016:1-4.
                            </a>
                        </p>
                        <p id="201">
                            <a id="bibliography_14" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESFF3B6B9DC93B19963C39D06D8B75C21E&amp;v=MjM4MTFDcGJRMzVOeGl4THE4eGFBPU5pZk9mY1hPSGFQSzNZWXhGK0lNZm4wd3hoQVFtVHgwUEgvazJCcEhmcmZuUjd2cUNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[14]</b> FAYEK H M, LECH M, CAVEDON L.Evaluating deep learning architectures for speech emotion recognition[J].Neural Networks, 2017, 92 (8) :60-68.
                            </a>
                        </p>
                        <p id="203">
                            <a id="bibliography_15" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201008001&amp;v=MzEzMjJMZlliRzRIOUhNcDQ5RlpZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVScUZDam5VTHJBS0M=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[15]</b> 汪洪桥, 孙富春, 蔡艳宁, 等.多核学习方法[J].自动化学报, 2010, 36 (8) :1037-1050.
                            </a>
                        </p>
                        <p id="205">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=SimpleMKL">

                                <b>[16]</b> RAKOTOMAMONJY A, BACH F R, CANU S, et al.SimpleMKL[J].Journal of Machine Learning Research, 2008, 9 (11) :2491-2521.
                            </a>
                        </p>
                        <p id="207">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning the kernel with hyperkernels">

                                <b>[17]</b> ONG C S, SMOLA A J, WILLIAMSON R C.Learning the kernel with hyperkernels[J].Journal of Machine Learning Research, 2005, 6 (7) :1043-1071.
                            </a>
                        </p>
                        <p id="209">
                            <a id="bibliography_18" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES875C4EAEC89C18E570483F8BB58785F6&amp;v=MzAzMzNCdUhZZk9HUWxmQ3BiUTM1TnhpeExxOHhhQT1OaWZPZmJ1L0c2TEkydjR3RitNR2YzMHh1aE1VNmp0MVN3bnEzbUF3Y2JXY1FNeVpDT052RlNpV1dyN0pJRnBtYQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[18]</b> AIOLLI F, DONINI M.EasyMKL:a scalable multiple kernel learning algorithm[J].Neurocomputing, 2015, 169:215-224.
                            </a>
                        </p>
                        <p id="211">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Convolutional MKL based multimodal emotion recognition and sentiment analysis">

                                <b>[19]</b> PORIA S, CHATURVEDI I, CAMBRIA E, et al.Convolutional MKL based multimodal emotion recognition and sentiment analysis[C]//Proceedings of the 16th International Conference on Data Mining.Washington D.C., USA:IEEE Press, 2016:439-448.
                            </a>
                        </p>
                        <p id="213">
                            <a id="bibliography_20" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DNDY201302005&amp;v=MTc2NzMzenFxQnRHRnJDVVJMT2VaZVJxRkNqblVMckFJU1BQZDdHNEg5TE1yWTlGWVlRS0RIODR2UjRUNmo1NE8=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[20]</b> JIN Yun, SONG Peng, ZHENG Wenming, et al.Novel feature fusion method for speech emotion recognition based on multiple kernel learning[J].Journal of Southeast University, 2013, 29 (2) :129-133.
                            </a>
                        </p>
                        <p id="215">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Speakerindependent speech emotion recognition based on two-layer multiple kernel learning">

                                <b>[21]</b> JIN Yun, SONG Peng, ZHENG Wenming, et al.Speaker-independent speech emotion recognition based on two-layer multiple kernel learning[J].IEICE Transactions on Information and Systems, 2013, 96 (10) :2286-2289.
                            </a>
                        </p>
                        <p id="217">
                            <a id="bibliography_22" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCMD85A04FDC8E1EF2D425D4BDEEDD65FA1&amp;v=MDQ2NjRhQnVIWWZPR1FsZkNwYlEzNU54aXhMcTh4YUE9TmlmSVk4ZXdHNkRNcS9reEYrTjZEUWxQeldJWDZEb0pUQTJXMldkQkRiU1JNOHVlQ09OdkZTaVdXcjdKSUZwbQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[22]</b> XU Xinzhou, DENG Jun, CUMMINS N, et al.A two-dimensional framework of multiple kernel subspace learning for recognizing emotion in speech[J].IEEE/ACM Transactions on Audio, Speech and Language Processing, 2017, 25 (7) :1436-1449.
                            </a>
                        </p>
                        <p id="219">
                            <a id="bibliography_23" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14121200104097&amp;v=MTUxNDFNbndaZVp1SHlqbVVMdkxJRndYYnhvPU5pZk9mYks4SDlQTnJZOUZaZXNMREhVK29CTVQ2VDRQUUgvaXJSZEdlcnFRVA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[23]</b> SUN Yaxin, WEN Guihua, WANG Jiabing.Weighted spectral features based on local hu moments for speech emotion recognition[J].Biomedical Signal Processing and Control, 2015, 18 (4) :80-90.
                            </a>
                        </p>
                        <p id="221">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning Salient Features for Speech Emotion Recognition Using Convolutional Neural Networks">

                                <b>[24]</b> MAO Qirong, DONG Ming, HUANG Zhengwei, et al.Learning salient features for speech emotion recognition using convolutional neural networks[J].IEEE Tran-sactions on Multimedia, 2014, 16 (8) :2203-2213.
                            </a>
                        </p>
                        <p id="223">
                            <a id="bibliography_25" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Database of German Emotional Speech">

                                <b>[25]</b> BURKHARDT F, PAESCHKE A, ROLFES M, et al.A database of German emotional speech[C]//Proceedings of the 9th European Conference on Speech Communication and Technology.[S.l.]:Curran Associates Inc., 2005:1-4.
                            </a>
                        </p>
                        <p id="225">
                            <a id="bibliography_26" target="_blank" href="http://scholar.cnki.net/result.aspx?q=An Optimal Two Stage Feature Selection for Speech Emotion Recognition using Acoustic Features">

                                <b>[26]</b> KUCHIBHOTLA S, VANKAYALAPATI H D, ANNE K R.An optimal two stage feature selection for speech emotion recognition using acoustic features[J].Inter-national Journal of Speech Technology, 2016, 19 (4) :657-667.
                            </a>
                        </p>
                        <p id="227">
                            <a id="bibliography_27" target="_blank" href="http://scholar.cnki.net/result.aspx?q=End-to-end speech emotion recognition based on neural network">

                                <b>[27]</b> ZHU Bing, ZHOU Wenkai, WANG Yutian, et al.End-to-end speech emotion recognition based on neural network[C]//Proceedings of the 17th International Con-ference on Communication Technology.Washington D.C., USA:IEEE Press, 2017:1634-1638.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJC201908041" />
        <input id="dpi" type="hidden" value="600" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201908041&amp;v=MDAyOTJGQ2puVUxyQUx6N0JiYkc0SDlqTXA0OUJaWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUnE=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
