<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637130530790655000%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJC201905033%26RESULT%3d1%26SIGN%3dFKC0WyjNUNok6WGOUUWSpvhqyEo%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201905033&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201905033&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201905033&amp;v=MjMwNDI3QmJiRzRIOWpNcW85R1o0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSb0Z5M2xWYnZBTHo=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#41" data-title="0 概述 ">0 概述</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#46" data-title="1 本文方法 ">1 本文方法</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#47" data-title="1.1 递归深度混合关注网络模型">1.1 递归深度混合关注网络模型</a></li>
                                                <li><a href="#50" data-title="1.2 混合关注">1.2 混合关注</a></li>
                                                <li><a href="#77" data-title="1.3 网络递归">1.3 网络递归</a></li>
                                                <li><a href="#88" data-title="1.4 网络训练步骤">1.4 网络训练步骤</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#94" data-title="2 实验与结果分析 ">2 实验与结果分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#96" data-title="2.1 网络模型对比结果">2.1 网络模型对比结果</a></li>
                                                <li><a href="#101" data-title="2.2 算法对比实验结果">2.2 算法对比实验结果</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#108" data-title="3 结束语 ">3 结束语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#49" data-title="&lt;b&gt;图1 网络模型框架&lt;/b&gt;"><b>图1 网络模型框架</b></a></li>
                                                <li><a href="#53" data-title="&lt;b&gt;图2 混合关注过程&lt;/b&gt;"><b>图2 混合关注过程</b></a></li>
                                                <li><a href="#76" data-title="&lt;b&gt;图3 混合关注模块网络结构&lt;/b&gt;"><b>图3 混合关注模块网络结构</b></a></li>
                                                <li><a href="#81" data-title="&lt;b&gt;图4 原图像与切割放大后图像对比&lt;/b&gt;"><b>图4 原图像与切割放大后图像对比</b></a></li>
                                                <li><a href="#98" data-title="&lt;b&gt;表1 3种网络对2个数据集的分类精度&lt;/b&gt;"><b>表1 3种网络对2个数据集的分类精度</b></a></li>
                                                <li><a href="#103" data-title="&lt;b&gt;表2 在Stanford Dogs数据集上算法分类精度&lt;/b&gt;"><b>表2 在Stanford Dogs数据集上算法分类精度</b></a></li>
                                                <li><a href="#104" data-title="&lt;b&gt;表3 在Stanford Cars数据集上算法分类精度&lt;/b&gt;"><b>表3 在Stanford Cars数据集上算法分类精度</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" title=" SIMONYAN K, ZISSERMAN A.Very deep convolutional networks for large-scale image recognition[J].Computer Science, 2014:1." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Very deep convolutional networks for large-scale image recognition">
                                        <b>[1]</b>
                                         SIMONYAN K, ZISSERMAN A.Very deep convolutional networks for large-scale image recognition[J].Computer Science, 2014:1.
                                    </a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" title=" HE Kaiming, ZHANG Xiangyu, REN Shaoqing, et al.Deep residual learning for image recognition[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2015:770-778." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep Residual Learning for Image Recognition">
                                        <b>[2]</b>
                                         HE Kaiming, ZHANG Xiangyu, REN Shaoqing, et al.Deep residual learning for image recognition[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2015:770-778.
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title=" HUANG Gao, LIU Zhuang, MAATEN L V D, et al.Densely connected convolutional networks[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2017:4700-4708." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Densely Connected Convolutional Networks">
                                        <b>[3]</b>
                                         HUANG Gao, LIU Zhuang, MAATEN L V D, et al.Densely connected convolutional networks[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2017:4700-4708.
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" title=" 李炳政.关于智能交通中的车型识别分析[J].数字技术与应用, 2016 (4) :230-230." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=SZJT201604169&amp;v=MDQwNDM0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSb0Z5M2xWYnZBTmpmQmVyRzRIOWZNcTQ1RGJZUUtESDg=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                         李炳政.关于智能交通中的车型识别分析[J].数字技术与应用, 2016 (4) :230-230.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" title=" 冯琼瑛, 陈粤超, 张家善, 等.红树林湿地鸟类查询与鉴别系统的设计与实现[J].计算机应用与软件, 2016, 33 (9) :64-66." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201609015&amp;v=MjU5ODk5Zk1wbzlFWVlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJvRnkzbFZidkFMelRaWkxHNEg=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                         冯琼瑛, 陈粤超, 张家善, 等.红树林湿地鸟类查询与鉴别系统的设计与实现[J].计算机应用与软件, 2016, 33 (9) :64-66.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" title=" KIAPOUR M H, HAN Xufeng, LAZEBNIK S, et al.Where to buy it:matching street clothing photos in online shops[C]//Proceedings of IEEE International Conference on Computer Vision.Washington D.C., USA:IEEE Press, 2015:3343-3351." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Where to Buy It:Matching Street Clothing Photos in Online Shops">
                                        <b>[6]</b>
                                         KIAPOUR M H, HAN Xufeng, LAZEBNIK S, et al.Where to buy it:matching street clothing photos in online shops[C]//Proceedings of IEEE International Conference on Computer Vision.Washington D.C., USA:IEEE Press, 2015:3343-3351.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" title=" 陈玄, 朱荣, 王中元.基于融合卷积神经网络模型的手写数字识别[J].计算机工程, 2017, 43 (11) :187-192." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201711031&amp;v=Mjc2NTJGeTNsVmJ2QUx6N0JiYkc0SDliTnJvOUdaWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm8=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                         陈玄, 朱荣, 王中元.基于融合卷积神经网络模型的手写数字识别[J].计算机工程, 2017, 43 (11) :187-192.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" title=" 易生, 梁华刚, 茹锋.基于多列深度3D卷积神经网络的手势识别[J].计算机工程, 2017, 43 (8) :243-248." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201708042&amp;v=MTUwMjdUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJvRnkzbFZidkFMejdCYmJHNEg5Yk1wNDlCWm9RS0RIODR2UjQ=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                         易生, 梁华刚, 茹锋.基于多列深度3D卷积神经网络的手势识别[J].计算机工程, 2017, 43 (8) :243-248.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" title=" GIRSHICK R, DONAHUE J, DARRELL T, et al.Rich feature hierarchies for accurate object detection and semantic segmentation[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2014:580-587." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Rich feature hierarchies for accurate object detection and semantic segmentation">
                                        <b>[9]</b>
                                         GIRSHICK R, DONAHUE J, DARRELL T, et al.Rich feature hierarchies for accurate object detection and semantic segmentation[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2014:580-587.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_10" title=" KRAUSE J, JIN Hailin.Fine-grained recognition without part annotations[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2015:5546-5555." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fine-grained recognition without part annotations">
                                        <b>[10]</b>
                                         KRAUSE J, JIN Hailin.Fine-grained recognition without part annotations[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2015:5546-5555.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_11" title=" LAM M, MAHASSENI B.Fine-grained recognition as HSnet search for informative image parts[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2017:6497-6506." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fine-Grained Recognition as HSnet Search for Informative Image Parts">
                                        <b>[11]</b>
                                         LAM M, MAHASSENI B.Fine-grained recognition as HSnet search for informative image parts[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2017:6497-6506.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_12" title=" ROYCHOWDHURY A.Bilinear CNN models for fine-grained visual recognition[C]//Proceedings of IEEE International Conference on Computer Vision.Washington D.C., USA:IEEE Press, 2015:1449-1457." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Bilinear CNN models for fine-grained visual recognition">
                                        <b>[12]</b>
                                         ROYCHOWDHURY A.Bilinear CNN models for fine-grained visual recognition[C]//Proceedings of IEEE International Conference on Computer Vision.Washington D.C., USA:IEEE Press, 2015:1449-1457.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_13" title=" KRAUSE J, STARK M, JIA Deng, et al.3D object representations for fine-grained categorization[C]//Proceedings of IEEE International Conference on Computer Vision.Washington D.C., USA:IEEE Press, 2014:554-561." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=3D object representations for fine-grained categorization">
                                        <b>[13]</b>
                                         KRAUSE J, STARK M, JIA Deng, et al.3D object representations for fine-grained categorization[C]//Proceedings of IEEE International Conference on Computer Vision.Washington D.C., USA:IEEE Press, 2014:554-561.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_14" title=" KHOSLA A, JAYADEVAPRAKASH N, YAO Bangpeng, et al.Novel dataset for fine-grained image categorization[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2011." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Novel datasets for fine-grained image categorization">
                                        <b>[14]</b>
                                         KHOSLA A, JAYADEVAPRAKASH N, YAO Bangpeng, et al.Novel dataset for fine-grained image categorization[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2011.
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_15" title=" LIU Xiao, XIA Tian, WANG Jiang, et al.Fully convolutional attention localization networks:efficient attention localization for fine-grained recognition[EB/OL].[2018-03-12].https://arxiv.org/pdf/1603.06765.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fully convolutional attention localization networks:efficient attention localization for fine-grained recognition">
                                        <b>[15]</b>
                                         LIU Xiao, XIA Tian, WANG Jiang, et al.Fully convolutional attention localization networks:efficient attention localization for fine-grained recognition[EB/OL].[2018-03-12].https://arxiv.org/pdf/1603.06765.pdf.
                                    </a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_16" title=" ZHAO Bo, WU Xiao, FENG Jiashi, et al.Diversified visual attention networks for fine-grained object classifica-tion[J].IEEE Transactions on Multimedia, 2017, 19 (6) :1245-1256." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Diversified visual attention networks for fine-grained object classification">
                                        <b>[16]</b>
                                         ZHAO Bo, WU Xiao, FENG Jiashi, et al.Diversified visual attention networks for fine-grained object classifica-tion[J].IEEE Transactions on Multimedia, 2017, 19 (6) :1245-1256.
                                    </a>
                                </li>
                                <li id="35">


                                    <a id="bibliography_17" title=" FU Jianlong, ZHENG Heliang, MEI Tao.Look closer to see better:recurrent attention convolutional neural network for fine-grained image recognition[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2017:4438-4446." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Look closer to see better:recurrent attention convolutional neural network for fine-grained image recognition">
                                        <b>[17]</b>
                                         FU Jianlong, ZHENG Heliang, MEI Tao.Look closer to see better:recurrent attention convolutional neural network for fine-grained image recognition[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2017:4438-4446.
                                    </a>
                                </li>
                                <li id="37">


                                    <a id="bibliography_18" title=" KONG Shu, FOWLKES C.Low-rank bilinear pooling for fine-grained classification[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2017:365-374." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Low-Rank Bilinear Pooling for Fine-Grained Classification">
                                        <b>[18]</b>
                                         KONG Shu, FOWLKES C.Low-rank bilinear pooling for fine-grained classification[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2017:365-374.
                                    </a>
                                </li>
                                <li id="39">


                                    <a id="bibliography_19" title=" CAI Sijia, ZUO Wangmeng, ZHANG Lei.Higher-order integration of hierarchical convolutional activations for fine-grained visual categorization[C]//Proceedings of IEEE International Conference on Computer Vision.Washington D.C., USA:IEEE Press, 2017:511-520." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Higher-order integration of hierarchical convolutional activations for fine-grained visual categorization">
                                        <b>[19]</b>
                                         CAI Sijia, ZUO Wangmeng, ZHANG Lei.Higher-order integration of hierarchical convolutional activations for fine-grained visual categorization[C]//Proceedings of IEEE International Conference on Computer Vision.Washington D.C., USA:IEEE Press, 2017:511-520.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJC" target="_blank">计算机工程</a>
                2019,45(05),205-209 DOI:10.19678/j.issn.1000-3428.0051191            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>递归深度混合关注网络的细粒度图像分类方法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%A1%82%E6%B1%9F%E7%94%9F&amp;code=28239712&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">桂江生</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%BA%BB%E9%99%88%E9%A3%9E&amp;code=39323285&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">麻陈飞</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%8C%85%E6%99%93%E5%AE%89&amp;code=09410519&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">包晓安</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%92%B1%E4%BF%8A%E5%BD%A6&amp;code=11056756&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">钱俊彦</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%B5%99%E6%B1%9F%E7%90%86%E5%B7%A5%E5%A4%A7%E5%AD%A6%E4%BF%A1%E6%81%AF%E5%AD%A6%E9%99%A2&amp;code=0167237&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">浙江理工大学信息学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%A1%82%E6%9E%97%E7%94%B5%E5%AD%90%E7%A7%91%E6%8A%80%E5%A4%A7%E5%AD%A6%E5%B9%BF%E8%A5%BF%E5%8F%AF%E4%BF%A1%E8%BD%AF%E4%BB%B6%E9%87%8D%E7%82%B9%E5%AE%9E%E9%AA%8C%E5%AE%A4&amp;code=0269119&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">桂林电子科技大学广西可信软件重点实验室</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>在细粒度图像的大量局部特征中, 只有少量特征具有判别性, 其提取较为困难。为此, 提出递归深度混合关注网络方法。通过在卷积结构单元中添加通道关注模块和空间关注模块, 实现网络的混合关注。以第1路网络输出特征的空间响应值为依据切割原图, 并将切割后的图像放大输入第2路网络, 进行由粗到细的网络递归。将2路网络提取的特征进行级联融合。在公开数据集Stanford Dogs、Stanford Cars中进行对比实验, 结果表明, 该方法的分类精度分别为87.1%、92.4%, 优于FCAN、HIHCA等方法。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%BB%86%E7%B2%92%E5%BA%A6%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">细粒度图像分类;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">卷积神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%80%9A%E9%81%93%E5%85%B3%E6%B3%A8&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">通道关注;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%A9%BA%E9%97%B4%E5%85%B3%E6%B3%A8&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">空间关注;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%80%92%E5%BD%92%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">递归网络;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    桂江生 (1978—) , 男, 副教授, 主研方向为计算机视觉、模式识别;;
                                </span>
                                <span>
                                    麻陈飞, 硕士研究生;;
                                </span>
                                <span>
                                    包晓安, 教授。E-mail: baoxiaoan@ zstu. edu. cn;
                                </span>
                                <span>
                                    钱俊彦, 教授。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-04-12</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金 (61502430, 61562015);</span>
                                <span>浙江省重大科技专项重点工业项目 (2014C01047);</span>
                                <span>广西自然科学重点基金 (2015GXNSFDA139038);</span>
                    </p>
            </div>
                    <h1><b>Fine-grained Image Classification Method for Recurrent Deep Hybrid Attention Network</b></h1>
                    <h2>
                    <span>GUI Jiangsheng</span>
                    <span>MA Chenfei</span>
                    <span>BAO Xiaoan</span>
                    <span>QIAN Junyan</span>
            </h2>
                    <h2>
                    <span>School of Informatics Science and Technology, Zhejiang Sci-tech University</span>
                    <span>Guangxi Key Laboratory of Trusted Software, Guilin University of Electronic Technology</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Among the large number of local features of fine-grained images, only a few of them are discriminative and are very difficult to extract.To solve this problem, a recurrent deep hybrid attention network is proposed.By adding channel attention module and spatial attention module into convolution units, the hybrid attention network is realized.Based on the spatial response value of the output feature of the first network, the original image is segmented, and the segmented image is enlarged and input into the second network to realize the coarse-to-fine network recursion.The features extracted from two networks are cascaded to achieve the purpose of feature fusion.Experiments are conducted in the open data sets.Stanford Dogs and Stanford Cars.Results show that the classification accuracies of the proposed algorithm are 87.1% and 92.4%, respectively, which is better than FCAN, HIHCA and other methods.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=fine-grained%20image%20classification&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">fine-grained image classification;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=convolutional%20neural%20network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">convolutional neural network;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=channel%20attention&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">channel attention;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=spatial%20attention&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">spatial attention;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=recurrent%20network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">recurrent network;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-04-12</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="41" name="41" class="anchor-tag">0 概述</h3>
                <div class="p1">
                    <p id="42">大尺度图像分类是区分物体大类, 如区分出图像中的物体是车、狗、服装等。随着深度学习的发展, 在大尺度图像分类中, 不断涌现出优秀的网络模型, 如VGGNet<citation id="110" type="reference"><link href="3" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>、ResNet<citation id="111" type="reference"><link href="5" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>、DenseNet<citation id="112" type="reference"><link href="7" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>等。而细粒度图像分类是对同一大类物体的下属子类图像进行分类, 如车型分类、狗分类、服装分类等, 其广泛应用于智慧交通<citation id="113" type="reference"><link href="9" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>、专家系统<citation id="114" type="reference"><link href="11" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>、电子商务<citation id="115" type="reference"><link href="13" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>等各个领域, 是一个非常值得研究又极具挑战性的课题。</p>
                </div>
                <div class="p1">
                    <p id="43">对简单的图像进行细粒度分类, 如手写数字识别、简单的手势识别等, 已有文献提出较高识别精度的方法<citation id="116" type="reference"><link href="15" rel="bibliography" /><link href="17" rel="bibliography" /><sup>[<a class="sup">7</a>,<a class="sup">8</a>]</sup></citation>。然而对较为复杂、包含类别较多的图像进行细粒度分类, 由于图像之间的区别往往体现在少量的局部特征中, 因此需要在大量的局部特征中提取少量具有判别性的特征。</p>
                </div>
                <div class="p1">
                    <p id="44">针对此问题, 研究人员提出了基于强监督信息模型的算法, 对数据集进行一些人工信息标注。例如在车辆分类中, 对车的位置、车头等进行标记。借助这些标记信息, 使网络更有针对性地提取特征, 以提高分类精度。代表性算法有RCNN<citation id="117" type="reference"><link href="19" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>、PA-CNN<citation id="118" type="reference"><link href="21" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>、HSnet<citation id="119" type="reference"><link href="23" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>等。这类算法虽然有较高的分类精度, 但需要额外的标注信息, 耗费大量人力, 实用性不强。另外, 人工标注信息容易出错, 影响算法的鲁棒性。于是有研究人员提出基于弱监督信息模型的算法。这类算法无需人工边框标记等信息, 只需给出图像中的物体类别信息。如B-CNN<citation id="120" type="reference"><link href="25" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>通过2路VGGNet联动, 实现了物体位置的检测与检测位置的特征提取。然而, 这类算法检测到的物体位置不够准确, 对检测位置提取的特征判别性不够强。</p>
                </div>
                <div class="p1">
                    <p id="45">为此, 本文提出递归深度混合关注网络的方法。该方法基于弱监督信息模型, 通过在卷积结构单元中添加级联的通道关注模块和空间关注模块, 实现对卷积特征的空间维度上和通道维度上的关注。利用网络提取的特征来定位特征明显区域的方法, 从而进行由粗到细的网络递归, 提取更为精细化的特征。</p>
                </div>
                <h3 id="46" name="46" class="anchor-tag">1 本文方法</h3>
                <h4 class="anchor-tag" id="47" name="47">1.1 递归深度混合关注网络模型</h4>
                <div class="p1">
                    <p id="48">本文的网络模型框架如图1所示。首先, 将在残差结构中添加了混合关注模块的ResNeXt50网络作为第1路网络提取特征。然后, 在第1路网络的最后一个残差结构单元后面添加切割放大模块。切割放大模块根据第1路网络最后一个卷积结构输出特征的空间响应值对原图进行切割, 并将切割出的图像放大到原图大小, 再将其送入第2路添加了混合关注模块的ResNeXt50网络进一步提取更为精细化的特征。最后, 将2路网络提取的特征进行级联, 用于最终分类。</p>
                </div>
                <div class="area_img" id="49">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201905033_049.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 网络模型框架" src="Detail/GetImg?filename=images/JSJC201905033_049.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图1 网络模型框架</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201905033_049.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="50" name="50">1.2 混合关注</h4>
                <div class="p1">
                    <p id="51">混合关注模块通过在卷积结构单元中添加通道关注模块和空间关注模块, 采用无监督的方式让模型学习通道维度上和空间维度上的权重分布, 然后将学习到的权重分布施加到原来的特征上, 从而实现了从通道维度上和空间维度上2个层面的关注, 达到提升网络性能的目的。</p>
                </div>
                <div class="p1">
                    <p id="52">混合关注过程如图2所示。经过卷积操作后的特征响应图<b><i>R</i></b>是一个<i>C</i>×<i>H</i>×<i>W</i>大小的特征响应图。其中, <i>C</i>表示特征响应图的通道数, <i>H</i>、<i>W</i>分别表示特征响应图在空间维度上的长度和宽度。其关注过程是先进行通道关注, 随后进行空间关注。</p>
                </div>
                <div class="area_img" id="53">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201905033_053.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 混合关注过程" src="Detail/GetImg?filename=images/JSJC201905033_053.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图2 混合关注过程</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201905033_053.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="54" name="54">1.2.1 通道关注</h4>
                <div class="p1">
                    <p id="55">通道关注具体步骤为:</p>
                </div>
                <div class="p1">
                    <p id="56">1) 在空间维度上压缩特征, 将在同一通道上的特征压缩成一个实数, 得到向量<b><i>C</i></b>。特征在通道维度上的响应可以由<b><i>C</i></b>来表示。这一步操作可以通过全局池化来实现:</p>
                </div>
                <div class="p1">
                    <p id="57" class="code-formula">
                        <mathml id="57"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">C</mi><mo>=</mo><mi>G</mi><mi>Ρ</mi><mrow><mo> (</mo><mi mathvariant="bold-italic">R</mi><mo>) </mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="58">其中, <i>GP</i>表示全局池化函数, <b><i>R</i></b>表示卷积输出的特征。</p>
                </div>
                <div class="p1">
                    <p id="59">2) 通过全连接层的学习, 获得特征在通道维度上的响应权重分布<b><i>C</i></b>′:</p>
                </div>
                <div class="p1">
                    <p id="60" class="code-formula">
                        <mathml id="60"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msup><mi mathvariant="bold-italic">C</mi><mo>′</mo></msup><mo>=</mo><mi>F</mi><mi>C</mi><mrow><mo> (</mo><mi mathvariant="bold-italic">C</mi><mo>) </mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="61">其中, <i>FC</i>表示全连接函数, <b><i>C</i></b>表示全局池化函数的输出。</p>
                </div>
                <div class="p1">
                    <p id="62">3) 将学习获得的权重分布<b><i>C</i></b>′施加到原来的特征上去, 实现在通道维度上的关注:</p>
                </div>
                <div class="p1">
                    <p id="63" class="code-formula">
                        <mathml id="63"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msup><mi mathvariant="bold-italic">R</mi><mo>′</mo></msup><mo>=</mo><mi>S</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>e</mi><mrow><mo> (</mo><mrow><mi mathvariant="bold-italic">R</mi><mo>, </mo><msup><mi mathvariant="bold-italic">C</mi><mo>′</mo></msup></mrow><mo>) </mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="64">其中, <i>Scale</i>表示权重施加函数。</p>
                </div>
                <h4 class="anchor-tag" id="65" name="65">1.2.2 空间关注</h4>
                <div class="p1">
                    <p id="66">空间关注具体步骤为:</p>
                </div>
                <div class="p1">
                    <p id="67">1) 在通道维度上压缩特征, 将在同一空间的特征压缩成一个实数, 得到矩阵<b><i>S</i></b>。特征在空间维度上的响应可以由<b><i>S</i></b>来表示。这一步操作可以通过通道压缩层来实现:</p>
                </div>
                <div class="p1">
                    <p id="68" class="code-formula">
                        <mathml id="68"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">S</mi><mo>=</mo><mi>C</mi><mi>F</mi><mrow><mo> (</mo><msup><mi mathvariant="bold-italic">R</mi><mo>′</mo></msup><mo>) </mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="69">其中, <i>CF</i>表示通道压缩函数, <b><i>R</i></b>′表示通道关注后获得的特征图。</p>
                </div>
                <div class="p1">
                    <p id="70">2) 通过全连接层的学习, 获得特征在空间维度上的响应权重分布<b><i>S</i></b>′:</p>
                </div>
                <div class="p1">
                    <p id="71" class="code-formula">
                        <mathml id="71"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msup><mi mathvariant="bold-italic">S</mi><mo>′</mo></msup><mo>=</mo><mi>F</mi><mi>C</mi><mrow><mo> (</mo><mi mathvariant="bold-italic">S</mi><mo>) </mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>5</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="72">其中, <b><i>S</i></b>表示通道压缩函数的输出。</p>
                </div>
                <div class="p1">
                    <p id="73">3) 将学习获得的权重分布<b><i>S</i></b>′施加到原来的特征上, 实现在空间维度上的关注:</p>
                </div>
                <div class="p1">
                    <p id="74" class="code-formula">
                        <mathml id="74"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msup><msup><mi mathvariant="bold-italic">R</mi><mo>′</mo></msup><mo>′</mo></msup><mo>=</mo><mi>S</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>e</mi><mrow><mo> (</mo><mrow><msup><mi mathvariant="bold-italic">R</mi><mo>′</mo></msup><mo>, </mo><msup><mi mathvariant="bold-italic">S</mi><mo>′</mo></msup></mrow><mo>) </mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>6</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="75">级联通道关注模块和空间关注模块构成的混合关注模块的网络结构如图3所示。</p>
                </div>
                <div class="area_img" id="76">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201905033_076.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 混合关注模块网络结构" src="Detail/GetImg?filename=images/JSJC201905033_076.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图3 混合关注模块网络结构</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201905033_076.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="77" name="77">1.3 网络递归</h4>
                <div class="p1">
                    <p id="78">实现由粗到细的网络递归的核心是图像切割放大模块, 该模块由2个级联的全连接层和1个切割放大层构成。</p>
                </div>
                <div class="p1">
                    <p id="79">通过2个级联的全连接层来学习定位出要切割的矩形区域的坐标。经过切割放大层在原图像上切割出对应的区域, 并将此区域放大到原图大小。将切割放大后的区域图像送入第2路网络。</p>
                </div>
                <div class="p1">
                    <p id="80">原图像和经过切割放大后的图像对比如图4所示。</p>
                </div>
                <div class="area_img" id="81">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201905033_081.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 原图像与切割放大后图像对比" src="Detail/GetImg?filename=images/JSJC201905033_081.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图4 原图像与切割放大后图像对比</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201905033_081.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="82">区域定位具体步骤为:</p>
                </div>
                <div class="p1">
                    <p id="83">1) 提取完成训练的第1路网络最后一个卷积结构单元输出的特征, 将其在通道维度上压缩, 得到平面特征, 用<i>F</i> (<i>i</i>, <i>j</i>) 表示第<i>i</i>行, 第<i>j</i>列的像素点的平面特征值。</p>
                </div>
                <div class="p1">
                    <p id="84">2) 遍历平面特征, 找出最大值记为<i>Value</i><sub>max</sub>。以<i>Th</i>=<i>λ</i>×<i>Value</i><sub>max</sub>为阈值 (<i>λ</i>决定所要切割的区域的大小, 根据经验可取0.1) , 对平面特征进行阈值分割:</p>
                </div>
                <div class="p1">
                    <p id="85" class="code-formula">
                        <mathml id="85"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>F</mi><mo stretchy="false"> (</mo><mi>i</mi><mo>, </mo><mi>j</mi><mo stretchy="false">) </mo><mo>=</mo><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mi>F</mi><mo stretchy="false"> (</mo><mi>i</mi><mo>, </mo><mi>j</mi><mo stretchy="false">) </mo><mo>, </mo><mi>F</mi><mo stretchy="false"> (</mo><mi>i</mi><mo>, </mo><mi>j</mi><mo stretchy="false">) </mo><mo>&gt;</mo><mi>Τ</mi><mi>h</mi></mtd></mtr><mtr><mtd><mn>0</mn><mo>, </mo><mtext>其</mtext><mtext>他</mtext></mtd></mtr></mtable></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>7</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="86">3) 对分割后的平面特征图进行种子填充, 找出面积最大的填充区域, 并用最小的矩形框框出此填充区域, 将此矩形区域根据卷积神经网络的感受野进行映射, 得到原图对应的矩形区域。</p>
                </div>
                <div class="p1">
                    <p id="87">4) 将提取的特征作为数据, 以计算获得的矩形区域为监督, 训练这2个级联的全连接层。</p>
                </div>
                <h4 class="anchor-tag" id="88" name="88">1.4 网络训练步骤</h4>
                <div class="p1">
                    <p id="89">网络训练具体步骤为:</p>
                </div>
                <div class="p1">
                    <p id="90">1) 使用在ImageNet数据集中预训练的ResNeXt50模型初始化混合关注的ResNeXt50网络, 可以加快网络收敛速度。将细粒度图像调整为448×448大小输入初始化后的第1路混合关注的ResNeXt50网络进行训练。</p>
                </div>
                <div class="p1">
                    <p id="91">2) 提取完成训练的第1路混合关注的ResNeXt50网络最后一个卷积单元输出的特征, 根据第1.3节所述方法训练切割放大模块。</p>
                </div>
                <div class="p1">
                    <p id="92">3) 保持第1路网络和切割放大模块参数不变, 训练第2路经过初始化的混合关注的ResNeXt50网络。</p>
                </div>
                <div class="p1">
                    <p id="93">4) 将2路网络的Softmax层的输出进行级联并接上1个全连接层。保持2路网络和切割放大模块参数不变, 训练最后的全连接层。</p>
                </div>
                <h3 id="94" name="94" class="anchor-tag">2 实验与结果分析</h3>
                <div class="p1">
                    <p id="95">实验使用公开数据集Stanford Dogs<citation id="121" type="reference"><link href="27" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>和Stanford Cars<citation id="122" type="reference"><link href="29" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>。其中, Stanford Dogs有120类狗, 共20 580张图像。由于该数据集没有划分训练集和测试集, 本文按照1.4∶1进行划分, 即训练集12 005张, 测试集8 575张。Stanford Cars有196类车, 共16 188张图像, 其中, 训练集8 144张, 测试集8 044张。</p>
                </div>
                <h4 class="anchor-tag" id="96" name="96">2.1 网络模型对比结果</h4>
                <div class="p1">
                    <p id="97">分别使用ResNeXt50网络、添加了混合关注模块的ResNeXt50网络以及递归混合关注的ResNeXt50网络在这2个公开数据集上实验, 所得结果如表1所示。</p>
                </div>
                <div class="area_img" id="98">
                    <p class="img_tit"><b>表1 3种网络对2个数据集的分类精度</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note">%</p>
                    <table id="98" border="1"><tr><td>数据集</td><td>ResNeXt50</td><td>混合关注<br />ResNeXt50</td><td>递归混合<br />关注ResNeXt50</td></tr><tr><td><br />Stanford Dogs</td><td>82.5</td><td>86.2</td><td>87.1</td></tr><tr><td><br />Stanford Cars</td><td>88.3</td><td>91.6</td><td>92.4</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="99">从表1可以看出, 在2个数据集上, 混合关注的ResNeXt50网络的分类精度比ResNeXt50网络的分类精度高3.7%和3.3%;递归混合关注的ResNeXt50网络的分类精度又比混合关注的ResNeXt50网络的分类精度高0.9%和0.8%。</p>
                </div>
                <div class="p1">
                    <p id="100">因此可以得出结论, 在ResNeXt50网络中添加混合关注模块可以大幅提升细粒度图像分类的精度, 递归混合关注的ResNeXt50网络可以进一步提升细粒度图像分类的精度。</p>
                </div>
                <h4 class="anchor-tag" id="101" name="101">2.2 算法对比实验结果</h4>
                <div class="p1">
                    <p id="102">分别在Stanford Dogs和Stanford Cars数据集上, 对递归混合关注的ResNeXt50网络和其他算法的分类精度进行对比。对比结果如表2和表3所示。</p>
                </div>
                <div class="area_img" id="103">
                    <p class="img_tit"><b>表2 在Stanford Dogs数据集上算法分类精度</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note">%</p>
                    <table id="103" border="1"><tr><td><br />算法</td><td>精度</td></tr><tr><td><br />FCAN<sup>[15]</sup></td><td>84.2</td></tr><tr><td><br />DVAN<sup>[16]</sup></td><td>81.5</td></tr><tr><td><br />RA-CNN<sup>[17]</sup></td><td>87.3</td></tr><tr><td><br />本文算法</td><td>87.1</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="104">
                    <p class="img_tit"><b>表3 在Stanford Cars数据集上算法分类精度</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note">%</p>
                    <table id="104" border="1"><tr><td><br />算法</td><td>精度</td><td>是否边框标记</td></tr><tr><td><br />RCNN<sup>[9]</sup></td><td>88.4</td><td>是</td></tr><tr><td><br />PA-CNN<sup>[10]</sup></td><td>92.8</td><td>是</td></tr><tr><td><br />HSnet<sup>[11]</sup></td><td>93.9</td><td>是</td></tr><tr><td><br />B-CNN<sup>[12]</sup></td><td>91.3</td><td>否</td></tr><tr><td><br />LRBP<sup>[18]</sup></td><td>90.9</td><td>否</td></tr><tr><td><br />HIHCA<sup>[19]</sup></td><td>91.7</td><td>否</td></tr><tr><td><br />本文算法</td><td>92.4</td><td>否</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="105">从表2可以看出, 在Stanford Dogs数据集上, 本文算法的分类精度远高于FCAN和DVAN算法, 与目前在该数据集上分类精度最高的RA-CNN算法相当。</p>
                </div>
                <div class="p1">
                    <p id="106">从表3可以看出, 在Stanford Cars数据集上, 本文算法比B-CNN、LRBP、HIHCA等基于弱监督信息模型的算法的分类精度分别高出1.1%、1.5%、0.7%, 和基于强监督信息模型的PA-CNN分类精度相当, 与目前最新的基于强监督信息模型的HSnet分类精度仅低1.5%, 比RCNN这一基于强监督信息模型的算法高出4%。因为ResNeXt50网络本身的特征提取能力比较强, 加入混合关注模块进一步提升了性能, 并且切割放大模块可以比较粗略地检测到车辆的位置, 通过由粗到细的网络递归实现了更为精细化特征的提取、多尺度和特征融合。RCNN算法中的特征提取网络性能较差, 并且只提取了预测区域的单一尺度特征用于分类, 即使有边框标记信息, 分类精度仍然相对较低。</p>
                </div>
                <div class="p1">
                    <p id="107">因此可以得出结论, 本文算法是一种性能优良的基于弱监督信息模型的细粒度图像分类算法。</p>
                </div>
                <h3 id="108" name="108" class="anchor-tag">3 结束语</h3>
                <div class="p1">
                    <p id="109">本文提出递归深度混合关注网络方法。通过在残差单元中添加混合关注模块以实现在通道维度上和空间维度上提升网络性能, 并经过网络递归实现提取更为精细化且具有判别性的局部特征, 达到多尺度和特征融合的目的。在Stanford Dogs、Stanford Cars中进行对比实验, 结果表明, 本文算法的分类精度分别为87.1%、92.4%, 优于FCAN、HIHCA等算法。且该算法基于弱监督信息, 无需提供边框标记等信息, 具有较高的实用性和鲁棒性。下一步将提高本文算法的分类精度, 以更好地应用于各个领域的细粒度图像分类。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Very deep convolutional networks for large-scale image recognition">

                                <b>[1]</b> SIMONYAN K, ZISSERMAN A.Very deep convolutional networks for large-scale image recognition[J].Computer Science, 2014:1.
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep Residual Learning for Image Recognition">

                                <b>[2]</b> HE Kaiming, ZHANG Xiangyu, REN Shaoqing, et al.Deep residual learning for image recognition[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2015:770-778.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Densely Connected Convolutional Networks">

                                <b>[3]</b> HUANG Gao, LIU Zhuang, MAATEN L V D, et al.Densely connected convolutional networks[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2017:4700-4708.
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=SZJT201604169&amp;v=MzE4OTR6cXFCdEdGckNVUkxPZVplUm9GeTNsVmJ2QU5qZkJlckc0SDlmTXE0NURiWVFLREg4NHZSNFQ2ajU0TzM=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b> 李炳政.关于智能交通中的车型识别分析[J].数字技术与应用, 2016 (4) :230-230.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201609015&amp;v=Mjg2MTZWYnZBTHpUWlpMRzRIOWZNcG85RVlZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSb0Z5M2w=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b> 冯琼瑛, 陈粤超, 张家善, 等.红树林湿地鸟类查询与鉴别系统的设计与实现[J].计算机应用与软件, 2016, 33 (9) :64-66.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Where to Buy It:Matching Street Clothing Photos in Online Shops">

                                <b>[6]</b> KIAPOUR M H, HAN Xufeng, LAZEBNIK S, et al.Where to buy it:matching street clothing photos in online shops[C]//Proceedings of IEEE International Conference on Computer Vision.Washington D.C., USA:IEEE Press, 2015:3343-3351.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201711031&amp;v=MjU0ODNVUkxPZVplUm9GeTNsVmJ2QUx6N0JiYkc0SDliTnJvOUdaWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckM=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b> 陈玄, 朱荣, 王中元.基于融合卷积神经网络模型的手写数字识别[J].计算机工程, 2017, 43 (11) :187-192.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201708042&amp;v=MzA4MjdlWmVSb0Z5M2xWYnZBTHo3QmJiRzRIOWJNcDQ5QlpvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE8=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b> 易生, 梁华刚, 茹锋.基于多列深度3D卷积神经网络的手势识别[J].计算机工程, 2017, 43 (8) :243-248.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Rich feature hierarchies for accurate object detection and semantic segmentation">

                                <b>[9]</b> GIRSHICK R, DONAHUE J, DARRELL T, et al.Rich feature hierarchies for accurate object detection and semantic segmentation[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2014:580-587.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fine-grained recognition without part annotations">

                                <b>[10]</b> KRAUSE J, JIN Hailin.Fine-grained recognition without part annotations[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2015:5546-5555.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fine-Grained Recognition as HSnet Search for Informative Image Parts">

                                <b>[11]</b> LAM M, MAHASSENI B.Fine-grained recognition as HSnet search for informative image parts[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2017:6497-6506.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Bilinear CNN models for fine-grained visual recognition">

                                <b>[12]</b> ROYCHOWDHURY A.Bilinear CNN models for fine-grained visual recognition[C]//Proceedings of IEEE International Conference on Computer Vision.Washington D.C., USA:IEEE Press, 2015:1449-1457.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=3D object representations for fine-grained categorization">

                                <b>[13]</b> KRAUSE J, STARK M, JIA Deng, et al.3D object representations for fine-grained categorization[C]//Proceedings of IEEE International Conference on Computer Vision.Washington D.C., USA:IEEE Press, 2014:554-561.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Novel datasets for fine-grained image categorization">

                                <b>[14]</b> KHOSLA A, JAYADEVAPRAKASH N, YAO Bangpeng, et al.Novel dataset for fine-grained image categorization[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2011.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fully convolutional attention localization networks:efficient attention localization for fine-grained recognition">

                                <b>[15]</b> LIU Xiao, XIA Tian, WANG Jiang, et al.Fully convolutional attention localization networks:efficient attention localization for fine-grained recognition[EB/OL].[2018-03-12].https://arxiv.org/pdf/1603.06765.pdf.
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Diversified visual attention networks for fine-grained object classification">

                                <b>[16]</b> ZHAO Bo, WU Xiao, FENG Jiashi, et al.Diversified visual attention networks for fine-grained object classifica-tion[J].IEEE Transactions on Multimedia, 2017, 19 (6) :1245-1256.
                            </a>
                        </p>
                        <p id="35">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Look closer to see better:recurrent attention convolutional neural network for fine-grained image recognition">

                                <b>[17]</b> FU Jianlong, ZHENG Heliang, MEI Tao.Look closer to see better:recurrent attention convolutional neural network for fine-grained image recognition[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2017:4438-4446.
                            </a>
                        </p>
                        <p id="37">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Low-Rank Bilinear Pooling for Fine-Grained Classification">

                                <b>[18]</b> KONG Shu, FOWLKES C.Low-rank bilinear pooling for fine-grained classification[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2017:365-374.
                            </a>
                        </p>
                        <p id="39">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Higher-order integration of hierarchical convolutional activations for fine-grained visual categorization">

                                <b>[19]</b> CAI Sijia, ZUO Wangmeng, ZHANG Lei.Higher-order integration of hierarchical convolutional activations for fine-grained visual categorization[C]//Proceedings of IEEE International Conference on Computer Vision.Washington D.C., USA:IEEE Press, 2017:511-520.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJC201905033" />
        <input id="dpi" type="hidden" value="600" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201905033&amp;v=MjMwNDI3QmJiRzRIOWpNcW85R1o0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSb0Z5M2xWYnZBTHo=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
