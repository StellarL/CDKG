<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637128045817686250%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJC201909039%26RESULT%3d1%26SIGN%3dx3GdLmifP0w7m6pJgns7fCK0lhc%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201909039&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201909039&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201909039&amp;v=MTE2MjVMT2VaZVJyRnk3bFZMekFMejdCYmJHNEg5ak1wbzlHYllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtJa2ZXcXc2ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#45" data-title="0 概述 ">0 概述</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#49" data-title="1 RC算法分析 ">1 RC算法分析</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#60" data-title="2 改进的RC算法 ">2 改进的RC算法</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#62" data-title="2.1 优化的SLIC超像素分割">2.1 优化的SLIC超像素分割</a></li>
                                                <li><a href="#86" data-title="2.2 基于&lt;i&gt;SLIC&lt;/i&gt;算法的&lt;i&gt;RC&lt;/i&gt;算法">2.2 基于<i>SLIC</i>算法的<i>RC</i>算法</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#92" data-title="3 系统实现与实验分析 ">3 系统实现与实验分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#93" data-title="3.1 系统实现">3.1 系统实现</a></li>
                                                <li><a href="#102" data-title="3.2 实验分析">3.2 实验分析</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#112" data-title="4 结束语 ">4 结束语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#68" data-title="&lt;b&gt;图1 K-means与SLIC算法搜索示意图&lt;/b&gt;"><b>图1 K-means与SLIC算法搜索示意图</b></a></li>
                                                <li><a href="#88" data-title="&lt;b&gt;图2 改进的RC算法流程&lt;/b&gt;"><b>图2 改进的RC算法流程</b></a></li>
                                                <li><a href="#91" data-title="&lt;b&gt;图3 显著性检测结果对比&lt;/b&gt;"><b>图3 显著性检测结果对比</b></a></li>
                                                <li><a href="#95" data-title="&lt;b&gt;图4 自动抠图系统流程&lt;/b&gt;"><b>图4 自动抠图系统流程</b></a></li>
                                                <li><a href="#97" data-title="&lt;b&gt;图5 显著性图像抠取系统运行截图&lt;/b&gt;"><b>图5 显著性图像抠取系统运行截图</b></a></li>
                                                <li><a href="#99" data-title="&lt;b&gt;图6 合成系统运行截图&lt;/b&gt;"><b>图6 合成系统运行截图</b></a></li>
                                                <li><a href="#101" data-title="&lt;b&gt;图7 合成图像截图&lt;/b&gt;"><b>图7 合成图像截图</b></a></li>
                                                <li><a href="#107" data-title="&lt;b&gt;图8 测试结果对比&lt;/b&gt;"><b>图8 测试结果对比</b></a></li>
                                                <li><a href="#110" data-title="&lt;b&gt;表1 算法执行平均时间&lt;/b&gt;"><b>表1 算法执行平均时间</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="5">


                                    <a id="bibliography_1" title=" BERMAN A,VLAHOS P,DADOURIAN A.Comprehensive method for removing from an image the background surrounding a selected subject:US6134345[P].2000-10-17." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Comprehensive method for removing from an image the background surrounding a selected subject">
                                        <b>[1]</b>
                                         BERMAN A,VLAHOS P,DADOURIAN A.Comprehensive method for removing from an image the background surrounding a selected subject:US6134345[P].2000-10-17.
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_2" title=" CHUANG Y Y,CURLESS B,SALESIN D H,et al.A Bayesian approach to digital matting[C]//Proceedings of CVPR’01.Washington D.C.,USA:IEEE Press,2001:264-271." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Bayesian approach to digital matting">
                                        <b>[2]</b>
                                         CHUANG Y Y,CURLESS B,SALESIN D H,et al.A Bayesian approach to digital matting[C]//Proceedings of CVPR’01.Washington D.C.,USA:IEEE Press,2001:264-271.
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_3" title=" RUZON M A,TOMASI C.Alpha estimation in natural images[C]//Proceedings of CVPR’00.Washington D.C.,USA:IEEE Press,2000:18-25." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Alpha estimation in natural images">
                                        <b>[3]</b>
                                         RUZON M A,TOMASI C.Alpha estimation in natural images[C]//Proceedings of CVPR’00.Washington D.C.,USA:IEEE Press,2000:18-25.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_4" title=" LEVIN A,LISCHINSKI D,WEISS Y.A closed-form solution to natural image matting[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2008,30(2):228-242." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A closed-form solution to natural image matting">
                                        <b>[4]</b>
                                         LEVIN A,LISCHINSKI D,WEISS Y.A closed-form solution to natural image matting[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2008,30(2):228-242.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_5" title=" GRADY L,SCHIWIETZ T,AHARON S,et al.Random walks for interactive alpha-matting[C]//Proceedings of VIIP’05.Calgary,Canada:ACTA Press,2005:423-429." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Random walks for interactive alpha-matting">
                                        <b>[5]</b>
                                         GRADY L,SCHIWIETZ T,AHARON S,et al.Random walks for interactive alpha-matting[C]//Proceedings of VIIP’05.Calgary,Canada:ACTA Press,2005:423-429.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_6" title=" JIANG Hao,XU Jie.Matching objects in multi-camera surveillance without geometric constraints[J].Journal of Convergence Information Technology,2010,5(6):79-86." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SPQD&amp;filename=SPQD00003386430&amp;v=MDI1NDV1ZHRGQzNsVmIvUEkxWT1OajNhYXJPNEh0SFBySWREWU9nUFkzazV6QmRoNGo5OVNYcVJyeG94Y01IN1I3cWVi&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtJa2ZXcXc2ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                         JIANG Hao,XU Jie.Matching objects in multi-camera surveillance without geometric constraints[J].Journal of Convergence Information Technology,2010,5(6):79-86.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_7" title=" ZHANG Qiaorong,ZHANG Yongqiang.Salient region detection in video using spatiotemporal visual attention model[J].International Journal of Digital Content Technology and its Applications,2012,6(11):35-47." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SPQD&amp;filename=SPQD120813007800&amp;v=MjMzMDlJOUZZK01QREJNOHp4VVNtRGQ5U0g3bjN4RTlmYnZuS3JpZlp1OXVGQ3ZnVTduSUoxMGNOajNhYXJLNkh0bk5y&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtJa2ZXcXc2ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                         ZHANG Qiaorong,ZHANG Yongqiang.Salient region detection in video using spatiotemporal visual attention model[J].International Journal of Digital Content Technology and its Applications,2012,6(11):35-47.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_8" title=" JIAN Muwei,DONG Junyu,MA Jun.Image retrieval using wavelet-based salient regions[J].The Imaging Science Journal,2011,59(4):219-231." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=STJD&amp;filename=STJDB33FD423CCD04951619D95D8F3C52D8F&amp;v=MjM2MzNOMWh3cmk0dzZBPU5qbkJhc0c3SGFlNHE0MUdGNWg3REhnd3loY1Y2ellKUVhxV3BHUTJDcmVXTWJMcENPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkNwYlEzNQ==&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtJa2ZXcXc2ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                         JIAN Muwei,DONG Junyu,MA Jun.Image retrieval using wavelet-based salient regions[J].The Imaging Science Journal,2011,59(4):219-231.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_9" title=" 高东东,张新生.基于空间卷积神经网络模型的图像显著性检测[J].计算机工程,2018,44(5):240-245." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201805040&amp;v=MTY1NTZWTHpBTHo3QmJiRzRIOW5NcW85QlpJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSckZ5N2w=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtJa2ZXcXc2ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                         高东东,张新生.基于空间卷积神经网络模型的图像显著性检测[J].计算机工程,2018,44(5):240-245.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_10" >
                                        <b>[10]</b>
                                     陈佳洲,曾碧,何元烈.一种应用于静态图像人体分割的显著性检测方法[J].小型微型计算机系统,2016,37(3):608-611.</a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_11" title=" GOFERMAN S,ZELNIK-MANOR L,TAL A.Context-aware saliency detection[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2012,34(10):1915-1926." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Context-Aware Saliency Detection">
                                        <b>[11]</b>
                                         GOFERMAN S,ZELNIK-MANOR L,TAL A.Context-aware saliency detection[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2012,34(10):1915-1926.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_12" title=" XIE Yunlin,LU Huchuan.Visual saliency detection based on Bayesian model[C]//Proceedings of the 18th IEEE International Conference on Image Processing.Washington D.C.,USA:IEEE Press,2011:645-648." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Visual Saliency Detection based on Bayesian Model">
                                        <b>[12]</b>
                                         XIE Yunlin,LU Huchuan.Visual saliency detection based on Bayesian model[C]//Proceedings of the 18th IEEE International Conference on Image Processing.Washington D.C.,USA:IEEE Press,2011:645-648.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_13" title=" HORNUNG A,PRITCH Y,KRAHENBUHL P,et al.Saliency filters:contrast based filtering for salient region detection[C]//Proceedings of CVPR’12.Washington D.C.,USA:IEEE Press,2012:733-740." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Saliency filters:Contrast based filtering for salient region detection">
                                        <b>[13]</b>
                                         HORNUNG A,PRITCH Y,KRAHENBUHL P,et al.Saliency filters:contrast based filtering for salient region detection[C]//Proceedings of CVPR’12.Washington D.C.,USA:IEEE Press,2012:733-740.
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_14" title=" CHENG Mingming,MITRA N J,HUANG Xiaolei,et al.Global contrast based salient region detection[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2015,37(3):569-582." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Global Contrast Based Salient Region Detection">
                                        <b>[14]</b>
                                         CHENG Mingming,MITRA N J,HUANG Xiaolei,et al.Global contrast based salient region detection[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2015,37(3):569-582.
                                    </a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_15" title=" ACHANTA R,SHAJI A,SMITH K,et al.SLIC superpixels:EPFL 149300[R].Lausanne,Switzerland:EPFL,2010:4-7." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=SLIC superpixels">
                                        <b>[15]</b>
                                         ACHANTA R,SHAJI A,SMITH K,et al.SLIC superpixels:EPFL 149300[R].Lausanne,Switzerland:EPFL,2010:4-7.
                                    </a>
                                </li>
                                <li id="35">


                                    <a id="bibliography_16" title=" 王淑敏,宫宁生,陈逸韬.加权的超像素级时空上下文目标跟踪[J].计算机应用研究,2016,34(1):270-274." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSYJ201701062&amp;v=MTg3NzZWTHpBTHo3U1pMRzRIOWJNcm85RFpvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSckZ5N2w=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtJa2ZXcXc2ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[16]</b>
                                         王淑敏,宫宁生,陈逸韬.加权的超像素级时空上下文目标跟踪[J].计算机应用研究,2016,34(1):270-274.
                                    </a>
                                </li>
                                <li id="37">


                                    <a id="bibliography_17" title=" ACHANTA R,HEMAMI S,ESTRADA F,et al.Frequency-tuned salient region detection[C]//Proceedings of CVPR’09.Washington D.C.,USA:IEEE Press,2009:1597-1604." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Frequency-tuned salient region detection">
                                        <b>[17]</b>
                                         ACHANTA R,HEMAMI S,ESTRADA F,et al.Frequency-tuned salient region detection[C]//Proceedings of CVPR’09.Washington D.C.,USA:IEEE Press,2009:1597-1604.
                                    </a>
                                </li>
                                <li id="39">


                                    <a id="bibliography_18" title=" FELZENSZWALB P F,HUTTENLOCHER D P.Efficient graph-based image segmentation[M].[S.l.]:Kluwer Academic Publishers,2004." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Efficient graph-based image segmentation">
                                        <b>[18]</b>
                                         FELZENSZWALB P F,HUTTENLOCHER D P.Efficient graph-based image segmentation[M].[S.l.]:Kluwer Academic Publishers,2004.
                                    </a>
                                </li>
                                <li id="41">


                                    <a id="bibliography_19" title=" ZHAI Yun,SHAH M.Visual attention detection in video sequences using spatiotemporal cues[C]//Proceedings of the 14th ACM International Conference on Multimedia.New York,USA:ACM Press,2006:815-824." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Visual attention detection in video se-quences using spatiotemporal cues">
                                        <b>[19]</b>
                                         ZHAI Yun,SHAH M.Visual attention detection in video sequences using spatiotemporal cues[C]//Proceedings of the 14th ACM International Conference on Multimedia.New York,USA:ACM Press,2006:815-824.
                                    </a>
                                </li>
                                <li id="43">


                                    <a id="bibliography_20" title=" SINGH S,JALAL A S.Automatic generation of trimap for image matting[J].International Journal of Machine Intelligence and Sensory Signal Processing,2014,1(3):232-250." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCW&amp;filename=SJCWEE47232E43ADC87AA4097B60C145387E&amp;v=MDA5MDZRMzVOMWh3cmk0dzZBPU5pZkllY2JOR3RiT3JJMHdZT2grZUE4eHlHZGk3ajkwVHcza3JHRTBmYmVYVGIzcUNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkNwYg==&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtJa2ZXcXc2ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[20]</b>
                                         SINGH S,JALAL A S.Automatic generation of trimap for image matting[J].International Journal of Machine Intelligence and Sensory Signal Processing,2014,1(3):232-250.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJC" target="_blank">计算机工程</a>
                2019,45(09),248-252 DOI:10.19678/j.issn.1000-3428.0051158            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于图像显著性识别的自动抠图系统</b></span>
 <span class="shoufa"></span>                                     </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9B%B9%E6%98%A5%E7%BA%A2&amp;code=10966173&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtJa2ZXcXc2ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">曹春红</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%AD%99%E6%A6%95&amp;code=39853565&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtJa2ZXcXc2ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">孙榕</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%92%9F%E7%90%B4&amp;code=39853566&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtJa2ZXcXc2ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">钟琴</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%9C%E5%8C%97%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0111402&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtJa2ZXcXc2ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">东北大学计算机科学与工程学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>RC算法引入区域级别的对比度,对颜色模型进行重新量化,能大幅提高处理速度、突出显著目标,然而其基于图的分割算法易出现分割区域不能较好地贴合物体边缘的问题。引入优化的SLIC算法代替基于图的分割算法,对RC算法进行改进,并实现一个基于图像显著性识别的自动抠图系统,克服传统抠图系统必须人工标记的缺点。实验结果表明,相比IT、MZ、GB、RC等经典算法,改进的RC算法抠取的显著目标更精确,其查准率、查全率、<i>F</i>值分别为0.82、0.85和0.83,系统能自动抠取显著目标并提供图片合成应用。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%98%BE%E8%91%97%E6%80%A7%E8%AF%86%E5%88%AB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtJa2ZXcXc2ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">显著性识别;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=SLIC%E5%88%86%E5%89%B2&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtJa2ZXcXc2ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">SLIC分割;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8C%BA%E5%9F%9F%E5%AF%B9%E6%AF%94%E5%BA%A6&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtJa2ZXcXc2ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">区域对比度;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%87%AA%E5%8A%A8%E6%8A%A0%E5%9B%BE&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtJa2ZXcXc2ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">自动抠图;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%B8%89%E5%88%86%E5%9B%BE&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtJa2ZXcXc2ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">三分图;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    曹春红(1976—),女,副教授、博士,主研方向为计算机图形学、计算机辅助设计;E-mail:caochunhong@ cse. neu. edu. cn;
                                </span>
                                <span>
                                    孙榕,本科生。;
                                </span>
                                <span>
                                    钟琴,本科生。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-04-10</p>

                    <p>

                            <b>基金：</b>
                                                        <span>中央高校基本科研业务费专项资金(N161602001);</span>
                                <span>辽宁省自然科学基金(20170540312,20180520001);</span>
                    </p>
            </div>
                    <h1><b>Automatic Matting System Based on Image Saliency Recognition</b></h1>
                    <h2>
                    <span>CAO Chunhong</span>
                    <span>SUN Rong</span>
                    <span>ZHONG Qin</span>
            </h2>
                    <h2>
                    <span>School of Computer Science and Engineering,Northeastern University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>The RC algorithm introduces regional-level contrast and re-quantizes the color model,which can greatly improve processing speed and highlight salient targets.However,the graph-based segmentation algorithm is prone to have the problem that the segmented region cannot fit the edge of the object well.This paper introduces an optimized SLIC algorithm instead of a graph-based segmentation algorithm to improve the RC algorithm,and implements an automatic matting system based on image saliency recognition,which overcomes the shortcomings of traditional matting systems that must be manually labeled.Experimental results show that compared with other classical algorithms such as IT,MZ,GB,RC,etc.,the improved RC algorithm captures salient targets with greater accuracy,its Precision,Recall,and <i>F</i> value are 0.82,0.85,and 0.83,respectively.The system can automatically capture salient targets and provide image synthesis applications.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=saliency%20recognition&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtJa2ZXcXc2ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">saliency recognition;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=SLIC%20segmentation&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtJa2ZXcXc2ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">SLIC segmentation;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=regional%20contrast&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtJa2ZXcXc2ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">regional contrast;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=automatic%20matting&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtJa2ZXcXc2ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">automatic matting;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=trimap&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtJa2ZXcXc2ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">trimap;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-04-10</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="45" name="45" class="anchor-tag">0 概述</h3>
                <div class="p1">
                    <p id="46">图像抠取技术在图像编辑中有着重要的应用,当前主要的抠图方法可分为基于采样的图像抠取技术与基于传播的图像抠取技术。基于采样的图像抠取技术对每个像素独立进行求解,因此算法易于并行化实现且耗时少,是图像抠取技术初期的研究热点,其典型代表有Knockout方法<citation id="120" type="reference"><link href="5" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>、Bayesian方法<citation id="121" type="reference"><link href="7" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>以及Ruzon-Tomasi方法<citation id="122" type="reference"><link href="9" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>等。但如果三分图中的未知区域较大,则需要配对采样的前、背景点增加,错误配对的可能性也随之增大。基于传播的图像抠取技术典型代表有Closed-form方法<citation id="123" type="reference"><link href="11" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>、Random-walk方法<citation id="124" type="reference"><link href="13" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>和Poisson方法<citation id="125" type="reference"><link href="15" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>等。相对于基于采样的图像抠取技术,基于传播的图像抠取技术可以得到更平滑的结果,但是需要花费更多时间进行扣取。上述的图像抠取方法都需要人工干预对图像进行标记,也就是需要对图像进行预处理后才能进行图像扣取。</p>
                </div>
                <div class="p1">
                    <p id="47">图像显著区域(即人类感兴趣的区域)的像素点都会映射到人类视觉系统的特殊区域。检测显著性区域就是模拟人类的视觉系统来处理复杂的图像信息,获得视觉范围内最重要的部分<citation id="130" type="reference"><link href="17" rel="bibliography" /><link href="19" rel="bibliography" /><link href="21" rel="bibliography" /><sup>[<a class="sup">7</a>,<a class="sup">8</a>,<a class="sup">9</a>]</sup></citation>。文献<citation id="126" type="reference">[<a class="sup">10</a>]</citation>将局部比较法和全局比较法相结合,提出上下文感知图像显著性检测模型。文献<citation id="127" type="reference">[<a class="sup">11</a>]</citation>结合角点凸包和贝叶斯推断,提出BM算法用于显著性检测。文献<citation id="128" type="reference">[<a class="sup">12</a>]</citation>指出显著目标应该分散分布在背景空间。文献<citation id="129" type="reference">[<a class="sup">13</a>]</citation>提出HC算法和RC算法。HC算法使用所有像素与其他像素的颜色差值来求解显著值,进而获得全分辨率的显著图。RC算法将原图像分割,通过计算一个区域与其余区域的区域像素加权颜色差值和空间距离来确定显著值。HC算法和RC算法对于背景简单的图片的处理效果较好,但不适用于复杂背景图像。</p>
                </div>
                <div class="p1">
                    <p id="48">本文对RC算法及其缺点进行分析,提出改进的RC算法,对闭形式软抠取方法进行研究和改进,根据改进的RC算法得到显著图并求解三分图作为输入进行图像抠取。</p>
                </div>
                <h3 id="49" name="49" class="anchor-tag">1 RC算法分析</h3>
                <div class="p1">
                    <p id="50">视觉显著性检测除了提取颜色特征以外,还需要关注其空间位置关系。由于计算像素级对比度工作量十分庞大,RC算法引入区域级别的对比度,在此基础上重新量化颜色模型,减少色彩的数量级以提高计算效率。</p>
                </div>
                <div class="p1">
                    <p id="51">对一幅已分割的图像,为每个区域建立颜色直方图<citation id="131" type="reference"><link href="31" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>,计算区域<i>r</i><sub><i>k</i></sub>和其他区域的颜色对比度来确定该区域的显著度,计算公式如下:</p>
                </div>
                <div class="p1">
                    <p id="52" class="code-formula">
                        <mathml id="52"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>S</mi><mrow><mo>(</mo><mrow><mi>r</mi><msub><mrow></mrow><mi>k</mi></msub></mrow><mo>)</mo></mrow><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>r</mi><msub><mrow></mrow><mi>k</mi></msub><mo>≠</mo><mi>r</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></munder><mi>w</mi></mstyle><mrow><mo>(</mo><mrow><mi>r</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mo>)</mo></mrow><mi>D</mi><msub><mrow></mrow><mi>r</mi></msub><mrow><mo>(</mo><mrow><mi>r</mi><msub><mrow></mrow><mi>k</mi></msub><mo>,</mo><mi>r</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mo>)</mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="53">其中,<mathml id="114"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>w</mi><mrow><mo>(</mo><mrow><mi>r</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mo>)</mo></mrow></mrow></math></mathml>为区域r<sub>i</sub>的权重系数,<mathml id="115"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>D</mi><msub><mrow></mrow><mi>r</mi></msub><mrow><mo>(</mo><mrow><mi>r</mi><msub><mrow></mrow><mi>k</mi></msub><mo>,</mo><mi>r</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mo>)</mo></mrow></mrow></math></mathml>表示<i>r</i><sub><i>i</i></sub>和<i>r</i><sub><i>k</i></sub> 2个区域的颜色距离度量。2个区域<i>r</i><sub>1</sub>和<i>r</i><sub>2</sub>的颜色距离为:</p>
                </div>
                <div class="p1">
                    <p id="54" class="code-formula">
                        <mathml id="54"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>D</mi><msub><mrow></mrow><mi>r</mi></msub><mrow><mo>(</mo><mrow><mi>r</mi><msub><mrow></mrow><mn>1</mn></msub><mo>,</mo><mi>r</mi><msub><mrow></mrow><mn>2</mn></msub></mrow><mo>)</mo></mrow><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>n</mi><msub><mrow></mrow><mn>1</mn></msub></mrow></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>n</mi><msub><mrow></mrow><mn>2</mn></msub></mrow></munderover><mi>f</mi></mstyle></mrow></mstyle><mrow><mo>(</mo><mrow><mi>c</mi><msub><mrow></mrow><mrow><mn>1</mn><mo>,</mo><mi>i</mi></mrow></msub></mrow><mo>)</mo></mrow><mi>f</mi><mrow><mo>(</mo><mrow><mi>c</mi><msub><mrow></mrow><mrow><mn>2</mn><mo>,</mo><mi>j</mi></mrow></msub></mrow><mo>)</mo></mrow><mi>D</mi><mrow><mo>(</mo><mrow><mi>c</mi><msub><mrow></mrow><mrow><mn>1</mn><mo>,</mo><mi>i</mi></mrow></msub><mo>,</mo><mi>c</mi><msub><mrow></mrow><mrow><mn>2</mn><mo>,</mo><mi>j</mi></mrow></msub></mrow><mo>)</mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="55">其中,<mathml id="116"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>f</mi><mrow><mo>(</mo><mrow><mi>c</mi><msub><mrow></mrow><mrow><mi>k</mi><mo>,</mo><mi>i</mi></mrow></msub></mrow><mo>)</mo></mrow></mrow></math></mathml>是第i种颜色c<sub>k,i</sub>在第k个区域r<sub>k</sub>中出现的概率,k={1,2}。</p>
                </div>
                <div class="p1">
                    <p id="56">为增加区域的空间影响效果,在式(1)中引入空间权值。2个区域相距越近,空间影响就越大,反之则反。r<sub>k</sub>区域的空间加权对比度计算公式如下:</p>
                </div>
                <div class="p1">
                    <p id="57" class="code-formula">
                        <mathml id="57"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>S</mi><mrow><mo>(</mo><mrow><mi>r</mi><msub><mrow></mrow><mi>k</mi></msub></mrow><mo>)</mo></mrow><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>r</mi><msub><mrow></mrow><mi>k</mi></msub><mo>≠</mo><mi>r</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></munder><mrow><mi>exp</mi></mrow></mstyle><mrow><mo>(</mo><mrow><mo>-</mo><mi>D</mi><msub><mrow></mrow><mi>s</mi></msub><mrow><mo>(</mo><mrow><mi>r</mi><msub><mrow></mrow><mi>k</mi></msub><mo>,</mo><mi>r</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mo>)</mo></mrow><mo>/</mo><mi>δ</mi><msubsup><mrow></mrow><mi>s</mi><mn>2</mn></msubsup></mrow><mo>)</mo></mrow><mi>ω</mi><mrow><mo>(</mo><mrow><mi>r</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mo>)</mo></mrow><mi>D</mi><msub><mrow></mrow><mi>r</mi></msub><mrow><mo>(</mo><mrow><mi>r</mi><msub><mrow></mrow><mi>k</mi></msub><mo>,</mo><mi>r</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mo>)</mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>3</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="58">其中,<mathml id="117"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>D</mi><msub><mrow></mrow><mi>s</mi></msub><mrow><mo>(</mo><mrow><mi>r</mi><msub><mrow></mrow><mi>k</mi></msub><mo>,</mo><mi>r</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mo>)</mo></mrow></mrow></math></mathml>表示区域<i>r</i><sub><i>k</i></sub>和<i>r</i><sub><i>i</i></sub>的空间距离,<i>δ</i><sub><i>s</i></sub>表示空间距离的权重值,其值越大表示空间距离在对比度计算中作用越小,导致较远区域的对比度会对当前显著区域性值作出较大的贡献。实验结果表明,当<i>δ</i><sup>2</sup><sub><i>s</i></sub>=0.4时,像素坐标归一化为[0,1]<citation id="132" type="reference"><link href="31" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="59">RC算法是目前效果最好的显著性检测算法之一。但由于其是区域对比度方法,算法执行前先要进行图像分割,因此分割算法的效果会直接影响最终结果。RC算法使用基于图的分割算法,分割出的像素块形状不规则,无法良好地贴合各个区域的边缘,导致原始图像中的前景与背景分不开。本文使用优化的简单线性迭代聚类(Simple Linear Iterative Clustering, SLIC)算法对RC算法改进。</p>
                </div>
                <h3 id="60" name="60" class="anchor-tag">2 改进的RC算法</h3>
                <div class="p1">
                    <p id="61">RC算法使用基于图的图像分割算法,将原图像作为一个无向图,图中的节点是各个像素,边的权重是2个节点的不相似度。当2个节点在同一区域时,其边的权重应小于其在不同区域时的权重。该算法速度快,但获得超像素不够均匀和规则。所以本文选用改进的SLIC算法对RC算法进行优化,以在图像预处理时获得大小均匀的像素块,且降低时间复杂度。</p>
                </div>
                <h4 class="anchor-tag" id="62" name="62">2.1 优化的SLIC超像素分割</h4>
                <div class="p1">
                    <p id="63">SLIC是一种时间复杂度低的分割算法,其分割的轮廓清晰,满足本文对算法的期望。SLIC算法步骤如下<citation id="133" type="reference"><link href="33" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>:</p>
                </div>
                <div class="p1">
                    <p id="64">1)图像分割块的初始化。设图像的像素个数为 <i>N</i>,每一个图像块都是一个聚类,聚类的中心称为超像素,聚类的个数为 <i>k</i>,则每个块的大小为<i>S</i>×<i>S</i>。</p>
                </div>
                <div class="p1">
                    <p id="65" class="code-formula">
                        <mathml id="65"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>S</mi><mo>=</mo><msqrt><mrow><mfrac><mi>Ν</mi><mi>k</mi></mfrac></mrow></msqrt></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="66">2)聚类中心的初始化。在划分好的图像块里,随机采样一个点作为聚类中心,为避免采样的初始点是噪声或在边缘处,在采样点附近 3×3 的区域计算临近像素点的梯度,选择临近点中梯度最小的点为聚类中心。</p>
                </div>
                <div class="p1">
                    <p id="67">3)计算像素点到聚类中心的距离。如图1(a)所示,K-means聚类算法会计算像素点到每一个聚类中心的距离,其时间复杂度为<i>O</i>(<i>kNI</i>),其中<i>I</i>为迭代次数。而SLIC 算法只计算每个聚类中心周围 2<i>S</i>×2<i>S</i>范围内的像素点与该聚类中心的距离,其时间复杂度为<i>O</i>(<i>N</i>),如图1(b)所示。</p>
                </div>
                <div class="area_img" id="68">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201909039_068.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtJa2ZXcXc2ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 K-means与SLIC算法搜索示意图" src="Detail/GetImg?filename=images/JSJC201909039_068.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtJa2ZXcXc2ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图1 K-means与SLIC算法搜索示意图</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201909039_068.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtJa2ZXcXc2ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="69">4)更新聚类中心,迭代计算残差直到误差收敛,即每个像素聚类中心不再改变。根据图片效果取迭代次数。在聚类过程结束时,可能保留不属于与其聚类中心相同的连接分量的一些“孤立”像素,这些像素通常是不希望存在或者不需要的。为了使分割效果更好,使用连通分量算法将这些像素分配给最近聚类中心进行修正。</p>
                </div>
                <div class="p1">
                    <p id="70">在CIELAB五维颜色空间中,SLIC算法分别计算每个像素点与聚类中心的空间距离和颜色距离,计算公式如下:</p>
                </div>
                <div class="p1">
                    <p id="71" class="code-formula">
                        <mathml id="71"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>d</mi><msub><mrow></mrow><mtext>c</mtext></msub><mo>=</mo><msqrt><mrow><mrow><mrow><mo>(</mo><mrow><mi>l</mi><msub><mrow></mrow><mi>j</mi></msub><mo>-</mo><mi>l</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mo>)</mo></mrow></mrow><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mrow><mrow><mo>(</mo><mrow><mi>a</mi><msub><mrow></mrow><mi>j</mi></msub><mo>-</mo><mi>a</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mo>)</mo></mrow></mrow><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mrow><mrow><mo>(</mo><mrow><mi>b</mi><msub><mrow></mrow><mi>j</mi></msub><mo>-</mo><mi>b</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mo>)</mo></mrow></mrow><msup><mrow></mrow><mn>2</mn></msup></mrow></msqrt><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>4</mn><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mi>d</mi><msub><mrow></mrow><mtext>s</mtext></msub><mo>=</mo><msqrt><mrow><mrow><mrow><mo>(</mo><mrow><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mi>x</mi><msub><mrow></mrow><mi>j</mi></msub></mrow><mo>)</mo></mrow></mrow><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mrow><mrow><mo>(</mo><mrow><mi>y</mi><msub><mrow></mrow><mi>j</mi></msub><mo>-</mo><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mo>)</mo></mrow></mrow><msup><mrow></mrow><mn>2</mn></msup></mrow></msqrt><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>5</mn><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><msup><mi>D</mi><mo>′</mo></msup><mo>=</mo><msqrt><mrow><mrow><mrow><mo>(</mo><mrow><mfrac><mrow><mi>d</mi><msub><mrow></mrow><mi>c</mi></msub></mrow><mrow><mi>Ν</mi><msub><mrow></mrow><mi>c</mi></msub></mrow></mfrac></mrow><mo>)</mo></mrow></mrow><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mrow><mrow><mo>(</mo><mrow><mfrac><mrow><mi>d</mi><msub><mrow></mrow><mi>s</mi></msub></mrow><mrow><mi>Ν</mi><msub><mrow></mrow><mi>s</mi></msub></mrow></mfrac></mrow><mo>)</mo></mrow></mrow><msup><mrow></mrow><mn>2</mn></msup></mrow></msqrt><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>6</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="72">其中,<i>d</i><sub>c</sub>是颜色距离,<i>d</i><sub>s</sub>表示空间距离,<i>N</i><sub>c</sub>代表最大颜色距离,<i>N</i><sub>s</sub>表示每个聚类内最大空间距离,定义为<mathml id="118"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>Ν</mi><msub><mrow></mrow><mtext>s</mtext></msub><mo>=</mo></mrow><mi>S</mi><mo>=</mo><msqrt><mrow><mfrac><mi>Ν</mi><mi>Κ</mi></mfrac></mrow></msqrt></mrow></math></mathml>。最大颜色距离与图像和聚类有关,因此使用一个固定常数m来改善这种不定参数的情况,则式(6)可改写为:</p>
                </div>
                <div class="p1">
                    <p id="73" class="code-formula">
                        <mathml id="73"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msup><mi>D</mi><mo>′</mo></msup><mo>=</mo><msqrt><mrow><mrow><mrow><mo>(</mo><mrow><mfrac><mrow><mi>d</mi><msub><mrow></mrow><mtext>c</mtext></msub></mrow><mi>m</mi></mfrac></mrow><mo>)</mo></mrow></mrow><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mrow><mrow><mo>(</mo><mrow><mfrac><mrow><mi>d</mi><msub><mrow></mrow><mtext>s</mtext></msub></mrow><mi>S</mi></mfrac></mrow><mo>)</mo></mrow></mrow><msup><mrow></mrow><mn>2</mn></msup></mrow></msqrt><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>7</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="74">超像素距离度量值D′越小表示与聚类中心的相似度越高,则该像素属于这个超像素的可能性越大。由于超像素距离度量值只进行定性比较,不需要定量的计算结果,因此式(7)两边平方后乘以系数m<sup>2</sup>S<sup>2</sup>,则最终的目标函数为:</p>
                </div>
                <div class="p1">
                    <p id="75">S<sup>2</sup>d<sup>2</sup><sub><i>c</i></sub>+m<sup>2</sup>d<sup>2</sup><sub><i>s</i></sub>      (8)</p>
                </div>
                <div class="p1">
                    <p id="76">其中,S是xy空间内最大的可能值,由输入图像自动得出,m为<mathml id="119"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>[</mo><mrow><mn>1</mn><mo>,</mo><mn>4</mn><mn>0</mn></mrow><mo>]</mo></mrow></mrow></math></mathml>的整数。通过式(8)可直接比较距离的度量,且避免了浮点型计算,缩短了显著性检测的处理时间。</p>
                </div>
                <div class="p1">
                    <p id="77">优化后的<i>SLIC</i>算法流程如下:</p>
                </div>
                <div class="p1">
                    <p id="78">1.初始化聚类中心(种子点)。以相邻像素距离为<i>S</i>均匀分配种子点。</p>
                </div>
                <div class="p1">
                    <p id="79" class="code-formula">
                        <mathml id="79"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtext>S</mtext><mo>=</mo><msqrt><mrow><mfrac><mtext>Ν</mtext><mtext>k</mtext></mfrac></mrow></msqrt></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="80">2.选择在3×3邻域内梯度最小的像素点为聚类中心。</p>
                </div>
                <div class="p1">
                    <p id="81">3.<i>for</i>每一个聚类中心<i>do</i></p>
                </div>
                <div class="p1">
                    <p id="82">4.<i>for</i>以2<i>S</i>×2<i>S</i>为搜索范围,比较区域内每个像素点到其周围聚类中心的相对距离<i>do</i></p>
                </div>
                <div class="p1">
                    <p id="83">5.<i>end for</i></p>
                </div>
                <div class="p1">
                    <p id="84">6.根据搜索到的像素点,计算比较颜色和空间距离,取值最小种子点更新为聚类中心。</p>
                </div>
                <div class="p1">
                    <p id="85">7.<i>end for</i></p>
                </div>
                <h4 class="anchor-tag" id="86" name="86">2.2 基于<i>SLIC</i>算法的<i>RC</i>算法</h4>
                <div class="p1">
                    <p id="87">本文引入SLIC超像素分割代替基于图的分割,改进的RC算法流程如图2所示。</p>
                </div>
                <div class="area_img" id="88">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201909039_088.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtJa2ZXcXc2ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 改进的RC算法流程" src="Detail/GetImg?filename=images/JSJC201909039_088.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtJa2ZXcXc2ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图2 改进的RC算法流程</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201909039_088.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtJa2ZXcXc2ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="89">改进后的SLIC算法根据颜色和距离2种特征对图形进行聚类,生成的像素块大小均匀紧凑,解决了原算法将部分前景目标归类到背景区域的问题,更好地突出了前景物体。且算法运行时间短,聚类后的边界清晰,区域划分明确,最终得到的显著图能够均匀突出显著目标区域,降低非显著主体区域显著点的噪声干扰,减少纹理、噪声和块效应所产生的高频率干扰。物体轮廓保持以及超像素形状方面比较符合人们期望的分割效果,可广泛用于图像分割,有利于图像的下一步处理。</p>
                </div>
                <div class="p1">
                    <p id="90">图3为显著性检测结果。由图3可以看出,本文算法处理的花朵图像中的花蕊部分纳入显著区域与标准结果更相近,而鞭炮显著区域的边缘也比RC算法更加清晰。</p>
                </div>
                <div class="area_img" id="91">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201909039_091.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtJa2ZXcXc2ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 显著性检测结果对比" src="Detail/GetImg?filename=images/JSJC201909039_091.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtJa2ZXcXc2ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图3 显著性检测结果对比</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201909039_091.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtJa2ZXcXc2ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h3 id="92" name="92" class="anchor-tag">3 系统实现与实验分析</h3>
                <h4 class="anchor-tag" id="93" name="93">3.1 系统实现</h4>
                <div class="p1">
                    <p id="94">本文设计一种自动生成三分图的方法,提取图像的显著区域,并将其细化和修正自动获取三分图来代替人工标记的图像抠取,从而实现无人工干预的自动抠图。在此基础上,笔者开发了基于显著性识别的自动抠图系统,其流程如图4所示。</p>
                </div>
                <div class="area_img" id="95">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201909039_095.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtJa2ZXcXc2ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 自动抠图系统流程" src="Detail/GetImg?filename=images/JSJC201909039_095.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtJa2ZXcXc2ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图4 自动抠图系统流程</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201909039_095.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtJa2ZXcXc2ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="96">图5为显著性图像抠取系统运行截图。图5(a)是系统运行截图,图5(b)是选择原始图像后的运行截图,图5(c)是分割图像的运行截图。</p>
                </div>
                <div class="area_img" id="97">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201909039_097.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtJa2ZXcXc2ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 显著性图像抠取系统运行截图" src="Detail/GetImg?filename=images/JSJC201909039_097.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtJa2ZXcXc2ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图5 显著性图像抠取系统运行截图</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201909039_097.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtJa2ZXcXc2ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="98">根据显著特征提取后的前景图像,笔者设计了一个后期图像合成子系统,其运行截图如图6所示。图6(a)是根据显著图抠取得到的图像前景,图6(b)是通过下拉图像列表点击更换图像背景的运行截图。</p>
                </div>
                <div class="area_img" id="99">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201909039_099.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtJa2ZXcXc2ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 合成系统运行截图" src="Detail/GetImg?filename=images/JSJC201909039_099.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtJa2ZXcXc2ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图6 合成系统运行截图</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201909039_099.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtJa2ZXcXc2ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="100">图7是合成的最终输出图像。在合成过程中用户可以手动调整前景图像的位置得到最终的输出图像。</p>
                </div>
                <div class="area_img" id="101">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201909039_101.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtJa2ZXcXc2ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 合成图像截图" src="Detail/GetImg?filename=images/JSJC201909039_101.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtJa2ZXcXc2ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图7 合成图像截图</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201909039_101.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtJa2ZXcXc2ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="102" name="102">3.2 实验分析</h4>
                <div class="p1">
                    <p id="103">本文使用的测试图像选取自MSRA1000数据集,其中包括1 000幅测试图像以及对应的标准显著图。本文分别对1 000幅图像进行抠图处理,取结果的平均值评价算法的性能。评价指标采用查准率(<i>Precision</i>,分割结果中准确部分占分割结果部分的比重)、查全率(<i>Recall</i>,分割结果中准确部分占标准分割结果部分的比重)和<i>F</i>值。</p>
                </div>
                <div class="p1">
                    <p id="104" class="code-formula">
                        <mathml id="104"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>F</mi><mo>=</mo><mfrac><mrow><mo stretchy="false">(</mo><mn>1</mn><mo>+</mo><mi>β</mi><msup><mrow></mrow><mn>2</mn></msup><mo stretchy="false">)</mo><mo>⋅</mo><mi>Ρ</mi><mi>r</mi><mi>e</mi><mi>c</mi><mi>i</mi><mi>s</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo>⋅</mo><mi>R</mi><mi>e</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>l</mi></mrow><mrow><mi>β</mi><msup><mrow></mrow><mn>2</mn></msup><mo>⋅</mo><mi>Ρ</mi><mi>r</mi><mi>e</mi><mi>c</mi><mi>i</mi><mi>s</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo>+</mo><mi>R</mi><mi>e</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>l</mi></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>9</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="105">根据文献<citation id="134" type="reference">[<a class="sup">16</a>]</citation>,本文取<i>β</i><sup>2</sup>=0.3。</p>
                </div>
                <div class="p1">
                    <p id="106">本文算法(IRC)与其他经典算法IT<citation id="135" type="reference"><link href="37" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>、MZ<citation id="136" type="reference"><link href="39" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>、GB<citation id="137" type="reference"><link href="33" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>、SR<citation id="138" type="reference"><link href="7" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>、AC<citation id="139" type="reference"><link href="9" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>、LC<citation id="140" type="reference"><link href="11" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>、FT<citation id="141" type="reference"><link href="41" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>、RC进行了对比,其中RC<citation id="142" type="reference"><link href="31" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>表示基于频率协调的显著性检测方法,测试结果如图8所示。</p>
                </div>
                <div class="area_img" id="107">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201909039_107.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtJa2ZXcXc2ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 测试结果对比" src="Detail/GetImg?filename=images/JSJC201909039_107.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtJa2ZXcXc2ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图8 测试结果对比</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201909039_107.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtJa2ZXcXc2ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="108">由图8可以看出,本文算法的<i>Precision</i>、<i>Recall</i>和<i>F</i>值分别为0.82、0.85和0.83,均优于其他算法。</p>
                </div>
                <div class="p1">
                    <p id="109">各算法执行的平均时间如表1所示,可见本文提出的算法运行时间较快。</p>
                </div>
                <div class="area_img" id="110">
                    <p class="img_tit"><b>表1 算法执行平均时间</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note">s</p>
                    <table id="110" border="1"><tr><td><br />算法</td><td>时间</td></tr><tr><td><br />IT</td><td>2.52</td></tr><tr><td><br />MZ</td><td>2.07</td></tr><tr><td><br />GB</td><td>4.32</td></tr><tr><td><br />SR</td><td>1.63</td></tr><tr><td><br />AC</td><td>1.87</td></tr><tr><td><br />LC</td><td>2.14</td></tr><tr><td><br />FT</td><td>1.24</td></tr><tr><td><br />RC</td><td>1.98</td></tr><tr><td><br />IRC</td><td>1.51</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="111">本文系统的平均运行时间为10.62 s,其中生成三分图的平均时间为1.63 s。而正常的需要人工标记的闭形式图像抠取算法平均需要85.87 s<citation id="143" type="reference"><link href="37" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>。现有文献涉及自动抠图方法运行时间数据的比较少,例如,在文献<citation id="144" type="reference">[<a class="sup">20</a>]</citation>中自动抠图方法所用时间为12.56 s,所以本文方法运行速度较快。</p>
                </div>
                <h3 id="112" name="112" class="anchor-tag">4 结束语</h3>
                <div class="p1">
                    <p id="113">本文对基于区域对比度的显著性检测算法进行研究和改进。RC使用基于图的分割算法易出现分割区域不能很好地贴合物体边缘,将属于前景的区域归入背景。针对上述情况,本文使用优化的SLIC分割算法,得到均匀规则的超像素,使得显著图质量进一步提高,利于最终抠取精确的前景图,并在此基础上设计一个自动抠图系统。实验结果表明,本文算法的<i>Precision</i>、<i>Recall</i>和<i>F</i>值分别为0.82、0.85和0.83,比IT、MZ、GB、RC等经典算法精确。自动抠图系统能够较快地抠取显著目标并提供图片合成应用。下一步将研究一幅图像中存在多个显著区域的抠图问题。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="5">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Comprehensive method for removing from an image the background surrounding a selected subject">

                                <b>[1]</b> BERMAN A,VLAHOS P,DADOURIAN A.Comprehensive method for removing from an image the background surrounding a selected subject:US6134345[P].2000-10-17.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Bayesian approach to digital matting">

                                <b>[2]</b> CHUANG Y Y,CURLESS B,SALESIN D H,et al.A Bayesian approach to digital matting[C]//Proceedings of CVPR’01.Washington D.C.,USA:IEEE Press,2001:264-271.
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Alpha estimation in natural images">

                                <b>[3]</b> RUZON M A,TOMASI C.Alpha estimation in natural images[C]//Proceedings of CVPR’00.Washington D.C.,USA:IEEE Press,2000:18-25.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A closed-form solution to natural image matting">

                                <b>[4]</b> LEVIN A,LISCHINSKI D,WEISS Y.A closed-form solution to natural image matting[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2008,30(2):228-242.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Random walks for interactive alpha-matting">

                                <b>[5]</b> GRADY L,SCHIWIETZ T,AHARON S,et al.Random walks for interactive alpha-matting[C]//Proceedings of VIIP’05.Calgary,Canada:ACTA Press,2005:423-429.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SPQD&amp;filename=SPQD00003386430&amp;v=MTQzNDJPNEh0SFBySWREWU9nUFkzazV6QmRoNGo5OVNYcVJyeG94Y01IN1I3cWVidWR0RkMzbFZiL1BJMVk9TmozYWFy&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtJa2ZXcXc2ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b> JIANG Hao,XU Jie.Matching objects in multi-camera surveillance without geometric constraints[J].Journal of Convergence Information Technology,2010,5(6):79-86.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SPQD&amp;filename=SPQD120813007800&amp;v=MDQ2Nzk2SHRuTnJJOUZZK01QREJNOHp4VVNtRGQ5U0g3bjN4RTlmYnZuS3JpZlp1OXVGQ3ZnVTduSUoxMGNOajNhYXJL&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtJa2ZXcXc2ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b> ZHANG Qiaorong,ZHANG Yongqiang.Salient region detection in video using spatiotemporal visual attention model[J].International Journal of Digital Content Technology and its Applications,2012,6(11):35-47.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=STJD&amp;filename=STJDB33FD423CCD04951619D95D8F3C52D8F&amp;v=MzExMjZZSlFYcVdwR1EyQ3JlV01iTHBDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZDcGJRMzVOMWh3cmk0dzZBPU5qbkJhc0c3SGFlNHE0MUdGNWg3REhnd3loY1Y2eg==&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtJa2ZXcXc2ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b> JIAN Muwei,DONG Junyu,MA Jun.Image retrieval using wavelet-based salient regions[J].The Imaging Science Journal,2011,59(4):219-231.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201805040&amp;v=MDI1MzlLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUnJGeTdsVkx6QUx6N0JiYkc0SDluTXFvOUJaSVE=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtJa2ZXcXc2ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b> 高东东,张新生.基于空间卷积神经网络模型的图像显著性检测[J].计算机工程,2018,44(5):240-245.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_10" >
                                    <b>[10]</b>
                                 陈佳洲,曾碧,何元烈.一种应用于静态图像人体分割的显著性检测方法[J].小型微型计算机系统,2016,37(3):608-611.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Context-Aware Saliency Detection">

                                <b>[11]</b> GOFERMAN S,ZELNIK-MANOR L,TAL A.Context-aware saliency detection[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2012,34(10):1915-1926.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Visual Saliency Detection based on Bayesian Model">

                                <b>[12]</b> XIE Yunlin,LU Huchuan.Visual saliency detection based on Bayesian model[C]//Proceedings of the 18th IEEE International Conference on Image Processing.Washington D.C.,USA:IEEE Press,2011:645-648.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Saliency filters:Contrast based filtering for salient region detection">

                                <b>[13]</b> HORNUNG A,PRITCH Y,KRAHENBUHL P,et al.Saliency filters:contrast based filtering for salient region detection[C]//Proceedings of CVPR’12.Washington D.C.,USA:IEEE Press,2012:733-740.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Global Contrast Based Salient Region Detection">

                                <b>[14]</b> CHENG Mingming,MITRA N J,HUANG Xiaolei,et al.Global contrast based salient region detection[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2015,37(3):569-582.
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=SLIC superpixels">

                                <b>[15]</b> ACHANTA R,SHAJI A,SMITH K,et al.SLIC superpixels:EPFL 149300[R].Lausanne,Switzerland:EPFL,2010:4-7.
                            </a>
                        </p>
                        <p id="35">
                            <a id="bibliography_16" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSYJ201701062&amp;v=MTk2NzBSckZ5N2xWTHpBTHo3U1pMRzRIOWJNcm85RFpvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmU=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtJa2ZXcXc2ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[16]</b> 王淑敏,宫宁生,陈逸韬.加权的超像素级时空上下文目标跟踪[J].计算机应用研究,2016,34(1):270-274.
                            </a>
                        </p>
                        <p id="37">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Frequency-tuned salient region detection">

                                <b>[17]</b> ACHANTA R,HEMAMI S,ESTRADA F,et al.Frequency-tuned salient region detection[C]//Proceedings of CVPR’09.Washington D.C.,USA:IEEE Press,2009:1597-1604.
                            </a>
                        </p>
                        <p id="39">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Efficient graph-based image segmentation">

                                <b>[18]</b> FELZENSZWALB P F,HUTTENLOCHER D P.Efficient graph-based image segmentation[M].[S.l.]:Kluwer Academic Publishers,2004.
                            </a>
                        </p>
                        <p id="41">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Visual attention detection in video se-quences using spatiotemporal cues">

                                <b>[19]</b> ZHAI Yun,SHAH M.Visual attention detection in video sequences using spatiotemporal cues[C]//Proceedings of the 14th ACM International Conference on Multimedia.New York,USA:ACM Press,2006:815-824.
                            </a>
                        </p>
                        <p id="43">
                            <a id="bibliography_20" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCW&amp;filename=SJCWEE47232E43ADC87AA4097B60C145387E&amp;v=MDUxOTBFMGZiZVhUYjNxQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQ3BiUTM1TjFod3JpNHc2QT1OaWZJZWNiTkd0Yk9ySTB3WU9oK2VBOHh5R2RpN2o5MFR3M2tyRw==&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtJa2ZXcXc2ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[20]</b> SINGH S,JALAL A S.Automatic generation of trimap for image matting[J].International Journal of Machine Intelligence and Sensory Signal Processing,2014,1(3):232-250.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJC201909039" />
        <input id="dpi" type="hidden" value="600" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtJa2ZXcXc2ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201909039&amp;v=MTE2MjVMT2VaZVJyRnk3bFZMekFMejdCYmJHNEg5ak1wbzlHYllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtJa2ZXcXc2ND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
