<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637131287077957500%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJC201902034%26RESULT%3d1%26SIGN%3dstDNeFkHqZj%252fTWVQM7zNHvjvazQ%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201902034&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201902034&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201902034&amp;v=MDM0MjRiYkc0SDlqTXJZOUdZSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm5GeWpsV3IzS0x6N0I=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#75" data-title="0概述 ">0概述</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#79" data-title="1 研究现状 ">1 研究现状</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#83" data-title="2 DC-GAN模型 ">2 DC-GAN模型</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#86" data-title="3 MFF-GAN模型 ">3 MFF-GAN模型</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#89" data-title="3.1 判别器训练">3.1 判别器训练</a></li>
                                                <li><a href="#93" data-title="3.2 生成器训练">3.2 生成器训练</a></li>
                                                <li><a href="#100" data-title="3.3 图像特征提取">3.3 图像特征提取</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#103" data-title="4 实验结果与分析 ">4 实验结果与分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#104" data-title="4.1 实验数据集介绍">4.1 实验数据集介绍</a></li>
                                                <li><a href="#106" data-title="4.2 模型参数设置">4.2 模型参数设置</a></li>
                                                <li><a href="#108" data-title="4.3 生成图像分析">4.3 生成图像分析</a></li>
                                                <li><a href="#111" data-title="4.4 与其他方法的对比">4.4 与其他方法的对比</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#126" data-title="5 结束语 ">5 结束语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#88" data-title="图1 MFF-GAN模型框图">图1 MFF-GAN模型框图</a></li>
                                                <li><a href="#102" data-title="图2 多特征图融合GAN网络特征提取框图">图2 多特征图融合GAN网络特征提取框图</a></li>
                                                <li><a href="#110" data-title="图3 DC-GAN和MFF-GAN模型生成图像">图3 DC-GAN和MFF-GAN模型生成图像</a></li>
                                                <li><a href="#124" data-title="表1 不同特征的多样性得分结果对比">表1 不同特征的多样性得分结果对比</a></li>
                                                <li><a href="#125" data-title="图4 NUS-WIDE plant图像集视觉总结结果">图4 NUS-WIDE plant图像集视觉总结结果</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="39">


                                    <a id="bibliography_1" title="SADEGHI F, TENA J R, FARHADI A, et al.Learning to select and order vacation photographs[C]//Proceedings of IEEE Winter Conference on Applications of Computer Vision.Washington D.C., USA:IEEE Press, 2015:510-517." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning to select and order vacation photographs">
                                        <b>[1]</b>
                                        SADEGHI F, TENA J R, FARHADI A, et al.Learning to select and order vacation photographs[C]//Proceedings of IEEE Winter Conference on Applications of Computer Vision.Washington D.C., USA:IEEE Press, 2015:510-517.
                                    </a>
                                </li>
                                <li id="41">


                                    <a id="bibliography_2" title="SIMON I, SNAVELY N, SEITZ S M.Scene summarization for online image collections[C]//Proceedings of the 11th International Conference on Computer Vision.Washington D.C., USA:IEEE Press, 2007:1-8." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Scene Summarization for OnlineImage Collections">
                                        <b>[2]</b>
                                        SIMON I, SNAVELY N, SEITZ S M.Scene summarization for online image collections[C]//Proceedings of the 11th International Conference on Computer Vision.Washington D.C., USA:IEEE Press, 2007:1-8.
                                    </a>
                                </li>
                                <li id="43">


                                    <a id="bibliography_3" title="LOWE D G.Distinctive image features from scaleinvariant keypoints[J].International Journal of Computer Vision, 2004, 60 (2) :91-110." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002830901&amp;v=MzIxNDhZM2s1ekJkaDRqOTlTWHFScnhveGNNSDdSN3FlYnVkdEZDSGxVNy9CSWx3PU5qN0Jhck80SHRIT3A0eEZiZXNP&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                        LOWE D G.Distinctive image features from scaleinvariant keypoints[J].International Journal of Computer Vision, 2004, 60 (2) :91-110.
                                    </a>
                                </li>
                                <li id="45">


                                    <a id="bibliography_4" title="TAN L, SONG Y, LIU S, et al.ImageHive:interactive content-aware image summarization[J].IEEE Computer Graphics and Applications, 2011, 32 (1) :46-55." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=ImageHive: Interactive content-aware image summarization">
                                        <b>[4]</b>
                                        TAN L, SONG Y, LIU S, et al.ImageHive:interactive content-aware image summarization[J].IEEE Computer Graphics and Applications, 2011, 32 (1) :46-55.
                                    </a>
                                </li>
                                <li id="47">


                                    <a id="bibliography_5" title="YANG C, SHEN J, FAN J.Effective summarization of large-scale web images[C]//Proceedings of the 19th ACM International Conference on Multimedia.New York, USA:ACM Press, 2011:1145-1148." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Effective summarization of large-scale web images">
                                        <b>[5]</b>
                                        YANG C, SHEN J, FAN J.Effective summarization of large-scale web images[C]//Proceedings of the 19th ACM International Conference on Multimedia.New York, USA:ACM Press, 2011:1145-1148.
                                    </a>
                                </li>
                                <li id="49">


                                    <a id="bibliography_6" title="YANG C, SHEN J, PENG J, et al.Image collection summarization via dictionary learning for sparse representation[J].Pattern Recognition, 2013, 46 (3) :948-961." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14010600107400&amp;v=MDMyMzNSZEdlcnFRVE1ud1plWnVIeWptVUxiSUlGNGRhQkE9TmlmT2ZiSzhIdERNcVk5Rlplc0lDSHc1b0JNVDZUNFBRSC9pcg==&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                        YANG C, SHEN J, PENG J, et al.Image collection summarization via dictionary learning for sparse representation[J].Pattern Recognition, 2013, 46 (3) :948-961.
                                    </a>
                                </li>
                                <li id="51">


                                    <a id="bibliography_7" title="ZHAO Y, HONG R C, JIANG J G.Visual summarization of image collections by fast RANSAC[J].Neurocomputing, 2016, 172 (C) :48-52." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES1EA15428C48CE7BA98B40E02C35AAA35&amp;v=MDMzMTJOaWZPZmJMTmI5REpxNDFORis4SGZ3ayt2V2NhNGsxNVNBcmlybUUyZk1QbE5MbWFDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZDcGJRMzVORmh4TGkyd3FvPQ==&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                        ZHAO Y, HONG R C, JIANG J G.Visual summarization of image collections by fast RANSAC[J].Neurocomputing, 2016, 172 (C) :48-52.
                                    </a>
                                </li>
                                <li id="53">


                                    <a id="bibliography_8" title="赵烨, 蒋建国, 洪日昌.基于RANSAC的SIFT匹配优化[J].光电工程, 2014, 41 (8) :58-65." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GDGC201408011&amp;v=MjQxNDI4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm5GeWpsV3IzS0lpbk1iYkc0SDlYTXA0OUVaWVFLREg=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                        赵烨, 蒋建国, 洪日昌.基于RANSAC的SIFT匹配优化[J].光电工程, 2014, 41 (8) :58-65.
                                    </a>
                                </li>
                                <li id="55">


                                    <a id="bibliography_9" title="SIMONYAN K, ZISSERMAN A.Very deep convolutional networks for large-scale image recognition[EB/OL].[2017-12-13].https://arxiv.org/abs/1409.1556." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Very Deep Convolutional Networks for Large-Scale Image Recognition">
                                        <b>[9]</b>
                                        SIMONYAN K, ZISSERMAN A.Very deep convolutional networks for large-scale image recognition[EB/OL].[2017-12-13].https://arxiv.org/abs/1409.1556.
                                    </a>
                                </li>
                                <li id="57">


                                    <a id="bibliography_10" title="KRIZHEVSKY A, SUTSKEVER I, HINTON G E.ImageNet classification with deep convolutional neural networks[C]//Proceedings of the 25th International Conference on Neural Information Processing Systems.New York, USA:ACM Press, 2012:1097-1105." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=ImageNet classification with deep convolutional neural networks">
                                        <b>[10]</b>
                                        KRIZHEVSKY A, SUTSKEVER I, HINTON G E.ImageNet classification with deep convolutional neural networks[C]//Proceedings of the 25th International Conference on Neural Information Processing Systems.New York, USA:ACM Press, 2012:1097-1105.
                                    </a>
                                </li>
                                <li id="59">


                                    <a id="bibliography_11" title="SHEN X, TIAN X.Multi-modal and multi-scale photo collection summarization[J].Multimedia Tools and Applications, 2016, 75 (5) :1-15." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multi-modal and multi-scale photo collection summarization">
                                        <b>[11]</b>
                                        SHEN X, TIAN X.Multi-modal and multi-scale photo collection summarization[J].Multimedia Tools and Applications, 2016, 75 (5) :1-15.
                                    </a>
                                </li>
                                <li id="61">


                                    <a id="bibliography_12" title="李志明.基于卷积神经网络的虹膜活体检测算法研究[J].计算机工程, 2016, 42 (5) :239-243, 248." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201605042&amp;v=MDkwMjg2ajU0TzN6cXFCdEdGckNVUkxPZVplUm5GeWpsV3IzS0x6N0JiYkc0SDlmTXFvOUJab1FLREg4NHZSNFQ=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[12]</b>
                                        李志明.基于卷积神经网络的虹膜活体检测算法研究[J].计算机工程, 2016, 42 (5) :239-243, 248.
                                    </a>
                                </li>
                                <li id="63">


                                    <a id="bibliography_13" title="GOODFELLOW I J, POUGETABADIE J, MIRZA M, et al.Generative adversarial nets[EB/OL].[2017-12-13].http://blog.csdn.net/wspba/article/details/54582391." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Generative adversarial nets">
                                        <b>[13]</b>
                                        GOODFELLOW I J, POUGETABADIE J, MIRZA M, et al.Generative adversarial nets[EB/OL].[2017-12-13].http://blog.csdn.net/wspba/article/details/54582391.
                                    </a>
                                </li>
                                <li id="65">


                                    <a id="bibliography_14" title="RADFORD A, METZ L, CHINTALA S.Unsupervised representation learning with deep convolutional generative adversarial networks[EB/OL].[2017-12-13].https://arxiv.org/abs/1511.06434." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Unsupervised representation learning with deep convolutional generative adversarial networks">
                                        <b>[14]</b>
                                        RADFORD A, METZ L, CHINTALA S.Unsupervised representation learning with deep convolutional generative adversarial networks[EB/OL].[2017-12-13].https://arxiv.org/abs/1511.06434.
                                    </a>
                                </li>
                                <li id="67">


                                    <a id="bibliography_15" title="LIN D, FU K, WANG Y, et al.MARTA GANs:unsupervised representation learning for remote sensing image classification[J].IEEE Geoscience and Remote Sensing Letters, 2017, 14 (11) :2092-2096." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=MARTA GANs:Unsupervised Representation Learning for Remote Sensing Image Classification">
                                        <b>[15]</b>
                                        LIN D, FU K, WANG Y, et al.MARTA GANs:unsupervised representation learning for remote sensing image classification[J].IEEE Geoscience and Remote Sensing Letters, 2017, 14 (11) :2092-2096.
                                    </a>
                                </li>
                                <li id="69">


                                    <a id="bibliography_16" title="CHUA T S, TANG J, HONG R, et al.NUS-WIDE:a real-world web image database from National University of Singapore[C]//Proceedings of ACM International Conference on Image and Video Retrieval.New York, USA:ACM Press, 2009:48-50." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=NUS-WIDE:A Real-world Web Image Database from National University of Singapore">
                                        <b>[16]</b>
                                        CHUA T S, TANG J, HONG R, et al.NUS-WIDE:a real-world web image database from National University of Singapore[C]//Proceedings of ACM International Conference on Image and Video Retrieval.New York, USA:ACM Press, 2009:48-50.
                                    </a>
                                </li>
                                <li id="71">


                                    <a id="bibliography_17" title="CAMARGO J E, GONZALEZ F A.Multimodal latent topic analysis for image collection summarization[J].Information Sciences, 2016, 328 (C) :270-287." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES6DEF9BD1EBD55091E6F5042AE83975D8&amp;v=MjI4ODdHUWxmQ3BiUTM1TkZoeExpMndxbz1OaWZPZmJYTWE2ZkYzZnRFRVpsN0NYazV4aGRtN0VsNFNIdmczV2M5ZXJ1VFFNNlhDT052RlNpV1dyN0pJRnBtYUJ1SFlmTw==&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[17]</b>
                                        CAMARGO J E, GONZALEZ F A.Multimodal latent topic analysis for image collection summarization[J].Information Sciences, 2016, 328 (C) :270-287.
                                    </a>
                                </li>
                                <li id="73">


                                    <a id="bibliography_18" title="DING C H Q, LI T, JORDAN M I.Convex and seminonnegative matrix factorizations[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2010, 32 (1) :45-55." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Convex and semi-nonnegative matrix factorizations">
                                        <b>[18]</b>
                                        DING C H Q, LI T, JORDAN M I.Convex and seminonnegative matrix factorizations[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2010, 32 (1) :45-55.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJC" target="_blank">计算机工程</a>
                2019,45(02),202-206 DOI:10.19678/j.issn.1000-3428.0050237            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm">基于MFF-GAN的图像集视觉总结</span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BC%A0%E6%96%87%E5%87%AF&amp;code=38708323&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">张文凯</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%AD%99%E7%9A%93&amp;code=24633254&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">孙皓</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%AD%99%E6%98%BE&amp;code=21920710&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">孙显</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E5%AE%8F%E7%90%A6&amp;code=09577136&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王宏琦</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E7%A7%91%E5%AD%A6%E9%99%A2%E7%94%B5%E5%AD%90%E5%AD%A6%E7%A0%94%E7%A9%B6%E6%89%80&amp;code=0227399&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国科学院电子学研究所</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E7%A7%91%E5%AD%A6%E9%99%A2%E7%A9%BA%E9%97%B4%E4%BF%A1%E6%81%AF%E5%A4%84%E7%90%86%E4%B8%8E%E5%BA%94%E7%94%A8%E7%B3%BB%E7%BB%9F%E6%8A%80%E6%9C%AF%E9%87%8D%E7%82%B9%E5%AE%9E%E9%AA%8C%E5%AE%A4&amp;code=1039195&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国科学院空间信息处理与应用系统技术重点实验室</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E7%A7%91%E5%AD%A6%E9%99%A2%E5%A4%A7%E5%AD%A6&amp;code=1698842&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国科学院大学</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>现有图像集视觉总结方法主要使用浅层视觉特征, 或者直接应用已训练的卷积神经网络模型提取图像深层特征, 选取的图像不具代表性。为此, 分析并研究图像集视觉总结的图像特征表示方法, 提出多特征图融合生成对抗网络 (MFF-GAN) 模型。该模型中的判别器通过多特征图融合的方式提取图像特征, 使提取的特征能表示图像细节和高层语义信息, 并在多特征图融合层后添加自编码网络对特征进行降维, 避免特征维度灾难问题。NUS-WIDE数据集上的实验结果验证了M FF-GAN模型的有效性, 并表明其能有效提升图像集视觉总结多样性。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">生成对抗网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%89%B9%E5%BE%81%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">特征学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%A7%86%E8%A7%89%E6%80%BB%E7%BB%93&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">视觉总结;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%A4%9A%E7%89%B9%E5%BE%81%E5%9B%BE%E8%9E%8D%E5%90%88&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">多特征图融合;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%87%AA%E7%BC%96%E7%A0%81%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">自编码网络;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    张文凯 (1990—) , 男, 博士研究生, 主研方向为计算机视觉、图像处理;E-mail: zhang. wenkai@ outlook. com
;
                                </span>
                                <span>
                                    孙皓、副研究员、博士;
;
                                </span>
                                <span>
                                    孙显, 副研究员、博士;
;
                                </span>
                                <span>
                                    王宏琦, 研究员、博士。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-01-23</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金 (41501485);</span>
                    </p>
            </div>
                    <h1>Image Set Visual Summarization Based on MFF-GAN</h1>
                    <h2>
                    <span>ZHANG Wenkai</span>
                    <span>SUN Hao</span>
                    <span>SUN Xian</span>
                    <span>WANG Hongqi</span>
            </h2>
                    <h2>
                    <span>Institute of Electronics, Chinese Academy of Sciences</span>
                    <span>Key Laboratory of Spatial Information Processing and Application System Technology, Chinese Academy of Sciences</span>
                    <span>University of Chinese Academy of Sciences</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Existing image set visual summarization methods primarily consider the low-level visual features of images, or deep features, which extracted from trained Convolutional Neural Network (CNN) model. It makes the selected image not representative. In order to solve the problem, this paper analyzes and studies the image feature representation method in the image set visual summarization, proposes a Multi-Feature Fusion Generative Adversarial Networks (MFF-GAN) model. The discriminator in the model extracts image features by means of multi-feature image fusion, so that the extracted features can represent image details and high-level semantic information. To reduce the dimensionality of feature, the encoder network is added after the fusion layer. Experimental results on NUS-WIDE dataset valify the effectiveness of the MFF-GAN model, and show it can improve the diversity of visual summarization.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Generative%20Adversarial%20Network%20(GAN)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Generative Adversarial Network (GAN) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=feature%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">feature learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=visual%20summarization&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">visual summarization;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=multi-feature%20fusion&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">multi-feature fusion;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=autoencoder%20network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">autoencoder network;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-01-23</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="75" name="75" class="anchor-tag">0概述</h3>
                <div class="p1">
                    <p id="76">随着照片分享网站上图像数量的爆炸性增长, 用户从网站上快速有效地找到感兴趣的图像变得越来越困难, 图像集视觉总结可以帮助用户快速找到感兴趣的图像。图像集视觉总结是从大量图像集中选择少量具有代表性的图像表示原始图像集。图像集视觉总结不仅有助于组织、管理大图像集<citation id="132" type="reference"><link href="39" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>, 而且可以提升照片搜索结果的准确性。</p>
                </div>
                <div class="p1">
                    <p id="77">近年来, 国内外许多专家学者致力于提升图像集视觉总结效果。现有研究使用低层视觉特征 (Color、SIFT、BOW等) 表示图像, 提取特征时需要根据不同的场景或目标进行设计, 特征提取费时费力, 或者直接使用训练好的卷积神经网络 (Convolutional Neural Network, CNN) 模型提取特征, 不能有效地适应各种图像集视觉总结数据集。</p>
                </div>
                <div class="p1">
                    <p id="78">本文分析并研究了图像集视觉总结中的图像特征表示, 基于深度卷积生成对抗网络模型, 提出多特征图融合生成对抗网络 (Multi-Feature Fusion Generative Adversarial Network, MFF-GAN) 模型。该模型能够在图像集视觉总结数据集中通过无监督训练提取适应于图像内容的特征。此外, 通过多特征图融合的方式提取的图像浅层和深层特征能准确表示图像。</p>
                </div>
                <h3 id="79" name="79" class="anchor-tag">1 研究现状</h3>
                <div class="p1">
                    <p id="80">由于图像集视觉总结具有重要的应用价值, 因此研究者提出了很多方法来提升视觉总结效果。文献<citation id="133" type="reference">[<a class="sup">2</a>]</citation>使用SIFT特征<citation id="134" type="reference"><link href="43" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>表示图像, 然后使用贪心聚类算法选择权威视角构建场景总结。文献<citation id="135" type="reference">[<a class="sup">4</a>]</citation>使用颜色直方图特征、边缘特征和SIFT特征表示图像, 然后利用K-Means对图像聚类, 最后对属于同一聚类中心的图像使用插入式算法生成图像集视觉摘要。文献<citation id="136" type="reference">[<a class="sup">5</a>,<a class="sup">6</a>]</citation>使用BOW特征表示图像, 然后将图像特征输入字典学习算法中, 产生的字典就是图像集总结结果。文献<citation id="137" type="reference">[<a class="sup">7</a>,<a class="sup">8</a>]</citation>使用SIFT特征表示图像, 然后利用关系扩散聚类算法进行聚类, 生成图像集视觉总结结果。上述方法仅使用SIFT、BOW等低层视觉特征, 这些特征需要根据不同的场景或目标进行设计, 而图像集视觉总结场景复杂、目标多样, 使用低层视觉特征表示费时费力, 特征提取困难, 不利于图像集视觉总结。</p>
                </div>
                <div class="p1">
                    <p id="81">深度卷积神经网络<citation id="142" type="reference"><link href="55" rel="bibliography" /><link href="57" rel="bibliography" /><sup>[<a class="sup">9</a>,<a class="sup">10</a>]</sup></citation>在计算机视觉领域取得了较好的效果, 并被其应用到图像集视觉总结<citation id="139" type="reference"><link href="59" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>。文献<citation id="140" type="reference">[<a class="sup">12</a>]</citation>利用卷积神经网络提取虹膜图像特征, 提升了虹膜图像的检测正确率。但是训练该网络需要大量的标注数据, 而图像集总结是一个无监督视觉分类过程, 直接使用训练好的卷积神经网络模型提取特征, 不能有效适应各种总结数据集。生成对抗网络<citation id="141" type="reference"><link href="63" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>是无监督学习算法中近期研究的热点, 在此基础上提出的深度卷积生成对抗网络 (DC-GAN) 模型<citation id="138" type="reference"><link href="65" rel="bibliography" /><link href="67" rel="bibliography" /><sup>[<a class="sup">14</a>,<a class="sup">15</a>]</sup></citation>在各种图像分析任务中均取得较好的结果, 展示了深度卷积生成对抗网络的强大特征学习能力, 但是其提取的特征维度较大, 容易造成维度灾难, 不利于图像集视觉总结的后期处理。</p>
                </div>
                <div class="p1">
                    <p id="82">针对以上问题, 本文提出MFF-GAN模型。该模型基于DC-GAN, 为了更好地适应图像集视觉总结, 对网络进行如下优化:1) 为获得图像的细节信息和高层语义信息, 改进判别器, 添加多特征图融合层, 利用多特征图融合方式提取图像浅层和深层特征;2) 为提升图像集总结效率, 避免维数灾难, 在多特征图融合层后面添加自编码网络, 对特征进行降维;3) 为更好地训练生成模型和判别模型, 改进生成模型损失函数, 不仅考虑图像分类误差, 而且考虑生成样本和真实图像特征之间的L<sub>1</sub>损失误差, 将2个误差相结合作为生成模型损失函数。</p>
                </div>
                <h3 id="83" name="83" class="anchor-tag">2 DC-GAN模型</h3>
                <div class="p1">
                    <p id="84">生成对抗网络模型由生成模型和判别模型组成, 其主要思想是生成模型和判别模型相互对抗学习, 生成模型生成与真实图像类似的样本图像, 判别模型分类样本图像和真实图像。生成模型的函数表达为G (z;θ<sub>g</sub>) , 参数θ<sub>g</sub>表示其在真实数据x中学习参数, 使得噪声z经过G (z;θ<sub>g</sub>) 后生成的样本图像的分布与x的分布一致。判别模型的函数表达为D (x;θ<sub>d</sub>) , 其输出为一个样本来自于真实数据的概率, 如果输入数据来自于真实的训练数据, 则判别模型输出的概率值大, 否则输出的概率值小。生成模型不断提供生成数据, 判别模型不断学习, 提升分类能力, 进而能够提取适应于数据集的图像特征。</p>
                </div>
                <div class="p1">
                    <p id="85">文献<citation id="143" type="reference">[<a class="sup">14</a>]</citation>提出的DC-GAN模型将卷积神经网络和生成对抗网络相结合, 具有以下特点:1) 在判别模型中使用带步长的卷积取代传统卷积神经网络中的空间池化, 容许网络学习自身的空间下采样, 对于生成模型使用反卷积, 容许其学习自身的空间上采样;2) 除了生成模型的输出层和判别模型的输入层, 在网络其他层上都使用批标准化, 解决初始化引起的训练问题, 使得梯度能传播到更深层次;3) 去除全连接层, 直接使用卷积层连接生成模型和判别模型的输入层及输出层;4) 生成模型中输出层用tanh激活函数, 其他层使用Re LU激活函数, 判别模型中所有层使用LeakyReLU函数。</p>
                </div>
                <h3 id="86" name="86" class="anchor-tag">3 MFF-GAN模型</h3>
                <div class="p1">
                    <p id="87">图1展示了多特征图融合生成对抗网络模型框图, 其中100z表示100维随机噪声。生成器用来生成样本图像, 判别器用来分类真实图像和样本图像。在判别器中, 添加多特征图融合层, 利用多特征图融合方式提取图像浅层和深层特征, 在多特征图融合层后添加自编码网络, 对特征降维, 提升图像集总结效率。训练MFF-GAN模型时, 使用分类损失对样本图像和真实图像特征之间的L<sub>1</sub>损失误差进行联合训练, 提升生成模型的生成样本准确性。</p>
                </div>
                <div class="area_img" id="88">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201902034_08800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 MFF-GAN模型框图" src="Detail/GetImg?filename=images/JSJC201902034_08800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 MFF-GAN模型框图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201902034_08800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="89" name="89">3.1 判别器训练</h4>
                <div class="p1">
                    <p id="90">在训练判别器时, 生成器的权值参数固定。对于每一幅真实图像x最大化D (x) , 对于每一个样本图像G (z) 最小化D (G (z) ) , 因此在训练判别时的目标函数可表示为:</p>
                </div>
                <div class="area_img" id="91">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJC201902034_09100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="92">其中, p<sub>data</sub> (x) 表示数据x服从p<sub>data</sub>分布, p<sub>g</sub>表示生成模型的分布, p<sub>g</sub> (z) 表示z服从p<sub>g</sub>分布。</p>
                </div>
                <h4 class="anchor-tag" id="93" name="93">3.2 生成器训练</h4>
                <div class="p1">
                    <p id="94">在训练生成器时, 判别器的权值参数固定。为使生成器生成的样本图像更接近真实图像, 考虑2个损失函数:1) 判别器损失函数, 即判别器判断输入图像是真实图像还是样本图像的准确性;2) 样本图像和真实图像的特征匹配L<sub>1</sub>损失, 即真实图像和样本图像特征之间的绝对值误差。对于判别器损失函数, 需要最大化D (G (z) ) , 因为D (G (z) ) 输出的概率值为[0, 1], 其等价于最小化ln (1-D (G (z) ) ) , 所以判别器的目标分类损失函数为:</p>
                </div>
                <div class="area_img" id="95">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJC201902034_09500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="96">对于特征匹配损失, 为了使得生成器提取的特征更能表示图像, 不仅要使判别器能够分辨出图像是样本图像还是真实图像, 而且要使生成图像的特征和真实图像的特征类似。本文使用f (·) 表示特征提取函数, 利用L<sub>1</sub>范数衡量特征相似性, 损失函数为:</p>
                </div>
                <div class="area_img" id="97">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJC201902034_09700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="98">在训练生成器时, 目标函数可以表示为:</p>
                </div>
                <div class="area_img" id="99">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJC201902034_09900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <h4 class="anchor-tag" id="100" name="100">3.3 图像特征提取</h4>
                <div class="p1">
                    <p id="101">对于生成器, 首先输入100维的随机噪声, 噪声满足[-1, 1]的均匀分布。然后依次经过6次反卷积, 反卷积中滤波器大小为4×4, 步长为2, 每层特征图个数分别为512、256、128、64、32、16。最后生成大小为256×256、通道数为3的样本图像。具体网络框架如图2 (a) 所示。对于判别器, 首先输入为256×256的3通道图像, 其次经过6次卷积, 生成512个4×4的特征图, 卷积滤波器的大小为5×5, 步长为2, 每层的特征图个数分别为16、32、64、128、256、512。然后分别对第3层～第6层进行16×16、8×8、4×4、2×2的最大值池化, 具体如图2 (b) 所示。将这些特征图与第7层进行合并, 生成992个4×4特征图, 使特征图拉伸为15 872维的向量, 使用全连接自编码网络将特征降至1 000维。最后将1 000维特征输入到sigmoid函数和L<sub>1</sub>损失函数2个分支。输入到sigmoid函数分支中产生类别标签, 用于计算分类损失, 输入到L<sub>1</sub>特征损失函数分支中, 用于计算样本图像和真实图像之间的特征损失。在提取图像特征时, 将图像输入至判别器, 最后得到的1 000维网络输出值就是图像的特征表示。</p>
                </div>
                <div class="area_img" id="102">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201902034_10200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 多特征图融合GAN网络特征提取框图" src="Detail/GetImg?filename=images/JSJC201902034_10200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 多特征图融合GAN网络特征提取框图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201902034_10200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h3 id="103" name="103" class="anchor-tag">4 实验结果与分析</h3>
                <h4 class="anchor-tag" id="104" name="104">4.1 实验数据集介绍</h4>
                <div class="p1">
                    <p id="105">实验采用NUS-WID<citation id="144" type="reference"><link href="69" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>作为图像集总结的数据集。NUS-WIDE数据集包含269 648幅图像, 每幅图像有相应的标注, 覆盖81个概念。本文选择NUS-WIDE数据集中4个概念的数据, 分别为beach (7 802) 、flower (9 804) 、plant (3 712) 、mountain (6 988) , 共计28 306幅图像。</p>
                </div>
                <h4 class="anchor-tag" id="106" name="106">4.2 模型参数设置</h4>
                <div class="p1">
                    <p id="107">本文在NUS-WIDE的4个数据集中, 使用随机梯度下降 (Stochastic Gradient Descent, SGD) 方法对模型进行训练, mini-batch大小为64, 迭代次数设置为300次。所有权值使用以均值为0、方差为0.02的正态分布进行初始化。本文设置LeaklyRelu激活函数参数为0.2, 使用Adam优化器进行优化, 学习率设置为0.000 2, beta1设置为0.5。每训练1次判别模型, 需训练2次生成模型。</p>
                </div>
                <h4 class="anchor-tag" id="108" name="108">4.3 生成图像分析</h4>
                <div class="p1">
                    <p id="109">为衡量本文模型的特征表示能力, 展示其在NUS-WIDE plant数据集中生成器生成的样本图像。图3给出一些示例图像, 样本图像越接近真实图像说明生成器性能越好, 判别器提取的特征也更好。由图3可知, 本文模型的生成器优于DC-GAN模型, 能够生成数据集内不同颜色的花朵, 同时也能生成复杂的场景, 如瀑布等, 从而证明本文提出的判别器能够较好地提取图像特征。</p>
                </div>
                <div class="area_img" id="110">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201902034_11000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 DC-GAN和MFF-GAN模型生成图像" src="Detail/GetImg?filename=images/JSJC201902034_11000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 DC-GAN和MFF-GAN模型生成图像  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201902034_11000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="111" name="111">4.4 与其他方法的对比</h4>
                <div class="p1">
                    <p id="128">为客观评价本文模型提取特征的有效性, 将其与BOW<citation id="146" type="reference"><link href="71" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>、VGG<citation id="147" type="reference"><link href="57" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>、DC-GAN等特征进行对比。对于每幅图像, 使用1 000维特征向量表示。对于VGG特征, 直接使用在ImageNet中训练好的模型提取图像特征, 将第7层4 096维的激活值作为图像特征, 然后使用PCA将特征维度从4 096维降至1 000维。对于DC-GAN特征, 使用判别器最后一层512×4×4作为图像特征, 然后应用PCA将特征维度从8 192维降至1 000维。对于所有特征, 使用最大最小值归一化方法将其归一化至[0, 1], 然后再输入图像集视觉总结算法。对于每个图像集, 可以使用特征矩阵<image id="129" type="formula" href="images/JSJC201902034_12900.jpg" display="inline" placement="inline"><alt></alt></image>表示, 其中, m表示特征维度, n表示图像集中的图像个数。</p>
                </div>
                <div class="p1">
                    <p id="114">在提取特征后, 使用Convex-NMF<citation id="145" type="reference"><link href="73" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>进行图像集视觉总结, 其目标函数为:</p>
                </div>
                <div class="area_img" id="115">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJC201902034_11500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="130">其中, <image id="131" type="formula" href="images/JSJC201902034_13100.jpg" display="inline" placement="inline"><alt></alt></image>为非负矩阵, n为图像个数, k为视觉主题个数。W中的每一列为视觉主题, 元素值表示每一幅图像对于该视觉主题的重要性, 权值越大, 说明图像对该视觉主题越重要。本文设置视觉主题个数k为30, 对于每个主题, 选择前5幅图像, 使用迭代求解算法<citation id="148" type="reference"><link href="71" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>求解式 (5) 。</p>
                </div>
                <div class="p1">
                    <p id="119">为客观评价视觉总结结果, 本文使用多样性评价标准进行衡量, 其定义如下:</p>
                </div>
                <div class="area_img" id="120">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJC201902034_12000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="121">其中, P、Q表示不同视觉主题中的图像, p<sub>i</sub>表示p图像的第i维特征。对于预测结果, 使用K表示每个视觉主题选择的图像个数, N表示主题个数, 多样性得分具体如下:</p>
                </div>
                <div class="area_img" id="122">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJC201902034_12200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="123">表1展示了本文模型与其他模型提取特征的多样性得分结果。由此可知, 使用MFF-GAN模型提取的特征进行图像集视觉总结, 多样性得分较高, 说明总结出的图像不相似, 包含图像集中内容比较丰富, 能够覆盖图像集中较多的内容。图4为使用本文模型提取特征进行视觉总结的结果。通过图像集总结可以代表性地展示flower数据集。例如图4 (a) 展示的是蝴蝶与花同时出现的图像, 图4 (b) 展示的是树林下与花相关的场景, 图4 (c) 展示的是排列紧密、形状类似但归属不同品种的花簇, 图4 (d) 展示的是花海, 图4 (e) 展示的是荷花。由此可知, 本文模型提取的特征能够有效表示不同场景 (花海) 和目标 (荷花) , 适用于图像集视觉总结。</p>
                </div>
                <div class="area_img" id="124">
                                            <p class="img_tit">
                                                表1 不同特征的多样性得分结果对比
                                                
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201902034_12400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/JSJC201902034_12400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201902034_12400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表1 不同特征的多样性得分结果对比" src="Detail/GetImg?filename=images/JSJC201902034_12400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>
                <div class="area_img" id="125">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201902034_12500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 NUS-WIDE plant图像集视觉总结结果" src="Detail/GetImg?filename=images/JSJC201902034_12500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 NUS-WIDE plant图像集视觉总结结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201902034_12500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h3 id="126" name="126" class="anchor-tag">5 结束语</h3>
                <div class="p1">
                    <p id="127">本文针对图像集视觉总结中的特征提取, 提出一个无监督特征学习模型MFF-GAN。该模型能够在需要总结的图像集中进行无监督训练, 解决了使用现有深度卷积神经网络提取特征时无法在无标注数据集中进行训练的问题。此外, 本文提出的多特征图融合方式考虑了低层和高层语义特征, 使得提取出的特征能够表示图像细节和高层语义信息, 并通过特征匹配方式训练模型, 提升生成器和判别器性能。在NUS-WIDE数据集中的实验结果验证了本文模型的有效性。然而, 本文仅对生成对抗网络判别器进行改进, 提高了判别器特征提取能力, 下一步将研究生成器网络设计, 提升生成器和判别器的综合性能。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="39">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning to select and order vacation photographs">

                                <b>[1]</b>SADEGHI F, TENA J R, FARHADI A, et al.Learning to select and order vacation photographs[C]//Proceedings of IEEE Winter Conference on Applications of Computer Vision.Washington D.C., USA:IEEE Press, 2015:510-517.
                            </a>
                        </p>
                        <p id="41">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Scene Summarization for OnlineImage Collections">

                                <b>[2]</b>SIMON I, SNAVELY N, SEITZ S M.Scene summarization for online image collections[C]//Proceedings of the 11th International Conference on Computer Vision.Washington D.C., USA:IEEE Press, 2007:1-8.
                            </a>
                        </p>
                        <p id="43">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002830901&amp;v=MjIzNjVqOTlTWHFScnhveGNNSDdSN3FlYnVkdEZDSGxVNy9CSWx3PU5qN0Jhck80SHRIT3A0eEZiZXNPWTNrNXpCZGg0&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b>LOWE D G.Distinctive image features from scaleinvariant keypoints[J].International Journal of Computer Vision, 2004, 60 (2) :91-110.
                            </a>
                        </p>
                        <p id="45">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=ImageHive: Interactive content-aware image summarization">

                                <b>[4]</b>TAN L, SONG Y, LIU S, et al.ImageHive:interactive content-aware image summarization[J].IEEE Computer Graphics and Applications, 2011, 32 (1) :46-55.
                            </a>
                        </p>
                        <p id="47">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Effective summarization of large-scale web images">

                                <b>[5]</b>YANG C, SHEN J, FAN J.Effective summarization of large-scale web images[C]//Proceedings of the 19th ACM International Conference on Multimedia.New York, USA:ACM Press, 2011:1145-1148.
                            </a>
                        </p>
                        <p id="49">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14010600107400&amp;v=MTE3ODRDSHc1b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VTGJJSUY0ZGFCQT1OaWZPZmJLOEh0RE1xWTlGWmVzSQ==&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b>YANG C, SHEN J, PENG J, et al.Image collection summarization via dictionary learning for sparse representation[J].Pattern Recognition, 2013, 46 (3) :948-961.
                            </a>
                        </p>
                        <p id="51">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES1EA15428C48CE7BA98B40E02C35AAA35&amp;v=MjU0MjIxNVNBcmlybUUyZk1QbE5MbWFDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZDcGJRMzVORmh4TGkyd3FvPU5pZk9mYkxOYjlESnE0MU5GKzhIZndrK3ZXY2E0aw==&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b>ZHAO Y, HONG R C, JIANG J G.Visual summarization of image collections by fast RANSAC[J].Neurocomputing, 2016, 172 (C) :48-52.
                            </a>
                        </p>
                        <p id="53">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GDGC201408011&amp;v=MjIyMzR6cXFCdEdGckNVUkxPZVplUm5GeWpsV3IzS0lpbk1iYkc0SDlYTXA0OUVaWVFLREg4NHZSNFQ2ajU0TzM=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b>赵烨, 蒋建国, 洪日昌.基于RANSAC的SIFT匹配优化[J].光电工程, 2014, 41 (8) :58-65.
                            </a>
                        </p>
                        <p id="55">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Very Deep Convolutional Networks for Large-Scale Image Recognition">

                                <b>[9]</b>SIMONYAN K, ZISSERMAN A.Very deep convolutional networks for large-scale image recognition[EB/OL].[2017-12-13].https://arxiv.org/abs/1409.1556.
                            </a>
                        </p>
                        <p id="57">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=ImageNet classification with deep convolutional neural networks">

                                <b>[10]</b>KRIZHEVSKY A, SUTSKEVER I, HINTON G E.ImageNet classification with deep convolutional neural networks[C]//Proceedings of the 25th International Conference on Neural Information Processing Systems.New York, USA:ACM Press, 2012:1097-1105.
                            </a>
                        </p>
                        <p id="59">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multi-modal and multi-scale photo collection summarization">

                                <b>[11]</b>SHEN X, TIAN X.Multi-modal and multi-scale photo collection summarization[J].Multimedia Tools and Applications, 2016, 75 (5) :1-15.
                            </a>
                        </p>
                        <p id="61">
                            <a id="bibliography_12" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201605042&amp;v=MTc1NTA1NE8zenFxQnRHRnJDVVJMT2VaZVJuRnlqbFdyM0tMejdCYmJHNEg5Zk1xbzlCWm9RS0RIODR2UjRUNmo=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[12]</b>李志明.基于卷积神经网络的虹膜活体检测算法研究[J].计算机工程, 2016, 42 (5) :239-243, 248.
                            </a>
                        </p>
                        <p id="63">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Generative adversarial nets">

                                <b>[13]</b>GOODFELLOW I J, POUGETABADIE J, MIRZA M, et al.Generative adversarial nets[EB/OL].[2017-12-13].http://blog.csdn.net/wspba/article/details/54582391.
                            </a>
                        </p>
                        <p id="65">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Unsupervised representation learning with deep convolutional generative adversarial networks">

                                <b>[14]</b>RADFORD A, METZ L, CHINTALA S.Unsupervised representation learning with deep convolutional generative adversarial networks[EB/OL].[2017-12-13].https://arxiv.org/abs/1511.06434.
                            </a>
                        </p>
                        <p id="67">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=MARTA GANs:Unsupervised Representation Learning for Remote Sensing Image Classification">

                                <b>[15]</b>LIN D, FU K, WANG Y, et al.MARTA GANs:unsupervised representation learning for remote sensing image classification[J].IEEE Geoscience and Remote Sensing Letters, 2017, 14 (11) :2092-2096.
                            </a>
                        </p>
                        <p id="69">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=NUS-WIDE:A Real-world Web Image Database from National University of Singapore">

                                <b>[16]</b>CHUA T S, TANG J, HONG R, et al.NUS-WIDE:a real-world web image database from National University of Singapore[C]//Proceedings of ACM International Conference on Image and Video Retrieval.New York, USA:ACM Press, 2009:48-50.
                            </a>
                        </p>
                        <p id="71">
                            <a id="bibliography_17" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES6DEF9BD1EBD55091E6F5042AE83975D8&amp;v=MjMzMjZOdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQ3BiUTM1TkZoeExpMndxbz1OaWZPZmJYTWE2ZkYzZnRFRVpsN0NYazV4aGRtN0VsNFNIdmczV2M5ZXJ1VFFNNlhDTw==&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[17]</b>CAMARGO J E, GONZALEZ F A.Multimodal latent topic analysis for image collection summarization[J].Information Sciences, 2016, 328 (C) :270-287.
                            </a>
                        </p>
                        <p id="73">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Convex and semi-nonnegative matrix factorizations">

                                <b>[18]</b>DING C H Q, LI T, JORDAN M I.Convex and seminonnegative matrix factorizations[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2010, 32 (1) :45-55.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJC201902034" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201902034&amp;v=MDM0MjRiYkc0SDlqTXJZOUdZSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm5GeWpsV3IzS0x6N0I=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
