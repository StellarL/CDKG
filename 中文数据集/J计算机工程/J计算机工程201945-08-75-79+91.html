<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637128889891082500%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJC201908013%26RESULT%3d1%26SIGN%3dosc5Km%252bMhazwHJe3u3lWHN%252fqwr8%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201908013&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201908013&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201908013&amp;v=MjE3ODRiYkc0SDlqTXA0OUVaNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUnFGeS9nVWI3QUx6N0I=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9xck9nSjc1VXR3SDJsaTcyc24rQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#37" data-title="0 概述 ">0 概述</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#45" data-title="1 少数类预处理及数据模糊化 ">1 少数类预处理及数据模糊化</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#53" data-title="2 基于犹豫模糊集的模糊决策树 ">2 基于犹豫模糊集的模糊决策树</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#55" data-title="2.1 犹豫模糊集">2.1 犹豫模糊集</a></li>
                                                <li><a href="#65" data-title="2.2 模糊决策树构建">2.2 模糊决策树构建</a></li>
                                                <li><a href="#97" data-title="2.3 模糊规则及其决策推理">2.3 模糊规则及其决策推理</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#109" data-title="3 实验与结果分析 ">3 实验与结果分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#110" data-title="3.1 实验数据集与评价指标">3.1 实验数据集与评价指标</a></li>
                                                <li><a href="#115" data-title="3.2 实验方法与结果分析">3.2 实验方法与结果分析</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#122" data-title="4 结束语 ">4 结束语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#108" data-title="&lt;b&gt;图1 基于犹豫模糊集的模糊决策树方法流程&lt;/b&gt;"><b>图1 基于犹豫模糊集的模糊决策树方法流程</b></a></li>
                                                <li><a href="#114" data-title="&lt;b&gt;表1 实验数据集&lt;/b&gt;"><b>表1 实验数据集</b></a></li>
                                                <li><a href="#117" data-title="&lt;b&gt;表2 本文算法分类及其描述&lt;/b&gt;"><b>表2 本文算法分类及其描述</b></a></li>
                                                <li><a href="#121" data-title="&lt;b&gt;表3 各类算法实验所得的平均AUC值&lt;/b&gt;"><b>表3 各类算法实验所得的平均AUC值</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="141">


                                    <a id="bibliography_1" title=" 叶枫, 丁峰.不平衡数据分类研究及其应用[J].计算机应用与软件, 2018, 35 (1) :132-136, 205." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201801024&amp;v=MDQ4MjlqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVScUZ5L2dVYjdBTHpUWlpMRzRIOW5Ncm85SFlJUUtESDg0dlI0VDY=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9xck9nSjc1VXR3SDJsaTcyc24rQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         叶枫, 丁峰.不平衡数据分类研究及其应用[J].计算机应用与软件, 2018, 35 (1) :132-136, 205.
                                    </a>
                                </li>
                                <li id="143">


                                    <a id="bibliography_2" title=" 翟云, 杨炳儒, 曲武.不平衡类数据挖掘研究综述[J].计算机科学, 2010, 37 (10) :27-32." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJA201010007&amp;v=MDQ5MzFOcjQ5Rlk0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVScUZ5L2dVYjdBTHo3QmI3RzRIOUg=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9xck9nSjc1VXR3SDJsaTcyc24rQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         翟云, 杨炳儒, 曲武.不平衡类数据挖掘研究综述[J].计算机科学, 2010, 37 (10) :27-32.
                                    </a>
                                </li>
                                <li id="145">


                                    <a id="bibliography_3" title=" CHAWLA N V, BOWYER K W, HALL L O, et al.SMOTE:synthetic minority over-sampling technique[J].Journal of Artificial Intelligence Research, 2002, 16 (1) :321-357." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=SMOTE: synthetic minority over-sampling technique">
                                        <b>[3]</b>
                                         CHAWLA N V, BOWYER K W, HALL L O, et al.SMOTE:synthetic minority over-sampling technique[J].Journal of Artificial Intelligence Research, 2002, 16 (1) :321-357.
                                    </a>
                                </li>
                                <li id="147">


                                    <a id="bibliography_4" title=" HAN Hui, WANG Wenyuan, MAO Binghuan.Borderline-SMOTE:a new over-sampling method in imbalanced data sets learning[J].Lecture Notes in Computer Science, 2005, 3644 (5) :878-887." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Borderline-SMOTE:a new over-sampling method in imbalanced data sets learning">
                                        <b>[4]</b>
                                         HAN Hui, WANG Wenyuan, MAO Binghuan.Borderline-SMOTE:a new over-sampling method in imbalanced data sets learning[J].Lecture Notes in Computer Science, 2005, 3644 (5) :878-887.
                                    </a>
                                </li>
                                <li id="149">


                                    <a id="bibliography_5" title=" BATISTA G, PRATI R C, MONARD M C.A study of the behavior of several methods for balancing machine learning training data[J].SIGKDD Explorations, 2004, 6 (1) :20-29." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM388195169BFE0E7494828A3B5E98AD35&amp;v=MTY5Mjc9TmlmSVk3Q3dGdERGcW81RGJabDVlWHhNeUJJYTdqZC9RQTdoM2hkQWNMcmxNYm1hQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQ3BiUTM1TnhodzcyOXdhQQ==&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9xck9nSjc1VXR3SDJsaTcyc24rQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                         BATISTA G, PRATI R C, MONARD M C.A study of the behavior of several methods for balancing machine learning training data[J].SIGKDD Explorations, 2004, 6 (1) :20-29.
                                    </a>
                                </li>
                                <li id="151">


                                    <a id="bibliography_6" title=" SUN Y, KAMEL M S, WONG A K C, et al.Cost-sensitive boosting for classification of imbalanced data[J].Pattern Recognition, 2007, 40 (12) :3358-3378." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011600739373&amp;v=MTk5MTlXYXhvPU5pZk9mYks3SHRETnFZOUZZK2dHRDNzNm9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVUx2SUoxcw==&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9xck9nSjc1VXR3SDJsaTcyc24rQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                         SUN Y, KAMEL M S, WONG A K C, et al.Cost-sensitive boosting for classification of imbalanced data[J].Pattern Recognition, 2007, 40 (12) :3358-3378.
                                    </a>
                                </li>
                                <li id="153">


                                    <a id="bibliography_7" title=" 师彦文, 王宏杰.基于新型不纯度度量的代价敏感随机森林分类器[J].计算机科学, 2017, 44 (S2) :98-101." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJA2017S2020&amp;v=MTA0MTA1NE8zenFxQnRHRnJDVVJMT2VaZVJxRnkvZ1ViN0FMejdCYjdHNEg5YXZyWTlIWklRS0RIODR2UjRUNmo=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9xck9nSjc1VXR3SDJsaTcyc24rQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                         师彦文, 王宏杰.基于新型不纯度度量的代价敏感随机森林分类器[J].计算机科学, 2017, 44 (S2) :98-101.
                                    </a>
                                </li>
                                <li id="155">


                                    <a id="bibliography_8" title=" 刘东启, 陈志坚.适用于不平衡数据集分类的改进SVM算法[J].传感器与微系统, 2018, 37 (3) :1-4." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=CGQJ201803035&amp;v=MDI2MzN5L2dVYjdBSmlyYVpMRzRIOW5Nckk5R1lZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVScUY=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9xck9nSjc1VXR3SDJsaTcyc24rQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                         刘东启, 陈志坚.适用于不平衡数据集分类的改进SVM算法[J].传感器与微系统, 2018, 37 (3) :1-4.
                                    </a>
                                </li>
                                <li id="157">


                                    <a id="bibliography_9" title=" 韩敏, 朱新荣.不平衡数据分类的混合算法[J].控制理论与应用, 2011, 28 (10) :1485-1489." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=KZLY201110024&amp;v=MDI4MjRkN0c0SDlETnI0OUhZSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUnFGeS9nVWI3QUxqZkg=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9xck9nSjc1VXR3SDJsaTcyc24rQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                         韩敏, 朱新荣.不平衡数据分类的混合算法[J].控制理论与应用, 2011, 28 (10) :1485-1489.
                                    </a>
                                </li>
                                <li id="159">


                                    <a id="bibliography_10" title=" 李明方, 张化祥.针对不平衡数据的Bagging的改进算法[J].计算机工程与应用, 2013, 49 (2) :40-42." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSGG201030013&amp;v=MjU0NDc0SDlIUHI0OUVaNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUnFGeS9nVWI3QUx6N01hYkc=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9xck9nSjc1VXR3SDJsaTcyc24rQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[10]</b>
                                         李明方, 张化祥.针对不平衡数据的Bagging的改进算法[J].计算机工程与应用, 2013, 49 (2) :40-42.
                                    </a>
                                </li>
                                <li id="161">


                                    <a id="bibliography_11" title=" MEHDIZADEH M, EFTEKHARI M.Generating fuzzy rule base classifier for highly imbalanced datasets using a hybrid of evolutionary algorithms and subtractive clustering[J].Journal of Intelligent and Fuzzy Systems, 2014, 27 (6) :3033-3046." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SIJD&amp;filename=SIJD15112600038645&amp;v=MTAyNzhET3FZOUZaT2dIQ25nOG9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVUx2SUoxc1dheG89TmlUQmFySzlIOQ==&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9xck9nSjc1VXR3SDJsaTcyc24rQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[11]</b>
                                         MEHDIZADEH M, EFTEKHARI M.Generating fuzzy rule base classifier for highly imbalanced datasets using a hybrid of evolutionary algorithms and subtractive clustering[J].Journal of Intelligent and Fuzzy Systems, 2014, 27 (6) :3033-3046.
                                    </a>
                                </li>
                                <li id="163">


                                    <a id="bibliography_12" title=" HINOJOSA C E, CAMARGO H A, T&#218;PAC V Y J.Learning fuzzy classification rules from imbalanced datasets using multi-objective evolutionary algorithm[C]//Proceedings of Latin America Congress on Computational Intelligence.Washington D.C., USA:IEEE Press, 2015:1-6." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning fuzzy classification rules from imbalanced datasets using multi-objective evolutionary algorithm">
                                        <b>[12]</b>
                                         HINOJOSA C E, CAMARGO H A, T&#218;PAC V Y J.Learning fuzzy classification rules from imbalanced datasets using multi-objective evolutionary algorithm[C]//Proceedings of Latin America Congress on Computational Intelligence.Washington D.C., USA:IEEE Press, 2015:1-6.
                                    </a>
                                </li>
                                <li id="165">


                                    <a id="bibliography_13" >
                                        <b>[13]</b>
                                     ZADEH L A.Fuzzy sets[J].Information Control, 1965, 8 (3) :338-353.</a>
                                </li>
                                <li id="167">


                                    <a id="bibliography_14" >
                                        <b>[14]</b>
                                     TORRA V.Hesitant fuzzy sets[J].International Journal of Intelligent Systems, 2010, 25 (6) :529-539.</a>
                                </li>
                                <li id="169">


                                    <a id="bibliography_15" title=" CHEN Na, XU Zeshui, XIA Meimei.Correlation coeffi-cients of hesitant sets and their applications to clustering analysis[J].Applied Mathematical Modelling, 2013, 37 (4) :2197-2211." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14010600277894&amp;v=MDU5NTVHZXJxUVRNbndaZVp1SHlqbVVMdklKMXNXYXhvPU5pZk9mYks4SHRETXFZOUZadXdJQkhVOW9CTVQ2VDRQUUgvaXJSZA==&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9xck9nSjc1VXR3SDJsaTcyc24rQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[15]</b>
                                         CHEN Na, XU Zeshui, XIA Meimei.Correlation coeffi-cients of hesitant sets and their applications to clustering analysis[J].Applied Mathematical Modelling, 2013, 37 (4) :2197-2211.
                                    </a>
                                </li>
                                <li id="171">


                                    <a id="bibliography_16" title=" UMANO M, OKAMOLO H, HATONO I, et al.Fuzzy decision trees by Fuzzy ID3 algorithm and its application to diagnosis system[C]//Proceedings of the 3rd IEEE International Conference on Fuzzy Systems.Washington D.C., USA:IEEE Press, 1994:2113-2118." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fuzzy decision trees by Fuzzy-ID3 algorithm and its application to diagnosis systems">
                                        <b>[16]</b>
                                         UMANO M, OKAMOLO H, HATONO I, et al.Fuzzy decision trees by Fuzzy ID3 algorithm and its application to diagnosis system[C]//Proceedings of the 3rd IEEE International Conference on Fuzzy Systems.Washington D.C., USA:IEEE Press, 1994:2113-2118.
                                    </a>
                                </li>
                                <li id="173">


                                    <a id="bibliography_17" title=" YUAN Yufei, SHAW M J.Induction of fuzzy decision trees[J].Fuzzy Sets and Systems, 1995, 69 (2) :125-139." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012300674618&amp;v=MjM4MjRvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVVMdklKMXNXYXhvPU5pZk9mYks3SHRET3JJOUZZdXdMQ24weA==&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9xck9nSjc1VXR3SDJsaTcyc24rQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[17]</b>
                                         YUAN Yufei, SHAW M J.Induction of fuzzy decision trees[J].Fuzzy Sets and Systems, 1995, 69 (2) :125-139.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJC" target="_blank">计算机工程</a>
                2019,45(08),75-79+91 DOI:10.19678/j.issn.1000-3428.0051759            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于犹豫模糊决策树的非均衡数据分类</b></span>
 <span class="shoufa"></span>                                     </h1>

                <div class="btn-downloads  btn-downloads-new">

                        <a class="read read-btn-special" target="_blank" href=http://x.cnki.net/search/common/testlunbo?dbcode=CJFD&amp;tablename=CJFDLAST2019&amp;filename=JSJC201908013&amp;filesourcetype=1><i class="i-btn i-care"></i>精读</a>
                    <div class="read-btn-l">
                        <a class="caj" target="_blank" href="http://kns.cnki.net/kns/download.aspx?filename=0c2cZ9kdGFUYmh2K6t2ZLBzSUdHc2sySvNGTNVGUiVVQwAHZKp1NrsmdzsUTWVnehd2Tv4mdah2SP5mdHp2UXplVJ9Wd3UmbzEUSMdFSZBXSXxkejtiaY5Ua2lnNRJ1NtZzcSVkc5FEb1dHOvFGWWpVaoBVcXRkS&tablename=CJFDLAST2019"><i class="i-btn i-caj"></i> CAJ下载</a>
                        <a class="pdf" target="_blank" href="http://kns.cnki.net/kns/download.aspx?filename=0c2cZ9kdGFUYmh2K6t2ZLBzSUdHc2sySvNGTNVGUiVVQwAHZKp1NrsmdzsUTWVnehd2Tv4mdah2SP5mdHp2UXplVJ9Wd3UmbzEUSMdFSZBXSXxkejtiaY5Ua2lnNRJ1NtZzcSVkc5FEb1dHOvFGWWpVaoBVcXRkS&tablename=CJFDLAST2019&dflag=pdfdown"><i class="i-btn i-pdf"></i>PDF下载</a>

                        <p>永久保存本文,请下载至本地</p>
                    </div>

                </div>
            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BC%A0%E6%97%AD&amp;code=08760500&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9xck9nSjc1VXR3SDJsaTcyc24rQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">张旭</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%91%A8%E6%96%B0%E5%BF%97&amp;code=08746112&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9xck9nSjc1VXR3SDJsaTcyc24rQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">周新志</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%B5%B5%E6%88%90%E8%90%8D&amp;code=09804487&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9xck9nSjc1VXR3SDJsaTcyc24rQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">赵成萍</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%82%B5%E4%BC%A6&amp;code=40392366&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9xck9nSjc1VXR3SDJsaTcyc24rQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">邵伦</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%9B%9B%E5%B7%9D%E5%A4%A7%E5%AD%A6%E7%94%B5%E5%AD%90%E4%BF%A1%E6%81%AF%E5%AD%A6%E9%99%A2&amp;code=0054367&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9xck9nSjc1VXR3SDJsaTcyc24rQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">四川大学电子信息学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>为优化针对非均衡数据的分类效果, 结合犹豫模糊集理论与决策树算法, 提出一种改进的模糊决策树算法。通过SMOTE算法对非均衡数据进行过采样处理, 使用K-means聚类方法获得各属性的聚类中心点, 利用2种不同的隶属度函数对数据集进行模糊化处理。在此基础上, 根据隶属度函数和犹豫模糊集的信息能量求得各属性的犹豫模糊信息增益, 选取最大值替代Fuzzy ID3算法中的模糊信息增益作为属性的分裂准则, 构建一个用于非均衡数据分类的犹豫模糊决策树模型。实验结果表明, 基于犹豫模糊决策树的分类器在AUC评价指标上相对于C4.5、KNN、随机森林等传统分类算法平均提高了12.6%。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%9D%9E%E5%9D%87%E8%A1%A1%E6%95%B0%E6%8D%AE&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9xck9nSjc1VXR3SDJsaTcyc24rQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">非均衡数据;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%8A%B9%E8%B1%AB%E6%A8%A1%E7%B3%8A%E9%9B%86&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9xck9nSjc1VXR3SDJsaTcyc24rQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">犹豫模糊集;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%8A%B9%E8%B1%AB%E6%A8%A1%E7%B3%8A%E5%86%B3%E7%AD%96%E6%A0%91&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9xck9nSjc1VXR3SDJsaTcyc24rQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">犹豫模糊决策树;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=K-means%E8%81%9A%E7%B1%BB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9xck9nSjc1VXR3SDJsaTcyc24rQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">K-means聚类;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Fuzzy%20ID3%E7%AE%97%E6%B3%95&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9xck9nSjc1VXR3SDJsaTcyc24rQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Fuzzy ID3算法;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    张旭 (1992—) , 男, 硕士研究生, 主研方向为智能控制、数据挖掘;;
                                </span>
                                <span>
                                    周新志, 教授、博士;;
                                </span>
                                <span>
                                    赵成萍, 副教授、博士;;
                                </span>
                                <span>
                                    邵伦, 硕士研究生。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-06-06</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家重点基础研究发展计划 (2013CB328903-2);</span>
                    </p>
            </div>
                    <h1><b>Unbalanced Data Classification Based on Hesitant Fuzzy Decision Tree</b></h1>
                    <h2>
                    <span>ZHANG Xu</span>
                    <span>ZHOU Xinzhi</span>
                    <span>ZHAO Chengping</span>
                    <span>SHAO Lun</span>
            </h2>
                    <h2>
                    <span>College of Electronics and Information Engineering, Sichuan University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>In order to optimize the classification effect of unbalanced data, an improved fuzzy decision tree algorithm is proposed combining the hesitant fuzzy set theory and the decision tree algorithm.The unbalanced data is oversampled by the SMOTE algorithm, the cluster center point of each attribute is obtained by using the K-means clustering method, and the datasets is fuzzy processed by using two different membership functions.On this basis, the Hesitant Fuzzy Information Gain (HFIG) of each attribute is obtained by the information energy of hesitant fuzzy sets and membership functions.The largest HFIG is used to replace the FIG in the Fuzzy ID3 algorithm as the split criterion of the attribute, and a Hesitant Fuzzy Decision Tree (HFDT) model is constructed for unbalanced data classification.Experimental results show that, compared with traditional classification algorithms such as C4.5, KNN and random forest, the classifier based on HFDT has an average increase of 12.6% on the AUC evaluation index.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=unbalanced%20data&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9xck9nSjc1VXR3SDJsaTcyc24rQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">unbalanced data;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=hesitant%20fuzzy%20sets&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9xck9nSjc1VXR3SDJsaTcyc24rQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">hesitant fuzzy sets;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Hesitant%20Fuzzy%20Decision%20Tree%20(HFDT)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9xck9nSjc1VXR3SDJsaTcyc24rQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Hesitant Fuzzy Decision Tree (HFDT) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=K-means%20clustering&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9xck9nSjc1VXR3SDJsaTcyc24rQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">K-means clustering;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Fuzzy%20ID3%20algorithm&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9xck9nSjc1VXR3SDJsaTcyc24rQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Fuzzy ID3 algorithm;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-06-06</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="37" name="37" class="anchor-tag">0 概述</h3>
                <div class="p1">
                    <p id="38">现有的分类器多数适用于均衡数据集的分类, 这些样本集中的类别分布相对比较均衡, 各类样本的错分代价也大致相等。然而在现实生活中却存在着很多样本类别分布不均衡的情况, 如短信诈骗、故障诊断、垃圾邮件等, 在这些数据中, 属于诈骗类、故障类和垃圾类的样本数量远小于正常样本的数量。在应用传统分类算法进行非均衡数据集的分类时, 容易造成分类器偏向于多数类而缺少对少数类的关注, 从而对多数类样本学习过拟合, 对少数类样本学习欠拟合, 使得分类性能受到影响, 虽然分类模型训练后的分类准确率很高, 但是对于小类别样本却容易造成误分类。由于小类别样本为主要的研究和挖掘对象, 其蕴含的价值也要高于大类别样本中潜在的信息, 因此少数类样本被错分类的代价远高于对多数类样本错分的代价。</p>
                </div>
                <div class="p1">
                    <p id="39">均衡数据集中样本类别分布相对均衡的特点, 使得传统的分类器能够很好地获取到足够多的不同类别的样本信息, 分类器在这种数据集上能够取得较好的识别效果。但是对于非均衡数据集, 由于少数类样本数占比例较小, 很容易被分类器忽略掉, 因此无法准确识别出小类别样本。</p>
                </div>
                <div class="p1">
                    <p id="40">为解决上述问题, 许多学者提出了多种解决方法, 这些方法大致可以分为3类:基于数据重构的方法<citation id="175" type="reference"><link href="141" rel="bibliography" /><link href="143" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>]</sup></citation>, 基于算法级层面的方法, 基于模糊的方法。</p>
                </div>
                <div class="p1">
                    <p id="41">基于数据重构的方法主要是通过对训练集进行重构, 使其达到类别均衡, 可采用的技术主要为过采样和欠采样。其中过采样是对小类别样本向上采样, 即通过增加小类别样本数来减少与大类别样本数之间的差距。过采样最具代表性的方法有SMOTE<citation id="176" type="reference"><link href="145" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>、Borderline-SMOTE<citation id="177" type="reference"><link href="147" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>等, 而欠采样则是通过减少大类别样本数, 降低分类器对大类别样本的关注度。典型的方法有Tomek-links、ENN等<citation id="178" type="reference"><link href="149" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>。数据重构的方法虽然降低了数据的不平衡度, 但是也可能会产生欠采样导致重要信息丢失、过采样引起分类器过学习等问题。</p>
                </div>
                <div class="p1">
                    <p id="42">基于算法级层面的方法主要是通过对算法进行改进, 使其能够很好地适应非均衡数据集的分类, 如代价敏感学习、集成学习等<citation id="179" type="reference"><link href="151" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>。在代价敏感学习方面:文献<citation id="180" type="reference">[<a class="sup">7</a>]</citation>将一种新型不纯度度量应用于代价敏感和随机森林的结合算法中, 构造了一种代价敏感随机森林分类器;文献<citation id="181" type="reference">[<a class="sup">8</a>]</citation>使用基于代价敏感的SVM算法训练, 缓解非均衡数据对超平面造成的偏移。在集成学习方面:文献<citation id="182" type="reference">[<a class="sup">9</a>]</citation>提出了一种径向基函数神经网络和随机森林集成的混合分类算法, 是基于数据层和算法层相结合的方法, 先对小样本使用SMOTE方法使其均衡, 再以径向基函数神经网络作为随机森林中的基分类器, 采用集成学习方法针对小类样本进行训练和测试, 有效提高了小类别样本的识别率;文献<citation id="183" type="reference">[<a class="sup">10</a>]</citation>将SMOTE算法和Bagging集成学习相结合, 利用SMOTE算法对样本集中少数类进行加工, 在Bagging算法中根据类值对各个样本的权重进行调整。</p>
                </div>
                <div class="p1">
                    <p id="43">近年来, 将模糊方法应用于非均衡数据集上, 被大量的研究和使用。其中:文献<citation id="184" type="reference">[<a class="sup">11</a>]</citation>通过将减法聚类、差分进化和多基因遗传编程进行组合, 构造了一种基于模糊规则的高度不平衡数据集的分类器;文献<citation id="185" type="reference">[<a class="sup">12</a>]</citation>提出了一种利用多目标遗传算法和迭代规则学习方法从不平衡数据集中学习模糊分类规则的方法, 首先使用SMOTE+TL预处理方法对非均衡数据集进行均衡, 然后采用迭代多目标进化算法学习模糊规则。</p>
                </div>
                <div class="p1">
                    <p id="44">本文基于数据重构技术和犹豫模糊集对非均衡数据进行分类。使用SMOTE对少数类样本预处理, 通过K-means聚类以及隶属度函数对数据做模糊化处理。在此基础上, 结合犹豫模糊集理论提出一种新的决策树属性分裂准则, 进而构造用于二分类非均衡数据分类的犹豫模糊决策树 (Hesitant Fuzzy Decision Tree, HFDT) 模型。</p>
                </div>
                <h3 id="45" name="45" class="anchor-tag">1 少数类预处理及数据模糊化</h3>
                <div class="p1">
                    <p id="46">原始的数据集是非均衡的二分类数据集, 其中少数类样本称为正例 (positive) , 多数类样本称为负例 (negative) 。对这类数据采用传统的分类器分类的效果并不理想, 分类器容易将少数类样本误分到多数类样本中, 因此, 多种数据均衡的方法被提出, 主要包括采样和欠采样方法。欠采样方法通过减少多数类样本以达到数据均衡的目的, 但是这样做容易造成重要数据丢失等问题。本文考虑使用合成少数过采样技术 (SMOTE算法) 对训练样本中的少数类样本进行过采样处理, 通过扩充少数类样本数量降低数据的非均衡比, 同时保证原始数据不丢失, 为防止过度过采样容易造成过拟合、样本重叠等问题, 将适度进行过采样以降低过拟合的干扰。</p>
                </div>
                <div class="p1">
                    <p id="47">对预处理后的样本集进行模糊化处理, 将原始数据集转变为模糊数据集<citation id="186" type="reference"><link href="165" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>, 以便应用于模糊决策树中。模糊化处理的步骤如下:</p>
                </div>
                <div class="p1">
                    <p id="48">1) 对于样本集中的离散属性可以直接根据其语义值, 将其隶属度值设置为0或1。</p>
                </div>
                <div class="p1">
                    <p id="49">2) 对于每一个连续属性<i>x</i><sub><i>i</i></sub>, 则使用K-means聚类对样本集进行聚类处理, 得到各个属性的聚类中心点 (<i>c</i><sub>1</sub>, <i>c</i><sub>2</sub>, …, <i>c</i><sub><i>k</i></sub>) 。</p>
                </div>
                <div class="p1">
                    <p id="50">3) 由聚类中心点将连续属性的值域划分为<i>k</i>个离散的区间。</p>
                </div>
                <div class="p1">
                    <p id="51">4) 将聚类中心点作为三角形隶属度函数<i>f</i> (<i>μ</i> (<i>x</i>) ) 的参数, 根据函数表达式得到各个属性对应的隶属度函数<i>f</i> (<i>μ</i> (<i>x</i><sub><i>i</i></sub>) ) 。</p>
                </div>
                <div class="p1">
                    <p id="52">5) 使用每个连续属性的隶属度函数求得<i>k</i>个离散值 (<i>μ</i><sub>1</sub> (<i>x</i><sub><i>i</i></sub>) , <i>μ</i><sub>2</sub> (<i>x</i><sub><i>i</i></sub>) , …, <i>μ</i><sub><i>k</i></sub> (<i>x</i><sub><i>i</i></sub>) ) , 各个属性的<i>k</i>个离散值组合为一组模糊数据集。</p>
                </div>
                <h3 id="53" name="53" class="anchor-tag">2 基于犹豫模糊集的模糊决策树</h3>
                <div class="p1">
                    <p id="54">在本节中, 首先给出犹豫模糊集的概念和相关定义, 然后使用上节中预处理和模糊化后的模糊数据集, 并结合犹豫模糊集理论和模糊决策树算法构建一种新的模糊决策树模型, 将构建的犹豫模糊决策树模型转化为一系列模糊规则, 可用于对非均衡数据进行分类。</p>
                </div>
                <h4 class="anchor-tag" id="55" name="55">2.1 犹豫模糊集</h4>
                <div class="p1">
                    <p id="56">在现实生活中不确定性现象广泛存在, 而这些不确定性问题很难用模糊集、区间数学等数学工具来准确描述。为此, 文献<citation id="187" type="reference">[<a class="sup">14</a>]</citation>提出了一种新的概念, 即犹豫模糊集。它是模糊集的一种拓展, 描述了在给模糊集合分配元素的隶属度时可能出现的犹豫不决引起的不确定性。在模糊集中元素的隶属度只能为一个值, 而犹豫模糊集允许元素隶属度有多个可能取值, 这更贴近实际中在给元素分配隶属度值时不确定的情况。</p>
                </div>
                <div class="p1">
                    <p id="57"><b>定义1</b> 设<i>X</i>为一个给定的集合, <i>X</i>上的犹豫模糊集<i>D</i>定义<citation id="188" type="reference"><link href="167" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>如下:</p>
                </div>
                <div class="p1">
                    <p id="58"><i>D</i>={〈<i>x</i>, <i>h</i><sub><i>D</i></sub> (<i>x</i>) 〉|<i>x</i>∈<i>X</i>}      (1) </p>
                </div>
                <div class="p1">
                    <p id="59">其中, <i>h</i><sub><i>D</i></sub> (<i>x</i>) 是[0, 1]中不同值的集合, 代表了元素<i>x</i>对<i>D</i>的可能隶属度。</p>
                </div>
                <div class="p1">
                    <p id="60"><b>定义2</b> 对于一个给定的犹豫模糊集<i>D</i>:</p>
                </div>
                <div class="p1">
                    <p id="61"><i>D</i>={〈<i>x</i><sub><i>i</i></sub>, <i>h</i><sub><i>D</i></sub> (<i>x</i><sub><i>i</i></sub>) 〉|<i>x</i><sub><i>i</i></sub>∈<i>X</i>, <i>i</i>=1, 2, …, <i>n</i>}</p>
                </div>
                <div class="p1">
                    <p id="62">则<i>D</i>的信息能量定义如式 (2) 所示<citation id="189" type="reference"><link href="169" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="63" class="code-formula">
                        <mathml id="63"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>E</mi><msub><mrow></mrow><mrow><mtext>Η</mtext><mtext>F</mtext><mtext>S</mtext></mrow></msub><mo stretchy="false"> (</mo><mi>D</mi><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><mrow><mo> (</mo><mrow><mfrac><mn>1</mn><mrow><mi>l</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>l</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></munderover><mi>h</mi></mstyle><msubsup><mrow></mrow><mrow><mi>D</mi><msub><mrow></mrow><mi>σ</mi></msub><mo stretchy="false"> (</mo><mi>j</mi><mo stretchy="false">) </mo></mrow><mn>2</mn></msubsup><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo></mrow><mo>) </mo></mrow></mrow></mstyle><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="64">其中, <i>l</i><sub><i>i</i></sub>为<i>x</i><sub><i>i</i></sub>的隶属度个数, <i>h</i><sub><i>Dσ</i> (<i>j</i>) </sub> (<i>x</i><sub><i>i</i></sub>) 是<i>x</i><sub><i>i</i></sub>中第<i>j</i>个隶属度值。</p>
                </div>
                <h4 class="anchor-tag" id="65" name="65">2.2 模糊决策树构建</h4>
                <div class="p1">
                    <p id="66">Fuzzy ID3算法<citation id="190" type="reference"><link href="171" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>是模糊决策树构建过程中常用算法之一, 该算法根据计算模糊数据集的模糊信息增益 (Fuzzy Information Gain, FIG) 作为分裂属性选取的标准。</p>
                </div>
                <div class="p1">
                    <p id="67">模糊数据集<i>S</i>的模糊熵<i>FE</i>定义为:</p>
                </div>
                <div class="p1">
                    <p id="68" class="code-formula">
                        <mathml id="68"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>F</mi><mi>E</mi><mo stretchy="false"> (</mo><mi>S</mi><mo stretchy="false">) </mo><mo>=</mo><mo>-</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mrow><mfrac><mrow><mrow><mo>|</mo><mrow><mi>S</mi><msup><mrow></mrow><mrow><mi>C</mi><msub><mrow></mrow><mi>k</mi></msub></mrow></msup></mrow><mo>|</mo></mrow></mrow><mrow><mrow><mo>|</mo><mi>S</mi><mo>|</mo></mrow></mrow></mfrac></mrow></mstyle><mtext>l</mtext><mtext>b</mtext><mfrac><mrow><mrow><mo>|</mo><mrow><mi>S</mi><msup><mrow></mrow><mrow><mi>C</mi><msub><mrow></mrow><mi>k</mi></msub></mrow></msup></mrow><mo>|</mo></mrow></mrow><mrow><mrow><mo>|</mo><mi>S</mi><mo>|</mo></mrow></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="69">其中, <i>S</i><sup><i>C</i><sub><i>k</i></sub></sup>表示在<i>S</i>的<i>m</i>个类别中类别为<i>C</i><sub><i>k</i></sub>的模糊子集。</p>
                </div>
                <div class="p1">
                    <p id="70">属性<i>A</i><sub><i>i</i></sub>的模糊信息增益为:</p>
                </div>
                <div class="p1">
                    <p id="71" class="code-formula">
                        <mathml id="71"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>F</mi><mi>Ι</mi><mi>G</mi><mo stretchy="false"> (</mo><mi>A</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>S</mi><mo stretchy="false">) </mo><mo>=</mo><mi>F</mi><mi>E</mi><mo stretchy="false"> (</mo><mi>S</mi><mo stretchy="false">) </mo><mo>-</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>r</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></munderover><mi>p</mi></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>×</mo><mi>F</mi><mi>E</mi><mo stretchy="false"> (</mo><mi>S</mi><mo stretchy="false">[</mo><mi>F</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mo stretchy="false"> (</mo><mi>j</mi><mo stretchy="false">) </mo></mrow></msubsup><mo stretchy="false">]</mo><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi>p</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mfrac><mrow><mo stretchy="false">|</mo><mi>S</mi><mo stretchy="false">[</mo><mi>F</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mo stretchy="false"> (</mo><mi>j</mi><mo stretchy="false">) </mo></mrow></msubsup><mo stretchy="false">]</mo><mo stretchy="false">|</mo></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>r</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></munderover><mo stretchy="false">|</mo></mstyle><mi>S</mi><mo stretchy="false">[</mo><mi>F</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mo stretchy="false"> (</mo><mi>k</mi><mo stretchy="false">) </mo></mrow></msubsup><mo stretchy="false">]</mo><mo stretchy="false">|</mo></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="72">其中, <i>S</i>[<i>F</i><mathml id="73"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mrow><mo stretchy="false"> (</mo><mi>j</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>]是对应于模糊项<i>F</i><mathml id="74"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mrow><mo stretchy="false"> (</mo><mi>j</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>的子节点上的模糊数据集, r<sub>i</sub>为模糊项的个数。</p>
                </div>
                <div class="p1">
                    <p id="75">上文中使用了三角形隶属度函数对数据集进行模糊化处理, 生成了一组模糊数据集, 下文将结合使用另外一种隶属度函数, 梯形隶属度函数。由这2种隶属度函数得到每个属性值域的2种隶属度值, 并根据式 (4) 求出每个属性A<sub>i</sub>的模糊信息增益, 记为FIG<sub><i>triangle</i></sub> (A<sub>i</sub>, S) 和FIG<sub><i>trapezoid</i></sub> (A<sub>i</sub>, S) 。</p>
                </div>
                <div class="p1">
                    <p id="76">对于属性A<sub>i</sub>的犹豫模糊集D (A<sub>i</sub>) 定义如下:</p>
                </div>
                <div class="p1">
                    <p id="77">D (A<sub>i</sub>) ={〈A<sub>i</sub>, h<sub>D</sub> (A<sub>i</sub>) 〉|A<sub>i</sub>∈A}      (5) </p>
                </div>
                <div class="p1">
                    <p id="78">其中, h<sub>D</sub> (A<sub>i</sub>) ={FIG<sub><i>triangle</i></sub> (A<sub>i</sub>, S) , FIG<sub><i>trapezoid</i></sub> (A<sub>i</sub>, S) }, A为模糊集的属性集合。</p>
                </div>
                <div class="p1">
                    <p id="79">结合犹豫模糊集理论和式 (2) 计算每个属性A<sub>i</sub>的犹豫模糊信息增益 (<i>Hesitant FIG</i>, <i>HFIG</i>) 如下:</p>
                </div>
                <div class="p1">
                    <p id="80" class="code-formula">
                        <mathml id="80"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>Η</mi><mi>F</mi><mi>Ι</mi><mi>G</mi><mo stretchy="false"> (</mo><mi>A</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>S</mi><mo stretchy="false">) </mo><mo>=</mo><mi>E</mi><msub><mrow></mrow><mrow><mtext>Η</mtext><mtext>F</mtext><mtext>S</mtext></mrow></msub><mo stretchy="false"> (</mo><mi>D</mi><mo stretchy="false"> (</mo><mi>A</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo>=</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo stretchy="false"> (</mo><mi>F</mi><mi>Ι</mi><mi>G</mi><msubsup><mrow></mrow><mrow><mtext>t</mtext><mtext>r</mtext><mtext>i</mtext><mtext>a</mtext><mtext>n</mtext><mtext>g</mtext><mtext>l</mtext><mtext>e</mtext></mrow><mn>2</mn></msubsup><mo stretchy="false"> (</mo><mi>A</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>S</mi><mo stretchy="false">) </mo><mo>+</mo><mi>F</mi><mi>Ι</mi><mi>G</mi><msubsup><mrow></mrow><mrow><mtext>t</mtext><mtext>r</mtext><mtext>a</mtext><mtext>p</mtext><mtext>e</mtext><mtext>z</mtext><mtext>o</mtext><mtext>i</mtext><mtext>d</mtext></mrow><mn>2</mn></msubsup><mo stretchy="false"> (</mo><mi>A</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>S</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>6</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="81">使用犹豫模糊集和<i>Fuzzy ID</i>3算法构造犹豫模糊决策树的过程如下:</p>
                </div>
                <div class="p1">
                    <p id="82"><b>步骤1</b> 生成根节点, 该节点包含所有的犹豫模糊数据集。</p>
                </div>
                <div class="p1">
                    <p id="83"><b>步骤2</b> 判断带有犹豫模糊数据集<i>S</i>的节点<i>t</i>是否满足以下条件:</p>
                </div>
                <div class="p1">
                    <p id="84">1) 属于类别<i>C</i><sub><i>k</i></sub>的数据集的比例大于或等于阈值<i>θ</i><sub><i>τ</i></sub>, 即<mathml id="85"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mrow><mo stretchy="false">|</mo><mtext>S</mtext></mrow><msup><mrow></mrow><mrow><mi>C</mi><msub><mrow></mrow><mi>k</mi></msub></mrow></msup><mo stretchy="false">|</mo></mrow><mrow><mrow><mo>|</mo><mi>S</mi><mo>|</mo></mrow></mrow></mfrac><mo>≥</mo><mi>θ</mi><msub><mrow></mrow><mi>τ</mi></msub></mrow></math></mathml>。</p>
                </div>
                <div class="p1">
                    <p id="86">2) 数据集的样本个数少于阈值θ<sub>n</sub>, 即|S|&lt;θ<sub>n</sub>。</p>
                </div>
                <div class="p1">
                    <p id="87">3) 没有可分类的属性, 即属性集A=Ø。</p>
                </div>
                <div class="p1">
                    <p id="88">若满足以上条件之一, 则将该节点作为叶节点, 并将类别C<sub>k</sub>所占比例指定到该叶节点上;若不满足以上任何条件, 则进入步骤3。</p>
                </div>
                <div class="p1">
                    <p id="89"><b>步骤3</b> 由式 (6) 计算出每个属性的HFIG, 并选择最大的犹豫模糊信息增益的属性<i>A</i><sub>max</sub>作为分枝属性。</p>
                </div>
                <div class="p1">
                    <p id="90"><b>步骤4</b> 根据<i>A</i><sub>max</sub>将数据集<i>S</i>分为若干个犹豫模糊子集{<i>S</i>[<i>F</i><mathml id="91"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>max</mi></mrow><mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>], <i>S</i>[<i>F</i><mathml id="92"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>max</mi></mrow><mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>], …, <i>S</i>[<i>F</i><mathml id="93"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>max</mi></mrow><mrow><mo stretchy="false"> (</mo><mi>r</mi><msub><mrow></mrow><mrow><mi>max</mi></mrow></msub><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>]}, 对应犹豫模糊子集生成新的子节点{<i>t</i><sub>1</sub>, <i>t</i><sub>1</sub>, …, <i>t</i><sub><i>r</i><sub>max</sub></sub>}, 节点<i>t</i>到子节点<i>t</i><sub><i>j</i></sub>的边对应于犹豫模糊项<i>F</i><mathml id="94"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>max</mi></mrow><mrow><mo stretchy="false"> (</mo><mi>j</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>。</p>
                </div>
                <div class="p1">
                    <p id="95"><b>步骤5</b> 从属性集<i>A</i>中去掉属性<i>A</i><sub>max</sub>, 并用犹豫模糊子集<i>S</i>[<i>F</i><mathml id="96"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>max</mi></mrow><mrow><mo stretchy="false"> (</mo><mi>j</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>]代替<i>S</i>重复步骤2～步骤5。</p>
                </div>
                <h4 class="anchor-tag" id="97" name="97">2.3 模糊规则及其决策推理</h4>
                <div class="p1">
                    <p id="98">将构建好的犹豫模糊决策树转化为一系列模糊规则, 每一条模糊规则对应于从根节点到叶节点的一条路径。不同于经典的决策树, 模糊决策树产生的规则可能以不同的隶属度将未知实例分到不同的类别中。下面给出了部分模糊规则的形式:</p>
                </div>
                <div class="p1">
                    <p id="99"><b>规则1</b> IF [<i>A</i><sub>1</sub> IS <i>F</i><sub>1</sub><sup> (1) </sup>] AND [<i>A</i><sub>2</sub> IS <i>F</i><sub>2</sub><sup> (1) </sup>]THEN <i>C</i><sub>1</sub> (<i>CF</i>=0.95) 。</p>
                </div>
                <div class="p1">
                    <p id="100"><b>规则2</b> IF [<i>A</i><sub>1</sub> IS <i>F</i><sub>1</sub><sup> (2) </sup>] AND [<i>A</i><sub>2</sub> IS <i>F</i><sub>2</sub><sup> (2) </sup>]AND  [<i>A</i><sub>3</sub> IS <i>F</i><sub>3</sub><sup> (1) </sup>] THEN <i>C</i><sub>2</sub> (<i>CF</i>=0.82) 。</p>
                </div>
                <div class="p1">
                    <p id="101"><b>规则3</b> IF [<i>A</i><sub>1</sub> IS <i>F</i><sub>2</sub><sup> (1) </sup>] AND [<i>A</i><sub>2</sub> IS <i>F</i><sub>2</sub><sup> (1) </sup>]THEN <i>C</i><sub>1</sub> (<i>CF</i>=0.8) 。</p>
                </div>
                <div class="p1">
                    <p id="102">其中:[<i>A</i><sub>1</sub> IS <i>F</i><sub>1</sub><sup> (1) </sup>]是条件决策, 表示属性<i>A</i><sub>1</sub>隶属于模糊项<i>F</i><sub>1</sub><sup> (1) </sup>;<i>C</i><sub>1</sub> (<i>CF</i>=0.95) 为决策结果, 表示该条规则将样本归为<i>C</i><sub>1</sub>类, 其置信度<i>CF</i>为0.95。</p>
                </div>
                <div class="p1">
                    <p id="103">利用模糊规则可对未知实例进行分类, 步骤如下<citation id="191" type="reference"><link href="173" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>:</p>
                </div>
                <div class="p1">
                    <p id="104">1) 对于每一条模糊规则, 将实例与条件部分匹配的隶属度取小运算后的结果作为实例属于某类的隶属度。</p>
                </div>
                <div class="p1">
                    <p id="105">2) 若多条规则以不同隶属度将此实例分为同一类, 则取最高隶属度。</p>
                </div>
                <div class="p1">
                    <p id="106">3) 若此实例以不同的隶属度分到不同的类中, 且需做出明确决策, 则取具有最高隶属度的类别作为实例的最终类别。</p>
                </div>
                <div class="p1">
                    <p id="107">基于犹豫模糊集的模糊决策树方法的整个过程如图1所示。通过犹豫模糊集的信息能量整合2种模糊化处理下的FIG得到的HFIG, 相比于模糊决策树中的FIG更能选择出较优的分裂属性。构造的HFDT相比于FDT也具有更好的分类性能。由于在整合FIG的过程中属于线性操作, 其时间复杂度也即为线性阶, 而构造HFDT的过程花费的时间也等同于构造FDT的时间, 因此整体过程的时间复杂度与改进前的Fuzzy ID3算法相比并没有增加。此外, 在整个过程中并没有申请额外的存储空间, 因此, 其空间复杂度等同于改进前的Fuzzy ID3算法。</p>
                </div>
                <div class="area_img" id="108">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201908013_108.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9xck9nSjc1VXR3SDJsaTcyc24rQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 基于犹豫模糊集的模糊决策树方法流程" src="Detail/GetImg?filename=images/JSJC201908013_108.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9xck9nSjc1VXR3SDJsaTcyc24rQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图1 基于犹豫模糊集的模糊决策树方法流程</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201908013_108.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9xck9nSjc1VXR3SDJsaTcyc24rQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h3 id="109" name="109" class="anchor-tag">3 实验与结果分析</h3>
                <h4 class="anchor-tag" id="110" name="110">3.1 实验数据集与评价指标</h4>
                <div class="p1">
                    <p id="111">本文中实验所用数据来自于KEEL数据库中的8组二分类非均衡数据集, 规定数据集中少数类样本为正样本, 多数类样本为负样本。表1给出了数据集的基本描述以及SMOTE处理后的数据类别分布情况, 其中根据原始数据集的非均衡比从低到高依次进行编号。</p>
                </div>
                <div class="p1">
                    <p id="112">在传统的分类问题中, 一般使用分类正确率作为分类器的性能评价指标, 然而在非均衡问题上正确率并不能有效地反映出分类器的性能。这是因为在非均衡数据集中正样本的数量要远远小于负样本的数量, 虽然最后求得的正确率很高, 但是分类器却忽略了对正样本的正确识别。</p>
                </div>
                <div class="p1">
                    <p id="113">常用的非均衡数据分类器性能评价指标有G-mean值、F-score和AUC值等, 本文使用AUC值作为分类器的性能评价指标。AUC值是指处于接受者操作特征 (Receiver Operating Characteristic, ROC) 曲线下的面积, 取值通常为0.5～1.0, AUC的值越大表明分类器的性能越好。</p>
                </div>
                <div class="area_img" id="114">
                    <p class="img_tit"><b>表1 实验数据集</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="114" border="1"><tr><td rowspan="2">编号</td><td rowspan="2">数据集</td><td rowspan="2">样本数</td><td rowspan="2">属性</td><td colspan="3"><br />原始样本</td><td colspan="3"><br />SMOTE处理后的样本</td></tr><tr><td><br />正样本数</td><td>负样本数</td><td>非均衡比</td><td><br />正样本数</td><td>负样本数</td><td>非均衡比</td></tr><tr><td>1</td><td>yeast-2_vs_4</td><td>514</td><td>8</td><td>51</td><td>463</td><td>9.08</td><td>171</td><td>463</td><td>2.71</td></tr><tr><td><br />2</td><td>glass-0-1-6_vs_2</td><td>192</td><td>9</td><td>17</td><td>175</td><td>10.29</td><td>56</td><td>175</td><td>3.13</td></tr><tr><td><br />3</td><td>glass2</td><td>214</td><td>9</td><td>17</td><td>197</td><td>11.59</td><td>42</td><td>197</td><td>4.69</td></tr><tr><td><br />4</td><td>glass4</td><td>214</td><td>9</td><td>13</td><td>201</td><td>15.46</td><td>23</td><td>201</td><td>8.74</td></tr><tr><td><br />5</td><td>ecoli4</td><td>336</td><td>7</td><td>20</td><td>316</td><td>15.80</td><td>68</td><td>316</td><td>4.65</td></tr><tr><td><br />6</td><td>abalone9-18</td><td>731</td><td>8</td><td>42</td><td>689</td><td>16.40</td><td>75</td><td>689</td><td>9.19</td></tr><tr><td><br />7</td><td>yeast-2_vs_8</td><td>482</td><td>8</td><td>20</td><td>462</td><td>23.10</td><td>36</td><td>462</td><td>12.83</td></tr><tr><td><br />8</td><td>yeast4</td><td>1 484</td><td>8</td><td>51</td><td>1 433</td><td>28.10</td><td>182</td><td>1 433</td><td>7.87</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="115" name="115">3.2 实验方法与结果分析</h4>
                <div class="p1">
                    <p id="116">本文使用2种隶属度函数对数据进行模糊化处理, 得到了2组模糊数据集。由2组模糊数据集分别获到各属性的模糊信息增益FIG, 再通过犹豫模糊集的信息能量整合为HFIG, 作为构建HFDT的属性分裂准则。2种隶属度函数对训练集和测试集模糊化处理的结果分别记为:三角形隶属函数模糊化的训练集 (训练集1) 和测试集 (测试集1) , 梯形隶属函数模糊化的训练集 (训练集2) 和测试集 (测试集2) 。为了验证不同的训练集和测试集组合的效果, 将本文算法分成了6类, 详细信息在表2中给出。</p>
                </div>
                <div class="area_img" id="117">
                    <p class="img_tit"><b>表2 本文算法分类及其描述</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="117" border="1"><tr><td><br />算法</td><td>算法描述           </td></tr><tr><td><br />算法1</td><td>使用训练集1和测试集1作为输入训练集和测试集, 由训练集1得到FIG</td></tr><tr><td><br />算法2</td><td>使用训练集2和测试集2作为输入训练集和测试集, 由训练集2得到FIG</td></tr><tr><td><br />算法3</td><td>使用训练集1和测试集1作为输入训练集和测试集, 由训练集1和训练集2构成犹豫模糊集并得到HFIG</td></tr><tr><td><br />算法4</td><td>使用训练集1和测试集2作为输入训练集和测试集, 由训练集1和训练集2构成犹豫模糊集并得到HFIG</td></tr><tr><td><br />算法5</td><td>使用训练集2和测试集2作为输入训练集和测试集, 由训练集1和训练集2构成犹豫模糊集并得到HFIG</td></tr><tr><td><br />算法6</td><td>使用训练集2和测试集1作为输入训练集和测试集, 由训练集1和训练集2构成犹豫模糊集并得到HFIG</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="118">在表2中, 算法1和算法2是分别单独使用三角形隶属度函数和梯形隶属度函数对数据进行模糊化处理, 并使用模糊信息增益FIG作为属性分裂准则, 为检验组合的效果是否优于未组合的效果, 算法1和算法2不采用两两组合的形式, 作为一种对比算法;而算法3～算法6则使用2种隶属度函数进行数据的模糊化处理, 并通过交叉组合形式而成, 再结合犹豫模糊集理论使用犹豫模糊信息增益HFIG作为属性分裂准则来构造HFDT模型。</p>
                </div>
                <div class="p1">
                    <p id="119">为评估本文提出的算法有效性, 在8组非均衡二分类数据集上进行了实验, 并与一些其他的分类算法进行了对比。其他的分类算法主要是C4.5算法、KNN算法 (<i>K</i>=3) 、随机森林算法以及SMOTE和C4.5的混合算法。对每种分类算法使用五折交叉验证进行实验, 计算出测试集的平均AUC值作为分类器性能的评价指标。另外, 在本文中规定当节点精度大于等于0.85时, 犹豫模糊决策树停止构建, 4种其他分类算法以及本文算法的实验结果对比如表3所示。表中黑色加粗数据表示其中AUC值最高的一个, 表中最后一行是不同算法在8组数据集上的AUC平均值。</p>
                </div>
                <div class="p1">
                    <p id="120">算法1和算法2的效果可以等价于模糊决策树的分类性能, 从表3中可以看出, 本文中作为对比的基于单个隶属度函数的算法1和算法2的分类效果, 整体不如基于整合2种隶属度函数的算法3～算法6, 说明基于犹豫模糊集的模糊决策树算法的分类效果要好于模糊决策树算法, 也即证明在本文中使用交叉组合的形式相比于未组合方法更能有效地分类非均衡数据集。而算法1和算法2的分类性能也平均优于其他4种分类算法, 说明模糊决策树算法在处理非均衡数据方面表现略优异些。从表中第5列, 也即SMOTE+C4.5的混合算法的结果可以看出, 通过SMOTE对小类别样本扩充后对分类器整体的分类效果提升并不明显, 甚至在yeast-2_vs_4数据集上分类性能降低了11%左右。而本文提出的算法3～算法6在8组数据集上表现整体优于前4种传统算法。由最后一行的平均值来看, 算法3～算法6中除了算法5的平均值略低于随机森林的平均值外, 剩余的都高于其他算法。其中算法4表现最好, 相对于其他4种对比算法精度平均提高约12.6%。</p>
                </div>
                <div class="area_img" id="121">
                    <p class="img_tit"><b>表3 各类算法实验所得的平均AUC值</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="121" border="1"><tr><td>数据集</td><td>C4.5</td><td>KNN</td><td>随机森林</td><td>SMOTE+C4.5</td><td>算法1</td><td>算法2</td><td>算法3</td><td>算法4</td><td>算法5</td><td>算法6</td></tr><tr><td>yeast-2_vs_4</td><td>0.816</td><td>0.832</td><td>0.825</td><td>0.726</td><td><b>0.837</b></td><td>0.752</td><td><b>0.837</b></td><td>0.809</td><td>0.771</td><td>0.780</td></tr><tr><td><br />glass-0-1-6_vs_2</td><td>0.575</td><td>0.611</td><td>0.661</td><td>0.579</td><td>0.557</td><td>0.671</td><td><b>0.811</b></td><td>0.639</td><td>0.682</td><td>0.775</td></tr><tr><td><br />glass2</td><td>0.712</td><td>0.667</td><td>0.683</td><td>0.750</td><td>0.731</td><td>0.705</td><td>0.692</td><td><b>0.853</b></td><td>0.724</td><td>0.712</td></tr><tr><td><br />glass4</td><td>0.762</td><td>0.783</td><td>0.806</td><td>0.797</td><td><b>0.933</b></td><td>0.642</td><td>0.875</td><td>0.896</td><td>0.629</td><td>0.808</td></tr><tr><td><br />ecoli4</td><td>0.867</td><td>0.875</td><td>0.859</td><td>0.869</td><td>0.977</td><td>0.828</td><td>0.977</td><td>0.961</td><td>0.842</td><td><b>0.992</b></td></tr><tr><td><br />abalone9-18</td><td>0.586</td><td>0.644</td><td>0.764</td><td>0.625</td><td>0.664</td><td>0.667</td><td>0.667</td><td><b>0.792</b></td><td>0.778</td><td>0.661</td></tr><tr><td><br />yeast-2_vs_8</td><td>0.525</td><td>0.732</td><td>0.715</td><td>0.750</td><td>0.739</td><td>0.747</td><td>0.742</td><td><b>0.819</b></td><td>0.745</td><td>0.739</td></tr><tr><td><br />yeast4</td><td>0.724</td><td>0.708</td><td>0.745</td><td>0.756</td><td>0.732</td><td>0.681</td><td>0.733</td><td><b>0.799</b></td><td>0.681</td><td>0.705</td></tr><tr><td><br />平均值</td><td>0.696</td><td>0.732</td><td>0.757</td><td>0.732</td><td>0.771</td><td>0.712</td><td>0.792</td><td><b>0.821</b></td><td>0.732</td><td>0.772</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h3 id="122" name="122" class="anchor-tag">4 结束语</h3>
                <div class="p1">
                    <p id="123">本文在模糊决策树的基础上, 将犹豫模糊集理论应用于模糊决策树中, 设计一种犹豫模糊决策树分类器。通过对少数类样本进行过采样处理, 降低了训练样本的高度非均衡比, 并使用2种隶属度函数进行数据的模糊化处理, 尝试不同的组合方式进行训练和测试。实验结果表明, 本文算法对于二分类非均衡数据集的分类性能较好。后续将研究针对多分类非均衡数据集的分类, 并进一步提高本文算法的分类性能。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="141">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201801024&amp;v=MTI0NTlHRnJDVVJMT2VaZVJxRnkvZ1ViN0FMelRaWkxHNEg5bk1ybzlIWUlRS0RIODR2UjRUNmo1NE8zenFxQnQ=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9xck9nSjc1VXR3SDJsaTcyc24rQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> 叶枫, 丁峰.不平衡数据分类研究及其应用[J].计算机应用与软件, 2018, 35 (1) :132-136, 205.
                            </a>
                        </p>
                        <p id="143">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJA201010007&amp;v=Mjc1MjRSTE9lWmVScUZ5L2dVYjdBTHo3QmI3RzRIOUhOcjQ5Rlk0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1U=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9xck9nSjc1VXR3SDJsaTcyc24rQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> 翟云, 杨炳儒, 曲武.不平衡类数据挖掘研究综述[J].计算机科学, 2010, 37 (10) :27-32.
                            </a>
                        </p>
                        <p id="145">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=SMOTE: synthetic minority over-sampling technique">

                                <b>[3]</b> CHAWLA N V, BOWYER K W, HALL L O, et al.SMOTE:synthetic minority over-sampling technique[J].Journal of Artificial Intelligence Research, 2002, 16 (1) :321-357.
                            </a>
                        </p>
                        <p id="147">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Borderline-SMOTE:a new over-sampling method in imbalanced data sets learning">

                                <b>[4]</b> HAN Hui, WANG Wenyuan, MAO Binghuan.Borderline-SMOTE:a new over-sampling method in imbalanced data sets learning[J].Lecture Notes in Computer Science, 2005, 3644 (5) :878-887.
                            </a>
                        </p>
                        <p id="149">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM388195169BFE0E7494828A3B5E98AD35&amp;v=Mjc2MjNNeUJJYTdqZC9RQTdoM2hkQWNMcmxNYm1hQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQ3BiUTM1TnhodzcyOXdhQT1OaWZJWTdDd0Z0REZxbzVEYlpsNWVYeA==&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9xck9nSjc1VXR3SDJsaTcyc24rQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b> BATISTA G, PRATI R C, MONARD M C.A study of the behavior of several methods for balancing machine learning training data[J].SIGKDD Explorations, 2004, 6 (1) :20-29.
                            </a>
                        </p>
                        <p id="151">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011600739373&amp;v=MDg3NDVycVFUTW53WmVadUh5am1VTHZJSjFzV2F4bz1OaWZPZmJLN0h0RE5xWTlGWStnR0QzczZvQk1UNlQ0UFFIL2lyUmRHZQ==&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9xck9nSjc1VXR3SDJsaTcyc24rQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b> SUN Y, KAMEL M S, WONG A K C, et al.Cost-sensitive boosting for classification of imbalanced data[J].Pattern Recognition, 2007, 40 (12) :3358-3378.
                            </a>
                        </p>
                        <p id="153">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJA2017S2020&amp;v=MTQ5OTZVYjdBTHo3QmI3RzRIOWF2clk5SFpJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVScUZ5L2c=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9xck9nSjc1VXR3SDJsaTcyc24rQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b> 师彦文, 王宏杰.基于新型不纯度度量的代价敏感随机森林分类器[J].计算机科学, 2017, 44 (S2) :98-101.
                            </a>
                        </p>
                        <p id="155">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=CGQJ201803035&amp;v=MjM0MDRaTEc0SDluTXJJOUdZWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUnFGeS9nVWI3QUppcmE=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9xck9nSjc1VXR3SDJsaTcyc24rQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b> 刘东启, 陈志坚.适用于不平衡数据集分类的改进SVM算法[J].传感器与微系统, 2018, 37 (3) :1-4.
                            </a>
                        </p>
                        <p id="157">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=KZLY201110024&amp;v=MTAxMjllUnFGeS9nVWI3QUxqZkhkN0c0SDlETnI0OUhZSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVo=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9xck9nSjc1VXR3SDJsaTcyc24rQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b> 韩敏, 朱新荣.不平衡数据分类的混合算法[J].控制理论与应用, 2011, 28 (10) :1485-1489.
                            </a>
                        </p>
                        <p id="159">
                            <a id="bibliography_10" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSGG201030013&amp;v=MTIzNjRhYkc0SDlIUHI0OUVaNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUnFGeS9nVWI3QUx6N00=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9xck9nSjc1VXR3SDJsaTcyc24rQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[10]</b> 李明方, 张化祥.针对不平衡数据的Bagging的改进算法[J].计算机工程与应用, 2013, 49 (2) :40-42.
                            </a>
                        </p>
                        <p id="161">
                            <a id="bibliography_11" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SIJD&amp;filename=SIJD15112600038645&amp;v=MDg5NzJheG89TmlUQmFySzlIOURPcVk5RlpPZ0hDbmc4b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VTHZJSjFzVw==&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9xck9nSjc1VXR3SDJsaTcyc24rQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[11]</b> MEHDIZADEH M, EFTEKHARI M.Generating fuzzy rule base classifier for highly imbalanced datasets using a hybrid of evolutionary algorithms and subtractive clustering[J].Journal of Intelligent and Fuzzy Systems, 2014, 27 (6) :3033-3046.
                            </a>
                        </p>
                        <p id="163">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning fuzzy classification rules from imbalanced datasets using multi-objective evolutionary algorithm">

                                <b>[12]</b> HINOJOSA C E, CAMARGO H A, TÚPAC V Y J.Learning fuzzy classification rules from imbalanced datasets using multi-objective evolutionary algorithm[C]//Proceedings of Latin America Congress on Computational Intelligence.Washington D.C., USA:IEEE Press, 2015:1-6.
                            </a>
                        </p>
                        <p id="165">
                            <a id="bibliography_13" >
                                    <b>[13]</b>
                                 ZADEH L A.Fuzzy sets[J].Information Control, 1965, 8 (3) :338-353.
                            </a>
                        </p>
                        <p id="167">
                            <a id="bibliography_14" >
                                    <b>[14]</b>
                                 TORRA V.Hesitant fuzzy sets[J].International Journal of Intelligent Systems, 2010, 25 (6) :529-539.
                            </a>
                        </p>
                        <p id="169">
                            <a id="bibliography_15" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14010600277894&amp;v=MjYyODBRSC9pclJkR2VycVFUTW53WmVadUh5am1VTHZJSjFzV2F4bz1OaWZPZmJLOEh0RE1xWTlGWnV3SUJIVTlvQk1UNlQ0UA==&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9xck9nSjc1VXR3SDJsaTcyc24rQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[15]</b> CHEN Na, XU Zeshui, XIA Meimei.Correlation coeffi-cients of hesitant sets and their applications to clustering analysis[J].Applied Mathematical Modelling, 2013, 37 (4) :2197-2211.
                            </a>
                        </p>
                        <p id="171">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fuzzy decision trees by Fuzzy-ID3 algorithm and its application to diagnosis systems">

                                <b>[16]</b> UMANO M, OKAMOLO H, HATONO I, et al.Fuzzy decision trees by Fuzzy ID3 algorithm and its application to diagnosis system[C]//Proceedings of the 3rd IEEE International Conference on Fuzzy Systems.Washington D.C., USA:IEEE Press, 1994:2113-2118.
                            </a>
                        </p>
                        <p id="173">
                            <a id="bibliography_17" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012300674618&amp;v=MTY0MjlycVFUTW53WmVadUh5am1VTHZJSjFzV2F4bz1OaWZPZmJLN0h0RE9ySTlGWXV3TENuMHhvQk1UNlQ0UFFIL2lyUmRHZQ==&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9xck9nSjc1VXR3SDJsaTcyc24rQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[17]</b> YUAN Yufei, SHAW M J.Induction of fuzzy decision trees[J].Fuzzy Sets and Systems, 1995, 69 (2) :125-139.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->

            <div class="btn-downloads">
                <span>
                    <a class="caj" target="_blank" href="http://kns.cnki.net/kns/download.aspx?filename=0c2cZ9kdGFUYmh2K6t2ZLBzSUdHc2sySvNGTNVGUiVVQwAHZKp1NrsmdzsUTWVnehd2Tv4mdah2SP5mdHp2UXplVJ9Wd3UmbzEUSMdFSZBXSXxkejtiaY5Ua2lnNRJ1NtZzcSVkc5FEb1dHOvFGWWpVaoBVcXRkS&tablename=CJFDLAST2019">CAJ下载</a>
                    <a class="pdf" target="_blank" href="http://kns.cnki.net/kns/download.aspx?filename=0c2cZ9kdGFUYmh2K6t2ZLBzSUdHc2sySvNGTNVGUiVVQwAHZKp1NrsmdzsUTWVnehd2Tv4mdah2SP5mdHp2UXplVJ9Wd3UmbzEUSMdFSZBXSXxkejtiaY5Ua2lnNRJ1NtZzcSVkc5FEb1dHOvFGWWpVaoBVcXRkS&tablename=CJFDLAST2019&dflag=pdfdown">PDF下载</a>
                </span>
                <p>永久保存本文,请下载至本地</p>
            </div>


    </div>

        <input id="fileid" type="hidden" value="JSJC201908013" />
        <input id="dpi" type="hidden" value="600" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcTdWa2FjcW9xck9nSjc1VXR3SDJsaTcyc24rQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201908013&amp;v=MjE3ODRiYkc0SDlqTXA0OUVaNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUnFGeS9nVWI3QUx6N0I=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9xck9nSjc1VXR3SDJsaTcyc24rQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
            <a class="icon" id="copytext" target="_blank" href="http://x.cnki.net/search/common/testlunbo?dbcode=CJFD&amp;tablename=CJFDLAST2019&amp;filename=JSJC201908013&amp;filesourcetype=1">划线</a>
            <a class="icon" id="copytext" target="_blank" href="http://x.cnki.net/search/common/testlunbo?dbcode=CJFD&amp;tablename=CJFDLAST2019&amp;filename=JSJC201908013&amp;filesourcetype=1">笔记</a>
            <a class="icon" id="copytext" target="_blank" href="http://x.cnki.net/search/common/testlunbo?dbcode=CJFD&amp;tablename=CJFDLAST2019&amp;filename=JSJC201908013&amp;filesourcetype=1">文摘</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
