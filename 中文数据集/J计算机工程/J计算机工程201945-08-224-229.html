<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637129059117775000%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJC201908037%26RESULT%3d1%26SIGN%3dCy4NrOnwwNs8Z6h1Jp27M9WISq4%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201908037&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201908037&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201908037&amp;v=MjIxNDF6N0JiYkc0SDlqTXA0OUdZNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUnFGQ2ptVjc3QUw=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#73" data-title="0 概述 ">0 概述</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#76" data-title="1 相关工作 ">1 相关工作</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#81" data-title="2 共识性信息与主观性信息 ">2 共识性信息与主观性信息</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#84" data-title="2.1 共识性信息">2.1 共识性信息</a></li>
                                                <li><a href="#91" data-title="2.2 主观性信息">2.2 主观性信息</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#94" data-title="3 融合模型 ">3 融合模型</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#107" data-title="4 实验结果与分析 ">4 实验结果与分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#110" data-title="4.1 模型评估标准">4.1 模型评估标准</a></li>
                                                <li><a href="#117" data-title="4.2 模型验证">4.2 模型验证</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#127" data-title="5 结束语 ">5 结束语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#83" data-title="&lt;b&gt;图1 预测模型系统框架&lt;/b&gt;"><b>图1 预测模型系统框架</b></a></li>
                                                <li><a href="#88" data-title="&lt;b&gt;图2 标准偏差分布&lt;/b&gt;"><b>图2 标准偏差分布</b></a></li>
                                                <li><a href="#90" data-title="&lt;b&gt;表1 FaceScrub数据库标签一致性测试结果&lt;/b&gt;"><b>表1 FaceScrub数据库标签一致性测试结果</b></a></li>
                                                <li><a href="#105" data-title="&lt;b&gt;图3 融合CNN网络架构&lt;/b&gt;"><b>图3 融合CNN网络架构</b></a></li>
                                                <li><a href="#109" data-title="&lt;b&gt;图4 测试者&lt;i&gt;A&lt;/i&gt;和测试者&lt;i&gt;B&lt;/i&gt;预测结果&lt;/b&gt;"><b>图4 测试者<i>A</i>和测试者<i>B</i>预测结果</b></a></li>
                                                <li><a href="#115" data-title="&lt;b&gt;表2 特征融合结果对比&lt;/b&gt;"><b>表2 特征融合结果对比</b></a></li>
                                                <li><a href="#119" data-title="&lt;b&gt;表3 单一方法与融合方法性能对比结果&lt;/b&gt;"><b>表3 单一方法与融合方法性能对比结果</b></a></li>
                                                <li><a href="#122" data-title="&lt;b&gt;表4 各融合模型在FaceScrub子集上的性能对比结果&lt;/b&gt;"><b>表4 各融合模型在FaceScrub子集上的性能对比结果</b></a></li>
                                                <li><a href="#125" data-title="&lt;b&gt;表5 各方法性能对比结果&lt;/b&gt;"><b>表5 各方法性能对比结果</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="162">


                                    <a id="bibliography_1" title=" WILLIS J, TODOROV A.First impressions:making up your mind after a 100-ms exposure to a face[J].Psychological Science, 2010, 17 (7) :592-598." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=First Impressions: Making Up Your Mind After a 100-Ms Exposure to a Face">
                                        <b>[1]</b>
                                         WILLIS J, TODOROV A.First impressions:making up your mind after a 100-ms exposure to a face[J].Psychological Science, 2010, 17 (7) :592-598.
                                    </a>
                                </li>
                                <li id="164">


                                    <a id="bibliography_2" title=" LANGLOIS J H, KALAKANIS L, RUBENSTEIN A J, et al.Maxims or myths of beauty?A meta-analytic and theoretical review[J].Psychological Bulletin, 2000, 126 (3) :390-423." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Maxims or myths of beauty? A meta-analytic and theoretical review">
                                        <b>[2]</b>
                                         LANGLOIS J H, KALAKANIS L, RUBENSTEIN A J, et al.Maxims or myths of beauty?A meta-analytic and theoretical review[J].Psychological Bulletin, 2000, 126 (3) :390-423.
                                    </a>
                                </li>
                                <li id="166">


                                    <a id="bibliography_3" title=" AARABI P, HUGHES D, MOHAJER K, et al.The automatic measurement of facial beauty[C]//Proceedings of IEEE International Conference on Systems, Man, and Cybernetics.Washington D.C., USA:IEEE Press, 2001:2644-2647." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The Automatic Measurement of Facial Beauty">
                                        <b>[3]</b>
                                         AARABI P, HUGHES D, MOHAJER K, et al.The automatic measurement of facial beauty[C]//Proceedings of IEEE International Conference on Systems, Man, and Cybernetics.Washington D.C., USA:IEEE Press, 2001:2644-2647.
                                    </a>
                                </li>
                                <li id="168">


                                    <a id="bibliography_4" title=" ALTWAIJRY H, BELONGIE S.Relative ranking of facial attractiveness[C]//Proceedings of 2013 IEEE Workshop on Applications of Computer Vision.Washington D.C., USA:IEEE Press, 2013:117-124." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Relative ranking of facial attractiveness">
                                        <b>[4]</b>
                                         ALTWAIJRY H, BELONGIE S.Relative ranking of facial attractiveness[C]//Proceedings of 2013 IEEE Workshop on Applications of Computer Vision.Washington D.C., USA:IEEE Press, 2013:117-124.
                                    </a>
                                </li>
                                <li id="170">


                                    <a id="bibliography_5" title=" ROTHE R, TIMOFTE R, GOOL L V.Some like it hot-visual guidance for preference prediction[C]//Proceedings of 2016 IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2016:5553-5561." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Some Like It Hot—Visual Guidance for Preference Prediction">
                                        <b>[5]</b>
                                         ROTHE R, TIMOFTE R, GOOL L V.Some like it hot-visual guidance for preference prediction[C]//Proceedings of 2016 IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2016:5553-5561.
                                    </a>
                                </li>
                                <li id="172">


                                    <a id="bibliography_6" title=" YAN Mengjia, DUAN Yuron, DENG Siqi, et al.Facial beauty assessment under unconstrained conditions[C]//Proceedings of International Conference on Electronics, Computers and Artificial Intelligence.Washington D.C., USA:IEEE Press, 2017:1-6." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Facial beauty assessment under unconstrained conditions">
                                        <b>[6]</b>
                                         YAN Mengjia, DUAN Yuron, DENG Siqi, et al.Facial beauty assessment under unconstrained conditions[C]//Proceedings of International Conference on Electronics, Computers and Artificial Intelligence.Washington D.C., USA:IEEE Press, 2017:1-6.
                                    </a>
                                </li>
                                <li id="174">


                                    <a id="bibliography_7" title=" EISENTHAL Y, DROR G, RUPPIN E.Facial attractive-ness:beauty and the machine[J].Neural Computation, 2006, 18 (1) :119-142." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJBK&amp;filename=SJBK15090500012451&amp;v=MTk3MTB2TElGMFFheG89TmlmSlpiSzlIdGpNcW85RlpPb05DSGs0b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VTA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                         EISENTHAL Y, DROR G, RUPPIN E.Facial attractive-ness:beauty and the machine[J].Neural Computation, 2006, 18 (1) :119-142.
                                    </a>
                                </li>
                                <li id="176">


                                    <a id="bibliography_8" title=" MAO Huiyun, JIN Lianwen, DU Minghui.Automatic classifica-tion of Chinese female facial beauty using support vector machine[C]//Proceedings of IEEE International Conference on Systems, Man and Cybernetics.Washington D.C., USA:IEEE Press, 2009:4842-4846." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Automatic classification of Chinese female facial beauty using support vector machine">
                                        <b>[8]</b>
                                         MAO Huiyun, JIN Lianwen, DU Minghui.Automatic classifica-tion of Chinese female facial beauty using support vector machine[C]//Proceedings of IEEE International Conference on Systems, Man and Cybernetics.Washington D.C., USA:IEEE Press, 2009:4842-4846.
                                    </a>
                                </li>
                                <li id="178">


                                    <a id="bibliography_9" title=" 毛慧芸.人脸美丽吸引力的特征分析与机器学习[D].广州:华南理工大学, 2011." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CDFD&amp;filename=1011188499.nh&amp;v=MjI0NDZLd0Z0WEZwcEViUElRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJxRkNqbVY3N0FWRjI2SDc=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                         毛慧芸.人脸美丽吸引力的特征分析与机器学习[D].广州:华南理工大学, 2011.
                                    </a>
                                </li>
                                <li id="180">


                                    <a id="bibliography_10" title=" 陈伊力.基于数据驱动的人脸美化[D].广州:华南理工大学, 2012." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=1012451946.nh&amp;v=MDQ5MTZWNzdBVkYyNkhMZTlIOWpJcVpFYlBJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVScUZDam0=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[10]</b>
                                         陈伊力.基于数据驱动的人脸美化[D].广州:华南理工大学, 2012.
                                    </a>
                                </li>
                                <li id="182">


                                    <a id="bibliography_11" title=" ZHANG D, CHEN Fangmei, XU Yong.Data-driven facial beauty analysis:prediction, retrieval and manipulation[M].Berlin, Germany:Springer, 2016." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Data-driven facial beauty analysis:prediction,retrieval and manipulation">
                                        <b>[11]</b>
                                         ZHANG D, CHEN Fangmei, XU Yong.Data-driven facial beauty analysis:prediction, retrieval and manipulation[M].Berlin, Germany:Springer, 2016.
                                    </a>
                                </li>
                                <li id="184">


                                    <a id="bibliography_12" title=" GAN Junying, LI Lichen, ZHAI Yikui.Facial beauty prediction model based on self-taught learning and convolutional restricted Boltzmann machine[C]//Proceedings of International Conference on Machine Learning and Cybernetics.Washington D.C., USA:IEEE Press, 2015:844-849." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Facial beauty prediction model based on self-taught learning and convolutional restricted Boltzmann machine">
                                        <b>[12]</b>
                                         GAN Junying, LI Lichen, ZHAI Yikui.Facial beauty prediction model based on self-taught learning and convolutional restricted Boltzmann machine[C]//Proceedings of International Conference on Machine Learning and Cybernetics.Washington D.C., USA:IEEE Press, 2015:844-849.
                                    </a>
                                </li>
                                <li id="186">


                                    <a id="bibliography_13" title=" GAN Junying, ZHOU Lei, ZHAI Yikui.A study for facial beauty prediction model[C]//Proceedings of International Conference on Wavelet Analysis and Pattern Recognition.Washington D.C., USA:IEEE Press, 2015:8-13." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Study for Facial Beauty Prediction Model">
                                        <b>[13]</b>
                                         GAN Junying, ZHOU Lei, ZHAI Yikui.A study for facial beauty prediction model[C]//Proceedings of International Conference on Wavelet Analysis and Pattern Recognition.Washington D.C., USA:IEEE Press, 2015:8-13.
                                    </a>
                                </li>
                                <li id="188">


                                    <a id="bibliography_14" title=" 李远豪, 甘俊英.深度自编码器用于人脸美丽吸引力预测的研究[J].五邑大学学报 (自然科学版) , 2014, 28 (4) :49-54." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=WYDW201404011&amp;v=MDY5OTR6cXFCdEdGckNVUkxPZVplUnFGQ2ptVjc3QU1qVFBlYkc0SDlYTXE0OUVaWVFLREg4NHZSNFQ2ajU0TzM=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[14]</b>
                                         李远豪, 甘俊英.深度自编码器用于人脸美丽吸引力预测的研究[J].五邑大学学报 (自然科学版) , 2014, 28 (4) :49-54.
                                    </a>
                                </li>
                                <li id="190">


                                    <a id="bibliography_15" title=" CHOUDHARY G, GANDHI T K.Indexing facial attrac-tiveness and well beings using machine learning[C]//Proceedings of 2016 IEEE Region 10 Humanitarian Technology Conference.Washington D.C., USA:IEEE Press, 2016:1-6." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Indexing facial attrac-tiveness and well beings using machine learning">
                                        <b>[15]</b>
                                         CHOUDHARY G, GANDHI T K.Indexing facial attrac-tiveness and well beings using machine learning[C]//Proceedings of 2016 IEEE Region 10 Humanitarian Technology Conference.Washington D.C., USA:IEEE Press, 2016:1-6.
                                    </a>
                                </li>
                                <li id="192">


                                    <a id="bibliography_16" title=" GRAY D, YU Kai, XU Wei, et al.Predicting facial beauty without landmarks[C]//Proceedings of European Conference on Computer Vision.Berlin, Germany:Springer, 2010:434-447." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Predicting Facial Beauty without Landmarks">
                                        <b>[16]</b>
                                         GRAY D, YU Kai, XU Wei, et al.Predicting facial beauty without landmarks[C]//Proceedings of European Conference on Computer Vision.Berlin, Germany:Springer, 2010:434-447.
                                    </a>
                                </li>
                                <li id="194">


                                    <a id="bibliography_17" title=" XU Jie, JIN Lianwen, LIANG Lingyu, et al.Facial attractiveness prediction using psychologically inspired convolutional neural network (PI-CNN) [C]//Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing.Washington D.C., USA:IEEE Press, 2017:1657-1661." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Facial attractiveness prediction using psychologically inspired convolutional neural network (PI-CNN)">
                                        <b>[17]</b>
                                         XU Jie, JIN Lianwen, LIANG Lingyu, et al.Facial attractiveness prediction using psychologically inspired convolutional neural network (PI-CNN) [C]//Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing.Washington D.C., USA:IEEE Press, 2017:1657-1661.
                                    </a>
                                </li>
                                <li id="196">


                                    <a id="bibliography_18" title=" KALAYCI S, EKENEL H K, GUNES H.Automatic analysis of facial attractiveness from video[C]//Proceedings of IEEE International Conference on Image Processing.Washington D.C., USA:IEEE Press, 2015:4191-4195." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Automatic analysis of facial attractiveness from video">
                                        <b>[18]</b>
                                         KALAYCI S, EKENEL H K, GUNES H.Automatic analysis of facial attractiveness from video[C]//Proceedings of IEEE International Conference on Image Processing.Washington D.C., USA:IEEE Press, 2015:4191-4195.
                                    </a>
                                </li>
                                <li id="198">


                                    <a id="bibliography_19" title=" LU Xi, CHANG Xiaobin, XIE Xiaohua, et al.Facial skin beautification via sparse representation over learned layer dictionary[C]//Proceedings of International Joint Conference on Neural Networks.Washington D.C., USA:IEEE Press, 2016:2534-2539." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Facial skin beautification via sparse representation over learned layer dictionary">
                                        <b>[19]</b>
                                         LU Xi, CHANG Xiaobin, XIE Xiaohua, et al.Facial skin beautification via sparse representation over learned layer dictionary[C]//Proceedings of International Joint Conference on Neural Networks.Washington D.C., USA:IEEE Press, 2016:2534-2539.
                                    </a>
                                </li>
                                <li id="200">


                                    <a id="bibliography_20" title=" LIU Shu, LI Bo, FAN Yangyu, et al.Facial attractiveness computation by label distribution learning with deep CNN and geometric features[C]//Proceedings of IEEE International Conference on Multimedia and Expo.Washington D.C., USA:IEEE Press, 2017:1344-1349." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Facial attractiveness computation by label distribution learning with deep CNN and geometric features">
                                        <b>[20]</b>
                                         LIU Shu, LI Bo, FAN Yangyu, et al.Facial attractiveness computation by label distribution learning with deep CNN and geometric features[C]//Proceedings of IEEE International Conference on Multimedia and Expo.Washington D.C., USA:IEEE Press, 2017:1344-1349.
                                    </a>
                                </li>
                                <li id="202">


                                    <a id="bibliography_21" title=" WHITEHILL J, MOVELLAN J R.Personalized facial attractiveness prediction[C]//Proceedings of IEEE International Conference on Automatic Face and Gesture Recognition.Washington D.C., USA:IEEE Press, 2008:1-7." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Personalized facial attractiveness prediction">
                                        <b>[21]</b>
                                         WHITEHILL J, MOVELLAN J R.Personalized facial attractiveness prediction[C]//Proceedings of IEEE International Conference on Automatic Face and Gesture Recognition.Washington D.C., USA:IEEE Press, 2008:1-7.
                                    </a>
                                </li>
                                <li id="204">


                                    <a id="bibliography_22" title=" DANTCHEVA A, DUGELAY J L.Assessment of female facial beauty based on anthropometric, non-permanent and acquisition characteristics[M].[S.l.]:Kluwer Academic Publishers, 2015." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Assessment of female facial beauty based on anthropometric,non-permanent and acquisition characteristics">
                                        <b>[22]</b>
                                         DANTCHEVA A, DUGELAY J L.Assessment of female facial beauty based on anthropometric, non-permanent and acquisition characteristics[M].[S.l.]:Kluwer Academic Publishers, 2015.
                                    </a>
                                </li>
                                <li id="206">


                                    <a id="bibliography_23" title=" 王少标.基于个性与共性探讨的人脸美化[D].合肥:中国科学技术大学, 2016." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=1016103350.nh&amp;v=MTA4OTh0R0ZyQ1VSTE9lWmVScUZDam1WNzdBVkYyNkdMSzRIZExKcjVFYlBJUUtESDg0dlI0VDZqNTRPM3pxcUI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[23]</b>
                                         王少标.基于个性与共性探讨的人脸美化[D].合肥:中国科学技术大学, 2016.
                                    </a>
                                </li>
                                <li id="208">


                                    <a id="bibliography_24" title=" 舒畅, 丁晓青, 方驰.多特征局部与全局融合的人脸识别方法[J].计算机工程, 2011, 37 (19) :145-147." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201119049&amp;v=MTYxMTdCdEdGckNVUkxPZVplUnFGQ2ptVjc3QUx6N0JiYkc0SDlETnBvOUJiWVFLREg4NHZSNFQ2ajU0TzN6cXE=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[24]</b>
                                         舒畅, 丁晓青, 方驰.多特征局部与全局融合的人脸识别方法[J].计算机工程, 2011, 37 (19) :145-147.
                                    </a>
                                </li>
                                <li id="210">


                                    <a id="bibliography_25" title=" 黄忠, 胡敏, 刘娟.基于多特征决策级融合的表情识别方法[J].计算机工程, 2015, 41 (10) :171-176." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201510033&amp;v=MjA2MjZPZVplUnFGQ2ptVjc3QUx6N0JiYkc0SDlUTnI0OUdaNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkw=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[25]</b>
                                         黄忠, 胡敏, 刘娟.基于多特征决策级融合的表情识别方法[J].计算机工程, 2015, 41 (10) :171-176.
                                    </a>
                                </li>
                                <li id="212">


                                    <a id="bibliography_26" title=" PEARSON K.Notes on the history of correlation[J].Biometrika, 1920, 13 (1) :25-45." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Note on the history of correlation">
                                        <b>[26]</b>
                                         PEARSON K.Notes on the history of correlation[J].Biometrika, 1920, 13 (1) :25-45.
                                    </a>
                                </li>
                                <li id="214">


                                    <a id="bibliography_27" title=" HAGHIGHAT M, ABDEL-MOTTALEB M, ALHALABI W.Fully automatic face normalization and single sample face recognition in unconstrained environments[J].Expert Systems with Applications, 2016, 47 (5) :23-34." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES954B818CF976DFD7ECF5E3D4260252E2&amp;v=MTIyNTJFdUlJQ2doUHV4Rm1tVWw0UFh5V3FCQXplYkNSUjgrZENPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkNwYlEzNU54aXhMdTd3YUE9TmlmT2ZicTlHcVBFcm9jMg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[27]</b>
                                         HAGHIGHAT M, ABDEL-MOTTALEB M, ALHALABI W.Fully automatic face normalization and single sample face recognition in unconstrained environments[J].Expert Systems with Applications, 2016, 47 (5) :23-34.
                                    </a>
                                </li>
                                <li id="216">


                                    <a id="bibliography_28" title=" MAO Huiyun, JIN Lianwen, DU Minghui.Facial beauty classification based on geometric features and c4.5[J].Pattern Recognition and Artificial Intelligence, 2010, 23 (6) :809-814." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201006011&amp;v=MDUxOTlHRnJDVVJMT2VaZVJxRkNqbVY3N0FLRDdZYkxHNEg5SE1xWTlFWllRS0RIODR2UjRUNmo1NE8zenFxQnQ=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[28]</b>
                                         MAO Huiyun, JIN Lianwen, DU Minghui.Facial beauty classification based on geometric features and c4.5[J].Pattern Recognition and Artificial Intelligence, 2010, 23 (6) :809-814.
                                    </a>
                                </li>
                                <li id="218">


                                    <a id="bibliography_29" title=" XIE Duorui, LIANG Lingyu, JIN Lianwen, et al.SCUT-FBP:a benchmark dataset for facial beauty perception[C]//Proceedings of IEEE International Conference on Systems, Man, and Cybernetics.Washington D.C., USA:IEEE Press, 2015:1821-1826." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=SCUT-FBP:a benchmark dataset for facial beauty perception">
                                        <b>[29]</b>
                                         XIE Duorui, LIANG Lingyu, JIN Lianwen, et al.SCUT-FBP:a benchmark dataset for facial beauty perception[C]//Proceedings of IEEE International Conference on Systems, Man, and Cybernetics.Washington D.C., USA:IEEE Press, 2015:1821-1826.
                                    </a>
                                </li>
                                <li id="220">


                                    <a id="bibliography_30" title=" NG H, WINKLER S.A data-driven approach to cleaning large face datasets[C]//Proceedings of IEEE International Conference on Image Processing.Washington D.C., USA:IEEE Press, 2015:343-347." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Data-driven Approach to Cleaning Large Face Datasets">
                                        <b>[30]</b>
                                         NG H, WINKLER S.A data-driven approach to cleaning large face datasets[C]//Proceedings of IEEE International Conference on Image Processing.Washington D.C., USA:IEEE Press, 2015:343-347.
                                    </a>
                                </li>
                                <li id="222">


                                    <a id="bibliography_31" title=" SZABO N S, TANAKA R I.Residue arithmetic and its applications to computer technology[J].SIAM Review, 2009, 11 (1) :143-150." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Residue arithmetic and its applications to computer technology">
                                        <b>[31]</b>
                                         SZABO N S, TANAKA R I.Residue arithmetic and its applications to computer technology[J].SIAM Review, 2009, 11 (1) :143-150.
                                    </a>
                                </li>
                                <li id="224">


                                    <a id="bibliography_32" title=" PESARAN M H, SMITH R J.A generalized R&lt;sup&gt;2&lt;/sup&gt;criterion for regression models estimated by the instrumental variables method[J].Econometrica, 1994, 62 (3) :705-710." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A generalized $R\sp 2$ criterion for regression models estimated by the instrumental variables method">
                                        <b>[32]</b>
                                         PESARAN M H, SMITH R J.A generalized R&lt;sup&gt;2&lt;/sup&gt;criterion for regression models estimated by the instrumental variables method[J].Econometrica, 1994, 62 (3) :705-710.
                                    </a>
                                </li>
                                <li id="226">


                                    <a id="bibliography_33" title=" LIU Shu, FAN Yangyu, GUO Zhe, et al.2.5d facial attractiveness computation based on data-driven geometric ratios[C]//Proceedings of International Conference on Intelligent Science and Big Data Engineering.Washington D.C., USA:IEEE Press, 2015:564-573." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=2.5D facial attractiveness computation based on data-driven geometric ratios">
                                        <b>[33]</b>
                                         LIU Shu, FAN Yangyu, GUO Zhe, et al.2.5d facial attractiveness computation based on data-driven geometric ratios[C]//Proceedings of International Conference on Intelligent Science and Big Data Engineering.Washington D.C., USA:IEEE Press, 2015:564-573.
                                    </a>
                                </li>
                                <li id="228">


                                    <a id="bibliography_34" title=" WANG Shuyang, SHAO Ming, FU Yun.Attractive or not?:beauty prediction with attractiveness-aware encoders and robust late fusion[C]//Proceedings of ACM International Conference on Multimedia.New York, USA:ACM Press, 2014:805-808." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Attractive or Not?:Beauty prediction with attractiveness-aware encoders and robust late fusion">
                                        <b>[34]</b>
                                         WANG Shuyang, SHAO Ming, FU Yun.Attractive or not?:beauty prediction with attractiveness-aware encoders and robust late fusion[C]//Proceedings of ACM International Conference on Multimedia.New York, USA:ACM Press, 2014:805-808.
                                    </a>
                                </li>
                                <li id="230">


                                    <a id="bibliography_35" title=" XU Jie, JIN Lianwen, LIANG Lingyu, et al.A new humanlike facial attractiveness predictor with cascaded fine-tuning deep learning model[J].Computer Science, 2015, 70 (1) :45-79." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A new humanlike facial attractiveness predictor with cascaded fine-tuning deep learning model">
                                        <b>[35]</b>
                                         XU Jie, JIN Lianwen, LIANG Lingyu, et al.A new humanlike facial attractiveness predictor with cascaded fine-tuning deep learning model[J].Computer Science, 2015, 70 (1) :45-79.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJC" target="_blank">计算机工程</a>
                2019,45(08),224-229 DOI:10.19678/j.issn.1000-3428.0051535            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>融合主观性与共识性信息的人脸吸引力预测</b></span>
 <span class="shoufa"></span>                                     </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%8E%E9%87%91%E8%94%93&amp;code=40274947&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">李金蔓</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%B1%AA%E5%89%91%E9%B8%A3&amp;code=08922930&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">汪剑鸣</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%87%91%E5%85%89%E6%B5%A9&amp;code=40274948&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">金光浩</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%A4%A9%E6%B4%A5%E5%B7%A5%E4%B8%9A%E5%A4%A7%E5%AD%A6%E7%94%B5%E5%AD%90%E4%B8%8E%E4%BF%A1%E6%81%AF%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0050423&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">天津工业大学电子与信息工程学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%A4%A9%E6%B4%A5%E5%B7%A5%E4%B8%9A%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF%E5%AD%A6%E9%99%A2&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">天津工业大学计算机科学与技术学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>为对特定对象的个性化审美偏好进行预测, 将机器学习与深度学习相结合, 提出融合共识性与主观性偏好信息的个性化人脸吸引力评估方法。从数据集中收集群体评分形成共识性信息, 构建个人评分预测模型以反映主观性信息。结合这2种信息并发挥共识性信息的普遍性优势以及主观性信息的独特性, 从而构建个性化人脸吸引力预测模型。在公开的SCUT-FBP和FaceScrub数据集上进行实验, 结果表明, 该方法的Pearson相关系数高达0.90, 残差值低至0.25。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%89%B9%E5%BE%81%E8%9E%8D%E5%90%88&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">特征融合;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%B8%AA%E6%80%A7%E5%8C%96%E9%A2%84%E6%B5%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">个性化预测;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%BA%BA%E8%84%B8%E5%90%B8%E5%BC%95%E5%8A%9B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">人脸吸引力;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%85%B1%E8%AF%86%E6%80%A7%E4%BF%A1%E6%81%AF&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">共识性信息;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%B8%BB%E8%A7%82%E6%80%A7%E4%BF%A1%E6%81%AF&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">主观性信息;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Pearson%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Pearson相关系数;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    李金蔓 (1993—) , 女, 硕士研究生, 主研方向为模式识别、计算机视觉;E-mail: tjpuljm@ 126. com;
                                </span>
                                <span>
                                    汪剑鸣, 教授、博士;;
                                </span>
                                <span>
                                    *金光浩 (通信作者) , 讲师、博士。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-05-14</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金 (61771340, 61302127, 61403278);</span>
                                <span>中国博士后科学基金 (2015M570228);</span>
                                <span>天津市自然科学基金 (16JCYBJC4 2300);</span>
                                <span>天津市应用基础与前沿技术研究计划 (15JCYBJC16600);</span>
                                <span>天津市高等学校创新团队培养计划 (TD13-5032);</span>
                    </p>
            </div>
                    <h1><b>Facial Attractiveness Prediction Integrating Subjective and Consensus Information</b></h1>
                    <h2>
                    <span>LI Jinman</span>
                    <span>WANG Jianming</span>
                    <span>JIN Guanghao</span>
            </h2>
                    <h2>
                    <span>School of Electronics and Information Engineering, Tianjin Polytechnic University</span>
                    <span>School of Computer Science and Technology, Tianjin Polytechnic University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>In order to predict the personalized aesthetic preferences of specific objects, machine learning and depth learning are combined, and a personalized facial attractiveness evaluation method that integrating consensus and subjective preference information is proposed.Group ratings are collected from the data set to form consensus information, and the prediction model of individual ratings is constructed to reflect subjective information.Combining these two kinds of information and giving full play to the universality of consensus information and the uniqueness of subjective information, a personalized facial attractiveness prediction model is constructed.Experiments on open SCUT-FBP and FaceScrub datasets show that the Pearson correlation coefficient of this method is as high as 0.90 and the residual value is only 0.25.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=feature%20integration&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">feature integration;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=personalized%20prediction&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">personalized prediction;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=facial%20attractiveness&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">facial attractiveness;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=consensus%20information&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">consensus information;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=subjective%20information&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">subjective information;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Pearson%20correlation%20coefficient&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Pearson correlation coefficient;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-05-14</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="73" name="73" class="anchor-tag">0 概述</h3>
                <div class="p1">
                    <p id="74">心理学的相关研究结果表明, 人们只需0.1 s即可产生对他人的第一印象<citation id="232" type="reference"><link href="162" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>, 且不易通过获取其他信息来改变这种先入为主的印象。随着社交媒体的发展, 网络中涌现出大量的人脸照片, 并成为人们快速筛选社交对象的依据。因此, 利用现有大量社交网站数据构建个性化偏好预测模型具有现实意义。在判断一张人脸照片是否具有吸引力时, 通常考虑2个因素。一是共识性的审美偏好, 如大眼睛、高鼻梁等, 这种共识性特征能够反映人类社会活动中的群体趋同性。二是个性化的主观偏好, 即每个人的独特审美取向。共识性偏好具有稳定性, 此外, 针对非常有吸引力和非常没有吸引力的一张人脸时, 共识性偏好具有相对稳定的一致性, 且这种一致性与文化、种族、年龄以及性别无关<citation id="233" type="reference"><link href="164" rel="bibliography" /><link href="166" rel="bibliography" /><link href="168" rel="bibliography" /><sup>[<a class="sup">2</a>,<a class="sup">3</a>,<a class="sup">4</a>]</sup></citation>。但个性化偏好除受到共识性特征的影响外, 还与个人的成长背景、生活工作环境等因素有关, 因此, 其与共识性偏好之间存在差异。</p>
                </div>
                <div class="p1">
                    <p id="75">现有预测方法多数基于共识性偏好进行人脸吸引力研究, 其目标是预测大众对一张人脸的吸引力评分。文献<citation id="234" type="reference">[<a class="sup">5</a>,<a class="sup">6</a>]</citation>提出在不同条件下评估面部吸引力的方法, 但其评价模型的精确度均较低。本文将共识性与主观性审美偏好进行融合, 以进行人脸吸引力评分预测。</p>
                </div>
                <h3 id="76" name="76" class="anchor-tag">1 相关工作</h3>
                <div class="p1">
                    <p id="77">随着机器学习、深度学习等技术的快速发展, 人脸识别、人脸检索取得了较多成果, 且越来越多的特征提取与分析技术被应用于面部吸引力预测中。文献<citation id="235" type="reference">[<a class="sup">7</a>]</citation>提出一种在机器学习环境中的面部吸引力概念。文献<citation id="244" type="reference">[<a class="sup">8</a>,<a class="sup">9</a>]</citation>通过引入支持向量机 (Support Vector Machine, SVM) 的机器学习算法, 开发一种面向中国女性人脸美丽程度的自动分类系统。随着特征提取技术的发展, 人脸吸引力由传统的单一特征扩展到多个特征融合。文献<citation id="236" type="reference">[<a class="sup">10</a>]</citation>提出基于纹理的人脸评价算法并引入几何特征, 在此基础上, 设计一种基于kNN的人脸美化系统。文献<citation id="237" type="reference">[<a class="sup">11</a>]</citation>将若干低级别的人脸特征和高级特征相结合, 形成一个特征向量并进行特征选择, 然后提出一种基于数据驱动的面部美丽程度分析框架。依靠手动标注和典型特征, 关于面部吸引力程度的研究取得了较多的成果。随着深度学习技术的广泛应用, 自动获取高级图像特征的方法在面部吸引力研究中得到发展。文献<citation id="238" type="reference">[<a class="sup">12</a>]</citation>在不依赖人工特征选择的情况下, 通过卷积限制玻尔兹曼机 (Convolution Restricted Boltzmann Machine, CRBM) 对面部美丽程度进行研究, 提取人脸图像的表观特征。文献<citation id="239" type="reference">[<a class="sup">13</a>]</citation>建立一种基于自适应反卷积网络 (Adaptive Deconvolution Network, ADN) 的面部吸引力预测模型, 文献<citation id="240" type="reference">[<a class="sup">14</a>]</citation>构建一种基于深度自编码器的人脸吸引力预测模型。文献<citation id="241" type="reference">[<a class="sup">15</a>]</citation>设计吸引力评级评估模型。文献<citation id="242" type="reference">[<a class="sup">16</a>]</citation>专注全自动学习方法研究, 该方法无需面部特征的手动注释或标签化, 只需将原始像素作为输入, 将图片的姿势、照明、背景、表情、年龄和人种信息用于训练与测试。文献<citation id="243" type="reference">[<a class="sup">17</a>]</citation>受心理学相关理论的启发, 提出一种卷积神经网络PI-CNN, 以实现面部吸引力程度的自动预测。</p>
                </div>
                <div class="p1">
                    <p id="78">目前, 面部吸引力预测不仅适用于静态图像, 在视频中也得到应用。相对于静止图像, 视频所包含的信息复杂, 导致其吸引力预测较困难。文献<citation id="245" type="reference">[<a class="sup">18</a>]</citation>分别从视频剪辑、静态帧中获得动态特征与静态特征, 以自动分析人脸吸引力。文献<citation id="246" type="reference">[<a class="sup">19</a>]</citation>将人脸图像分解为3层:光照层, 细节层, 颜色层, 提出一种面部皮肤美化框架, 以层次化地分析人脸吸引力程度。文献<citation id="247" type="reference">[<a class="sup">20</a>]</citation>提出基于深度卷积神经网络的端到端标签分布式学习框架和一种增量特征选择方法, 该方法挑选一些特征作为深度卷积神经网络的补充, 以优化预测结果。</p>
                </div>
                <div class="p1">
                    <p id="79">在个性化吸引力预测研究中, 文献<citation id="248" type="reference">[<a class="sup">21</a>]</citation>基于epsiv-SVM进行学习, 将低级图像特征映射到吸引力评级, 然后提出一种回归函数。文献<citation id="249" type="reference">[<a class="sup">22</a>]</citation>研究女性面部美学的主观感知与选定的客观参数之间的关系, 其中, 客观参数包括面部特征、照片质量以及非永久性的面部特征。文献<citation id="250" type="reference">[<a class="sup">5</a>]</citation>提出一种协作过滤方法, 该方法可以预测视觉信息, 处理视觉查询的新型回归结果, 在无需获知研究对象的历史评分时也能预测面部吸引力。文献<citation id="251" type="reference">[<a class="sup">23</a>]</citation>将个性化与共识性审美信息相结合, 以进行人脸美化。</p>
                </div>
                <div class="p1">
                    <p id="80">上述方法虽然在共识性研究或个性化预测方面取得了较多成果, 但是未将共识性与个性化2类信息进行结合。在越来越重视个性化的现代社会, 将社会共识性与个性化信息融合进行相关预测, 是未来人工智能研究中的一个重要方向。文献<citation id="252" type="reference">[<a class="sup">24</a>]</citation>融合全局与局部信息进行人脸识别并取得了较好效果, 文献<citation id="253" type="reference">[<a class="sup">25</a>]</citation>利用多源特征互补融合实现表情识别。本文将大众的共识性审美偏好与主观性审美偏好信息进行融合, 并提出一种个性化人脸吸引力预测框架。</p>
                </div>
                <h3 id="81" name="81" class="anchor-tag">2 共识性信息与主观性信息</h3>
                <div class="p1">
                    <p id="82">本文将共识性审美偏好与个性化主观偏好引入到模型构建中。该模型的系统框架包括一种面部吸引力评分采集模块、用于提取深度特征的卷积神经网络、用于融合信息的标准支持向量回归 (Support Vector Regression, SVR) , 以及预测个人审美偏好的评分模块。此外, 本文融合全局特征向量与局部Gabor特征, 以分析面部吸引力, 并设计一种用于融合共识性和主观性偏好信息的CNN架构。本文预测模型的系统框架如图1所示, 其预测性能在SCUT-FBP数据集和FaceScrub子集上得到了验证。</p>
                </div>
                <div class="area_img" id="83">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201908037_083.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 预测模型系统框架" src="Detail/GetImg?filename=images/JSJC201908037_083.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图1 预测模型系统框架</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201908037_083.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="84" name="84">2.1 共识性信息</h4>
                <div class="p1">
                    <p id="85">共识性信息代表公众视野中具有吸引力的面孔的共同特征。随着大数据相关研究的发展, 可以通过各种社交网站获取公共评分数据。公开的SCUT-FBP数据库带有人脸吸引力标签<sup></sup><citation id="254" type="reference"><link href="200" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>, 而FaceScrub数据集没有吸引力标签<citation id="255" type="reference"><link href="202" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>。因此, 本文设计一个在线评分系统, 以收集FaceScrub子集的评分。</p>
                </div>
                <div class="p1">
                    <p id="86">本文选择FaceScrub数据库的图像, 这些图像均为女性面孔, 脸部没有被遮挡, 没有强烈的光线变化与表情变化。测试人员通过在线评分系统对子集进行评分, 获得每张照片的评分及其分布。然后平均所有测试者的评分, 以计算每张图片的标签。标签通过整体概率密度分布进行分析, 结果表明, 评分接近高斯分布, 且在2.5分附近有较高的比例, 这意味着一般面孔比非常漂亮或非常不漂亮的面孔更普遍, 与现实世界情况相符。</p>
                </div>
                <div class="p1">
                    <p id="87">本文通过图2所示的标准偏差分布来验证标签的有效性。从图2可以看出, 当评分接近1.0和5.0时, 偏差很小, 在3.0～3.5之间偏差最大。该结果表明, 人们对非常有吸引力和非常没有吸引力的面孔的偏好非常一致, 但是对于一般面孔则体现出主观性。</p>
                </div>
                <div class="area_img" id="88">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201908037_088.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 标准偏差分布" src="Detail/GetImg?filename=images/JSJC201908037_088.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图2 标准偏差分布</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201908037_088.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="89">将测试者随机分成2组, 计算2组之间的平均Pearson相关系数 (PC) 值<citation id="256" type="reference"><link href="212" rel="bibliography" /><sup>[<a class="sup">26</a>]</sup></citation>, 结果如表1所示。从表1可以看出, 数据库标签具有高度一致性。</p>
                </div>
                <div class="area_img" id="90">
                    <p class="img_tit"><b>表1 FaceScrub数据库标签一致性测试结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="90" border="1"><tr><td><br />实验编号</td><td>PC值</td></tr><tr><td><br />1</td><td>0.88</td></tr><tr><td><br />2</td><td>0.90</td></tr><tr><td><br />3</td><td>0.89</td></tr><tr><td><br />4</td><td>0.90</td></tr><tr><td><br />5</td><td>0.89</td></tr><tr><td><br />平均</td><td>0.89</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="91" name="91">2.2 主观性信息</h4>
                <div class="p1">
                    <p id="92">主观性信息指个人的审美偏好, 本文通过在线评分系统获取个人主观偏好信息。为确保分数的准确性, 要求测试人员在一段时间后再重复对图像进行评分。对于相同的图像, 将最高次数的分数作为测试者的主观评价, 以此得到个人主观评级。将SVM、多元线性回归 (Linear) 、随机森林 (Random Forest) 算法以及CNN对人脸的吸引力评分效果进行比较分析。对于前3种传统机器学习方法, 通过CCA<citation id="257" type="reference"><link href="214" rel="bibliography" /><sup>[<a class="sup">27</a>]</sup></citation>融合17维特征向量<citation id="258" type="reference"><link href="178" rel="bibliography" /><link href="216" rel="bibliography" /><sup>[<a class="sup">9</a>,<a class="sup">28</a>]</sup></citation>和局部Gabor特征进行回归分析。实验结果表明, 多种特征融合在个性化人脸吸引力预测中具有有效性。</p>
                </div>
                <div class="p1">
                    <p id="93">传统机器学习方法效率较低, 原因是其高度依赖提取的特征。深度学习可以从输入数据中自动学习特征, 因此, 其效果优于机器学习<citation id="259" type="reference"><link href="194" rel="bibliography" /><link href="200" rel="bibliography" /><sup>[<a class="sup">17</a>,<a class="sup">20</a>]</sup></citation>。由于CNN在特征提取方面有一定优势, 本文选择CNN特征提取来预测主观性评分。在实验中, 从SCUT-FBP数据集中选择400幅图像作为训练集, 100幅图像作为测试集, 将主观历史评分记录和相应的图像输入到本文设计的6层CNN网络并进行训练。</p>
                </div>
                <h3 id="94" name="94" class="anchor-tag">3 融合模型</h3>
                <div class="p1">
                    <p id="95">本文提出一种主观性信息与共识性信息融合算法, 用于个性化人脸美学评价研究。具体融合方法如下:</p>
                </div>
                <div class="p1">
                    <p id="96">1) CNN+AVE融合:信息融合分为数据级融合、特征级融合和决策级融合。本文将CNN输出的主观评分与数据库自带的共识性评分标签进行决策级融合。AVE融合指取主观和共识评分的平均分, 即共识性评分和主观性评分按照相同的权重参与评分决策。</p>
                </div>
                <div class="p1">
                    <p id="97">2) CNN+LAE融合:由于不同个体受群众影响程度的差异, 模型需要具备自适应性。共识性评分与主观性评分高度相关, 故模型还应具有普遍一致性。主观性评分和共识性评分间是线性关系, 因此, 本文基于最基本的线性模型来设计一种自适应模型LAE。使用残差作为损失函数, 最小化残差可以求解模型参数。损失函数的形式是:</p>
                </div>
                <div class="p1">
                    <p id="98" class="code-formula">
                        <mathml id="98"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><mo>=</mo><mfrac><mn>1</mn><mi>m</mi></mfrac><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mo stretchy="false"> (</mo></mstyle><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mi>f</mi><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo stretchy="false">) </mo></mrow><msup><mrow></mrow><mn>2</mn></msup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="99">其中, <i>m</i>是样本数量, <i>y</i><sub><i>i</i></sub>是真实值, <i>f</i> (<i>x</i><sub><i>i</i></sub>) 是预测值。</p>
                </div>
                <div class="p1">
                    <p id="100">本文使用LAE建立主观和协商一致的融合模型:</p>
                </div>
                <div class="p1">
                    <p id="101"><i>R</i>=<i>αR</i><sub><i>C</i></sub>+<i>βR</i><sub><i>P</i></sub>      (2) </p>
                </div>
                <div class="p1">
                    <p id="102"><i>α</i>+<i>β</i>=1      (3) </p>
                </div>
                <div class="p1">
                    <p id="103">其中, <i>α</i>、<i>β</i>是权重系数, 0&lt;<i>α</i>、<i>β</i>&lt;1, <i>R</i>表示最终的个性化预测分数, <i>R</i><sub><i>C</i></sub>表示图片的共识性分数, <i>R</i><sub><i>P</i></sub>表示图片的主观性分数。权重系数根据不同个体自动选择取值。</p>
                </div>
                <div class="p1">
                    <p id="104">3) CNN融合:本文在CNN结构中添加融合层, 网络结构如图3所示。融合层将主观审美偏好信息和共识性审美偏好信息进行融合, 并最终通过全连接层来输出预测评级。通过这种方式, 可以实现端到端自动提取测量图像的特征, 从而进行个性化的人脸吸引力预测。</p>
                </div>
                <div class="area_img" id="105">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201908037_105.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 融合CNN网络架构" src="Detail/GetImg?filename=images/JSJC201908037_105.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图3 融合CNN网络架构</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201908037_105.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="106">4) CNN+SVR融合:本文使用CNN的主观评估分数和数据库客观评估分数标签, 通过SVR建立主观审美信息和客观审美信息的融合模型。SVR通过高维空间中的线性决策函数来构造, 并由核函数实现, 最终对个性化面部吸引力进行评分预测。</p>
                </div>
                <h3 id="107" name="107" class="anchor-tag">4 实验结果与分析</h3>
                <div class="p1">
                    <p id="108">本次实验在2个公开的人脸数据集上进行评估:SCUT-FBP数据集<citation id="260" type="reference"><link href="218" rel="bibliography" /><sup>[<a class="sup">29</a>]</sup></citation>, 包括500张具有吸引力评分的亚洲女性照片;FaceScrub数据集<citation id="261" type="reference"><link href="220" rel="bibliography" /><sup>[<a class="sup">30</a>]</sup></citation>, 包含6 195位公众人物的141 130张人脸照片。对于这2个数据集, 通过本文融合模型进行特征提取, 图4所示为利用CNN+SVR框架对测试者<i>A</i>和测试者<i>B</i>进行预测的实验结果。其中, 打×的部分代表预测值与真实值误差大于等于0.5。由图4可以看出, 本文融合模型能够针对不同个体的吸引力偏好输出合理的预测结果, 但其中存在预测失败的少量样本, 今后将对这些样本进行分析研究。</p>
                </div>
                <div class="area_img" id="109">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201908037_109.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 测试者A和测试者B预测结果" src="Detail/GetImg?filename=images/JSJC201908037_109.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图4 测试者<i>A</i>和测试者<i>B</i>预测结果</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201908037_109.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="110" name="110">4.1 模型评估标准</h4>
                <div class="p1">
                    <p id="111">本文选取PC值<citation id="262" type="reference"><link href="212" rel="bibliography" /><sup>[<a class="sup">26</a>]</sup></citation>、残差 (RES) <citation id="263" type="reference"><link href="222" rel="bibliography" /><sup>[<a class="sup">31</a>]</sup></citation>以及决定系数 (R<sup>2</sup>) <citation id="264" type="reference"><link href="224" rel="bibliography" /><sup>[<a class="sup">32</a>]</sup></citation>来评估不同算法的预测性能。3个评估指标计算如下:</p>
                </div>
                <div class="p1">
                    <p id="112" class="code-formula">
                        <mathml id="112"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>Ρ</mi><mi>C</mi><mo>=</mo><mfrac><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mrow><mrow><mo> (</mo><mrow><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mover><mstyle mathsize="140%" displaystyle="true"><mi>y</mi></mstyle><mrow><mspace width="0.25em" /><mo>-</mo></mrow></mover></mrow><mo>) </mo></mrow></mrow></mstyle><mrow><mo> (</mo><mrow><mi>f</mi><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>-</mo><mover accent="true"><mrow><mi>f</mi><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo></mrow><mo stretchy="true">¯</mo></mover></mrow><mo>) </mo></mrow></mrow><mrow><msqrt><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mrow><mrow><mo> (</mo><mrow><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mover><mstyle mathsize="140%" displaystyle="true"><mi>y</mi></mstyle><mrow><mspace width="0.25em" /><mo>-</mo></mrow></mover></mrow><mo>) </mo></mrow></mrow></mstyle><msup><mrow></mrow><mn>2</mn></msup><mrow><mrow><mo> (</mo><mrow><mi>f</mi><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>-</mo><mover accent="true"><mrow><mi>f</mi><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo></mrow><mo stretchy="true">¯</mo></mover></mrow><mo>) </mo></mrow></mrow><msup><mrow></mrow><mn>2</mn></msup></mrow></msqrt></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi>R</mi><mi>E</mi><mi>S</mi><mo>=</mo><mfrac><mn>1</mn><mi>m</mi></mfrac><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mrow><mrow><mo> (</mo><mrow><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mi>f</mi><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo></mrow><mo>) </mo></mrow></mrow></mstyle></mrow><msup><mrow></mrow><mn>2</mn></msup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>5</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi>R</mi><msup><mrow></mrow><mn>2</mn></msup><mo>=</mo><mfrac><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mrow><mrow><mo> (</mo><mrow><mi>f</mi><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>-</mo><mover><mstyle mathsize="140%" displaystyle="true"><mi>y</mi></mstyle><mrow><mspace width="0.25em" /><mo>-</mo></mrow></mover></mrow><mo>) </mo></mrow></mrow></mstyle><msup><mrow></mrow><mn>2</mn></msup></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mrow><mrow><mo> (</mo><mrow><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mover><mstyle mathsize="140%" displaystyle="true"><mi>y</mi></mstyle><mrow><mspace width="0.25em" /><mo>-</mo></mrow></mover></mrow><mo>) </mo></mrow></mrow></mstyle><msup><mrow></mrow><mn>2</mn></msup></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>6</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="113">PC值越接近1或-1, 相关性越强, PC值越接近0, 相关性越弱。通常, PC值在0.8～1.0间表明强相关关系。RES包含有关模型基本假设的重要信息。如果回归模型正确, 可以使用RES作为误差的观测值, 其需要符合模型的假设并具有误差的一些性质。R<sup>2</sup>反映了变量的整体变化, 其可以通过回归关系来解释, R<sup>2</sup>值较高, 则表示回归关系的拟合程度较高。</p>
                </div>
                <div class="p1">
                    <p id="114">本次实验的对比机器学习方法包括SVM、Linear、Random Forest, 用10折交叉法进行验证。此外, 为证明特征融合的效果优于单个几何特征, 使用CCA算法融合17维特征向量和由Gabor滤波器获得的局部特征。特征融合的实验结果如表2所示。</p>
                </div>
                <div class="area_img" id="115">
                    <p class="img_tit"><b>表2 特征融合结果对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="115" border="1"><tr><td>特征方式</td><td>方法</td><td>PC值</td><td>RES值</td><td>R<sup>2</sup>值</td></tr><tr><td rowspan="3"><br />单一特征</td><td>Linear</td><td>0.77</td><td>0.60</td><td>0.59</td></tr><tr><td><br />SVM</td><td>0.89</td><td>0.71</td><td>0.79</td></tr><tr><td><br />Random Forest</td><td>0.41</td><td>0.64</td><td>0.17</td></tr><tr><td rowspan="3"><br />多特征融合</td><td>Linear</td><td>0.85</td><td>0.51</td><td>0.72</td></tr><tr><td><br />SVM</td><td>0.86</td><td>0.59</td><td>0.75</td></tr><tr><td><br />Random Forest</td><td>0.71</td><td>0.56</td><td>0.51</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="116">从表2可以看出, SVM方法的PC指标最好, Linear在PC、RES、R<sup>2</sup>上也表现良好。此外, 融合特征比单一特征表现出更好的性能。</p>
                </div>
                <h4 class="anchor-tag" id="117" name="117">4.2 模型验证</h4>
                <div class="p1">
                    <p id="118">本次实验在SCUT-FBP数据集中, 利用CNN自动提取特征, 并将其输入主观性与共识性融合模型, 从而预测最终评分。在实验中, 对比单一方法和融合方法的性能。单一方法由简单的SCNN (主观性评分预测CNN) 和OCNN (共识性评分预测CNN) 组成, 其仅向网络输入主观性评分或共识性评分。融合方法包括CNN融合、SVR融合、AVE融合以及LAE融合。实验结果如表3所示。</p>
                </div>
                <div class="area_img" id="119">
                    <p class="img_tit"><b>表3 单一方法与融合方法性能对比结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="119" border="1"><tr><td>指标</td><td>SCNN</td><td>OCNN</td><td>CNN融合</td><td>SVR融合</td><td>AVE融合</td><td>LAE融合</td></tr><tr><td><br />PC值</td><td>0.46</td><td>0.64</td><td>0.82</td><td>0.88</td><td>0.84</td><td>0.84</td></tr><tr><td><br />RES值</td><td>0.24</td><td>0.48</td><td>0.29</td><td>0.25</td><td>0.35</td><td>0.40</td></tr><tr><td><br />R<sup>2</sup>值</td><td>0.21</td><td>0.45</td><td>0.67</td><td>0.79</td><td>0.71</td><td>0.71</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="120">从表3可以看出, 由SVR融合得到的PC值最佳。此外, 上述实验结果验证了共识性和主观性信息融合的有效性。</p>
                </div>
                <div class="p1">
                    <p id="121">进一步在FaceScrub子集上进行实验, 比较4种融合模型的性能。与SCUT-FBP不同, FaceScrub子集中的图像都是西方女性面孔, 这也是模型预测的一个不确定因素。通过自洽性和一致性实验, 本文构建的子集标签的数据分布符合一般人脸吸引力程度分布规律。各融合模型的性能对比结果如表4所示。</p>
                </div>
                <div class="area_img" id="122">
                    <p class="img_tit"><b>表4 各融合模型在FaceScrub子集上的性能对比结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="122" border="1"><tr><td>融合模型</td><td>PC值</td><td>RES值</td><td>R<sup>2</sup>值</td></tr><tr><td><br />LAE融合</td><td>0.84</td><td>0.44</td><td>0.70</td></tr><tr><td><br />AVE融合</td><td>0.83</td><td>0.47</td><td>0.68</td></tr><tr><td><br />CNN融合</td><td>0.79</td><td>0.34</td><td>0.63</td></tr><tr><td><br />SVR融合</td><td>0.90</td><td>0.26</td><td>0.82</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="123">在上述单一随机实验中, SVR融合的PC值可达0.99, RES值达0.01。</p>
                </div>
                <div class="p1">
                    <p id="124">将本文融合方法与已有相关预测方法进行性能对比, 结果如表5所示。其中, 最后2行分别为SCUT-FBP数据集和FaceScrub子集上本文方法的实验结果。</p>
                </div>
                <div class="area_img" id="125">
                    <p class="img_tit"><b>表5 各方法性能对比结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="125" border="1"><tr><td rowspan="3"><br />方法</td><td rowspan="3">特征模型</td><td colspan="2"><br />个性化预测</td><td colspan="2">共识性预测</td></tr><tr><td colspan="2"><br /></td><td colspan="2"></td></tr><tr></tr><tr><td>文献[29]方法</td><td>Handcrafted, Gabor</td><td>—</td><td>—</td><td>0.65</td><td>0.27</td></tr><tr><td><br />文献[33]方法</td><td>Ratio</td><td>—</td><td>—</td><td>0.69</td><td>0.32</td></tr><tr><td><br />文献[34]方法</td><td>One layer autoencoder</td><td>—</td><td>—</td><td>0.64</td><td>1.25</td></tr><tr><td><br />文献[35]方法</td><td>CNN</td><td>0.690</td><td>0.58</td><td>0.88</td><td>—</td></tr><tr><td><br />文献[11]方法</td><td>LBP, AAM, PCANet</td><td>—</td><td>—</td><td>0.84</td><td>0.20</td></tr><tr><td><br />文献[21]方法</td><td>Eigenface, Gabor, EOH</td><td>0.450</td><td>—</td><td>—</td><td>—</td></tr><tr><td><br />文献[5]方法</td><td>Collaborative filtering</td><td>0.670</td><td>—</td><td>—</td><td>—</td></tr><tr><td><br />文献[9]方法</td><td>FeaturePoint Gabor+PCA</td><td>—</td><td>—</td><td>0.90</td><td>0.98</td></tr><tr><td><br />文献[23]方法</td><td>PG-SVR2</td><td>0.386</td><td>—</td><td>—</td><td>—</td></tr><tr><td rowspan="2"><br />本文方法</td><td rowspan="2">CNN+SVR融合</td><td>0.880</td><td>0.25</td><td>—</td><td>—</td></tr><tr><td><br />0.900</td><td>0.26</td><td>—</td><td>—</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="126">从表5可以看出, 将共识性信息和个性化信息进行融合, 能够有效提高个性化的人脸吸引力预测性能, 相比只利用个性化信息的预测模型, 其在多个方面均有性能提升。在本文方法中, 共识性信息发挥其数据量较大的特点, 反映人群普遍的审美倾向, 而个性化信息的引入, 有助于构建准确的人脸吸引力预测模型。本文通过建立融合模型, 在相对复杂的个性化人脸吸引力预测中取得了很好的效果, 充分验证了该方法的有效性。实验结果还表明, 如果对深度网络所提取特征进行补充, 有针对性地融合传统算法提取的融合特征与CNN提取的深度特征, 然后进行预测, 将进一步提升预测效果。此外, 在更大的数据库上进行训练和预测, 有助于本文模型的实用性验证。</p>
                </div>
                <h3 id="127" name="127" class="anchor-tag">5 结束语</h3>
                <div class="p1">
                    <p id="128">针对个性化人脸吸引力预测问题, 本文利用CNN提取深度特征, 通过SVR将共识性信息引入到预测模型中, 提出一种融合主观性与共识性信息的人脸吸引力预测方法。利用2个公开的人脸数据库进行实验, 结果表明, 相对传统预测方法, 该方法的PC值较高, 残差值较低。下一步将选择更多的融合特征作为CNN的补充特征并加入到预测模型中, 以提高模型的预测性能。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="162">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=First Impressions: Making Up Your Mind After a 100-Ms Exposure to a Face">

                                <b>[1]</b> WILLIS J, TODOROV A.First impressions:making up your mind after a 100-ms exposure to a face[J].Psychological Science, 2010, 17 (7) :592-598.
                            </a>
                        </p>
                        <p id="164">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Maxims or myths of beauty? A meta-analytic and theoretical review">

                                <b>[2]</b> LANGLOIS J H, KALAKANIS L, RUBENSTEIN A J, et al.Maxims or myths of beauty?A meta-analytic and theoretical review[J].Psychological Bulletin, 2000, 126 (3) :390-423.
                            </a>
                        </p>
                        <p id="166">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The Automatic Measurement of Facial Beauty">

                                <b>[3]</b> AARABI P, HUGHES D, MOHAJER K, et al.The automatic measurement of facial beauty[C]//Proceedings of IEEE International Conference on Systems, Man, and Cybernetics.Washington D.C., USA:IEEE Press, 2001:2644-2647.
                            </a>
                        </p>
                        <p id="168">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Relative ranking of facial attractiveness">

                                <b>[4]</b> ALTWAIJRY H, BELONGIE S.Relative ranking of facial attractiveness[C]//Proceedings of 2013 IEEE Workshop on Applications of Computer Vision.Washington D.C., USA:IEEE Press, 2013:117-124.
                            </a>
                        </p>
                        <p id="170">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Some Like It Hot—Visual Guidance for Preference Prediction">

                                <b>[5]</b> ROTHE R, TIMOFTE R, GOOL L V.Some like it hot-visual guidance for preference prediction[C]//Proceedings of 2016 IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2016:5553-5561.
                            </a>
                        </p>
                        <p id="172">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Facial beauty assessment under unconstrained conditions">

                                <b>[6]</b> YAN Mengjia, DUAN Yuron, DENG Siqi, et al.Facial beauty assessment under unconstrained conditions[C]//Proceedings of International Conference on Electronics, Computers and Artificial Intelligence.Washington D.C., USA:IEEE Press, 2017:1-6.
                            </a>
                        </p>
                        <p id="174">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJBK&amp;filename=SJBK15090500012451&amp;v=MjU1ODdOQ0hrNG9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVUx2TElGMFFheG89TmlmSlpiSzlIdGpNcW85RlpPbw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b> EISENTHAL Y, DROR G, RUPPIN E.Facial attractive-ness:beauty and the machine[J].Neural Computation, 2006, 18 (1) :119-142.
                            </a>
                        </p>
                        <p id="176">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Automatic classification of Chinese female facial beauty using support vector machine">

                                <b>[8]</b> MAO Huiyun, JIN Lianwen, DU Minghui.Automatic classifica-tion of Chinese female facial beauty using support vector machine[C]//Proceedings of IEEE International Conference on Systems, Man and Cybernetics.Washington D.C., USA:IEEE Press, 2009:4842-4846.
                            </a>
                        </p>
                        <p id="178">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CDFD&amp;filename=1011188499.nh&amp;v=MTI4NTdJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVScUZDam1WNzdBVkYyNkg3S3dGdFhGcHBFYlA=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b> 毛慧芸.人脸美丽吸引力的特征分析与机器学习[D].广州:华南理工大学, 2011.
                            </a>
                        </p>
                        <p id="180">
                            <a id="bibliography_10" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=1012451946.nh&amp;v=MTMxMjFyQ1VSTE9lWmVScUZDam1WNzdBVkYyNkhMZTlIOWpJcVpFYlBJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0Y=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[10]</b> 陈伊力.基于数据驱动的人脸美化[D].广州:华南理工大学, 2012.
                            </a>
                        </p>
                        <p id="182">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Data-driven facial beauty analysis:prediction,retrieval and manipulation">

                                <b>[11]</b> ZHANG D, CHEN Fangmei, XU Yong.Data-driven facial beauty analysis:prediction, retrieval and manipulation[M].Berlin, Germany:Springer, 2016.
                            </a>
                        </p>
                        <p id="184">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Facial beauty prediction model based on self-taught learning and convolutional restricted Boltzmann machine">

                                <b>[12]</b> GAN Junying, LI Lichen, ZHAI Yikui.Facial beauty prediction model based on self-taught learning and convolutional restricted Boltzmann machine[C]//Proceedings of International Conference on Machine Learning and Cybernetics.Washington D.C., USA:IEEE Press, 2015:844-849.
                            </a>
                        </p>
                        <p id="186">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Study for Facial Beauty Prediction Model">

                                <b>[13]</b> GAN Junying, ZHOU Lei, ZHAI Yikui.A study for facial beauty prediction model[C]//Proceedings of International Conference on Wavelet Analysis and Pattern Recognition.Washington D.C., USA:IEEE Press, 2015:8-13.
                            </a>
                        </p>
                        <p id="188">
                            <a id="bibliography_14" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=WYDW201404011&amp;v=MTY5NzNDam1WNzdBTWpUUGViRzRIOVhNcTQ5RVpZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVScUY=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[14]</b> 李远豪, 甘俊英.深度自编码器用于人脸美丽吸引力预测的研究[J].五邑大学学报 (自然科学版) , 2014, 28 (4) :49-54.
                            </a>
                        </p>
                        <p id="190">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Indexing facial attrac-tiveness and well beings using machine learning">

                                <b>[15]</b> CHOUDHARY G, GANDHI T K.Indexing facial attrac-tiveness and well beings using machine learning[C]//Proceedings of 2016 IEEE Region 10 Humanitarian Technology Conference.Washington D.C., USA:IEEE Press, 2016:1-6.
                            </a>
                        </p>
                        <p id="192">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Predicting Facial Beauty without Landmarks">

                                <b>[16]</b> GRAY D, YU Kai, XU Wei, et al.Predicting facial beauty without landmarks[C]//Proceedings of European Conference on Computer Vision.Berlin, Germany:Springer, 2010:434-447.
                            </a>
                        </p>
                        <p id="194">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Facial attractiveness prediction using psychologically inspired convolutional neural network (PI-CNN)">

                                <b>[17]</b> XU Jie, JIN Lianwen, LIANG Lingyu, et al.Facial attractiveness prediction using psychologically inspired convolutional neural network (PI-CNN) [C]//Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing.Washington D.C., USA:IEEE Press, 2017:1657-1661.
                            </a>
                        </p>
                        <p id="196">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Automatic analysis of facial attractiveness from video">

                                <b>[18]</b> KALAYCI S, EKENEL H K, GUNES H.Automatic analysis of facial attractiveness from video[C]//Proceedings of IEEE International Conference on Image Processing.Washington D.C., USA:IEEE Press, 2015:4191-4195.
                            </a>
                        </p>
                        <p id="198">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Facial skin beautification via sparse representation over learned layer dictionary">

                                <b>[19]</b> LU Xi, CHANG Xiaobin, XIE Xiaohua, et al.Facial skin beautification via sparse representation over learned layer dictionary[C]//Proceedings of International Joint Conference on Neural Networks.Washington D.C., USA:IEEE Press, 2016:2534-2539.
                            </a>
                        </p>
                        <p id="200">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Facial attractiveness computation by label distribution learning with deep CNN and geometric features">

                                <b>[20]</b> LIU Shu, LI Bo, FAN Yangyu, et al.Facial attractiveness computation by label distribution learning with deep CNN and geometric features[C]//Proceedings of IEEE International Conference on Multimedia and Expo.Washington D.C., USA:IEEE Press, 2017:1344-1349.
                            </a>
                        </p>
                        <p id="202">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Personalized facial attractiveness prediction">

                                <b>[21]</b> WHITEHILL J, MOVELLAN J R.Personalized facial attractiveness prediction[C]//Proceedings of IEEE International Conference on Automatic Face and Gesture Recognition.Washington D.C., USA:IEEE Press, 2008:1-7.
                            </a>
                        </p>
                        <p id="204">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Assessment of female facial beauty based on anthropometric,non-permanent and acquisition characteristics">

                                <b>[22]</b> DANTCHEVA A, DUGELAY J L.Assessment of female facial beauty based on anthropometric, non-permanent and acquisition characteristics[M].[S.l.]:Kluwer Academic Publishers, 2015.
                            </a>
                        </p>
                        <p id="206">
                            <a id="bibliography_23" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=1016103350.nh&amp;v=MDc2MDFyQ1VSTE9lWmVScUZDam1WNzdBVkYyNkdMSzRIZExKcjVFYlBJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0Y=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[23]</b> 王少标.基于个性与共性探讨的人脸美化[D].合肥:中国科学技术大学, 2016.
                            </a>
                        </p>
                        <p id="208">
                            <a id="bibliography_24" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201119049&amp;v=Mjk5ODJDVVJMT2VaZVJxRkNqbVY3N0FMejdCYmJHNEg5RE5wbzlCYllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[24]</b> 舒畅, 丁晓青, 方驰.多特征局部与全局融合的人脸识别方法[J].计算机工程, 2011, 37 (19) :145-147.
                            </a>
                        </p>
                        <p id="210">
                            <a id="bibliography_25" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201510033&amp;v=MjY3NDJDVVJMT2VaZVJxRkNqbVY3N0FMejdCYmJHNEg5VE5yNDlHWjRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[25]</b> 黄忠, 胡敏, 刘娟.基于多特征决策级融合的表情识别方法[J].计算机工程, 2015, 41 (10) :171-176.
                            </a>
                        </p>
                        <p id="212">
                            <a id="bibliography_26" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Note on the history of correlation">

                                <b>[26]</b> PEARSON K.Notes on the history of correlation[J].Biometrika, 1920, 13 (1) :25-45.
                            </a>
                        </p>
                        <p id="214">
                            <a id="bibliography_27" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES954B818CF976DFD7ECF5E3D4260252E2&amp;v=MTU5MzZDZ2hQdXhGbW1VbDRQWHlXcUJBemViQ1JSOCtkQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQ3BiUTM1TnhpeEx1N3dhQT1OaWZPZmJxOUdxUEVyb2MyRXVJSQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[27]</b> HAGHIGHAT M, ABDEL-MOTTALEB M, ALHALABI W.Fully automatic face normalization and single sample face recognition in unconstrained environments[J].Expert Systems with Applications, 2016, 47 (5) :23-34.
                            </a>
                        </p>
                        <p id="216">
                            <a id="bibliography_28" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201006011&amp;v=MDU5ODFyQ1VSTE9lWmVScUZDam1WNzdBS0Q3WWJMRzRIOUhNcVk5RVpZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0Y=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[28]</b> MAO Huiyun, JIN Lianwen, DU Minghui.Facial beauty classification based on geometric features and c4.5[J].Pattern Recognition and Artificial Intelligence, 2010, 23 (6) :809-814.
                            </a>
                        </p>
                        <p id="218">
                            <a id="bibliography_29" target="_blank" href="http://scholar.cnki.net/result.aspx?q=SCUT-FBP:a benchmark dataset for facial beauty perception">

                                <b>[29]</b> XIE Duorui, LIANG Lingyu, JIN Lianwen, et al.SCUT-FBP:a benchmark dataset for facial beauty perception[C]//Proceedings of IEEE International Conference on Systems, Man, and Cybernetics.Washington D.C., USA:IEEE Press, 2015:1821-1826.
                            </a>
                        </p>
                        <p id="220">
                            <a id="bibliography_30" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Data-driven Approach to Cleaning Large Face Datasets">

                                <b>[30]</b> NG H, WINKLER S.A data-driven approach to cleaning large face datasets[C]//Proceedings of IEEE International Conference on Image Processing.Washington D.C., USA:IEEE Press, 2015:343-347.
                            </a>
                        </p>
                        <p id="222">
                            <a id="bibliography_31" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Residue arithmetic and its applications to computer technology">

                                <b>[31]</b> SZABO N S, TANAKA R I.Residue arithmetic and its applications to computer technology[J].SIAM Review, 2009, 11 (1) :143-150.
                            </a>
                        </p>
                        <p id="224">
                            <a id="bibliography_32" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A generalized $R\sp 2$ criterion for regression models estimated by the instrumental variables method">

                                <b>[32]</b> PESARAN M H, SMITH R J.A generalized R<sup>2</sup>criterion for regression models estimated by the instrumental variables method[J].Econometrica, 1994, 62 (3) :705-710.
                            </a>
                        </p>
                        <p id="226">
                            <a id="bibliography_33" target="_blank" href="http://scholar.cnki.net/result.aspx?q=2.5D facial attractiveness computation based on data-driven geometric ratios">

                                <b>[33]</b> LIU Shu, FAN Yangyu, GUO Zhe, et al.2.5d facial attractiveness computation based on data-driven geometric ratios[C]//Proceedings of International Conference on Intelligent Science and Big Data Engineering.Washington D.C., USA:IEEE Press, 2015:564-573.
                            </a>
                        </p>
                        <p id="228">
                            <a id="bibliography_34" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Attractive or Not?:Beauty prediction with attractiveness-aware encoders and robust late fusion">

                                <b>[34]</b> WANG Shuyang, SHAO Ming, FU Yun.Attractive or not?:beauty prediction with attractiveness-aware encoders and robust late fusion[C]//Proceedings of ACM International Conference on Multimedia.New York, USA:ACM Press, 2014:805-808.
                            </a>
                        </p>
                        <p id="230">
                            <a id="bibliography_35" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A new humanlike facial attractiveness predictor with cascaded fine-tuning deep learning model">

                                <b>[35]</b> XU Jie, JIN Lianwen, LIANG Lingyu, et al.A new humanlike facial attractiveness predictor with cascaded fine-tuning deep learning model[J].Computer Science, 2015, 70 (1) :45-79.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJC201908037" />
        <input id="dpi" type="hidden" value="600" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201908037&amp;v=MjIxNDF6N0JiYkc0SDlqTXA0OUdZNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUnFGQ2ptVjc3QUw=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
