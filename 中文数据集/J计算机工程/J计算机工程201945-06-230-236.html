<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637130386422176250%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJC201906037%26RESULT%3d1%26SIGN%3ddqG8yzVRgyX%252bVE1RtvkGtz2nofk%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201906037&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201906037&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201906037&amp;v=Mjc0NDY0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSb0Z5bmxWYnpMTHo3QmJiRzRIOWpNcVk5R1k0UUtESDg0dlI=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#61" data-title="0 概述 ">0 概述</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#65" data-title="1 相关工作 ">1 相关工作</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#66" data-title="1.1 多模态图正则化">1.1 多模态图正则化</a></li>
                                                <li><a href="#89" data-title="1.2 特征选择">1.2 特征选择</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#125" data-title="2 跨媒体检索方法 ">2 跨媒体检索方法</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#126" data-title="2.1 耦合字典学习">2.1 耦合字典学习</a></li>
                                                <li><a href="#165" data-title="2.2 联合图像正则化">2.2 联合图像正则化</a></li>
                                                <li><a href="#180" data-title="2.3 跨媒体检索的测试阶段">2.3 跨媒体检索的测试阶段</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#183" data-title="3 实验结果与分析 ">3 实验结果与分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#184" data-title="3.1 实验设置">3.1 实验设置</a></li>
                                                <li><a href="#195" data-title="3.2 &lt;i&gt;Wiki&lt;/i&gt;数据集结果分析">3.2 <i>Wiki</i>数据集结果分析</a></li>
                                                <li><a href="#202" data-title="3.3 Pascal VOC数据集结果分析">3.3 Pascal VOC数据集结果分析</a></li>
                                                <li><a href="#209" data-title="3.4 不同特征类型表现">3.4 不同特征类型表现</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#212" data-title="4 结束语 ">4 结束语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#199" data-title="&lt;b&gt;表1 Wiki数据集的跨媒体检索精度比较1&lt;/b&gt;"><b>表1 Wiki数据集的跨媒体检索精度比较1</b></a></li>
                                                <li><a href="#200" data-title="&lt;b&gt;图1 不同方法在Wiki数据集上图像检索文本精度对比&lt;/b&gt;"><b>图1 不同方法在Wiki数据集上图像检索文本精度对比</b></a></li>
                                                <li><a href="#201" data-title="&lt;b&gt;图2 不同方法在Wiki数据集上文本检索图像精度对比&lt;/b&gt;"><b>图2 不同方法在Wiki数据集上文本检索图像精度对比</b></a></li>
                                                <li><a href="#206" data-title="&lt;b&gt;表2 Pascal VOC数据集的跨媒体检索精度比较&lt;/b&gt;"><b>表2 Pascal VOC数据集的跨媒体检索精度比较</b></a></li>
                                                <li><a href="#207" data-title="&lt;b&gt;图3 不同方法在Pascal VOC数据集上图像检索文本精度对比&lt;/b&gt;"><b>图3 不同方法在Pascal VOC数据集上图像检索文本精度对比</b></a></li>
                                                <li><a href="#208" data-title="&lt;b&gt;图4 不同方法在Pascal VOC数据集上文本检索图像精度对比&lt;/b&gt;"><b>图4 不同方法在Pascal VOC数据集上文本检索图像精度对比</b></a></li>
                                                <li><a href="#211" data-title="&lt;b&gt;表3 Wiki数据集的跨媒体检索精度比较2&lt;/b&gt;"><b>表3 Wiki数据集的跨媒体检索精度比较2</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="250">


                                    <a id="bibliography_1" title=" WANG Kaiye, YIN Qiyue, WANG Wei, et al.A comprehensive survey on cross-modal retrieval [EB/OL].[2018-08-02].https://arxiv.org/pdf/1607.06215.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A comprehensive survey on cross-modal retrieval">
                                        <b>[1]</b>
                                         WANG Kaiye, YIN Qiyue, WANG Wei, et al.A comprehensive survey on cross-modal retrieval [EB/OL].[2018-08-02].https://arxiv.org/pdf/1607.06215.pdf.
                                    </a>
                                </li>
                                <li id="252">


                                    <a id="bibliography_2" title=" HARDOON D R, SZEDMAK S, SHAWE-TAYLOR J.Canonical correlation analysis:an overview with application to learning methods[J].Neural Computation, 2004, 16 (12) :2639-2664." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJBK&amp;filename=SJBK15090500012263&amp;v=MjM4MDViSzlIdGpNcW85RlpPb05Ebm82b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VTG5JSVY0U2FSRT1OaWZKWg==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         HARDOON D R, SZEDMAK S, SHAWE-TAYLOR J.Canonical correlation analysis:an overview with application to learning methods[J].Neural Computation, 2004, 16 (12) :2639-2664.
                                    </a>
                                </li>
                                <li id="254">


                                    <a id="bibliography_3" title=" ROSIPAL R, KR&#196;MER N.Overview and recent advances in partial least squares [C]//Proceedings of International Conference on Subspace, Latent Structure and Feature Selection.Berlin, Germany:Springer, 2005:34-51." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Overview and recent advances in partial least squares">
                                        <b>[3]</b>
                                         ROSIPAL R, KR&#196;MER N.Overview and recent advances in partial least squares [C]//Proceedings of International Conference on Subspace, Latent Structure and Feature Selection.Berlin, Germany:Springer, 2005:34-51.
                                    </a>
                                </li>
                                <li id="256">


                                    <a id="bibliography_4" title=" TENENBAUM J B, FREEMAN W T.Separating style and content with bilinear model[J].Neural Computation, 2000, 12 (6) :1247-1283." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJBK&amp;filename=SJBK15090500011833&amp;v=MTU4OTBud1plWnVIeWptVUxuSUlWNFNhUkU9TmlmSlpiSzlIdGpNcW85RlpPb09CSDg2b0JNVDZUNFBRSC9pclJkR2VycVFUTQ==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                         TENENBAUM J B, FREEMAN W T.Separating style and content with bilinear model[J].Neural Computation, 2000, 12 (6) :1247-1283.
                                    </a>
                                </li>
                                <li id="258">


                                    <a id="bibliography_5" title=" MAHADEVAN V, PEREIRA J C, VASCONCELOSN, et al.Maximum covariance unfolding:manifold learning for bimodal data[EB/OL].[2018-08-02].http://www.svcl.ucsd.edu/publications/conference/2011/nips2011/mcu.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Maximum covariance unfolding:manifold learning for bimodal data">
                                        <b>[5]</b>
                                         MAHADEVAN V, PEREIRA J C, VASCONCELOSN, et al.Maximum covariance unfolding:manifold learning for bimodal data[EB/OL].[2018-08-02].http://www.svcl.ucsd.edu/publications/conference/2011/nips2011/mcu.pdf.
                                    </a>
                                </li>
                                <li id="260">


                                    <a id="bibliography_6" title=" MAO Xiangbo, LIN Binbin, CAI Deng, et al.Parallel field alignment for cross media retrieval[C]//Proceedings of ACM International Conference on Multimedia.New York, USA:ACM Press, 2013:897-906." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Parallel field alignment for cross media retrieval">
                                        <b>[6]</b>
                                         MAO Xiangbo, LIN Binbin, CAI Deng, et al.Parallel field alignment for cross media retrieval[C]//Proceedings of ACM International Conference on Multimedia.New York, USA:ACM Press, 2013:897-906.
                                    </a>
                                </li>
                                <li id="262">


                                    <a id="bibliography_7" title=" LIN Dahua, TANG Xiaoou.Inter-modality face recogni-tion[C]//Proceedings of the 9th European conference on Computer Vision.Berlin, Germany:Springer, 2006:13-26." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Inter-modality face recognition">
                                        <b>[7]</b>
                                         LIN Dahua, TANG Xiaoou.Inter-modality face recogni-tion[C]//Proceedings of the 9th European conference on Computer Vision.Berlin, Germany:Springer, 2006:13-26.
                                    </a>
                                </li>
                                <li id="264">


                                    <a id="bibliography_8" title=" SHARMA A.Generalized multiview analysis:a discriminative latent space[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Computer Society, 2012:2160-2167." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Generalized Multiview analysis:A discriminative latent space">
                                        <b>[8]</b>
                                         SHARMA A.Generalized multiview analysis:a discriminative latent space[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Computer Society, 2012:2160-2167.
                                    </a>
                                </li>
                                <li id="266">


                                    <a id="bibliography_9" title=" 陈祥, 于治楼.基于不同模态语义匹配的跨媒体检索[J].山东师范大学学报 (自然科学版) , 2017, 32 (3) :9-15." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=SDZK201703002&amp;v=MDY3Mjk5Yk1ySTlGWm9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJvRnlubFZiekxOaW5SWmJHNEg=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                         陈祥, 于治楼.基于不同模态语义匹配的跨媒体检索[J].山东师范大学学报 (自然科学版) , 2017, 32 (3) :9-15.
                                    </a>
                                </li>
                                <li id="268">


                                    <a id="bibliography_10" title=" ZHAI Xiaohua, PENG Yuxin, XIAO Jianguo.Learning cross-media joint representation with sparse and semisupervised regularization[J].IEEE Transactions on Circuits and Systems for Video Technology, 2014, 24 (6) :965-978." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning Cross-media Joint Representation with Sparse and Semi-supervised Regularization">
                                        <b>[10]</b>
                                         ZHAI Xiaohua, PENG Yuxin, XIAO Jianguo.Learning cross-media joint representation with sparse and semisupervised regularization[J].IEEE Transactions on Circuits and Systems for Video Technology, 2014, 24 (6) :965-978.
                                    </a>
                                </li>
                                <li id="270">


                                    <a id="bibliography_11" title=" XU Xing, SHIMADA A, TANIGUCHI R, et al.Coupled dictionary learning and feature mapping for cross-modal retrieval[C]//Proceedings of International Conference on Multimedia and Expo.Washington D.C., USA:IEEE Press, 2015:1-6." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Coupled dictionary learning and feature mapping for cross-modal retrieval">
                                        <b>[11]</b>
                                         XU Xing, SHIMADA A, TANIGUCHI R, et al.Coupled dictionary learning and feature mapping for cross-modal retrieval[C]//Proceedings of International Conference on Multimedia and Expo.Washington D.C., USA:IEEE Press, 2015:1-6.
                                    </a>
                                </li>
                                <li id="272">


                                    <a id="bibliography_12" title=" ZHUANG Yueting, WANG Yanfei, WU Fei, et al.Supervised coupled dictionary learning with group structures for multi-modal retrieval [C]//Proceedings of the 27th AAAI Conference on Artificial Intelligence.[S.l.]:AAAI Press, 2013:1070-1076." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Supervised Coupled Dictionary Learning with Group Structures for Multi-modal Retrieval">
                                        <b>[12]</b>
                                         ZHUANG Yueting, WANG Yanfei, WU Fei, et al.Supervised coupled dictionary learning with group structures for multi-modal retrieval [C]//Proceedings of the 27th AAAI Conference on Artificial Intelligence.[S.l.]:AAAI Press, 2013:1070-1076.
                                    </a>
                                </li>
                                <li id="274">


                                    <a id="bibliography_13" title=" ZHAI Xiaohua, PENG Yuxin, XIAO Jianguo.Heterogeneous metric learning with joint graph regularization for cross-media retrieval [C]//Proceedings of the 27th AAAI Conference on Artificial Intelligence.[S.l.]:AAAI Press, 2013:1198-1204." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Heterogeneous metric learning with joint graph regularization for cross-media retrieval">
                                        <b>[13]</b>
                                         ZHAI Xiaohua, PENG Yuxin, XIAO Jianguo.Heterogeneous metric learning with joint graph regularization for cross-media retrieval [C]//Proceedings of the 27th AAAI Conference on Artificial Intelligence.[S.l.]:AAAI Press, 2013:1198-1204.
                                    </a>
                                </li>
                                <li id="276">


                                    <a id="bibliography_14" title=" NGIAM J, KHOSLA A, KIM M, et al.Multimodal deep learning[EB/OL].[2018-08-02].https://www.mendeley.com/catalogue/multimodal-deep-learning/." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multimodal deep learning">
                                        <b>[14]</b>
                                         NGIAM J, KHOSLA A, KIM M, et al.Multimodal deep learning[EB/OL].[2018-08-02].https://www.mendeley.com/catalogue/multimodal-deep-learning/.
                                    </a>
                                </li>
                                <li id="278">


                                    <a id="bibliography_15" title=" SRIVASTAVA N, SALAKHUTDINOV R.Multimodal learning with deep boltzmann machines [J].Journal of Machine Learning Research.2014, 15:2949-2980." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multimodal learning with deep Boltzmann machines">
                                        <b>[15]</b>
                                         SRIVASTAVA N, SALAKHUTDINOV R.Multimodal learning with deep boltzmann machines [J].Journal of Machine Learning Research.2014, 15:2949-2980.
                                    </a>
                                </li>
                                <li id="280">


                                    <a id="bibliography_16" title=" ANDREW G, ARORA R, BILMES J, et al.Deep canonical correlation analysis[C]//Proceedings of the 30th International Conference on International Conference on Machine Learning.[S.l.]:JMLR.org, 2013:1247-1255." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep canonical cor-relation analysis">
                                        <b>[16]</b>
                                         ANDREW G, ARORA R, BILMES J, et al.Deep canonical correlation analysis[C]//Proceedings of the 30th International Conference on International Conference on Machine Learning.[S.l.]:JMLR.org, 2013:1247-1255.
                                    </a>
                                </li>
                                <li id="282">


                                    <a id="bibliography_17" title=" CAI Deng, HE Xiaofei, HAN Jiawei.Spectral regression for efficient regularized subspace learning[C]//Proceedings of the 11th International Conference on Computer Vision.Washington D.C., USA:IEEE Press, 2007:1-8." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Spectral regression for efficient regularized subspacelearning">
                                        <b>[17]</b>
                                         CAI Deng, HE Xiaofei, HAN Jiawei.Spectral regression for efficient regularized subspace learning[C]//Proceedings of the 11th International Conference on Computer Vision.Washington D.C., USA:IEEE Press, 2007:1-8.
                                    </a>
                                </li>
                                <li id="284">


                                    <a id="bibliography_18" title=" ZHOU Jile, DING Guiguang, GUO Yuchen.Latent semantic sparse hashing for cross-modal similarity search[C]//Proceedings of the 37th International ACM SIGIR Conference on Research and Development in Information Retrieval.New York, USA:ACM Press, 2014:415-424." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Latent semantic sparse hashing for cross-modal similarity search">
                                        <b>[18]</b>
                                         ZHOU Jile, DING Guiguang, GUO Yuchen.Latent semantic sparse hashing for cross-modal similarity search[C]//Proceedings of the 37th International ACM SIGIR Conference on Research and Development in Information Retrieval.New York, USA:ACM Press, 2014:415-424.
                                    </a>
                                </li>
                                <li id="286">


                                    <a id="bibliography_19" title=" YU Zhou, WU Fei, YANG Yi, et al.Discriminative coupled dictionary hashing for fast cross-media retrieval[C]//Proceedings of the 37th International ACM SIGIR Conference on Research and Development in Information Retrieval.New York, USA:ACM Press, 2014:395-404." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Discriminative coupled dictionary hashing for fast cross-media retrieval">
                                        <b>[19]</b>
                                         YU Zhou, WU Fei, YANG Yi, et al.Discriminative coupled dictionary hashing for fast cross-media retrieval[C]//Proceedings of the 37th International ACM SIGIR Conference on Research and Development in Information Retrieval.New York, USA:ACM Press, 2014:395-404.
                                    </a>
                                </li>
                                <li id="288">


                                    <a id="bibliography_20" title=" HE Ran, TAN Tieniu, WANG Liang, et al.L&lt;sub&gt;2&lt;/sub&gt;, &lt;sub&gt;1&lt;/sub&gt; regularized correntropy for robust feature selection[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Computer Society, 2012:2504-2511." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=12,1 Regularized correntropy for robust feature selection">
                                        <b>[20]</b>
                                         HE Ran, TAN Tieniu, WANG Liang, et al.L&lt;sub&gt;2&lt;/sub&gt;, &lt;sub&gt;1&lt;/sub&gt; regularized correntropy for robust feature selection[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Computer Society, 2012:2504-2511.
                                    </a>
                                </li>
                                <li id="290">


                                    <a id="bibliography_21" title=" NIKOLOVA M, NG M K.Analysis of half-quadratic minimization methods for signal and image recovery[J].SIAM Journal on Scientific Computing, 2005, 27 (3) :937-966." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Analysis of Half-Quadratic Minimization Methods for Signal and Image Recovery">
                                        <b>[21]</b>
                                         NIKOLOVA M, NG M K.Analysis of half-quadratic minimization methods for signal and image recovery[J].SIAM Journal on Scientific Computing, 2005, 27 (3) :937-966.
                                    </a>
                                </li>
                                <li id="292">


                                    <a id="bibliography_22" title=" HUANG Dean, WANG Y F.Coupled dictionary and feature space Learning with applications to cross-domain image synthesis and recognition[C]//Proceedings of IEEE International Conference on Computer Vision.Washington D.C., USA:IEEE Computer Society, 2013:2496-2503." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Coupled dictionary and feature space learning with applications to cross-domain image synthesis and recognition">
                                        <b>[22]</b>
                                         HUANG Dean, WANG Y F.Coupled dictionary and feature space Learning with applications to cross-domain image synthesis and recognition[C]//Proceedings of IEEE International Conference on Computer Vision.Washington D.C., USA:IEEE Computer Society, 2013:2496-2503.
                                    </a>
                                </li>
                                <li id="294">


                                    <a id="bibliography_23" title=" LEE H, BATTLE A, RAINA R, et al.Efficient sparse coding algorithms[C]//Proceedings of the 19th International Conference on Neural Information Processing Systems.Cambridge, USA:MIT Press, 2006:801-808." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Efficient sparse coding algorithms">
                                        <b>[23]</b>
                                         LEE H, BATTLE A, RAINA R, et al.Efficient sparse coding algorithms[C]//Proceedings of the 19th International Conference on Neural Information Processing Systems.Cambridge, USA:MIT Press, 2006:801-808.
                                    </a>
                                </li>
                                <li id="296">


                                    <a id="bibliography_24" title=" MAIRAL J, BACH F, PONCE J, et al.Online dictionary learning for sparse coding[C]//Proceedings of the 26th Annual International Conference on Machine Learning.New York, USA:ACM Press, 2009:689-696." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Online dictionary learning for sparse coding">
                                        <b>[24]</b>
                                         MAIRAL J, BACH F, PONCE J, et al.Online dictionary learning for sparse coding[C]//Proceedings of the 26th Annual International Conference on Machine Learning.New York, USA:ACM Press, 2009:689-696.
                                    </a>
                                </li>
                                <li id="298">


                                    <a id="bibliography_25" title=" RASIWASIA N, PEREIRA J C, COVIELLO E, et al.A new approach to cross-modal multimedia retrieval[C]//Proceedings of the 18th ACM International Conference on Multimedia.New York, USA:ACM Press, 2010:251-260." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A New Approach to Cross-modal Multimedia Retrieval">
                                        <b>[25]</b>
                                         RASIWASIA N, PEREIRA J C, COVIELLO E, et al.A new approach to cross-modal multimedia retrieval[C]//Proceedings of the 18th ACM International Conference on Multimedia.New York, USA:ACM Press, 2010:251-260.
                                    </a>
                                </li>
                                <li id="300">


                                    <a id="bibliography_26" title=" HWANG S J, GRAUMAN K.Reading between the lines:object localization using implicit cues from image tags[J].IEEE Transactions on Software Engineering, 2012, 34 (6) :1145-1158." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Reading between the Lines: Object Localization Using Implicit Cues from Image Tags">
                                        <b>[26]</b>
                                         HWANG S J, GRAUMAN K.Reading between the lines:object localization using implicit cues from image tags[J].IEEE Transactions on Software Engineering, 2012, 34 (6) :1145-1158.
                                    </a>
                                </li>
                                <li id="302">


                                    <a id="bibliography_27" title=" GONG Yunchao, KE Qifa, ISARD M, et al.A multi-view embedding space for modeling internet images, tags, and their semantics[J].International Journal of Computer Vision, 2014, 106 (2) :210-233." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD14030300028340&amp;v=MjQzNzBNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VTG5JSVY0U2FSRT1OajdCYXJLOEh0TE1ySTlGWk9rSEQzZzVvQg==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[27]</b>
                                         GONG Yunchao, KE Qifa, ISARD M, et al.A multi-view embedding space for modeling internet images, tags, and their semantics[J].International Journal of Computer Vision, 2014, 106 (2) :210-233.
                                    </a>
                                </li>
                                <li id="304">


                                    <a id="bibliography_28" title=" WANG Kaiye, HE Ran, WANG Wei, et al.Learning coupled feature spaces for cross-modal matching[C]//Proceedings of International Conference on Computer Vision.Washington D.C., USA:IEEE Computer Society, 2013:2088-2095." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning coupled feature spaces for cross-modal matching">
                                        <b>[28]</b>
                                         WANG Kaiye, HE Ran, WANG Wei, et al.Learning coupled feature spaces for cross-modal matching[C]//Proceedings of International Conference on Computer Vision.Washington D.C., USA:IEEE Computer Society, 2013:2088-2095.
                                    </a>
                                </li>
                                <li id="306">


                                    <a id="bibliography_29" title=" LOWE D G.Distinctive image features from scale-invariant keypoints[J].International Journal of Computer Vision, 2004, 60 (2) :91-110." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002830901&amp;v=MTUzNTA3bFVyL09JMTA9Tmo3QmFyTzRIdEhPcDR4RmJlc09ZM2s1ekJkaDRqOTlTWHFScnhveGNNSDdSN3FlYnVkdEZD&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[29]</b>
                                         LOWE D G.Distinctive image features from scale-invariant keypoints[J].International Journal of Computer Vision, 2004, 60 (2) :91-110.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJC" target="_blank">计算机工程</a>
                2019,45(06),230-236 DOI:10.19678/j.issn.1000-3428.0052565            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>联合耦合字典学习与图像正则化的跨媒体检索方法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%88%98%E8%8A%B8&amp;code=24135689&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">刘芸</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E4%BA%8E%E6%B2%BB%E6%A5%BC&amp;code=13087581&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">于治楼</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E4%BB%98%E5%BC%BA&amp;code=09149255&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">付强</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%B1%B1%E4%B8%9C%E5%B8%88%E8%8C%83%E5%A4%A7%E5%AD%A6%E4%BF%A1%E6%81%AF%E7%A7%91%E5%AD%A6%E4%B8%8E%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0047921&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">山东师范大学信息科学与工程学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%B5%AA%E6%BD%AE%E9%9B%86%E5%9B%A2%E6%9C%89%E9%99%90%E5%85%AC%E5%8F%B8&amp;code=0027899&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">浪潮集团有限公司</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>跨媒体检索方法多数将2个模态的原始特征映射到公共子空间, 在子空间中执行跨媒体检索, 忽略了判别特征的选择以及模态间的关系。为此, 提出一种基于耦合字典学习和图形正则化的新型跨模态检索方法。通过关联和联合更新不同模态的字典, 为不同的模态生成均匀的稀疏表示。将不同模态的稀疏表示投影到由类标签信息定义的公共子空间中, 以执行跨模态匹配, 同时对投影矩阵施加21范数项, 选择特征空间的相关和辨别性特征。在此基础上, 利用图正则化项保留模态间和模态内相似关系。实验结果表明, 与典型相关分析方法相比, 该方法跨媒体检索精度较高。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%B7%A8%E5%AA%92%E4%BD%93%E6%A3%80%E7%B4%A2&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">跨媒体检索;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">特征选择;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%80%A6%E5%90%88%E5%AD%97%E5%85%B8%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">耦合字典学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%9B%BE%E5%83%8F%E6%AD%A3%E5%88%99%E5%8C%96&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">图像正则化;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%89%B9%E5%BE%81%E6%98%A0%E5%B0%84&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">特征映射;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    刘芸 (1992—) , 女, 硕士研究生, 主研方向为跨媒体检索、机器学习;;
                                </span>
                                <span>
                                    *于治楼 (通信作者) , 研究员;E-mail: zhilyu1@163.com;
                                </span>
                                <span>
                                    付强, 硕士研究生。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-09-04</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金 (61373081);</span>
                    </p>
            </div>
                    <h1><b>Cross-media Retrieval Method Fusing with Coupled Dictionary Learning and Image Regularization</b></h1>
                    <h2>
                    <span>LIU Yun</span>
                    <span>YU Zhilou</span>
                    <span>FU Qiang</span>
            </h2>
                    <h2>
                    <span>School of Information Science and Engineering, Shandong Normal University</span>
                    <span>Inspur Group Co., Ltd.</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>The method of cross-media retrieval mostly maps the original features of two modalities to the common subspace, and performs cross-media retrieval in the subspace, ignoring the selection of discriminant features and the relationship between modalities.Therefore, a new cross-modal retrieval method based on coupled dictionary learning and graph regularization is proposed.A uniform sparse representation is generated for different modalities by associating and jointly updating dictionaries of different modalities.The sparse representations of the different modalities are then projected into the common subspace defined by the class label information to perform cross-modal matching while applying 21 norm terms to the projection matrix to select the correlation and discriminative features of the feature space.On this basis, the regularization term of the graph is used to preserve the inter-modal and intra-modal similar relationship.Experimental results show that compared with the Canonical Correlation Analysis (CCA) method, the method has higher accuracy in cross-media retrieval.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=cross-media%20retrieval&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">cross-media retrieval;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=feature%20selection&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">feature selection;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=coupled%20dictionary%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">coupled dictionary learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=graph%20regularization&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">graph regularization;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=feature%20mapping&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">feature mapping;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-09-04</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="61" name="61" class="anchor-tag">0 概述</h3>
                <div class="p1">
                    <p id="62">随着互联网的发展, 多模态数据被广泛应用于跨媒体检索<citation id="308" type="reference"><link href="250" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>。由于多模态数据通常有不同的特征空间, 因此不同模态特征之间的异质差异是跨媒体检索研究的重点。</p>
                </div>
                <div class="p1">
                    <p id="63">典型相关分析 (Canonical Correlation Analysis, CCA) <citation id="309" type="reference"><link href="252" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>通过寻找2组变量的最优基本向量建立相关性来学习潜在子空间。基于CCA, 国内外学者提出不同算法来处理不同模态问题, 试图学习子空间来进行跨模态检索, 如偏最小二乘 (Partial Least Squares, PLS) <citation id="310" type="reference"><link href="254" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>和双线性模型 (Bilinear Model, BLM) <citation id="311" type="reference"><link href="256" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>。文献<citation id="312" type="reference">[<a class="sup">5</a>]</citation>提出最大协方差展开算法, 将不同输入模态的数据进行降维。文献<citation id="313" type="reference">[<a class="sup">6</a>]</citation>介绍了一种平行字段对齐的跨媒体检索方法。文献<citation id="314" type="reference">[<a class="sup">7</a>]</citation>提出一种通用的判别特征提取 (Common Discriminant Feature Extraction, CDFE) 方法来学习一个共同的特征子空间。文献<citation id="315" type="reference">[<a class="sup">8</a>]</citation>将线性判别分析 (Linear Discriminant Analysis, LDA) 和边际Fisher分析 (Marginal Fisher Analysis , MFA) 扩展到多视图中, 如广义多视图LDA (Generalized Multiview Linear Discriminant Analysis, GMLDA) 和广义多视图MFA (Generalized Multiview Marginal Fisher Analysis, GMMFA) 。GMLDA和GMMFA考虑了语义类别, 并将其运用到语义学习<citation id="316" type="reference"><link href="266" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>。文献<citation id="317" type="reference">[<a class="sup">10</a>]</citation>提出联合表示学习 (Joint Representation Learning, JRL) 方法, 联合使用成对关联和语义信息到统一的优化框架中。文献<citation id="318" type="reference">[<a class="sup">11</a>]</citation>利用字典学习进行跨媒体检索。文献<citation id="319" type="reference">[<a class="sup">12</a>]</citation>提出监督耦合词典学习算法。文献<citation id="320" type="reference">[<a class="sup">13</a>]</citation>提出异构度量学习方法, 能够测量不同媒体类型之间的内容相似度。文献<citation id="321" type="reference">[<a class="sup">14</a>]</citation>应用深度网络学习多种模态特征。文献<citation id="322" type="reference">[<a class="sup">15</a>]</citation>提出深度限制玻耳兹曼机器方法, 其成功地学习多模态数据的联合表示。文献<citation id="323" type="reference">[<a class="sup">16</a>]</citation>提供了深度典型相关分析 (Deep Canonical Correlation Analysis, DCCA) , 可以学习不同形式的数据的复杂非线性投影。文献<citation id="324" type="reference">[<a class="sup">17</a>]</citation>提出一种有效正则化子空间学习的谱回归方法。文献<citation id="325" type="reference">[<a class="sup">18</a>,<a class="sup">19</a>]</citation>提出哈希算法。然而, 多数方法主要关注相关度量, 忽略了内在多样性和相关结构。</p>
                </div>
                <div class="p1">
                    <p id="64">为改进跨模态检索结果, 本文提出一种联合耦合字典学习和特征映射的跨媒体检索方法。通过关联和更新字典, 将多模态数据转换为稀疏表示, 利用耦合线性回归映射稀疏表示, 并投影每个模态的数据到公共关键字子空间中, 同时在映射过程中施加21范式, 使用图像正则化项确保模态内部与模态之间的相似度。</p>
                </div>
                <h3 id="65" name="65" class="anchor-tag">1 相关工作</h3>
                <h4 class="anchor-tag" id="66" name="66">1.1 多模态图正则化</h4>
                <div class="p1">
                    <p id="67">模态间相似性关系:如果不同数据模态属于同一类, 那么不同的数据模态在该类上相似。根据模态间相似性关系, <i>p</i>模态和<i>q</i>模态之间的相似性矩阵<b><i>W</i></b><sup><i>pq</i></sup>定义如下:</p>
                </div>
                <div class="p1">
                    <p id="68" class="code-formula">
                        <mathml id="68"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">W</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mrow><mi>p</mi><mi>q</mi></mrow></msubsup><mo>=</mo><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mn>1</mn><mo>, </mo><mtext>模</mtext><mtext>态</mtext><mi>p</mi><mtext>的</mtext><mtext>第</mtext><mi>i</mi><mtext>个</mtext><mtext>样</mtext><mtext>本</mtext><mtext>与</mtext><mtext>模</mtext><mtext>态</mtext><mi>q</mi><mtext>的</mtext><mtext>第</mtext><mi>j</mi><mtext>个</mtext><mtext>样</mtext><mtext>本</mtext><mtext>有</mtext><mtext>相</mtext><mtext>似</mtext><mtext>语</mtext><mtext>义</mtext></mtd></mtr><mtr><mtd><mn>0</mn><mo>, </mo><mtext>其</mtext><mtext>他</mtext></mtd></mtr></mtable></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="69">模态内相似性关系:如果具有邻域关系的数据对象在公共空间中彼此接近, 那么保留每个模态中的局部结构信息, 构造KNN相似性图。<i>p</i>模态中的相似性矩阵<b><i>W</i></b><sup><i>p</i></sup>定义如下:</p>
                </div>
                <div class="p1">
                    <p id="70" class="code-formula">
                        <mathml id="70"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">W</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mi>p</mi></msubsup><mo>=</mo><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mi>exp</mi><mo stretchy="false"> (</mo><mo>-</mo><mi>z</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mi>p</mi></msubsup><mo>/</mo><mn>2</mn><mi>σ</mi><msup><mrow></mrow><mn>2</mn></msup><mo stretchy="false">) </mo><mo>, </mo><mspace width="0.25em" /><mspace width="0.25em" /><mi mathvariant="bold-italic">x</mi><msubsup><mrow></mrow><mi>i</mi><mi>p</mi></msubsup><mo>∈</mo><mi>Ν</mi><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><msubsup><mrow></mrow><mi>j</mi><mi>p</mi></msubsup><mo stretchy="false">) </mo><mtext>或</mtext><mi mathvariant="bold-italic">x</mi><msubsup><mrow></mrow><mi>j</mi><mi>p</mi></msubsup><mo>∈</mo><mi>Ν</mi><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><msubsup><mrow></mrow><mi>i</mi><mi>p</mi></msubsup><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mn>0</mn><mo>, </mo><mtext>其</mtext><mtext>他</mtext></mtd></mtr></mtable></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="71">其中, <i>z</i><mathml id="72"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mi>p</mi></msubsup></mrow></math></mathml>是<b><i>x</i></b><mathml id="73"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>p</mi></msubsup></mrow></math></mathml>和<b><i>x</i></b><mathml id="74"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mi>p</mi></msubsup></mrow></math></mathml>之间的欧氏距离, <mathml id="75"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>z</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mi>p</mi></msubsup><mo>=</mo><mrow><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">x</mi><msubsup><mrow></mrow><mi>i</mi><mi>p</mi></msubsup><mo>-</mo><mi mathvariant="bold-italic">z</mi><msubsup><mrow></mrow><mi>j</mi><mi>p</mi></msubsup><mo stretchy="false">∥</mo></mrow><msup><mrow></mrow><mn>2</mn></msup><mo>, </mo><mi>Ν</mi><msub><mrow></mrow><mi>k</mi></msub><mrow><mo> (</mo><mrow><mi mathvariant="bold-italic">x</mi><msubsup><mrow></mrow><mi>i</mi><mi>p</mi></msubsup></mrow><mo>) </mo></mrow></mrow></math></mathml>表示<b><i>x</i></b><mathml id="76"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>p</mi></msubsup></mrow></math></mathml>的一组k个最近邻。式 (2) 保留每种模态中数据对象之间的模态内部相似性关系。</p>
                </div>
                <div class="p1">
                    <p id="77">根据上述相似性关系, 将数据的所有不同模态放入到一个联合的多模态图中。整体相似度矩阵<b><i>W</i></b>定义如下:</p>
                </div>
                <div class="p1">
                    <p id="78" class="code-formula">
                        <mathml id="78"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">W</mi><mo>=</mo><mrow><mo>[</mo><mrow><mtable><mtr><mtd><mi>β</mi><mi>W</mi><msup><mrow></mrow><mn>1</mn></msup></mtd><mtd><mi>W</mi><msup><mrow></mrow><mrow><mn>1</mn><mn>2</mn></mrow></msup></mtd><mtd><mo>⋯</mo></mtd><mtd><mi>W</mi><msup><mrow></mrow><mrow><mn>1</mn><mi>Μ</mi></mrow></msup></mtd></mtr><mtr><mtd><mi>W</mi><msup><mrow></mrow><mrow><mn>2</mn><mn>1</mn></mrow></msup></mtd><mtd><mi>β</mi><mi>W</mi><msup><mrow></mrow><mn>2</mn></msup></mtd><mtd><mo>⋯</mo></mtd><mtd><mi>W</mi><msup><mrow></mrow><mrow><mn>2</mn><mi>Μ</mi></mrow></msup></mtd></mtr><mtr><mtd><mo>⋮</mo></mtd><mtd><mo>⋮</mo></mtd><mtd></mtd><mtd><mo>⋮</mo></mtd></mtr><mtr><mtd><mi>W</mi><msup><mrow></mrow><mrow><mi>Μ</mi><mn>1</mn></mrow></msup></mtd><mtd><mi>W</mi><msup><mrow></mrow><mrow><mi>Μ</mi><mn>2</mn></mrow></msup></mtd><mtd><mo>⋯</mo></mtd><mtd><mi>β</mi><mi>W</mi><msup><mrow></mrow><mi>Μ</mi></msup></mtd></mtr></mtable></mrow><mo>]</mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="79">其中, <i>β</i>用于平衡模态间相似性和模态内相似性的参数, <i>W</i><sup><i>ij</i></sup>表示由式 (1) 定义的模态间相似性, <i>W</i><sup><i>i</i></sup>表示由式 (2) 定义的模态内相似性, <i>i</i>、<i>j</i>=1, 2, …, <i>M</i>。</p>
                </div>
                <div class="p1">
                    <p id="80">基于多模态图, 多模态图表项可表示为:</p>
                </div>
                <div class="p1">
                    <p id="81" class="code-formula">
                        <mathml id="81"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ω</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">U</mi><msub><mrow></mrow><mn>1</mn></msub><mo>, </mo><mi mathvariant="bold-italic">U</mi><msub><mrow></mrow><mn>2</mn></msub><mo>, </mo><mo>⋯</mo><mo>, </mo><mi mathvariant="bold-italic">U</mi><msub><mrow></mrow><mi>Μ</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mi mathvariant="bold-italic">W</mi></mstyle></mrow></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mrow><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">f</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mi mathvariant="bold-italic">f</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">∥</mo></mrow><msup><mrow></mrow><mn>2</mn></msup><mo>=</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="82">tr (<b><i>FLF</i></b><sup>T</sup>)      (4) </p>
                </div>
                <div class="p1">
                    <p id="83">其中, <i>N</i>为所有模态的总样本数量, <b><i>L</i></b>=<b><i>D</i></b>-<b><i>W</i></b>是拉普拉斯矩阵, <b><i>F</i></b>= (<b><i>F</i></b><sup>T</sup><sub>1</sub>, <b><i>F</i></b><sup>T</sup><sub>2</sub>, …, <b><i>F</i></b><sup>T</sup><sub><i>M</i></sub>) = (<b><i>U</i></b><sup>T</sup><sub>1</sub><b><i>X</i></b><mathml id="84"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>1</mn><mi>b</mi></msubsup></mrow></math></mathml>, <b><i>U</i></b><sup>T</sup><sub>2</sub><b><i>X</i></b><mathml id="85"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>2</mn><mi>b</mi></msubsup></mrow></math></mathml>, …, <b><i>U</i></b><sup>T</sup><sub><i>M</i></sub><b><i>X</i></b><mathml id="86"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>Μ</mi><mi>b</mi></msubsup></mrow></math></mathml>) 表示在共同空间中映射数据的全部模态, 则式 (4) 可表示为:</p>
                </div>
                <div class="p1">
                    <p id="87" class="code-formula">
                        <mathml id="87"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ω</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">U</mi><msub><mrow></mrow><mn>1</mn></msub><mo>, </mo><mi mathvariant="bold-italic">U</mi><msub><mrow></mrow><mn>2</mn></msub><mo>, </mo><mo>⋯</mo><mo>, </mo><mi mathvariant="bold-italic">U</mi><msub><mrow></mrow><mi>Μ</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>p</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>q</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mtext>t</mtext></mstyle></mrow></mstyle><mtext>r</mtext><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">U</mi><msubsup><mrow></mrow><mi>p</mi><mtext>Τ</mtext></msubsup><mi mathvariant="bold-italic">X</mi><msubsup><mrow></mrow><mi>p</mi><mi>b</mi></msubsup><mi mathvariant="bold-italic">L</mi><msub><mrow></mrow><mrow><mi>p</mi><mi>q</mi></mrow></msub><mrow><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">X</mi><msubsup><mrow></mrow><mi>q</mi><mi>b</mi></msubsup><mo stretchy="false">) </mo></mrow><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">U</mi><msub><mrow></mrow><mi>q</mi></msub><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>5</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="88">多模态图正则化项可以在映射过程中保持模态间和模态内相似性, 且在多模态图中可以将不同类型的数据考虑在内。</p>
                </div>
                <h4 class="anchor-tag" id="89" name="89">1.2 特征选择</h4>
                <div class="p1">
                    <p id="90">特征选择使用选择标准来定位一组最佳特征, 通过消除不相关和多余的特性来降低数据的差异性, 减少存储容量和计算成本, 有助于理解学习模型或数据。</p>
                </div>
                <div class="p1">
                    <p id="91">对于矩阵<b><i>M</i></b>∈<image href="images/JSJC201906037_092.jpg" type="" display="inline" placement="inline"><alt></alt></image><sup><i>n</i>×<i>m</i></sup>, <b><i>m</i></b><sub><i>i</i></sub>代表矩阵的第<i>i</i>行, <b><i>m</i></b><sub><i>j</i></sub>表示矩阵的第<i>j</i>列。矩阵<b><i>M</i></b>的<i>F</i>范式定义为:<mathml id="93"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">Μ</mi><mo stretchy="false">∥</mo><msub><mrow></mrow><mi>F</mi></msub><mo>=</mo><msqrt><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mo stretchy="false">∥</mo></mstyle><mi mathvariant="bold-italic">m</mi><msup><mrow></mrow><mi>i</mi></msup><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></msqrt><mo>, </mo><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">Μ</mi><mo stretchy="false">∥</mo><msub><mrow></mrow><mrow><mn>2</mn><mn>1</mn></mrow></msub></mrow></math></mathml>代表矩阵<b><i>M</i></b>所有行2范式的和:<mathml id="94"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">Μ</mi><mo stretchy="false">∥</mo><msub><mrow></mrow><mrow><mn>2</mn><mn>1</mn></mrow></msub><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mo stretchy="false">∥</mo></mstyle><mi mathvariant="bold-italic">m</mi><msup><mrow></mrow><mi>i</mi></msup><mo stretchy="false">∥</mo><msub><mrow></mrow><mn>2</mn></msub></mrow></math></mathml>。通过文献<citation id="326" type="reference">[<a class="sup">20</a>]</citation>中对21范式的分析, 定义如下:</p>
                </div>
                <div class="p1">
                    <p id="95" class="code-formula">
                        <mathml id="95"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>f</mi><mrow><mo> (</mo><mi>x</mi><mo>) </mo></mrow><mo>=</mo><msqrt><mrow><mi>x</mi><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mi>ε</mi></mrow></msqrt><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>6</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="96">使用<mathml id="97"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>d</mi><msub><mrow></mrow><mi>p</mi></msub></mrow></munderover><mi>f</mi></mstyle><mo stretchy="false"> (</mo><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">u</mi><msubsup><mrow></mrow><mi>p</mi><mi>i</mi></msubsup><mo stretchy="false">∥</mo><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">) </mo></mrow></math></mathml>代替 ‖<b><i>U</i></b><sub><i>p</i></sub>‖<sub>21</sub>, <i>ε</i>是平滑项, 通常被设置为一个很小的数值, 可以证明<mathml id="98"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>f</mi><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mo>=</mo><msqrt><mrow><mi>x</mi><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mi>ε</mi></mrow></msqrt></mrow></math></mathml>满足以下所有条件:<mathml id="99"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>x</mi><mo>→</mo><mi>f</mi><mrow><mo> (</mo><mi>x</mi><mo>) </mo></mrow></mrow></math></mathml>在<image href="images/JSJC201906037_100.jpg" type="" display="inline" placement="inline"><alt></alt></image>上是凸的, <mathml id="101"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>x</mi><mo>→</mo><mi>f</mi></mrow><mrow><mo> (</mo><mrow><msqrt><mi>x</mi></msqrt></mrow><mo>) </mo></mrow></mrow></math></mathml>在<image href="images/JSJC201906037_102.jpg" type="" display="inline" placement="inline"><alt></alt></image><sub>+</sub>上是凹的, <mathml id="103"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>f</mi><mrow><mo> (</mo><mi>x</mi><mo>) </mo></mrow><mo>=</mo><mi>f</mi><mrow><mo> (</mo><mrow><mo>-</mo><mi>x</mi></mrow><mo>) </mo></mrow><mo>, </mo><mo>∀</mo><mi>x</mi><mo>∈</mo></mrow></math></mathml><image href="images/JSJC201906037_104.jpg" type="" display="inline" placement="inline"><alt></alt></image>, <mathml id="105"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>f</mi><mrow><mo> (</mo><mi>x</mi><mo>) </mo></mrow></mrow></math></mathml>在<image href="images/JSJC201906037_106.jpg" type="" display="inline" placement="inline"><alt></alt></image>上是常数, <mathml id="107"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>f</mi><msup><mtext> </mtext><mo>″</mo></msup><mrow><mo> (</mo><mrow><mn>0</mn><msup><mrow></mrow><mo>+</mo></msup></mrow><mo>) </mo></mrow><mo>&gt;</mo><mn>0</mn><mo>, </mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>lim</mi></mrow></mstyle><mrow><mi>x</mi><mo>→</mo><mi>∞</mi></mrow></munder><mi>f</mi><mrow><mo> (</mo><mi>x</mi><mo>) </mo></mrow><mo>/</mo><mi>x</mi><msup><mrow></mrow><mn>2</mn></msup><mo>=</mo><mn>0</mn></mrow></math></mathml>。</p>
                </div>
                <div class="p1">
                    <p id="108">根据引理1, 以半二次型方式<citation id="327" type="reference"><link href="290" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>优化<mathml id="109"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>f</mi><mrow><mo> (</mo><mi>x</mi><mo>) </mo></mrow></mrow></math></mathml>。</p>
                </div>
                <div class="p1">
                    <p id="110"><b>引理1</b> 设<mathml id="111"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>f</mi><mrow><mo> (</mo><mi>x</mi><mo>) </mo></mrow></mrow></math></mathml>是满足上述所有条件的函数, 对于固定的‖<b><i>u</i></b><sup><i>i</i></sup>‖<sub>2</sub>, 存在一个双重潜在函数, 可表示为:</p>
                </div>
                <div class="p1">
                    <p id="112"><i>φ</i> (‖<b><i>u</i></b><sup><i>i</i></sup>‖<sub>2</sub>) =inf<i>s</i>∈<image href="images/JSJC201906037_113.jpg" type="" display="inline" placement="inline"><alt></alt></image>{<i>s</i>‖<b><i>u</i></b><sup><i>i</i></sup>‖<mathml id="114"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></math></mathml>+<i>φ</i> (<i>s</i>) }      (7) </p>
                </div>
                <div class="p1">
                    <p id="115">其中, <i>s</i>由最小化函数<i>φ</i> (<i>s</i>) 决定。根据引理1, 可以得到:</p>
                </div>
                <div class="p1">
                    <p id="116" class="code-formula">
                        <mathml id="116"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mrow><mi>U</mi><msub><mrow></mrow><mn>1</mn></msub><mo>, </mo><mi>U</mi><msub><mrow></mrow><mn>2</mn></msub><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>U</mi><msub><mrow></mrow><mi>Μ</mi></msub></mrow></munder><mspace width="0.25em" /><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>p</mi><mo>=</mo><mn>1</mn></mrow><mi>Μ</mi></munderover><mrow></mrow></mstyle></mrow><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">U</mi><msub><mrow></mrow><mi>p</mi></msub><mo stretchy="false">∥</mo><msub><mrow></mrow><mrow><mn>2</mn><mn>1</mn></mrow></msub><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mrow><mi>U</mi><msub><mrow></mrow><mn>1</mn></msub><mo>, </mo><mi>U</mi><msub><mrow></mrow><mn>2</mn></msub><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>U</mi><msub><mrow></mrow><mi>Μ</mi></msub></mrow></munder><mspace width="0.25em" /><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>p</mi><mo>=</mo><mn>1</mn></mrow><mi>Μ</mi></munderover><mrow></mrow></mstyle></mrow><mtext>t</mtext><mtext>r</mtext><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">U</mi><msubsup><mrow></mrow><mi>p</mi><mtext>Τ</mtext></msubsup><mi mathvariant="bold-italic">R</mi><msub><mrow></mrow><mi>p</mi></msub><mi mathvariant="bold-italic">U</mi><msub><mrow></mrow><mi>p</mi></msub><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>8</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="117">其中, <b><i>R</i></b><sub><i>p</i></sub>=Diag (<b><i>r</i></b><sub><i>p</i></sub>) , <b><i>r</i></b><sub><i>p</i></sub>是21 范式的附加向量, 第<i>i</i>个元素为<i>r</i><mathml id="118"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>p</mi><mi>i</mi></msubsup></mrow></math></mathml>=12‖<b><i>u</i></b><mathml id="119"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>p</mi><mi>i</mi></msubsup></mrow></math></mathml>‖<sub>2</sub>, 对<b><i>r</i></b><sub><i>p</i></sub>的元素进行规则化, 有:</p>
                </div>
                <div class="p1">
                    <p id="120" class="code-formula">
                        <mathml id="120"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>r</mi><msubsup><mrow></mrow><mi>p</mi><mi>i</mi></msubsup><mo>=</mo><mfrac><mn>1</mn><mrow><mn>2</mn><msqrt><mrow><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">u</mi><msubsup><mrow></mrow><mi>p</mi><mi>i</mi></msubsup><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup><mo>+</mo><mi>ε</mi></mrow></msqrt></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>9</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="121">其中, ‖<b><i>u</i></b><mathml id="122"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>p</mi><mi>i</mi></msubsup></mrow></math></mathml>‖<sub>2</sub>在理论上可以为0, 但不能将<i>r</i><mathml id="123"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>p</mi><mi>i</mi></msubsup></mrow></math></mathml>设置为0, 否则迭代算法不能保证收敛。为解决此问题, 在式 (9) 中规则化r<mathml id="124"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>p</mi><mi>i</mi></msubsup></mrow></math></mathml>。</p>
                </div>
                <h3 id="125" name="125" class="anchor-tag">2 跨媒体检索方法</h3>
                <h4 class="anchor-tag" id="126" name="126">2.1 耦合字典学习</h4>
                <div class="p1">
                    <p id="127">在跨媒体检索中, 通常将M设置为2, 即图像与文本。本文将图像模态的数据表示为<mathml id="128"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">V</mi><mo>=</mo><mrow><mo> (</mo><mrow><mi>V</mi><msub><mrow></mrow><mn>1</mn></msub><mo>, </mo><mi>V</mi><msub><mrow></mrow><mn>2</mn></msub><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>V</mi><msub><mrow></mrow><mi>n</mi></msub></mrow><mo>) </mo></mrow></mrow></math></mathml>, 来自文本模态的数据表示为<b><i>T</i></b>= (<i>T</i><sub>1</sub>, <i>T</i><sub>2</sub>, …, <i>T</i><sub><i>n</i></sub>) , 其中, <i>n</i>代表样本的总数量, <b><i>V</i></b>中的特征是<i>d</i><sub>1</sub>维度, <b><i>T</i></b>中的特征是<i>d</i><sub>2</sub>维度。本文通过字典学习的稀疏表示来呈现每种模态, 以实现<b><i>V</i></b>和<b><i>T</i></b>这2种不同模态的多样性。对于耦合项, 本文不直接找到不同模态字典之间的关系。相反, 本文利用强制关联函数来关联不同模态的稀疏表示, 得到<b><i>A</i></b><sub><i>V</i></sub>和<b><i>A</i></b><sub><i>T</i></sub>之间的关系, 就可以得到<b><i>D</i></b><sub><i>V</i></sub>和<b><i>D</i></b><sub><i>T</i></sub>之间的关系。因此, 耦合字典学习可以转换为:</p>
                </div>
                <div class="p1">
                    <p id="129" class="code-formula">
                        <mathml id="129"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mrow><mi>D</mi><msub><mrow></mrow><mi>V</mi></msub><mo>, </mo><mi>D</mi><msub><mrow></mrow><mi>Τ</mi></msub><mo>, </mo><mi>A</mi><msub><mrow></mrow><mi>V</mi></msub><mo>, </mo><mi>A</mi><msub><mrow></mrow><mi>Τ</mi></msub></mrow></munder><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">V</mi><mo>-</mo><mi mathvariant="bold-italic">D</mi><msub><mrow></mrow><mi>V</mi></msub><mi mathvariant="bold-italic">A</mi><msub><mrow></mrow><mi>V</mi></msub><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mi>F</mi><mn>2</mn></msubsup><mo>+</mo><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">Τ</mi><mo>-</mo><mi mathvariant="bold-italic">D</mi><msub><mrow></mrow><mi>Τ</mi></msub><mi mathvariant="bold-italic">A</mi><msub><mrow></mrow><mi>Τ</mi></msub><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mi>F</mi><mn>2</mn></msubsup><mo>+</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mi>σ</mi><mo stretchy="false"> (</mo><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">A</mi><msub><mrow></mrow><mi>V</mi></msub><mo stretchy="false">∥</mo><msub><mrow></mrow><mn>1</mn></msub><mo>+</mo><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">A</mi><msub><mrow></mrow><mi>Τ</mi></msub><mo stretchy="false">∥</mo><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">) </mo><mo>+</mo><mi>f</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">A</mi><msub><mrow></mrow><mi>V</mi></msub><mo>, </mo><mi mathvariant="bold-italic">A</mi><msub><mrow></mrow><mi>Τ</mi></msub><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mtext>s</mtext><mo>.</mo><mtext>t</mtext><mo>.</mo><mo stretchy="false">∥</mo><mi>d</mi><msub><mrow></mrow><mrow><mi>v</mi><mo>, </mo><mi>i</mi></mrow></msub><mo stretchy="false">∥</mo><msub><mrow></mrow><mn>2</mn></msub><mo>≤</mo><mn>1</mn><mo>, </mo><mo stretchy="false">∥</mo><mi>d</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>, </mo><mi>i</mi></mrow></msub><mo stretchy="false">∥</mo><msub><mrow></mrow><mn>2</mn></msub><mo>≤</mo><mn>1</mn><mo>, </mo><mo>∀</mo><mi>i</mi><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>0</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="130">其中, <i>σ</i>是正则化参数, <b><i>V</i></b>和<b><i>T</i></b>是2种模态的原始特征, <b><i>A</i></b><sub><i>V</i></sub>、<b><i>A</i></b><sub><i>T</i></sub>是2种模态的稀疏表示, <b><i>D</i></b><sub><i>V</i></sub>∈<image href="images/JSJC201906037_131.jpg" type="" display="inline" placement="inline"><alt></alt></image><sup><i>d</i><sub>1</sub>×<i>k</i><sub>1</sub></sup>、<b><i>D</i></b><sub><i>T</i></sub>∈<image href="images/JSJC201906037_132.jpg" type="" display="inline" placement="inline"><alt></alt></image><sup><i>d</i><sub>2</sub>×<i>k</i><sub>2</sub></sup>是2种模态的字典, <i>k</i><sub>1</sub>和<i>k</i><sub>2</sub>是2个字典<b><i>D</i></b><sub><i>V</i></sub>和<b><i>D</i></b><sub><i>T</i></sub>的大小, <i>f</i> (<b><i>A</i></b><sub><i>V</i></sub>, <b><i>A</i></b><sub><i>T</i></sub>) 是<b><i>A</i></b><sub><i>V</i></sub>、<b><i>A</i></b><sub><i>T</i></sub>定义跨模态关系的关联函数, 通过得到<b><i>A</i></b><sub><i>V</i></sub>和<b><i>A</i></b><sub><i>T</i></sub>之间的关系, 可以相应地更新字典<b><i>D</i></b><sub><i>V</i></sub>和<b><i>D</i></b><sub><i>T</i></sub>。</p>
                </div>
                <div class="p1">
                    <p id="133">对于<i>f</i> (<b><i>A</i></b><sub><i>V</i></sub>, <b><i>A</i></b><sub><i>T</i></sub>) , 根据文献[22]方案, 本文引入一个<i>k</i><sub><i>C</i></sub>维的公共特征空间<i>P</i>来关联不同模态的稀疏表示, 设定<i>k</i><sub><i>C</i></sub>=<i>k</i><sub>1</sub>=<i>k</i><sub>2</sub>, 使数据的不同模态在<i>P</i>中具有可比性。<b><i>P</i></b><sub><i>T</i></sub>=<b><i>U</i></b><sub><i>T</i></sub>×<b><i>A</i></b><sub><i>T</i></sub>是来自文本模态的稀疏表示<b><i>A</i></b><sub><i>T</i></sub>的投影数据, 其中, <b><i>U</i></b><sub><i>T</i></sub>∈<image href="images/JSJC201906037_134.jpg" type="" display="inline" placement="inline"><alt></alt></image><sup><i>k</i><sub><i>c</i></sub>×<i>k</i><sub>2</sub></sup>是映射矩阵, <b><i>P</i></b><sub><i>V</i></sub>=<b><i>U</i></b><sub><i>V</i></sub>×<b><i>A</i></b><sub><i>V</i></sub>是来自图像模态的稀疏表示<b><i>A</i></b><sub><i>V</i></sub>的投影数据, <b><i>U</i></b><sub><i>V</i></sub>∈<image href="images/JSJC201906037_135.jpg" type="" display="inline" placement="inline"><alt></alt></image><sup><i>k</i><sub><i>c</i></sub>×<i>k</i><sub>1</sub></sup>是映射矩阵, 将一个文本模态的稀疏表示映射到公共特征空间<i>P</i>的任意的一个实例<b><i>p</i></b>=<b><i>U</i></b><sub><i>T</i></sub><i>α</i><sub><i>T</i></sub>, 重建图像模态的相应稀疏表示<i>α</i><sub><i>V</i></sub>=<b><i>U</i></b><mathml id="136"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>V</mi><mrow><mo>-</mo><mn>1</mn></mrow></msubsup></mrow></math></mathml><b><i>p</i></b>, <i>f</i> (<b><i>A</i></b><sub><i>V</i></sub>, <b><i>A</i></b><sub><i>T</i></sub>) 定义为:</p>
                </div>
                <div class="p1">
                    <p id="137"><i>f</i> (<b><i>A</i></b><sub><i>V</i></sub>, <b><i>A</i></b><sub><i>T</i></sub>) =<i>γ</i> (‖<b><i>A</i></b><sub><i>V</i></sub>-<b><i>U</i></b><sup>-1</sup><sub><i>V</i></sub><b><i>P</i></b><sub><i>T</i></sub>‖<sup>2</sup><sub>F</sub>+</p>
                </div>
                <div class="p1">
                    <p id="138">‖<b><i>A</i></b><sub><i>T</i></sub>-<b><i>U</i></b><sup>-1</sup><sub><i>T</i></sub><b><i>P</i></b><sub><i>V</i></sub>‖<sup>2</sup><sub>F</sub>) +<i>μ</i> (‖<b><i>U</i></b><sup>-1</sup><sub><i>V</i></sub>‖<sup>2</sup><sub>F</sub>+‖<b><i>U</i></b><sup>-1</sup><sub><i>T</i></sub>‖<sup>2</sup><sub>F</sub>)      (11) </p>
                </div>
                <div class="p1">
                    <p id="139">其中, 参数<i>γ</i>用来平衡图像表示, 对<b><i>U</i></b><mathml id="140"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>V</mi><mrow><mo>-</mo><mn>1</mn></mrow></msubsup></mrow></math></mathml>和<b><i>U</i></b><mathml id="141"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>Τ</mi><mrow><mo>-</mo><mn>1</mn></mrow></msubsup></mrow></math></mathml>施加了额外约束以保证数值稳定性和避免过度拟合。将式 (11) 代入式 (10) 中, 可以得到耦合字典学习过程的函数。目标函数对于字典<mathml id="142"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>{</mo><mrow><mi mathvariant="bold-italic">D</mi><msub><mrow></mrow><mi>V</mi></msub><mo>, </mo><mi mathvariant="bold-italic">D</mi><msub><mrow></mrow><mi>Τ</mi></msub></mrow><mo>}</mo></mrow></mrow></math></mathml>、稀疏表示<mathml id="143"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>{</mo><mrow><mi mathvariant="bold-italic">A</mi><msub><mrow></mrow><mi>V</mi></msub><mo>, </mo><mi mathvariant="bold-italic">A</mi><msub><mrow></mrow><mi>Τ</mi></msub></mrow><mo>}</mo></mrow></mrow></math></mathml>和映射矩阵<mathml id="144"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>{</mo><mrow><mi mathvariant="bold-italic">U</mi><msub><mrow></mrow><mi>V</mi></msub><mo>, </mo><mi mathvariant="bold-italic">U</mi><msub><mrow></mrow><mi>Τ</mi></msub></mrow><mo>}</mo></mrow></mrow></math></mathml>是非凸的, 但是在固定其他变量时, 它们相对于每一个是凸的, 通过固定另外2个来求其中一个的交替方式迭代地进行更新。应用迭代算法分别优化字典<b><i>D</i></b>、稀疏系数<b><i>A</i></b>和投影矩阵<b><i>U</i></b>。</p>
                </div>
                <div class="p1">
                    <p id="145">本文应用联合字典学习方法来计算初始化的<b><i>D</i></b><sub><i>V</i></sub>和<b><i>D</i></b><sub><i>T</i></sub>。当在每次迭代期间更新2个字典时, 将稀疏系数<b><i>A</i></b>和投影矩阵<b><i>U</i></b>视为常数。因此, 式 (10) 可以简化为以下形式:</p>
                </div>
                <div class="p1">
                    <p id="146" class="code-formula">
                        <mathml id="146"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mrow><mi>D</mi><msub><mrow></mrow><mi>V</mi></msub></mrow></munder><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">V</mi><mo>-</mo><mi mathvariant="bold-italic">D</mi><msub><mrow></mrow><mi>V</mi></msub><mi mathvariant="bold-italic">A</mi><msub><mrow></mrow><mi>V</mi></msub><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mtext>F</mtext><mn>2</mn></msubsup><mtext> </mtext><mtext>s</mtext><mo>.</mo><mtext>t</mtext><mo>.</mo><mo stretchy="false">∥</mo><mi>d</mi><msub><mrow></mrow><mrow><mi>v</mi><mo>, </mo><mi>i</mi></mrow></msub><mo stretchy="false">∥</mo><msub><mrow></mrow><mn>2</mn></msub><mo>≤</mo><mn>1</mn></mtd></mtr><mtr><mtd><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mrow><mi>D</mi><msub><mrow></mrow><mi>Τ</mi></msub></mrow></munder><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">Τ</mi><mo>-</mo><mi mathvariant="bold-italic">D</mi><msub><mrow></mrow><mi>Τ</mi></msub><mi mathvariant="bold-italic">A</mi><msub><mrow></mrow><mi>Τ</mi></msub><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mtext>F</mtext><mn>2</mn></msubsup><mtext> </mtext><mtext>s</mtext><mo>.</mo><mtext>t</mtext><mo>.</mo><mo stretchy="false">∥</mo><mi>d</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>, </mo><mi>i</mi></mrow></msub><mo stretchy="false">∥</mo><msub><mrow></mrow><mn>2</mn></msub><mo>≤</mo><mn>1</mn></mtd></mtr></mtable></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="147">式 (12) 是关于<b><i>D</i></b><sub><i>V</i></sub>或<b><i>D</i></b><sub><i>V</i></sub>的二次约束二次规划问题, 可以使用拉格朗日双重技术解决<citation id="328" type="reference"><link href="294" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>。当在每次迭代期间更新2个稀疏系数时, 将字典<b><i>D</i></b>和投影矩阵<b><i>U</i></b>视为常数, 有:</p>
                </div>
                <div class="p1">
                    <p id="148" class="code-formula">
                        <mathml id="148"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mrow><mi>A</mi><msub><mrow></mrow><mi>V</mi></msub></mrow></munder><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">V</mi><mo>-</mo><mi mathvariant="bold-italic">D</mi><msub><mrow></mrow><mi>V</mi></msub><mi mathvariant="bold-italic">A</mi><msub><mrow></mrow><mi>V</mi></msub><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mtext>F</mtext><mn>2</mn></msubsup><mo>+</mo><mi>σ</mi><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">A</mi><msub><mrow></mrow><mi>V</mi></msub><mo stretchy="false">∥</mo><msub><mrow></mrow><mn>1</mn></msub><mo>+</mo><mi>γ</mi><mrow><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">A</mi><msub><mrow></mrow><mi>V</mi></msub><mo>-</mo><mi mathvariant="bold-italic">U</mi><msubsup><mrow></mrow><mi>V</mi><mrow><mo>-</mo><mn>1</mn></mrow></msubsup><mi mathvariant="bold-italic">Ρ</mi><msub><mrow></mrow><mi>Τ</mi></msub><mo stretchy="false">∥</mo></mrow><msubsup><mrow></mrow><mtext>F</mtext><mn>2</mn></msubsup></mtd></mtr><mtr><mtd><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mrow><mi>A</mi><msub><mrow></mrow><mi>Τ</mi></msub></mrow></munder><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">Τ</mi><mo>-</mo><mi mathvariant="bold-italic">D</mi><msub><mrow></mrow><mi>Τ</mi></msub><mi mathvariant="bold-italic">A</mi><msub><mrow></mrow><mi>Τ</mi></msub><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mtext>F</mtext><mn>2</mn></msubsup><mo>+</mo><mi>σ</mi><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">A</mi><msub><mrow></mrow><mi>Τ</mi></msub><mo stretchy="false">∥</mo><msub><mrow></mrow><mn>1</mn></msub><mo>+</mo><mi>γ</mi><mrow><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">U</mi><msubsup><mrow></mrow><mi>Τ</mi><mrow><mo>-</mo><mn>1</mn></mrow></msubsup><mi mathvariant="bold-italic">Ρ</mi><msub><mrow></mrow><mi>V</mi></msub><mo>-</mo><mi mathvariant="bold-italic">A</mi><msub><mrow></mrow><mi>Τ</mi></msub><mo stretchy="false">∥</mo></mrow><msubsup><mrow></mrow><mtext>F</mtext><mn>2</mn></msubsup></mtd></mtr></mtable></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="149">为简化上述问题, 将式 (13) 中的第一项和最后一项结合起来, 并按如下方式定义最小化问题。</p>
                </div>
                <div class="p1">
                    <p id="150" class="code-formula">
                        <mathml id="150"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mrow><mi>A</mi><msub><mrow></mrow><mi>V</mi></msub></mrow></munder><mo stretchy="false">∥</mo><mover><mstyle mathsize="140%" displaystyle="true"><mi mathvariant="bold-italic">V</mi></mstyle><mrow><mspace width="0.25em" /><mo>∼</mo></mrow></mover><mo>-</mo><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi mathvariant="bold-italic">D</mi></mstyle><mrow><mspace width="0.25em" /><mo>∼</mo></mrow></mover><msub><mrow></mrow><mi>V</mi></msub></mrow><mi mathvariant="bold-italic">A</mi><msub><mrow></mrow><mi>V</mi></msub><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mtext>F</mtext><mn>2</mn></msubsup><mo>+</mo><mi>σ</mi><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">A</mi><msub><mrow></mrow><mi>V</mi></msub><mo stretchy="false">∥</mo><msub><mrow></mrow><mn>1</mn></msub></mtd></mtr><mtr><mtd><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mrow><mi>A</mi><msub><mrow></mrow><mi>Τ</mi></msub></mrow></munder><mo stretchy="false">∥</mo><mover><mstyle mathsize="140%" displaystyle="true"><mi mathvariant="bold-italic">Τ</mi></mstyle><mrow><mspace width="0.25em" /><mo>∼</mo></mrow></mover><mo>-</mo><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi mathvariant="bold-italic">D</mi></mstyle><mrow><mspace width="0.25em" /><mo>∼</mo></mrow></mover><msub><mrow></mrow><mi>Τ</mi></msub></mrow><mi mathvariant="bold-italic">A</mi><msub><mrow></mrow><mi>Τ</mi></msub><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mtext>F</mtext><mn>2</mn></msubsup><mo>+</mo><mi>σ</mi><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">A</mi><msub><mrow></mrow><mi>Τ</mi></msub><mo stretchy="false">∥</mo><msub><mrow></mrow><mn>1</mn></msub></mtd></mtr></mtable></mrow></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="151">其中, </p>
                </div>
                <div class="p1">
                    <p id="152" class="code-formula">
                        <mathml id="152"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi mathvariant="bold-italic">V</mi></mstyle><mrow><mspace width="0.25em" /><mo>∼</mo></mrow></mover><mo>=</mo><mrow><mo>[</mo><mrow><mtable><mtr><mtd><mi mathvariant="bold-italic">V</mi></mtd></mtr><mtr><mtd><msqrt><mi>γ</mi></msqrt><mi mathvariant="bold-italic">U</mi><msubsup><mrow></mrow><mi>V</mi><mrow><mo>-</mo><mn>1</mn></mrow></msubsup><mi mathvariant="bold-italic">Ρ</mi><msub><mrow></mrow><mi>Τ</mi></msub></mtd></mtr></mtable></mrow><mo>]</mo></mrow><mo>, </mo><mover><mstyle mathsize="140%" displaystyle="true"><mi mathvariant="bold-italic">Τ</mi></mstyle><mrow><mspace width="0.25em" /><mo>∼</mo></mrow></mover><mo>=</mo><mrow><mo>[</mo><mrow><mtable><mtr><mtd><mi mathvariant="bold-italic">Τ</mi></mtd></mtr><mtr><mtd><msqrt><mi>γ</mi></msqrt><mi mathvariant="bold-italic">U</mi><msubsup><mrow></mrow><mi>Τ</mi><mrow><mo>-</mo><mn>1</mn></mrow></msubsup><mi mathvariant="bold-italic">Ρ</mi><msub><mrow></mrow><mi>V</mi></msub></mtd></mtr></mtable></mrow><mo>]</mo></mrow><mo>, </mo><mover><mstyle mathsize="140%" displaystyle="true"><mi mathvariant="bold-italic">D</mi></mstyle><mrow><mspace width="0.25em" /><mo>∼</mo></mrow></mover><msub><mrow></mrow><mi>V</mi></msub><mo>=</mo><mrow><mo>[</mo><mrow><mtable><mtr><mtd><mi mathvariant="bold-italic">D</mi><msub><mrow></mrow><mi>V</mi></msub></mtd></mtr><mtr><mtd><msqrt><mi>γ</mi></msqrt><mi mathvariant="bold-italic">Ι</mi></mtd></mtr></mtable></mrow><mo>]</mo></mrow><mo>, </mo><mover><mstyle mathsize="140%" displaystyle="true"><mi mathvariant="bold-italic">D</mi></mstyle><mrow><mspace width="0.25em" /><mo>∼</mo></mrow></mover><msub><mrow></mrow><mi>Τ</mi></msub><mo>=</mo><mrow><mo>[</mo><mrow><mtable><mtr><mtd><mi mathvariant="bold-italic">D</mi><msub><mrow></mrow><mi>Τ</mi></msub></mtd></mtr><mtr><mtd><msqrt><mi>γ</mi></msqrt><mi mathvariant="bold-italic">Ι</mi></mtd></mtr></mtable></mrow><mo>]</mo></mrow></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="153">。该简化版本精确公式与标准稀疏编码一样, 选择<i>SPAMS</i><citation id="329" type="reference"><link href="296" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>求解器推导出解。</p>
                </div>
                <div class="p1">
                    <p id="154">在更新投影矩阵时, 需要将式 (10) 中与<b><i>U</i></b><sub><i>V</i></sub>和<b><i>U</i></b><sub><i>T</i></sub>相关联的项考虑到优化过程中。将字典<b><i>D</i></b>和稀疏系数<b><i>A</i></b>固定, 采用岭回归问题来更新<b><i>U</i></b>, 有:</p>
                </div>
                <div class="p1">
                    <p id="155" class="code-formula">
                        <mathml id="155"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mrow><mi>U</mi><msubsup><mrow></mrow><mi>V</mi><mrow><mo>-</mo><mn>1</mn></mrow></msubsup></mrow></munder><mi>γ</mi><mrow><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">A</mi><msub><mrow></mrow><mi>V</mi></msub><mo>-</mo><mi mathvariant="bold-italic">U</mi><msubsup><mrow></mrow><mi>V</mi><mrow><mo>-</mo><mn>1</mn></mrow></msubsup><mi mathvariant="bold-italic">Ρ</mi><msub><mrow></mrow><mi>Τ</mi></msub><mo stretchy="false">∥</mo></mrow><msubsup><mrow></mrow><mtext>F</mtext><mn>2</mn></msubsup><mo>+</mo><mi>μ</mi><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">U</mi><msubsup><mrow></mrow><mi>V</mi><mrow><mo>-</mo><mn>1</mn></mrow></msubsup><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mtext>F</mtext><mn>2</mn></msubsup></mtd></mtr><mtr><mtd><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mrow><mi>U</mi><msubsup><mrow></mrow><mi>Τ</mi><mrow><mo>-</mo><mn>1</mn></mrow></msubsup></mrow></munder><mi>γ</mi><mrow><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">U</mi><msubsup><mrow></mrow><mi>Τ</mi><mrow><mo>-</mo><mn>1</mn></mrow></msubsup><mi mathvariant="bold-italic">Ρ</mi><msub><mrow></mrow><mi>V</mi></msub><mo>-</mo><mi mathvariant="bold-italic">A</mi><msub><mrow></mrow><mi>Τ</mi></msub><mo stretchy="false">∥</mo></mrow><msubsup><mrow></mrow><mtext>F</mtext><mn>2</mn></msubsup><mo>+</mo><mi>μ</mi><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">U</mi><msubsup><mrow></mrow><mi>Τ</mi><mrow><mo>-</mo><mn>1</mn></mrow></msubsup><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mtext>F</mtext><mn>2</mn></msubsup></mtd></mtr></mtable></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>4</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="156">通过式 (6) , 可以得到<b><i>U</i></b>的解析解为:</p>
                </div>
                <div class="p1">
                    <p id="157" class="code-formula">
                        <mathml id="157"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi mathvariant="bold-italic">U</mi><msubsup><mrow></mrow><mi>V</mi><mrow><mo>-</mo><mn>1</mn></mrow></msubsup><mo>=</mo><mi mathvariant="bold-italic">A</mi><msub><mrow></mrow><mi>V</mi></msub><mi mathvariant="bold-italic">Ρ</mi><msubsup><mrow></mrow><mi>Τ</mi><mtext>Τ</mtext></msubsup><mrow><mrow><mo> (</mo><mrow><mi mathvariant="bold-italic">Ρ</mi><msub><mrow></mrow><mi>Τ</mi></msub><mi mathvariant="bold-italic">Ρ</mi><msubsup><mrow></mrow><mi>Τ</mi><mtext>Τ</mtext></msubsup><mo>+</mo><mrow><mo> (</mo><mrow><mfrac><mi>μ</mi><mi>γ</mi></mfrac></mrow><mo>) </mo></mrow><mi mathvariant="bold-italic">Ι</mi></mrow><mo>) </mo></mrow></mrow><msup><mrow></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup></mtd></mtr><mtr><mtd><mi mathvariant="bold-italic">U</mi><msubsup><mrow></mrow><mi>Τ</mi><mrow><mo>-</mo><mn>1</mn></mrow></msubsup><mo>=</mo><mi mathvariant="bold-italic">A</mi><msub><mrow></mrow><mi>Τ</mi></msub><mi mathvariant="bold-italic">Ρ</mi><msubsup><mrow></mrow><mi>V</mi><mtext>Τ</mtext></msubsup><mrow><mrow><mo> (</mo><mrow><mi mathvariant="bold-italic">Ρ</mi><msub><mrow></mrow><mi>V</mi></msub><mi mathvariant="bold-italic">Ρ</mi><msubsup><mrow></mrow><mi>V</mi><mtext>Τ</mtext></msubsup><mo>+</mo><mrow><mo> (</mo><mrow><mfrac><mi>μ</mi><mi>γ</mi></mfrac></mrow><mo>) </mo></mrow><mi mathvariant="bold-italic">Ι</mi></mrow><mo>) </mo></mrow></mrow><msup><mrow></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>5</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="158">为验证<b><i>U</i></b><mathml id="159"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>V</mi><mrow><mo>-</mo><mn>1</mn></mrow></msubsup></mrow></math></mathml>和<b><i>U</i></b><mathml id="160"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>Τ</mi><mrow><mo>-</mo><mn>1</mn></mrow></msubsup></mrow></math></mathml>可逆, 以<b><i>U</i></b><mathml id="161"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>V</mi><mrow><mo>-</mo><mn>1</mn></mrow></msubsup></mrow></math></mathml>为例, 需使式 (15) 中的<b><i>A</i></b><sub><i>V</i></sub><b><i>P</i></b><sup>T</sup><sub><i>T</i></sub>=<b><i>A</i></b><sub><i>V</i></sub><b><i>A</i></b><sup>T</sup><sub><i>T</i></sub><b><i>U</i></b><sup>T</sup><sub><i>T</i></sub>为非奇异。其中, <b><i>A</i></b><sub><i>V</i></sub>∈<image href="images/JSJC201906037_162.jpg" type="" display="inline" placement="inline"><alt></alt></image><sup><i>k</i><sub>1</sub>×<i>n</i></sup>, <b><i>A</i></b><sub><i>T</i></sub>∈<image href="images/JSJC201906037_163.jpg" type="" display="inline" placement="inline"><alt></alt></image><sup><i>k</i><sub>2</sub>×<i>n</i></sup>, 且<i>k</i><sub>1</sub>=<i>k</i><sub>2</sub>。对于图像数据有实例的数量<i>n</i>≥<i>k</i><sub>1</sub>, 其不太可能有奇异<b><i>A</i></b><sub><i>V</i></sub><b><i>A</i></b><sup>T</sup><sub><i>T</i></sub>∈<image href="images/JSJC201906037_164.jpg" type="" display="inline" placement="inline"><alt></alt></image><sup><i>k</i><sub>1</sub>×<i>k</i><sub>1</sub></sup>。</p>
                </div>
                <h4 class="anchor-tag" id="165" name="165">2.2 联合图像正则化</h4>
                <div class="p1">
                    <p id="166">本文<b><i>Y</i></b>=[<i>y</i><sub>1</sub>, <i>y</i><sub>2</sub>, …, <i>y</i><sub><i>n</i></sub>]∈<image href="images/JSJC201906037_167.jpg" type="" display="inline" placement="inline"><alt></alt></image><sup><i>c</i>×<i>n</i></sup>是语义矩阵, 在耦合特征映射过程中, 主要将2种模态的稀疏表示<b><i>A</i></b><sub><i>V</i></sub>和<b><i>A</i></b><sub><i>T</i></sub>映射到通过语义矩阵定义的共同空间中, 在映射过程中可以得到2个映射矩阵<b><i>W</i></b><sub><i>V</i></sub>∈<image href="images/JSJC201906037_168.jpg" type="" display="inline" placement="inline"><alt></alt></image><sup><i>k</i><sub>1</sub>×<i>c</i></sup>和<b><i>W</i></b><sub><i>T</i></sub>∈<image href="images/JSJC201906037_169.jpg" type="" display="inline" placement="inline"><alt></alt></image><sup><i>k</i><sub>2</sub>×<i>c</i></sup>, <b><i>W</i></b><sub><i>V</i></sub>和<b><i>W</i></b><sub><i>T</i></sub>与耦合字典过程中的<b><i>U</i></b><sub><i>T</i></sub>和<b><i>U</i></b><sub><i>v</i></sub>不同, 因为前者<b><i>W</i></b><sub><i>V</i></sub>和<b><i>W</i></b><sub><i>T</i></sub>将稀疏表示映射到语义空间, 而后者<b><i>U</i></b><sub><i>T</i></sub>和<b><i>U</i></b><sub><i>v</i></sub>在中间空间<i>P</i>上执行。</p>
                </div>
                <div class="p1">
                    <p id="170">本文将21范式与多模态图像正则化应用到耦合线性回归中, 通过21范式进行特征选择, 从而可以选择来自稀疏表示相关的和具有辨别性的特征来增强不同模态之间的相关性, 通过使用多模态图像正则化, 在映射过程中可以保持模态内部与模态之间的相关性, 有:</p>
                </div>
                <div class="p1">
                    <p id="171" class="code-formula">
                        <mathml id="171"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mrow><mi>W</mi><msub><mrow></mrow><mi>V</mi></msub><mo>, </mo><mi>W</mi><msub><mrow></mrow><mi>Τ</mi></msub></mrow></munder><mfrac><mn>1</mn><mn>2</mn></mfrac><mo stretchy="false"> (</mo><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">W</mi><msubsup><mrow></mrow><mi>V</mi><mtext>Τ</mtext></msubsup><mi mathvariant="bold-italic">A</mi><msub><mrow></mrow><mi>V</mi></msub><mo>-</mo><mi mathvariant="bold-italic">Y</mi><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mtext>F</mtext><mn>2</mn></msubsup><mo>+</mo><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">W</mi><msubsup><mrow></mrow><mi>Τ</mi><mtext>Τ</mtext></msubsup><mi mathvariant="bold-italic">A</mi><msub><mrow></mrow><mi>Τ</mi></msub><mo>-</mo><mi mathvariant="bold-italic">Y</mi><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mtext>F</mtext><mn>2</mn></msubsup><mo stretchy="false">) </mo><mo>+</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mi>λ</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false"> (</mo><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mi>V</mi></msub><mo stretchy="false">∥</mo><msub><mrow></mrow><mrow><mn>2</mn><mn>1</mn></mrow></msub><mo>+</mo><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mi>Τ</mi></msub><mo stretchy="false">∥</mo><msub><mrow></mrow><mrow><mn>2</mn><mn>1</mn></mrow></msub><mo stretchy="false">) </mo><mo>+</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mi>λ</mi><msub><mrow></mrow><mn>2</mn></msub><mrow><mo>[</mo><mrow><mtext>t</mtext><mtext>r</mtext><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">W</mi><msubsup><mrow></mrow><mi>Τ</mi><mtext>Τ</mtext></msubsup><mi mathvariant="bold-italic">A</mi><msub><mrow></mrow><mi>Τ</mi></msub><mi mathvariant="bold-italic">L</mi><msub><mrow></mrow><mi>Τ</mi></msub><mi mathvariant="bold-italic">A</mi><msubsup><mrow></mrow><mi>Τ</mi><mtext>Τ</mtext></msubsup><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mi>Τ</mi></msub><mo stretchy="false">) </mo><mo>+</mo><mtext>t</mtext><mtext>r</mtext><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">W</mi><msubsup><mrow></mrow><mi>V</mi><mtext>Τ</mtext></msubsup><mi mathvariant="bold-italic">A</mi><msub><mrow></mrow><mi>V</mi></msub><mi mathvariant="bold-italic">L</mi><msub><mrow></mrow><mi>V</mi></msub><mi mathvariant="bold-italic">A</mi><msubsup><mrow></mrow><mi>V</mi><mtext>Τ</mtext></msubsup><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mi>V</mi></msub><mo stretchy="false">) </mo><mo>+</mo></mrow></mrow></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mrow><mrow><mtext>t</mtext><mtext>r</mtext><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">W</mi><msubsup><mrow></mrow><mi>Τ</mi><mtext>Τ</mtext></msubsup><mi mathvariant="bold-italic">A</mi><msub><mrow></mrow><mi>Τ</mi></msub><mi mathvariant="bold-italic">L</mi><msub><mrow></mrow><mrow><mi>Τ</mi><mi>V</mi></mrow></msub><mi mathvariant="bold-italic">A</mi><msubsup><mrow></mrow><mi>V</mi><mtext>Τ</mtext></msubsup><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mi>V</mi></msub><mo stretchy="false">) </mo><mo>+</mo><mtext>t</mtext><mtext>r</mtext><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">W</mi><msubsup><mrow></mrow><mi>V</mi><mtext>Τ</mtext></msubsup><mi mathvariant="bold-italic">A</mi><msub><mrow></mrow><mi>V</mi></msub><mi mathvariant="bold-italic">L</mi><msub><mrow></mrow><mrow><mi>V</mi><mi>Τ</mi></mrow></msub><mi mathvariant="bold-italic">A</mi><msubsup><mrow></mrow><mi>Τ</mi><mtext>Τ</mtext></msubsup><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mi>Τ</mi></msub><mo stretchy="false">) </mo></mrow><mo>]</mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>6</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="172">根据式 (8) , 可以得到:</p>
                </div>
                <div class="p1">
                    <p id="173" class="code-formula">
                        <mathml id="173"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mrow><mi>W</mi><msub><mrow></mrow><mi>V</mi></msub><mo>, </mo><mi>W</mi><msub><mrow></mrow><mi>Τ</mi></msub></mrow></munder><mspace width="0.25em" /><mfrac><mn>1</mn><mn>2</mn></mfrac><mo stretchy="false"> (</mo><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">W</mi><msubsup><mrow></mrow><mi>V</mi><mtext>Τ</mtext></msubsup><mi mathvariant="bold-italic">A</mi><msub><mrow></mrow><mi>V</mi></msub><mo>-</mo><mi mathvariant="bold-italic">Y</mi><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mtext>F</mtext><mn>2</mn></msubsup><mo>+</mo><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">W</mi><msubsup><mrow></mrow><mi>Τ</mi><mtext>Τ</mtext></msubsup><mi mathvariant="bold-italic">A</mi><msub><mrow></mrow><mi>Τ</mi></msub><mo>-</mo><mi mathvariant="bold-italic">Y</mi><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mtext>F</mtext><mn>2</mn></msubsup><mo stretchy="false">) </mo><mo>+</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mi>λ</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false"> (</mo><mtext>t</mtext><mtext>r</mtext><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">W</mi><msubsup><mrow></mrow><mi>V</mi><mtext>Τ</mtext></msubsup><mi mathvariant="bold-italic">R</mi><msub><mrow></mrow><mi>V</mi></msub><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mi>V</mi></msub><mo stretchy="false">) </mo><mo>+</mo><mtext>t</mtext><mtext>r</mtext><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">W</mi><msubsup><mrow></mrow><mi>Τ</mi><mtext>Τ</mtext></msubsup><mi mathvariant="bold-italic">R</mi><msub><mrow></mrow><mi>Τ</mi></msub><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mi>Τ</mi></msub><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo>+</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="174" class="code-formula">
                        <mathml id="174"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mrow><mtext> </mtext><mtext> </mtext></mrow><mi>λ</mi><msub><mrow></mrow><mn>2</mn></msub><mrow><mo>[</mo><mrow><mtext>t</mtext><mtext>r</mtext><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">W</mi><msubsup><mrow></mrow><mi>Τ</mi><mtext>Τ</mtext></msubsup><mi mathvariant="bold-italic">A</mi><msub><mrow></mrow><mi>Τ</mi></msub><mi mathvariant="bold-italic">L</mi><msub><mrow></mrow><mi>Τ</mi></msub><mi mathvariant="bold-italic">A</mi><msubsup><mrow></mrow><mi>Τ</mi><mtext>Τ</mtext></msubsup><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mi>Τ</mi></msub><mo stretchy="false">) </mo><mo>+</mo><mtext>t</mtext><mtext>r</mtext><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">W</mi><msubsup><mrow></mrow><mi>V</mi><mtext>Τ</mtext></msubsup><mi mathvariant="bold-italic">A</mi><msub><mrow></mrow><mi>V</mi></msub><mi mathvariant="bold-italic">L</mi><msub><mrow></mrow><mi>V</mi></msub><mi mathvariant="bold-italic">A</mi><msubsup><mrow></mrow><mi>V</mi><mtext>Τ</mtext></msubsup><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mi>V</mi></msub><mo stretchy="false">) </mo><mo>+</mo></mrow></mrow></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mrow><mrow><mtext>t</mtext><mtext>r</mtext><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">W</mi><msubsup><mrow></mrow><mi>Τ</mi><mtext>Τ</mtext></msubsup><mi mathvariant="bold-italic">A</mi><msub><mrow></mrow><mi>Τ</mi></msub><mi mathvariant="bold-italic">L</mi><msub><mrow></mrow><mrow><mi>Τ</mi><mi>V</mi></mrow></msub><mi mathvariant="bold-italic">A</mi><msubsup><mrow></mrow><mi>V</mi><mtext>Τ</mtext></msubsup><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mi>V</mi></msub><mo stretchy="false">) </mo><mo>+</mo><mtext>t</mtext><mtext>r</mtext><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">W</mi><msubsup><mrow></mrow><mi>V</mi><mtext>Τ</mtext></msubsup><mi mathvariant="bold-italic">A</mi><msub><mrow></mrow><mi>V</mi></msub><mi mathvariant="bold-italic">L</mi><msub><mrow></mrow><mrow><mi>V</mi><mi>Τ</mi></mrow></msub><mi mathvariant="bold-italic">A</mi><msubsup><mrow></mrow><mi>Τ</mi><mtext>Τ</mtext></msubsup><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mi>Τ</mi></msub><mo stretchy="false">) </mo></mrow><mo>]</mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>7</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="175">对式 (17) 中的<b><i>W</i></b><sub><i>V</i></sub>和<b><i>W</i></b><sub><i>T</i></sub>求导, 可以得到:</p>
                </div>
                <div class="p1">
                    <p id="176" class="code-formula">
                        <mathml id="176"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mfrac><mrow><mo>∂</mo><mi>Ο</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mi>V</mi></msub><mo>, </mo><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mi>Τ</mi></msub><mo stretchy="false">) </mo></mrow><mrow><mo>∂</mo><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mi>Τ</mi></msub></mrow></mfrac><mo>=</mo><mi>λ</mi><msub><mrow></mrow><mn>2</mn></msub><mi mathvariant="bold-italic">A</mi><msub><mrow></mrow><mi>Τ</mi></msub><mi mathvariant="bold-italic">L</mi><msub><mrow></mrow><mi>Τ</mi></msub><mi mathvariant="bold-italic">A</mi><msubsup><mrow></mrow><mi>Τ</mi><mtext>Τ</mtext></msubsup><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mi>Τ</mi></msub><mo>+</mo><mi>λ</mi><msub><mrow></mrow><mn>2</mn></msub><mi mathvariant="bold-italic">A</mi><msub><mrow></mrow><mi>Τ</mi></msub><mi mathvariant="bold-italic">L</mi><msub><mrow></mrow><mrow><mi>Τ</mi><mi>V</mi></mrow></msub><mi mathvariant="bold-italic">A</mi><msubsup><mrow></mrow><mi>V</mi><mtext>Τ</mtext></msubsup><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mi>V</mi></msub><mo>+</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mi mathvariant="bold-italic">A</mi><msub><mrow></mrow><mi>Τ</mi></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">A</mi><msubsup><mrow></mrow><mi>Τ</mi><mtext>Τ</mtext></msubsup><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mi>Τ</mi></msub><mo>-</mo><mi mathvariant="bold-italic">Y</mi><mo stretchy="false">) </mo><mo>+</mo><mi>λ</mi><msub><mrow></mrow><mn>1</mn></msub><mi>R</mi><msub><mrow></mrow><mi>Τ</mi></msub><mi>W</mi><msub><mrow></mrow><mi>Τ</mi></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>8</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mfrac><mrow><mo>∂</mo><mi>Ο</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mi>V</mi></msub><mo>, </mo><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mi>Τ</mi></msub><mo stretchy="false">) </mo></mrow><mrow><mo>∂</mo><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mi>V</mi></msub></mrow></mfrac><mo>=</mo><mi>λ</mi><msub><mrow></mrow><mn>2</mn></msub><mi mathvariant="bold-italic">A</mi><msub><mrow></mrow><mi>V</mi></msub><mi mathvariant="bold-italic">L</mi><msub><mrow></mrow><mi>V</mi></msub><mi mathvariant="bold-italic">A</mi><msubsup><mrow></mrow><mi>V</mi><mtext>Τ</mtext></msubsup><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mi>V</mi></msub><mo>+</mo><mi>λ</mi><msub><mrow></mrow><mn>2</mn></msub><mi mathvariant="bold-italic">A</mi><msub><mrow></mrow><mi>V</mi></msub><mi mathvariant="bold-italic">L</mi><msub><mrow></mrow><mrow><mi>Τ</mi><mi>V</mi></mrow></msub><mi mathvariant="bold-italic">A</mi><msubsup><mrow></mrow><mi>Τ</mi><mtext>Τ</mtext></msubsup><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mi>Τ</mi></msub><mo>+</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mi mathvariant="bold-italic">A</mi><msub><mrow></mrow><mi>Τ</mi></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">A</mi><msubsup><mrow></mrow><mi>V</mi><mtext>Τ</mtext></msubsup><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mi>V</mi></msub><mo>-</mo><mi mathvariant="bold-italic">Y</mi><mo stretchy="false">) </mo><mo>+</mo><mi>λ</mi><msub><mrow></mrow><mn>1</mn></msub><mi mathvariant="bold-italic">R</mi><msub><mrow></mrow><mi>V</mi></msub><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mi>V</mi></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>9</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="177">将偏导数设为0时, 由式 (18) 和式 (19) 可得到闭合形式的解。</p>
                </div>
                <div class="p1">
                    <p id="178"><b><i>W</i></b><sub><i>T</i></sub>= (<i>λ</i><sub>2</sub><b><i>A</i></b><sub><i>T</i></sub><b><i>L</i></b><sub><i>T</i></sub><b><i>A</i></b><sup>T</sup><sub><i>T</i></sub>+<b><i>A</i></b><sub><i>T</i></sub><b><i>A</i></b><sup>T</sup><sub><i>T</i></sub>+<i>λ</i><sub>1</sub><b><i>R</i></b><sub><i>T</i></sub>) <sup>-1</sup> (<b><i>A</i></b><sub><i>T</i></sub><b><i>Y</i></b>-<i>λ</i><sub>2</sub><b><i>A</i></b><sub><i>T</i></sub><b><i>L</i></b><sub><i>TV</i></sub><b><i>A</i></b><sup>T</sup><sub><i>V</i></sub><b><i>W</i></b><sub><i>V</i></sub>)      (20) </p>
                </div>
                <div class="p1">
                    <p id="179"><b><i>W</i></b><sub><i>V</i></sub>= (<i>λ</i><sub>2</sub><b><i>A</i></b><sub><i>V</i></sub><b><i>L</i></b><sub><i>V</i></sub><b><i>A</i></b><sup>T</sup><sub><i>V</i></sub>+<b><i>A</i></b><sup>T</sup><sub><i>V</i></sub>+<i>λ</i><sub>1</sub><b><i>R</i></b><sub><i>V</i></sub>) <sup>-1</sup> (<b><i>A</i></b><sub><i>V</i></sub><b><i>Y</i></b>-<i>λ</i><sub>2</sub><b><i>A</i></b><sub><i>V</i></sub><b><i>L</i></b><sub><i>TV</i></sub><b><i>A</i></b><sup>T</sup><sub><i>T</i></sub><b><i>W</i></b><sub><i>T</i></sub>)      (21) </p>
                </div>
                <h4 class="anchor-tag" id="180" name="180">2.3 跨媒体检索的测试阶段</h4>
                <div class="p1">
                    <p id="181">在训练阶段完成后, 可获得2个模态的字典{<b><i>D</i></b><sub><i>V</i></sub>, <b><i>D</i></b><sub><i>T</i></sub>}、稀疏表示{<b><i>A</i></b><sub><i>V</i></sub>, <b><i>A</i></b><sub><i>T</i></sub>}、投影矩阵{<b><i>W</i></b><sub><i>V</i></sub>, <b><i>W</i></b><sub><i>T</i></sub>}。在测试阶段, 给定一个测试<mathml id="182"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi mathvariant="bold-italic">Ι</mi></mstyle><mrow><mspace width="0.25em" /><mo>∼</mo></mrow></mover><mo>=</mo><mrow><mo> (</mo><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi mathvariant="bold-italic">v</mi></mstyle><mrow><mspace width="0.25em" /><mo>∼</mo></mrow></mover><mo>, </mo><mover><mstyle mathsize="140%" displaystyle="true"><mi mathvariant="bold-italic">t</mi></mstyle><mrow><mspace width="0.25em" /><mo>∼</mo></mrow></mover></mrow><mo>) </mo></mrow></mrow></math></mathml>, 基于学习到的字典{<b><i>D</i></b><sub><i>V</i></sub>, <b><i>D</i></b><sub><i>T</i></sub>}生成稀疏表示{<b><i>A</i></b><sub><i>V</i></sub>, <b><i>A</i></b><sub><i>T</i></sub>}, 然后通过投影矩阵{<b><i>W</i></b><sub><i>V</i></sub>, <b><i>W</i></b><sub><i>T</i></sub>}将{<b><i>A</i></b><sub><i>V</i></sub>, <b><i>A</i></b><sub><i>T</i></sub>}投影到语义空间中。在跨模态检索任务中, 一般使用一种模态的数据作为查询去检索另一种模态的数据。</p>
                </div>
                <h3 id="183" name="183" class="anchor-tag">3 实验结果与分析</h3>
                <h4 class="anchor-tag" id="184" name="184">3.1 实验设置</h4>
                <div class="p1">
                    <p id="185">本文在<i>Wiki</i>图像文本<citation id="330" type="reference"><link href="298" rel="bibliography" /><sup>[<a class="sup">25</a>]</sup></citation>和<i>Pascal VOC</i><citation id="331" type="reference"><link href="300" rel="bibliography" /><sup>[<a class="sup">26</a>]</sup></citation>这2个数据集进行评估, 主要考虑图像查询文本数据库、文本查询图像数据库 2种跨模态检索任务, 设置对比方法有<i>PLS</i><citation id="332" type="reference"><link href="252" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>、<i>BLM</i><citation id="333" type="reference"><link href="256" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>、<i>CCA</i><citation id="334" type="reference"><link href="252" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>、<i>CDFE</i><citation id="335" type="reference"><link href="262" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>、<i>CCA</i>-3<i>V</i><citation id="336" type="reference"><link href="302" rel="bibliography" /><sup>[<a class="sup">27</a>]</sup></citation>、<i>GMLDA</i><citation id="337" type="reference"><link href="264" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>、<i>GMMFA</i><citation id="338" type="reference"><link href="264" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>、耦合特征空间学习 (<i>Learning Coupled Feature Spaces</i>, <i>LCFS</i>) <citation id="339" type="reference"><link href="304" rel="bibliography" /><sup>[<a class="sup">28</a>]</sup></citation>, 具体描述如下:</p>
                </div>
                <div class="p1">
                    <p id="186">1) <i>PLS</i>、<i>BLM</i>、<i>CCA</i>是3种经典方法, 使用成对信息来学习多模态数据中的潜在子空间。在公共子空间中, 可以测量不同数据模式之间的相似性。</p>
                </div>
                <div class="p1">
                    <p id="187">2) <i>CDFE</i>方法指学习一个共同的特征子空间, 其中散布矩阵内部和散布矩阵之间的差异被最大化。</p>
                </div>
                <div class="p1">
                    <p id="188">3) <i>CCA</i>-3<i>V</i>方法是三视图典型相关性分析方法。</p>
                </div>
                <div class="p1">
                    <p id="189">4) <i>GMLDA</i>方法通过找到一组投影矩阵, 使得来自同一类的样本彼此接近而来自不同类别的样本分开。</p>
                </div>
                <div class="p1">
                    <p id="190">5) <i>GMMFA</i>方法是<i>CCA</i>方法的监督扩展, 同时考虑<i>CCA</i>方法约束和语义约束。</p>
                </div>
                <div class="p1">
                    <p id="191">6) <i>LCFS</i>方法将耦合线性回归、21范数和迹范数整合到一个通用最小化公式, 且子空间学习和耦合特征选择可以同时执行。</p>
                </div>
                <div class="p1">
                    <p id="192">平均精度均值 (<i>Mean Average Precision</i>, <i>MAP</i>) <citation id="340" type="reference"><link href="302" rel="bibliography" /><sup>[<a class="sup">27</a>]</sup></citation>是跨模态检索的经典性能评估标准。具体来说, 给定一组查询, 每个查询的平均精度 (<i>Average Precision</i>, <i>AP</i>) 被定义为:</p>
                </div>
                <div class="p1">
                    <p id="193" class="code-formula">
                        <mathml id="193"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>A</mi><mi>Ρ</mi><mo>=</mo><mfrac><mn>1</mn><mi>Τ</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>r</mi><mo>=</mo><mn>1</mn></mrow><mi>R</mi></munderover><mi>Ρ</mi></mstyle><mrow><mo> (</mo><mi>r</mi><mo>) </mo></mrow><mi>δ</mi><mo stretchy="false"> (</mo><mi>r</mi><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="194">其中, T是在检索集合中相关文档的数量, P (r) 表示前r个检索文档的精度, 如果第r个检索到的文件相关 (相关代表属于查询的类) , 则δ (r) =1;否则为0。然后对查询集中所有查询的AP值进行平均来计算<i>MAP</i>。 <i>MAP</i>值越大, 跨模态检索的表现越好。除<i>MAP</i>之外, 本文还使用精度召回曲线来评估不同方法的有效性。</p>
                </div>
                <h4 class="anchor-tag" id="195" name="195">3.2 <i>Wiki</i>数据集结果分析</h4>
                <div class="p1">
                    <p id="196"><i>Wiki</i>数据集<citation id="341" type="reference"><link href="298" rel="bibliography" /><sup>[<a class="sup">25</a>]</sup></citation>包含来自10个专业类的2 866个图像文本对, 将2 173个图像文本对用于训练, 693个图像文本对用于测试。对于文本, 本文采用潜在的<i>Dirichlet</i>分配线性判别分析 (<i>Linear Discriminant Analysis</i>, <i>LDA</i>) 来提取10个维度表示, 128维描述子直方图<citation id="342" type="reference"><link href="306" rel="bibliography" /><sup>[<a class="sup">29</a>]</sup></citation>用于表示图像。</p>
                </div>
                <div class="p1">
                    <p id="197">表1显示本文方法和其他方法的平均精度均值。图1、图2显示了不同方法跨模态检索的精度对比结果。本文方法对图像查询的<i>MAP</i>为0.282 1, 对文本查询的<i>MAP</i>为0.223 0, 表现优于其他对比算法。由于加入了语义信息, 本文方法、<i>CDFE</i>方法、<i>GMMFA</i>方法、<i>GMLDA</i>方法、<i>CCA</i>-3<i>V</i>方法优于<i>PLS</i>方法、<i>BLM</i>方法、<i>CCA</i>方法。</p>
                </div>
                <div class="area_img" id="199">
                    <p class="img_tit"><b>表1 Wiki数据集的跨媒体检索精度比较1</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="199" border="1"><tr><td>方法</td><td>图像查询</td><td>文本查询</td><td>平均值</td></tr><tr><td>PLS方法</td><td>0.240 2</td><td>0.163 3</td><td>0.203 2</td></tr><tr><td><br />BLM方法</td><td>0.256 2</td><td>0.202 3</td><td>0.229 3</td></tr><tr><td><br />CCA方法</td><td>0.254 9</td><td>0.184 6</td><td>0.219 8</td></tr><tr><td><br />CDFE方法</td><td>0.265 5</td><td>0.205 9</td><td>0.235 7</td></tr><tr><td><br />CCA-3V方法</td><td>0.275 2</td><td>0.224 2</td><td>0.249 7</td></tr><tr><td><br />GMLDA方法</td><td>0.275 1</td><td>0.209 8</td><td>0.242 5</td></tr><tr><td><br />GMMFA方法</td><td>0.275 0</td><td>0.213 9</td><td>0.244 5</td></tr><tr><td><br />LCFS方法</td><td>0.279 8</td><td>0.214 1</td><td>0.247 0</td></tr><tr><td><br />本文方法</td><td>0.282 1</td><td>0.223 0</td><td>0.252 6</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="200">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201906037_200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 不同方法在Wiki数据集上图像检索文本精度对比" src="Detail/GetImg?filename=images/JSJC201906037_200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图1 不同方法在Wiki数据集上图像检索文本精度对比</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201906037_200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="201">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201906037_201.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 不同方法在Wiki数据集上文本检索图像精度对比" src="Detail/GetImg?filename=images/JSJC201906037_201.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图2 不同方法在Wiki数据集上文本检索图像精度对比</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201906037_201.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="202" name="202">3.3 Pascal VOC数据集结果分析</h4>
                <div class="p1">
                    <p id="203">Pascal VOC数据集<citation id="343" type="reference"><link href="300" rel="bibliography" /><sup>[<a class="sup">26</a>]</sup></citation>由来自20个不同类别图像标签对组成, 其中, 训练数据有5 011对, 测试数据有4 952对。在实验中选择仅对应一个对象的图像, 因此训练集合为2 808对, 测试集合为2 841对, 使用512维GIST特征来表示图像, 399维度词频特征来表示文本。</p>
                </div>
                <div class="p1">
                    <p id="204">表2显示本文方法和其他方法的平均精度均值。图3、图4显示不同方法跨模态检索的精度对比结果。本文方法对图像查询的MAP为0.384 1, 对文本查询的MAP为0.306 2, 优于其他方法。</p>
                </div>
                <div class="area_img" id="206">
                    <p class="img_tit"><b>表2 Pascal VOC数据集的跨媒体检索精度比较</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="206" border="1"><tr><td>方法</td><td>图像查询</td><td>文本查询</td><td>平均值</td></tr><tr><td>PLS方法</td><td>0.275 7</td><td>0.199 7</td><td>0.237 7</td></tr><tr><td><br />BLM方法</td><td>0.266 7</td><td>0.240 8</td><td>0.253 8</td></tr><tr><td><br />CCA方法</td><td>0.265 5</td><td>0.221 5</td><td>0.243 5</td></tr><tr><td><br />CDFE方法</td><td>0.292 8</td><td>0.221 1</td><td>0.256 9</td></tr><tr><td><br />CCA-3V方法</td><td>0.314 6</td><td>0.256 2</td><td>0.285 4</td></tr><tr><td><br />GMLDA方法</td><td>0.309 4</td><td>0.244 8</td><td>0.277 1</td></tr><tr><td><br />GMMFA方法</td><td>0.309 0</td><td>0.230 8</td><td>0.269 9</td></tr><tr><td><br />LCFS方法</td><td>0.343 8</td><td>0.267 4</td><td>0.305 6</td></tr><tr><td><br />本文方法</td><td>0.384 1</td><td>0.306 2</td><td>0.345 2</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="207">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201906037_207.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 不同方法在Pascal VOC数据集上图像检索文本精度对比" src="Detail/GetImg?filename=images/JSJC201906037_207.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图3 不同方法在Pascal VOC数据集上图像检索文本精度对比</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201906037_207.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="208">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201906037_208.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 不同方法在Pascal VOC数据集上文本检索图像精度对比" src="Detail/GetImg?filename=images/JSJC201906037_208.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图4 不同方法在Pascal VOC数据集上文本检索图像精度对比</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201906037_208.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="209" name="209">3.4 不同特征类型表现</h4>
                <div class="p1">
                    <p id="210">本文使用Wiki数据集中图像和文本的不同类型特征来测试跨模态检索性能。 除Wiki数据集本身提供的特征外, 对于图像, 通过Caffe提取4 096维图像的CNN特征;对于文本, 通过LDA提取100维的文本特征。表3显示了Wiki数据集上具有不同类型特征的GMMFA方法、GMLDA方法、SM方法和SCM方法的平均精度均值对比。结果表明, 本文方法优于其他方法。</p>
                </div>
                <div class="area_img" id="211">
                    <p class="img_tit"><b>表3 Wiki数据集的跨媒体检索精度比较2</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="211" border="1"><tr><td>方法</td><td>图像查询</td><td>文本查询</td><td>平均值</td></tr><tr><td>GMMFA方法</td><td>0.371 0</td><td>0.322 0</td><td>0.346 0</td></tr><tr><td><br />GMLDA方法</td><td>0.372 0</td><td>0.322 0</td><td>0.347 0</td></tr><tr><td><br />SCM方法</td><td>0.351 0</td><td>0.324 0</td><td>0.337 0</td></tr><tr><td><br />SM方法</td><td>0.403 0</td><td>0.357 0</td><td>0.380 0</td></tr><tr><td><br />本文方法</td><td>0.422 3</td><td>0.372 1</td><td>0.397 2</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h3 id="212" name="212" class="anchor-tag">4 结束语</h3>
                <div class="p1">
                    <p id="213">本文提出一种联合耦合字典学习和图像正则化的跨媒体检索方法。利用耦合字典学习得到不同模态的稀疏表示, 学习不同的投影矩阵从而将不同的模态稀疏表示映射到语义空间, 并且在投影过程中选择不同模态的相关和判别特征, 使用图形正则化表征模态之间与模态内部的关系。在Wikipedia数据集和Pascal VOC数据集上的实验结果表明, 本文方法提高了多模态之间的检索效率。下一步将结合多视图找到最优表示, 从多视图中学习共同的特征空间。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="250">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A comprehensive survey on cross-modal retrieval">

                                <b>[1]</b> WANG Kaiye, YIN Qiyue, WANG Wei, et al.A comprehensive survey on cross-modal retrieval [EB/OL].[2018-08-02].https://arxiv.org/pdf/1607.06215.pdf.
                            </a>
                        </p>
                        <p id="252">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJBK&amp;filename=SJBK15090500012263&amp;v=MTUwMDZud1plWnVIeWptVUxuSUlWNFNhUkU9TmlmSlpiSzlIdGpNcW85RlpPb05Ebm82b0JNVDZUNFBRSC9pclJkR2VycVFUTQ==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> HARDOON D R, SZEDMAK S, SHAWE-TAYLOR J.Canonical correlation analysis:an overview with application to learning methods[J].Neural Computation, 2004, 16 (12) :2639-2664.
                            </a>
                        </p>
                        <p id="254">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Overview and recent advances in partial least squares">

                                <b>[3]</b> ROSIPAL R, KRÄMER N.Overview and recent advances in partial least squares [C]//Proceedings of International Conference on Subspace, Latent Structure and Feature Selection.Berlin, Germany:Springer, 2005:34-51.
                            </a>
                        </p>
                        <p id="256">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJBK&amp;filename=SJBK15090500011833&amp;v=MzExMDFCTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVUxuSUlWNFNhUkU9TmlmSlpiSzlIdGpNcW85RlpPb09CSDg2bw==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b> TENENBAUM J B, FREEMAN W T.Separating style and content with bilinear model[J].Neural Computation, 2000, 12 (6) :1247-1283.
                            </a>
                        </p>
                        <p id="258">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Maximum covariance unfolding:manifold learning for bimodal data">

                                <b>[5]</b> MAHADEVAN V, PEREIRA J C, VASCONCELOSN, et al.Maximum covariance unfolding:manifold learning for bimodal data[EB/OL].[2018-08-02].http://www.svcl.ucsd.edu/publications/conference/2011/nips2011/mcu.pdf.
                            </a>
                        </p>
                        <p id="260">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Parallel field alignment for cross media retrieval">

                                <b>[6]</b> MAO Xiangbo, LIN Binbin, CAI Deng, et al.Parallel field alignment for cross media retrieval[C]//Proceedings of ACM International Conference on Multimedia.New York, USA:ACM Press, 2013:897-906.
                            </a>
                        </p>
                        <p id="262">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Inter-modality face recognition">

                                <b>[7]</b> LIN Dahua, TANG Xiaoou.Inter-modality face recogni-tion[C]//Proceedings of the 9th European conference on Computer Vision.Berlin, Germany:Springer, 2006:13-26.
                            </a>
                        </p>
                        <p id="264">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Generalized Multiview analysis:A discriminative latent space">

                                <b>[8]</b> SHARMA A.Generalized multiview analysis:a discriminative latent space[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Computer Society, 2012:2160-2167.
                            </a>
                        </p>
                        <p id="266">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=SDZK201703002&amp;v=MjQzOTMzenFxQnRHRnJDVVJMT2VaZVJvRnlubFZiekxOaW5SWmJHNEg5Yk1ySTlGWm9RS0RIODR2UjRUNmo1NE8=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b> 陈祥, 于治楼.基于不同模态语义匹配的跨媒体检索[J].山东师范大学学报 (自然科学版) , 2017, 32 (3) :9-15.
                            </a>
                        </p>
                        <p id="268">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning Cross-media Joint Representation with Sparse and Semi-supervised Regularization">

                                <b>[10]</b> ZHAI Xiaohua, PENG Yuxin, XIAO Jianguo.Learning cross-media joint representation with sparse and semisupervised regularization[J].IEEE Transactions on Circuits and Systems for Video Technology, 2014, 24 (6) :965-978.
                            </a>
                        </p>
                        <p id="270">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Coupled dictionary learning and feature mapping for cross-modal retrieval">

                                <b>[11]</b> XU Xing, SHIMADA A, TANIGUCHI R, et al.Coupled dictionary learning and feature mapping for cross-modal retrieval[C]//Proceedings of International Conference on Multimedia and Expo.Washington D.C., USA:IEEE Press, 2015:1-6.
                            </a>
                        </p>
                        <p id="272">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Supervised Coupled Dictionary Learning with Group Structures for Multi-modal Retrieval">

                                <b>[12]</b> ZHUANG Yueting, WANG Yanfei, WU Fei, et al.Supervised coupled dictionary learning with group structures for multi-modal retrieval [C]//Proceedings of the 27th AAAI Conference on Artificial Intelligence.[S.l.]:AAAI Press, 2013:1070-1076.
                            </a>
                        </p>
                        <p id="274">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Heterogeneous metric learning with joint graph regularization for cross-media retrieval">

                                <b>[13]</b> ZHAI Xiaohua, PENG Yuxin, XIAO Jianguo.Heterogeneous metric learning with joint graph regularization for cross-media retrieval [C]//Proceedings of the 27th AAAI Conference on Artificial Intelligence.[S.l.]:AAAI Press, 2013:1198-1204.
                            </a>
                        </p>
                        <p id="276">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multimodal deep learning">

                                <b>[14]</b> NGIAM J, KHOSLA A, KIM M, et al.Multimodal deep learning[EB/OL].[2018-08-02].https://www.mendeley.com/catalogue/multimodal-deep-learning/.
                            </a>
                        </p>
                        <p id="278">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multimodal learning with deep Boltzmann machines">

                                <b>[15]</b> SRIVASTAVA N, SALAKHUTDINOV R.Multimodal learning with deep boltzmann machines [J].Journal of Machine Learning Research.2014, 15:2949-2980.
                            </a>
                        </p>
                        <p id="280">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep canonical cor-relation analysis">

                                <b>[16]</b> ANDREW G, ARORA R, BILMES J, et al.Deep canonical correlation analysis[C]//Proceedings of the 30th International Conference on International Conference on Machine Learning.[S.l.]:JMLR.org, 2013:1247-1255.
                            </a>
                        </p>
                        <p id="282">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Spectral regression for efficient regularized subspacelearning">

                                <b>[17]</b> CAI Deng, HE Xiaofei, HAN Jiawei.Spectral regression for efficient regularized subspace learning[C]//Proceedings of the 11th International Conference on Computer Vision.Washington D.C., USA:IEEE Press, 2007:1-8.
                            </a>
                        </p>
                        <p id="284">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Latent semantic sparse hashing for cross-modal similarity search">

                                <b>[18]</b> ZHOU Jile, DING Guiguang, GUO Yuchen.Latent semantic sparse hashing for cross-modal similarity search[C]//Proceedings of the 37th International ACM SIGIR Conference on Research and Development in Information Retrieval.New York, USA:ACM Press, 2014:415-424.
                            </a>
                        </p>
                        <p id="286">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Discriminative coupled dictionary hashing for fast cross-media retrieval">

                                <b>[19]</b> YU Zhou, WU Fei, YANG Yi, et al.Discriminative coupled dictionary hashing for fast cross-media retrieval[C]//Proceedings of the 37th International ACM SIGIR Conference on Research and Development in Information Retrieval.New York, USA:ACM Press, 2014:395-404.
                            </a>
                        </p>
                        <p id="288">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=12,1 Regularized correntropy for robust feature selection">

                                <b>[20]</b> HE Ran, TAN Tieniu, WANG Liang, et al.L<sub>2</sub>, <sub>1</sub> regularized correntropy for robust feature selection[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Computer Society, 2012:2504-2511.
                            </a>
                        </p>
                        <p id="290">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Analysis of Half-Quadratic Minimization Methods for Signal and Image Recovery">

                                <b>[21]</b> NIKOLOVA M, NG M K.Analysis of half-quadratic minimization methods for signal and image recovery[J].SIAM Journal on Scientific Computing, 2005, 27 (3) :937-966.
                            </a>
                        </p>
                        <p id="292">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Coupled dictionary and feature space learning with applications to cross-domain image synthesis and recognition">

                                <b>[22]</b> HUANG Dean, WANG Y F.Coupled dictionary and feature space Learning with applications to cross-domain image synthesis and recognition[C]//Proceedings of IEEE International Conference on Computer Vision.Washington D.C., USA:IEEE Computer Society, 2013:2496-2503.
                            </a>
                        </p>
                        <p id="294">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Efficient sparse coding algorithms">

                                <b>[23]</b> LEE H, BATTLE A, RAINA R, et al.Efficient sparse coding algorithms[C]//Proceedings of the 19th International Conference on Neural Information Processing Systems.Cambridge, USA:MIT Press, 2006:801-808.
                            </a>
                        </p>
                        <p id="296">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Online dictionary learning for sparse coding">

                                <b>[24]</b> MAIRAL J, BACH F, PONCE J, et al.Online dictionary learning for sparse coding[C]//Proceedings of the 26th Annual International Conference on Machine Learning.New York, USA:ACM Press, 2009:689-696.
                            </a>
                        </p>
                        <p id="298">
                            <a id="bibliography_25" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A New Approach to Cross-modal Multimedia Retrieval">

                                <b>[25]</b> RASIWASIA N, PEREIRA J C, COVIELLO E, et al.A new approach to cross-modal multimedia retrieval[C]//Proceedings of the 18th ACM International Conference on Multimedia.New York, USA:ACM Press, 2010:251-260.
                            </a>
                        </p>
                        <p id="300">
                            <a id="bibliography_26" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Reading between the Lines: Object Localization Using Implicit Cues from Image Tags">

                                <b>[26]</b> HWANG S J, GRAUMAN K.Reading between the lines:object localization using implicit cues from image tags[J].IEEE Transactions on Software Engineering, 2012, 34 (6) :1145-1158.
                            </a>
                        </p>
                        <p id="302">
                            <a id="bibliography_27" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD14030300028340&amp;v=MDM1NDhySTlGWk9rSEQzZzVvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVVMbklJVjRTYVJFPU5qN0Jhcks4SHRMTQ==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[27]</b> GONG Yunchao, KE Qifa, ISARD M, et al.A multi-view embedding space for modeling internet images, tags, and their semantics[J].International Journal of Computer Vision, 2014, 106 (2) :210-233.
                            </a>
                        </p>
                        <p id="304">
                            <a id="bibliography_28" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning coupled feature spaces for cross-modal matching">

                                <b>[28]</b> WANG Kaiye, HE Ran, WANG Wei, et al.Learning coupled feature spaces for cross-modal matching[C]//Proceedings of International Conference on Computer Vision.Washington D.C., USA:IEEE Computer Society, 2013:2088-2095.
                            </a>
                        </p>
                        <p id="306">
                            <a id="bibliography_29" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002830901&amp;v=MTk2Mjk3cWVidWR0RkM3bFVyL09JMTA9Tmo3QmFyTzRIdEhPcDR4RmJlc09ZM2s1ekJkaDRqOTlTWHFScnhveGNNSDdS&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[29]</b> LOWE D G.Distinctive image features from scale-invariant keypoints[J].International Journal of Computer Vision, 2004, 60 (2) :91-110.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJC201906037" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201906037&amp;v=Mjc0NDY0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSb0Z5bmxWYnpMTHo3QmJiRzRIOWpNcVk5R1k0UUtESDg0dlI=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
