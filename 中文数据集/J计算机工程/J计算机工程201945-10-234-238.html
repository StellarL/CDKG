<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637126203188052500%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJC201910039%26RESULT%3d1%26SIGN%3dEJIbE4ukfefPL2%252bZMpkbIuKv0UI%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201910039&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201910039&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201910039&amp;v=Mjg1MTM0OUdiWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUnRGeTNoVkx2Tkx6N0JiYkc0SDlqTnI=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0V2Tk9CMmt5SXJncHRzMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#33" data-title="0 概述 ">0 概述</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#37" data-title="1 模型剪枝后泛化能力 ">1 模型剪枝后泛化能力</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#38" data-title="1.1 模型剪枝操作">1.1 模型剪枝操作</a></li>
                                                <li><a href="#43" data-title="1.2 泛化能力">1.2 泛化能力</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#52" data-title="2 实验结果与分析 ">2 实验结果与分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#53" data-title="2.1 实验设置">2.1 实验设置</a></li>
                                                <li><a href="#55" data-title="2.2 结果分析">2.2 结果分析</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#85" data-title="3 结束语 ">3 结束语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#40" data-title="&lt;b&gt;图1 神经网络剪枝过程&lt;/b&gt;"><b>图1 神经网络剪枝过程</b></a></li>
                                                <li><a href="#46" data-title="&lt;b&gt;图2 数字1的灰度图和矩阵形式&lt;/b&gt;"><b>图2 数字1的灰度图和矩阵形式</b></a></li>
                                                <li><a href="#48" data-title="&lt;b&gt;图3 遮挡上半部分的数字1&lt;/b&gt;"><b>图3 遮挡上半部分的数字1</b></a></li>
                                                <li><a href="#50" data-title="&lt;b&gt;图4 数字1的另一种形式&lt;/b&gt;"><b>图4 数字1的另一种形式</b></a></li>
                                                <li><a href="#57" data-title="&lt;b&gt;表1 不同Dropout下的准确率对比&lt;/b&gt;"><b>表1 不同Dropout下的准确率对比</b></a></li>
                                                <li><a href="#63" data-title="&lt;b&gt;表2 完整训练集不同Dropout下的准确率对比1&lt;/b&gt;"><b>表2 完整训练集不同Dropout下的准确率对比1</b></a></li>
                                                <li><a href="#64" data-title="&lt;b&gt;表3 遮挡训练集不同Dropout下的准确率对比1&lt;/b&gt;"><b>表3 遮挡训练集不同Dropout下的准确率对比1</b></a></li>
                                                <li><a href="#67" data-title="&lt;b&gt;表4 完整训练集不同Dropout下的准确率对比2&lt;/b&gt;"><b>表4 完整训练集不同Dropout下的准确率对比2</b></a></li>
                                                <li><a href="#68" data-title="&lt;b&gt;表5 遮挡训练集不同Dropout下的准确率对比2&lt;/b&gt;"><b>表5 遮挡训练集不同Dropout下的准确率对比2</b></a></li>
                                                <li><a href="#71" data-title="&lt;b&gt;图5 表2～表5对应的4种情况示意图&lt;/b&gt;"><b>图5 表2～表5对应的4种情况示意图</b></a></li>
                                                <li><a href="#75" data-title="&lt;b&gt;表6 完整训练集不同剪枝比例下的准确率对比1&lt;/b&gt;"><b>表6 完整训练集不同剪枝比例下的准确率对比1</b></a></li>
                                                <li><a href="#76" data-title="&lt;b&gt;表7 遮挡训练集不同剪枝比例下的准确率对比1&lt;/b&gt;"><b>表7 遮挡训练集不同剪枝比例下的准确率对比1</b></a></li>
                                                <li><a href="#79" data-title="&lt;b&gt;表8 完整训练集不同剪枝比例下的准确率对比2&lt;/b&gt;"><b>表8 完整训练集不同剪枝比例下的准确率对比2</b></a></li>
                                                <li><a href="#80" data-title="&lt;b&gt;表9 遮挡训练集不同剪枝比例下的准确率对比2&lt;/b&gt;"><b>表9 遮挡训练集不同剪枝比例下的准确率对比2</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" title=" 周飞燕,金林鹏,董军.卷积神经网络研究综述[J].计算机学报,2017,40(6):1229-1251." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJX201706001&amp;v=MDU2NTVGWllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJ0RnkzaFZMdk5MejdCZHJHNEg5Yk1xWTk=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0V2Tk9CMmt5SXJncHRzMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         周飞燕,金林鹏,董军.卷积神经网络研究综述[J].计算机学报,2017,40(6):1229-1251.
                                    </a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" title=" HAN Song.Efficient methods and hardware for deep learning[D].San Francisco,USA:Stanford University,2017." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Efficient methods and hardware for deep learning">
                                        <b>[2]</b>
                                         HAN Song.Efficient methods and hardware for deep learning[D].San Francisco,USA:Stanford University,2017.
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title=" 张新钰,高洪波,赵建辉,等.基于深度学习的自动驾驶技术综述[J].清华大学学报(自然科学版),2018,58(4):438-444" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=DKFX&amp;filename=QHXB201804017&amp;v=MTI3NTdCdEdGckNVUkxPZVplUnRGeTNoVkx2Tk5DWFRiTEc0SDluTXE0OUVZNFFLREg4NHZSNFQ2ajU0TzN6cXE=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0V2Tk9CMmt5SXJncHRzMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         张新钰,高洪波,赵建辉,等.基于深度学习的自动驾驶技术综述[J].清华大学学报(自然科学版),2018,58(4):438-444
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" title=" HAN Song,JEFF P,JOHN T,et al.Learning both weights and connections for efficient neural networks[C]//Proceedings of the 28th International Conference on Neural Information Processing Systems.Cambridge,USA:MIT Press,2015:1135-1143." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning both weights and connections for efficient neural network">
                                        <b>[4]</b>
                                         HAN Song,JEFF P,JOHN T,et al.Learning both weights and connections for efficient neural networks[C]//Proceedings of the 28th International Conference on Neural Information Processing Systems.Cambridge,USA:MIT Press,2015:1135-1143.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" title=" HAN Song,MAO Huizi,DALLY W J.Deep compression:compressing deep neural networks with pruning,trained quantization and Huffman coding [EB/OL].[2018-06-18].https://arxiv.org/pdf/1510.00149.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep compression:compressing deep neural networks with pruning trained quantization and huffman coding">
                                        <b>[5]</b>
                                         HAN Song,MAO Huizi,DALLY W J.Deep compression:compressing deep neural networks with pruning,trained quantization and Huffman coding [EB/OL].[2018-06-18].https://arxiv.org/pdf/1510.00149.pdf.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" title=" ROSA G H D,PAPA J P,YANG X S.Handling dropout probability estimation in convolution neural networks using meta-heuristics[J].Soft Computing,2017,22(18):6147-6156." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Handling dropout probability estimation in convolution neural networks using meta-heuristics">
                                        <b>[6]</b>
                                         ROSA G H D,PAPA J P,YANG X S.Handling dropout probability estimation in convolution neural networks using meta-heuristics[J].Soft Computing,2017,22(18):6147-6156.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" title=" POERNOMO A,KANG D K.Biased dropout and crossmap dropout:learning towards effective dropout regularization in convolutional neural network[J].Neural Networks,2018,104:60-67." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES3A3A26C7053E20B18DA8A3D24F9E1F8C&amp;v=MDQ2NTJGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkNwYlEzNU50aHdieTR4SzA9TmlmT2ZiREpIYURPcWZ4Q1pPNE1lWDQ1dlJjYm5rNTFPWHlXcmhaRGNNZVZNN0xzQ09Odg==&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0V2Tk9CMmt5SXJncHRzMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                         POERNOMO A,KANG D K.Biased dropout and crossmap dropout:learning towards effective dropout regularization in convolutional neural network[J].Neural Networks,2018,104:60-67.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" title=" MESHGI K,MAEDA S I,OBA S,et al.Constructing a meta-tracker using Dropout to imitate the behavior of an arbitrary black-box tracker.[J].Neural Networks,2017,87:132-148." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESA092056E8BBEF609E40F862D4E8D1778&amp;v=Mjc4NzE0eEswPU5pZk9mY0s0RjlQTXFva3diSmw5ZVFvL3p4OW03ajhMUUhuZzJCWkFjY2FWUXIyWENPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkNwYlEzNU50aHdieQ==&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0V2Tk9CMmt5SXJncHRzMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                         MESHGI K,MAEDA S I,OBA S,et al.Constructing a meta-tracker using Dropout to imitate the behavior of an arbitrary black-box tracker.[J].Neural Networks,2017,87:132-148.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" >
                                        <b>[9]</b>
                                     SRIVASTAVA N,HINTON G,KRIZHEVSKY A,et al.Dropout:a simple way to prevent neural networks from overfitting[J].The Journal of Machine Learning Research,2014,15(1):1929-1958.</a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_10" >
                                        <b>[10]</b>
                                     LECUN Y,BOTTOU L,BENGIO Y,et al.Gradient-based learning applied to document recognition[J].Proceedings of the IEEE,1998,86(11):2278-2324.</a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_11" title=" KRIZHEVSKY A,SUTSKEVER I,HINTON G E.ImageNet classification with deep convolutional neural networks[C]//Proceedings of the 25th International Conference on Neural Information Processing Systems.[S.l.]:Curran Associates Inc.,2012:1097-1105." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image Net Classification with Deep Convolutional Neural Networks">
                                        <b>[11]</b>
                                         KRIZHEVSKY A,SUTSKEVER I,HINTON G E.ImageNet classification with deep convolutional neural networks[C]//Proceedings of the 25th International Conference on Neural Information Processing Systems.[S.l.]:Curran Associates Inc.,2012:1097-1105.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_12" title=" HINTON G E,SRIVASTAVA N,KRIZHEVSKY A,et al.Improving neural networks by preventing co-adaptation of feature detectors[J].Computer Science,2012,3(4):212-223." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Improving neural networks by preventing co-adaptation of feature detectors">
                                        <b>[12]</b>
                                         HINTON G E,SRIVASTAVA N,KRIZHEVSKY A,et al.Improving neural networks by preventing co-adaptation of feature detectors[J].Computer Science,2012,3(4):212-223.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_13" title=" ZHANG Chiyuan,BENGIO S,HARDT M,et al.Understanding deep learning requires rethinking generalization[EB/OL].[2018-06-18].https://arxiv.org/pdf/1611.03530.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Understanding deep learning requires rethinking generalization">
                                        <b>[13]</b>
                                         ZHANG Chiyuan,BENGIO S,HARDT M,et al.Understanding deep learning requires rethinking generalization[EB/OL].[2018-06-18].https://arxiv.org/pdf/1611.03530.pdf.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_14" title=" CHEN Binghui,DENG Weihong,DU Junping.Noisy softmax:improving the generalization ability of DCNN via postponing the early softmax saturation [EB/OL].[2018-06-18].https://arxiv.org/pdf/1708.03769.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Noisy softmax:improving the generalization ability of DCNN via postponing the early softmax saturation">
                                        <b>[14]</b>
                                         CHEN Binghui,DENG Weihong,DU Junping.Noisy softmax:improving the generalization ability of DCNN via postponing the early softmax saturation [EB/OL].[2018-06-18].https://arxiv.org/pdf/1708.03769.pdf.
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_15" title=" 李小薪,梁荣华.有遮挡人脸识别综述:从子空间回归到深度学习[J].计算机学报,2018,41(1):177-207." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJX201801011&amp;v=MjYxOTJPM3pxcUJ0R0ZyQ1VSTE9lWmVSdEZ5M2hWTHZOTHo3QmRyRzRIOW5Ncm85RVpZUUtESDg0dlI0VDZqNTQ=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0V2Tk9CMmt5SXJncHRzMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[15]</b>
                                         李小薪,梁荣华.有遮挡人脸识别综述:从子空间回归到深度学习[J].计算机学报,2018,41(1):177-207.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJC" target="_blank">计算机工程</a>
                2019,45(10),234-238 DOI:10.19678/j.issn.1000-3428.0052165            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>一种神经网络模型剪枝后泛化能力的验证方法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%88%98%E5%B4%87%E9%98%B3&amp;code=39402361&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0V2Tk9CMmt5SXJncHRzMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">刘崇阳</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%88%98%E5%8B%A4%E8%AE%A9&amp;code=17417509&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0V2Tk9CMmt5SXJncHRzMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">刘勤让</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%9B%BD%E5%AE%B6%E6%95%B0%E5%AD%97%E4%BA%A4%E6%8D%A2%E7%B3%BB%E7%BB%9F%E5%B7%A5%E7%A8%8B%E6%8A%80%E6%9C%AF%E7%A0%94%E7%A9%B6%E4%B8%AD%E5%BF%83&amp;code=0291391&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0V2Tk9CMmt5SXJncHRzMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">国家数字交换系统工程技术研究中心</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>针对神经网络模型在剪枝操作中Dropout率下调造成的过拟合问题,提出一种剪枝模型泛化能力的验证方法。研究人为遮挡数据集模拟图像范围的变化情况,分析不同Dropout值和剪枝比例对模型准确率的影响,进而得到剪枝操作后模型泛化能力变化的原因。在卷积神经网络模型lenet-5上进行实验,结果表明,剪枝模型泛化能力减弱是因为Dropout率下调和剪枝操作时参数量的变化。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0V2Tk9CMmt5SXJncHRzMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%A8%A1%E5%9E%8B%E5%89%AA%E6%9E%9D&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0V2Tk9CMmt5SXJncHRzMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">模型剪枝;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0V2Tk9CMmt5SXJncHRzMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B3%9B%E5%8C%96%E8%83%BD%E5%8A%9B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0V2Tk9CMmt5SXJncHRzMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">泛化能力;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%81%AE%E6%8C%A1%E6%95%B0%E6%8D%AE%E9%9B%86&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0V2Tk9CMmt5SXJncHRzMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">遮挡数据集;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    刘崇阳(1994—),男,硕士研究生,主研方向为人工智能、深度学习;;
                                </span>
                                <span>
                                    刘勤让,研究员、博士。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-07-19</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家科技重大专项(2016ZX01012101);</span>
                                <span>国家自然科学基金面上项目(61572520);国家自然科学基金创新研究群体项目(61521003);</span>
                    </p>
            </div>
                    <h1><b>A Verification Method on Post-Pruning Generalization Ability of Neural Network Model</b></h1>
                    <h2>
                    <span>LIU Chongyang</span>
                    <span>LIU Qinrang</span>
            </h2>
                    <h2>
                    <span>China National Digital Switching System Engineering and Technological R&amp;D Center</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>To address the over-fitting problem caused by the down-regulation of the Dropout rate in the pruning operation of the neural network model,a verification method for the generalization ability of the pruning model is proposed.By artificially occluding the dataset to simulate the change of the image range,the effects of different Dropout values and pruning ratios on the accuracy of the model are analyzed,and the reasons for the change of the generalization ability of the model after pruning operation are obtained.Experiments on the convolutional neural network model lenet-5 show that the generalization ability of the pruning model is weakened because of the drop of the Dropout rate and the change of the parameter amount during the pruning operation.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Deep%20Neural%20Networks(DNN)&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0V2Tk9CMmt5SXJncHRzMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Deep Neural Networks(DNN);</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=model%20pruning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0V2Tk9CMmt5SXJncHRzMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">model pruning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Deep%20Learning(DL)&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0V2Tk9CMmt5SXJncHRzMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Deep Learning(DL);</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=generalization%20ability&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0V2Tk9CMmt5SXJncHRzMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">generalization ability;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=occlusion%20datasets&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0V2Tk9CMmt5SXJncHRzMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">occlusion datasets;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-07-19</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="33" name="33" class="anchor-tag">0 概述</h3>
                <div class="p1">
                    <p id="34">随着人工智能技术的不断发展,未来将出现大量廉价、便携以及功耗低的智能设备。机器通过不断学习会变得更加智能,而深度神经网络(Deep Neural Networks, DNN)是机器学习的主要技术之一<citation id="89" type="reference"><link href="3" rel="bibliography" /><link href="5" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>]</sup></citation>。虽然深度学习(Deep Learning, DL)在学术界的发展速度较快,但在产业界并未取得重大进展。例如,深度学习技术在自动驾驶中应用发展多年,但并未达到商业化要求<citation id="87" type="reference"><link href="7" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>。这是因为深度学习算法及模型复杂度高、计算量大,较难部署在硬件资源有限的嵌入式设备中<citation id="88" type="reference"><link href="9" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>。工艺的发展进步是解决该问题的一个途径,但减小模型参数量和计算量是更为有效的方法。</p>
                </div>
                <div class="p1">
                    <p id="35">文献<citation id="90" type="reference">[<a class="sup">5</a>]</citation>提出深度压缩(Deep Compression,DC)技术,通过对神经网络模型进行剪枝操作,将模型权重参数丢弃,在剪枝操作完成后,这些参数固定为0。由于在剪枝过程中减小了模型容量,因此需重新调整Dropout率。</p>
                </div>
                <div class="p1">
                    <p id="36">剪枝操作能够降低模型容量,防止过拟合<sup></sup><citation id="92" type="reference"><link href="13" rel="bibliography" /><link href="15" rel="bibliography" /><sup>[<a class="sup">6</a>,<a class="sup">7</a>]</sup></citation>,但Dropout率降低会导致多模型融合效率降低,使模型的泛化能力和可拓展性能变差<citation id="93" type="reference"><link href="17" rel="bibliography" /><link href="19" rel="bibliography" /><sup>[<a class="sup">8</a>,<a class="sup">9</a>]</sup></citation>。为此,本文建立一种基于剪枝的神经网络模型。描述神经网络剪枝操作过程,并探究其泛化能力,在此基础上,以MNIST数据集上的lenet-5<citation id="91" type="reference"><link href="21" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>为模型,设计Dropout和剪枝操作的对比实验。</p>
                </div>
                <h3 id="37" name="37" class="anchor-tag">1 模型剪枝后泛化能力</h3>
                <h4 class="anchor-tag" id="38" name="38">1.1 模型剪枝操作</h4>
                <div class="p1">
                    <p id="39">随着数据的快速增长,神经网络训练数据成倍增加。研究人员需要数天甚至数周的时间来训练大型神经网络模型,如Alexnet的训练时间长达6天(基于2块GTX 580 3 GB GPU)<citation id="94" type="reference"><link href="23" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>。根据模型的移值性要求,在不影响模型精度的情况下,通过减少参数量来提高训练效率。文献<citation id="95" type="reference">[<a class="sup">5</a>]</citation>的神经网络剪枝过程如图1所示。</p>
                </div>
                <div class="area_img" id="40">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201910039_040.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0V2Tk9CMmt5SXJncHRzMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 神经网络剪枝过程" src="Detail/GetImg?filename=images/JSJC201910039_040.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0V2Tk9CMmt5SXJncHRzMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图1 神经网络剪枝过程</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201910039_040.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0V2Tk9CMmt5SXJncHRzMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="41">剪枝操作不仅减少了连接权重(图1箭头所示),而且神经元节点也会减少(图1圆圈所示)。剪枝操作使网络变稀疏,能够简化网络的复杂性。模型复杂性降低会减少过拟合情况的发生,提升模型泛化能力。同时,网络中需要确定的参数量减少。根据训练数据确定模型参数,以进一步降低训练数据的数量。在剪枝后,对稀疏网络进行重新训练,否则模型会有精度损失。重新训练和模型最开始的训练过程类似,已经裁剪掉的不用训练,根据文献<citation id="96" type="reference">[<a class="sup">5</a>]</citation>,剪枝比例一般超过50%,重新训练的剩余部分只占少数,若继续保持最开始训练时的Dropout率,训练出来的模型精度会下降,因此可减少Dropout率。</p>
                </div>
                <div class="p1">
                    <p id="42">Dropout层一般放置在全连接层后。在训练过程中,Dropout层会丢弃一定数量的信息,只有部分数据发挥作用。在预测过程中,Dropout层将使用所有参数。具体来说,Dropout将隐含层输出以一定的概率置0,此神经元不参与前向和后向传播。由于采用随机丢弃的方式,每一次进行前向后向计算时,丢弃掉的数据不同,模型每一次的前向后向计算的表现也不同。Dropout可以看成是一种组合模型,每个样本训练出不一样的网络结构。Dropout减少了神经元之间的共适应关系,一个神经元不会依赖另一个神经元,使网络学习到更加鲁棒的特征表示<citation id="97" type="reference"><link href="25" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>。改变Dropout率具有如下优点:在一定程度上减轻过拟合的情况;使模型具有多模型融合的效果,即提高模型的拓展性和泛化能力。</p>
                </div>
                <h4 class="anchor-tag" id="43" name="43">1.2 泛化能力</h4>
                <div class="p1">
                    <p id="44">神经网络的泛化能力是训练好的网络对于不在训练集内数据的拟合能力<citation id="99" type="reference"><link href="27" rel="bibliography" /><link href="29" rel="bibliography" /><sup>[<a class="sup">13</a>,<a class="sup">14</a>]</sup></citation>。目前,神经网络被广泛应用于计算机视觉,本文以视觉领域最基础的分类任务来探究其泛化能力。分类问题中存在一个十分经典的问题,即遮挡问题<citation id="98" type="reference"><link href="31" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>。本文研究基于遮挡问题的神经网络模型泛化能力。人类具有利用部分信息进行分析推断的能力。对于人类来说,如果遮挡物体的一部分,只要不是遮挡住最关键的信息,就可以通过局部信息识别一个完整的物体。计算机如果有人类的分析推理能力,其应具有较好的泛化能力。神经网络中最简单的手写数字体识别是一个分类问题,本文结合遮挡问题来探究其泛化能力。</p>
                </div>
                <div class="p1">
                    <p id="45">手写数字体识别的数据集是0～9的数字,以灰度图形式表示。灰度图的每个像素点用0～255的数字表示灰度值,值越大越接近纯白色。图2所示为数字1的灰度图和像素点矩阵形式。</p>
                </div>
                <div class="area_img" id="46">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201910039_046.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0V2Tk9CMmt5SXJncHRzMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 数字1的灰度图和矩阵形式" src="Detail/GetImg?filename=images/JSJC201910039_046.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0V2Tk9CMmt5SXJncHRzMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图2 数字1的灰度图和矩阵形式</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201910039_046.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0V2Tk9CMmt5SXJncHRzMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="47">如果将图2(a)中数字1的上半部分全置0,就达到了遮挡的效果,如图3所示。</p>
                </div>
                <div class="area_img" id="48">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201910039_048.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0V2Tk9CMmt5SXJncHRzMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 遮挡上半部分的数字1" src="Detail/GetImg?filename=images/JSJC201910039_048.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0V2Tk9CMmt5SXJncHRzMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图3 遮挡上半部分的数字1</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201910039_048.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0V2Tk9CMmt5SXJncHRzMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="49">在图3中,数字1较容易确认。图4所示为另外一种形式的数字1,可以看出,若遮挡上半部分,无法确认是数字1还是数字7。基于图2～图4所描述的遮挡过程,本文研究训练集和测试集有差异时神经网络表现出的泛化能力。</p>
                </div>
                <div class="area_img" id="50">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201910039_050.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0V2Tk9CMmt5SXJncHRzMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 数字1的另一种形式" src="Detail/GetImg?filename=images/JSJC201910039_050.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0V2Tk9CMmt5SXJncHRzMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图4 数字1的另一种形式</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201910039_050.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0V2Tk9CMmt5SXJncHRzMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="51">为探究剪枝后泛化能力的变化,本文在未剪枝模型上测试Dropout率变化对泛化能力的影响,对重新训练好的模型进行泛化能力实验,通过对比分析验证剪枝对泛化能力的影响。</p>
                </div>
                <h3 id="52" name="52" class="anchor-tag">2 实验结果与分析</h3>
                <h4 class="anchor-tag" id="53" name="53">2.1 实验设置</h4>
                <div class="p1">
                    <p id="54">本文实验基于Caffe深度学习框架,使用MNIST数据集,包含60 000张训练数据和10 000张测试数据,采用lenet-5手写数字体识别卷积神经网络。Caffe官网文件库(Model Zoo)没有经过训练的lenet-5模型权重文件,本文根据下载的MNIST数据集训练得到模型权重文件(Caffe中以caffemodel形式存储)。实验计算机配置为:Intel<sup>©</sup> Core i5-4210M双核处理器,内存8 GB RAM,Ubuntu16.04,利用Python脚本实现数据集的处理和剪枝操作。从官网下载的MNIST数据集转换为图片格式,然后对图片进行遮挡操作,再构建txt标签,生成可供训练(mnist_train_lmdb)和测试(mnist_test_lmdb)的lmdb格式的数据集。遮挡操作遮挡数字图上半部分,Dropout层加在lenet-5的第一个全连接层后面,Dropout率从0.0～0.9间隔0.1变化,剪枝操作逐层进行,剪枝极限依据文献<citation id="100" type="reference">[<a class="sup">5</a>]</citation>提供的数据。所有实验都进行10次,计算平均值作为最终结果。</p>
                </div>
                <h4 class="anchor-tag" id="55" name="55">2.2 结果分析</h4>
                <div class="p1">
                    <p id="56"><b>实验1</b> Caffe自带的lenet-5网络模型中没有Dropout层,在完整的MNIST训练集和测试集上lenet-5模型的准确率<i>acc</i>=0.991 1(<i>Dropout</i>=0.0),在第一个全连接层加Dropout层,调节Dropout率大小,实验结果如表1所示。</p>
                </div>
                <div class="area_img" id="57">
                    <p class="img_tit"><b>表1 不同Dropout下的准确率对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note">%</p>
                    <table id="57" border="1"><tr><td><br />Dropout</td><td>准确率</td></tr><tr><td><br />0.0</td><td>0.991 1</td></tr><tr><td><br />0.1</td><td>0.991 5</td></tr><tr><td><br />0.2</td><td>0.991 0</td></tr><tr><td><br />0.3</td><td>0.990 9</td></tr><tr><td><br />0.4</td><td>0.992 0</td></tr><tr><td><br />0.5</td><td>0.991 9</td></tr><tr><td><br />0.6</td><td>0.992 1</td></tr><tr><td><br />0.7</td><td>0.991 3</td></tr><tr><td><br />0.8</td><td>0.991 7</td></tr><tr><td><br />0.9</td><td>0.989 2</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="58">从表1可以看出,不同Dropout的准确率变化程度较小,因此,调整Dropout对准确率影响较小。这可能由于Caffe自带改进的lenet-5模型文件中不含Dropout层。</p>
                </div>
                <div class="p1">
                    <p id="59"><b>实验2</b> 训练集保持完整,测试集遮挡上半部分,未加Dropout层的准确率<i>acc</i>=0.459 3。将Dropout层加在第一个全连接层后,以3种模式进行训练,具体分析如下:</p>
                </div>
                <div class="p1">
                    <p id="60">1)以随机初始化的方式进行训练,由于分类结果有10个,准确率会从10%逐渐提升。</p>
                </div>
                <div class="p1">
                    <p id="61">2)以完整训练测试集训练出一个caffemodel,再在caffemodel的基础上进行微调,即将预先训练好的模型参数作为初始化参数。</p>
                </div>
                <div class="p1">
                    <p id="62">3)在训练测试集遮挡上半部分的数据集上训练出一个caffemodel,然后进行微调。前2种模式效果一样,因为训练过程是基于训练集进行调参,参数的确定由训练集决定,测试集给出模型准确率,模式1和模式2根据同一个训练集而来,模式2在模式1的基础上多训了一遍,相当于同一种类型。改变Dropout率的结果如表2和表3所示。</p>
                </div>
                <div class="area_img" id="63">
                    <p class="img_tit"><b>表2 完整训练集不同Dropout下的准确率对比1</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="63" border="1"><tr><td><br />Dropout</td><td>准确率</td></tr><tr><td><br />0.0</td><td>0.459 3</td></tr><tr><td><br />0.1</td><td>0.460 9</td></tr><tr><td><br />0.2</td><td>0.479 7</td></tr><tr><td><br />0.3</td><td>0.485 9</td></tr><tr><td><br />0.4</td><td>0.494 9</td></tr><tr><td><br />0.5</td><td>0.503 4</td></tr><tr><td><br />0.6</td><td>0.501 1</td></tr><tr><td><br />0.7</td><td>0.512 5</td></tr><tr><td><br />0.8</td><td>0.518 9</td></tr><tr><td><br />0.9</td><td>0.520 6</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="64">
                    <p class="img_tit"><b>表3 遮挡训练集不同Dropout下的准确率对比1</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="64" border="1"><tr><td><br />Dropout</td><td>准确率</td></tr><tr><td><br />0.0</td><td>0.857 3</td></tr><tr><td><br />0.1</td><td>0.829 0</td></tr><tr><td><br />0.2</td><td>0.787 5</td></tr><tr><td><br />0.3</td><td>0.759 2</td></tr><tr><td><br />0.4</td><td>0.748 3</td></tr><tr><td><br />0.5</td><td>0.729 2</td></tr><tr><td><br />0.6</td><td>0.705 4</td></tr><tr><td><br />0.7</td><td>0.673 1</td></tr><tr><td><br />0.8</td><td>0.625 8</td></tr><tr><td><br />0.9</td><td>0.564 1</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="65">从表2可以看出,未加Dropout层的准确率<i>acc</i>=0.459 3,比实验1的准确率低。在训练的过程中,根据终端的快照,某些轮次的测试集准确率比0.459 3要高一些,但是会出现越训练效果不升反降的情况。这是由于训练集和测试集的数据分布和信息容量不同。在训练时,使用了全部的数据信息,在识别时每个位置都会被当作识别的特征加以提取。在测试时,被遮挡后只有一半的数据,有确定的特征突然消失,导致神经网络模型准确率下降。此外,表2中准确率呈上升趋势,说明 Dropout能够使模型具有较高的泛化能力。</p>
                </div>
                <div class="p1">
                    <p id="66"><b>实验3</b> 训练集遮挡上半部分,测试集完整,Dropout层加在第一个全连接层后。在2种不同的caffemodel基础上进行微调,结果如表4和表5所示。</p>
                </div>
                <div class="area_img" id="67">
                    <p class="img_tit"><b>表4 完整训练集不同Dropout下的准确率对比2</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="67" border="1"><tr><td><br />Dropout</td><td>准确率</td></tr><tr><td><br />0.0</td><td>0.968 0</td></tr><tr><td><br />0.1</td><td>0.957 4</td></tr><tr><td><br />0.2</td><td>0.952 3</td></tr><tr><td><br />0.3</td><td>0.952 1</td></tr><tr><td><br />0.4</td><td>0.950 6</td></tr><tr><td><br />0.5</td><td>0.949 7</td></tr><tr><td><br />0.6</td><td>0.947 0</td></tr><tr><td><br />0.7</td><td>0.939 3</td></tr><tr><td><br />0.8</td><td>0.930 6</td></tr><tr><td><br />0.9</td><td>0.901 0</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="68">
                    <p class="img_tit"><b>表5 遮挡训练集不同Dropout下的准确率对比2</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="68" border="1"><tr><td><br />Dropout</td><td>准确率</td></tr><tr><td><br />0.0</td><td>0.851 9</td></tr><tr><td><br />0.1</td><td>0.849 8</td></tr><tr><td><br />0.2</td><td>0.849 3</td></tr><tr><td><br />0.3</td><td>0.845 8</td></tr><tr><td><br />0.4</td><td>0.845 5</td></tr><tr><td><br />0.5</td><td>0.845 2</td></tr><tr><td><br />0.6</td><td>0.840 8</td></tr><tr><td><br />0.7</td><td>0.839 4</td></tr><tr><td><br />0.8</td><td>0.833 9</td></tr><tr><td><br />0.9</td><td>0.829 4</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="69">从表5可以看出,随着Dropout率上升,准确率逐渐下降。表5和表2表示结果相反,2个实验的测试集和训练集都不同,实验3的测试数据比实验2测试数据多1倍。可以看出,测试数据较多时,提高Dropout率会放弃更多的特征,使模型的准确率变差。如果实验3对应的测试集遮挡3/4,则 Dropout率越高准确率越高。表5和表2对比也可以说明Dropout的泛化能力提升仅针对测试集变小的情况。</p>
                </div>
                <div class="p1">
                    <p id="70">表2～表5实验具体情况如图5所示。表3和表4两者初始的caffemodel和测试集保持一致,训练过程以增大(减少)一半的训练集进行调参,调参的越多就会和测试集偏差越多,因此增大Dropout率会放弃更多和测试集相同的特征,导致准确率一直下降。</p>
                </div>
                <div class="area_img" id="71">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201910039_071.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0V2Tk9CMmt5SXJncHRzMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 表2～表5对应的4种情况示意图" src="Detail/GetImg?filename=images/JSJC201910039_071.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0V2Tk9CMmt5SXJncHRzMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图5 表2～表5对应的4种情况示意图</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201910039_071.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0V2Tk9CMmt5SXJncHRzMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="72"><b>实验4</b> 不加Dropout层,同时遮挡训练和测试集上半部分后的模型准确率<i>acc</i>=0.912 4。加入Dropout并改变其大小,准确率基本维持不变。测试集被遮挡一半,如果把训练数据也变成只有一半,与表2中Dropout为0的情况相比,不加Dropout层,训练测试集同时遮挡上半部分后的模型准确率<i>acc</i>=0.912 4。</p>
                </div>
                <div class="p1">
                    <p id="73"><b>实验5</b> 对于剪枝后的模型,本文实验均未加Dropout层。剪枝极限参考文献<citation id="101" type="reference">[<a class="sup">5</a>]</citation>,本文稍有改动,剪枝率的参数值设置如下:第1个卷积层为33%,第2个卷积层为80%,第1个全连接层为90%及第2个全连接层为80%。根据完整的MNIST训练集和测试集出来的lenet-5剪枝模型准确率<i>acc</i>=0.988 9。向下调整剪枝比例后准确率变化不大,保持在0.99附近。加入Dropout层准确率基本没有变化。实验结果表明,适当剪枝不会影响准确率。</p>
                </div>
                <div class="p1">
                    <p id="74"><b>实验6</b> 训练集保持完整,测试集遮挡上半部分,在2种不同的caffemodel基础上进行微调,并调整剪枝比例,得到的结果如表6和表7所示。剪枝模型在一个固定剪枝比例的基础上改变Dropout后的准确率和表2、表3情况类似,不再赘述(以33%、80%、90%、80%比例剪枝的模型逐渐增大Dropout率,模型准确率呈上升趋势)。</p>
                </div>
                <div class="area_img" id="75">
                    <p class="img_tit"><b>表6 完整训练集不同剪枝比例下的准确率对比1</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="75" border="1"><tr><td><br />剪枝比例/%</td><td>准确率</td></tr><tr><td><br />33,80,90,80</td><td>0.413 2</td></tr><tr><td><br />0,80,90,80</td><td>0.413 8</td></tr><tr><td><br />33,0,90,80</td><td>0.423 7</td></tr><tr><td><br />33,80,0,80</td><td>0.426 8</td></tr><tr><td><br />33,80,80,0</td><td>0.450 3</td></tr><tr><td><br />16,40,90,80</td><td>0.421 3</td></tr><tr><td><br />33,80,45,40</td><td>0.452 4</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="76">
                    <p class="img_tit"><b>表7 遮挡训练集不同剪枝比例下的准确率对比1</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="76" border="1"><tr><td><br />剪枝比例/%</td><td>准确率</td></tr><tr><td><br />33,80,90,80</td><td>0.837 3</td></tr><tr><td><br />0,80,90,80</td><td>0.839 3</td></tr><tr><td><br />33,0,90,80</td><td>0.840 8</td></tr><tr><td><br />33,80,0,80</td><td>0.830 2</td></tr><tr><td><br />33,80,80,0</td><td>0.846 0</td></tr><tr><td><br />16,40,90,80</td><td>0.838 5</td></tr><tr><td><br />33,80,45,40</td><td>0.837 8</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="77">从表6可以看出,经过剪枝的准确率小于未剪枝时的准确率(表2中的0.459 3),说明剪枝过程本身会降低模型的泛化能力。这是由于剪枝模型权重参数减少,放弃了更多的细微特征,虽然不是主要特征,但在识别阶段提取特征时,很多微小特征也有可能具有明显的相同特征,导致模型准确率降低。加上Dropout后和表2情况一样,则对于本来有Dropout层的网络模型,剪枝必然伴随Dropout率的下调,剪枝后模型的泛化能力还会进一步降低。另外,各层剪枝后的泛化能力稍有区别,例如对单个层不进行剪枝,模型准确率有较大区别,对比表6中第2行～第7行数据,可以得出全连接层比卷积层敏感的结论。</p>
                </div>
                <div class="p1">
                    <p id="78"><b>实验7</b> 训练集遮挡上半部分,测试集完整,在2种不同的caffemodel基础上进行微调,并调整剪枝比例,得到的结果如表8和表9所示。剪枝模型固定剪枝率后调节Dropout后情况和表4、表5情况类似,不再赘述。</p>
                </div>
                <div class="area_img" id="79">
                    <p class="img_tit"><b>表8 完整训练集不同剪枝比例下的准确率对比2</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="79" border="1"><tr><td><br />剪枝比例/%</td><td>准确率</td></tr><tr><td><br />33,80,90,80</td><td>0.959 8</td></tr><tr><td><br />0,80,90,80</td><td>0.949 5</td></tr><tr><td><br />33,0,90,80</td><td>0.958 3</td></tr><tr><td><br />33,80,0,80</td><td>0.955 9</td></tr><tr><td><br />33,80,80,0</td><td>0.960 5</td></tr><tr><td><br />16,40,90,80</td><td>0.958 8</td></tr><tr><td><br />33,80,45,40</td><td>0.960 8</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="80">
                    <p class="img_tit"><b>表9 遮挡训练集不同剪枝比例下的准确率对比2</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="80" border="1"><tr><td><br />剪枝比例/%</td><td>准确率</td></tr><tr><td><br />33,80,90,80</td><td>0.825 1</td></tr><tr><td><br />0,80,90,80</td><td>0.821 2</td></tr><tr><td><br />33,0,90,80</td><td>0.830 2</td></tr><tr><td><br />33,80,0,80</td><td>0.837 3</td></tr><tr><td><br />33,80,80,0</td><td>0.832 2</td></tr><tr><td><br />16,40,90,80</td><td>0.834 0</td></tr><tr><td><br />33,80,45,40</td><td>0.849 7</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="81">从表9可以看出,经过剪枝的准确率小于未剪枝时的准确率(表5中的0.851 9)。说明模型剪枝后泛化能力减弱不仅发生在测试集缩小的情况,放大测试集后其泛化能力也会减弱。另外,各层剪枝后的泛化能力稍有区别,与表6结论一致,不再赘述。</p>
                </div>
                <div class="p1">
                    <p id="82"><b>实验8</b> 训练集和测试集都遮挡上半部分,模型准确率<i>acc</i>=0.910 2。往下调整剪枝比例后准确率变化不大,保持在0.91附近,加入Dropout层准确率也基本没有变化。实验8的结论同实验5的结论。</p>
                </div>
                <div class="p1">
                    <p id="83">表7和表8与表3和表4中Dropout为0的情况相比,准确率都降低。准确率下降的原因同实验6,不再赘述。</p>
                </div>
                <div class="p1">
                    <p id="84">综上,通过对比表2、表5、表6和表9,可以得到模型的泛化能力:Dropout提升泛化能力仅对测试集变小的情况有效;对于模型剪枝,所有情况的准确率比未剪枝的低,泛化能力比未剪枝的弱。因此,如果模型有Dropout层,在剪枝操作中下调Dropout率,能够进一步降低泛化能力。</p>
                </div>
                <h3 id="85" name="85" class="anchor-tag">3 结束语</h3>
                <div class="p1">
                    <p id="86">剪枝操作中Dropout率下调会给模型带来负面影响。为此,本文提出一种神经网络模型泛化能力的验证方法。在lenet-5模型上的实验结果表明,剪枝后的模型泛化能力减弱与Dropout率下调和参数量减少有关。下一步将在神经网络模型上研究Dropout层的位置对剪枝模型泛化能力的影响。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJX201706001&amp;v=MTAwNjRSTE9lWmVSdEZ5M2hWTHZOTHo3QmRyRzRIOWJNcVk5RlpZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1U=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0V2Tk9CMmt5SXJncHRzMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> 周飞燕,金林鹏,董军.卷积神经网络研究综述[J].计算机学报,2017,40(6):1229-1251.
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Efficient methods and hardware for deep learning">

                                <b>[2]</b> HAN Song.Efficient methods and hardware for deep learning[D].San Francisco,USA:Stanford University,2017.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=DKFX&amp;filename=QHXB201804017&amp;v=MTYwMDZPZVplUnRGeTNoVkx2Tk5DWFRiTEc0SDluTXE0OUVZNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkw=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0V2Tk9CMmt5SXJncHRzMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> 张新钰,高洪波,赵建辉,等.基于深度学习的自动驾驶技术综述[J].清华大学学报(自然科学版),2018,58(4):438-444
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning both weights and connections for efficient neural network">

                                <b>[4]</b> HAN Song,JEFF P,JOHN T,et al.Learning both weights and connections for efficient neural networks[C]//Proceedings of the 28th International Conference on Neural Information Processing Systems.Cambridge,USA:MIT Press,2015:1135-1143.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep compression:compressing deep neural networks with pruning trained quantization and huffman coding">

                                <b>[5]</b> HAN Song,MAO Huizi,DALLY W J.Deep compression:compressing deep neural networks with pruning,trained quantization and Huffman coding [EB/OL].[2018-06-18].https://arxiv.org/pdf/1510.00149.pdf.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Handling dropout probability estimation in convolution neural networks using meta-heuristics">

                                <b>[6]</b> ROSA G H D,PAPA J P,YANG X S.Handling dropout probability estimation in convolution neural networks using meta-heuristics[J].Soft Computing,2017,22(18):6147-6156.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES3A3A26C7053E20B18DA8A3D24F9E1F8C&amp;v=MDA5NDI0NXZSY2JuazUxT1h5V3JoWkRjTWVWTTdMc0NPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkNwYlEzNU50aHdieTR4SzA9TmlmT2ZiREpIYURPcWZ4Q1pPNE1lWA==&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0V2Tk9CMmt5SXJncHRzMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b> POERNOMO A,KANG D K.Biased dropout and crossmap dropout:learning towards effective dropout regularization in convolutional neural network[J].Neural Networks,2018,104:60-67.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESA092056E8BBEF609E40F862D4E8D1778&amp;v=MDE5MzJOaWZPZmNLNEY5UE1xb2t3YkpsOWVRby96eDltN2o4TFFIbmcyQlpBY2NhVlFyMlhDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZDcGJRMzVOdGh3Ynk0eEswPQ==&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0V2Tk9CMmt5SXJncHRzMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b> MESHGI K,MAEDA S I,OBA S,et al.Constructing a meta-tracker using Dropout to imitate the behavior of an arbitrary black-box tracker.[J].Neural Networks,2017,87:132-148.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" >
                                    <b>[9]</b>
                                 SRIVASTAVA N,HINTON G,KRIZHEVSKY A,et al.Dropout:a simple way to prevent neural networks from overfitting[J].The Journal of Machine Learning Research,2014,15(1):1929-1958.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_10" >
                                    <b>[10]</b>
                                 LECUN Y,BOTTOU L,BENGIO Y,et al.Gradient-based learning applied to document recognition[J].Proceedings of the IEEE,1998,86(11):2278-2324.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image Net Classification with Deep Convolutional Neural Networks">

                                <b>[11]</b> KRIZHEVSKY A,SUTSKEVER I,HINTON G E.ImageNet classification with deep convolutional neural networks[C]//Proceedings of the 25th International Conference on Neural Information Processing Systems.[S.l.]:Curran Associates Inc.,2012:1097-1105.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Improving neural networks by preventing co-adaptation of feature detectors">

                                <b>[12]</b> HINTON G E,SRIVASTAVA N,KRIZHEVSKY A,et al.Improving neural networks by preventing co-adaptation of feature detectors[J].Computer Science,2012,3(4):212-223.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Understanding deep learning requires rethinking generalization">

                                <b>[13]</b> ZHANG Chiyuan,BENGIO S,HARDT M,et al.Understanding deep learning requires rethinking generalization[EB/OL].[2018-06-18].https://arxiv.org/pdf/1611.03530.pdf.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Noisy softmax:improving the generalization ability of DCNN via postponing the early softmax saturation">

                                <b>[14]</b> CHEN Binghui,DENG Weihong,DU Junping.Noisy softmax:improving the generalization ability of DCNN via postponing the early softmax saturation [EB/OL].[2018-06-18].https://arxiv.org/pdf/1708.03769.pdf.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_15" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJX201801011&amp;v=MjYzNDBMejdCZHJHNEg5bk1ybzlFWllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJ0RnkzaFZMdk4=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0V2Tk9CMmt5SXJncHRzMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[15]</b> 李小薪,梁荣华.有遮挡人脸识别综述:从子空间回归到深度学习[J].计算机学报,2018,41(1):177-207.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJC201910039" />
        <input id="dpi" type="hidden" value="600" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0V2Tk9CMmt5SXJncHRzMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201910039&amp;v=Mjg1MTM0OUdiWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUnRGeTNoVkx2Tkx6N0JiYkc0SDlqTnI=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0V2Tk9CMmt5SXJncHRzMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
