<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637128044653155000%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJC201909036%26RESULT%3d1%26SIGN%3dj8K2hXp%252b%252breaC129JLaQLAUoGoE%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201909036&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201909036&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201909036&amp;v=MDQxMzN5N2xWcnpQTHo3QmJiRzRIOWpNcG85R1lvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSckY=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2tGeFdRSFdwMmViYVMxWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#201" data-title="0 概述 ">0 概述</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#205" data-title="1 生成式对抗网络 ">1 生成式对抗网络</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#207" data-title="1.1 生成式对抗网络模型基本框架">1.1 生成式对抗网络模型基本框架</a></li>
                                                <li><a href="#211" data-title="1.2 理论公式">1.2 理论公式</a></li>
                                                <li><a href="#219" data-title="1.3 GAN的优势与不足">1.3 GAN的优势与不足</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#222" data-title="2 GAN的扩展与改进 ">2 GAN的扩展与改进</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#223" data-title="2.1 基于数据相似性度量改进">2.1 基于数据相似性度量改进</a></li>
                                                <li><a href="#239" data-title="2.2 基于模型框架和训练方法的改进">2.2 基于模型框架和训练方法的改进</a></li>
                                                <li><a href="#257" data-title="2.3 GAN与其他模型的结合">2.3 GAN与其他模型的结合</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#265" data-title="3 GAN应用 ">3 GAN应用</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#267" data-title="3.1 图像合成">3.1 图像合成</a></li>
                                                <li><a href="#269" data-title="3.2 风格迁移">3.2 风格迁移</a></li>
                                                <li><a href="#274" data-title="3.3 超分辨率图像">3.3 超分辨率图像</a></li>
                                                <li><a href="#278" data-title="3.4 视觉概念矢量运算">3.4 视觉概念矢量运算</a></li>
                                                <li><a href="#282" data-title="3.5 人脸修复与识别">3.5 人脸修复与识别</a></li>
                                                <li><a href="#287" data-title="3.6 图像标注">3.6 图像标注</a></li>
                                                <li><a href="#290" data-title="3.7 其他应用">3.7 其他应用</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#293" data-title="4 未来发展趋势 ">4 未来发展趋势</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#295" data-title="1)网络框架及算法的改进">1)网络框架及算法的改进</a></li>
                                                <li><a href="#297" data-title="2)与其他学习方法结合">2)与其他学习方法结合</a></li>
                                                <li><a href="#299" data-title="3)样本生成和数据增广">3)样本生成和数据增广</a></li>
                                                <li><a href="#301" data-title="4)评估方法的完善">4)评估方法的完善</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#303" data-title="5 结束语 ">5 结束语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#209" data-title="&lt;b&gt;图1 生成式对抗网络模型基本框架&lt;/b&gt;"><b>图1 生成式对抗网络模型基本框架</b></a></li>
                                                <li><a href="#234" data-title="&lt;b&gt;图2 WGAN在LSUN数据集中bedroom数据的实验结果&lt;/b&gt;"><b>图2 WGAN在LSUN数据集中bedroom数据的实验结果</b></a></li>
                                                <li><a href="#247" data-title="&lt;b&gt;图3 InfoGAN在MNIST数据集上的实验结果&lt;/b&gt;"><b>图3 InfoGAN在MNIST数据集上的实验结果</b></a></li>
                                                <li><a href="#273" data-title="&lt;b&gt;表1 基于Cityscapes数据集中不同模型的性能对比&lt;/b&gt;"><b>表1 基于Cityscapes数据集中不同模型的性能对比</b></a></li>
                                                <li><a href="#277" data-title="&lt;b&gt;表2 不同模型PSNR及SSIM的对比结果&lt;/b&gt;"><b>表2 不同模型PSNR及SSIM的对比结果</b></a></li>
                                                <li><a href="#289" data-title="&lt;b&gt;表3 GAN在5个应用领域的衍生模型及结构&lt;/b&gt;"><b>表3 GAN在5个应用领域的衍生模型及结构</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="404">


                                    <a id="bibliography_1" >
                                        <b>[1]</b>
                                     HINTON G E,SALAKHUTDINOV R R.Reducing the dimensionality of data with neural networks[J].Science,2006,313(5786):504-507.</a>
                                </li>
                                <li id="406">


                                    <a id="bibliography_2" title=" BENGIO Y.Learning deep architectures for AI[J].Foundations and Trends in Machine Learning,2009,2(1):1-127." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning deep architectures for AI">
                                        <b>[2]</b>
                                         BENGIO Y.Learning deep architectures for AI[J].Foundations and Trends in Machine Learning,2009,2(1):1-127.
                                    </a>
                                </li>
                                <li id="408">


                                    <a id="bibliography_3" title=" SCHMIDHUBER J.Deep learning in neural networks:an overview[J].Neural Networks,2015,61:85-117." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14110700296012&amp;v=MjEzMDc3b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VTHJJSmw0UmFSUT1OaWZPZmJLOEg5RE1xSTlGWnVJSkRIMA==&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2tGeFdRSFdwMmViYVMxWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         SCHMIDHUBER J.Deep learning in neural networks:an overview[J].Neural Networks,2015,61:85-117.
                                    </a>
                                </li>
                                <li id="410">


                                    <a id="bibliography_4" title=" HINTON G,DENG L,YU D,et al.Deep neural networks for acoustic modeling in speech recognition:the shared views of four research groups[J].IEEE Signal Processing Magazine,2012,29(6):82-97." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups">
                                        <b>[4]</b>
                                         HINTON G,DENG L,YU D,et al.Deep neural networks for acoustic modeling in speech recognition:the shared views of four research groups[J].IEEE Signal Processing Magazine,2012,29(6):82-97.
                                    </a>
                                </li>
                                <li id="412">


                                    <a id="bibliography_5" title=" KRIZHEVSKY A,SUTSKEVER I,HINTON G E.Imagenet classification with deep convolutional neural networks[C]//Proceedings of International Conference on Neural Information Processing Systems.Cambridge,USA:MIT Press,2012:1097-1105." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image Net classification with deep convolutional neural networks">
                                        <b>[5]</b>
                                         KRIZHEVSKY A,SUTSKEVER I,HINTON G E.Imagenet classification with deep convolutional neural networks[C]//Proceedings of International Conference on Neural Information Processing Systems.Cambridge,USA:MIT Press,2012:1097-1105.
                                    </a>
                                </li>
                                <li id="414">


                                    <a id="bibliography_6" >
                                        <b>[6]</b>
                                     GOODFELLOW I,POUGET-ABADIE J,MIRZA M,et al.Generative adversarial nets[C]//Proceedings of International Conference on Neural Information Processing Systems.Cambridge,USA:MIT Press,2014:2672-2680.</a>
                                </li>
                                <li id="416">


                                    <a id="bibliography_7" title=" KINGMA D P,WELLING M.Auto-encoding variational bayes[EB/OL].[2018-05-22].https://arxiv.org/pdf/1312.6114.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Auto-encoding variational Bayes">
                                        <b>[7]</b>
                                         KINGMA D P,WELLING M.Auto-encoding variational bayes[EB/OL].[2018-05-22].https://arxiv.org/pdf/1312.6114.pdf.
                                    </a>
                                </li>
                                <li id="418">


                                    <a id="bibliography_8" title=" 王坤峰,苟超,段艳杰,等.生成式对抗网络GAN的研究进展与展望[J].自动化学报,2017,43(3):321-332." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201703001&amp;v=MjU2MDVMT2VaZVJyRnk3bFZyek9LQ0xmWWJHNEg5Yk1ySTlGWllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2tGeFdRSFdwMmViYVMxWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                         王坤峰,苟超,段艳杰,等.生成式对抗网络GAN的研究进展与展望[J].自动化学报,2017,43(3):321-332.
                                    </a>
                                </li>
                                <li id="420">


                                    <a id="bibliography_9" title=" ARJOVSKY M,CHINTALA S,BOTTOU L.Wasserstein GAN[EB/OL].[2018-05-22].https://arxiv.org/pdf/1701.07875.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Wasserstein GAN">
                                        <b>[9]</b>
                                         ARJOVSKY M,CHINTALA S,BOTTOU L.Wasserstein GAN[EB/OL].[2018-05-22].https://arxiv.org/pdf/1701.07875.pdf.
                                    </a>
                                </li>
                                <li id="422">


                                    <a id="bibliography_10" title=" FUGLEDE B,TOPSOE F.Jensen-Shannon divergence and Hilbert space embedding[C]//Proceedings of International Symposium on Information Theory.Washington D.C.,USA:IEEE Press,2004:31." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Jensen-Shannon divergence and Hilbert space embedding">
                                        <b>[10]</b>
                                         FUGLEDE B,TOPSOE F.Jensen-Shannon divergence and Hilbert space embedding[C]//Proceedings of International Symposium on Information Theory.Washington D.C.,USA:IEEE Press,2004:31.
                                    </a>
                                </li>
                                <li id="424">


                                    <a id="bibliography_11" title=" GULRAJANI I,AHMED F,ARJOVSKY M,et al.Improved training of wasserstein GANs[EB/OL].[2018-05-22].https://arxiv.org/pdf/1704.00028.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Improved Training of Wasserstein GANs[C/OL]">
                                        <b>[11]</b>
                                         GULRAJANI I,AHMED F,ARJOVSKY M,et al.Improved training of wasserstein GANs[EB/OL].[2018-05-22].https://arxiv.org/pdf/1704.00028.pdf.
                                    </a>
                                </li>
                                <li id="426">


                                    <a id="bibliography_12" title=" NOWOZIN S,CSEKE B,TOMIOKA R.f-GAN:training generative neural samplers using variational divergence minimization[C]//Proceedings of International Conference on Neural Information Processing Systems.Cambridge,USA:MIT Press,2016:271-279." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=f-GAN:training generative neural samplers using variational divergence minimization">
                                        <b>[12]</b>
                                         NOWOZIN S,CSEKE B,TOMIOKA R.f-GAN:training generative neural samplers using variational divergence minimization[C]//Proceedings of International Conference on Neural Information Processing Systems.Cambridge,USA:MIT Press,2016:271-279.
                                    </a>
                                </li>
                                <li id="428">


                                    <a id="bibliography_13" title=" MAO Xudong,LI Qing,XIE Haoran,et al.Least squares generative adversarial networks[EB/OL].[2018-05-22].https://arxiv.org/pdf/1611.04076.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Least squares generative adversarial networks">
                                        <b>[13]</b>
                                         MAO Xudong,LI Qing,XIE Haoran,et al.Least squares generative adversarial networks[EB/OL].[2018-05-22].https://arxiv.org/pdf/1611.04076.pdf.
                                    </a>
                                </li>
                                <li id="430">


                                    <a id="bibliography_14" title=" QI Guojun.Loss-sensitive generative adversarial networks on lipschitz densities[EB/OL].[2018-05-22].https://arxiv.org/pdf/1701.06264.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Loss-sensitive generative adversarial networks on lipschitz densities">
                                        <b>[14]</b>
                                         QI Guojun.Loss-sensitive generative adversarial networks on lipschitz densities[EB/OL].[2018-05-22].https://arxiv.org/pdf/1701.06264.pdf.
                                    </a>
                                </li>
                                <li id="432">


                                    <a id="bibliography_15" title=" ZHAO Junbo,MATHIEU M,LECUN Y.Energy-based generative adversarial network[EB/OL].[2018-05-22].https://arxiv.org/pdf/1609.03126.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Energy-based generative adversarial network">
                                        <b>[15]</b>
                                         ZHAO Junbo,MATHIEU M,LECUN Y.Energy-based generative adversarial network[EB/OL].[2018-05-22].https://arxiv.org/pdf/1609.03126.pdf.
                                    </a>
                                </li>
                                <li id="434">


                                    <a id="bibliography_16" title=" DAI Zihang,ALMAHAIRI A,BACHMAN P,et al.Calibrating energy-based generative adversarial networks[EB/OL].[2018-05-22].https://arxiv.org/pdf/1702.01691.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Calibrating energy-based generative adversarial networks">
                                        <b>[16]</b>
                                         DAI Zihang,ALMAHAIRI A,BACHMAN P,et al.Calibrating energy-based generative adversarial networks[EB/OL].[2018-05-22].https://arxiv.org/pdf/1702.01691.pdf.
                                    </a>
                                </li>
                                <li id="436">


                                    <a id="bibliography_17" title=" WANG Dilin,LIU Qiang.Learning to draw samples:with application to amortized mle for generative adversarial learning[EB/OL].[2018-05-22].https://arxiv.org/pdf/1611.01722.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning to draw samples:with application to amortized mle for generative adversarial learning">
                                        <b>[17]</b>
                                         WANG Dilin,LIU Qiang.Learning to draw samples:with application to amortized mle for generative adversarial learning[EB/OL].[2018-05-22].https://arxiv.org/pdf/1611.01722.pdf.
                                    </a>
                                </li>
                                <li id="438">


                                    <a id="bibliography_18" title=" BERTHELOT D,SCHUMM T,METZ L.BEGAN:boundary equilibrium generative adversarial networks [EB/OL].[2018-05-22].https://arxiv.org/pdf/1703.10717.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=BEGAN:boundary equilibrium generative adversarial networks">
                                        <b>[18]</b>
                                         BERTHELOT D,SCHUMM T,METZ L.BEGAN:boundary equilibrium generative adversarial networks [EB/OL].[2018-05-22].https://arxiv.org/pdf/1703.10717.pdf.
                                    </a>
                                </li>
                                <li id="440">


                                    <a id="bibliography_19" title=" CHEN Xi,DUAN Yan,HOUTHOOFT R,et al.InfoGAN:interpretable representation learning by information maximizing generative adversarial nets[C]//Proceedings of International Conference on Neural Information Processing Systems.Cambridge,USA:MIT Press,2016:2172-2180." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Info GAN:Interpretable representation learning by information maximizing generative adversarial nets">
                                        <b>[19]</b>
                                         CHEN Xi,DUAN Yan,HOUTHOOFT R,et al.InfoGAN:interpretable representation learning by information maximizing generative adversarial nets[C]//Proceedings of International Conference on Neural Information Processing Systems.Cambridge,USA:MIT Press,2016:2172-2180.
                                    </a>
                                </li>
                                <li id="442">


                                    <a id="bibliography_20" title=" MAKHZANI A,FREY B J.PixelGAN autoencoders[C]//Proceedings of International Conference on Neural Information Processing Systems.Cambridge,USA:MIT Press,2017:1972-1982." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=PixelGAN autoencoders">
                                        <b>[20]</b>
                                         MAKHZANI A,FREY B J.PixelGAN autoencoders[C]//Proceedings of International Conference on Neural Information Processing Systems.Cambridge,USA:MIT Press,2017:1972-1982.
                                    </a>
                                </li>
                                <li id="444">


                                    <a id="bibliography_21" title=" MIRZA M,OSINDERO S.Conditional generative adversarial nets[EB/OL].[2018-05-22].https://arxiv.org/pdf/1411.1784.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Conditional generative adversarial nets">
                                        <b>[21]</b>
                                         MIRZA M,OSINDERO S.Conditional generative adversarial nets[EB/OL].[2018-05-22].https://arxiv.org/pdf/1411.1784.pdf.
                                    </a>
                                </li>
                                <li id="446">


                                    <a id="bibliography_22" title=" DENG Zhijie,ZHANG Hao,LIANG Xiaodan,et al.Structured generative adversarial networks[C]//Proceedings of International Conference in Neural Information Processing Systems.[S.l.]:Curran Associates Inc.,2017:3902-3912." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Structured generative adversarial networks">
                                        <b>[22]</b>
                                         DENG Zhijie,ZHANG Hao,LIANG Xiaodan,et al.Structured generative adversarial networks[C]//Proceedings of International Conference in Neural Information Processing Systems.[S.l.]:Curran Associates Inc.,2017:3902-3912.
                                    </a>
                                </li>
                                <li id="448">


                                    <a id="bibliography_23" title=" SPRINGENBERGJ T.Unsupervised and semi-supervised learning with categorical generative adversarial networks[EB/OL].[2018-05-22].https://arxiv.org/pdf/1511.06390.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Unsupervised and semi-supervised learning with categorical generative adversarial networks">
                                        <b>[23]</b>
                                         SPRINGENBERGJ T.Unsupervised and semi-supervised learning with categorical generative adversarial networks[EB/OL].[2018-05-22].https://arxiv.org/pdf/1511.06390.pdf.
                                    </a>
                                </li>
                                <li id="450">


                                    <a id="bibliography_24" title=" ZHANG Han,XU Tao,LI Hongsheng,et al.StackGAN:text to photo-realistic image synthesis with stacked generative adversarial networks[EB/OL].[2018-05-22].https://arxiv.org/pdf/1612.03242.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=StackGAN:Text to Photo-Realistic Image Synthesis with Stacked Generative Adversarial Networks[C/OL]">
                                        <b>[24]</b>
                                         ZHANG Han,XU Tao,LI Hongsheng,et al.StackGAN:text to photo-realistic image synthesis with stacked generative adversarial networks[EB/OL].[2018-05-22].https://arxiv.org/pdf/1612.03242.pdf.
                                    </a>
                                </li>
                                <li id="452">


                                    <a id="bibliography_25" title=" RADFORD A,METZ L,CHINTALA S.Unsupervised representation learning with deep convolutional generative adversarial networks[EB/OL].[2018-05-22].https://arxiv.org/pdf/1511.06434.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks">
                                        <b>[25]</b>
                                         RADFORD A,METZ L,CHINTALA S.Unsupervised representation learning with deep convolutional generative adversarial networks[EB/OL].[2018-05-22].https://arxiv.org/pdf/1511.06434.pdf.
                                    </a>
                                </li>
                                <li id="454">


                                    <a id="bibliography_26" >
                                        <b>[26]</b>
                                     LECUN Y,BOTTOU L,BENGIO Y,et al.Gradient-based learning applied to document recognition[J].Proceedings of the IEEE,1998,86(11):2278-2324.</a>
                                </li>
                                <li id="456">


                                    <a id="bibliography_27" title=" LECUN Y,BOSER B,DENKER J S,et al.Backpropagation applied to handwritten zip code recognition[J].Neural Computation,1989,1(4):541-551." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJBK&amp;filename=SJBK15090500012738&amp;v=MDk5NzdlWnVIeWptVUxySUpsNFJhUlE9TmlmSlpiSzlIdGpNcW85RlpPb05DMzh4b0JNVDZUNFBRSC9pclJkR2VycVFUTW53Wg==&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2tGeFdRSFdwMmViYVMxWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[27]</b>
                                         LECUN Y,BOSER B,DENKER J S,et al.Backpropagation applied to handwritten zip code recognition[J].Neural Computation,1989,1(4):541-551.
                                    </a>
                                </li>
                                <li id="458">


                                    <a id="bibliography_28" title=" YI Zili,ZHANG Hao,PING Tan,et al.DualGAN:unsupervised dual learning for image-to-image translation[EB/OL].[2018-05-22].https://arxiv.org/pdf/1704.02510.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=DualGAN:unsupervised dual learning for image-to-image translation">
                                        <b>[28]</b>
                                         YI Zili,ZHANG Hao,PING Tan,et al.DualGAN:unsupervised dual learning for image-to-image translation[EB/OL].[2018-05-22].https://arxiv.org/pdf/1704.02510.pdf.
                                    </a>
                                </li>
                                <li id="460">


                                    <a id="bibliography_29" title=" KIM T,CHA M,KIM H,et al.Learning to discover cross-domain relations with generative adversarial networks[C]//Proceedings of the 34th International Conference on Machine Learning.Berlin,Germany:Springer,2017:1857-1865." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning to discover cross-domain relations with generative adversarial networks">
                                        <b>[29]</b>
                                         KIM T,CHA M,KIM H,et al.Learning to discover cross-domain relations with generative adversarial networks[C]//Proceedings of the 34th International Conference on Machine Learning.Berlin,Germany:Springer,2017:1857-1865.
                                    </a>
                                </li>
                                <li id="462">


                                    <a id="bibliography_30" title=" ZHU Junyan,PARK T,ISOLA P,et al.Unpaired image-to-image translation using cycle-consistent adversarial networks[EB/OL].[2018-05-22].https://arxiv.org/pdf/1703.10593.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Unpaired image-to-image translation using cycle-consistent adversarial networks">
                                        <b>[30]</b>
                                         ZHU Junyan,PARK T,ISOLA P,et al.Unpaired image-to-image translation using cycle-consistent adversarial networks[EB/OL].[2018-05-22].https://arxiv.org/pdf/1703.10593.pdf.
                                    </a>
                                </li>
                                <li id="464">


                                    <a id="bibliography_31" title=" GAN Zhe,CHEN Liqun,WANG Weiyao,et al.Triangle generative adversarial networks[C]//Proceedings of International Conference on Neural Information Processing Systems.Cambridge,USA:MIT Press,2017:5253-5262." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Triangle generative adversarial networks">
                                        <b>[31]</b>
                                         GAN Zhe,CHEN Liqun,WANG Weiyao,et al.Triangle generative adversarial networks[C]//Proceedings of International Conference on Neural Information Processing Systems.Cambridge,USA:MIT Press,2017:5253-5262.
                                    </a>
                                </li>
                                <li id="466">


                                    <a id="bibliography_32" title=" NGUYEN T,LE T,VU H,et al.Dual discriminator generative adversarial nets[C]//Proceedings of International Conference on Neural Information Processing Systems.Cambridge,USA:MIT Press,2017:2667-2677." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Dual discriminator generative adversarial nets">
                                        <b>[32]</b>
                                         NGUYEN T,LE T,VU H,et al.Dual discriminator generative adversarial nets[C]//Proceedings of International Conference on Neural Information Processing Systems.Cambridge,USA:MIT Press,2017:2667-2677.
                                    </a>
                                </li>
                                <li id="468">


                                    <a id="bibliography_33" title=" LI Chongxuan,XU Tun,ZHU Jun,et al.Triple generative adversarial nets[C]//Proceedings of International Conference on Neural Information Processing Systems.Cambridge,USA:MIT Press,2017:4091-4101." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Triple generative adversarial nets">
                                        <b>[33]</b>
                                         LI Chongxuan,XU Tun,ZHU Jun,et al.Triple generative adversarial nets[C]//Proceedings of International Conference on Neural Information Processing Systems.Cambridge,USA:MIT Press,2017:4091-4101.
                                    </a>
                                </li>
                                <li id="470">


                                    <a id="bibliography_34" title=" CHAVDAROVA T,FLEURET F.SGAN:an alternative training of generative adversarial networks[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Press,2018:9407-9415" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=SGAN:an alternative training of generative adversarial networks">
                                        <b>[34]</b>
                                         CHAVDAROVA T,FLEURET F.SGAN:an alternative training of generative adversarial networks[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Press,2018:9407-9415
                                    </a>
                                </li>
                                <li id="472">


                                    <a id="bibliography_35" title=" DENTON E,CHINTALA S,FERGUS R.Deep generative image models using a Laplacian pyramid of adversarial networks[C]//Proceedings of International Conference on Neural Information Processing Systems.Cambridge,USA:MIT Press,2015:1486-1494." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep generative image models using a Laplacian pyramid of adversarial networks">
                                        <b>[35]</b>
                                         DENTON E,CHINTALA S,FERGUS R.Deep generative image models using a Laplacian pyramid of adversarial networks[C]//Proceedings of International Conference on Neural Information Processing Systems.Cambridge,USA:MIT Press,2015:1486-1494.
                                    </a>
                                </li>
                                <li id="474">


                                    <a id="bibliography_36" title=" IM D J,KIM C D,JIANG Hui,et al.Generating images with recurrent adversarial networks[EB/OL].[2018-05-22].https://arxiv.org/pdf/1602.05110.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Generating images with recurrent adversarial networks">
                                        <b>[36]</b>
                                         IM D J,KIM C D,JIANG Hui,et al.Generating images with recurrent adversarial networks[EB/OL].[2018-05-22].https://arxiv.org/pdf/1602.05110.pdf.
                                    </a>
                                </li>
                                <li id="476">


                                    <a id="bibliography_37" title=" GREGOR K,DANIHELKA I,GRAVES A,et al.DRAW:a recurrent neural network for image generation[C]//Proceedings of International Conference on International Conference on Machine Learning.[S.l.]:JMLR.org,2015:1462-1471." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=DRAW:a recurrent neural network for image generation">
                                        <b>[37]</b>
                                         GREGOR K,DANIHELKA I,GRAVES A,et al.DRAW:a recurrent neural network for image generation[C]//Proceedings of International Conference on International Conference on Machine Learning.[S.l.]:JMLR.org,2015:1462-1471.
                                    </a>
                                </li>
                                <li id="478">


                                    <a id="bibliography_38" title=" SALIMANS T,GOODFELLOW I,ZAREMBA W,et al.Improved techniques for training GANs[C]//Proceedings of International Conference on Neural Information Processing Systems.Cambridge,USA:MIT Press,2016:2234-2242." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Improved techniques for training GANs">
                                        <b>[38]</b>
                                         SALIMANS T,GOODFELLOW I,ZAREMBA W,et al.Improved techniques for training GANs[C]//Proceedings of International Conference on Neural Information Processing Systems.Cambridge,USA:MIT Press,2016:2234-2242.
                                    </a>
                                </li>
                                <li id="480">


                                    <a id="bibliography_39" title=" LARSEN A B L,S∅NDERBY S K,LAROCHELLE H,et al.Autoencoding beyond pixels using a learned similarity metric[C]//Proceedings of International Conference on Machine Learning.[S.l.]:JMLR.org,2016:1558-1566." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Autoencoding beyond pixels using a learned similarity metric">
                                        <b>[39]</b>
                                         LARSEN A B L,S∅NDERBY S K,LAROCHELLE H,et al.Autoencoding beyond pixels using a learned similarity metric[C]//Proceedings of International Conference on Machine Learning.[S.l.]:JMLR.org,2016:1558-1566.
                                    </a>
                                </li>
                                <li id="482">


                                    <a id="bibliography_40" title=" MESCHEDER L,NOWOZIN S,GEIGER A.Adversarial variational Bayes:unifying variational autoencoders and generative adversarial networks[C]//Proceedings of International Conference on International Conference on Machine Learning.[S.l.]:JMLR.org,2017:2391-2400." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Adversarial variational Bayes:unifying variational autoencoders and generative adversarial networks">
                                        <b>[40]</b>
                                         MESCHEDER L,NOWOZIN S,GEIGER A.Adversarial variational Bayes:unifying variational autoencoders and generative adversarial networks[C]//Proceedings of International Conference on International Conference on Machine Learning.[S.l.]:JMLR.org,2017:2391-2400.
                                    </a>
                                </li>
                                <li id="484">


                                    <a id="bibliography_41" title=" SALIMANS T,KARPATHY A,CHEN Xi,et al.PixelCNN++:improving the pixelCNN with discretized logistic mixture likelihood and other modifications[EB/OL].[2018-05-22].https://arxiv.org/pdf/1701.05517.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=PixelCNN++:improving the pixelCNN with discretized logistic mixture likelihood and other modifications">
                                        <b>[41]</b>
                                         SALIMANS T,KARPATHY A,CHEN Xi,et al.PixelCNN++:improving the pixelCNN with discretized logistic mixture likelihood and other modifications[EB/OL].[2018-05-22].https://arxiv.org/pdf/1701.05517.pdf.
                                    </a>
                                </li>
                                <li id="486">


                                    <a id="bibliography_42" title=" OORDA V D,KALCHBRENNER N,KAVUKCUOGLU K.Pixel recurrent neural networks[C]//Proceedings of International Conference on International Conference on Machine Learning.[S.l.]:JMLR.org,2016:1747-1756." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Pixel recurrent neural networks">
                                        <b>[42]</b>
                                         OORDA V D,KALCHBRENNER N,KAVUKCUOGLU K.Pixel recurrent neural networks[C]//Proceedings of International Conference on International Conference on Machine Learning.[S.l.]:JMLR.org,2016:1747-1756.
                                    </a>
                                </li>
                                <li id="488">


                                    <a id="bibliography_43" title=" DOSOVITSKIY A,BROX T.Generating images with perceptual similarity metrics based on deep networks[C]//Proceedings of International Conference on Neural Information Processing Systems.Cambridge,USA:MIT Press,2016:658-666." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Generating Images with Perceptual Similarity Metrics Based on Deep Networks">
                                        <b>[43]</b>
                                         DOSOVITSKIY A,BROX T.Generating images with perceptual similarity metrics based on deep networks[C]//Proceedings of International Conference on Neural Information Processing Systems.Cambridge,USA:MIT Press,2016:658-666.
                                    </a>
                                </li>
                                <li id="490">


                                    <a id="bibliography_44" title=" GRAVES A.Generating sequences with recurrent neural networks[EB/OL].[2018-05-22].https://arxiv.org/pdf/1308.0850.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Generating sequences with recurrent neural networks">
                                        <b>[44]</b>
                                         GRAVES A.Generating sequences with recurrent neural networks[EB/OL].[2018-05-22].https://arxiv.org/pdf/1308.0850.pdf.
                                    </a>
                                </li>
                                <li id="492">


                                    <a id="bibliography_45" title=" LEE H Y,TSENG B H,WEN T H,et al.Personalizing recurrent-neural-network-based language model by social network[J].IEEE/ACM Transactions on Audio,Speech,and Language Processing,2017,25(3):519-530." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM9EE1B363A4382B80977C8D6DEFBB8DD7&amp;v=MTMyNzJ4eFlhN1RnT1FBdmsyR2REQzhDY01jNllDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZDcGJRMzVOMWh3cmk2dzY0PU5pZklZN3JOYTlDK3JJbEdGZThNQkg1TA==&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2tGeFdRSFdwMmViYVMxWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[45]</b>
                                         LEE H Y,TSENG B H,WEN T H,et al.Personalizing recurrent-neural-network-based language model by social network[J].IEEE/ACM Transactions on Audio,Speech,and Language Processing,2017,25(3):519-530.
                                    </a>
                                </li>
                                <li id="494">


                                    <a id="bibliography_46" title=" MOGRENO.C-RNN-GAN:continuous recurrent neural networks with adversarial training[EB/OL].[2018-05-22].https://arxiv.org/pdf/1611.09904.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=C-RNN-GAN:continuous recurrent neural networks with adversarial training">
                                        <b>[46]</b>
                                         MOGRENO.C-RNN-GAN:continuous recurrent neural networks with adversarial training[EB/OL].[2018-05-22].https://arxiv.org/pdf/1611.09904.pdf.
                                    </a>
                                </li>
                                <li id="496">


                                    <a id="bibliography_47" title=" YU Lantao,ZHANG Weinan,WANG Jun,et al.SeqGAN:sequence generative adversarial nets with policy gradient[C]//Proceedings of Conference on Artificial Intelligence.[S.l.]:AAAI Press,2017:2852-2858." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=SeqGAN:sequence generative adversarial nets with policy gradient">
                                        <b>[47]</b>
                                         YU Lantao,ZHANG Weinan,WANG Jun,et al.SeqGAN:sequence generative adversarial nets with policy gradient[C]//Proceedings of Conference on Artificial Intelligence.[S.l.]:AAAI Press,2017:2852-2858.
                                    </a>
                                </li>
                                <li id="498">


                                    <a id="bibliography_48" title=" HOCHREITER S,SCHMIDHUBER J.Long short-term memory[J].Neural Computation,1997,9(8):1735-1780." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJBK&amp;filename=SJBK15090500014198&amp;v=MTEwMDF5am1VTHJJSmw0UmFSUT1OaWZKWmJLOUh0ak1xbzlGWk9vTERYVXhvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SA==&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2tGeFdRSFdwMmViYVMxWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[48]</b>
                                         HOCHREITER S,SCHMIDHUBER J.Long short-term memory[J].Neural Computation,1997,9(8):1735-1780.
                                    </a>
                                </li>
                                <li id="500">


                                    <a id="bibliography_49" title=" SAK H,SENIOR A,BEAUFAYS F.Long short-term memory based recurrent neural network architectures for large vocabulary speech recognition[EB/OL].[2018-05-22].https://arxiv.org/pdf/1402.1128v1.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Long short-term memory based recurrent neural network architectures for large vocabulary speech recognition">
                                        <b>[49]</b>
                                         SAK H,SENIOR A,BEAUFAYS F.Long short-term memory based recurrent neural network architectures for large vocabulary speech recognition[EB/OL].[2018-05-22].https://arxiv.org/pdf/1402.1128v1.pdf.
                                    </a>
                                </li>
                                <li id="502">


                                    <a id="bibliography_50" title=" KWAK H,ZHANG B T.Generating images part by part with composite generative adversarial networks[EB/OL].[2018-05-22].https://arxiv.org/pdf/1607.05387.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Generating images part by part with composite generative adversarial networks">
                                        <b>[50]</b>
                                         KWAK H,ZHANG B T.Generating images part by part with composite generative adversarial networks[EB/OL].[2018-05-22].https://arxiv.org/pdf/1607.05387.pdf.
                                    </a>
                                </li>
                                <li id="504">


                                    <a id="bibliography_51" title=" YANG Jianwei,KANNAN A,BATRA D,et al.LR-GAN:Layered recursive generative adversarial networks for image generation[EB/OL].[2018-05-22].https://arxiv.org/pdf/1703.01560.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=LR-GAN:Layered recursive generative adversarial networks for image generation">
                                        <b>[51]</b>
                                         YANG Jianwei,KANNAN A,BATRA D,et al.LR-GAN:Layered recursive generative adversarial networks for image generation[EB/OL].[2018-05-22].https://arxiv.org/pdf/1703.01560.pdf.
                                    </a>
                                </li>
                                <li id="506">


                                    <a id="bibliography_52" title=" GATYS L A,ECKER A S,BETHGE M.A neural algorithm of artistic style[EB/OL].[2018-05-22].https://arxiv.org/pdf/1508.06576.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A neural algorithm of artistic style">
                                        <b>[52]</b>
                                         GATYS L A,ECKER A S,BETHGE M.A neural algorithm of artistic style[EB/OL].[2018-05-22].https://arxiv.org/pdf/1508.06576.pdf.
                                    </a>
                                </li>
                                <li id="508">


                                    <a id="bibliography_53" title=" JOHNSON J,ALAHI A,LI Feifei.Perceptual losses for real-time style transfer and super-resolution[C]//Proceedings of European Conference on Computer Vision.Berlin,Germany:Springer,2016:694-711." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Perceptual losses for real-time style transfer and superresolution">
                                        <b>[53]</b>
                                         JOHNSON J,ALAHI A,LI Feifei.Perceptual losses for real-time style transfer and super-resolution[C]//Proceedings of European Conference on Computer Vision.Berlin,Germany:Springer,2016:694-711.
                                    </a>
                                </li>
                                <li id="510">


                                    <a id="bibliography_54" title=" BOUSMALIS K,SILBERMAN N,DOHAN D,et al.Unsupervised pixel-level domain adaptation with generative adversarial networks[EB/OL].[2018-05-22].https://arxiv.org/pdf/1612.05424.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Unsupervised pixel-level domain adaptation with generative adversarial networks">
                                        <b>[54]</b>
                                         BOUSMALIS K,SILBERMAN N,DOHAN D,et al.Unsupervised pixel-level domain adaptation with generative adversarial networks[EB/OL].[2018-05-22].https://arxiv.org/pdf/1612.05424.pdf.
                                    </a>
                                </li>
                                <li id="512">


                                    <a id="bibliography_55" title=" LIU Mingyu,BREUEL T,KAUTZ J.Unsupervised image-to-image translation networks[C]//Proceedings of International Conference on Neural Information Processing Systems.Cambridge,USA:MIT Press,2017:700-708." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Unsupervised image-to-image translation networks">
                                        <b>[55]</b>
                                         LIU Mingyu,BREUEL T,KAUTZ J.Unsupervised image-to-image translation networks[C]//Proceedings of International Conference on Neural Information Processing Systems.Cambridge,USA:MIT Press,2017:700-708.
                                    </a>
                                </li>
                                <li id="514">


                                    <a id="bibliography_56" title=" SHRIVASTAVA A,PFISTER T,TUZEL O,et al.Learning from simulated and unsupervised images through adversarial training[EB/OL].[2018-05-22].https://arxiv.org/pdf/1612.07828.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning from simulated and unsupervised images through adversarial training">
                                        <b>[56]</b>
                                         SHRIVASTAVA A,PFISTER T,TUZEL O,et al.Learning from simulated and unsupervised images through adversarial training[EB/OL].[2018-05-22].https://arxiv.org/pdf/1612.07828.pdf.
                                    </a>
                                </li>
                                <li id="516">


                                    <a id="bibliography_57" title=" ISOLA P,ZHU Junyan,ZHOU Tinghui,et al.Image-to-image translation with conditional adversarial networks[EB/OL].[2018-05-22].https://arxiv.org/pdf/1611.07004.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image-to-Image Translation with Conditional Adversarial Networks">
                                        <b>[57]</b>
                                         ISOLA P,ZHU Junyan,ZHOU Tinghui,et al.Image-to-image translation with conditional adversarial networks[EB/OL].[2018-05-22].https://arxiv.org/pdf/1611.07004.pdf.
                                    </a>
                                </li>
                                <li id="518">


                                    <a id="bibliography_58" title=" LIU Mingyu,TUZEL O.Coupled generative adversarial networks[C]//Proceedings of International Conference on Neural Information Processing Systems.Cambridge,USA:MIT Press,2016:469-477." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Coupled generative adversarial networks">
                                        <b>[58]</b>
                                         LIU Mingyu,TUZEL O.Coupled generative adversarial networks[C]//Proceedings of International Conference on Neural Information Processing Systems.Cambridge,USA:MIT Press,2016:469-477.
                                    </a>
                                </li>
                                <li id="520">


                                    <a id="bibliography_59" title=" LEDIG C,THEIS L,HUSZ&#193;R F,et al.Photo-realistic single image super-resolution using a generative adversarial network[EB/OL].[2018-05-22].https://arxiv.org/pdf/1609.04802v1.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network[C/OL]">
                                        <b>[59]</b>
                                         LEDIG C,THEIS L,HUSZ&#193;R F,et al.Photo-realistic single image super-resolution using a generative adversarial network[EB/OL].[2018-05-22].https://arxiv.org/pdf/1609.04802v1.pdf.
                                    </a>
                                </li>
                                <li id="522">


                                    <a id="bibliography_60" title=" HE Kaiming,ZHANG Xiangyu,REN Shaoqing,et al.Deep residual learning for image recognition[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Press,2016:770-778." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep Residual Learning for Image Recognition">
                                        <b>[60]</b>
                                         HE Kaiming,ZHANG Xiangyu,REN Shaoqing,et al.Deep residual learning for image recognition[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Press,2016:770-778.
                                    </a>
                                </li>
                                <li id="524">


                                    <a id="bibliography_61" title=" BOER J F D,CENSE B,PARK B H,et al.Improved signal-to-noise ratio in spectral-domain compared with time-domain optical coherence tomography[J].Optics Letters,2003,28(21):2067-2069." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Improved signal-to-noise ratio in spectral-domain compared with time-domain optical coherence tomography">
                                        <b>[61]</b>
                                         BOER J F D,CENSE B,PARK B H,et al.Improved signal-to-noise ratio in spectral-domain compared with time-domain optical coherence tomography[J].Optics Letters,2003,28(21):2067-2069.
                                    </a>
                                </li>
                                <li id="526">


                                    <a id="bibliography_62" title=" ARDENKJ&#198;R-LARSEN J H,FRIDLUND B,GRAM A,et al.Increase in signal-to-noise ratio of &amp;gt; 10,000 times in liquid-state NMR[J].Proceedings of the National Academy of Sciences,2003,100(18):10158-10163." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Increase in signal-to-noise ratio of &amp;gt; 10,000 times in liquid-state NMR">
                                        <b>[62]</b>
                                         ARDENKJ&#198;R-LARSEN J H,FRIDLUND B,GRAM A,et al.Increase in signal-to-noise ratio of &amp;gt; 10,000 times in liquid-state NMR[J].Proceedings of the National Academy of Sciences,2003,100(18):10158-10163.
                                    </a>
                                </li>
                                <li id="528">


                                    <a id="bibliography_63" title=" SIMONYAN K,ZISSERMAN A.Very deep convolutional networks for large-scale image recognition[EB/OL].[2018-05-22].https://arxiv.org/pdf/1409.1556.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Very deep convolutional networks for large-scale image recognition">
                                        <b>[63]</b>
                                         SIMONYAN K,ZISSERMAN A.Very deep convolutional networks for large-scale image recognition[EB/OL].[2018-05-22].https://arxiv.org/pdf/1409.1556.pdf.
                                    </a>
                                </li>
                                <li id="530">


                                    <a id="bibliography_64" title=" ZHANG Jun,YAN Yong,LADES M.Face recognition:eigenface,elastic matching,and neural nets[J].Proceedings of the IEEE,1997,85(9):1423-1435." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Face recognition: Eigenface, elastic matching, and neural nets">
                                        <b>[64]</b>
                                         ZHANG Jun,YAN Yong,LADES M.Face recognition:eigenface,elastic matching,and neural nets[J].Proceedings of the IEEE,1997,85(9):1423-1435.
                                    </a>
                                </li>
                                <li id="532">


                                    <a id="bibliography_65" title=" CHOPRA S,HADSELL R,LECUN Y.Learning a similarity metric discriminatively,with application to face verification[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Press,2005:539-546." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning a Similarity Metric Discriminatively, with Application to Face verification">
                                        <b>[65]</b>
                                         CHOPRA S,HADSELL R,LECUN Y.Learning a similarity metric discriminatively,with application to face verification[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Press,2005:539-546.
                                    </a>
                                </li>
                                <li id="534">


                                    <a id="bibliography_66" title=" HUANG Rui,ZHANG Shu,LI Tianyu,et al.Beyond face rotation:global and local perception GAN for photorealistic and identity preserving frontal view synthesis[EB/OL].[2018-05-22].https://arxiv.org/pdf/1704.04086.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Beyond face rotation:global and local perception GAN for photorealistic and identity preserving frontal view synthesis">
                                        <b>[66]</b>
                                         HUANG Rui,ZHANG Shu,LI Tianyu,et al.Beyond face rotation:global and local perception GAN for photorealistic and identity preserving frontal view synthesis[EB/OL].[2018-05-22].https://arxiv.org/pdf/1704.04086.pdf.
                                    </a>
                                </li>
                                <li id="536">


                                    <a id="bibliography_67" title=" 姚乃明,郭清沛,乔逢春,等.基于生成式对抗网络的鲁棒人脸表情识别[J].自动化学报,2018,44(5):865-877." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201805010&amp;v=MjAzNjFIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJyRnk3bFZyek9LQ0xmWWJHNEg5bk1xbzlFWklRS0Q=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2tGeFdRSFdwMmViYVMxWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[67]</b>
                                         姚乃明,郭清沛,乔逢春,等.基于生成式对抗网络的鲁棒人脸表情识别[J].自动化学报,2018,44(5):865-877.
                                    </a>
                                </li>
                                <li id="538">


                                    <a id="bibliography_68" title=" LI Yijun,LIU Sifei,YANG Jimei,et al.Generative face completion[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Press,2017:3911-3919." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Generative Face Completion">
                                        <b>[68]</b>
                                         LI Yijun,LIU Sifei,YANG Jimei,et al.Generative face completion[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Press,2017:3911-3919.
                                    </a>
                                </li>
                                <li id="540">


                                    <a id="bibliography_69" title=" KARPATHY A,LI Feifei.Deep visual-semantic alignments for generating image descriptions[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Press,2015:3128-3137." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep visual-semantic alignments for generating image descriptions">
                                        <b>[69]</b>
                                         KARPATHY A,LI Feifei.Deep visual-semantic alignments for generating image descriptions[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Press,2015:3128-3137.
                                    </a>
                                </li>
                                <li id="542">


                                    <a id="bibliography_70" title=" REED S,AKATA Z,YAN Xinchen,et al.Generative adversarial text to image synthesis[C]//Proceedings of International Conference on International Conference on Machine Learning.[S.l.]:JMLR.org,2016:1060-1069." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Generative adversarial text to image synthesis">
                                        <b>[70]</b>
                                         REED S,AKATA Z,YAN Xinchen,et al.Generative adversarial text to image synthesis[C]//Proceedings of International Conference on International Conference on Machine Learning.[S.l.]:JMLR.org,2016:1060-1069.
                                    </a>
                                </li>
                                <li id="544">


                                    <a id="bibliography_71" title=" XU Tao,ZHANG Pengchuan,HUANG Qiuyuan,et al.AttnGAN:fine-grained text to image generation with attentional generative adversarial networks[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Press,2018:1316-1324." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=AttnGAN:fine-grained text to image generation with attentional generative adversarial networks">
                                        <b>[71]</b>
                                         XU Tao,ZHANG Pengchuan,HUANG Qiuyuan,et al.AttnGAN:fine-grained text to image generation with attentional generative adversarial networks[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Press,2018:1316-1324.
                                    </a>
                                </li>
                                <li id="546">


                                    <a id="bibliography_72" title=" CHEN Yang,LAI Yukun,LIU Yongjin.CartoonGAN:generative adversarial networks for photo cartoonization[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Press,2018:9465-9474." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=CartoonGAN:Generative Adversarial Networks for Photo Cartoonization">
                                        <b>[72]</b>
                                         CHEN Yang,LAI Yukun,LIU Yongjin.CartoonGAN:generative adversarial networks for photo cartoonization[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Press,2018:9465-9474.
                                    </a>
                                </li>
                                <li id="548">


                                    <a id="bibliography_73" title=" PERARNAU G,WEIJER J V,RADUCANU B,et al.Invertible conditional GANs for image editing[EB/OL].[2018-05-22].https://arxiv.org/pdf/1611.06355.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Invertible conditional GANs for image editing">
                                        <b>[73]</b>
                                         PERARNAU G,WEIJER J V,RADUCANU B,et al.Invertible conditional GANs for image editing[EB/OL].[2018-05-22].https://arxiv.org/pdf/1611.06355.pdf.
                                    </a>
                                </li>
                                <li id="550">


                                    <a id="bibliography_74" title=" KOSSAIFI J,TRAN L,PANAGAKIS Y,et al.GAGAN:geometry-aware generative adverserial networks[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Press,2018:878-887." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=GAGAN:geometry-aware generative adverserial networks">
                                        <b>[74]</b>
                                         KOSSAIFI J,TRAN L,PANAGAKIS Y,et al.GAGAN:geometry-aware generative adverserial networks[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Press,2018:878-887.
                                    </a>
                                </li>
                                <li id="552">


                                    <a id="bibliography_75" title=" MA Shuang,FU Jianlong,CHEN Changwei,et al.DA-GAN:instance-level image translation by deep attention generative adversarial networks[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Press,2018:5657-5666." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=DA-GAN:instancelevel image translation by deep attention generative adversarial networks">
                                        <b>[75]</b>
                                         MA Shuang,FU Jianlong,CHEN Changwei,et al.DA-GAN:instance-level image translation by deep attention generative adversarial networks[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Press,2018:5657-5666.
                                    </a>
                                </li>
                                <li id="554">


                                    <a id="bibliography_76" title=" WANG Zongwei,TANG Xu,LUO Weixin,et al.Face aging with identity-preserved conditional generative adversarial networks[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Press,2018:7939-7947." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Face aging with identity-preserved conditional generative adversarial networks">
                                        <b>[76]</b>
                                         WANG Zongwei,TANG Xu,LUO Weixin,et al.Face aging with identity-preserved conditional generative adversarial networks[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Press,2018:7939-7947.
                                    </a>
                                </li>
                                <li id="556">


                                    <a id="bibliography_77" title=" CHOI Y,CHOI M,KIM M,et al.StarGAN:unified generative adversarial networks for multi-domain image-to-image translation[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Press,2018:8789-8797." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=StarGAN:Unified generative adversarial networks for multi-domain image-to-image translation">
                                        <b>[77]</b>
                                         CHOI Y,CHOI M,KIM M,et al.StarGAN:unified generative adversarial networks for multi-domain image-to-image translation[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Press,2018:8789-8797.
                                    </a>
                                </li>
                                <li id="558">


                                    <a id="bibliography_78" title=" REED S E,AKATA Z,MOHAN S,et al.Learning what and where to draw[C]//Proceedings of International Conference on Neural Information Processing Systems.Cambridge,USA:MIT Press,2016:217-225." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning what and where to draw">
                                        <b>[78]</b>
                                         REED S E,AKATA Z,MOHAN S,et al.Learning what and where to draw[C]//Proceedings of International Conference on Neural Information Processing Systems.Cambridge,USA:MIT Press,2016:217-225.
                                    </a>
                                </li>
                                <li id="560">


                                    <a id="bibliography_79" title=" BAI Yancheng,ZHANG Yongqiang,DING Mingli,et al.Finding tiny faces in the wild with generative adversarial network[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Press,2018:21-30." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Finding tiny faces in the wild with generative adversarial network">
                                        <b>[79]</b>
                                         BAI Yancheng,ZHANG Yongqiang,DING Mingli,et al.Finding tiny faces in the wild with generative adversarial network[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Press,2018:21-30.
                                    </a>
                                </li>
                                <li id="562">


                                    <a id="bibliography_80" title=" WANG Jifeng,LI Xiang,HUI Le,et al.Stacked conditional generative adversarial networks for jointly learning shadow detection and shadow removal[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Press,2018:1788-1797." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Stacked conditional generative adversarial networks for jointly learning shadow detection and shadow removal">
                                        <b>[80]</b>
                                         WANG Jifeng,LI Xiang,HUI Le,et al.Stacked conditional generative adversarial networks for jointly learning shadow detection and shadow removal[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Press,2018:1788-1797.
                                    </a>
                                </li>
                                <li id="564">


                                    <a id="bibliography_81" title=" QIAN Rui,TAN R T,YANG Wenhan,et al.Attentive generative adversarial network for raindrop removal from a single image[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Press,2018:2482-2491." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Attentive generative adversarial network for raindrop removal from a single image">
                                        <b>[81]</b>
                                         QIAN Rui,TAN R T,YANG Wenhan,et al.Attentive generative adversarial network for raindrop removal from a single image[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Press,2018:2482-2491.
                                    </a>
                                </li>
                                <li id="566">


                                    <a id="bibliography_82" title=" CHEN J,CHEN J,CHAO H,et al.Image blind denoising with generative adversarial network based noise modeling[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Press,2018:3155-3164." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image blind denoising with generative adversarial network based noise modeling">
                                        <b>[82]</b>
                                         CHEN J,CHEN J,CHAO H,et al.Image blind denoising with generative adversarial network based noise modeling[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Press,2018:3155-3164.
                                    </a>
                                </li>
                                <li id="568">


                                    <a id="bibliography_83" title=" LI Ruide,PAN Jinshan,LI Zechao,et al.Single image dehazing via conditional generative adversarial network[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Computer Society,2018:8202-8211." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Single image dehazing via conditional generative adversarial network">
                                        <b>[83]</b>
                                         LI Ruide,PAN Jinshan,LI Zechao,et al.Single image dehazing via conditional generative adversarial network[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Computer Society,2018:8202-8211.
                                    </a>
                                </li>
                                <li id="570">


                                    <a id="bibliography_84" title=" NGUYEN V,VICENTE T F Y,ZHAO Maozheng,et al.Shadow detection with conditional generative adversarial networks[C]//Proceedings of IEEE International Conference on Computer Vision.Washington D.C.,USA:IEEE Press,2017:4520-4528." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Shadow detection with conditional generative adversarial networks">
                                        <b>[84]</b>
                                         NGUYEN V,VICENTE T F Y,ZHAO Maozheng,et al.Shadow detection with conditional generative adversarial networks[C]//Proceedings of IEEE International Conference on Computer Vision.Washington D.C.,USA:IEEE Press,2017:4520-4528.
                                    </a>
                                </li>
                                <li id="572">


                                    <a id="bibliography_85" title=" VONDRICK C,PIRSIAVASH H,TORRALBA A.Generating videos with scene dynamics[C]//Proceedings of International Conference on Neural Information Processing Systems.Cambridge,USA:MIT Press,2016:613-621." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Generating videos with scene dynamics">
                                        <b>[85]</b>
                                         VONDRICK C,PIRSIAVASH H,TORRALBA A.Generating videos with scene dynamics[C]//Proceedings of International Conference on Neural Information Processing Systems.Cambridge,USA:MIT Press,2016:613-621.
                                    </a>
                                </li>
                                <li id="574">


                                    <a id="bibliography_86" title=" BERNHARD K,HUANG Zhiwu,DANDA P P,et al.Improving video generation for multi-functional applications[EB/OL].[2018-05-22].https://arxiv.org/pdf/1711.11453v2.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Improving video generation for multi-functional applications">
                                        <b>[86]</b>
                                         BERNHARD K,HUANG Zhiwu,DANDA P P,et al.Improving video generation for multi-functional applications[EB/OL].[2018-05-22].https://arxiv.org/pdf/1711.11453v2.pdf.
                                    </a>
                                </li>
                                <li id="576">


                                    <a id="bibliography_87" title=" SAITO M,MATSUMOTO E,SAITO S.Temporal generative adversarial nets with singular value clipping[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Press,2017:2830-2839." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Temporal generative adversarial nets with singular value clipping">
                                        <b>[87]</b>
                                         SAITO M,MATSUMOTO E,SAITO S.Temporal generative adversarial nets with singular value clipping[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Press,2017:2830-2839.
                                    </a>
                                </li>
                                <li id="578">


                                    <a id="bibliography_88" title=" TULYAKOV S,LIU Mingyu,YANG Xiaodong,et al.MoCoGAN:decomposing motion and content for video generation[EB/OL].[2018-05-22].https://arxiv.org/pdf/1707.04993.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=MoCoGAN:decomposing motion and content for video generation">
                                        <b>[88]</b>
                                         TULYAKOV S,LIU Mingyu,YANG Xiaodong,et al.MoCoGAN:decomposing motion and content for video generation[EB/OL].[2018-05-22].https://arxiv.org/pdf/1707.04993.pdf.
                                    </a>
                                </li>
                                <li id="580">


                                    <a id="bibliography_89" title=" 孔德江,汤斯亮,吴飞.时空嵌入式生成对抗网络的地点预测方法[J].模式识别与人工智能,2018,31(1):49-60." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201801007&amp;v=MjMxNTFyRnk3bFZyek9LRDdZYkxHNEg5bk1ybzlGWTRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVI=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2tGeFdRSFdwMmViYVMxWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[89]</b>
                                         孔德江,汤斯亮,吴飞.时空嵌入式生成对抗网络的地点预测方法[J].模式识别与人工智能,2018,31(1):49-60.
                                    </a>
                                </li>
                                <li id="582">


                                    <a id="bibliography_90" title=" XIONG Wei,LUO Wenhan,MA Lin,et al.Learning to generate time-lapse videos using multi-stage dynamic generative adversarial networks[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Press,2018:2364-2373." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning to generate time-lapse videos using multi-stage dynamic generative adversarial networks">
                                        <b>[90]</b>
                                         XIONG Wei,LUO Wenhan,MA Lin,et al.Learning to generate time-lapse videos using multi-stage dynamic generative adversarial networks[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Press,2018:2364-2373.
                                    </a>
                                </li>
                                <li id="584">


                                    <a id="bibliography_91" title=" GUPTA A,JOHNSON J,LI Feifei,et al.Social GAN:socially acceptable trajectories with generative adversarial networks[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Press,2018:2255-2264." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Social GAN:socially acceptable trajectories with generative adversarial networks">
                                        <b>[91]</b>
                                         GUPTA A,JOHNSON J,LI Feifei,et al.Social GAN:socially acceptable trajectories with generative adversarial networks[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Press,2018:2255-2264.
                                    </a>
                                </li>
                                <li id="586">


                                    <a id="bibliography_92" title=" WANG Weiyue,HUANG Qiangui,YOU Suya,et al.Shape inpainting using 3D generative adversarial network and recurrent convolutional networks[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Press,2017:2298-2306." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Shape inpainting using 3D generative adversarial network and recurrent convolutional networks">
                                        <b>[92]</b>
                                         WANG Weiyue,HUANG Qiangui,YOU Suya,et al.Shape inpainting using 3D generative adversarial network and recurrent convolutional networks[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Press,2017:2298-2306.
                                    </a>
                                </li>
                                <li id="588">


                                    <a id="bibliography_93" title=" WU Jiajun,ZHANG Chengkai,XUE Tianfan,et al.Learning a probabilistic latent space of object shapes via 3d generative-adversarial modeling[C]//Proceedings of International Conference on Neural Information Processing Systems.Cambridge,USA:MIT Press,2016:82-90." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Learning a probabilistic latent space of object shapes via 3d generative-adversarial modeling,&amp;quot;">
                                        <b>[93]</b>
                                         WU Jiajun,ZHANG Chengkai,XUE Tianfan,et al.Learning a probabilistic latent space of object shapes via 3d generative-adversarial modeling[C]//Proceedings of International Conference on Neural Information Processing Systems.Cambridge,USA:MIT Press,2016:82-90.
                                    </a>
                                </li>
                                <li id="590">


                                    <a id="bibliography_94" title=" SAGE A,AGUSTSSON E,TIMOFTE R,et al.Logo synthesis and manipulation with clustered generative adversarial networks[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Press,2018:5879-5888." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Logo synthesis and manipulation with clustered generative adversarial networks">
                                        <b>[94]</b>
                                         SAGE A,AGUSTSSON E,TIMOFTE R,et al.Logo synthesis and manipulation with clustered generative adversarial networks[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Press,2018:5879-5888.
                                    </a>
                                </li>
                                <li id="592">


                                    <a id="bibliography_95" title=" DOLHANSKY B,FERRER C C.Eye in-painting with exemplar generative adversarial networks[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Press,2018:5657-5666." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Eye in-painting with exemplar generative adversarial networks">
                                        <b>[95]</b>
                                         DOLHANSKY B,FERRER C C.Eye in-painting with exemplar generative adversarial networks[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Press,2018:5657-5666.
                                    </a>
                                </li>
                                <li id="594">


                                    <a id="bibliography_96" title=" HAUSMAN K,CHEBOTAR Y,SCHAAL S,et al.Multi-modal imitation learning from unstructured demonstrations using generative adversarial nets[EB/OL].[2018-05-22].https://arxiv.org/pdf/1705.10479.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multi-modal imitation learning from unstructured demonstrations using generative adversarial nets">
                                        <b>[96]</b>
                                         HAUSMAN K,CHEBOTAR Y,SCHAAL S,et al.Multi-modal imitation learning from unstructured demonstrations using generative adversarial nets[EB/OL].[2018-05-22].https://arxiv.org/pdf/1705.10479.pdf.
                                    </a>
                                </li>
                                <li id="596">


                                    <a id="bibliography_97" title=" BARAM N,ANSCHEL O,CASPI I,et al.End-to-end differentiable adversarial imitation learning[C]//Proceedings of International Conference on International Conference on Machine Learning.[S.l.]:JMLR.org,2017:390-399." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=End-to-end differentiable adversarial imitation learning">
                                        <b>[97]</b>
                                         BARAM N,ANSCHEL O,CASPI I,et al.End-to-end differentiable adversarial imitation learning[C]//Proceedings of International Conference on International Conference on Machine Learning.[S.l.]:JMLR.org,2017:390-399.
                                    </a>
                                </li>
                                <li id="598">


                                    <a id="bibliography_98" title=" ZHANG Yizhe,GAN Zhe,FAN Kai,et al.Adversarial feature matching for text generation[C]//Proceedings of International Conference on International Conference on Machine Learning.[S.l.]:JMLR.org,2017:4006-4015." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Adversarial feature matching for text generation">
                                        <b>[98]</b>
                                         ZHANG Yizhe,GAN Zhe,FAN Kai,et al.Adversarial feature matching for text generation[C]//Proceedings of International Conference on International Conference on Machine Learning.[S.l.]:JMLR.org,2017:4006-4015.
                                    </a>
                                </li>
                                <li id="600">


                                    <a id="bibliography_99" title=" MAKHZANI A,SHLENS J,JAITLY N,et al.Adversarial autoencoders[EB/OL].[2018-05-22].https://arxiv.org/pdf/1511.05644.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Adversarial autoencoders">
                                        <b>[99]</b>
                                         MAKHZANI A,SHLENS J,JAITLY N,et al.Adversarial autoencoders[EB/OL].[2018-05-22].https://arxiv.org/pdf/1511.05644.pdf.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJC" target="_blank">计算机工程</a>
                2019,45(09),222-234 DOI:10.19678/j.issn.1000-3428.0051964            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>生成式对抗网络研究与应用进展</b></span>
 <span class="shoufa"></span>                                     </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9F%B4%E6%A2%A6%E5%A9%B7&amp;code=37493409&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2tGeFdRSFdwMmViYVMxWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">柴梦婷</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9C%B1%E8%BF%9C%E5%B9%B3&amp;code=30891109&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2tGeFdRSFdwMmViYVMxWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">朱远平</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%A4%A9%E6%B4%A5%E5%B8%88%E8%8C%83%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%B8%8E%E4%BF%A1%E6%81%AF%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0150718&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2tGeFdRSFdwMmViYVMxWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">天津师范大学计算机与信息工程学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>基于零和博弈思想的生成式对抗网络(GAN)可通过无监督学习获得数据的分布,并生成较逼真的数据。基于GAN的基础概念及理论框架,研究各类GAN模型及其在特定领域的应用情况,从数据相似性度量、模型框架、训练方法3个方面进行分析,对GAN改进与扩展的相关研究成果进行总结,并从图像合成、风格迁移等应用领域展开讨论,归纳出GAN的优势与不足,同时对其应用前景进行展望。分析结果表明,GAN的学习能力与可塑性强,改进潜力大,应用范围广,但其发展面临的挑战是训练过程不稳定,且缺乏生成数据质量的客观评价标准。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%94%9F%E6%88%90%E5%BC%8F%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2tGeFdRSFdwMmViYVMxWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">生成式对抗网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%94%9F%E6%88%90%E5%BC%8F%E6%A8%A1%E5%9E%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2tGeFdRSFdwMmViYVMxWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">生成式模型;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%AF%B9%E6%8A%97%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2tGeFdRSFdwMmViYVMxWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">对抗学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2tGeFdRSFdwMmViYVMxWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2tGeFdRSFdwMmViYVMxWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">人工智能;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    柴梦婷(1992—),女,硕士研究生,主研方向为图像处理、模式识别;;
                                </span>
                                <span>
                                    朱远平,教授、博士。E-mail:zhuyuanping@ tjnu. edu. cn;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-06-29</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金(61602345,61703306);</span>
                                <span>天津市科技计划项目(14RCGFGX00847);</span>
                    </p>
            </div>
                    <h1><b>Research and Application Progress of Generative Adversarial Networks</b></h1>
                    <h2>
                    <span>CHAI Mengting</span>
                    <span>ZHU Yuanping</span>
            </h2>
                    <h2>
                    <span>College of Computer and Information Engineering,Tianjin Normal University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>The Generative Adversarial Networks(GAN) based on the zero-sum game idea can obtain the distribution of data through unsupervised learning and generate more realistic data.Based on the basic concepts and theoretical framework of the generated confrontation network,the GAN models and the application results in specific fields are studied,and the data similarity measure,model framework and training method are summarized.The research and related research results of the improvement and expansion are analyzed,and the practical application fields such as image synthesis and style migration are discussed.The advantages and disadvantages of GAN are summarized,and the application prospects are prospected.Analysis results show that the GAN has strong learning ability and plasticity,great potential for improvement and wide application range.However,its development challenges are unstable training process and lack of objective evaluation criteria for generating data quality.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Generative%20Adversarial%20Networks%20(GAN)&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2tGeFdRSFdwMmViYVMxWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Generative Adversarial Networks (GAN);</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=generative%20model&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2tGeFdRSFdwMmViYVMxWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">generative model;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=adversarial%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2tGeFdRSFdwMmViYVMxWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">adversarial learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=deep%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2tGeFdRSFdwMmViYVMxWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">deep learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=artificial%20intelligence&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2tGeFdRSFdwMmViYVMxWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">artificial intelligence;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-06-29</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="201" name="201" class="anchor-tag">0 概述</h3>
                <div class="p1">
                    <p id="202">深度学习<citation id="605" type="reference"><link href="404" rel="bibliography" /><link href="406" rel="bibliography" /><link href="408" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>,<a class="sup">3</a>]</sup></citation>作为机器学习的一个分支,是一种以人工神经网络为架构,对大规模数据进行表征学习的算法。自2012年以来,深度神经网络(Deep Neural Networks,DNN)在语音识别<citation id="602" type="reference"><link href="410" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>和图像识别<citation id="603" type="reference"><link href="412" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>领域被广泛应用,推动了人工智能技术的发展。深度神经网络模型及深度学习模型的提出,也推动了深度学习的全面发展。如文献<citation id="604" type="reference">[<a class="sup">6</a>]</citation>提出一种生成式对抗网络(Generative Adversarial Networks,GAN)模型,其包括生成式和判别式2种模型。生成式模型是指可以生成数据分布的模型,主要有朴素贝叶斯、K近邻(KNN)以及混合高斯模型等。生成式模型的学习和计算过程复杂,且一个模型只对应一类数据。而判别式模型由于结构简单灵活,适用于多类别识别。GAN使得生成式模型受到研究者的关注,其可以被用来生成图像、文字和语音等样本,且效果逼真。</p>
                </div>
                <div class="p1">
                    <p id="203">由于GAN可以生成随机样本,并通过模型训练和学习得到与真实分布相似的数据,受到学者的广泛关注。同时,研究表明,GAN的思想与VAE(Variational Auto-Encoder)<citation id="606" type="reference"><link href="416" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>相似,但实际上两者在数据的相似性度量方式和学习方式上有较大的不同,VAE模型性能直接依赖预先假设的损失函数和近似分布,预先定义需要一定的经验作基础,因此最终生成的概率分布会受到已定义参数的影响。而GAN的训练过程是无监督学习,其原则上可以逼近任意概率分布,且不需要预先定义参数。GAN在许多领域都有发展,获得较多优秀的研究成果。其中,图像处理仍是GAN研究和应用较广泛的领域之一<citation id="607" type="reference"><link href="418" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>。GAN使生成式模型成为人工智能技术的热门研究方向。</p>
                </div>
                <div class="p1">
                    <p id="204">本文介绍原始GAN模型框架及原理,归纳和分析改进GAN模型,在此基础上,给出GAN的优势与不足,并对GAN未来发展方向进行展望。</p>
                </div>
                <h3 id="205" name="205" class="anchor-tag">1 生成式对抗网络</h3>
                <div class="p1">
                    <p id="206">GAN由2个网络构成:生成器和判别器。这2个网络在同一时间进行训练,并在极小极大游戏中相互博弈。在训练过程中,生成器捕获真实样本的数据分布,从而将样本数据送入判别器,判别器则估计该样本是来自于训练数据还是生成数据,以保证不被生成器判断错误。最终两者会达到一个平衡,即生成器生成以假乱真的数据样本,判别器输出则逼近一个固定的概率值。</p>
                </div>
                <h4 class="anchor-tag" id="207" name="207">1.1 生成式对抗网络模型基本框架</h4>
                <div class="p1">
                    <p id="208">图1所示为生成式对抗网络模型基本框架,该框架由2个模型构成:生成器<i>G</i>和判别器<i>D</i>,这2个模型本质都是函数,在GAN定义中通常为多层感知器(Multi-Layer Perception,MLP)。GAN采用神经网络模型,可以看出,首先将一个100维的随机噪声<i>z</i>输入生成器,在构成生成器的多层感知器或神经网络中该噪声映射到一个新的数据分布,得到<i>G</i>(<i>z</i>),将真实数据分布<i>x</i>与<i>G</i>(<i>z</i>)共同输入到判别器中,判别器对于输入的2个数据分别做出判断,输出一个概率值,真实数据的概率值接近1,生成数据的概率值接近0,这说明此时生成器生成的数据置信度不高,判别器会将生成数据中需要调整的参数反馈给生成器,生成器在接收到调整的梯度信号后进行调整重新生成。如此循环,当判别器无法区分真实数据和生成数据时,就可以认为生成器此时达到最优。这个不断训练的过程,就是生成器<i>G</i>与判别器<i>D</i>的极大极小博弈过程,<i>D</i>和<i>G</i>的性能在迭代中不断被提高,直到最终<i>D</i>(<i>G</i>(<i>z</i>))与<i>D</i>(<i>x</i>)一致,此时<i>G</i>和<i>D</i>均已达到最优。</p>
                </div>
                <div class="area_img" id="209">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201909036_209.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2tGeFdRSFdwMmViYVMxWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 生成式对抗网络模型基本框架" src="Detail/GetImg?filename=images/JSJC201909036_209.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2tGeFdRSFdwMmViYVMxWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图1 生成式对抗网络模型基本框架</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201909036_209.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2tGeFdRSFdwMmViYVMxWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="210">原始GAN是基于无监督学习的生成模型,其输入服从随机噪声分布,期望分布则服从近似但不完全等同于真实数据样本的分布,保证了生成样本的多样性。</p>
                </div>
                <h4 class="anchor-tag" id="211" name="211">1.2 理论公式</h4>
                <div class="p1">
                    <p id="212">文献<citation id="608" type="reference">[<a class="sup">6</a>]</citation>提出极大极小的相互博弈公式如下:</p>
                </div>
                <div class="p1">
                    <p id="213" class="code-formula">
                        <mathml id="213"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mi>G</mi></munder><mspace width="0.25em" /><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mi>D</mi></munder><mspace width="0.25em" /><mi>V</mi><mo stretchy="false">(</mo><mi>D</mi><mo>,</mo><mi>G</mi><mo stretchy="false">)</mo><mo>=</mo><mi>E</mi><msub><mrow></mrow><mrow><mi>x</mi><mo>∼</mo><mi>p</mi><msub><mrow></mrow><mrow><mtext>d</mtext><mtext>a</mtext><mtext>t</mtext><mtext>a</mtext></mrow></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></msub><mo stretchy="false">[</mo><mrow><mi>lg</mi></mrow><mspace width="0.25em" /><mi>D</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mo>+</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mi>E</mi><msub><mrow></mrow><mrow><mi>z</mi><mo>∼</mo><mi>p</mi><msub><mrow></mrow><mi>z</mi></msub><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo></mrow></msub><mo stretchy="false">[</mo><mrow><mi>lg</mi></mrow><mo stretchy="false">(</mo><mi>D</mi><mo stretchy="false">(</mo><mi>G</mi><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="214">由式(1)可以看出,该公式在<i>D</i>(<i>x</i>)和<i>D</i>(<i>G</i>(<i>z</i>))的基础上加入求对数和求期望值的操作。对数求解是统计学中常用的方法,目的是降低不可预期的干扰噪声对数据分布的影响和缓解数据分布偏差问题。期望值的计算是使最终生成的数据分布<i>p</i><sub><i>g</i></sub>(<i>G</i>(<i>z</i>))与真实数据分布<i>p</i><sub>data</sub>(<i>x</i>)一致,即希望生成数据尽量地拟合真实数据但不是生成真实数据本身,这样才能保证<i>G</i>产生出的数据既与真实数据有一定相似性,同时又不同于真实数据。</p>
                </div>
                <div class="p1">
                    <p id="215">在式(1)中,右侧第1项是真实数据分布<i>p</i><sub>data</sub>(<i>x</i>)的数据通过判别器的熵,判别器的目的是将其最大化为1;第2项是来自随机噪声输入<i>P</i><sub><i>z</i></sub>(<i>z</i>)的数据通过生成器的熵。生成器生成的样本通过判别器判定其真伪,判别器则尝试将其最大化为0。总体而言,判别器的目标是最大化函数<i>V</i>(<i>D</i>,<i>G</i>),而生成器的任务完全相反,其试图最小化函数<i>V</i>(<i>D</i>,<i>G</i>),使真实数据和生成数据之间的差距减小。</p>
                </div>
                <div class="p1">
                    <p id="216">关于如何同时优化<i>D</i>和<i>G</i>,文献<citation id="609" type="reference">[<a class="sup">6</a>]</citation>只考虑<i>D</i>达到最优,使<i>G</i>生成的数据过拟合,为在迭代过程中每一次迭代的<i>D</i>都可以给出当前状态下的最优值,提出在更新多次<i>D</i>后更新一次<i>G</i>,可以保证<i>D</i>优先<i>G</i>一步达到当前最优,如此循环迭代直到两者共同达到最优,即<i>p</i><sub><i>g</i></sub>=<i>p</i><sub>data</sub>,对于最终结果的判定,给出一个当<i>G</i>固定时的最优判别器公式,如式(2)所示。</p>
                </div>
                <div class="p1">
                    <p id="217" class="code-formula">
                        <mathml id="217"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>D</mi><msubsup><mrow></mrow><mi>G</mi><mo>*</mo></msubsup><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><mi>p</mi><msub><mrow></mrow><mrow><mtext>d</mtext><mtext>a</mtext><mtext>t</mtext><mtext>a</mtext></mrow></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><mrow><mi>p</mi><msub><mrow></mrow><mrow><mtext>d</mtext><mtext>a</mtext><mtext>t</mtext><mtext>a</mtext></mrow></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>+</mo><mi>p</mi><msub><mrow></mrow><mi>g</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="218">当<i>p</i><sub><i>g</i></sub>=<i>p</i><sub>data</sub>时,<mathml id="305"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>D</mi><msubsup><mrow></mrow><mi>G</mi><mo>*</mo></msubsup><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><mn>2</mn></mfrac></mrow></math></mathml>,表示此时<i>D</i>和<i>G</i>达到最优。</p>
                </div>
                <h4 class="anchor-tag" id="219" name="219">1.3 GAN的优势与不足</h4>
                <div class="p1">
                    <p id="220">在文献<citation id="610" type="reference">[<a class="sup">6</a>]</citation>提出的原始生成式对抗网络中,为使GAN具有无限建模能力,其输入是一个仅为100维的随机噪声,不需要附加条件或限制,生成结果可以是任何数据分布,即可以拟合所有分布。GAN的模型框架与算法较简单,将生成器与判别器的对抗作为训练准则,使用反向传播算法(Back Propagation,BP)进行训练,在训练过程中不需要使用效率较低的马尔科夫链方法和各种近似推理过程,这些优点可降低生成式模型训练的难度,并提高训练的效率。</p>
                </div>
                <div class="p1">
                    <p id="221">GAN的建模能力存在诸多问题,由于对数据没有任何假设,训练过程不可控,生成结果过于自由,易出现训练过程不稳定、无法收敛和模型崩溃的问题,导致生成的样本过拟合。同时GAN对于损失函数的设定也存在问题,由于生成样本与真实数据本质上存在较大的维度差,使得损失函数难以进行计算,进而导致生成器无法调整,使最终的生成结果较难逼近真实样本。</p>
                </div>
                <h3 id="222" name="222" class="anchor-tag">2 GAN的扩展与改进</h3>
                <h4 class="anchor-tag" id="223" name="223">2.1 基于数据相似性度量改进</h4>
                <h4 class="anchor-tag" id="224" name="224">2.1.1 WGAN模型</h4>
                <div class="p1">
                    <p id="225">文献<citation id="611" type="reference">[<a class="sup">9</a>]</citation>利用数据相似性度量对GAN进行改进,提出WGAN(Wasserstein GAN)模型,将原始GAN中概率分布的距离度量公式替换为Wasserstein距离,并对GAN算法进行部分调整,优化了原始GAN训练过程不稳定(需要小心平衡判别器和生成器的训练程度)、训练后期生成器梯度消失、模型崩溃的问题。</p>
                </div>
                <div class="p1">
                    <p id="226">根据文献<citation id="612" type="reference">[<a class="sup">9</a>]</citation>的GAN计算公式,有:</p>
                </div>
                <div class="p1">
                    <p id="227" class="code-formula">
                        <mathml id="227"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>C</mi><mo stretchy="false">(</mo><mi>G</mi><mo stretchy="false">)</mo><mo>=</mo><mo>-</mo><mrow><mi>lg</mi></mrow><mspace width="0.25em" /><mn>4</mn><mo>+</mo><mi>Κ</mi><mi>L</mi><mrow><mo>(</mo><mrow><mi>p</mi><msub><mrow></mrow><mrow><mtext>d</mtext><mtext>a</mtext><mtext>t</mtext><mtext>a</mtext></mrow></msub><mo stretchy="false">∥</mo><mfrac><mrow><mi>p</mi><msub><mrow></mrow><mrow><mtext>d</mtext><mtext>a</mtext><mtext>t</mtext><mtext>a</mtext></mrow></msub><mo>+</mo><mi>p</mi><msub><mrow></mrow><mi>g</mi></msub></mrow><mn>2</mn></mfrac></mrow><mo>)</mo></mrow><mo>+</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mi>Κ</mi><mi>L</mi><mrow><mo>(</mo><mrow><mi>p</mi><msub><mrow></mrow><mi>g</mi></msub><mo stretchy="false">∥</mo><mfrac><mrow><mi>p</mi><msub><mrow></mrow><mrow><mtext>d</mtext><mtext>a</mtext><mtext>t</mtext><mtext>a</mtext></mrow></msub><mo>+</mo><mi>p</mi><msub><mrow></mrow><mi>g</mi></msub></mrow><mn>2</mn></mfrac></mrow><mo>)</mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>3</mn><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mi>J</mi><mi>S</mi><mrow><mo>(</mo><mrow><mi>p</mi><msub><mrow></mrow><mrow><mtext>d</mtext><mtext>a</mtext><mtext>t</mtext><mtext>a</mtext></mrow></msub><mo stretchy="false">∥</mo><mi>p</mi><msub><mrow></mrow><mi>g</mi></msub></mrow><mo>)</mo></mrow><mo>=</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mi>Κ</mi><mi>L</mi><mrow><mo>(</mo><mrow><mi>p</mi><msub><mrow></mrow><mrow><mtext>d</mtext><mtext>a</mtext><mtext>t</mtext><mtext>a</mtext></mrow></msub><mo stretchy="false">∥</mo><mfrac><mrow><mi>p</mi><msub><mrow></mrow><mrow><mtext>d</mtext><mtext>a</mtext><mtext>t</mtext><mtext>a</mtext></mrow></msub><mo>+</mo><mi>p</mi><msub><mrow></mrow><mi>g</mi></msub></mrow><mn>2</mn></mfrac></mrow><mo>)</mo></mrow><mo>+</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mi>Κ</mi><mi>L</mi><mrow><mo>(</mo><mrow><mi>p</mi><msub><mrow></mrow><mi>g</mi></msub><mo stretchy="false">∥</mo><mfrac><mrow><mi>p</mi><msub><mrow></mrow><mrow><mtext>d</mtext><mtext>a</mtext><mtext>t</mtext><mtext>a</mtext></mrow></msub><mo>+</mo><mi>p</mi><msub><mrow></mrow><mi>g</mi></msub></mrow><mn>2</mn></mfrac></mrow><mo>)</mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>4</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="228">可以得出:</p>
                </div>
                <div class="p1">
                    <p id="229"><i>C</i>(<i>G</i>)=-lg 4+2·<i>JS</i>(<i>p</i><sub>data</sub>‖<i>p</i><sub><i>g</i></sub>)      (5)</p>
                </div>
                <div class="p1">
                    <p id="230">根据式(5),GAN模型要计算JS<citation id="613" type="reference"><link href="422" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>散度,前提是<i>p</i><sub>data</sub>和<i>p</i><sub><i>g</i></sub>这2个分布有重叠。但低维与高维之间完全没有重叠或有微小到可忽略的重叠,这种可能性非常大。因此,生成器的损失为一个常量(-lg 4)而非线性变化,无法继续做出调整,也就无法使生成的样本不断逼近真实样本。因此,使用式(6)的Wasserstein距离代替JS散度。</p>
                </div>
                <div class="p1">
                    <p id="231" class="code-formula">
                        <mathml id="231"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>W</mi><mo stretchy="false">(</mo><mi>Ρ</mi><msub><mrow></mrow><mi>r</mi></msub><mo>,</mo><mi>Ρ</mi><msub><mrow></mrow><mi>g</mi></msub><mo stretchy="false">)</mo><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>inf</mi></mrow></mstyle><mrow><mtext>ϒ</mtext><mo>∈</mo><mstyle displaystyle="true"><mo>∏</mo><mo stretchy="false">(</mo></mstyle><mi>Ρ</mi><msub><mrow></mrow><mi>r</mi></msub><mo>,</mo><mi>Ρ</mi><msub><mrow></mrow><mi>g</mi></msub><mo stretchy="false">)</mo></mrow></munder><mi>E</mi><msub><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>∼</mo><mi>y</mi></mrow></msub><mrow><mo>[</mo><mrow><mo stretchy="false">∥</mo><mi>x</mi><mo>-</mo><mi>y</mi><mo stretchy="false">∥</mo></mrow><mo>]</mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>6</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="232">Wasserstein距离相比KL散度、JS散度的优越性在于即使2个分布没有重叠,Wasserstein距离仍然能够反映其远近,可同时优化GAN原始模型训练过程不稳定和进程指标不恰当的问题。</p>
                </div>
                <div class="p1">
                    <p id="233">为更直观验证WGAN生成逼真图像的能力,本文使用与文献<citation id="614" type="reference">[<a class="sup">9</a>]</citation>中相同的数据集进行训练。图2给出WGAN训练LSUN数据集中的bedroom数据迭代10 000次和30 000次的实验结果。为更直接观察样本生成的过程,选取3张生成样本在迭代10 000次(第1行)和迭代30 000次(第2行)时的效果对比,如图2(c)所示。可以看出,迭代30 000次生成结果较逼真,但放大之后发现样本局部仍与真实效果不符,这与模型在运行过程中出现的梯度消失问题有关。</p>
                </div>
                <div class="area_img" id="234">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201909036_234.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2tGeFdRSFdwMmViYVMxWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 WGAN在LSUN数据集中bedroom数据的实验结果" src="Detail/GetImg?filename=images/JSJC201909036_234.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2tGeFdRSFdwMmViYVMxWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图2 WGAN在LSUN数据集中bedroom数据的实验结果</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201909036_234.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2tGeFdRSFdwMmViYVMxWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="235">文献<citation id="615" type="reference">[<a class="sup">9</a>]</citation>的WGAN存在梯度消失问题,在WGAN中加入梯度惩罚(Gradient Penalty,GP),即WGAN-GP,使梯度在传播过程中保持平稳,同时该模型还改进了对整个分布进行参数限制的方式,只针对真假样本以及两者之间的过渡部分进行限制,不仅能够提升训练速度,还使生成结果变得多样化<citation id="616" type="reference"><link href="424" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="236">与WGAN相类似,各种方法被提出,例如:在f-GAN中引入f-divergence度量样本之间距离<citation id="617" type="reference"><link href="426" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>,在LSGAN(Least Squares GAN)中引入Pearson卡方散度对2个分布进行相似性度量<citation id="618" type="reference"><link href="428" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>,LS-GAN( Loss Sensitive GAN )<sup></sup><citation id="619" type="reference"><link href="430" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>对生成样本中距离真实数据分布较远的样本进行重点优化,即“按需分配”思想,EBGAN(Energy-based GAN)<citation id="621" type="reference"><link href="432" rel="bibliography" /><link href="434" rel="bibliography" /><link href="436" rel="bibliography" /><sup>[<a class="sup">15</a>,<a class="sup">16</a>,<a class="sup">17</a>]</sup></citation>是从能量模型出发,以能量值作为衡量标准,BEGAN(Boundary Equilibrium GAN)则是在EBGAN和WGAN的基础上,提出边界均衡GAN架构,使用标准的训练步骤实现快速、稳定的收敛<citation id="620" type="reference"><link href="438" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>。</p>
                </div>
                <h4 class="anchor-tag" id="237" name="237">2.1.2 EBGAN模型</h4>
                <div class="p1">
                    <p id="238">与WGAN基于样本间的距离度量不同,EBGAN<citation id="622" type="reference"><link href="432" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>是从能量模型的角度对GAN进行改进。该模型将判别器看作一个能量函数,在真实数据范围内该能量函数的能量值会减小,在非真实数据(即认为是生成数据)范围内能量值会增大。这样生成器的目标就是产生能量值足够小的样本,而判别器则是以对生成的样本赋高的能量值以及对真实样本赋低的能量值为目标。EBGAN的意义是给予GAN一种不同于其他模型通过距离度量定义损失函数的能量模型的定义,即用更宽泛的结构和更多样的损失函数类型来训练GAN模型。</p>
                </div>
                <h4 class="anchor-tag" id="239" name="239">2.2 基于模型框架和训练方法的改进</h4>
                <h4 class="anchor-tag" id="240" name="240">2.2.1 InfoGAN模型</h4>
                <div class="p1">
                    <p id="241">InfoGAN(Information Maximizing Generative Adversarial Nets)模型改进了GAN的输入信息,使得GAN 的输入不再只是语义模糊的随机噪声,实现对随机噪声与生成图像特征之间对应关系建模,并且可以通过控制相应维度的变量来控制生成的样本产生相应的变化,同时该模型还提出一个约束函数,称为互信息,可以保证附加的输入信息较好地被利用<citation id="623" type="reference"><link href="440" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>,对GAN公式进行修改,如式(7)所示。</p>
                </div>
                <div class="p1">
                    <p id="242" class="code-formula">
                        <mathml id="242"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mi>G</mi></munder><mspace width="0.25em" /><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mi>D</mi></munder><mi>V</mi><msub><mrow></mrow><mi>Ι</mi></msub><mo>=</mo><mi>V</mi><mrow><mo>(</mo><mrow><mi>D</mi><mo>,</mo><mi>G</mi></mrow><mo>)</mo></mrow><mo>-</mo><mi>λ</mi><mi>Ι</mi><mrow><mo>(</mo><mrow><mi>c</mi><mo>;</mo><mi>G</mi><mrow><mo>(</mo><mrow><mi>z</mi><mo>,</mo><mi>c</mi></mrow><mo>)</mo></mrow></mrow><mo>)</mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>7</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="243">其中,<i>c</i>为具有隐含意义并且可解释的附加信息,而剩下的无法明确描述的信息由<i>z</i>提供,这样使生成的结果具有一定的可控性,同时为保证隐变量<i>c</i>被有效使用,提出互信息<i>I</i>,其计算公式如下:</p>
                </div>
                <div class="p1">
                    <p id="244" class="code-formula">
                        <mathml id="244"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ι</mi><mrow><mo>(</mo><mrow><mi>X</mi><mo>;</mo><mi>Y</mi></mrow><mo>)</mo></mrow><mo>=</mo><mi>Η</mi><mrow><mo>(</mo><mi>X</mi><mo>)</mo></mrow><mo>-</mo><mi>Η</mi><mrow><mo>(</mo><mrow><mi>X</mi><mo stretchy="false">|</mo><mi>Y</mi></mrow><mo>)</mo></mrow><mo>=</mo><mi>Η</mi><mrow><mo>(</mo><mi>Y</mi><mo>)</mo></mrow><mo>-</mo><mi>Η</mi><mrow><mo>(</mo><mrow><mi>Y</mi><mo stretchy="false">|</mo><mi>X</mi></mrow><mo>)</mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>8</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="245">对于式(8)中的<i>X</i>和<i>Y</i>,两者之间有较高的互信息才能确保彼此有联系,因此优化的过程就是互信息升高的过程。</p>
                </div>
                <div class="p1">
                    <p id="246">将InfoGAN真正用于训练时,发现其给出的训练代码目前只有基于MNIST数据集。图3为由一个10维的离散码、2个1维的连续码以及62维的噪声变量控制的InfoGAN模型训练50个epoch的实验结果。其中,图3(a)为离散码控制的数字分类结果,图3(b)和图3(c)为2个连续码控制数字旋转和宽窄的结果。与文献<citation id="624" type="reference">[<a class="sup">19</a>]</citation>的实验结果不同,本文给出的代码无法准确控制数字的单一属性。为使数字旋转效果更明显,文献<citation id="625" type="reference">[<a class="sup">19</a>]</citation>将连续码范围设置为(-2,2),而目前给出的代码只支持1维的连续码。因此实验未达到预期结果,数字在旋转的同时宽窄也在改变。</p>
                </div>
                <div class="area_img" id="247">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201909036_247.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2tGeFdRSFdwMmViYVMxWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 InfoGAN在MNIST数据集上的实验结果" src="Detail/GetImg?filename=images/JSJC201909036_247.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2tGeFdRSFdwMmViYVMxWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图3 InfoGAN在MNIST数据集上的实验结果</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201909036_247.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2tGeFdRSFdwMmViYVMxWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="248">对于InfoGAN,包括其提出的互信息公式不仅可应用于生成样本与隐变量之间的计算,也可与VAE<citation id="626" type="reference"><link href="416" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>模型结合,这样原始信息的处理就不仅是对真实数据的一个编码解码再输入的过程,而且可以在原始信息的基础上加入具有隐含意义的条件<i>c</i>,在过程中利用互信息进行约束后再输入,得到期望的样本。</p>
                </div>
                <div class="p1">
                    <p id="249">与InfoGAN相类似的还有PixelGAN Autoencoders<citation id="627" type="reference"><link href="442" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>、CGAN (Conditional Generative Adversarial Nets)<citation id="628" type="reference"><link href="444" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>、Structured GAN<citation id="629" type="reference"><link href="446" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>以及Cat GAN (Categorical Generative Adversarial Networks)<citation id="630" type="reference"><link href="448" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>,同样是在原始随机噪声的基础上加入条件变量或是在条件变量的基础上加入数据熵判断。文献<citation id="631" type="reference">[<a class="sup">20</a>]</citation>以潜在代码为条件,通过无监督的学习方式分离图像的样式和内容信息。在CGAN中条件变量多为类别标签信息,相当于将原始的无监督GAN 变为有监督的模型,除利用数据集中的标签生成指定类别的图像,文献<citation id="632" type="reference">[<a class="sup">21</a>]</citation>将CGAN用于图像自动标注的应用,在MIR Flickr25000数据集上,以图像特征为条件变量,生成该图像的标注。与CGAN密切相关的StackGAN<citation id="633" type="reference"><link href="450" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>其本质就是CGAN的堆叠,第1层生成一个低分辨率的图像样本,第2层则在此基础上进行优化。</p>
                </div>
                <h4 class="anchor-tag" id="250" name="250">2.2.2 DCGAN模型</h4>
                <div class="p1">
                    <p id="251">原始GAN的生成器和判别器都是由最简单的多层感知器构成,训练效率以及生成的样本质量都较低,而DCGAN(Deep Convolutional Generative Adversarial Networks)<citation id="634" type="reference"><link href="452" rel="bibliography" /><sup>[<a class="sup">25</a>]</sup></citation>模型将GAN中的生成网络和判别网络替换为经过一定程度改进的CNN (Convolutional Neural Networks)<citation id="635" type="reference"><link href="454" rel="bibliography" /><link href="456" rel="bibliography" /><sup>[<a class="sup">26</a>,<a class="sup">27</a>]</sup></citation>,其算法将有监督学习中的CNN和无监督学习中的GAN相结合,为GAN的训练提供较好的网络结构,同时提高了训练过程的稳定性和生成结果的质量。此外,该模型可通过特征学习或是特征向量计算得到一个稳定的向量来进行特定变换。</p>
                </div>
                <h4 class="anchor-tag" id="252" name="252">2.2.3 结构扩展模型</h4>
                <div class="p1">
                    <p id="253">GAN的优势在于除了可以在生成网络或判别网络上做改进,还可以构建多个神经网络来辅助其训练。Dual GAN<citation id="636" type="reference"><link href="458" rel="bibliography" /><sup>[<a class="sup">28</a>]</sup></citation>、Disco GAN<citation id="637" type="reference"><link href="460" rel="bibliography" /><sup>[<a class="sup">29</a>]</sup></citation>、Cycle GAN<citation id="638" type="reference"><link href="462" rel="bibliography" /><sup>[<a class="sup">30</a>]</sup></citation>及Triangle GAN<citation id="639" type="reference"><link href="464" rel="bibliography" /><sup>[<a class="sup">31</a>]</sup></citation>在原始GAN结构基础上,增加了一个生成器和一个判别器,通过双生成器双判别器模型更好地学习2个域之间的双向条件分布。Dual GAN基于成对的训练集实现了一个数据分布到另一个数据分布的映射,但该模型的最终目的是为了实现重建图像与原始图像的高度拟合,来解决高质量匹配图像对的难题。Triangle GAN则不仅只局限于学习2个图像域之间的双向条件分布,还成功学习了图像域与标签域之间的双向分布,最终实现了半监督图像分类和基于属性的图像生成工作。</p>
                </div>
                <div class="p1">
                    <p id="254">D2GAN(Dual Discriminator Generative Adversarial Nets)<citation id="640" type="reference"><link href="466" rel="bibliography" /><sup>[<a class="sup">32</a>]</sup></citation>即双判别器模型,与生成器共同训练,其中一个判别器偏向来自真实样本的数据分布,而另一个判别器则偏向来自生成器生成的数据,以生成高质量和多样化的样本。Triple-GAN<citation id="641" type="reference"><link href="468" rel="bibliography" /><sup>[<a class="sup">33</a>]</sup></citation>通过构造一个生成器、一个判别器和一个分类器模型实现深度生成模型中最先进的分类结果,模型的生成器和分类器表征图像和标签之间的条件分布,判别器识别伪造的图像标签对以此来实现深度生成模型中最先进的分类结果。SGAN<citation id="642" type="reference"><link href="470" rel="bibliography" /><sup>[<a class="sup">34</a>]</sup></citation>模型定义了<i>N</i>个生成器<i>G</i>和判别器<i>D</i>,一对全局GAN及<i>N</i>-1对局部GAN,训练时全局生成器和判别器分别使用局部生成器和判别器进行训练,这种方法较好地缓解了模式崩溃、收敛时的稳定性并提高了收敛速度。</p>
                </div>
                <h4 class="anchor-tag" id="255" name="255">2.2.4 递归模型</h4>
                <div class="p1">
                    <p id="256">LAPGAN(Laplacian Generative Adversarial Networks)<citation id="643" type="reference"><link href="472" rel="bibliography" /><sup>[<a class="sup">35</a>]</sup></citation>将原始GAN的生成器与判别器定义为Laplacian Pyramids框架内的级联卷积网络,以LAPGAN的生成模型为例,文中定义多个生成器由粗到精生成样本。GRAN(Generating images with Recurrent Adversarial Networks)<citation id="644" type="reference"><link href="474" rel="bibliography" /><sup>[<a class="sup">36</a>]</sup></citation>同样是通过重复几次步骤的顺序生成相似的模型,基于VAE(Variational Auto-Encoder)<citation id="645" type="reference"><link href="416" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>,分多步对图像编码再解码而不是一步生成,其来自于DRAW (Deep Recurrent Attentive Writer)<citation id="646" type="reference"><link href="476" rel="bibliography" /><sup>[<a class="sup">37</a>]</sup></citation>,逐步生成一个完整的图像。这种模型的优点是重复产生以前一步状态为基础的输出,通过将复杂数据分布映射到简单的顺序上的方法简化图像的生成问题。improved GAN<citation id="647" type="reference"><link href="478" rel="bibliography" /><sup>[<a class="sup">38</a>]</sup></citation>通过改变训练过程中的顺序和方法同样也提高了模型的稳定性。</p>
                </div>
                <h4 class="anchor-tag" id="257" name="257">2.3 GAN与其他模型的结合</h4>
                <h4 class="anchor-tag" id="258" name="258">2.3.1 GAN+VAE模型</h4>
                <div class="p1">
                    <p id="259">GAN和VAE虽不同,但两者并非完全冲突,文献<citation id="649" type="reference">[<a class="sup">39</a>,<a class="sup">40</a>]</citation>将GAN与VAE结合作为双方的辅助模型,以更好地提升样本生成的质量。文献<citation id="648" type="reference">[<a class="sup">39</a>]</citation>将GAN作为VAE的一个辅助,两者结合后可以通过机器学习建立一个公式来进行合理的度量,而不是通过预先人为制定一个适合度量标准问题的方法。将GAN的判别器部分作为VAE中decoder部分的一个辅助,原因在于GAN判别器网络隐含着学习图像的丰富的相似性度量的能力,其替换了此前VAE采用的像素级误差,虽然实现起来更容易,但其并不适合图像数据。因为对于整幅图来说,细微的变化人们往往不会察觉到,反而是一些关键地方的变化,影响着人们对整幅图像的印象。将像素级误差度量替换为GAN判别器的相似性度量,效果会更佳。</p>
                </div>
                <div class="p1">
                    <p id="260">除了GAN与VAE的结合,还有GAN与pixel CNN<citation id="650" type="reference"><link href="484" rel="bibliography" /><sup>[<a class="sup">41</a>]</sup></citation>以及pixel RNN<citation id="651" type="reference"><link href="486" rel="bibliography" /><sup>[<a class="sup">42</a>]</sup></citation>的结合,是将原本对像素的操作改为对整幅图像的衡量与评价,以及文献<citation id="652" type="reference">[<a class="sup">43</a>]</citation>将通常在原始图像空间的损失度量替换为在特征空间的损失度量,同时在原有GAN的对抗训练损失基础上又加入了2个损失项,提升了GAN生成图像的清晰度和训练时的收敛速度。</p>
                </div>
                <h4 class="anchor-tag" id="261" name="261">2.3.2 GAN+RNN模型</h4>
                <div class="p1">
                    <p id="262">循环神经网络 (Recurrent Neural Network,RNN)<citation id="653" type="reference"><link href="490" rel="bibliography" /><link href="492" rel="bibliography" /><sup>[<a class="sup">44</a>,<a class="sup">45</a>]</sup></citation>,其网络的内部结构可以展示动态时序行为,在自然语言处理、手写识别和语音识别领域应用广泛。</p>
                </div>
                <div class="p1">
                    <p id="263">Continuous-RNN-GAN<citation id="654" type="reference"><link href="494" rel="bibliography" /><sup>[<a class="sup">46</a>]</sup></citation>是GAN在音乐生成领域的较早应用,也是GAN处理连续序列数据的针对性研究。其模型在SeqGAN<citation id="655" type="reference"><link href="496" rel="bibliography" /><sup>[<a class="sup">47</a>]</sup></citation>提出的判别器对于序列性数据的区分方法的基础上提出了一种循环对抗神经网络架构,针对以往音乐计算研究中使用符号特征(Symbolic Representation)的不足(计算机更容易理解数字表达),以及GAN的优势,将GAN与RNN结合对古典音乐进行学习并模拟其风格生成连续的序列数据。该模型采用Deep LSTM (Long Short-Term Memory)<citation id="656" type="reference"><link href="498" rel="bibliography" /><link href="500" rel="bibliography" /><sup>[<a class="sup">48</a>,<a class="sup">49</a>]</sup></citation>单元构建循环神经网络,每一个LSTM单元都有输入门、遗忘门和输出门,有用的信息被传递给循环神经网络的下一个单元,无用的信息则被遗忘,以保证信息有效的长期记忆。Continuous-RNN-GAN的整个训练过程为生成器产生连续数据序列,按顺序分段传递给每一个LSTM,然后将生成的结果传递给判别器,判别器来区分真实音乐和生成数据。同时该模型还基于音调的变化范围等提出了自己的判别指标,使用韵律学的方式,根据Polyphony(2个音同时弹奏的频率)、Scale consistency(标准音程的比例)、Repetitions(音符组合重复的频率)、Tone span(整段音乐的最低最高音阶差这几个方面进行计算。</p>
                </div>
                <div class="p1">
                    <p id="264">本节主要对GAN的扩展与改进工作进行归纳和分析,详略介绍了具有代表性的研究成果。从数据相似性度量、模型框架和训练方法以及与其他模型相结合3个方面展开讨论。对于较为经典的模型如WGAN,InfoGAN等,给出实验结果分析,但由于所训练的数据以及设备的差异都会造成实验结果的不同,因此不能作为研究结果。通过对各类研究成果分析发现,对于GAN的改进与扩展主要集中于训练过程中损失计算方法的改进或对于模型框架结构的扩展,利用该方式达到训练过程快速收敛或优化生成样本质量的目的。</p>
                </div>
                <h3 id="265" name="265" class="anchor-tag">3 GAN应用</h3>
                <div class="p1">
                    <p id="266">近年来,GAN的学习能力和改进潜力受到广泛关注,研究者提出各种形态的GAN并将其应用于各领域,推动了GAN研究与应用的进展。</p>
                </div>
                <h4 class="anchor-tag" id="267" name="267">3.1 图像合成</h4>
                <div class="p1">
                    <p id="268">文献<citation id="657" type="reference">[<a class="sup">50</a>]</citation>提出Composite GAN,通过多个生成器生成部分图像并合成整幅图像,例如在人脸生成中将背景和人脸分为两部分,由不同的生成器生成放入RNN中最后合成,同样可生成较为逼真的图像。该模型与PixelNN的本质不同在于其结果都是模型通过训练时学习后生成的,生成的结果与原图相类似但并不相同。文献<citation id="658" type="reference">[<a class="sup">51</a>]</citation>提出LR-GAN,将图像分为前景和背景,以递归的方式分别生成再合成产生完整的图像,其研究成果对于推动GAN在图像合成领域的进一步应用具有重要意义。</p>
                </div>
                <h4 class="anchor-tag" id="269" name="269">3.2 风格迁移</h4>
                <div class="p1">
                    <p id="270">风格迁移(Style Transfer,ST)是深度学习应用中非常有趣的一种,可使用这种方法将一张图片的风格“迁移”到另一张图片上。深度学习最早是基于CNN框架并尝试进行图像风格迁移<citation id="659" type="reference"><link href="506" rel="bibliography" /><sup>[<a class="sup">52</a>]</sup></citation>。但该模型有如下2个缺点:一是训练速度太慢,每一次生成都相当于重新训练一个模型;二是对于训练样本要求较高,需要在成对的数据上训练,在实际生活中并没有如此完美的成对训练数据。</p>
                </div>
                <div class="p1">
                    <p id="271">基于GAN的自主学习和生成随机样本的特点,与加入条件变量生成特定样本的方法相结合,研究者将GAN改进后能够在无监督条件下,通过不成对的训练数据学习图像风格变换<citation id="662" type="reference"><link href="508" rel="bibliography" /><link href="510" rel="bibliography" /><link href="512" rel="bibliography" /><sup>[<a class="sup">53</a>,<a class="sup">54</a>,<a class="sup">55</a>]</sup></citation>。文献<citation id="660" type="reference">[<a class="sup">53</a>]</citation>利用一张所需风格图对模型进行无监督学习训练,得到前向生成网络,在测试模型时直接将需要转换的一批图像输入到网络中,即可得到具有预先训练的风格内容图集。文献<citation id="661" type="reference">[<a class="sup">56</a>]</citation>是苹果公司为训练神经网络并用于用户照片上的人脸和其他物体的检测,通过改进GAN模型生成大量的人物合成图像,未依靠人力采集数据,使训练神经网络的成本更低而且速度也更快。</p>
                </div>
                <div class="p1">
                    <p id="272">GAN在图像风格迁移方面的出色表现,为其后在大型数据集的生成工作上奠定了基础,用较小的数据集完成GAN的无监督训练,然后以此来生成更多与数据样本相类似的图像,可节约大量成本。表1列出各类GAN模型在基于Cityscapes数据集上照片到标签实验的性能对比。</p>
                </div>
                <div class="area_img" id="273">
                    <p class="img_tit"><b>表1 基于Cityscapes数据集中不同模型的性能对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="273" border="1"><tr><td rowspan="2"><br />模型</td><td colspan="2"><br />准确率</td><td rowspan="2">相关度</td></tr><tr><td><br />像素级</td><td>分类</td></tr><tr><td><br />GAN<sup>[6]</sup></td><td>0.22</td><td>0.10</td><td>0.05</td></tr><tr><td><br />DualGAN<sup>[28]</sup></td><td>0.27</td><td>0.13</td><td>0.06</td></tr><tr><td><br />CycleGAN<sup>[30]</sup></td><td>0.58</td><td>0.22</td><td>0.16</td></tr><tr><td><br />SimGAN<sup>[56]</sup></td><td>0.47</td><td>0.11</td><td>0.07</td></tr><tr><td><br />cGAN<sup>[57]</sup></td><td>0.54</td><td>0.33</td><td>0.19</td></tr><tr><td><br />CoGAN<sup>[58]</sup></td><td>0.45</td><td>0.11</td><td>0.08</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="274" name="274">3.3 超分辨率图像</h4>
                <div class="p1">
                    <p id="275">SRGAN(Super Resolution GAN)<citation id="663" type="reference"><link href="520" rel="bibliography" /><sup>[<a class="sup">59</a>]</sup></citation>以文献<citation id="664" type="reference">[<a class="sup">53</a>]</citation>中perceptual loss为基础,是一种新的损失函数与GAN本身的损失结合,实现了从低分辨率图像到超分辨率图像的生成。与其他超分图像处理的模型相比,SRGAN的生成图像要比Bicubic算法以及对残差网(Residual Network)<citation id="665" type="reference"><link href="522" rel="bibliography" /><sup>[<a class="sup">60</a>]</sup></citation>进行改进后得到的SRResNet (super-resolution residual network)<citation id="666" type="reference"><link href="520" rel="bibliography" /><sup>[<a class="sup">59</a>]</sup></citation>的效果更加清晰,在实验过程中发现,较高的PSNR (peak signal-to-noise ratio)<citation id="668" type="reference"><link href="524" rel="bibliography" /><link href="526" rel="bibliography" /><sup>[<a class="sup">61</a>,<a class="sup">62</a>]</sup></citation>并不能带来较好的视觉感受,因此提出新的损失函数作为评判标准,将生成数据和真实数据分别输入VGG-19<citation id="667" type="reference"><link href="528" rel="bibliography" /><sup>[<a class="sup">63</a>]</sup></citation>网络,根据得到的feature map的差异来定义损失项,可以使得超分辨率效果更能满足人的感官。</p>
                </div>
                <div class="p1">
                    <p id="276">SRGAN的实验结果表明,图像看上去比较清晰,但仔细观察会发现图像虽然清晰了,细节却仍与原图不符。如果能在处理整幅图像的基础上加入一个局部图像的生成和判别模型,效果应该会更好。表2列出了各类应用于超分辨率图像领域的GAN改进模型实验结果性能对比,其中文献<citation id="669" type="reference">[<a class="sup">53</a>]</citation>给出基于2种不同损失函数下生成的样本,基于单个像素的损失和基于特征重建的损失,基于特征重建的损失为文献<citation id="670" type="reference">[<a class="sup">53</a>]</citation>提出的损失函数即feature reconstruction loss(或perceptual loss)。经过对比同样证实高的PSNR和SSIM并不代表有更好的视觉观感。</p>
                </div>
                <div class="area_img" id="277">
                    <p class="img_tit"><b>表2 不同模型PSNR及SSIM的对比结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="277" border="1"><tr><td><br />模型</td><td>PSNR/dB</td><td>SSIM</td></tr><tr><td><br />perceptual loss <sup>[53]</sup></td><td>21.66</td><td>0.555 8</td></tr><tr><td><br />SRGAN<sup>[59]</sup></td><td>21.15</td><td>0.686 8</td></tr><tr><td><br />SRResNet<sup>[59]</sup></td><td>23.53</td><td>0.783 2</td></tr><tr><td><br />SRCNN</td><td>22.53</td><td>0.652 4</td></tr><tr><td><br />Bicubic</td><td>21.69</td><td>0.584 0</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="278" name="278">3.4 视觉概念矢量运算</h4>
                <div class="p1">
                    <p id="279">当前,虽然各类大规模的图像数据集不断出现,但要得到具有指定特征的图像较难。DCGAN<citation id="671" type="reference"><link href="452" rel="bibliography" /><sup>[<a class="sup">25</a>]</sup></citation>模型可以通过对具有2个不同特征图像间的运算提取所需的图像特征,并与第三类图像合成来达到生成特定样本的目的,即图像的视觉概念矢量运算。通过对具有相应属性的图像之间的计算得出所需特定属性的图像。除实现这类单一属性的创建,还通过人脸的左侧和右侧不同角度的图像样本创建“转向”矢量,实现人脸角度的转换。</p>
                </div>
                <div class="p1">
                    <p id="280">CoGAN(Coupled Generative Adversarial Networks)<citation id="672" type="reference"><link href="514" rel="bibliography" /><sup>[<a class="sup">56</a>]</sup></citation>提出的思想是对于训练集不同域中的相应图像属性的联合分布,从而在没有完全相对应属性样本数据的前提下生成所需的图像</p>
                </div>
                <div class="p1">
                    <p id="281">上述应用不仅只局限于实现了图像视觉概念的矢量计算,而是当需要某一类图像时,除带着特定的目标去进行数据采样,还可选择通过可用的样本数据来生成所需的数据,降低训练成本。</p>
                </div>
                <h4 class="anchor-tag" id="282" name="282">3.5 人脸修复与识别</h4>
                <div class="p1">
                    <p id="283">人类可以通过局部信息分析整体,例如看到一个人的侧脸,可以大概得到此人本来的面目。因此,将GAN与现有的人脸识别和验证方法<citation id="673" type="reference"><link href="530" rel="bibliography" /><link href="532" rel="bibliography" /><sup>[<a class="sup">64</a>,<a class="sup">65</a>]</sup></citation>相结合,应用于人脸识别领域中。</p>
                </div>
                <div class="p1">
                    <p id="284">文献<citation id="674" type="reference">[<a class="sup">66</a>]</citation>提出TP-GAN(Two-Pathway GAN),通过侧面人脸照片合成正面人脸图像,取得较好的结果。该模型生成网络的特点为双路径生成,2个路径分别处理局部和全局变换,具体来说,就是一条路径通过侧脸提取的部分五官产生脸部完整五官,另一条路径产生一个模糊的完整人脸。局部生成路径和全局结构路径都由卷积神经网络构成编码器和解码器,除此之外还应用了4个定位标记的补丁网络(Landmark Located Patch Networks,LLPN)处理人脸的五官,通过分工合作,合成一张完整逼真的人脸图像。TP-GAN的实验结果在相同条件下与其他合成方法相比效果较好,能逼真地还原正脸。</p>
                </div>
                <div class="p1">
                    <p id="285">除侧脸到正脸的复原,面部被遮挡的图片研究也较多,在没有参考的情况下,将其生成与现有的背景修复技术相比,难度会更高。文献<citation id="675" type="reference">[<a class="sup">67</a>]</citation>提出基于WGAN的人脸图像生成网络,能够为图像中的遮挡区域生成该人脸的补全图像,目的是为了进行人脸表情的识别,通过建立表情识别任务和身份识别任务的对抗关系来提取表情特征,并推断表情类别。</p>
                </div>
                <div class="p1">
                    <p id="286">对于人脸修复,文献<citation id="676" type="reference">[<a class="sup">68</a>]</citation>提出的修复模型由一个生成器和2个判别器以及一个解析网络构成,将被遮挡的人脸图像送入由神经网络构成的编码器和解码器的生成模型中,之后分为两部分判别,一部分为遮盖部分的生成内容与原图遮盖部分进行区分,另一部分为生成的完整人脸图像与原始图像进行区分。最后的解析网络是预先训练好的,进一步确保生成的内容足够逼真,同时也保证生成图像与原始图像的像素一致性。但该模型对于不对称的人脸图像,无法生成特定五官的关键特征,并且无法有效利用相邻像素之间的相关性进行生成。为进一步评估该模型在人脸识别中的性能,提出对一张完整的人脸图像进行6种不同的遮挡作为输入数据,将模型生成的样本作为查找标准,在包含原始完整图像的数据集中进行搜寻,以搜索出相同身份的图像为目标,结果显示无论哪种遮挡类型,该模型的生成结果都最接近于原始图像。但不同的遮挡类型对于生成样本最后的识别精度也有较大的影响,遮挡的部分越多,生成的样本越不容易与相同身份的图像进行匹配。最后提出使用基于像素的循环神经网络PixelRNN<citation id="677" type="reference"><link href="486" rel="bibliography" /><sup>[<a class="sup">42</a>]</sup></citation>来解决该模型的局限性。</p>
                </div>
                <h4 class="anchor-tag" id="287" name="287">3.6 图像标注</h4>
                <div class="p1">
                    <p id="288">当前基于RNN与CNN结合的方法根据图像样本生成图像标注已经取得了显著的成果<citation id="678" type="reference"><link href="540" rel="bibliography" /><sup>[<a class="sup">69</a>]</sup></citation>。RNN的时序性使其在自然语言生成领域被广泛应用。基于GAN的改进方法对自然语言的处理则不需要依赖RNN。CGAN<citation id="679" type="reference"><link href="444" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>在多模态实验中同样实现了图像标注,模型以图像特征为条件变量通过卷积神经网络来生成标签向量的条件分布,实现图像的多标签自动标注。模型WGAN-GP<citation id="680" type="reference"><link href="424" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>将文本视为概率分布向量,不需要RNN或是任何基于RNN的语言模型,在输入高斯噪声后通过多层反卷积神经网络实现直接一次生成32个字符的纯文本生成任务。通过图像能够生成文本,同样可以利用GAN实现从文本生成图像<citation id="682" type="reference"><link href="542" rel="bibliography" /><link href="544" rel="bibliography" /><sup>[<a class="sup">70</a>,<a class="sup">71</a>]</sup></citation>。文献<citation id="681" type="reference">[<a class="sup">70</a>]</citation>的GAN通过文本描述生成图像的工作,其模型训练过程是将描述文本编码后与随机噪声共同输入到生成器中生成与文本相符的图像,同时编码后的文本也作为判别条件输入判别器中用来构建目标函数。该模型解决了文本自动合成图像在人工智能技术具有挑战性的难题,将文本编码和图像建模技术相结合,并将视觉概念从字符表示转换为像素表示,取得了较好的实验结果,对GAN更好地应用于生成特定图像研究有突破性的意义。表3列出了5个应用领域内GAN衍生模型及结构。其中,CGAN可实现image to label,WGAN-GP和DCGAN可用于噪声建模,CNN用于去噪,iVGAN实现了视频上色、视频修复等。</p>
                </div>
                <div class="area_img" id="289">
                    <p class="img_tit"><b>表3 GAN在5个应用领域的衍生模型及结构</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="289" border="1"><tr><td><br />应用</td><td>模型</td><td>模型结构主要特征</td></tr><tr><td rowspan="8"><br />Image to image</td><td><br />DCGAN<sup>[25]</sup>,DiscoGAN<sup>[29]</sup>,CartoonGAN<sup>[72]</sup></td><td>CNN<sup>[26-27]</sup></td></tr><tr><td><br />InfoGAN<sup>[19]</sup></td><td>DCGAN<sup>[25]</sup></td></tr><tr><td><br />CycleGAN<sup>[30,53]</sup></td><td>CNN(Instance Normalization)</td></tr><tr><td><br />DualGAN<sup>[28]</sup></td><td>WGAN<sup>[9]</sup></td></tr><tr><td><br />IcGAN<sup>[73]</sup>,GAGAN<sup>[74]</sup>,Triangle GAN<sup>[31]</sup></td><td>CGAN<sup>[21]</sup></td></tr><tr><td><br />DA-GAN<sup>[75]</sup></td><td>Autoencoder,VGG<sup>[63]</sup></td></tr><tr><td><br />IPCGAN<sup>[76]</sup></td><td>CGAN,LSGAN<sup>[13]</sup></td></tr><tr><td><br />StarGAN<sup>[77]</sup></td><td>CGAN,CycleGAN<sup>[30]</sup></td></tr><tr><td rowspan="5"><br />Text to image </td><td><br />CGAN,GAWWN<sup>[78]</sup></td><td>CNN</td></tr><tr><td><br />GAN-INT-CLS<sup>[69]</sup>,Triangle-GAN<sup>[31]</sup></td><td>CGAN</td></tr><tr><td><br />StackGAN<sup>[24]</sup></td><td>CNN,encoder-decoder</td></tr><tr><td><br />AttnGAN<sup>[71]</sup></td><td>CNN,LSTM<sup>[48-49]</sup></td></tr><tr><td><br />DA-GAN<sup>[75]</sup></td><td>Autoencoder,VGG</td></tr><tr><td rowspan="3"><br />Super-resolution</td><td><br />Perceptual loss<sup>[53]</sup></td><td>CNN</td></tr><tr><td><br />SRGAN<sup>[59]</sup></td><td>CNN,VGG,SRResNet<sup>[59]</sup></td></tr><tr><td><br />Finding tiny faces<sup>[79]</sup></td><td>CNN,VGG</td></tr><tr><td rowspan="5"><br />  图像增强、图像复原</td><td><br />Stacked CGAN<sup>[80]</sup></td><td>CGAN</td></tr><tr><td><br />Attentive GAN<sup>[81]</sup></td><td>CNN,LSTM,ResNet<sup>[60]</sup>,Autoencoder</td></tr><tr><td><br />GCBD<sup>[82]</sup></td><td>WGAN-GP<sup>[11]</sup>,DCGAN<sup>[25]</sup>,CNN</td></tr><tr><td><br />Dehazing CGAN<sup>[83]</sup></td><td>CGAN,VGG,CNN</td></tr><tr><td><br />SCGAN<sup>[84]</sup></td><td>U-net,CGAN</td></tr><tr><td rowspan="6"><br />  视频生成、行为预测</td><td><br />VGAN<sup>[85]</sup></td><td>CNN</td></tr><tr><td><br />iVGAN<sup>[86]</sup>,TGAN<sup>[87]</sup></td><td>CNN,WGAN<sup>[9]</sup></td></tr><tr><td><br />MoCoGAN<sup>[88]</sup></td><td>RNN<sup>[44-45]</sup>,CNN</td></tr><tr><td><br />时空嵌入式GAN<sup>[89]</sup></td><td>CNN,LSTM</td></tr><tr><td><br />MD-GAN<sup>[90]</sup></td><td>3D U-net,encoder-decoder</td></tr><tr><td><br />Social GAN<sup>[91]</sup></td><td>LSTM,encoder-decoder,MLP</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="290" name="290">3.7 其他应用</h4>
                <div class="p1">
                    <p id="291">除上述具有代表性领域的应用成果,GAN在其他领域也开始有所发展。改进后的GAN模型用于图像以及文本与图像(无论是动态图像还是静态图像)的研究与当前该领域的常用模型相比,具有较好的性能。此外,国内外学者还将研究重点聚焦于将GAN与RNN结合生成具有时间序列性质的数据生成工作中。Continuous-RNN-GAN<citation id="683" type="reference"><link href="494" rel="bibliography" /><sup>[<a class="sup">46</a>]</sup></citation>正是将GAN生成的特定数据按时间顺序放入RNN的每个单元中,最后生成曲风相似但与原始训练数据不同模型的古典音乐<citation id="693" type="reference"><link href="546" rel="bibliography" /><link href="548" rel="bibliography" /><link href="550" rel="bibliography" /><sup>[<a class="sup">72</a>,<a class="sup">73</a>,<a class="sup">74</a>]</sup></citation>。文献 <citation id="694" type="reference">[<a class="sup">75</a>,<a class="sup">76</a>,<a class="sup">77</a>]</citation>基于不同改进方向的GAN实现在不同域间的图像转换。由CNN构建的GAN网络结构模型通过学习可以生成具体的图像绘制指令<citation id="684" type="reference"><link href="558" rel="bibliography" /><sup>[<a class="sup">78</a>]</sup></citation>。将模糊的小图经过GAN后生成高分辨率人脸图像并进行识别<citation id="685" type="reference"><link href="560" rel="bibliography" /><sup>[<a class="sup">79</a>]</sup></citation>。通过GAN实现图像的去阴影、去雨滴、去雾等图像增强<citation id="695" type="reference"><link href="562" rel="bibliography" /><link href="564" rel="bibliography" /><sup>[<a class="sup">80</a>,<a class="sup">81</a>]</sup></citation>和复原工作<citation id="696" type="reference"><link href="566" rel="bibliography" /><link href="568" rel="bibliography" /><link href="570" rel="bibliography" /><sup>[<a class="sup">82</a>,<a class="sup">83</a>,<a class="sup">84</a>]</sup></citation>。文献<citation id="697" type="reference">[<a class="sup">85</a>,<a class="sup">86</a>,<a class="sup">87</a>]</citation>实现了基于GAN的视频生成和行为预测<citation id="698" type="reference"><link href="578" rel="bibliography" /><link href="580" rel="bibliography" /><link href="582" rel="bibliography" /><sup>[<a class="sup">88</a>,<a class="sup">89</a>,<a class="sup">90</a>]</sup></citation>,生成结果是很多帧具有序列性的图像<citation id="686" type="reference"><link href="584" rel="bibliography" /><sup>[<a class="sup">91</a>]</sup></citation>。基于CNN的GAN通过无监督学习实现了完整且高分辨率的3D物体的生成<citation id="699" type="reference"><link href="586" rel="bibliography" /><link href="588" rel="bibliography" /><sup>[<a class="sup">92</a>,<a class="sup">93</a>]</sup></citation>。研究者将GAN基于收集到的真实商标图像数据用于商标的设计<citation id="687" type="reference"><link href="590" rel="bibliography" /><sup>[<a class="sup">94</a>]</sup></citation>。ExGAN<citation id="688" type="reference"><link href="592" rel="bibliography" /><sup>[<a class="sup">95</a>]</sup></citation>实现了通过闭合双眼的人脸图像生成合理的双眼睁开的人脸图像。通过GAN突破单一任务的模仿学习实现了多模态仿真学习,使得机器人可以同时执行多项任务以便于更好地适应现实场景<citation id="689" type="reference"><link href="594" rel="bibliography" /><sup>[<a class="sup">96</a>]</sup></citation>。文献<citation id="690" type="reference">[<a class="sup">97</a>]</citation>提出基于模型的生成对抗模仿学习(MGAIL)算法,使用相对较少的专家样本和与环境的相互作用来训练并生成策略样本进行政策模仿。TextGAN<citation id="691" type="reference"><link href="598" rel="bibliography" /><sup>[<a class="sup">98</a>]</sup></citation>采用长短期记忆网络作为生成器,卷积神经网作为判别器实现了生成语法框架合理的语句。GAN的延伸模型对抗式自编码(Adversarial Auto-Encoders,AAE)<citation id="692" type="reference"><link href="600" rel="bibliography" /><sup>[<a class="sup">99</a>]</sup></citation>还被应用于医药研制,以AAE为基础,加上已知的各类药物成分的医疗特性和有效浓度,将之用在训练神经网络上,将所需化合物相关的信息输入到网络中,得到合成该化合物所需的各成分准确比例。这项工作对于未来基于人工智能的药物研发具有指导意义。</p>
                </div>
                <div class="p1">
                    <p id="292">本节主要对GAN的应用工作进行梳理和介绍。从图像合成、风格迁移、超分辨率图像、视觉概念矢量运算、人脸修复与识别、图像标注与文本到图像等方面进行具体的描述和讨论。在其他应用中列举了视频生成、文字生成以及利用GAN进行多任务的仿真学习等研究成果。与在此之前的深度神经网络更多的是用于对样本的优化和分类不同,GAN的样本生成能力优于其他模型。GAN在各领域的广泛应用是对其能力的肯定。</p>
                </div>
                <h3 id="293" name="293" class="anchor-tag">4 未来发展趋势</h3>
                <div class="p1">
                    <p id="294">各类GAN不断被提出,应用涉及的领域越来越多,GAN的不足之处也更加突显。首先,最棘手也最需要解决的就是训练过程不稳定问题,虽然WGAN等在GAN模型崩溃和训练不收敛的问题上有一定的改进,但无法保证每次的训练结果的收敛速率。GAN生成的样本质量的优劣没有一个完全适合的判定标准,如直接由人的肉眼来评判,没有将第三方的客观评价作为标准,这就使得模型的判别在一定程度上受主观因素的影响。显然,GAN还有较多待解决的问题,但这不影响其在人工智能技术中进一步的发展与应用。GAN未来的发展趋势可分为以下方面:</p>
                </div>
                <h4 class="anchor-tag" id="295" name="295">1)网络框架及算法的改进</h4>
                <div class="p1">
                    <p id="296">WGAN、DCGAN等的出现改善了GAN模型训练过程的不稳定性与模型崩溃等问题,但未彻底解决此问题,梯度消失、过拟合以及生成样本自由度较高等模型性能问题仍是当前GAN发展所面临的阻碍。但GAN网络结构设计的自由度较高,能够整合各类损失函数,因此解决上述问题即GAN模型的优化是一个重要的研究方向。</p>
                </div>
                <h4 class="anchor-tag" id="297" name="297">2)与其他学习方法结合</h4>
                <div class="p1">
                    <p id="298">基于GAN较强的融合性将其与其他学习方法相结合,例如DCGAN将有监督与无监督学习相结合,实现了GAN的半监督学习,将半监督学习GAN应用于人脸、文字、语音等识别领域。类似地,GAN也可以与迁移学习、特征学习、强化学习等结合来解决更加有针对性的实际应用问题,更好地推动其在各领域发展。</p>
                </div>
                <h4 class="anchor-tag" id="299" name="299">3)样本生成和数据增广</h4>
                <div class="p1">
                    <p id="300">在各行业应用中,获取大量真实样本需要耗费财力、人力、物力,GAN可通过对有限数据的某个维度稍作改变,生成大量仿真数据,但当前基于GAN生成样本的方法其可控程度明显不足,不一定能够获得期望形式、效果和质量的样本。因此,研究精准可控的GAN数据生成方法,生成指定形式、效果和质量的样本,是解决数据增广和数据集扩充问题的有效途径和重要研究方向。</p>
                </div>
                <h4 class="anchor-tag" id="301" name="301">4)评估方法的完善</h4>
                <div class="p1">
                    <p id="302">由于GAN目前仍缺乏对于模型性能、准确性以及生成的样本质量等方面的客观评价标准,各项研究工作中的判定方法仅适用于当前研究成果,使得模型的判定在一定程度上受主观因素的影响。因此构建一个更加精确化、标准化、通用化的第三方综合评价标准同样是GAN研究领域一个亟待解决的问题。</p>
                </div>
                <h3 id="303" name="303" class="anchor-tag">5 结束语</h3>
                <div class="p1">
                    <p id="304">GAN将机器学习中的生成式模型与判别式模型相结合,并应用于深度神经网络,根据数据类型构建合适的模型结构。GAN不仅可以生成逼真的数据,还可通过无监督学习获得数据的分布。分析结果表明,GAN具有较强的学习能力和可塑性。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="404">
                            <a id="bibliography_1" >
                                    <b>[1]</b>
                                 HINTON G E,SALAKHUTDINOV R R.Reducing the dimensionality of data with neural networks[J].Science,2006,313(5786):504-507.
                            </a>
                        </p>
                        <p id="406">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning deep architectures for AI">

                                <b>[2]</b> BENGIO Y.Learning deep architectures for AI[J].Foundations and Trends in Machine Learning,2009,2(1):1-127.
                            </a>
                        </p>
                        <p id="408">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14110700296012&amp;v=MTk2MjNHZXJxUVRNbndaZVp1SHlqbVVMcklKbDRSYVJRPU5pZk9mYks4SDlETXFJOUZadUlKREgwN29CTVQ2VDRQUUgvaXJSZA==&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2tGeFdRSFdwMmViYVMxWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> SCHMIDHUBER J.Deep learning in neural networks:an overview[J].Neural Networks,2015,61:85-117.
                            </a>
                        </p>
                        <p id="410">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups">

                                <b>[4]</b> HINTON G,DENG L,YU D,et al.Deep neural networks for acoustic modeling in speech recognition:the shared views of four research groups[J].IEEE Signal Processing Magazine,2012,29(6):82-97.
                            </a>
                        </p>
                        <p id="412">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image Net classification with deep convolutional neural networks">

                                <b>[5]</b> KRIZHEVSKY A,SUTSKEVER I,HINTON G E.Imagenet classification with deep convolutional neural networks[C]//Proceedings of International Conference on Neural Information Processing Systems.Cambridge,USA:MIT Press,2012:1097-1105.
                            </a>
                        </p>
                        <p id="414">
                            <a id="bibliography_6" >
                                    <b>[6]</b>
                                 GOODFELLOW I,POUGET-ABADIE J,MIRZA M,et al.Generative adversarial nets[C]//Proceedings of International Conference on Neural Information Processing Systems.Cambridge,USA:MIT Press,2014:2672-2680.
                            </a>
                        </p>
                        <p id="416">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Auto-encoding variational Bayes">

                                <b>[7]</b> KINGMA D P,WELLING M.Auto-encoding variational bayes[EB/OL].[2018-05-22].https://arxiv.org/pdf/1312.6114.pdf.
                            </a>
                        </p>
                        <p id="418">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201703001&amp;v=MDE4NDJDVVJMT2VaZVJyRnk3bFZyek9LQ0xmWWJHNEg5Yk1ySTlGWllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnI=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2tGeFdRSFdwMmViYVMxWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b> 王坤峰,苟超,段艳杰,等.生成式对抗网络GAN的研究进展与展望[J].自动化学报,2017,43(3):321-332.
                            </a>
                        </p>
                        <p id="420">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Wasserstein GAN">

                                <b>[9]</b> ARJOVSKY M,CHINTALA S,BOTTOU L.Wasserstein GAN[EB/OL].[2018-05-22].https://arxiv.org/pdf/1701.07875.pdf.
                            </a>
                        </p>
                        <p id="422">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Jensen-Shannon divergence and Hilbert space embedding">

                                <b>[10]</b> FUGLEDE B,TOPSOE F.Jensen-Shannon divergence and Hilbert space embedding[C]//Proceedings of International Symposium on Information Theory.Washington D.C.,USA:IEEE Press,2004:31.
                            </a>
                        </p>
                        <p id="424">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Improved Training of Wasserstein GANs[C/OL]">

                                <b>[11]</b> GULRAJANI I,AHMED F,ARJOVSKY M,et al.Improved training of wasserstein GANs[EB/OL].[2018-05-22].https://arxiv.org/pdf/1704.00028.pdf.
                            </a>
                        </p>
                        <p id="426">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=f-GAN:training generative neural samplers using variational divergence minimization">

                                <b>[12]</b> NOWOZIN S,CSEKE B,TOMIOKA R.f-GAN:training generative neural samplers using variational divergence minimization[C]//Proceedings of International Conference on Neural Information Processing Systems.Cambridge,USA:MIT Press,2016:271-279.
                            </a>
                        </p>
                        <p id="428">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Least squares generative adversarial networks">

                                <b>[13]</b> MAO Xudong,LI Qing,XIE Haoran,et al.Least squares generative adversarial networks[EB/OL].[2018-05-22].https://arxiv.org/pdf/1611.04076.pdf.
                            </a>
                        </p>
                        <p id="430">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Loss-sensitive generative adversarial networks on lipschitz densities">

                                <b>[14]</b> QI Guojun.Loss-sensitive generative adversarial networks on lipschitz densities[EB/OL].[2018-05-22].https://arxiv.org/pdf/1701.06264.pdf.
                            </a>
                        </p>
                        <p id="432">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Energy-based generative adversarial network">

                                <b>[15]</b> ZHAO Junbo,MATHIEU M,LECUN Y.Energy-based generative adversarial network[EB/OL].[2018-05-22].https://arxiv.org/pdf/1609.03126.pdf.
                            </a>
                        </p>
                        <p id="434">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Calibrating energy-based generative adversarial networks">

                                <b>[16]</b> DAI Zihang,ALMAHAIRI A,BACHMAN P,et al.Calibrating energy-based generative adversarial networks[EB/OL].[2018-05-22].https://arxiv.org/pdf/1702.01691.pdf.
                            </a>
                        </p>
                        <p id="436">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning to draw samples:with application to amortized mle for generative adversarial learning">

                                <b>[17]</b> WANG Dilin,LIU Qiang.Learning to draw samples:with application to amortized mle for generative adversarial learning[EB/OL].[2018-05-22].https://arxiv.org/pdf/1611.01722.pdf.
                            </a>
                        </p>
                        <p id="438">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=BEGAN:boundary equilibrium generative adversarial networks">

                                <b>[18]</b> BERTHELOT D,SCHUMM T,METZ L.BEGAN:boundary equilibrium generative adversarial networks [EB/OL].[2018-05-22].https://arxiv.org/pdf/1703.10717.pdf.
                            </a>
                        </p>
                        <p id="440">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Info GAN:Interpretable representation learning by information maximizing generative adversarial nets">

                                <b>[19]</b> CHEN Xi,DUAN Yan,HOUTHOOFT R,et al.InfoGAN:interpretable representation learning by information maximizing generative adversarial nets[C]//Proceedings of International Conference on Neural Information Processing Systems.Cambridge,USA:MIT Press,2016:2172-2180.
                            </a>
                        </p>
                        <p id="442">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=PixelGAN autoencoders">

                                <b>[20]</b> MAKHZANI A,FREY B J.PixelGAN autoencoders[C]//Proceedings of International Conference on Neural Information Processing Systems.Cambridge,USA:MIT Press,2017:1972-1982.
                            </a>
                        </p>
                        <p id="444">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Conditional generative adversarial nets">

                                <b>[21]</b> MIRZA M,OSINDERO S.Conditional generative adversarial nets[EB/OL].[2018-05-22].https://arxiv.org/pdf/1411.1784.pdf.
                            </a>
                        </p>
                        <p id="446">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Structured generative adversarial networks">

                                <b>[22]</b> DENG Zhijie,ZHANG Hao,LIANG Xiaodan,et al.Structured generative adversarial networks[C]//Proceedings of International Conference in Neural Information Processing Systems.[S.l.]:Curran Associates Inc.,2017:3902-3912.
                            </a>
                        </p>
                        <p id="448">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Unsupervised and semi-supervised learning with categorical generative adversarial networks">

                                <b>[23]</b> SPRINGENBERGJ T.Unsupervised and semi-supervised learning with categorical generative adversarial networks[EB/OL].[2018-05-22].https://arxiv.org/pdf/1511.06390.pdf.
                            </a>
                        </p>
                        <p id="450">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=StackGAN:Text to Photo-Realistic Image Synthesis with Stacked Generative Adversarial Networks[C/OL]">

                                <b>[24]</b> ZHANG Han,XU Tao,LI Hongsheng,et al.StackGAN:text to photo-realistic image synthesis with stacked generative adversarial networks[EB/OL].[2018-05-22].https://arxiv.org/pdf/1612.03242.pdf.
                            </a>
                        </p>
                        <p id="452">
                            <a id="bibliography_25" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks">

                                <b>[25]</b> RADFORD A,METZ L,CHINTALA S.Unsupervised representation learning with deep convolutional generative adversarial networks[EB/OL].[2018-05-22].https://arxiv.org/pdf/1511.06434.pdf.
                            </a>
                        </p>
                        <p id="454">
                            <a id="bibliography_26" >
                                    <b>[26]</b>
                                 LECUN Y,BOTTOU L,BENGIO Y,et al.Gradient-based learning applied to document recognition[J].Proceedings of the IEEE,1998,86(11):2278-2324.
                            </a>
                        </p>
                        <p id="456">
                            <a id="bibliography_27" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJBK&amp;filename=SJBK15090500012738&amp;v=MDE1NzlGWk9vTkMzOHhvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVVMcklKbDRSYVJRPU5pZkpaYks5SHRqTXFvOQ==&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2tGeFdRSFdwMmViYVMxWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[27]</b> LECUN Y,BOSER B,DENKER J S,et al.Backpropagation applied to handwritten zip code recognition[J].Neural Computation,1989,1(4):541-551.
                            </a>
                        </p>
                        <p id="458">
                            <a id="bibliography_28" target="_blank" href="http://scholar.cnki.net/result.aspx?q=DualGAN:unsupervised dual learning for image-to-image translation">

                                <b>[28]</b> YI Zili,ZHANG Hao,PING Tan,et al.DualGAN:unsupervised dual learning for image-to-image translation[EB/OL].[2018-05-22].https://arxiv.org/pdf/1704.02510.pdf.
                            </a>
                        </p>
                        <p id="460">
                            <a id="bibliography_29" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning to discover cross-domain relations with generative adversarial networks">

                                <b>[29]</b> KIM T,CHA M,KIM H,et al.Learning to discover cross-domain relations with generative adversarial networks[C]//Proceedings of the 34th International Conference on Machine Learning.Berlin,Germany:Springer,2017:1857-1865.
                            </a>
                        </p>
                        <p id="462">
                            <a id="bibliography_30" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Unpaired image-to-image translation using cycle-consistent adversarial networks">

                                <b>[30]</b> ZHU Junyan,PARK T,ISOLA P,et al.Unpaired image-to-image translation using cycle-consistent adversarial networks[EB/OL].[2018-05-22].https://arxiv.org/pdf/1703.10593.pdf.
                            </a>
                        </p>
                        <p id="464">
                            <a id="bibliography_31" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Triangle generative adversarial networks">

                                <b>[31]</b> GAN Zhe,CHEN Liqun,WANG Weiyao,et al.Triangle generative adversarial networks[C]//Proceedings of International Conference on Neural Information Processing Systems.Cambridge,USA:MIT Press,2017:5253-5262.
                            </a>
                        </p>
                        <p id="466">
                            <a id="bibliography_32" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Dual discriminator generative adversarial nets">

                                <b>[32]</b> NGUYEN T,LE T,VU H,et al.Dual discriminator generative adversarial nets[C]//Proceedings of International Conference on Neural Information Processing Systems.Cambridge,USA:MIT Press,2017:2667-2677.
                            </a>
                        </p>
                        <p id="468">
                            <a id="bibliography_33" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Triple generative adversarial nets">

                                <b>[33]</b> LI Chongxuan,XU Tun,ZHU Jun,et al.Triple generative adversarial nets[C]//Proceedings of International Conference on Neural Information Processing Systems.Cambridge,USA:MIT Press,2017:4091-4101.
                            </a>
                        </p>
                        <p id="470">
                            <a id="bibliography_34" target="_blank" href="http://scholar.cnki.net/result.aspx?q=SGAN:an alternative training of generative adversarial networks">

                                <b>[34]</b> CHAVDAROVA T,FLEURET F.SGAN:an alternative training of generative adversarial networks[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Press,2018:9407-9415
                            </a>
                        </p>
                        <p id="472">
                            <a id="bibliography_35" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep generative image models using a Laplacian pyramid of adversarial networks">

                                <b>[35]</b> DENTON E,CHINTALA S,FERGUS R.Deep generative image models using a Laplacian pyramid of adversarial networks[C]//Proceedings of International Conference on Neural Information Processing Systems.Cambridge,USA:MIT Press,2015:1486-1494.
                            </a>
                        </p>
                        <p id="474">
                            <a id="bibliography_36" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Generating images with recurrent adversarial networks">

                                <b>[36]</b> IM D J,KIM C D,JIANG Hui,et al.Generating images with recurrent adversarial networks[EB/OL].[2018-05-22].https://arxiv.org/pdf/1602.05110.pdf.
                            </a>
                        </p>
                        <p id="476">
                            <a id="bibliography_37" target="_blank" href="http://scholar.cnki.net/result.aspx?q=DRAW:a recurrent neural network for image generation">

                                <b>[37]</b> GREGOR K,DANIHELKA I,GRAVES A,et al.DRAW:a recurrent neural network for image generation[C]//Proceedings of International Conference on International Conference on Machine Learning.[S.l.]:JMLR.org,2015:1462-1471.
                            </a>
                        </p>
                        <p id="478">
                            <a id="bibliography_38" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Improved techniques for training GANs">

                                <b>[38]</b> SALIMANS T,GOODFELLOW I,ZAREMBA W,et al.Improved techniques for training GANs[C]//Proceedings of International Conference on Neural Information Processing Systems.Cambridge,USA:MIT Press,2016:2234-2242.
                            </a>
                        </p>
                        <p id="480">
                            <a id="bibliography_39" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Autoencoding beyond pixels using a learned similarity metric">

                                <b>[39]</b> LARSEN A B L,S∅NDERBY S K,LAROCHELLE H,et al.Autoencoding beyond pixels using a learned similarity metric[C]//Proceedings of International Conference on Machine Learning.[S.l.]:JMLR.org,2016:1558-1566.
                            </a>
                        </p>
                        <p id="482">
                            <a id="bibliography_40" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Adversarial variational Bayes:unifying variational autoencoders and generative adversarial networks">

                                <b>[40]</b> MESCHEDER L,NOWOZIN S,GEIGER A.Adversarial variational Bayes:unifying variational autoencoders and generative adversarial networks[C]//Proceedings of International Conference on International Conference on Machine Learning.[S.l.]:JMLR.org,2017:2391-2400.
                            </a>
                        </p>
                        <p id="484">
                            <a id="bibliography_41" target="_blank" href="http://scholar.cnki.net/result.aspx?q=PixelCNN++:improving the pixelCNN with discretized logistic mixture likelihood and other modifications">

                                <b>[41]</b> SALIMANS T,KARPATHY A,CHEN Xi,et al.PixelCNN++:improving the pixelCNN with discretized logistic mixture likelihood and other modifications[EB/OL].[2018-05-22].https://arxiv.org/pdf/1701.05517.pdf.
                            </a>
                        </p>
                        <p id="486">
                            <a id="bibliography_42" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Pixel recurrent neural networks">

                                <b>[42]</b> OORDA V D,KALCHBRENNER N,KAVUKCUOGLU K.Pixel recurrent neural networks[C]//Proceedings of International Conference on International Conference on Machine Learning.[S.l.]:JMLR.org,2016:1747-1756.
                            </a>
                        </p>
                        <p id="488">
                            <a id="bibliography_43" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Generating Images with Perceptual Similarity Metrics Based on Deep Networks">

                                <b>[43]</b> DOSOVITSKIY A,BROX T.Generating images with perceptual similarity metrics based on deep networks[C]//Proceedings of International Conference on Neural Information Processing Systems.Cambridge,USA:MIT Press,2016:658-666.
                            </a>
                        </p>
                        <p id="490">
                            <a id="bibliography_44" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Generating sequences with recurrent neural networks">

                                <b>[44]</b> GRAVES A.Generating sequences with recurrent neural networks[EB/OL].[2018-05-22].https://arxiv.org/pdf/1308.0850.pdf.
                            </a>
                        </p>
                        <p id="492">
                            <a id="bibliography_45" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM9EE1B363A4382B80977C8D6DEFBB8DD7&amp;v=MDU4NjVmT0dRbGZDcGJRMzVOMWh3cmk2dzY0PU5pZklZN3JOYTlDK3JJbEdGZThNQkg1THh4WWE3VGdPUUF2azJHZERDOENjTWM2WUNPTnZGU2lXV3I3SklGcG1hQnVIWQ==&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2tGeFdRSFdwMmViYVMxWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[45]</b> LEE H Y,TSENG B H,WEN T H,et al.Personalizing recurrent-neural-network-based language model by social network[J].IEEE/ACM Transactions on Audio,Speech,and Language Processing,2017,25(3):519-530.
                            </a>
                        </p>
                        <p id="494">
                            <a id="bibliography_46" target="_blank" href="http://scholar.cnki.net/result.aspx?q=C-RNN-GAN:continuous recurrent neural networks with adversarial training">

                                <b>[46]</b> MOGRENO.C-RNN-GAN:continuous recurrent neural networks with adversarial training[EB/OL].[2018-05-22].https://arxiv.org/pdf/1611.09904.pdf.
                            </a>
                        </p>
                        <p id="496">
                            <a id="bibliography_47" target="_blank" href="http://scholar.cnki.net/result.aspx?q=SeqGAN:sequence generative adversarial nets with policy gradient">

                                <b>[47]</b> YU Lantao,ZHANG Weinan,WANG Jun,et al.SeqGAN:sequence generative adversarial nets with policy gradient[C]//Proceedings of Conference on Artificial Intelligence.[S.l.]:AAAI Press,2017:2852-2858.
                            </a>
                        </p>
                        <p id="498">
                            <a id="bibliography_48" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJBK&amp;filename=SJBK15090500014198&amp;v=MTIxMzI2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVUxySUpsNFJhUlE9TmlmSlpiSzlIdGpNcW85RlpPb0xEWFV4b0JNVA==&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2tGeFdRSFdwMmViYVMxWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[48]</b> HOCHREITER S,SCHMIDHUBER J.Long short-term memory[J].Neural Computation,1997,9(8):1735-1780.
                            </a>
                        </p>
                        <p id="500">
                            <a id="bibliography_49" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Long short-term memory based recurrent neural network architectures for large vocabulary speech recognition">

                                <b>[49]</b> SAK H,SENIOR A,BEAUFAYS F.Long short-term memory based recurrent neural network architectures for large vocabulary speech recognition[EB/OL].[2018-05-22].https://arxiv.org/pdf/1402.1128v1.pdf.
                            </a>
                        </p>
                        <p id="502">
                            <a id="bibliography_50" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Generating images part by part with composite generative adversarial networks">

                                <b>[50]</b> KWAK H,ZHANG B T.Generating images part by part with composite generative adversarial networks[EB/OL].[2018-05-22].https://arxiv.org/pdf/1607.05387.pdf.
                            </a>
                        </p>
                        <p id="504">
                            <a id="bibliography_51" target="_blank" href="http://scholar.cnki.net/result.aspx?q=LR-GAN:Layered recursive generative adversarial networks for image generation">

                                <b>[51]</b> YANG Jianwei,KANNAN A,BATRA D,et al.LR-GAN:Layered recursive generative adversarial networks for image generation[EB/OL].[2018-05-22].https://arxiv.org/pdf/1703.01560.pdf.
                            </a>
                        </p>
                        <p id="506">
                            <a id="bibliography_52" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A neural algorithm of artistic style">

                                <b>[52]</b> GATYS L A,ECKER A S,BETHGE M.A neural algorithm of artistic style[EB/OL].[2018-05-22].https://arxiv.org/pdf/1508.06576.pdf.
                            </a>
                        </p>
                        <p id="508">
                            <a id="bibliography_53" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Perceptual losses for real-time style transfer and superresolution">

                                <b>[53]</b> JOHNSON J,ALAHI A,LI Feifei.Perceptual losses for real-time style transfer and super-resolution[C]//Proceedings of European Conference on Computer Vision.Berlin,Germany:Springer,2016:694-711.
                            </a>
                        </p>
                        <p id="510">
                            <a id="bibliography_54" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Unsupervised pixel-level domain adaptation with generative adversarial networks">

                                <b>[54]</b> BOUSMALIS K,SILBERMAN N,DOHAN D,et al.Unsupervised pixel-level domain adaptation with generative adversarial networks[EB/OL].[2018-05-22].https://arxiv.org/pdf/1612.05424.pdf.
                            </a>
                        </p>
                        <p id="512">
                            <a id="bibliography_55" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Unsupervised image-to-image translation networks">

                                <b>[55]</b> LIU Mingyu,BREUEL T,KAUTZ J.Unsupervised image-to-image translation networks[C]//Proceedings of International Conference on Neural Information Processing Systems.Cambridge,USA:MIT Press,2017:700-708.
                            </a>
                        </p>
                        <p id="514">
                            <a id="bibliography_56" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning from simulated and unsupervised images through adversarial training">

                                <b>[56]</b> SHRIVASTAVA A,PFISTER T,TUZEL O,et al.Learning from simulated and unsupervised images through adversarial training[EB/OL].[2018-05-22].https://arxiv.org/pdf/1612.07828.pdf.
                            </a>
                        </p>
                        <p id="516">
                            <a id="bibliography_57" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image-to-Image Translation with Conditional Adversarial Networks">

                                <b>[57]</b> ISOLA P,ZHU Junyan,ZHOU Tinghui,et al.Image-to-image translation with conditional adversarial networks[EB/OL].[2018-05-22].https://arxiv.org/pdf/1611.07004.pdf.
                            </a>
                        </p>
                        <p id="518">
                            <a id="bibliography_58" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Coupled generative adversarial networks">

                                <b>[58]</b> LIU Mingyu,TUZEL O.Coupled generative adversarial networks[C]//Proceedings of International Conference on Neural Information Processing Systems.Cambridge,USA:MIT Press,2016:469-477.
                            </a>
                        </p>
                        <p id="520">
                            <a id="bibliography_59" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network[C/OL]">

                                <b>[59]</b> LEDIG C,THEIS L,HUSZÁR F,et al.Photo-realistic single image super-resolution using a generative adversarial network[EB/OL].[2018-05-22].https://arxiv.org/pdf/1609.04802v1.pdf.
                            </a>
                        </p>
                        <p id="522">
                            <a id="bibliography_60" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep Residual Learning for Image Recognition">

                                <b>[60]</b> HE Kaiming,ZHANG Xiangyu,REN Shaoqing,et al.Deep residual learning for image recognition[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Press,2016:770-778.
                            </a>
                        </p>
                        <p id="524">
                            <a id="bibliography_61" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Improved signal-to-noise ratio in spectral-domain compared with time-domain optical coherence tomography">

                                <b>[61]</b> BOER J F D,CENSE B,PARK B H,et al.Improved signal-to-noise ratio in spectral-domain compared with time-domain optical coherence tomography[J].Optics Letters,2003,28(21):2067-2069.
                            </a>
                        </p>
                        <p id="526">
                            <a id="bibliography_62" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Increase in signal-to-noise ratio of &amp;gt; 10,000 times in liquid-state NMR">

                                <b>[62]</b> ARDENKJÆR-LARSEN J H,FRIDLUND B,GRAM A,et al.Increase in signal-to-noise ratio of &gt; 10,000 times in liquid-state NMR[J].Proceedings of the National Academy of Sciences,2003,100(18):10158-10163.
                            </a>
                        </p>
                        <p id="528">
                            <a id="bibliography_63" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Very deep convolutional networks for large-scale image recognition">

                                <b>[63]</b> SIMONYAN K,ZISSERMAN A.Very deep convolutional networks for large-scale image recognition[EB/OL].[2018-05-22].https://arxiv.org/pdf/1409.1556.pdf.
                            </a>
                        </p>
                        <p id="530">
                            <a id="bibliography_64" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Face recognition: Eigenface, elastic matching, and neural nets">

                                <b>[64]</b> ZHANG Jun,YAN Yong,LADES M.Face recognition:eigenface,elastic matching,and neural nets[J].Proceedings of the IEEE,1997,85(9):1423-1435.
                            </a>
                        </p>
                        <p id="532">
                            <a id="bibliography_65" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning a Similarity Metric Discriminatively, with Application to Face verification">

                                <b>[65]</b> CHOPRA S,HADSELL R,LECUN Y.Learning a similarity metric discriminatively,with application to face verification[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Press,2005:539-546.
                            </a>
                        </p>
                        <p id="534">
                            <a id="bibliography_66" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Beyond face rotation:global and local perception GAN for photorealistic and identity preserving frontal view synthesis">

                                <b>[66]</b> HUANG Rui,ZHANG Shu,LI Tianyu,et al.Beyond face rotation:global and local perception GAN for photorealistic and identity preserving frontal view synthesis[EB/OL].[2018-05-22].https://arxiv.org/pdf/1704.04086.pdf.
                            </a>
                        </p>
                        <p id="536">
                            <a id="bibliography_67" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201805010&amp;v=MjczMzMzenFxQnRHRnJDVVJMT2VaZVJyRnk3bFZyek9LQ0xmWWJHNEg5bk1xbzlFWklRS0RIODR2UjRUNmo1NE8=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2tGeFdRSFdwMmViYVMxWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[67]</b> 姚乃明,郭清沛,乔逢春,等.基于生成式对抗网络的鲁棒人脸表情识别[J].自动化学报,2018,44(5):865-877.
                            </a>
                        </p>
                        <p id="538">
                            <a id="bibliography_68" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Generative Face Completion">

                                <b>[68]</b> LI Yijun,LIU Sifei,YANG Jimei,et al.Generative face completion[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Press,2017:3911-3919.
                            </a>
                        </p>
                        <p id="540">
                            <a id="bibliography_69" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep visual-semantic alignments for generating image descriptions">

                                <b>[69]</b> KARPATHY A,LI Feifei.Deep visual-semantic alignments for generating image descriptions[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Press,2015:3128-3137.
                            </a>
                        </p>
                        <p id="542">
                            <a id="bibliography_70" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Generative adversarial text to image synthesis">

                                <b>[70]</b> REED S,AKATA Z,YAN Xinchen,et al.Generative adversarial text to image synthesis[C]//Proceedings of International Conference on International Conference on Machine Learning.[S.l.]:JMLR.org,2016:1060-1069.
                            </a>
                        </p>
                        <p id="544">
                            <a id="bibliography_71" target="_blank" href="http://scholar.cnki.net/result.aspx?q=AttnGAN:fine-grained text to image generation with attentional generative adversarial networks">

                                <b>[71]</b> XU Tao,ZHANG Pengchuan,HUANG Qiuyuan,et al.AttnGAN:fine-grained text to image generation with attentional generative adversarial networks[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Press,2018:1316-1324.
                            </a>
                        </p>
                        <p id="546">
                            <a id="bibliography_72" target="_blank" href="http://scholar.cnki.net/result.aspx?q=CartoonGAN:Generative Adversarial Networks for Photo Cartoonization">

                                <b>[72]</b> CHEN Yang,LAI Yukun,LIU Yongjin.CartoonGAN:generative adversarial networks for photo cartoonization[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Press,2018:9465-9474.
                            </a>
                        </p>
                        <p id="548">
                            <a id="bibliography_73" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Invertible conditional GANs for image editing">

                                <b>[73]</b> PERARNAU G,WEIJER J V,RADUCANU B,et al.Invertible conditional GANs for image editing[EB/OL].[2018-05-22].https://arxiv.org/pdf/1611.06355.pdf.
                            </a>
                        </p>
                        <p id="550">
                            <a id="bibliography_74" target="_blank" href="http://scholar.cnki.net/result.aspx?q=GAGAN:geometry-aware generative adverserial networks">

                                <b>[74]</b> KOSSAIFI J,TRAN L,PANAGAKIS Y,et al.GAGAN:geometry-aware generative adverserial networks[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Press,2018:878-887.
                            </a>
                        </p>
                        <p id="552">
                            <a id="bibliography_75" target="_blank" href="http://scholar.cnki.net/result.aspx?q=DA-GAN:instancelevel image translation by deep attention generative adversarial networks">

                                <b>[75]</b> MA Shuang,FU Jianlong,CHEN Changwei,et al.DA-GAN:instance-level image translation by deep attention generative adversarial networks[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Press,2018:5657-5666.
                            </a>
                        </p>
                        <p id="554">
                            <a id="bibliography_76" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Face aging with identity-preserved conditional generative adversarial networks">

                                <b>[76]</b> WANG Zongwei,TANG Xu,LUO Weixin,et al.Face aging with identity-preserved conditional generative adversarial networks[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Press,2018:7939-7947.
                            </a>
                        </p>
                        <p id="556">
                            <a id="bibliography_77" target="_blank" href="http://scholar.cnki.net/result.aspx?q=StarGAN:Unified generative adversarial networks for multi-domain image-to-image translation">

                                <b>[77]</b> CHOI Y,CHOI M,KIM M,et al.StarGAN:unified generative adversarial networks for multi-domain image-to-image translation[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Press,2018:8789-8797.
                            </a>
                        </p>
                        <p id="558">
                            <a id="bibliography_78" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning what and where to draw">

                                <b>[78]</b> REED S E,AKATA Z,MOHAN S,et al.Learning what and where to draw[C]//Proceedings of International Conference on Neural Information Processing Systems.Cambridge,USA:MIT Press,2016:217-225.
                            </a>
                        </p>
                        <p id="560">
                            <a id="bibliography_79" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Finding tiny faces in the wild with generative adversarial network">

                                <b>[79]</b> BAI Yancheng,ZHANG Yongqiang,DING Mingli,et al.Finding tiny faces in the wild with generative adversarial network[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Press,2018:21-30.
                            </a>
                        </p>
                        <p id="562">
                            <a id="bibliography_80" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Stacked conditional generative adversarial networks for jointly learning shadow detection and shadow removal">

                                <b>[80]</b> WANG Jifeng,LI Xiang,HUI Le,et al.Stacked conditional generative adversarial networks for jointly learning shadow detection and shadow removal[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Press,2018:1788-1797.
                            </a>
                        </p>
                        <p id="564">
                            <a id="bibliography_81" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Attentive generative adversarial network for raindrop removal from a single image">

                                <b>[81]</b> QIAN Rui,TAN R T,YANG Wenhan,et al.Attentive generative adversarial network for raindrop removal from a single image[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Press,2018:2482-2491.
                            </a>
                        </p>
                        <p id="566">
                            <a id="bibliography_82" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image blind denoising with generative adversarial network based noise modeling">

                                <b>[82]</b> CHEN J,CHEN J,CHAO H,et al.Image blind denoising with generative adversarial network based noise modeling[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Press,2018:3155-3164.
                            </a>
                        </p>
                        <p id="568">
                            <a id="bibliography_83" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Single image dehazing via conditional generative adversarial network">

                                <b>[83]</b> LI Ruide,PAN Jinshan,LI Zechao,et al.Single image dehazing via conditional generative adversarial network[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Computer Society,2018:8202-8211.
                            </a>
                        </p>
                        <p id="570">
                            <a id="bibliography_84" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Shadow detection with conditional generative adversarial networks">

                                <b>[84]</b> NGUYEN V,VICENTE T F Y,ZHAO Maozheng,et al.Shadow detection with conditional generative adversarial networks[C]//Proceedings of IEEE International Conference on Computer Vision.Washington D.C.,USA:IEEE Press,2017:4520-4528.
                            </a>
                        </p>
                        <p id="572">
                            <a id="bibliography_85" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Generating videos with scene dynamics">

                                <b>[85]</b> VONDRICK C,PIRSIAVASH H,TORRALBA A.Generating videos with scene dynamics[C]//Proceedings of International Conference on Neural Information Processing Systems.Cambridge,USA:MIT Press,2016:613-621.
                            </a>
                        </p>
                        <p id="574">
                            <a id="bibliography_86" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Improving video generation for multi-functional applications">

                                <b>[86]</b> BERNHARD K,HUANG Zhiwu,DANDA P P,et al.Improving video generation for multi-functional applications[EB/OL].[2018-05-22].https://arxiv.org/pdf/1711.11453v2.pdf.
                            </a>
                        </p>
                        <p id="576">
                            <a id="bibliography_87" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Temporal generative adversarial nets with singular value clipping">

                                <b>[87]</b> SAITO M,MATSUMOTO E,SAITO S.Temporal generative adversarial nets with singular value clipping[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Press,2017:2830-2839.
                            </a>
                        </p>
                        <p id="578">
                            <a id="bibliography_88" target="_blank" href="http://scholar.cnki.net/result.aspx?q=MoCoGAN:decomposing motion and content for video generation">

                                <b>[88]</b> TULYAKOV S,LIU Mingyu,YANG Xiaodong,et al.MoCoGAN:decomposing motion and content for video generation[EB/OL].[2018-05-22].https://arxiv.org/pdf/1707.04993.pdf.
                            </a>
                        </p>
                        <p id="580">
                            <a id="bibliography_89" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201801007&amp;v=MTUxOTQ5Rlk0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSckZ5N2xWcnpPS0Q3WWJMRzRIOW5Ncm8=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2tGeFdRSFdwMmViYVMxWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[89]</b> 孔德江,汤斯亮,吴飞.时空嵌入式生成对抗网络的地点预测方法[J].模式识别与人工智能,2018,31(1):49-60.
                            </a>
                        </p>
                        <p id="582">
                            <a id="bibliography_90" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning to generate time-lapse videos using multi-stage dynamic generative adversarial networks">

                                <b>[90]</b> XIONG Wei,LUO Wenhan,MA Lin,et al.Learning to generate time-lapse videos using multi-stage dynamic generative adversarial networks[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Press,2018:2364-2373.
                            </a>
                        </p>
                        <p id="584">
                            <a id="bibliography_91" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Social GAN:socially acceptable trajectories with generative adversarial networks">

                                <b>[91]</b> GUPTA A,JOHNSON J,LI Feifei,et al.Social GAN:socially acceptable trajectories with generative adversarial networks[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Press,2018:2255-2264.
                            </a>
                        </p>
                        <p id="586">
                            <a id="bibliography_92" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Shape inpainting using 3D generative adversarial network and recurrent convolutional networks">

                                <b>[92]</b> WANG Weiyue,HUANG Qiangui,YOU Suya,et al.Shape inpainting using 3D generative adversarial network and recurrent convolutional networks[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Press,2017:2298-2306.
                            </a>
                        </p>
                        <p id="588">
                            <a id="bibliography_93" target="_blank" href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Learning a probabilistic latent space of object shapes via 3d generative-adversarial modeling,&amp;quot;">

                                <b>[93]</b> WU Jiajun,ZHANG Chengkai,XUE Tianfan,et al.Learning a probabilistic latent space of object shapes via 3d generative-adversarial modeling[C]//Proceedings of International Conference on Neural Information Processing Systems.Cambridge,USA:MIT Press,2016:82-90.
                            </a>
                        </p>
                        <p id="590">
                            <a id="bibliography_94" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Logo synthesis and manipulation with clustered generative adversarial networks">

                                <b>[94]</b> SAGE A,AGUSTSSON E,TIMOFTE R,et al.Logo synthesis and manipulation with clustered generative adversarial networks[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Press,2018:5879-5888.
                            </a>
                        </p>
                        <p id="592">
                            <a id="bibliography_95" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Eye in-painting with exemplar generative adversarial networks">

                                <b>[95]</b> DOLHANSKY B,FERRER C C.Eye in-painting with exemplar generative adversarial networks[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Press,2018:5657-5666.
                            </a>
                        </p>
                        <p id="594">
                            <a id="bibliography_96" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multi-modal imitation learning from unstructured demonstrations using generative adversarial nets">

                                <b>[96]</b> HAUSMAN K,CHEBOTAR Y,SCHAAL S,et al.Multi-modal imitation learning from unstructured demonstrations using generative adversarial nets[EB/OL].[2018-05-22].https://arxiv.org/pdf/1705.10479.pdf.
                            </a>
                        </p>
                        <p id="596">
                            <a id="bibliography_97" target="_blank" href="http://scholar.cnki.net/result.aspx?q=End-to-end differentiable adversarial imitation learning">

                                <b>[97]</b> BARAM N,ANSCHEL O,CASPI I,et al.End-to-end differentiable adversarial imitation learning[C]//Proceedings of International Conference on International Conference on Machine Learning.[S.l.]:JMLR.org,2017:390-399.
                            </a>
                        </p>
                        <p id="598">
                            <a id="bibliography_98" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Adversarial feature matching for text generation">

                                <b>[98]</b> ZHANG Yizhe,GAN Zhe,FAN Kai,et al.Adversarial feature matching for text generation[C]//Proceedings of International Conference on International Conference on Machine Learning.[S.l.]:JMLR.org,2017:4006-4015.
                            </a>
                        </p>
                        <p id="600">
                            <a id="bibliography_99" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Adversarial autoencoders">

                                <b>[99]</b> MAKHZANI A,SHLENS J,JAITLY N,et al.Adversarial autoencoders[EB/OL].[2018-05-22].https://arxiv.org/pdf/1511.05644.pdf.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJC201909036" />
        <input id="dpi" type="hidden" value="600" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2tGeFdRSFdwMmViYVMxWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201909036&amp;v=MDQxMzN5N2xWcnpQTHo3QmJiRzRIOWpNcG85R1lvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSckY=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2tGeFdRSFdwMmViYVMxWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="1" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
