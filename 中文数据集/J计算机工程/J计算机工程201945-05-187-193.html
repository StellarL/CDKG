<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637130530615655000%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJC201905030%26RESULT%3d1%26SIGN%3dIDs0%252fuZJ%252bFSdSTJKTqgBLdVX%252fM0%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201905030&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201905030&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201905030&amp;v=MTczNTQ5R1pJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSb0Z5M21VcnJJTHo3QmJiRzRIOWpNcW8=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#39" data-title="0 概述 ">0 概述</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#44" data-title="1 BLPOC模型及问题描述 ">1 BLPOC模型及问题描述</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#66" data-title="2 修正的带限相位相关算法 ">2 修正的带限相位相关算法</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#67" data-title="2.1 图像尺寸调节">2.1 图像尺寸调节</a></li>
                                                <li><a href="#81" data-title="2.2 带限窗口选择">2.2 带限窗口选择</a></li>
                                                <li><a href="#104" data-title="2.3 遍历范围划定">2.3 遍历范围划定</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#111" data-title="3 实验验证与结果分析 ">3 实验验证与结果分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#117" data-title="3.1 时间效率评测">3.1 时间效率评测</a></li>
                                                <li><a href="#126" data-title="3.2 拒真率和认假率评测">3.2 拒真率和认假率评测</a></li>
                                                <li><a href="#130" data-title="3.3 识别率评测">3.3 识别率评测</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#133" data-title="4 结束语 ">4 结束语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#70" data-title="&lt;b&gt;图1 指静脉图像ROI提取&lt;/b&gt;"><b>图1 指静脉图像ROI提取</b></a></li>
                                                <li><a href="#83" data-title="&lt;b&gt;图2 指静脉图像的频谱图&lt;/b&gt;"><b>图2 指静脉图像的频谱图</b></a></li>
                                                <li><a href="#85" data-title="&lt;b&gt;图3 图像样本对在不同带限范围内的相关性&lt;/b&gt;"><b>图3 图像样本对在不同带限范围内的相关性</b></a></li>
                                                <li><a href="#109" data-title="&lt;b&gt;图4 不同时刻图像的ROI提取&lt;/b&gt;"><b>图4 不同时刻图像的ROI提取</b></a></li>
                                                <li><a href="#110" data-title="&lt;b&gt;图5 相似ROI图像的偏移率&lt;/b&gt;"><b>图5 相似ROI图像的偏移率</b></a></li>
                                                <li><a href="#113" data-title="&lt;b&gt;图6 指静脉采集设备&lt;/b&gt;"><b>图6 指静脉采集设备</b></a></li>
                                                <li><a href="#115" data-title="&lt;b&gt;表1 3种指静脉数据库的参数对比&lt;/b&gt;"><b>表1 3种指静脉数据库的参数对比</b></a></li>
                                                <li><a href="#119" data-title="&lt;b&gt;表2 3种算法循环次数对比&lt;/b&gt;"><b>表2 3种算法循环次数对比</b></a></li>
                                                <li><a href="#124" data-title="&lt;b&gt;图7 3种算法在不同数据集上的时间效率对比&lt;/b&gt;"><b>图7 3种算法在不同数据集上的时间效率对比</b></a></li>
                                                <li><a href="#125" data-title="&lt;b&gt;表3 3种算法在不同数据库上所用时间的对比&lt;/b&gt;"><b>表3 3种算法在不同数据库上所用时间的对比</b></a></li>
                                                <li><a href="#129" data-title="&lt;b&gt;图8 3种算法的ROC曲线&lt;/b&gt;"><b>图8 3种算法的ROC曲线</b></a></li>
                                                <li><a href="#132" data-title="&lt;b&gt;图9 3种算法的相关系数分布&lt;/b&gt;"><b>图9 3种算法的相关系数分布</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" title=" LIN Chunyi, LI Mingzhong, SUN Xiao.A finger vein recognition algorithm based on gradient correlation[C]//Proceedings of AASRI Conference on Intelligence and Bioinformatics.Berlin, Germany:Springer, 2012:40-45." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13050900118197&amp;v=MDI5NDREWFUrb0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VTG5JSlYwVmJ4ST1OaWZPZmJLN0h0VE1wbzlGWmVvSA==&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         LIN Chunyi, LI Mingzhong, SUN Xiao.A finger vein recognition algorithm based on gradient correlation[C]//Proceedings of AASRI Conference on Intelligence and Bioinformatics.Berlin, Germany:Springer, 2012:40-45.
                                    </a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" title=" ZHAO Wenyi, CHELLAPPA R, PHILLIPS P J, et al.Face recognition:a literature survey[J].ACM Computing Surveys, 2003, 35 (4) :399-458." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000017620&amp;v=MDY1MDVNbndaZVp1SHlqbVVMbklKVjBWYnhJPU5pZklZN0s3SHRqTnI0OUZaT29JQ240NW9CTVQ2VDRQUUgvaXJSZEdlcnFRVA==&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         ZHAO Wenyi, CHELLAPPA R, PHILLIPS P J, et al.Face recognition:a literature survey[J].ACM Computing Surveys, 2003, 35 (4) :399-458.
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title=" JIANG Hui.Confidence measures for speech recognition:a survey[J].Speech Communication, 2005, 45 (4) :455-470." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012300465888&amp;v=MDk0MDZReG9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVUxuSUpWMFZieEk9TmlmT2ZiSzdIdERPckk5RllPMEtCSA==&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         JIANG Hui.Confidence measures for speech recognition:a survey[J].Speech Communication, 2005, 45 (4) :455-470.
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" title=" ITO K, NAKAJIMA H, KOBAYASHI K, et al.A fingerprint matching algorithm using phase-only correlation[J].IEICE Transactions on Fundamentals of Electronics Communications and Computer Sciences, 2004, 87 (3) :682-691." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Fingerprint Matching Algorithm Using Phase-Only Correlation">
                                        <b>[4]</b>
                                         ITO K, NAKAJIMA H, KOBAYASHI K, et al.A fingerprint matching algorithm using phase-only correlation[J].IEICE Transactions on Fundamentals of Electronics Communications and Computer Sciences, 2004, 87 (3) :682-691.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" title=" WANG Desong, LI Jianping, MEMIK G.User identification based on finger-vein patterns for consumer electronics devices[J].IEEE Transactions on Consumer Electronics, 2010, 56 (2) :799-804." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=User Identification Based on Finger-vein Patterns for Consumer Electronics Devices">
                                        <b>[5]</b>
                                         WANG Desong, LI Jianping, MEMIK G.User identification based on finger-vein patterns for consumer electronics devices[J].IEEE Transactions on Consumer Electronics, 2010, 56 (2) :799-804.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" title=" MIURA N, NAGASAKA A, MIYATAKE T.Feature extraction of finger-vein patterns based on repeated line tracking and its application to personal identification[J].Machine Vision and Applications, 2004, 15 (4) :194-203." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002056992&amp;v=MzA0Njhhck80SHRIT3I0cERiZUlOWTNrNXpCZGg0ajk5U1hxUnJ4b3hjTUg3UjdxZWJ1ZHRGQzdsVnJ6SkpWND1OajdC&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                         MIURA N, NAGASAKA A, MIYATAKE T.Feature extraction of finger-vein patterns based on repeated line tracking and its application to personal identification[J].Machine Vision and Applications, 2004, 15 (4) :194-203.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" title=" KUMAR A, ZHOU Y B.Human identification using finger images[J].IEEE Transactions on Image Processing, 2012, 21 (4) :2228-2244." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Human Identification Using Finger Images">
                                        <b>[7]</b>
                                         KUMAR A, ZHOU Y B.Human identification using finger images[J].IEEE Transactions on Image Processing, 2012, 21 (4) :2228-2244.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" title=" YANG Wenming, HUANG Xiaola, ZHOU Fei, et al.Comparative competitive coding for personal identification by using finger vein and finger dorsal texture fusion[J].Information Sciences, 2014, 268 (6) :20-32." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14010600065106&amp;v=MTA1MTJlcnFRVE1ud1plWnVIeWptVUxuSUpWMFZieEk9TmlmT2ZiSzhIdERNcVk5RlpPMEtEWHcvb0JNVDZUNFBRSC9pclJkRw==&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                         YANG Wenming, HUANG Xiaola, ZHOU Fei, et al.Comparative competitive coding for personal identification by using finger vein and finger dorsal texture fusion[J].Information Sciences, 2014, 268 (6) :20-32.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" title=" NIVETHA K, SARASWADY D.Enhancing security for multimodal biometric using hyper image encryption algorithm[C]//Proceedings of International Conference on Electronics and Communication Systems.Washington D.C., USA:IEEE Press, 2015:943-947." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Enhancing security for multimodal biometric using hyper image encryption algorithm">
                                        <b>[9]</b>
                                         NIVETHA K, SARASWADY D.Enhancing security for multimodal biometric using hyper image encryption algorithm[C]//Proceedings of International Conference on Electronics and Communication Systems.Washington D.C., USA:IEEE Press, 2015:943-947.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_10" title=" MANJUNATHSWAMY B E, THRUVENI J, VENUGOPAL K R, et al.Multi model personal authentication using finger vein and face images[C]//Proceedings of International Conference on Parallel, Distributed and Grid Computing.Washington D.C., USA:IEEE Press, 2015:339-344." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multi model personal authentication using finger vein and face images">
                                        <b>[10]</b>
                                         MANJUNATHSWAMY B E, THRUVENI J, VENUGOPAL K R, et al.Multi model personal authentication using finger vein and face images[C]//Proceedings of International Conference on Parallel, Distributed and Grid Computing.Washington D.C., USA:IEEE Press, 2015:339-344.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_11" title=" SUDHAMANI M J, VENKATESHA M K, RADHIKA K R.Fusion at decision level in multimodal biometric authentication system using iris and finger vein with novel feature extraction[C]//Proceedings of 2014 Annual IEEE India Conference.Washington D.C., USA:IEEE Press, 2014:1-6." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fusion at decision level in multimodal biometric authentication system using iris and finger vein with novel feature extraction">
                                        <b>[11]</b>
                                         SUDHAMANI M J, VENKATESHA M K, RADHIKA K R.Fusion at decision level in multimodal biometric authentication system using iris and finger vein with novel feature extraction[C]//Proceedings of 2014 Annual IEEE India Conference.Washington D.C., USA:IEEE Press, 2014:1-6.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_12" title=" FLANNERY D L, LOOMIS J S, MILKOVICH M E.Design elements of binary phase-only correlation filters[J].Applied Optics, 1988, 27 (20) :4231-4235." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Design elements of binary phase-only correlation filters">
                                        <b>[12]</b>
                                         FLANNERY D L, LOOMIS J S, MILKOVICH M E.Design elements of binary phase-only correlation filters[J].Applied Optics, 1988, 27 (20) :4231-4235.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_13" title=" MIYAZAWA K, ITO K, AOKI T, et al.An effective approach for iris recognition using phase-based image matching[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2008, 30 (10) :1741-1756." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=An effective approach for Iris recognition using phase-based image matching">
                                        <b>[13]</b>
                                         MIYAZAWA K, ITO K, AOKI T, et al.An effective approach for iris recognition using phase-based image matching[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2008, 30 (10) :1741-1756.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_14" title=" MAHRI N, SUANDI S A S, ROSDI B A.Finger vein recognition algorithm using phase only correlation[C]//Proceedings of International Workshop on Emerging Techniques and Challenges for Hand-Based Biometrics.Washington D.C., USA:IEEE Press, 2010:1-6." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Finger-vein recognition algorithm using phase only correlation">
                                        <b>[14]</b>
                                         MAHRI N, SUANDI S A S, ROSDI B A.Finger vein recognition algorithm using phase only correlation[C]//Proceedings of International Workshop on Emerging Techniques and Challenges for Hand-Based Biometrics.Washington D.C., USA:IEEE Press, 2010:1-6.
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_15" title=" ITO K, AOKI T, NAKAJIMA H, et al.A palmprint recognition algorithm using phase-only correlation[J].IEICE Transactions on Fundamentals of Electronics Communications and Computer Sciences, 2008, 91 (4) :1023-1030." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A palmprint recognition algorithm using phase-only correlation">
                                        <b>[15]</b>
                                         ITO K, AOKI T, NAKAJIMA H, et al.A palmprint recognition algorithm using phase-only correlation[J].IEICE Transactions on Fundamentals of Electronics Communications and Computer Sciences, 2008, 91 (4) :1023-1030.
                                    </a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_16" title=" ASAARI M S M, SUANDI S A, ROSDI B A.Fusion of band limited phase only correlation and width centroid contour distance for finger based biometrics[J].Expert Systems with Applications, 2014, 41 (7) :3367-3382." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14032300110907&amp;v=MTgyMzlGWmVvUEJYdytvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVVMbklKVjBWYnhJPU5pZk9mYks4SHRMT3JJOQ==&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[16]</b>
                                         ASAARI M S M, SUANDI S A, ROSDI B A.Fusion of band limited phase only correlation and width centroid contour distance for finger based biometrics[J].Expert Systems with Applications, 2014, 41 (7) :3367-3382.
                                    </a>
                                </li>
                                <li id="35">


                                    <a id="bibliography_17" title=" BHATI S H, PATI U C.Novel algorithm for fingerprint mosaicing using phase correlation method[C]//Proceedings of Global Conference on Communication Technologies.Washington D.C., USA:IEEE Press, 2015:29-33." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Novel algorithm for fingerprint mosaicing using phase correlation method">
                                        <b>[17]</b>
                                         BHATI S H, PATI U C.Novel algorithm for fingerprint mosaicing using phase correlation method[C]//Proceedings of Global Conference on Communication Technologies.Washington D.C., USA:IEEE Press, 2015:29-33.
                                    </a>
                                </li>
                                <li id="37">


                                    <a id="bibliography_18" title=" 王科俊, 马慧.使用改进的方向滤波与修正的Hausdorff距离的指静脉识别方法[J].计算机辅助设计与图形学学报, 2011, 23 (3) :385-391." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJF201103002&amp;v=MDA3NDVSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm9GeTNtVXJySUx6N0JhTEc0SDlETXJJOUZab1FLREg4NHY=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[18]</b>
                                         王科俊, 马慧.使用改进的方向滤波与修正的Hausdorff距离的指静脉识别方法[J].计算机辅助设计与图形学学报, 2011, 23 (3) :385-391.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJC" target="_blank">计算机工程</a>
                2019,45(05),187-193 DOI:10.19678/j.issn.1000-3428.0050817            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>修正的带限相位相关指静脉识别算法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%9B%B7%E8%95%BE&amp;code=27566213&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">雷蕾</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%B8%AD%E5%B3%B0&amp;code=24058491&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">席峰</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%99%88%E8%83%9C%E5%9E%9A&amp;code=27282462&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">陈胜垚</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%8D%97%E4%BA%AC%E7%90%86%E5%B7%A5%E5%A4%A7%E5%AD%A6%E7%94%B5%E5%AD%90%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%85%89%E7%94%B5%E6%8A%80%E6%9C%AF%E5%AD%A6%E9%99%A2&amp;code=0077991&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">南京理工大学电子工程与光电技术学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>基于带限相位相关的指静脉识别算法通常忽略带限窗口的影响, 其识别精度和速度有待提高。为此, 提出一种修正的带限相位相关算法。根据指静脉频谱图及其成像特征设计并提取带限和遍历参数, 利用相关系数峰值完成图像的匹配识别。在SDU-MLA、THU-FV和NUST-FV 3个指静脉数据库上进行实验, 结果表明, 该算法的指静脉识别速度和精度得到提高。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%94%9F%E7%89%A9%E8%AF%86%E5%88%AB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">生物识别;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%8C%87%E9%9D%99%E8%84%89&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">指静脉;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%B8%A6%E9%99%90%E7%9B%B8%E4%BD%8D%E7%9B%B8%E5%85%B3&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">带限相位相关;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%A2%91%E8%B0%B1%E5%9B%BE&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">频谱图;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%AF%86%E5%88%AB%E9%80%9F%E5%BA%A6&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">识别速度;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    雷蕾 (1986—) , 女, 副研究员、博士, 主研方向为图像处理、生物识别;E-mail: leilei4428@ 126. com;
                                </span>
                                <span>
                                    席峰, 副教授、博士。;
                                </span>
                                <span>
                                    陈胜垚, 副教授、博士。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-03-16</p>

            </div>
                    <h1><b>Finger Vein Recognition Algorithm Using Modified Band Limited Phase Only Correlation</b></h1>
                    <h2>
                    <span>LEI Lei</span>
                    <span>XI Feng</span>
                    <span>CHEN Shengyao</span>
            </h2>
                    <h2>
                    <span>School of Electronic and Optical Engineering, Nanjing University of Science and Technology</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>The influence of band limited parameters is usually ignored in finger vein recognition algorithm based on the Band Limited Phase Only Correlation (BLPOC) and its recognition accuracy and speed need to be improved.Thus, a novel algorithm using the modified BLPOC is proposed.The band limited and scanning range is determined using the spectrum and the features of finger vein image.Then, the correlation coefficient peak value is used to complete the match of finger vein image.Experiments are based on three finger vein databases, namely, SDU-MLA, THU-FV and NUST-FV, the results show that, the recognition accuracy and speed of the novel algorithm is effectively improved.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=biometric%20recognition&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">biometric recognition;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=finger%20vein&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">finger vein;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Band%20Limited%20Phase%20Only%20Correlation%20(BLPOC)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Band Limited Phase Only Correlation (BLPOC) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=spectrum%20image&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">spectrum image;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=recognition%20speed&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">recognition speed;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-03-16</p>
                            </div>


        <!--brief start-->
                        <h3 id="39" name="39" class="anchor-tag">0 概述</h3>
                <div class="p1">
                    <p id="40">随着信息技术的发展, 人们对信息安全的要求不断提高, 安全高效的识别技术具有广阔的发展前景。当前主流的识别方式有密码识别和生物识别。密码识别过程简洁, 但难以满足高安全性行业的要求, 且识别对象不唯一。生物识别基于人体生物特征来鉴别身份, 具有唯一性、高安全性、高精度等特点, 在安检、门禁、考试等场景下均得到了较好的应用<citation id="135" type="reference"><link href="3" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>。现阶段发展较为成熟的生物识别技术主要基于人脸<citation id="136" type="reference"><link href="5" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>、语音<citation id="137" type="reference"><link href="7" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>、指纹<citation id="138" type="reference"><link href="9" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>和虹膜<citation id="139" type="reference"><link href="11" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>等, 虽然这些技术能改善传统方式的不足, 但都存在缺点, 如精度低、易受环境影响、易损伤等。近年来, 随着计算机和生物医学技术的发展, 出现了基于手指静脉的生物识别方式。该技术因其需活体采集、唯一性好、安全稳定等优点, 受到国内外学者的广泛关注, 其相关的研究具有重要意义。</p>
                </div>
                <div class="p1">
                    <p id="41">匹配识别是指静脉识别系统的核心环节<citation id="140" type="reference"><link href="13" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>, 其直接影响识别性能。现有的指静脉识别算法根据匹配方式的不同可分为结构匹配和模板匹配。结构匹配需要提取额外的特征点, 对于指静脉特征点较少的情况, 则需采用多模态生物识别系统, 即利用指静脉与其他生物系统 (如指纹<citation id="141" type="reference"><link href="15" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>、指背纹<citation id="142" type="reference"><link href="17" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>、指纹及视网膜<citation id="143" type="reference"><link href="19" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>、人脸<citation id="144" type="reference"><link href="21" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>、虹膜<citation id="145" type="reference"><link href="23" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>等) 相结合的方式来实现, 其过程复杂, 计算开销较大。</p>
                </div>
                <div class="p1">
                    <p id="42">不同于结构匹配, 模板匹配方式无需提取特征点, 而是利用图像的全部结构信息来实现快速识别匹配, 因此, 该匹配技术日益受到关注。文献<citation id="146" type="reference">[<a class="sup">12</a>]</citation>首次提出相位相关 (Phase Only Correlation, POC) 模板匹配法, 该算法已在生物识别领域得到广泛应用<citation id="150" type="reference"><link href="27" rel="bibliography" /><link href="29" rel="bibliography" /><link href="31" rel="bibliography" /><sup>[<a class="sup">13</a>,<a class="sup">14</a>,<a class="sup">15</a>]</sup></citation>。事实上, 图像的有效信息主要集中在低频部分, 该算法却对全局信息进行处理, 极大地浪费了计算量。针对这一问题, 文献<citation id="147" type="reference">[<a class="sup">13</a>]</citation>提出利用图像低频分量进行计算的带限相位相关 (Band Limited Phase Only Correlation, BLPOC) 法。文献<citation id="148" type="reference">[<a class="sup">14</a>]</citation>在其基础上进一步将BLPOC算法应用于指静脉识别领域, 识别过程的计算量显著减少。然而, 文献<citation id="149" type="reference">[<a class="sup">14</a>]</citation>并未指出带限窗口参数的确定方法, 其识别效率有待提高。</p>
                </div>
                <div class="p1">
                    <p id="43">针对BLPOC算法存在的缺陷, 本文提出一种修正的带限相位相关 (Modified Band Limited Phase Only Correlation, MBPOC) 算法。对原始图像进行兴趣区域 (Region of Interest, ROI) 裁剪, 以初步减少计算量。根据指静脉频谱图计算带限的范围, 并采用BLPOC算法获取指静脉图像的相关函数。通过指静脉图像的偏移范围限定图像的遍历范围, 根据相关系数峰值以实现匹配识别。</p>
                </div>
                <h3 id="44" name="44" class="anchor-tag">1 BLPOC模型及问题描述</h3>
                <div class="p1">
                    <p id="45">传统POC算法通常采用相关函数来描述2个图像的相关性。该相关函数存在一个脉冲峰值, 且峰值大小随图像相关性的减弱而变小甚至消失, 因而可将其作为图像相关性的度量值<citation id="151" type="reference"><link href="25" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>。同时, BLPOC算法可使合法指静脉间的峰值更明显, 而对非法峰值的影响不大, 因此, BLPOC算法具有计算量少、识别精度高的优点<citation id="152" type="reference"><link href="27" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="46">对于尺寸为<i>m</i>×<i>n</i>的指静脉模板图像<b><i>t</i></b>和样本图像<b><i>s</i></b>, 令坐标变化范围为<mathml id="47"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>x</mi><mo>=</mo><mo>-</mo><mi>Μ</mi><mo>, </mo><mo>-</mo><mrow><mo> (</mo><mrow><mi>Μ</mi><mo>-</mo><mn>1</mn></mrow><mo>) </mo></mrow><mo>, </mo><mo>⋯</mo><mo>, </mo><mrow><mi>Μ</mi><mrow><mo> (</mo><mrow><mi>Μ</mi><mo>&gt;</mo><mn>0</mn></mrow><mo>) </mo></mrow></mrow><mo>, </mo><mi>y</mi><mo>=</mo><mo>-</mo><mi>Ν</mi><mo>, </mo><mo>-</mo><mrow><mo> (</mo><mrow><mi>Ν</mi><mo>-</mo><mn>1</mn></mrow><mo>) </mo></mrow><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>Ν</mi><mrow><mo> (</mo><mrow><mi>Ν</mi><mo>&gt;</mo><mn>0</mn></mrow><mo>) </mo></mrow></mrow></math></mathml>, 即满足条件<i>m</i>=2<i>M</i>+1, <i>n</i>=2<i>N</i>+1。<mathml id="48"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>t</mi><mrow><mo> (</mo><mrow><mi>x</mi><mo>, </mo><mi>y</mi></mrow><mo>) </mo></mrow></mrow></math></mathml>、<mathml id="49"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>s</mi><mrow><mo> (</mo><mrow><mi>x</mi><mo>, </mo><mi>y</mi></mrow><mo>) </mo></mrow></mrow></math></mathml>分别表示图像<b><i>t</i></b>和图像<b><i>s</i></b>在点<mathml id="50"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo> (</mo><mrow><mi>x</mi><mo>, </mo><mi>y</mi></mrow><mo>) </mo></mrow></mrow></math></mathml>处的灰度值, 则在<i>BLPOC</i>算法中, 两指静脉图像的最大相关系数R可表示为:</p>
                </div>
                <div class="p1">
                    <p id="51" class="code-formula">
                        <mathml id="51"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>R</mi><mo>=</mo><mrow><mi>max</mi></mrow><mo stretchy="false">{</mo><mi>r</mi><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo><mo stretchy="false">}</mo><mo>=</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mrow><mi>max</mi></mrow><mrow><mo>{</mo><mrow><mfrac><mn>1</mn><mrow><msup><mi>Μ</mi><mo>′</mo></msup><msup><mi>Ν</mi><mo>′</mo></msup></mrow></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>u</mi><mo>=</mo><mo>-</mo><msup><mi>Μ</mi><mo>′</mo></msup></mrow><msup><mi>Μ</mi><mo>′</mo></msup></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>u</mi><mo>=</mo><mo>-</mo><msup><mi>Ν</mi><mo>′</mo></msup></mrow><msup><mi>Ν</mi><mo>′</mo></msup></munderover><mi mathvariant="bold-italic">Ρ</mi></mstyle></mrow></mstyle><mo stretchy="false"> (</mo><mi>u</mi><mo>, </mo><mi>v</mi><mo stretchy="false">) </mo><mtext>e</mtext><msup><mrow></mrow><mrow><mtext>j</mtext><mn>2</mn><mtext>π</mtext><mrow><mo> (</mo><mrow><mfrac><mrow><mi>u</mi><mi>x</mi></mrow><msup><mi>Μ</mi><mo>′</mo></msup></mfrac><mo>+</mo><mfrac><mrow><mi>v</mi><mi>y</mi></mrow><msup><mi>Ν</mi><mo>′</mo></msup></mfrac></mrow><mo>) </mo></mrow></mrow></msup></mrow><mo>}</mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="52">其中, x∈ (-M′′, M′′) , y∈ (-N′′, N′′) , 表示相关系数遍历窗口的范围, M′和N′分别表示频带窗口在横纵方向的上限参数值, 且满足M&gt;M′≥M′′, N&gt;N′≥N′′, r (x, y) 代表指静脉图像<b><i>t</i></b>和<b><i>s</i></b>间的相关函数, <b><i>P</i></b>代表2幅图像的互功率谱函数, 具体表达形式为:</p>
                </div>
                <div class="p1">
                    <p id="53" class="code-formula">
                        <mathml id="53"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">Ρ</mi><mrow><mo> (</mo><mrow><mi>u</mi><mo>, </mo><mi>v</mi></mrow><mo>) </mo></mrow><mo>=</mo><mfrac><mrow><mi mathvariant="bold-italic">S</mi><mrow><mo> (</mo><mrow><mi>u</mi><mo>, </mo><mi>v</mi></mrow><mo>) </mo></mrow><mi mathvariant="bold-italic">Τ</mi><msup><mrow></mrow><mo>*</mo></msup><mrow><mo> (</mo><mrow><mi>u</mi><mo>, </mo><mi>v</mi></mrow><mo>) </mo></mrow></mrow><mrow><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">S</mi><mrow><mo> (</mo><mrow><mi>u</mi><mo>, </mo><mi>v</mi></mrow><mo>) </mo></mrow><mi mathvariant="bold-italic">Τ</mi><msup><mrow></mrow><mo>*</mo></msup><mrow><mo> (</mo><mrow><mi>u</mi><mo>, </mo><mi>v</mi></mrow><mo>) </mo></mrow></mrow><mo>|</mo></mrow></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="54">其中, <mathml id="55"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">S</mi><mrow><mo> (</mo><mrow><mi>u</mi><mo>, </mo><mi>v</mi></mrow><mo>) </mo></mrow></mrow></math></mathml>代表图像<b><i>s</i></b>的二维离散傅立叶变换, 其计算过程如下:</p>
                </div>
                <div class="p1">
                    <p id="56" class="code-formula">
                        <mathml id="56"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi mathvariant="bold-italic">S</mi><mo stretchy="false"> (</mo><mi>u</mi><mo>, </mo><mi>v</mi><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>x</mi><mo>=</mo><mo>-</mo><mi>Μ</mi></mrow><mi>Μ</mi></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>y</mi><mo>=</mo><mo>-</mo><mi>Ν</mi></mrow><mi>Ν</mi></munderover><mi>s</mi></mstyle></mrow></mstyle><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo><mtext>e</mtext><msup><mrow></mrow><mrow><mo>-</mo><mtext>j</mtext><mn>2</mn><mtext>π</mtext><mrow><mo> (</mo><mrow><mfrac><mrow><mi>u</mi><mi>x</mi></mrow><mi>Μ</mi></mfrac><mo>+</mo><mfrac><mrow><mi>v</mi><mi>y</mi></mrow><mi>Ν</mi></mfrac></mrow><mo>) </mo></mrow></mrow></msup><mo>=</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mi mathvariant="bold-italic">A</mi><msub><mrow></mrow><mi>S</mi></msub><mo stretchy="false"> (</mo><mi>u</mi><mo>, </mo><mi>v</mi><mo stretchy="false">) </mo><mtext>e</mtext><msup><mrow></mrow><mrow><mtext>j</mtext><mi>θ</mi><msub><mrow></mrow><mi>S</mi></msub><mo stretchy="false"> (</mo><mi>u</mi><mo>, </mo><mi>v</mi><mo stretchy="false">) </mo></mrow></msup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="57">其中, <b><i>A</i></b><sub><i>S</i></sub>和<i>θ</i><sub><i>S</i></sub>分别表示<mathml id="58"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">S</mi><mrow><mo> (</mo><mrow><mi>u</mi><mo>, </mo><mi>v</mi></mrow><mo>) </mo></mrow></mrow></math></mathml>的幅度和相位, <mathml id="59"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">Τ</mi><msup><mrow></mrow><mo>*</mo></msup><mrow><mo> (</mo><mrow><mi>u</mi><mo>, </mo><mi>v</mi></mrow><mo>) </mo></mrow></mrow></math></mathml>为图像<b><i>t</i></b>的二维离散傅立叶变换<mathml id="60"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">Τ</mi><mrow><mo> (</mo><mrow><mi>u</mi><mo>, </mo><mi>v</mi></mrow><mo>) </mo></mrow></mrow></math></mathml>的复共轭矩阵。</p>
                </div>
                <div class="p1">
                    <p id="61" class="code-formula">
                        <mathml id="61"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi mathvariant="bold-italic">Τ</mi><mo stretchy="false"> (</mo><mi>u</mi><mo>, </mo><mi>v</mi><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>x</mi><mo>=</mo><mo>-</mo><mi>Μ</mi></mrow><mi>Μ</mi></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>y</mi><mo>=</mo><mo>-</mo><mi>Ν</mi></mrow><mi>Ν</mi></munderover><mi>t</mi></mstyle></mrow></mstyle><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo><mtext>e</mtext><msup><mrow></mrow><mrow><mo>-</mo><mtext>j</mtext><mn>2</mn><mtext>π</mtext><mrow><mo> (</mo><mrow><mfrac><mrow><mi>u</mi><mi>x</mi></mrow><mi>Μ</mi></mfrac><mo>+</mo><mfrac><mrow><mi>v</mi><mi>y</mi></mrow><mi>Ν</mi></mfrac></mrow><mo>) </mo></mrow></mrow></msup><mo>=</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mi mathvariant="bold-italic">A</mi><msub><mrow></mrow><mi>Τ</mi></msub><mo stretchy="false"> (</mo><mi>u</mi><mo>, </mo><mi>v</mi><mo stretchy="false">) </mo><mtext>e</mtext><msup><mrow></mrow><mrow><mtext>j</mtext><mi>θ</mi><msub><mrow></mrow><mi>Τ</mi></msub><mo stretchy="false"> (</mo><mi>u</mi><mo>, </mo><mi>v</mi><mo stretchy="false">) </mo></mrow></msup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi mathvariant="bold-italic">Τ</mi><msup><mrow></mrow><mo>*</mo></msup><mo stretchy="false"> (</mo><mi>u</mi><mo>, </mo><mi>v</mi><mo stretchy="false">) </mo><mo>=</mo><mi mathvariant="bold-italic">A</mi><msub><mrow></mrow><mi>Τ</mi></msub><mo stretchy="false"> (</mo><mi>u</mi><mo>, </mo><mi>v</mi><mo stretchy="false">) </mo><mtext>e</mtext><msup><mrow></mrow><mrow><mo>-</mo><mtext>j</mtext><mi>θ</mi><msub><mrow></mrow><mi>Τ</mi></msub><mo stretchy="false"> (</mo><mi>u</mi><mo>, </mo><mi>v</mi><mo stretchy="false">) </mo></mrow></msup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>5</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="62">其中, <b><i>A</i></b><sub><i>T</i></sub>和<i>θ</i><sub><i>T</i></sub>分别代表的<mathml id="63"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">Τ</mi><mrow><mo> (</mo><mrow><mi>u</mi><mo>, </mo><mi>v</mi></mrow><mo>) </mo></mrow></mrow></math></mathml>幅度和相位。由式 (1) ～式 (5) 可知, 求解式 (1) 中2幅图像的相关性时, 影响其效率的参数主要有:</p>
                </div>
                <div class="p1">
                    <p id="64">1) 参与计算的图像尺寸m×n。原始指静脉图像的尺寸一般较大, 其不仅包含指静脉信息, 也包含手指边界背景等无用信息。为减少计算冗余, 在计算前需对原始指静脉图像进行<i>ROI</i>提取。本文根据手指静脉成像的特征, 对原始图像进行裁剪和尺度归一化, 在保留指静脉信息的前提下, 缩减m×n的范围, 从而大幅提高识别效率。</p>
                </div>
                <div class="p1">
                    <p id="65">2) 带限窗口M′×N′及遍历窗口M′′×N′′的尺寸。理论上, 窗口尺寸大, 函数所包含的信息多, 识别结果的准确性和稳健性就好, 但其需要更长的处理时间。反之, 窗口尺寸小虽能加快计算速度, 但有效信息也相应减少甚至丢失, 识别的准确性和稳健性也较差。因此, 合理地选取2个窗口的尺寸可保证算法的计算效率和精度。然而, 现有研究均从图像中心截取经验窗口作为计算区域<citation id="153" type="reference"><link href="9" rel="bibliography" /><link href="27" rel="bibliography" /><link href="33" rel="bibliography" /><sup>[<a class="sup">4</a>,<a class="sup">13</a>,<a class="sup">16</a>]</sup></citation>, 该方法虽简单易操作, 但无法确保算法的性能。对此, 本文提出一种能准确确定窗口参数的方法, 保证算法性能的最优化。</p>
                </div>
                <h3 id="66" name="66" class="anchor-tag">2 修正的带限相位相关算法</h3>
                <h4 class="anchor-tag" id="67" name="67">2.1 图像尺寸调节</h4>
                <div class="p1">
                    <p id="68">本文设计的指静脉采集器以手指第1个关节为中心, 对手指末节和中节的静脉进行采集。在采集过程中, 由于手指区域及物距较小, 手指的2个边缘可近似为2条直线。由于骨关节处具有较高的透光性, 因此在近红外透射下该区域的成像亮度较高。本文根据此特征来进行指静脉图像的<i>ROI</i>提取。具体步骤为:</p>
                </div>
                <div class="p1">
                    <p id="69">1) 沿手指边界预裁剪指静脉的候选窗口<b><i>W</i></b><sub>1</sub>, 尺寸大小为<i>m</i><sub>1</sub>×<i>n</i><sub>1</sub>, 其中, <i>n</i><sub>1</sub>为手指边界的最宽值, 如图1 (a) 所示。</p>
                </div>
                <div class="area_img" id="70">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201905030_070.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 指静脉图像ROI提取" src="Detail/GetImg?filename=images/JSJC201905030_070.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图1 指静脉图像ROI提取</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201905030_070.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="71">2) 对窗口<b><i>W</i></b><sub>1</sub>中每一行数据的像素进行累加求和:</p>
                </div>
                <div class="p1">
                    <p id="72" class="code-formula">
                        <mathml id="72"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>φ</mi><mrow><mo> (</mo><mi>i</mi><mo>) </mo></mrow><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>n</mi><msub><mrow></mrow><mn>1</mn></msub></mrow></munderover><mi mathvariant="bold-italic">W</mi></mstyle><msub><mrow></mrow><mn>1</mn></msub><mrow><mo> (</mo><mrow><mi>i</mi><mo>, </mo><mi>j</mi></mrow><mo>) </mo></mrow><mo>, </mo><mi>i</mi><mo>=</mo><mn>1</mn><mo>, </mo><mn>2</mn><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>m</mi><msub><mrow></mrow><mn>1</mn></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>6</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="73">3) <i>φ</i> (<i>i</i>) 最大值的行号即定义为指骨连接处<i>r</i><sub><i>k</i></sub>, 如图1 (b) 所示, 其计算过程如下:</p>
                </div>
                <div class="p1">
                    <p id="74" class="code-formula">
                        <mathml id="74"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>r</mi><msub><mrow></mrow><mi>k</mi></msub><mo>=</mo><mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mtext>a</mtext><mtext>r</mtext><mtext>g</mtext><mtext>m</mtext><mtext>a</mtext><mtext>x</mtext></mrow></mstyle><mrow><mi>i</mi><mo>∈</mo><mo stretchy="false">[</mo><mn>1</mn><mo>, </mo><mi>m</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">]</mo></mrow></munder></mrow><mspace width="0.25em" /><mo stretchy="false"> (</mo><mi>φ</mi><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>7</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="75">4) 按照<i>r</i><sub><i>k</i></sub>在图像2/3处的原则对图像进行裁剪, 得到窗口<b><i>W</i></b><sub>2</sub>:</p>
                </div>
                <div class="p1">
                    <p id="76" class="code-formula">
                        <mathml id="76"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo><mo>=</mo><mrow><mo>{</mo><mrow><mtable><mtr><mtd columnalign="left"><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mn>1</mn></msub><mrow><mo> (</mo><mrow><mrow><mo> (</mo><mrow><mi>x</mi><mo>+</mo><mfrac><mrow><mo stretchy="false"> (</mo><mn>3</mn><mo>⋅</mo><mi>r</mi><msub><mrow></mrow><mi>k</mi></msub><mo>-</mo><mi>m</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">) </mo></mrow><mn>2</mn></mfrac></mrow><mo>) </mo></mrow><mo>, </mo><mi>y</mi></mrow><mo>) </mo></mrow><mo>, </mo><mi>r</mi><msub><mrow></mrow><mi>k</mi></msub><mo>&gt;</mo><mfrac><mrow><mi>m</mi><msub><mrow></mrow><mn>1</mn></msub></mrow><mn>3</mn></mfrac></mtd></mtr><mtr><mtd columnalign="left"><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo><mo>, </mo><mi>r</mi><msub><mrow></mrow><mi>k</mi></msub><mo>≤</mo><mfrac><mrow><mi>m</mi><msub><mrow></mrow><mn>1</mn></msub></mrow><mn>3</mn></mfrac></mtd></mtr></mtable></mrow></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>8</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="77">其中, <mathml id="78"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>x</mi><mo>∈</mo><mrow><mo> (</mo><mrow><mn>1</mn><mo>, </mo><mn>3</mn><mi>r</mi><msub><mrow></mrow><mi>k</mi></msub></mrow><mo>) </mo></mrow><mo>, </mo><mi>y</mi><mo>∈</mo><mrow><mo> (</mo><mrow><mn>1</mn><mo>, </mo><mi>n</mi><msub><mrow></mrow><mn>1</mn></msub></mrow><mo>) </mo></mrow></mrow></math></mathml>。</p>
                </div>
                <div class="p1">
                    <p id="79">不同于本文采集方式, 部分研究方法会对整个手指静脉进行图像采集。对此, 由于手指的平面转动及粗细变化, 其边缘不再呈现平行状, 若继续按照上述方法对此类数据库进行<i>ROI</i>提取, 所得窗口<b><i>W</i></b><sub>2</sub>会包含无用的背景区域, 如图1 (e) 所示。因此, 需对<b><i>W</i></b><sub>2</sub>进一步定位裁剪。本文以<b><i>W</i></b><sub>2</sub>窗口内手指轮廓线的最窄处作为ROI提取的宽度, 对<b><i>W</i></b><sub>2</sub>进行裁剪, 得到<b><i>W</i></b><sub>3</sub>, 如图1 (f) 所示。</p>
                </div>
                <div class="p1">
                    <p id="80">值得注意的是, 在图像采集过程中, 不同手指的成像不同, 即使同一手指的成像也会因采集时刻、光线、角度等的影响而存在较大差别。因此, 图像的提取窗口<b><i>W</i></b><sub>2</sub> (或<b><i>W</i></b><sub>3</sub>) 在尺寸上也不同。为方便计算, 本文将ROI裁剪图像的尺寸归一化为<i>m</i>×<i>n</i>=150×100, 如图1 (c) 、图1 (g) 所示。</p>
                </div>
                <h4 class="anchor-tag" id="81" name="81">2.2 带限窗口选择</h4>
                <div class="p1">
                    <p id="82">频谱图可直接反映指静脉信息的分布, 其主要信息集中在低频部分, 高频部分为噪声<citation id="154" type="reference"><link href="29" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>。由于低频信息主要分布于频谱图的中心部分, 如图2 (<i>b</i>) 所示, 因此可选取该区域作为带限窗口M′×N′的范围。</p>
                </div>
                <div class="area_img" id="83">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201905030_083.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 指静脉图像的频谱图" src="Detail/GetImg?filename=images/JSJC201905030_083.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图2 指静脉图像的频谱图</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201905030_083.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="84">以往研究均采用固定窗口的方式<citation id="155" type="reference"><link href="9" rel="bibliography" /><link href="27" rel="bibliography" /><link href="33" rel="bibliography" /><sup>[<a class="sup">4</a>,<a class="sup">13</a>,<a class="sup">16</a>]</sup></citation>, 事实上, 图像最佳带限窗口的上限并非固定不变, 如图3所示。无论图像的频谱图如何改变, 期望均值窗口内都能包含大部分低频信息, 如图2 (c) 、图2 (d) 所示。因此, 可以通过频谱图在<i>x</i>方向和<i>y</i>方向映射的期望均值来界定窗口上限值。</p>
                </div>
                <div class="area_img" id="85">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201905030_085.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 图像样本对在不同带限范围内的相关性" src="Detail/GetImg?filename=images/JSJC201905030_085.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图3 图像样本对在不同带限范围内的相关性</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201905030_085.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="86">界定窗口上限值的具体过程如下:</p>
                </div>
                <div class="p1">
                    <p id="87">1) 根据式 (9) 拆分频谱图像<b><i>Sp</i></b>。</p>
                </div>
                <div class="p1">
                    <p id="88"><b><i>Sp</i></b>=<b><i>Sp</i></b><sub><i>l</i></sub>+<b><i>Sp</i></b><sub><i>h</i></sub>      (9) </p>
                </div>
                <div class="p1">
                    <p id="89">其中, <b><i>Sp</i></b><sub><i>l</i></sub>代表图像的低频信息, 即有效信息, <b><i>Sp</i></b><sub><i>h</i></sub>为去除低频信息的噪声部分。</p>
                </div>
                <div class="p1">
                    <p id="90">2) 假定低频信息为:</p>
                </div>
                <div class="p1">
                    <p id="91" class="code-formula">
                        <mathml id="91"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">S</mi><mi mathvariant="bold-italic">p</mi><msub><mrow></mrow><mi>l</mi></msub><mrow><mo> (</mo><mrow><mi>x</mi><mo>, </mo><mi>y</mi></mrow><mo>) </mo></mrow><mo>=</mo><mi mathvariant="bold-italic">S</mi><mi mathvariant="bold-italic">p</mi><mrow><mo> (</mo><mrow><mi>x</mi><mo>, </mo><mi>y</mi></mrow><mo>) </mo></mrow><mo>⋅</mo><mi mathvariant="bold-italic">B</mi><mrow><mo> (</mo><mrow><mi>x</mi><mo>, </mo><mi>y</mi></mrow><mo>) </mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>0</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="92">其中, <b><i>A</i></b>为<mathml id="93"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo> (</mo><mrow><mn>2</mn><msup><mi>Μ</mi><mo>′</mo></msup><mo>+</mo><mn>1</mn></mrow><mo>) </mo></mrow><mo>×</mo><mrow><mo> (</mo><mrow><mn>2</mn><msup><mi>Ν</mi><mo>′</mo></msup><mo>+</mo><mn>1</mn></mrow><mo>) </mo></mrow></mrow></math></mathml>维的全一矩阵, <b><i>B</i></b>为以<b><i>A</i></b>为中心其余数据均为0的矩阵, 具体如式 (11) 所示。</p>
                </div>
                <div class="p1">
                    <p id="94" class="code-formula">
                        <mathml id="94"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtext> </mtext><mtext> </mtext><mi mathvariant="bold-italic">B</mi><mo>=</mo><mrow><mo>[</mo><mrow><mtable><mtr><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd></mtr><mtr><mtd><mn>0</mn></mtd><mtd><mi mathvariant="bold-italic">A</mi></mtd><mtd><mn>0</mn></mtd></mtr><mtr><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd></mtr></mtable></mrow><mo>]</mo></mrow><msub><mrow></mrow><mrow><mi>m</mi><mo>×</mo><mi>n</mi></mrow></msub><mo>, </mo><mspace width="0.25em" /><mspace width="0.25em" /><mi mathvariant="bold-italic">A</mi><mo>=</mo><mrow><mo>[</mo><mrow><mtable><mtr><mtd><mn>1</mn></mtd><mtd><mn>1</mn></mtd><mtd><mo>⋯</mo></mtd><mtd><mn>1</mn></mtd></mtr><mtr><mtd><mn>1</mn></mtd><mtd><mn>1</mn></mtd><mtd><mo>⋯</mo></mtd><mtd><mn>1</mn></mtd></mtr><mtr><mtd><mo>⋮</mo></mtd><mtd><mo>⋮</mo></mtd><mtd></mtd><mtd><mo>⋮</mo></mtd></mtr><mtr><mtd><mn>1</mn></mtd><mtd><mn>1</mn></mtd><mtd><mo>⋯</mo></mtd><mtd><mn>1</mn></mtd></mtr></mtable></mrow><mo>]</mo></mrow><msub><mrow></mrow><mrow><mo stretchy="false"> (</mo><mn>2</mn><msup><mi>Μ</mi><mo>′</mo></msup><mo>+</mo><mn>1</mn><mo stretchy="false">) </mo><mo>×</mo><mo stretchy="false"> (</mo><mn>2</mn><msup><mi>Ν</mi><mo>′</mo></msup><mo>+</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="95">将频谱图<b><i>Sp</i></b>在<i>x</i>和<i>y</i>方向上进行分解, 通过式 (12) ～式 (14) 计算出<b><i>Sp</i></b>在<i>x</i>和<i>y</i>方向的投影值<mathml id="96"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>S</mi><mi>p</mi><msub><mrow></mrow><mi>x</mi></msub><mrow><mo> (</mo><mi>y</mi><mo>) </mo></mrow></mrow></math></mathml>和<mathml id="97"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>S</mi><mi>p</mi><msub><mrow></mrow><mi>y</mi></msub><mrow><mo> (</mo><mi>x</mi><mo>) </mo></mrow></mrow></math></mathml>, 以及频谱密度均值<mathml id="98"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mrow><mi>S</mi><mi>p</mi></mrow><mo stretchy="true">¯</mo></mover></mrow></math></mathml>。</p>
                </div>
                <div class="p1">
                    <p id="99" class="code-formula">
                        <mathml id="99"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi mathvariant="bold-italic">S</mi><mi mathvariant="bold-italic">p</mi><mo>=</mo><mrow><mo>{</mo><mrow><mtable><mtr><mtd><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mn>1</mn></msub></mtd></mtr><mtr><mtd><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mn>2</mn></msub></mtd></mtr><mtr><mtd><mo>⋮</mo></mtd></mtr><mtr><mtd><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>m</mi></msub></mtd></mtr></mtable></mrow><mo>}</mo></mrow><mo>=</mo><mrow><mo>{</mo><mrow><mtable><mtr><mtd><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mn>1</mn></msub></mtd><mtd><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mn>2</mn></msub></mtd><mtd><mo>⋯</mo></mtd><mtd><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>n</mi></msub></mtd></mtr></mtable></mrow><mo>}</mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>2</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mrow><mo>{</mo><mrow><mtable><mtr><mtd columnalign="left"><mi>S</mi><mi>p</mi><msub><mrow></mrow><mi>x</mi></msub><mrow><mo> (</mo><mi>i</mi><mo>) </mo></mrow><mo>=</mo><mo>〈</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi mathvariant="bold-italic">g</mi><msub><mrow></mrow><mi>x</mi></msub><mo>〉</mo></mtd></mtr><mtr><mtd columnalign="left"><mi>S</mi><mi>p</mi><msub><mrow></mrow><mi>y</mi></msub><mrow><mo> (</mo><mi>i</mi><mo>) </mo></mrow><mo>=</mo><mo>〈</mo><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi mathvariant="bold-italic">g</mi><msub><mrow></mrow><mi>y</mi></msub><mo>〉</mo></mtd></mtr><mtr><mtd columnalign="left"><mover accent="true"><mrow><mi>S</mi><mi>p</mi></mrow><mo stretchy="true">¯</mo></mover><mo>=</mo><mo>〈</mo><mi>S</mi><mi>p</mi><msub><mrow></mrow><mi>x</mi></msub><mo>, </mo><mi mathvariant="bold-italic">g</mi><msub><mrow></mrow><mi>y</mi></msub><mo>〉</mo><mo>=</mo><mo>〈</mo><mi>S</mi><mi>p</mi><msub><mrow></mrow><mi>y</mi></msub><mo>, </mo><mi mathvariant="bold-italic">g</mi><msub><mrow></mrow><mi>x</mi></msub><mo>〉</mo></mtd></mtr></mtable></mrow></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>3</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mrow><mo>{</mo><mrow><mtable><mtr><mtd><mi mathvariant="bold-italic">g</mi><msub><mrow></mrow><mi>x</mi></msub><mo>=</mo><mrow><mrow><mo> (</mo><mrow><mtable><mtr><mtd><mfrac><mn>1</mn><mi>n</mi></mfrac></mtd><mtd><mfrac><mn>1</mn><mi>n</mi></mfrac></mtd><mtd><mo>⋯</mo></mtd><mtd><mfrac><mn>1</mn><mi>n</mi></mfrac></mtd></mtr></mtable></mrow><mo>) </mo></mrow></mrow><msubsup><mrow></mrow><mrow><mn>1</mn><mo>×</mo><mi>n</mi></mrow><mtext>Τ</mtext></msubsup></mtd></mtr><mtr><mtd><mi mathvariant="bold-italic">g</mi><msub><mrow></mrow><mi>y</mi></msub><mo>=</mo><mrow><mrow><mo> (</mo><mrow><mtable><mtr><mtd><mfrac><mn>1</mn><mi>m</mi></mfrac></mtd><mtd><mfrac><mn>1</mn><mi>m</mi></mfrac></mtd><mtd><mo>⋯</mo></mtd><mtd><mfrac><mn>1</mn><mi>m</mi></mfrac></mtd></mtr></mtable></mrow><mo>) </mo></mrow></mrow><msub><mrow></mrow><mrow><mn>1</mn><mo>×</mo><mi>m</mi></mrow></msub></mtd></mtr></mtable></mrow></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>4</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="100">3) 借助图像在x、y方向的频谱密度均值, 计算出带限窗口的上限范围, 如图2 (<i>c</i>) 、图2 (<i>d</i>) 所示, 计算过程如下:</p>
                </div>
                <div class="p1">
                    <p id="101" class="code-formula">
                        <mathml id="101"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>{</mo><mrow><mtable><mtr><mtd columnalign="left"><msup><mi>Μ</mi><mo>′</mo></msup><mo>=</mo><mi>max</mi><mspace width="0.25em" /><mrow><mo> (</mo><mrow><mrow><mo>{</mo><mrow><msup><mi>Μ</mi><mo>′</mo></msup><mrow><mo>|</mo><mrow><mi>S</mi><mi>p</mi><msub><mrow></mrow><mi>y</mi></msub><mrow><mo> (</mo><msup><mi>Μ</mi><mo>′</mo></msup><mo>) </mo></mrow><mo>≥</mo><mover accent="true"><mrow><mi>S</mi><mi>p</mi></mrow><mo stretchy="true">¯</mo></mover><mo>, </mo><mn>0</mn><mo>&lt;</mo><msup><mi>Μ</mi><mo>′</mo></msup><mo>≤</mo><mi>Μ</mi></mrow></mrow></mrow><mo>}</mo></mrow></mrow><mo>) </mo></mrow></mtd></mtr><mtr><mtd columnalign="left"><msup><mi>Ν</mi><mo>′</mo></msup><mo>=</mo><mi>max</mi><mspace width="0.25em" /><mrow><mo> (</mo><mrow><mrow><mo>{</mo><mrow><msup><mi>Ν</mi><mo>′</mo></msup><mrow><mo>|</mo><mrow><mi>S</mi><mi>p</mi><msub><mrow></mrow><mi>x</mi></msub><mrow><mo> (</mo><msup><mi>Ν</mi><mo>′</mo></msup><mo>) </mo></mrow><mo>≥</mo><mover accent="true"><mrow><mi>S</mi><mi>p</mi></mrow><mo stretchy="true">¯</mo></mover><mo>, </mo><mn>0</mn><mo>&lt;</mo><msup><mi>Ν</mi><mo>′</mo></msup><mo>≤</mo><mi>Ν</mi></mrow></mrow></mrow><mo>}</mo></mrow></mrow><mo>) </mo></mrow><mspace width="0.25em" /></mtd></mtr></mtable></mrow></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>5</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="102">值得注意的是, 投影图关于原点呈中心对称, 因此带限窗口大小为<mathml id="103"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo> (</mo><mrow><mn>2</mn><msup><mi>Μ</mi><mo>′</mo></msup><mo>+</mo><mn>1</mn></mrow><mo>) </mo></mrow><mo>×</mo><mrow><mo> (</mo><mrow><mn>2</mn><msup><mi>Ν</mi><mo>′</mo></msup><mo>+</mo><mn>1</mn></mrow><mo>) </mo></mrow></mrow></math></mathml>。</p>
                </div>
                <h4 class="anchor-tag" id="104" name="104">2.3 遍历范围划定</h4>
                <div class="p1">
                    <p id="105">上文指出, 式 (1) 中相关函数r的峰值R可代表2幅图像的相似度, 进而可作为图像识别匹配的标准<citation id="156" type="reference"><link href="9" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>。然而, 若对整幅图像进行遍历计算, 会存在较大的计算冗余。已有文献证明峰值R即为图像偏移量<mathml id="106"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo> (</mo><mrow><mi>x</mi><msub><mrow></mrow><mn>0</mn></msub><mo>, </mo><mi>y</mi><msub><mrow></mrow><mn>0</mn></msub></mrow><mo>) </mo></mrow></mrow></math></mathml>处的r值<citation id="157" type="reference"><link href="35" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>, 若能确定<mathml id="107"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo> (</mo><mrow><mi>x</mi><msub><mrow></mrow><mn>0</mn></msub><mo>, </mo><mi>y</mi><msub><mrow></mrow><mn>0</mn></msub></mrow><mo>) </mo></mrow></mrow></math></mathml>的值或缩减r的遍历窗口, 便能减少求取峰值R时的计算量。本文识别匹配的图像已经过<i>ROI</i>提取、尺寸归一化等预处理, 因此只需讨论提取图像间的偏移变化。</p>
                </div>
                <div class="p1">
                    <p id="108">由于采集设备固定, 图像在采集时的物距和焦距几乎不变, 因此相似图像的尺寸变化可忽略不计。对同一手指在不同时刻采集的图像而言, 由于指骨连接位置固定不变, <i>ROI</i>提取后的图像位移量较小, 如图4所示。随机抽取16对<i>ROI</i>图像进行偏移量测试, 结果如图5所示, 其可进一步证明偏移量较小。由此, 可给定经验参数值来减小遍历窗口, 本文采取的参数为M′′=0.1M′, N′′=0.15N′。</p>
                </div>
                <div class="area_img" id="109">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201905030_109.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 不同时刻图像的ROI提取" src="Detail/GetImg?filename=images/JSJC201905030_109.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图4 不同时刻图像的ROI提取</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201905030_109.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="110">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201905030_110.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 相似ROI图像的偏移率" src="Detail/GetImg?filename=images/JSJC201905030_110.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图5 相似ROI图像的偏移率</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201905030_110.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h3 id="111" name="111" class="anchor-tag">3 实验验证与结果分析</h3>
                <div class="p1">
                    <p id="112">为验证本文算法的有效性, 开展了实验研究。本文实验所需的指静脉数据通过自制设备获取, 如图6所示。该设备主要由光源波段为850 nm的近红外LED阵列光源、手指放置凹槽和CMOS成像器件3个部分组成。其中, CMOS成像器为Ominvision公司的图像传感器OV7725, 镜头焦距为8 mm, 手指凹槽置于LED阵列和图像传感器之间, 距离均为10 mm。</p>
                </div>
                <div class="area_img" id="113">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201905030_113.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 指静脉采集设备" src="Detail/GetImg?filename=images/JSJC201905030_113.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图6 指静脉采集设备</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201905030_113.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="114">在进行采集时, 首先将手指背对LED光阵列水平放置在凹槽内, 然后调节近红外光照强度至合适范围, 最后通过采集按钮获取指静脉图像。采集的数据来源于40位志愿者, 男女比例3∶2, 年龄为20岁～35岁。分别对每位志愿者双手的食指和中指进行图像采集, 同一手指采集6次, 共计960幅指静脉图像, 分辨率均为640像素×480像素, 这些图像构成了本文的NUST-FV数据库。为更好地对本文算法进行测试, 本文还引用山东大学的SDU-MLA指静脉数据库<citation id="158" type="reference"><link href="37" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>以及清华大学的THU-FV指静脉数据库, 相关数据如表1所示。</p>
                </div>
                <div class="area_img" id="115">
                    <p class="img_tit"><b>表1 3种指静脉数据库的参数对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="115" border="1"><tr><td>数据库</td><td>人数</td><td>手指数/<br /> (根·人<sup>-1</sup>) </td><td>每根手指<br />采集次数</td><td>图像大小/<br /> (像素×像素) </td><td></td></tr><tr><td>THU-FV</td><td>610</td><td>1</td><td>4</td><td>576×720</td><td></td></tr><tr><td><br />SDU-MLA</td><td>106</td><td>6</td><td>6</td><td>320×240</td><td></td></tr><tr><td><br />NUST-FV</td><td>40</td><td>4</td><td>6</td><td>640×480</td><td></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="116">基于不同数据库, 对时间效率、拒真率和认假率、识别率等系统性能指标进行测评。</p>
                </div>
                <h4 class="anchor-tag" id="117" name="117">3.1 时间效率评测</h4>
                <div class="p1">
                    <p id="118">比较本文的MBPOC算法和传统的POC、BLPOC算法可知, 其主要区别在于循环次数, 即带限范围和遍历范围取值不同, 具体如表2所示。</p>
                </div>
                <div class="area_img" id="119">
                    <p class="img_tit"><b>表2 3种算法循环次数对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="119" border="1"><tr><td><br />算法</td><td>带限范围</td><td>遍历范围</td></tr><tr><td><br />POC算法</td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo> (</mo><mrow><mn>2</mn><mi>Μ</mi><mo>+</mo><mn>1</mn></mrow><mo>) </mo></mrow><mo>×</mo><mrow><mo> (</mo><mrow><mn>2</mn><mi>Ν</mi><mo>+</mo><mn>1</mn></mrow><mo>) </mo></mrow></mrow></math></td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo> (</mo><mrow><mn>2</mn><mi>Μ</mi><mo>+</mo><mn>1</mn></mrow><mo>) </mo></mrow><mo>×</mo><mrow><mo> (</mo><mrow><mn>2</mn><mi>Ν</mi><mo>+</mo><mn>1</mn></mrow><mo>) </mo></mrow></mrow></math></td></tr><tr><td><br /><i>BLPOC</i>算法</td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo> (</mo><mrow><mn>2</mn><msup><mi>Μ</mi><mo>′</mo></msup><mo>+</mo><mn>1</mn></mrow><mo>) </mo></mrow><mo>×</mo><mrow><mo> (</mo><mrow><mn>2</mn><msup><mi>Ν</mi><mo>′</mo></msup><mo>+</mo><mn>1</mn></mrow><mo>) </mo></mrow></mrow></math></td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo> (</mo><mrow><mn>2</mn><mi>Μ</mi><mo>+</mo><mn>1</mn></mrow><mo>) </mo></mrow><mo>×</mo><mrow><mo> (</mo><mrow><mn>2</mn><mi>Ν</mi><mo>+</mo><mn>1</mn></mrow><mo>) </mo></mrow></mrow></math></td></tr><tr><td><br /><i>MBPOC</i>算法</td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo> (</mo><mrow><mn>2</mn><msup><mi>Μ</mi><mo>′</mo></msup><mo>+</mo><mn>1</mn></mrow><mo>) </mo></mrow><mo>×</mo><mrow><mo> (</mo><mrow><mn>2</mn><msup><mi>Ν</mi><mo>′</mo></msup><mo>+</mo><mn>1</mn></mrow><mo>) </mo></mrow></mrow></math></td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo> (</mo><mrow><mn>2</mn><msup><msup><mi>Μ</mi><mo>′</mo></msup><mo>′</mo></msup><mo>+</mo><mn>1</mn></mrow><mo>) </mo></mrow><mo>×</mo><mrow><mo> (</mo><mrow><mn>2</mn><msup><msup><mi>Ν</mi><mo>′</mo></msup><mo>′</mo></msup><mo>+</mo><mn>1</mn></mrow><mo>) </mo></mrow></mrow></math></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="120">假定算法总复杂度为O, 忽略功率谱密度的计算, 整个识别算法主要包括:傅里叶变换求<b><i>S</i></b>和<b><i>T</i></b>, 反傅里叶变换求<i>r</i>以及遍历求最大值<i>R</i> 3个部分。假定单条指令的时间开销分别为<i>A</i>、<i>B</i>和<i>C</i>, 可得POC算法、BLPOC算法及MBPOC算法的时间开销<i>O</i><sub>POC</sub>、<i>O</i><sub>BLPOC</sub>及<i>O</i><sub>MBPOC</sub>, 计算过程如下:</p>
                </div>
                <div class="p1">
                    <p id="121" class="code-formula">
                        <mathml id="121"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mi>Ο</mi><msub><mrow></mrow><mrow><mtext>Ρ</mtext><mtext>Ο</mtext><mtext>C</mtext></mrow></msub><mo>=</mo><mrow><mo> (</mo><mrow><mn>2</mn><mi>Μ</mi><mo>+</mo><mn>1</mn></mrow><mo>) </mo></mrow><mo>×</mo><mrow><mo> (</mo><mrow><mn>2</mn><mi>Ν</mi><mo>+</mo><mn>1</mn></mrow><mo>) </mo></mrow><mo>×</mo><mrow><mo> (</mo><mrow><mi>A</mi><mo>+</mo><mi>B</mi><mo>+</mo><mi>C</mi></mrow><mo>) </mo></mrow></mtd></mtr><mtr><mtd><mi>Ο</mi><msub><mrow></mrow><mrow><mtext>B</mtext><mtext>L</mtext><mtext>Ρ</mtext><mtext>Ο</mtext><mtext>C</mtext></mrow></msub><mo>=</mo><mrow><mo> (</mo><mrow><mn>2</mn><mi>Μ</mi><mo>+</mo><mn>1</mn></mrow><mo>) </mo></mrow><mo>×</mo><mrow><mo> (</mo><mrow><mn>2</mn><mi>Ν</mi><mo>+</mo><mn>1</mn></mrow><mo>) </mo></mrow><mo>×</mo><mi>A</mi><mo>+</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mrow><mo> (</mo><mrow><mn>2</mn><msup><mi>Μ</mi><mo>′</mo></msup><mo>+</mo><mn>1</mn></mrow><mo>) </mo></mrow><mo>×</mo><mrow><mo> (</mo><mrow><mn>2</mn><msup><mi>Ν</mi><mo>′</mo></msup><mo>+</mo><mn>1</mn></mrow><mo>) </mo></mrow><mo>×</mo><mi>B</mi><mo>+</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mrow><mo> (</mo><mrow><mn>2</mn><mi>Μ</mi><mo>+</mo><mn>1</mn></mrow><mo>) </mo></mrow><mo>×</mo><mrow><mo> (</mo><mrow><mn>2</mn><mi>Ν</mi><mo>+</mo><mn>1</mn></mrow><mo>) </mo></mrow><mo>×</mo><mi>C</mi></mtd></mtr><mtr><mtd><mi>Ο</mi><msub><mrow></mrow><mrow><mtext>Μ</mtext><mtext>B</mtext><mtext>Ρ</mtext><mtext>Ο</mtext><mtext>C</mtext></mrow></msub><mo>=</mo><mrow><mo> (</mo><mrow><mn>2</mn><mi>Μ</mi><mo>+</mo><mn>1</mn></mrow><mo>) </mo></mrow><mo>×</mo><mrow><mo> (</mo><mrow><mn>2</mn><mi>Ν</mi><mo>+</mo><mn>1</mn></mrow><mo>) </mo></mrow><mo>×</mo><mi>A</mi><mo>+</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mrow><mo> (</mo><mrow><mn>2</mn><msup><mi>Μ</mi><mo>′</mo></msup><mo>+</mo><mn>1</mn></mrow><mo>) </mo></mrow><mo>×</mo><mrow><mo> (</mo><mrow><mn>2</mn><msup><mi>Ν</mi><mo>′</mo></msup><mo>+</mo><mn>1</mn></mrow><mo>) </mo></mrow><mo>×</mo><mi>B</mi><mo>+</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mrow><mo> (</mo><mrow><mn>2</mn><msup><msup><mi>Μ</mi><mo>′</mo></msup><mo>′</mo></msup><mo>+</mo><mn>1</mn></mrow><mo>) </mo></mrow><mo>×</mo><mrow><mo> (</mo><mrow><mn>2</mn><msup><msup><mi>Ν</mi><mo>′</mo></msup><mo>′</mo></msup><mo>+</mo><mn>1</mn></mrow><mo>) </mo></mrow><mo>×</mo><mi>C</mi></mtd></mtr></mtable></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>6</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="122">由前文可知, <i>M</i>×<i>N</i>&gt;<i>M</i>′×<i>N</i>′≥<i>M</i>′′×<i>N</i>′′, 则有:<i>O</i><sub>POC</sub>&gt;<i>O</i><sub>BLPOC</sub>≥<i>O</i><sub>MBPOC</sub>。因此在理论上, 本文算法可有效降低计算的复杂度, 进而提高识别的速度。</p>
                </div>
                <div class="p1">
                    <p id="123">同时, 本文从上述3个数据库中随机选取指静脉图像对, 并测量不同算法的识别速度。通过CPU计算时间来表征识别速度, 且计时只包含识别算法部分。仿真采用的计算机硬件配置为主频3.3 GHz的英特尔酷睿i5-4590处理器和4 GB内存, 仿真环境为 Matlab 2011b。时间效率对比如图7所示, 由图7可知, MBPOC算法的计算速度比其他2种算法更快。图像对的平均耗时统计结果如表3所示。由表3可知, MBPOC算法的指静脉识别速度更快, 进一步证明本文算法可获得更快的处理速度。</p>
                </div>
                <div class="area_img" id="124">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201905030_124.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 3种算法在不同数据集上的时间效率对比" src="Detail/GetImg?filename=images/JSJC201905030_124.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图7 3种算法在不同数据集上的时间效率对比</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201905030_124.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="125">
                    <p class="img_tit"><b>表3 3种算法在不同数据库上所用时间的对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="125" border="1"><tr><td>数据库</td><td>算法</td><td>平均耗时<br />/ms</td><td>提高百分比/%</td></tr><tr><td rowspan="3"><br />SDU-MLA</td><td><br />POC算法</td><td>5.2</td><td>—</td></tr><tr><td><br />BLPOC算法</td><td>4.1</td><td>21.2</td></tr><tr><td><br />MBPOC算法</td><td>2.8</td><td>46.2</td></tr><tr><td rowspan="3"><br />THU-FV</td><td><br />POC算法</td><td>5.9</td><td>—</td></tr><tr><td><br />BLPOC算法</td><td>3.9</td><td>33.9</td></tr><tr><td><br />MBPOC算法</td><td>2.6</td><td>55.9</td></tr><tr><td rowspan="3"><br />NUST-FV</td><td><br />POC算法</td><td>6.3</td><td>—</td></tr><tr><td><br />BLPOC算法</td><td>4.1</td><td>34.9</td></tr><tr><td><br />MBPOC算法</td><td>2.9</td><td>54.0</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="126" name="126">3.2 拒真率和认假率评测</h4>
                <div class="p1">
                    <p id="127">拒真率 (False Rejection Rate, FRR) 和认假率 (False Acceptance Rate, FAR) 是评估指静脉识别系统性能的2个重要指标。其中, 拒真率代表将合法指静脉图像识别成非法的概率, 认假率则表示将非法指静脉图像标识成合法的概率<citation id="159" type="reference"><link href="15" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>。本文采取NUST-FV数据库进行测试, 通过统计数据库中960幅图像间的识别结果来获取拒真率。数据库指静脉图像共40×4=160类, 其中任一图像的合法匹配次数为6×5=30次, 故数据库的总合法匹配次数为160×30=4 800。假定实验中误拒的总次数为<i>N</i>, 则<i>FRR</i>=<i>N</i>/4 800。对于<i>FAR</i>值的计算, 则选取实验数据库中的80类指静脉图像作为模板数据, 共80×6=480幅, 剩余的80类则作为样本数据。此时, 非法匹配总次数为:480×480=230 400, 假定实验中误识总次数为<i>N</i>′, 则<i>FAR</i>=<i>N</i>′/230 400。</p>
                </div>
                <div class="p1">
                    <p id="128">为直观地反映不同阈值条件下<i>FAR</i>和<i>FRR</i>的关系, 本文统计了3种算法的受试者工作特征 (Receiver Operating Characteristic, ROC) 曲线, 如图8所示。ROC曲线越接近坐标轴, <i>FAR</i>和<i>FRR</i>值就越低, 算法性能越好<citation id="160" type="reference"><link href="37" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>。由图8计算可知, 当<i>FAR</i>=0时, MBPOC算法、BLPOC算法、POC算法的<i>FFR</i>值分别为4.93%、7.86%和27.16%, 且MBPOC算法的ROC曲线均位于POC算法和BLPOC算法曲线的下方, 表明相比于POC和BLPOC算法, MBPOC能达到更小的<i>FAR</i>和<i>FRR</i>。等识率 (Equal Error Rate, EER) 代表<i>FAR</i>=<i>FRR</i>时的值, 可用于综合评估算法的识别性能, 在图8中, MBPOC算法、BLPOC算法和POC算法的<i>EER</i>值分别为2.05%、3.35%和7.60%, MBPOC算法和BLPOC算法的<i>EER</i>值相差不大, 但POC算法的<i>EER</i>值约为MBPOC算法的3倍, 这也证明了本文MBPOC算法的性能更优。</p>
                </div>
                <div class="area_img" id="129">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201905030_129.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 3种算法的ROC曲线" src="Detail/GetImg?filename=images/JSJC201905030_129.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图8 3种算法的ROC曲线</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201905030_129.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="130" name="130">3.3 识别率评测</h4>
                <div class="p1">
                    <p id="131">此外, 本文还测量了MBPOC、POC和BLPOC 3种算法对于NUST-FV数据库中960幅合法图像和480幅非法图像的相关系数散点分布情况, 通过识别率直观地描述算法的优劣, 结果如图9所示。在图9 (a) 中, 当MBPOC算法给定阈值为0.5时, 系统误识率为5.5%, 在同等条件下, POC算法的误识率却高达37.3%。在图9 (b) 中, 对于合法图像, MBPOC算法和BLPOC算法的相关系数值相同, 但对于非法图像, MBPOC算法的相关系数值均小于或等于BLPOC算法。这表明在合法匹配中, 2种算法的效果相同, 但对于非法匹配, MBPOC算法的相关系数值则小于或等于BLPOC算法, 其误识率更低。因此, 本文MBPOC算法的识别精度优于传统的POC算法和BLPOC算法。</p>
                </div>
                <div class="area_img" id="132">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201905030_132.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图9 3种算法的相关系数分布" src="Detail/GetImg?filename=images/JSJC201905030_132.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图9 3种算法的相关系数分布</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201905030_132.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h3 id="133" name="133" class="anchor-tag">4 结束语</h3>
                <div class="p1">
                    <p id="134">本文提出一种基于MBPOC的指静脉识别算法。通过ROI裁剪减少算法的计算量, 并在BLPOC技术的基础上, 提出带限窗口的计算方法。根据图像的偏移范围来限定遍历范围, 利用相关系数峰值实现匹配识别。实验结果表明, 该算法识别效率和精度较高。下一步将对匹配决策的算法进行研究, 以构造一套完整、高效的指静脉识别系统。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13050900118197&amp;v=MTMyMzl3WmVadUh5am1VTG5JSlYwVmJ4ST1OaWZPZmJLN0h0VE1wbzlGWmVvSERYVStvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbg==&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> LIN Chunyi, LI Mingzhong, SUN Xiao.A finger vein recognition algorithm based on gradient correlation[C]//Proceedings of AASRI Conference on Intelligence and Bioinformatics.Berlin, Germany:Springer, 2012:40-45.
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000017620&amp;v=MTcwNTF1SHlqbVVMbklKVjBWYnhJPU5pZklZN0s3SHRqTnI0OUZaT29JQ240NW9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWg==&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> ZHAO Wenyi, CHELLAPPA R, PHILLIPS P J, et al.Face recognition:a literature survey[J].ACM Computing Surveys, 2003, 35 (4) :399-458.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012300465888&amp;v=MDMwNjNPZmJLN0h0RE9ySTlGWU8wS0JIUXhvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVVMbklKVjBWYnhJPU5pZg==&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> JIANG Hui.Confidence measures for speech recognition:a survey[J].Speech Communication, 2005, 45 (4) :455-470.
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Fingerprint Matching Algorithm Using Phase-Only Correlation">

                                <b>[4]</b> ITO K, NAKAJIMA H, KOBAYASHI K, et al.A fingerprint matching algorithm using phase-only correlation[J].IEICE Transactions on Fundamentals of Electronics Communications and Computer Sciences, 2004, 87 (3) :682-691.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=User Identification Based on Finger-vein Patterns for Consumer Electronics Devices">

                                <b>[5]</b> WANG Desong, LI Jianping, MEMIK G.User identification based on finger-vein patterns for consumer electronics devices[J].IEEE Transactions on Consumer Electronics, 2010, 56 (2) :799-804.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002056992&amp;v=MzI3NjVWND1OajdCYXJPNEh0SE9yNHBEYmVJTlkzazV6QmRoNGo5OVNYcVJyeG94Y01IN1I3cWVidWR0RkM3bFZyekpK&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b> MIURA N, NAGASAKA A, MIYATAKE T.Feature extraction of finger-vein patterns based on repeated line tracking and its application to personal identification[J].Machine Vision and Applications, 2004, 15 (4) :194-203.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Human Identification Using Finger Images">

                                <b>[7]</b> KUMAR A, ZHOU Y B.Human identification using finger images[J].IEEE Transactions on Image Processing, 2012, 21 (4) :2228-2244.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14010600065106&amp;v=MjM4MTgwS0RYdy9vQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVVMbklKVjBWYnhJPU5pZk9mYks4SHRETXFZOUZaTw==&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b> YANG Wenming, HUANG Xiaola, ZHOU Fei, et al.Comparative competitive coding for personal identification by using finger vein and finger dorsal texture fusion[J].Information Sciences, 2014, 268 (6) :20-32.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Enhancing security for multimodal biometric using hyper image encryption algorithm">

                                <b>[9]</b> NIVETHA K, SARASWADY D.Enhancing security for multimodal biometric using hyper image encryption algorithm[C]//Proceedings of International Conference on Electronics and Communication Systems.Washington D.C., USA:IEEE Press, 2015:943-947.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multi model personal authentication using finger vein and face images">

                                <b>[10]</b> MANJUNATHSWAMY B E, THRUVENI J, VENUGOPAL K R, et al.Multi model personal authentication using finger vein and face images[C]//Proceedings of International Conference on Parallel, Distributed and Grid Computing.Washington D.C., USA:IEEE Press, 2015:339-344.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fusion at decision level in multimodal biometric authentication system using iris and finger vein with novel feature extraction">

                                <b>[11]</b> SUDHAMANI M J, VENKATESHA M K, RADHIKA K R.Fusion at decision level in multimodal biometric authentication system using iris and finger vein with novel feature extraction[C]//Proceedings of 2014 Annual IEEE India Conference.Washington D.C., USA:IEEE Press, 2014:1-6.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Design elements of binary phase-only correlation filters">

                                <b>[12]</b> FLANNERY D L, LOOMIS J S, MILKOVICH M E.Design elements of binary phase-only correlation filters[J].Applied Optics, 1988, 27 (20) :4231-4235.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=An effective approach for Iris recognition using phase-based image matching">

                                <b>[13]</b> MIYAZAWA K, ITO K, AOKI T, et al.An effective approach for iris recognition using phase-based image matching[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2008, 30 (10) :1741-1756.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Finger-vein recognition algorithm using phase only correlation">

                                <b>[14]</b> MAHRI N, SUANDI S A S, ROSDI B A.Finger vein recognition algorithm using phase only correlation[C]//Proceedings of International Workshop on Emerging Techniques and Challenges for Hand-Based Biometrics.Washington D.C., USA:IEEE Press, 2010:1-6.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A palmprint recognition algorithm using phase-only correlation">

                                <b>[15]</b> ITO K, AOKI T, NAKAJIMA H, et al.A palmprint recognition algorithm using phase-only correlation[J].IEICE Transactions on Fundamentals of Electronics Communications and Computer Sciences, 2008, 91 (4) :1023-1030.
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_16" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14032300110907&amp;v=MjQxMjN1SHlqbVVMbklKVjBWYnhJPU5pZk9mYks4SHRMT3JJOUZaZW9QQlh3K29CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWg==&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[16]</b> ASAARI M S M, SUANDI S A, ROSDI B A.Fusion of band limited phase only correlation and width centroid contour distance for finger based biometrics[J].Expert Systems with Applications, 2014, 41 (7) :3367-3382.
                            </a>
                        </p>
                        <p id="35">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Novel algorithm for fingerprint mosaicing using phase correlation method">

                                <b>[17]</b> BHATI S H, PATI U C.Novel algorithm for fingerprint mosaicing using phase correlation method[C]//Proceedings of Global Conference on Communication Technologies.Washington D.C., USA:IEEE Press, 2015:29-33.
                            </a>
                        </p>
                        <p id="37">
                            <a id="bibliography_18" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJF201103002&amp;v=MDE2NzdycklMejdCYUxHNEg5RE1ySTlGWm9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJvRnkzbVU=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[18]</b> 王科俊, 马慧.使用改进的方向滤波与修正的Hausdorff距离的指静脉识别方法[J].计算机辅助设计与图形学学报, 2011, 23 (3) :385-391.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJC201905030" />
        <input id="dpi" type="hidden" value="600" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201905030&amp;v=MTczNTQ5R1pJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSb0Z5M21VcnJJTHo3QmJiRzRIOWpNcW8=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
