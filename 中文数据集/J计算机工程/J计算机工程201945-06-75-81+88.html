<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637130373760301250%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJC201906012%26RESULT%3d1%26SIGN%3dHNw50A%252fOztUqZc7w%252b3ZPNlT1xx4%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201906012&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201906012&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201906012&amp;v=MjM2NjM0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSb0ZpRGhWTC9QTHo3QmJiRzRIOWpNcVk5RVpvUUtESDg=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#37" data-title="0 概述 ">0 概述</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#41" data-title="1 相关算法 ">1 相关算法</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#42" data-title="1.1 传统加权模糊C均值算法">1.1 传统加权模糊C均值算法</a></li>
                                                <li><a href="#51" data-title="1.2 扩展增量聚类算法">1.2 扩展增量聚类算法</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#55" data-title="2 改进的增量模糊聚类算法 ">2 改进的增量模糊聚类算法</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#56" data-title="2.1 初始聚类中心优化">2.1 初始聚类中心优化</a></li>
                                                <li><a href="#77" data-title="2.2 类间中心点影响">2.2 类间中心点影响</a></li>
                                                <li><a href="#93" data-title="2.3 改进的余弦距离加权FCM算法">2.3 改进的余弦距离加权FCM算法</a></li>
                                                <li><a href="#118" data-title="2.4 算法步骤">2.4 算法步骤</a></li>
                                                <li><a href="#149" data-title="2.5 算法复杂度分析">2.5 算法复杂度分析</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#153" data-title="3 实验结果与分析 ">3 实验结果与分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#154" data-title="3.1 实验设置">3.1 实验设置</a></li>
                                                <li><a href="#160" data-title="3.2 算法性能分析">3.2 算法性能分析</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#171" data-title="4 结束语 ">4 结束语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#151" data-title="&lt;b&gt;表1 各算法的时间和空间复杂度对比&lt;/b&gt;"><b>表1 各算法的时间和空间复杂度对比</b></a></li>
                                                <li><a href="#157" data-title="&lt;b&gt;表2 实验1数据集信息&lt;/b&gt;"><b>表2 实验1数据集信息</b></a></li>
                                                <li><a href="#159" data-title="&lt;b&gt;表3 实验2数据集信息&lt;/b&gt;"><b>表3 实验2数据集信息</b></a></li>
                                                <li><a href="#162" data-title="&lt;b&gt;表4 未进行分块处理时2种算法的ARI值对比&lt;/b&gt;"><b>表4 未进行分块处理时2种算法的ARI值对比</b></a></li>
                                                <li><a href="#163" data-title="&lt;b&gt;表5 9种聚类算法的ARI值对比&lt;/b&gt;"><b>表5 9种聚类算法的ARI值对比</b></a></li>
                                                <li><a href="#169" data-title="&lt;b&gt;图1 spF (c+l) M 和oF (c+l) M在4个数据集上运用2种初始化方法效果对比&lt;/b&gt;"><b>图1 spF (c+l) M 和oF (c+l) M在4个数据集上运用2种初始化方法效果对比</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" title=" TANG Tinglong, CHEN Shengyong, ZHAO Meng, et al.Very large-scale data classification based on K-means clustering and multi-kernel SVM[J].Soft Computing, 2018 (1) :1-9." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Very large-scale data classification based on K-means clustering and multi-kernel SVM">
                                        <b>[1]</b>
                                         TANG Tinglong, CHEN Shengyong, ZHAO Meng, et al.Very large-scale data classification based on K-means clustering and multi-kernel SVM[J].Soft Computing, 2018 (1) :1-9.
                                    </a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" title=" CASTELLANO G, FANELLI A M.Classification of data streams by incremental semi-supervised fuzzy clustering[M]//PETROSINO A, LOIA V, PEDRYCZ W.Fuzzy logic and soft computing applications.Berlin, Germany:Springer, 2016:185-194." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Classification of data streams by incremental semi-supervised fuzzy clustering">
                                        <b>[2]</b>
                                         CASTELLANO G, FANELLI A M.Classification of data streams by incremental semi-supervised fuzzy clustering[M]//PETROSINO A, LOIA V, PEDRYCZ W.Fuzzy logic and soft computing applications.Berlin, Germany:Springer, 2016:185-194.
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title=" WU Xidong, ZHU Xingquan, WU Gongqing, et al.Data mining with big data[J].IEEE Transactions on Knowledge and Data Engineering, 2014, 26 (1) :97-107." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Data mining with big data">
                                        <b>[3]</b>
                                         WU Xidong, ZHU Xingquan, WU Gongqing, et al.Data mining with big data[J].IEEE Transactions on Knowledge and Data Engineering, 2014, 26 (1) :97-107.
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" title=" CAN F, DROCHAK N D I.Incremental clustering for dynamic document databases[C]//Proceedings of 1990 Symposium on Applied Computing.Washington D.C., USA:IEEE Press, 1990:61-67." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Incremental clustering for dynamic document databases">
                                        <b>[4]</b>
                                         CAN F, DROCHAK N D I.Incremental clustering for dynamic document databases[C]//Proceedings of 1990 Symposium on Applied Computing.Washington D.C., USA:IEEE Press, 1990:61-67.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" title=" 陈卓, 贺明霞, 刘相双.基于扩展凝聚点和网格的增量聚类算法[J].哈尔滨工业大学学报, 2006, 38 (8) :1382-1385." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=HEBX200608045&amp;v=MjYzMDdUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJvRmlEaFZML1BMU2pKZHJHNEh0Zk1wNDlCWVlRS0RIODR2UjQ=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                         陈卓, 贺明霞, 刘相双.基于扩展凝聚点和网格的增量聚类算法[J].哈尔滨工业大学学报, 2006, 38 (8) :1382-1385.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" title=" 孟凡荣, 李晓翠, 周勇.一种基于代表点的增量聚类算法[J].计算机应用研究, 2012, 29 (8) :2865-2867." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSYJ201208018&amp;v=MjY1ODhIOVBNcDQ5RWJJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSb0ZpRGhWTC9QTHo3U1pMRzQ=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                         孟凡荣, 李晓翠, 周勇.一种基于代表点的增量聚类算法[J].计算机应用研究, 2012, 29 (8) :2865-2867.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" title=" KOTHARI D, NARAYANAN S T, DEVI K K.Extended fuzzy c-means with random sampling techniques for clustering large data[J].International Journal of Innovative Research in Advanced Engineering, 2014, 1 (1) :1-4." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Extended fuzzy c-means with random sampling techniques for clustering large data">
                                        <b>[7]</b>
                                         KOTHARI D, NARAYANAN S T, DEVI K K.Extended fuzzy c-means with random sampling techniques for clustering large data[J].International Journal of Innovative Research in Advanced Engineering, 2014, 1 (1) :1-4.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" title=" HORE P, HALL L O, GOLDGOF D B, et al.Online fuzzy c means[C]//Proceedings of NAFIPS’08.Washington D.C., USA:IEEE Press, 2008:1-5." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Online fuzzy C means">
                                        <b>[8]</b>
                                         HORE P, HALL L O, GOLDGOF D B, et al.Online fuzzy c means[C]//Proceedings of NAFIPS’08.Washington D.C., USA:IEEE Press, 2008:1-5.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" title=" HORE P, HALL L, GOLDGOF D.Single pass fuzzy C means[C]//Proceedings of IEEE International Fuzzy Systems Conference.Washington D.C., USA:IEEE Press, 2007:1-7." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Single pass fuzzy c means">
                                        <b>[9]</b>
                                         HORE P, HALL L, GOLDGOF D.Single pass fuzzy C means[C]//Proceedings of IEEE International Fuzzy Systems Conference.Washington D.C., USA:IEEE Press, 2007:1-7.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_10" title=" HAVENS T C, BEZDEK J C, LECKIE C, et al.Fuzzy c-means algorithms for very large data[J].IEEE Transactions on Fuzzy Systems, 2012, 20 (6) :1130-1146." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fuzzy c-Means Algorithms for Very Large Data">
                                        <b>[10]</b>
                                         HAVENS T C, BEZDEK J C, LECKIE C, et al.Fuzzy c-means algorithms for very large data[J].IEEE Transactions on Fuzzy Systems, 2012, 20 (6) :1130-1146.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_11" title=" WANG Yangtao, CHEN Lihui, MEI Jianping.Incremental fuzzy clustering with multiple medoids for large data[J].IEEE Transactions on Fuzzy Systems, 2014, 22 (6) :1557-1568." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Incremental fuzzy clustering withmultiple medoids for large data">
                                        <b>[11]</b>
                                         WANG Yangtao, CHEN Lihui, MEI Jianping.Incremental fuzzy clustering with multiple medoids for large data[J].IEEE Transactions on Fuzzy Systems, 2014, 22 (6) :1557-1568.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_12" title=" 戴阳阳, 李朝锋, 徐华.初始点优化与参数自适应的密度聚类算法[J].计算机工程, 2016, 42 (1) :203-209." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201601036&amp;v=MjY5NDZHNEg5Zk1ybzlHWW9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJvRmlEaFZML1BMejdCYmI=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[12]</b>
                                         戴阳阳, 李朝锋, 徐华.初始点优化与参数自适应的密度聚类算法[J].计算机工程, 2016, 42 (1) :203-209.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_13" title=" JIAO Runhai, LIU Shaolong, WEN Wen, et al.Incremental kernel fuzzy c-means with optimizing cluster center initialization and delivery[J].Kybernetes, 2016, 45 (8) :1273-1291." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SPQD&amp;filename=SPQD8F177F157213A84F2216E805F5E06BBC&amp;v=MjY2NjViUTM1TjVnekx5NHdLOD1OajNhYXJ2T0g5YkwyWTVBWStrT0R3MHh5MkFSNkQ1N1BYZmlxV1F3RExLU044anNDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZDcA==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[13]</b>
                                         JIAO Runhai, LIU Shaolong, WEN Wen, et al.Incremental kernel fuzzy c-means with optimizing cluster center initialization and delivery[J].Kybernetes, 2016, 45 (8) :1273-1291.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_14" title=" 冯振华, 钱雪忠, 赵娜娜.Greedy DBSCAN:一种针对多密度聚类的DBSCAN改进算法[J].计算机应用研究, 2016, 33 (9) :2693-2696, 2700." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSYJ201609030&amp;v=MjgzMTVoVkwvUEx6N1NaTEc0SDlmTXBvOUdaSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm9GaUQ=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[14]</b>
                                         冯振华, 钱雪忠, 赵娜娜.Greedy DBSCAN:一种针对多密度聚类的DBSCAN改进算法[J].计算机应用研究, 2016, 33 (9) :2693-2696, 2700.
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_15" title=" LESKI J M.Fuzzy (c+p) -means clustering and its application to a fuzzy rule-based classifier:toward good generalization and good interpretability[J].IEEE Transactions on Fuzzy Systems, 2015, 23 (4) :802-812." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fuzzy (c+p)—means clustering and its application to a fuzzy rule-based classifier:Towards good generalization and good interpretability">
                                        <b>[15]</b>
                                         LESKI J M.Fuzzy (c+p) -means clustering and its application to a fuzzy rule-based classifier:toward good generalization and good interpretability[J].IEEE Transactions on Fuzzy Systems, 2015, 23 (4) :802-812.
                                    </a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_16" title=" DENG Zhaohong, CHOI K S, CHUNG F L, et al.Enhanced soft subspace clustering integrating within-cluster and between-cluster information[J].Pattern Recognition, 2010, 43 (3) :767-781." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011600738594&amp;v=Mjc3NzZRSC9pclJkR2VycVFUTW53WmVadUh5am1VTG5KS0ZvVGFoVT1OaWZPZmJLN0h0RE5xWTlGWStnSENYVTlvQk1UNlQ0UA==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[16]</b>
                                         DENG Zhaohong, CHOI K S, CHUNG F L, et al.Enhanced soft subspace clustering integrating within-cluster and between-cluster information[J].Pattern Recognition, 2010, 43 (3) :767-781.
                                    </a>
                                </li>
                                <li id="35">


                                    <a id="bibliography_17" title=" LIU Jun, MOHAMMED J, CARTER J, et al.Distance-based clustering of CGH data[J].Bioinformatics, 2006, 22 (16) :1971-1978." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Distance-based clustering of CGH data">
                                        <b>[17]</b>
                                         LIU Jun, MOHAMMED J, CARTER J, et al.Distance-based clustering of CGH data[J].Bioinformatics, 2006, 22 (16) :1971-1978.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJC" target="_blank">计算机工程</a>
                2019,45(06),75-81+88 DOI:10.19678/j.issn.1000-3428.0051839            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>面向稀疏高维大数据的扩展增量模糊聚类算法</b></span>
 <span class="shoufa"></span>                                     </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%92%B1%E9%9B%AA%E5%BF%A0&amp;code=08026119&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">钱雪忠</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%A7%9A%E7%90%B3%E7%87%95&amp;code=40355910&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">姚琳燕</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%B1%9F%E5%8D%97%E5%A4%A7%E5%AD%A6%E7%89%A9%E8%81%94%E7%BD%91%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2%E7%89%A9%E8%81%94%E7%BD%91%E6%8A%80%E6%9C%AF%E5%BA%94%E7%94%A8%E6%95%99%E8%82%B2%E9%83%A8%E5%B7%A5%E7%A8%8B%E7%A0%94%E7%A9%B6%E4%B8%AD%E5%BF%83&amp;code=0074200&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">江南大学物联网工程学院物联网技术应用教育部工程研究中心</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>模糊C均值 (FCM) 聚类算法对初始中心点敏感, 不考虑类别间中心点的相互影响, 且仅能处理低维数据。为此, 设计一种改进的初始中心点选择方法, 并基于条件模糊聚类思想, 将传统FCM算法中的欧氏距离替换为余弦距离后提出wHFCLM算法。将该算法与扩展增量聚类算法spFCM、oFCM和rseFCM相结合, 得到对应的扩展增量模糊聚类算法spHF (c+l) M、oHF (c+l) M以及rseHF (c+l) M。实验结果表明, 与spFCM算法、oFCM算法和rseFCM算法相比, 扩展增量模糊聚类算法对初始中心点的选择敏感性较低, 能较好地处理大规模稀疏高维数据集, 且在合适的分块大小下具有更优的聚类性能。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%89%A9%E5%B1%95%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">扩展聚类算法;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%9D%A1%E4%BB%B6%E8%81%9A%E7%B1%BB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">条件聚类;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%A8%80%E7%96%8F%E9%AB%98%E7%BB%B4%E5%A4%A7%E6%95%B0%E6%8D%AE&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">稀疏高维大数据;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%A8%A1%E7%B3%8A%E8%81%9A%E7%B1%BB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">模糊聚类;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%88%9D%E5%A7%8B%E4%B8%AD%E5%BF%83%E7%82%B9&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">初始中心点;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    钱雪忠 (1967—) , 男, 副教授、硕士, 主研方向为数据挖掘、数据库技术、网络安全;;
                                </span>
                                <span>
                                    *姚琳燕 (通信作者) , 硕士研究生。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-06-15</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金 (61673193);</span>
                                <span>中央高校基本科研业务费专项资金 (JUSRP51510, JUSRP51635B);</span>
                    </p>
            </div>
                    <h1><b>Extended Incremental Fuzzy Clustering Algorithm for Sparse High-dimensional Big Data</b></h1>
                    <h2>
                    <span>QIAN Xuezhong</span>
                    <span>YAO Linyan</span>
            </h2>
                    <h2>
                    <span>Engineering Research Center of IoT Technology and Application, Ministry of Education, College of Internet of Things Engineering, Jiangnan University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Fuzzy C-Means (FCM) clustering algorithm can only deal with low-dimensional data and is sensitive to the initial center, without considering the interactions between class centers.For this reason, an improved method of initial center selection is designed based on the idea of conditional fuzzy clustering, replacing the Euclidean distance in the traditional FCM algorithm with the cosine distance.A wHFCLM algorithm is proposed and combined with extended incremental clustering algorithms, spFCM, oFCM and rseFCM, to generate their extended incremental fuzzy clustering algorithms, spHF (c+l) M, oHF (c+l) M and rseHF (c+l) M.Experimental results show that compared with spFCM, oFCM and rseFCM, the extended incremental fuzzy clustering algorithms is less sensitive to the selection of initial centers.It can better handle large-scale sparse high-dimensional data sets, and has better clustering performance under blocks of the appropriate size.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=extended%20clustering%20algorithm&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">extended clustering algorithm;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=conditional%20clustering&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">conditional clustering;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=sparse%20high-dimensional%20big%20data&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">sparse high-dimensional big data;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=fuzzy%20clustering&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">fuzzy clustering;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=initial%20center&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">initial center;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-06-15</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="37" name="37" class="anchor-tag">0 概述</h3>
                <div class="p1">
                    <p id="38">聚类是数据挖掘和无监督学习的重要方法, 其将数据划分为不同的类, 使得同一类的对象之间具有较高的相似度, 不同类的对象之间具有较低的相似度。目前, 聚类分析已广泛应用于数据挖掘、图像压缩、图像边缘检测、基因识别、面部识别以及文档检索等领域。聚类分析算法能够有效处理低维数据, 其典型代表有k-means<citation id="173" type="reference"><link href="3" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>、模糊C均值 (Fuzzy C-Means, FCM) 算法<citation id="174" type="reference"><link href="5" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>等。随着信息技术的发展, 特别是Web的出现, 数据和环境发生重大变化, 需要越来越大的空间存储数据。在处理大数据时, 传统聚类算法在内部存储器和外部存储器之间交换数据时会产生大量的时间开销, 且无法满足内存的需求<citation id="175" type="reference"><link href="7" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>。为解决该问题, 文献<citation id="176" type="reference">[<a class="sup">4</a>]</citation>提出一种增量式聚类算法。</p>
                </div>
                <div class="p1">
                    <p id="39">目前, 学者们设计了较多策略来解决增量问题, 这些策略主要分为单点增量聚类、采样增量聚类和分布式增量聚类。单点增量聚类依次处理增量数据, 其典型代表有基于扩展凝聚点与网格的增量聚类算法<citation id="177" type="reference"><link href="11" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>、基于代表点的增量聚类算法<citation id="178" type="reference"><link href="13" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>。采样增量聚类只需采集小的数据集合进行聚类, 然后将聚类结果扩展到整个数据集上而无需迭代整个数据集。这类方法的典型代表是rseFCM算法<citation id="179" type="reference"><link href="15" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>。分布式聚类将大规模数据集划分为小块, 每个数据块都单独聚类, 增量模糊聚类大多采用分块处理的方式。这类方法的典型代表是在线模糊C均值聚类oFCM (oline Fuzzy C-Means) <citation id="180" type="reference"><link href="17" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>、单通模糊C均值聚类spFCM (single pass Fuzzy C-Means) <citation id="181" type="reference"><link href="19" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>, 以及在此基础上发展的基于核的模糊C均值算法spkFCM和okFCM<citation id="182" type="reference"><link href="21" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>。但传统增量模糊C均值聚类都只有一个代表点, 为解决该问题, 文献<citation id="183" type="reference">[<a class="sup">11</a>]</citation>提出一种多代表点的增量聚类算法。</p>
                </div>
                <div class="p1">
                    <p id="40">上述算法大多对初始中心点敏感<citation id="184" type="reference"><link href="25" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>, 容易陷入局部最优。为此, 本文在聚类过程中考虑类间中心点之间的相互影响, 在FCM的基础上引入新的初始中心点选择法和条件聚类法, 并通过权重机制将欧式距离替换为余弦距离, 提出一种wHFCLM算法, 将其与扩展增量算法spFCM、oFCM和rseFCM相结合, 提出对应的3种用以处理大规模稀疏高维数据集的增量模糊聚类算法spHF (c+l) M、oHF (c+l) M以及rseHF (c+l) M。</p>
                </div>
                <h3 id="41" name="41" class="anchor-tag">1 相关算法</h3>
                <h4 class="anchor-tag" id="42" name="42">1.1 传统加权模糊C均值算法</h4>
                <div class="p1">
                    <p id="43">加权模糊C均值 (wFCM) 算法对FCM算法进行了扩展。wFCM考虑每个对象的不同权重, 使得具有较大权重的对象在聚类中扮演比较重要的角色。将数据集<b><i>X</i></b>=[<b><i>X</i></b><sub>1</sub>, <b><i>X</i></b><sub>2</sub>, …, <b><i>X</i></b><sub><i>n</i></sub>]聚类为<i>k</i>个簇, wFCM的目标是最小化如下函数:</p>
                </div>
                <div class="p1">
                    <p id="44" class="code-formula">
                        <mathml id="44"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>J</mi><msub><mrow></mrow><mrow><mtext>w</mtext><mtext>F</mtext><mtext>C</mtext><mtext>Μ</mtext></mrow></msub><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>c</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>w</mi></mstyle></mrow></mstyle><msub><mrow></mrow><mi>i</mi></msub><mi>u</mi><msubsup><mrow></mrow><mrow><mi>c</mi><mi>i</mi></mrow><mi>m</mi></msubsup><mi>d</mi><msub><mrow></mrow><mi>E</mi></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi mathvariant="bold-italic">δ</mi><msub><mrow></mrow><mi>c</mi></msub><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="45">其中, <i>u</i><sub><i>ci</i></sub>是对象<i>i</i>相对于集群<i>c</i>的模糊隶属度。</p>
                </div>
                <div class="p1">
                    <p id="46"><i>d</i><sub><i>E</i></sub> (<b><i>X</i></b><sub><i>i</i></sub>, <i>δ</i><sub><i>c</i></sub>) =‖<b><i>X</i></b><sub><i>i</i></sub>-<i>δ</i><sub><i>c</i></sub>‖<sup>2</sup>      (2) </p>
                </div>
                <div class="p1">
                    <p id="47">式 (2) 是数据点<b><i>X</i></b><sub><i>i</i></sub>与簇<i>c</i>的中心点<i>δ</i><sub><i>c</i></sub>之间的欧式距离。式 (1) 中的wFCM成员隶属度计算公式和传统FCM相同:</p>
                </div>
                <div class="p1">
                    <p id="48" class="code-formula">
                        <mathml id="48"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>u</mi><msub><mrow></mrow><mrow><mi>c</mi><mi>i</mi></mrow></msub><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>f</mi><mo>=</mo><mn>1</mn></mrow><mi>c</mi></munderover><mrow><mfrac><mrow><mi>d</mi><msubsup><mrow></mrow><mi>E</mi><mn>2</mn></msubsup><mrow><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mi mathvariant="bold-italic">δ</mi><msub><mrow></mrow><mi>f</mi></msub><mo stretchy="false">) </mo></mrow><msup><mrow></mrow><mrow><mn>1</mn><mo>/</mo><mi>m</mi><mo>-</mo><mn>1</mn></mrow></msup></mrow><mrow><mi>d</mi><msubsup><mrow></mrow><mi>E</mi><mn>2</mn></msubsup><mrow><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mi mathvariant="bold-italic">δ</mi><msub><mrow></mrow><mi>c</mi></msub><mo stretchy="false">) </mo></mrow><msup><mrow></mrow><mrow><mn>1</mn><mo>/</mo><mi>m</mi><mo>-</mo><mn>1</mn></mrow></msup></mrow></mfrac></mrow></mstyle><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="49">质心矩阵的计算公式如下:</p>
                </div>
                <div class="p1">
                    <p id="50" class="code-formula">
                        <mathml id="50"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">δ</mi><msub><mrow></mrow><mi>c</mi></msub><mo>=</mo><mfrac><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>w</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mi>u</mi><msubsup><mrow></mrow><mrow><mi>c</mi><mi>i</mi></mrow><mi>m</mi></msubsup><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>w</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mi>u</mi><msubsup><mrow></mrow><mrow><mi>c</mi><mi>i</mi></mrow><mi>m</mi></msubsup></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <h4 class="anchor-tag" id="51" name="51">1.2 扩展增量聚类算法</h4>
                <div class="p1">
                    <p id="52">为了以较低的计算量和内存来处理整个数据集, 扩展增量聚类算法应运而生。其中, <i>FCM</i>算法被用来执行一个个由随机划分的小子集构成的数据块。整个数据集被随机分成若干个数据块, 每个块抽取的集群结构在单通道进程中被继承, 或者在并行进程中被合并。</p>
                </div>
                <div class="p1">
                    <p id="53">典型的增量聚类算法有<i>spFCM</i>和<i>oFCM</i>。在<i>spFCM</i>中, <i>FCM</i>或者<i>wFCM</i>按照顺序依次执行, 其中, 前面一个数据块的聚类中心加入到下一个数据块中参加聚类。最终在数据块中得到的聚类中心就是整个数据集的聚类中心, 使用最终的聚类中心重新计算数据对象的隶属度。与<i>spFCM</i>不同, <i>oFCM</i>最终的聚类中心是对所有数据块的聚类结果进行最终聚类而获得的。</p>
                </div>
                <div class="p1">
                    <p id="54">随机抽样加扩展<i>FCM</i> (<i>rseFCM</i>) 是一种简单的可扩展方法。在该方法中, 首先从给定数据集中选择足够小的对象的子集, 以便加载到内存中, 然后对该子集进行聚类。后续过程是扩展, 其中, 采样子集的聚类结果用于标记所有没有采样的对象, 以便原始数据集中的所有对象都被聚类。虽然<i>rseFCM</i>提供了一种处理大型数据集的简单方法, 但是它过度依赖于找到一种适当的抽样方法来生成一个足以代表整个数据集的子集。尽管<i>rseFCM</i>具有较高的计算效率, 但是由于大部分数据对象从簇结构估计中被移除, 集群聚类质量可能会较低。在<i>rseFCM</i>中, 将<i>FCM</i>应用于随机采样生成的子集上以产生k个聚类质心, 这些质心以非迭代方式计算其他所有对象的隶属度。</p>
                </div>
                <h3 id="55" name="55" class="anchor-tag">2 改进的增量模糊聚类算法</h3>
                <h4 class="anchor-tag" id="56" name="56">2.1 初始聚类中心优化</h4>
                <div class="p1">
                    <p id="57"><i>wFCM</i>算法随机选择数据块的初始中心点<citation id="185" type="reference"><link href="27" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>, 这种随机选择的方法容易导致标准函数陷入局部最小值。避免陷入局部最小值的常用方法是重复多次随机初始化中心点, 但是这种方法会耗费大量时间, 且不一定能取得较好效果。当初始中心点选择得太近时, 容易陷入局部最优。因此, 本文提出一种特殊的初始中心点选择方法。该方法将初始中心点选择在数据集的凸包边界上, 使中心点尽量远离, 从而降低陷入标准函数局部最小值的可能性。</p>
                </div>
                <div class="p1">
                    <p id="58">本文参考贪心算法<citation id="186" type="reference"><link href="29" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>的思想, 在初始中心点选择方法中, 每一步都选择最优的解, 最后逼近整体最优解。方法具体步骤为:</p>
                </div>
                <div class="p1">
                    <p id="59">1) 计算数据集的平均值, 将距离平均值最远的数据点作为一个初始中心点。</p>
                </div>
                <div class="p1">
                    <p id="60">2) 计算初始中心点和所有数据之间的距离。</p>
                </div>
                <div class="p1">
                    <p id="61">3) 依照贪心选择策略选择尽可能远离之前中心点和平均值的数据作为下一个初始中心点, 直到选择出k个初始中心点。</p>
                </div>
                <div class="p1">
                    <p id="62">4) 将每个初始中心点向平均值方向移动两者距离的10%。</p>
                </div>
                <div class="p1">
                    <p id="63">算法伪代码如下:</p>
                </div>
                <div class="p1">
                    <p id="64"><b>算法1</b> 中心点初始化算法</p>
                </div>
                <div class="p1">
                    <p id="65">1.初始化一组索引M=Φ</p>
                </div>
                <div class="p1">
                    <p id="66">2.计算数据集的平均值<mathml id="67"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtext>X</mtext><msub><mrow></mrow><mrow><mtext>m</mtext><mtext>e</mtext><mtext>a</mtext><mtext>n</mtext></mrow></msub><mo>=</mo><mfrac><mn>1</mn><mtext>Ν</mtext></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mtext>k</mtext><mo>=</mo><mn>1</mn></mrow><mtext>Ν</mtext></munderover><mtext>X</mtext></mstyle><msub><mrow></mrow><mtext>k</mtext></msub></mrow></math></mathml></p>
                </div>
                <div class="p1">
                    <p id="68">3.计算所有数据点和平均值之间的距离D<sub>1k</sub>=‖X<sub>k</sub>-X<sub>mean</sub>‖, k=1, 2, …, N</p>
                </div>
                <div class="p1">
                    <p id="69">4.找到距离平均值最远的点作为一个中心点<mathml id="70"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtext>m</mtext><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mtext>a</mtext><mtext>r</mtext><mtext>g</mtext><mtext>m</mtext><mtext>a</mtext><mtext>x</mtext></mrow></mstyle><mrow><mtext>X</mtext><msub><mrow></mrow><mtext>k</mtext></msub></mrow></munder><mtext>D</mtext><msub><mrow></mrow><mrow><mn>1</mn><mtext>k</mtext></mrow></msub><mo>, </mo><mtext>Μ</mtext><mo>=</mo><mtext>Μ</mtext><mtext>U</mtext></mrow></math></mathml><mathml id="71"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>{</mo><mtext>m</mtext><mo>}</mo></mrow></mrow></math></mathml></p>
                </div>
                <div class="p1">
                    <p id="72" class="code-formula">
                        <mathml id="72"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mn>5</mn><mo>.</mo><mtext>计</mtext><mtext>算</mtext><mtext>D</mtext><msub><mrow></mrow><mrow><mn>2</mn><mtext>k</mtext></mrow></msub><mo>=</mo><mtext>D</mtext><msub><mrow></mrow><mrow><mn>1</mn><mtext>k</mtext></mrow></msub><mo stretchy="false">∥</mo><mtext>X</mtext><msub><mrow></mrow><mtext>m</mtext></msub><mo>-</mo><mtext>X</mtext><msub><mrow></mrow><mtext>k</mtext></msub><mo stretchy="false">∥</mo><mo>, </mo><mtext>k</mtext><mo>=</mo><mn>1</mn><mo>, </mo><mn>2</mn><mo>, </mo><mo>⋯</mo><mo>, </mo><mtext>Ν</mtext></mtd></mtr><mtr><mtd><mn>6</mn><mo>.</mo><mtext>m</mtext><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mtext>a</mtext><mtext>r</mtext><mtext>g</mtext><mtext>m</mtext><mtext>a</mtext><mtext>x</mtext></mrow></mstyle><mrow><mtext>X</mtext><msub><mrow></mrow><mtext>k</mtext></msub></mrow></munder><mtext>D</mtext><msub><mrow></mrow><mrow><mn>2</mn><mtext>k</mtext></mrow></msub><mo>, </mo><mtext>Μ</mtext><mo>=</mo><mtext>Μ</mtext><mtext>U</mtext><mrow><mo>{</mo><mtext>m</mtext><mo>}</mo></mrow></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="73">7.D<sub>2k</sub>=D<sub>2k</sub>‖X<sub>m</sub>-X<sub>k</sub>‖, k=1, 2, …, N</p>
                </div>
                <div class="p1">
                    <p id="74">8.集合M中的数据数目如果小于要选出的初始中心点的数目C, 则返回步骤6继续执行算法, 否则跳转到步骤9</p>
                </div>
                <div class="p1">
                    <p id="75">9.V<sub>k</sub>=X<sub>mk</sub>-0.1 (X<sub>mk</sub>-X<sub>mean</sub>) , k=1, 2, …, C</p>
                </div>
                <div class="p1">
                    <p id="76">在步骤5和步骤7中, 使用代数乘积作为建模和连接的t范数。步骤5～步骤7寻找离已选择的中心点<i>X</i><sub><i>m</i></sub>较远并且远离平均值<i>X</i><sub>mean</sub>的点作为下一个中心点, 直到选择出<i>k</i>个中心点。算法1虽然不排除会陷入标准函数的局部最小值, 但是它能够确保初始原型的多样性, 并且作为确定性算法, 总是具有可重复的结果。步骤9将每个初始中心点向平均值移动两者间距离的10%, 该步骤并非必要, 但是它能够加快聚类的收敛速度, 减少外围数据对聚类结果的影响。</p>
                </div>
                <h4 class="anchor-tag" id="77" name="77">2.2 类间中心点影响</h4>
                <div class="p1">
                    <p id="78">在已有的相关研究中, 无论单线程还是在线, 单代表点和多代表点的基于FCM的模糊聚类算法, 通常只考虑中心点与其他点之间的相互影响, 而没有考虑类与类之间中心点的相互排斥影响<citation id="187" type="reference"><link href="31" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>。将数据一次性加载进内存的方式无法增量地处理大规模数据, 该方法只对小规模数据集的聚类效果较好。因此, 本文引入权重机制, 采用通过固定已知的一类或多类求其他类的方法, 将传统批处理wFCM算法改进为wFCLM算法。</p>
                </div>
                <div class="p1">
                    <p id="79">对于一个类, 应该将簇心引到属于这个类的数据密集区域, 同时排除属于其他类的数据。由于FCM算法使用其他数据对象和簇心之间的不相似性度量, 因此从这些数据中获得排斥力非常困难, 但是获得簇心和簇心之间的排斥力是可能的。其中, 这些互相排斥的簇心应根据与其他类的数据的不相似性度量值来获得。在wFCLM算法中, 假设有<i>c</i>个聚类中心来自已知的一类, <i>l</i>个聚类中心来自其他类, 对于这2种类别, 算法包括类别对象的交替聚类, 并将来自不同类别的对象作为已知的原型中心点。本文的目标是对一个给定的类进行聚类, 使得簇心被吸引到数据密集的区域, 并同时被属于其他类的对象所排斥。根据FCM的目标函数, 将已知类和未知类都加入到目标函数公式中, 得到wFCLM的目标函数如下:</p>
                </div>
                <div class="p1">
                    <p id="80" class="code-formula">
                        <mathml id="80"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>J</mi><mo stretchy="false"> (</mo><mi>U</mi><mo>, </mo><mi>Τ</mi><mo>, </mo><mi>V</mi><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>c</mi></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mi>w</mi></mstyle></mrow></mstyle><msub><mrow></mrow><mi>k</mi></msub><mi>α</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mi>k</mi></mrow><mi>m</mi></msubsup><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>k</mi></msub><mo>-</mo><mi mathvariant="bold-italic">V</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">∥</mo><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>l</mi></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mi>w</mi></mstyle></mrow></mstyle><msub><mrow></mrow><mi>k</mi></msub><mi>β</mi><msubsup><mrow></mrow><mrow><mi>j</mi><mi>k</mi></mrow><mi>m</mi></msubsup><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>k</mi></msub><mo>-</mo><mi mathvariant="bold-italic">R</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">∥</mo><msup><mrow></mrow><mn>2</mn></msup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>5</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="81">其中, <b><i>V</i></b><sub><i>i</i></sub>表示未知类中第<i>i</i>簇的聚类中心, <b><i>R</i></b><sub><i>j</i></sub>表示已知类的聚类中心, <i>w</i><sub><i>k</i></sub>表示权重, <i>α</i><sub><i>ik</i></sub>表示第<i>k</i>个数据属于第<i>i</i>簇的程度, <i>β</i><sub><i>jk</i></sub>表示第<i>k</i>个数据属于第<i>j</i>簇的程度。所有样本满足以下关系:</p>
                </div>
                <div class="p1">
                    <p id="82" class="code-formula">
                        <mathml id="82"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>c</mi></munderover><mi>α</mi></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mi>k</mi></mrow></msub><mo>+</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>l</mi></munderover><mi>β</mi></mstyle><msub><mrow></mrow><mrow><mi>j</mi><mi>k</mi></mrow></msub><mo>=</mo><mn>1</mn></mtd></mtr><mtr><mtd><mi>α</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>k</mi></mrow></msub><mo>∈</mo><mo stretchy="false">[</mo><mn>0</mn><mo>, </mo><mn>1</mn><mo stretchy="false">]</mo><mo>, </mo><mi>β</mi><msub><mrow></mrow><mrow><mi>j</mi><mi>k</mi></mrow></msub><mo>∈</mo><mo stretchy="false">[</mo><mn>0</mn><mo>, </mo><mn>1</mn><mo stretchy="false">]</mo><mo>, </mo><mo>∀</mo><mi>k</mi><mo>∈</mo><mo stretchy="false">[</mo><mn>1</mn><mo>, </mo><mi>Ν</mi><mo stretchy="false">]</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>6</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="83">本文利用拉格朗日乘子法进行求解:</p>
                </div>
                <div class="p1">
                    <p id="84" class="code-formula">
                        <mathml id="84"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>G</mi><mo stretchy="false"> (</mo><mi>U</mi><mo>, </mo><mi>Τ</mi><mo>, </mo><mi>V</mi><mo>, </mo><mi>λ</mi><mo stretchy="false">) </mo><mo>=</mo><mi>J</mi><mo stretchy="false"> (</mo><mi>U</mi><mo>, </mo><mi>Τ</mi><mo>, </mo><mi>V</mi><mo stretchy="false">) </mo><mo>-</mo><mi>λ</mi><mo stretchy="false"> (</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>c</mi></munderover><mi>α</mi></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mi>k</mi></mrow></msub><mo>+</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>l</mi></munderover><mi>β</mi></mstyle><msub><mrow></mrow><mrow><mi>j</mi><mi>k</mi></mrow></msub><mo>-</mo><mn>1</mn><mo stretchy="false">) </mo><mo>=</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>c</mi></munderover><mi>w</mi></mstyle><msub><mrow></mrow><mi>k</mi></msub><mi>α</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mi>k</mi></mrow><mi>m</mi></msubsup><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>k</mi></msub><mo>-</mo><mi mathvariant="bold-italic">V</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">∥</mo><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>l</mi></munderover><mi>w</mi></mstyle><msub><mrow></mrow><mi>k</mi></msub><mi>β</mi><msubsup><mrow></mrow><mrow><mi>j</mi><mi>k</mi></mrow><mi>m</mi></msubsup><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>k</mi></msub><mo>-</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mi mathvariant="bold-italic">R</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">∥</mo><mo>-</mo><mi>λ</mi><mo stretchy="false"> (</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>c</mi></munderover><mi>α</mi></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mi>k</mi></mrow></msub><mo>+</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>l</mi></munderover><mi>β</mi></mstyle><msub><mrow></mrow><mrow><mi>j</mi><mi>k</mi></mrow></msub><mo>-</mo><mn>1</mn><mo stretchy="false">) </mo><mo>, </mo><mo>∀</mo><mi>k</mi><mo>∈</mo><mo stretchy="false">[</mo><mn>1</mn><mo>, </mo><mi>Ν</mi><mo stretchy="false">]</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>7</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="85">其中, <i>λ</i>为拉格朗日乘法算子。</p>
                </div>
                <div class="p1">
                    <p id="86">对<i>G</i> (<i>U</i>, <i>T</i>, <i>V</i>, <i>λ</i>) 中的<b><i>V</i></b><sub><i>i</i></sub>变量求偏导, 得:</p>
                </div>
                <div class="p1">
                    <p id="87" class="code-formula">
                        <mathml id="87"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mo>∂</mo><mrow><mi>G</mi><mo stretchy="false"> (</mo><mi>U</mi><mo>, </mo><mi>Τ</mi><mo>, </mo><mi>V</mi><mo>, </mo><mi>λ</mi><mo stretchy="false">) </mo></mrow></mrow><mrow><mo>∂</mo><mrow><mi mathvariant="bold-italic">V</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></mrow></mfrac><mo>=</mo><mo>-</mo><mn>2</mn><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>c</mi></munderover><mi>w</mi></mstyle><msub><mrow></mrow><mi>k</mi></msub><mi>α</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mi>k</mi></mrow><mi>m</mi></msubsup><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>k</mi></msub><mo>-</mo><mi mathvariant="bold-italic">V</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">∥</mo><mo>=</mo><mn>0</mn><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>8</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="88">由式 (8) 易得:</p>
                </div>
                <div class="p1">
                    <p id="89" class="code-formula">
                        <mathml id="89"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">V</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mfrac><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mi>w</mi></mstyle><msub><mrow></mrow><mi>k</mi></msub><mi>α</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mi>k</mi></mrow><mi>m</mi></msubsup><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>k</mi></msub></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mi>w</mi></mstyle><msub><mrow></mrow><mi>k</mi></msub><mi>α</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mi>k</mi></mrow><mi>m</mi></msubsup></mrow></mfrac><mo>, </mo><mo>∀</mo><mi>i</mi><mo>∈</mo><mo stretchy="false">[</mo><mn>1</mn><mo>, </mo><mi>c</mi><mo stretchy="false">]</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>9</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="90">同理, 按照上述步骤对<i>G</i> (<i>U</i>, <i>T</i>, <i>V</i>, <i>λ</i>) 中的<i>α</i><sub><i>ik</i></sub>和<i>β</i><sub><i>jk</i></sub>变量求偏导可得:</p>
                </div>
                <div class="p1">
                    <p id="91" class="code-formula">
                        <mathml id="91"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>α</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>k</mi></mrow></msub><mo>=</mo><mfrac><mrow><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>k</mi></msub><mo>-</mo><mi mathvariant="bold-italic">V</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">∥</mo><msup><mrow></mrow><mrow><mfrac><mn>2</mn><mrow><mn>1</mn><mo>-</mo><mi>m</mi></mrow></mfrac></mrow></msup></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>a</mi><mo>=</mo><mn>1</mn></mrow><mi>c</mi></munderover><mo stretchy="false">∥</mo></mstyle><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>k</mi></msub><mo>-</mo><mi mathvariant="bold-italic">V</mi><msub><mrow></mrow><mi>a</mi></msub><mo stretchy="false">∥</mo><msup><mrow></mrow><mrow><mfrac><mn>2</mn><mrow><mn>1</mn><mo>-</mo><mi>m</mi></mrow></mfrac></mrow></msup><mo>+</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>b</mi><mo>=</mo><mn>1</mn></mrow><mi>l</mi></munderover><mo stretchy="false">∥</mo></mstyle><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>k</mi></msub><mo>-</mo><mi mathvariant="bold-italic">R</mi><msub><mrow></mrow><mi>b</mi></msub><mo stretchy="false">∥</mo><msup><mrow></mrow><mrow><mfrac><mn>2</mn><mrow><mn>1</mn><mo>-</mo><mi>m</mi></mrow></mfrac></mrow></msup></mrow></mfrac><mo>, </mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mo>∀</mo><mi>k</mi><mo>∈</mo><mo stretchy="false">[</mo><mn>1</mn><mo>, </mo><mi>Ν</mi><mo stretchy="false">]</mo><mo>, </mo><mi>i</mi><mo>∈</mo><mo stretchy="false">[</mo><mn>1</mn><mo>, </mo><mi>c</mi><mo stretchy="false">]</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>0</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi>β</mi><msub><mrow></mrow><mrow><mi>j</mi><mi>k</mi></mrow></msub><mo>=</mo><mfrac><mrow><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>k</mi></msub><mo>-</mo><mi mathvariant="bold-italic">R</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">∥</mo><msup><mrow></mrow><mrow><mfrac><mn>2</mn><mrow><mn>1</mn><mo>-</mo><mi>m</mi></mrow></mfrac></mrow></msup></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>a</mi><mo>=</mo><mn>1</mn></mrow><mi>c</mi></munderover><mo stretchy="false">∥</mo></mstyle><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>k</mi></msub><mo>-</mo><mi mathvariant="bold-italic">V</mi><msub><mrow></mrow><mi>a</mi></msub><mo stretchy="false">∥</mo><msup><mrow></mrow><mrow><mfrac><mn>2</mn><mrow><mn>1</mn><mo>-</mo><mi>m</mi></mrow></mfrac></mrow></msup><mo>+</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>b</mi><mo>=</mo><mn>1</mn></mrow><mi>l</mi></munderover><mo stretchy="false">∥</mo></mstyle><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>k</mi></msub><mo>-</mo><mi mathvariant="bold-italic">R</mi><msub><mrow></mrow><mi>b</mi></msub><mo stretchy="false">∥</mo><msup><mrow></mrow><mrow><mfrac><mn>2</mn><mrow><mn>1</mn><mo>-</mo><mi>m</mi></mrow></mfrac></mrow></msup></mrow></mfrac><mo>, </mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mo>∀</mo><mi>k</mi><mo>∈</mo><mo stretchy="false">[</mo><mn>1</mn><mo>, </mo><mi>Ν</mi><mo stretchy="false">]</mo><mo>, </mo><mi>j</mi><mo>∈</mo><mo stretchy="false">[</mo><mn>1</mn><mo>, </mo><mi>l</mi><mo stretchy="false">]</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>1</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="92">wFCLM算法可以看作FCM聚类算法的一个特例, <i>l</i>个额外的中心点是预先确定的。wFCLM算法也可以被认为是条件聚类的泛化, 其中, FCM算法从未知中心点的迭代变为一些中心点已知、一些中心点未知的迭代, 其加强了关于FCM聚类算法的可变簇心的不同分布, 即不同类簇的质心竞争其他数据点之间重叠区域中的特征空间。簇心之间的排斥力由权重指数<i>m</i>来控制, <i>m</i>的值越接近1, 产生的排斥力越大。该聚类方法可以看作是条件模糊聚类概念的泛化。</p>
                </div>
                <h4 class="anchor-tag" id="93" name="93">2.3 改进的余弦距离加权FCM算法</h4>
                <div class="p1">
                    <p id="94"><i>FCM</i>算法和<i>wFCM</i>算法在低维数据集上的聚类效果较好, 但不适用于稀疏高维的大规模文档数据。<i>spFCM</i>、<i>oFCM</i>、<i>rseFCM</i> 3种扩展方案都需要对子集进行聚类, 这意味着利用一小部分文档生成合理簇的能力直接影响这些扩展方法的整体有效性。根据式 (3) 可知, <i>FCM</i>或<i>wFCM</i>中的隶属度u<sub>ci</sub>是由簇中心点和其他对象之间的欧氏距离d<sub>E</sub> (<b><i>X</i></b><sub><i>i</i></sub>, <i>δ</i><sub><i>c</i></sub>) 所决定的:</p>
                </div>
                <div class="p1">
                    <p id="95"><i>d</i><sub><i>E</i></sub> (<b><i>X</i></b><sub><i>i</i></sub>, <i>δ</i><sub><i>c</i></sub>) =‖<b><i>X</i></b><sub><i>i</i></sub>-<i>δ</i><sub><i>c</i></sub>‖<sup>2</sup>=‖<b><i>X</i></b><sub><i>i</i></sub>‖<sup>2</sup>+‖<i>δ</i><sub><i>c</i></sub>‖<sup>2</sup>-2<i>δ</i><sup>T</sup><sub><i>c</i></sub><b><i>X</i></b><sub><i>i</i></sub>      (12) </p>
                </div>
                <div class="p1">
                    <p id="96">由于式 (12) 右边第1项中的<b><i>X</i></b><sub><i>i</i></sub>固定, 因此只有后2项能够决定对象分配到哪个簇。为便于讨论, 本文只考虑硬聚类的情况, 只将每个对象分配给距质心最近的簇, 即<mathml id="97"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>c</mi><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mtext>a</mtext><mtext>r</mtext><mtext>g</mtext><mtext>m</mtext><mtext>i</mtext><mtext>n</mtext></mrow></mstyle><mi>f</mi></munder><mspace width="0.25em" /><mi>d</mi><msub><mrow></mrow><mi>E</mi></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi mathvariant="bold-italic">δ</mi><msub><mrow></mrow><mi>f</mi></msub><mo stretchy="false">) </mo></mrow></math></mathml>。在这种情况下, <i>δ</i><sub><i>c</i></sub>是属于该簇的所有对象的线性组合:</p>
                </div>
                <div class="p1">
                    <p id="98" class="code-formula">
                        <mathml id="98"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">δ</mi><msub><mrow></mrow><mi>c</mi></msub><mo>=</mo><mfrac><mn>1</mn><mrow><mrow><mo>|</mo><mrow><mi>χ</mi><msub><mrow></mrow><mi>c</mi></msub></mrow><mo>|</mo></mrow></mrow></mfrac><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>j</mi><mo>∈</mo><mi>χ</mi><msub><mrow></mrow><mi>c</mi></msub></mrow></munder><mi mathvariant="bold-italic">X</mi></mstyle><msub><mrow></mrow><mi>j</mi></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="99">根据式 (13) 有:</p>
                </div>
                <div class="p1">
                    <p id="100" class="code-formula">
                        <mathml id="100"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">δ</mi><msub><mrow></mrow><mi>c</mi></msub><mo stretchy="false">∥</mo><msup><mrow></mrow><mn>2</mn></msup><mo>=</mo><mi mathvariant="bold-italic">δ</mi><msubsup><mrow></mrow><mi>c</mi><mtext>Τ</mtext></msubsup><mo stretchy="false"> (</mo><mfrac><mn>1</mn><mrow><mrow><mo>|</mo><mrow><mi>χ</mi><msub><mrow></mrow><mi>c</mi></msub></mrow><mo>|</mo></mrow></mrow></mfrac><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>j</mi><mo>∈</mo><mi>χ</mi><msub><mrow></mrow><mi>c</mi></msub></mrow></munder><mi mathvariant="bold-italic">X</mi></mstyle><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mfrac><mn>1</mn><mrow><mrow><mo>|</mo><mrow><mi>χ</mi><msub><mrow></mrow><mi>c</mi></msub></mrow><mo>|</mo></mrow></mrow></mfrac><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>j</mi><mo>∈</mo><mi>χ</mi><msub><mrow></mrow><mi>c</mi></msub></mrow></munder><mi mathvariant="bold-italic">δ</mi></mstyle><msubsup><mrow></mrow><mi>c</mi><mtext>Τ</mtext></msubsup><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>j</mi></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>4</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="101">当各数据块较小或每个数据块中的对象较少时, 分配给每个簇的对象数量也很少, 即<i>χ</i><sub><i>c</i></sub>很小。考虑每个簇只有一个对象的极端情况, 可以得到:</p>
                </div>
                <div class="p1">
                    <p id="102">‖<i>δ</i><sub><i>c</i></sub>‖<sup>2</sup>=<b><i>X</i></b><sup>T</sup><sub><i>j</i></sub><b><i>X</i></b><sub><i>j</i></sub>, 2<i>δ</i><sup>T</sup><sub><i>c</i></sub><b><i>X</i></b><sub><i>i</i></sub>=2<b><i>X</i></b><sup>T</sup><sub><i>j</i></sub><b><i>X</i></b><sub><i>i</i></sub>      (15) </p>
                </div>
                <div class="p1">
                    <p id="103">对于稀疏高维文档向量<b><i>X</i></b><sub><i>i</i></sub>, 只有相当小的一部分元素是非零的。因此, 文档<b><i>X</i></b><sub><i>i</i></sub>与其他文档之间常用词的数量远小于每个文档中包含的词数量。本文使用二进制方法对文档向量中的词进行加权, 如果文档中出现单词, 则<b><i>X</i></b><sub><i>ij</i></sub>=1, 否则为0。由此可以得到:</p>
                </div>
                <div class="p1">
                    <p id="104"><b><i>X</i></b><sup>T</sup><sub><i>i</i></sub><b><i>X</i></b><sub><i>j</i></sub>&gt;&gt;<b><i>X</i></b><sup>T</sup><sub><i>j</i></sub><b><i>X</i></b><sub><i>i</i></sub>      (16) </p>
                </div>
                <div class="p1">
                    <p id="105">可得:</p>
                </div>
                <div class="p1">
                    <p id="106">‖<i>δ</i><sub>c</sub>‖<sup>2</sup>&gt;&gt;2<b><i>X</i></b><sup>T</sup><sub><i>i</i></sub><i>δ</i><sub><i>c</i></sub>      (17) </p>
                </div>
                <div class="p1">
                    <p id="107">式 (17) 表示对于稀疏高维数据, 当数据块大小较小时, 距离<i>d</i><sub><i>E</i></sub> (<b><i>X</i></b><sub><i>i</i></sub>, <i>δ</i><sub><i>c</i></sub>) 很可能由‖<i>δ</i><sub><i>c</i></sub>‖<sup>2</sup>单独决定, 且‖<i>δ</i><sub><i>c</i></sub>‖<sup>2</sup>的值和<b><i>X</i></b><sub><i>i</i></sub>无关, 这会导致“相同簇”问题, 即所有的对象被分配给同一个与质心具有最小欧氏距离的簇。为解决该问题, 本文在每次迭代后对所有质心进行单位标准化:</p>
                </div>
                <div class="p1">
                    <p id="108" class="code-formula">
                        <mathml id="108"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msup><mi mathvariant="bold-italic">δ</mi><mo>′</mo></msup><msub><mrow></mrow><mi>c</mi></msub><mo>=</mo><mfrac><mrow><mi mathvariant="bold-italic">δ</mi><msub><mrow></mrow><mi>c</mi></msub></mrow><mrow><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">δ</mi><msub><mrow></mrow><mi>c</mi></msub><mo stretchy="false">∥</mo></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>8</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="109">在归一化质心的情况下, 对象到质心的距离由式 (12) 右边的第3项决定, 该项是文档向量和质心之间的内积, 这使式 (1) 中的模糊隶属度有了意义。对于wFCM算法, 有:</p>
                </div>
                <div class="p1">
                    <p id="110" class="code-formula">
                        <mathml id="110"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">δ</mi><msub><mrow></mrow><mi>c</mi></msub><mo stretchy="false">∥</mo><mo>=</mo><mo stretchy="false">∥</mo><mfrac><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mi>i</mi></munder><mi>u</mi></mstyle><msubsup><mrow></mrow><mrow><mi>c</mi><mi>i</mi></mrow><mi>m</mi></msubsup><mi>w</mi><msub><mrow></mrow><mi>i</mi></msub><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mi>i</mi></munder><mi>u</mi></mstyle><msubsup><mrow></mrow><mrow><mi>c</mi><mi>i</mi></mrow><mi>m</mi></msubsup><mi>w</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></mfrac><mo stretchy="false">∥</mo><mo>=</mo><mfrac><mrow><mo stretchy="false">∥</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>i</mi></munder><mi>u</mi></mstyle><msubsup><mrow></mrow><mrow><mi>c</mi><mi>i</mi></mrow><mi>m</mi></msubsup><mi>w</mi><msub><mrow></mrow><mi>i</mi></msub><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">∥</mo></mrow><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mi>i</mi></munder><mi>u</mi></mstyle><msubsup><mrow></mrow><mrow><mi>c</mi><mi>i</mi></mrow><mi>m</mi></msubsup><mi>w</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>9</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="111">式 (19) 变形后得:</p>
                </div>
                <div class="p1">
                    <p id="112" class="code-formula">
                        <mathml id="112"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msup><mi mathvariant="bold-italic">δ</mi><mo>′</mo></msup><msub><mrow></mrow><mi>c</mi></msub><mo>=</mo><mfrac><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mi>i</mi></munder><mi>u</mi></mstyle><msubsup><mrow></mrow><mrow><mi>c</mi><mi>i</mi></mrow><mi>m</mi></msubsup><mi>w</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mrow><mo stretchy="false">∥</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>i</mi></munder><mi>u</mi></mstyle><msubsup><mrow></mrow><mrow><mi>c</mi><mi>i</mi></mrow><mi>m</mi></msubsup><mi>w</mi><msub><mrow></mrow><mi>i</mi></msub><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">∥</mo></mrow></mfrac><mo>=</mo><mfrac><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mi>i</mi></munder><mi>u</mi></mstyle><msubsup><mrow></mrow><mrow><mi>c</mi><mi>i</mi></mrow><mi>m</mi></msubsup><mi>w</mi><msub><mrow></mrow><mi>i</mi></msub><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mrow><msqrt><mrow><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mi>j</mi></munder><mrow><mrow><mo> (</mo><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mi>i</mi></munder><mi>u</mi></mstyle><msubsup><mrow></mrow><mrow><mi>c</mi><mi>i</mi></mrow><mi>m</mi></msubsup><mi>w</mi><msub><mrow></mrow><mi>i</mi></msub><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow><mo>) </mo></mrow></mrow></mstyle></mrow><msup><mrow></mrow><mn>2</mn></msup></mrow></msqrt></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mn>0</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="113">这意味着在wFCM算法中, 每次迭代更新式 (2) 后对质心进行归一化, 等于直接通过式 (20) 计算归一化质心。当‖<i>δ</i>′<sub><i>c</i></sub>‖=1时, 有:</p>
                </div>
                <div class="p1">
                    <p id="114"><i>d</i><sub><i>E</i></sub> (<b><i>X</i></b><sub><i>i</i></sub>, <i>δ</i><sub><i>c</i></sub>) =‖<b><i>X</i></b><sub><i>i</i></sub>-<i>δ</i><sub><i>c</i></sub>‖<sup>2</sup>=‖<b><i>X</i></b><sub><i>i</i></sub>‖<sup>2</sup>+1-2<i>δ</i><sub><i>c</i></sub>′<b><i>X</i></b><sub><i>i</i></sub>      (21) </p>
                </div>
                <div class="p1">
                    <p id="115">尽管‖<b><i>X</i></b><sub><i>i</i></sub>‖不会影响硬聚类的分配, 但当该值远大于<b><i>X</i></b><sup>T</sup><sub><i>i</i></sub><i>δ</i><sub><i>c</i></sub>′时可能会导致模糊聚类中<b><i>X</i></b><sub><i>i</i></sub>在所有簇中的隶属度变得非常相似。为避免这种情况, 一个简单方法是将每个目标矢量归一化为单位范数, 即‖<b><i>X</i></b><sub><i>i</i></sub>‖=1。欧式距离的公式即变成:</p>
                </div>
                <div class="p1">
                    <p id="116"><i>d</i><sub><i>E</i></sub> (<b><i>X</i></b><sub><i>i</i></sub>, <i>δ</i>′<sub><i>c</i></sub>) =2 (1-<b><i>X</i></b><sup>T</sup><sub><i>i</i></sub><i>δ</i><sub><i>c</i></sub>′) =2<i>d</i><sub>cos</sub> (<b><i>X</i></b><sub><i>i</i></sub>, <i>δ</i>′<sub><i>c</i></sub>)      (22) </p>
                </div>
                <div class="p1">
                    <p id="117">式 (22) 表明, 当文档向量归一化为单位长度时, 在wFCM算法的每次迭代中, 更新后的质心归一化实际上是将该算法变成基于加权余弦距离的wHFCM算法。</p>
                </div>
                <h4 class="anchor-tag" id="118" name="118">2.4 算法步骤</h4>
                <div class="p1">
                    <p id="119">本文在<i>wFCLM</i>算法计算簇心与其他数据对象的距离时, 用余弦距离替换欧氏距离, 将<i>wFCLM</i>算法改进为可处理稀疏高维数据的<i>wHFCLM</i>算法, 并与扩展增量聚类算法的分块处理思想相结合, 替代传统<i>FCM</i>算法来对大规模数据进行分块处理。这样既能保留<i>wFCLM</i>算法对<i>FCM</i>算法在聚类精度上的提升, 也能处理大规模稀疏高维数据, 无需重复运行整个数据集, 最终降低算法的运行时间。同时, 在<i>spHF</i> (<i>c</i>+<i>l</i>) <i>M</i>算法中, 本文将距离簇中心点最近的k个近邻和当前数据块的数据中心点一同加入下一块的计算中, 由于当前数据块的聚类中心有可能与下一块数据距离较远, 本文权重机制的引入在一定程度上能够减少历史信息对聚类效果的影响。<i>spHF</i> (<i>c</i>+<i>l</i>) <i>M</i>、<i>oHF</i> (<i>c</i>+<i>l</i>) <i>M</i>与<i>rseHF</i> (<i>c</i>+<i>l</i>) <i>M</i>算法具体描述如下:</p>
                </div>
                <div class="p1">
                    <p id="120"><b>算法2</b> spHF (c+l) M算法</p>
                </div>
                <div class="p1">
                    <p id="121"><b>输入</b><i>X</i>, <i>c</i>, <i>l</i>, <i>m</i>, <i>k</i>, <i>ε</i></p>
                </div>
                <div class="p1">
                    <p id="122"><b>输出</b><i>V</i></p>
                </div>
                <div class="p1">
                    <p id="123">1.将数据集X随机划分成大小相等的n个子集X={X<sub>1</sub>, X<sub>2</sub>, …, X<sub>n</sub>};</p>
                </div>
                <div class="p1">
                    <p id="124">2.定义空集合X<sub>pass</sub>, X<sub>near</sub>;</p>
                </div>
                <div class="p1">
                    <p id="125">3.使用wHFCLM算法遍历所有数据块获取聚类中心:</p>
                </div>
                <div class="p1">
                    <p id="126">4.for p=1, 2, …, n</p>
                </div>
                <div class="p1">
                    <p id="127">5.采用算法1初始化已知类和未知类的聚类中心V, R;</p>
                </div>
                <div class="p1">
                    <p id="128">6.将从上一数据块获得的聚类结果X<sub>pass</sub>加入当前数据块, 即X<sub>p</sub>={X<sub>p</sub>∪X<sub>pass</sub>};</p>
                </div>
                <div class="p1">
                    <p id="129">7.将式 (9) ～式 (11) 中的欧氏距离计算公式 (2) 替换为余弦距离计算公式 (12) , 用改进后的式 (9) ～式 (11) 迭代计算当前数据块的聚类中心V<sub>p</sub>;</p>
                </div>
                <div class="p1">
                    <p id="130">8.当前数据块离聚类中心点最近的k个点放入X<sub>near</sub>;当前聚类中心V<sub>p</sub>和其最近的k个点放入X<sub>pass</sub>, 即X<sub>pass</sub>={V<sub>p</sub>∪X<sub>near</sub>};</p>
                </div>
                <div class="p1">
                    <p id="131">9.end for</p>
                </div>
                <div class="p1">
                    <p id="132"><b>算法3</b> oHF (c+l) M算法</p>
                </div>
                <div class="p1">
                    <p id="133"><b>输入</b><i>X</i>, <i>c</i>, <i>l</i>, <i>m</i>, <i>ε</i></p>
                </div>
                <div class="p1">
                    <p id="134"><b>输出</b><i>V</i></p>
                </div>
                <div class="p1">
                    <p id="135">1.将数据集X随机划分成大小相等的n个子集X={X<sub>1</sub>, X<sub>2</sub>, …, X<sub>n</sub>};</p>
                </div>
                <div class="p1">
                    <p id="136">2.使用wHFCLM算法遍历所有数据块获取聚类中心。</p>
                </div>
                <div class="p1">
                    <p id="137">3.for p=1, 2, …, n</p>
                </div>
                <div class="p1">
                    <p id="138">4.采用算法1初始化已知类和未知类的聚类中心V, R ;</p>
                </div>
                <div class="p1">
                    <p id="139">5.将式 (9) ～式 (11) 中的欧氏距离计算公式 (式 (2) ) 替换为余弦距离计算公式 (式 (12) ) , 用改进后的式 (9) ～式 (11) 迭代计算当前数据块的聚类中心V<sub>p</sub>;</p>
                </div>
                <div class="p1">
                    <p id="140">6.end for</p>
                </div>
                <div class="p1">
                    <p id="141">7.对所有数据块的运算结果V={V<sub>1</sub>, V<sub>2</sub>, …, V<sub>n</sub>}再次使用wHFCLM算法进行聚类得到最终的聚类中心。</p>
                </div>
                <div class="p1">
                    <p id="142"><b>算法4</b> rseHF (c+l) M算法</p>
                </div>
                <div class="p1">
                    <p id="143"><b>输入</b><i>X</i>, <i>c</i>, <i>l</i>, <i>m</i>, <i>ε</i></p>
                </div>
                <div class="p1">
                    <p id="144"><b>输出</b><i>U</i>, <i>V</i></p>
                </div>
                <div class="p1">
                    <p id="145">1.从数据集X中随机抽取n<sub>s</sub>个数据对象, 记作X<sub>s</sub>;</p>
                </div>
                <div class="p1">
                    <p id="146">2.使用wHFCLM算法处理X<sub>s</sub>数据块;</p>
                </div>
                <div class="p1">
                    <p id="147">3.通过式 (3) 将步骤2的结果扩展到整个数据集上, 这些质心以非迭代的方式计算其他所有对象的隶属度。</p>
                </div>
                <div class="p1">
                    <p id="148">其中, 使用wHFCLM算法遍历所有数据块获取聚类中心是算法2～算法4的主要迭代过程。在wHFCLM算法中, 式 (9) 是聚类中心<i>V</i>的更新公式, 式 (10) 、式 (11) 分别是未知类、已知类的隶属度<i>α</i><sub><i>ik</i></sub>和<i>β</i><sub><i>jk</i></sub>的更新公式。式 (22) 表示用数据点和聚类中心之间的余弦距离替代欧氏距离。当聚类中心连续变化值的Frobenius范数小于<i>ε</i>时, wHFCLM算法迭代停止。spHF (c+l) M算法在步骤8中将距离簇中心点最近的<i>k</i>个近邻和当前数据块的聚类中心一同加入下一块的计算中, <i>X</i><sub>near</sub>用于存放距离中心点最近的<i>k</i>个点。算法2在遍历完所有数据块并取得最后的聚类中心后终止迭代。</p>
                </div>
                <h4 class="anchor-tag" id="149" name="149">2.5 算法复杂度分析</h4>
                <div class="p1">
                    <p id="150">本文所提各算法的复杂度如表1所示。其中, <i>t</i>表示迭代次数, <i>c</i>表示未知类的个数, <i>l</i>表示已知类的个数, <i>d</i>表示数据的维度, <i>s</i>表示数据块的个数, <i>n</i>表示数据集中样本点的个数, <i>k</i>表示spHF (c+l) M算法中每个数据块中距离中心点最近的数据点数目。由表1可以看出, wHFCLM、spHF (c+l) M和oHF (c+l) M都具有相同的大<i>O</i>时间复杂度, 原因是这些算法都是在相同环境下对相同的数据集<i>X</i>进行处理, 但是这些算法的运行时间差别很大。由于spHF (c+l) M和oHF (c+l) M在每个数据块的处理过程中能够加速收敛, 因此算法的运行总时间较少。与其他算法相比, rseHF (c+l) M的时间复杂度较低, 原因是该算法只聚类了一个简化过的子集而非整个数据集。</p>
                </div>
                <div class="area_img" id="151">
                    <p class="img_tit"><b>表1 各算法的时间和空间复杂度对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="151" border="1"><tr><td><br />算法</td><td>时间复杂度</td><td>空间复杂度</td></tr><tr><td><br />wHFCLM</td><td><i>O</i>[<i>t</i> (<i>c</i>+<i>l</i>) <i>dn</i>+<i>tc</i>]</td><td><i>O</i>[<i>n</i> (<i>d</i>+<i>c</i>+<i>l</i>) ]</td></tr><tr><td><br />spHF (c+l) M</td><td><i>O</i>[<i>t</i> (<i>c</i>+<i>l</i>) <i>dn</i>+<i>tc</i>]</td><td><i>O</i>[ (<i>d</i>+<i>c</i>+<i>l</i>+<i>k</i>) (<i>n</i>/<i>s</i>) ]</td></tr><tr><td><br />oHF (c+l) M</td><td><i>O</i>[<i>t</i> (<i>c</i>+<i>l</i>) <i>dn</i>+<i>tc</i>]</td><td><i>O</i>[ (<i>d</i>+<i>c</i>+<i>l</i>) (<i>n</i>/<i>s</i>) +<i>cs</i>]</td></tr><tr><td><br />rseHF (c+l) M</td><td><i>O</i>[<i>t</i> (<i>c</i>+<i>l</i>) <i>dn</i>/<i>s</i>+<i>tc</i>]</td><td><i>O</i>[ (<i>d</i>+<i>c</i>+<i>l</i>) (<i>n</i>/<i>s</i>) ]</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="152">本文聚类算法都是对整个数据集进行分块处理, 因此, 每个待处理的数据块需要占用的空间为<i>n</i>/<i>s</i>。与spHF (c+l) M算法相比, oHF (c+l) M算法具有稍大的空间复杂度, 原因是其每个数据块在处理完后必须存储数据块的中心点。由于spHF (c+l) M需额外存储距离聚类中心最近的<i>k</i>个数据点, 因此, 与rseHF (c+l) M相比, 该算法占用较多的存储空间, 空间复杂度较高。</p>
                </div>
                <h3 id="153" name="153" class="anchor-tag">3 实验结果与分析</h3>
                <h4 class="anchor-tag" id="154" name="154">3.1 实验设置</h4>
                <div class="p1">
                    <p id="155">本文采用归一化互信息 (Normalized Mutual Information, NMI) 指标和调整兰德指数 (Adjusted Rand Index, ARI) <citation id="188" type="reference"><link href="33" rel="bibliography" /><link href="35" rel="bibliography" /><sup>[<a class="sup">16</a>,<a class="sup">17</a>]</sup></citation>对各算法的性能进行评价。NMI和ARI的取值范围均为[0, 1], 取值越大表示算法的聚类有效性越高。所有算法均通过Matlab R2014a工具实现并处理实验结果, 实验环境:CPU为Intel (R) Xeon (R) E5-2620 v4 2.10 GHz, 内存为256 GB, OS为Linux Ubuntu。实验分2次进行:</p>
                </div>
                <div class="p1">
                    <p id="156"><b>实验1</b> 采用4种真实文档数据进行实验, 数据集信息如表2所示。la2数据集是TREC-5集合的一部分, 包含来自洛杉矶时报的新闻文章。20Newsgroups总共包含20个不同新闻组收集的约20 000篇文章。Multi5是20Newsgroups的子集, 由其中的5个主题组成, 每个主题包含约100个文档。数据集tr12来源于TREC-5、TREC-6和TREC-7的集合。数据集k1a从WebACE项目中提取, 其中, 每个文档对应雅虎主题层次结构中列出的网页。这些数据集通过移除停用词并应用词干来进行预处理。使用tf-idf方法加权文档的每一项后将每个文档进行归一化。在这4个有标签的数据集中, 只有Multi5比较容易聚类, 因为其数据集簇的大小平衡, 只有前1 000个具有最大用户信息的单词被选择。其他3个数据集因为高维、簇的大小不平衡且主题之间存在重叠, 从而较难聚类。</p>
                </div>
                <div class="area_img" id="157">
                    <p class="img_tit"><b>表2 实验1数据集信息</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="157" border="1"><tr><td>数据集</td><td>内容</td><td>文档数</td><td>单词数</td><td>主题数</td></tr><tr><td>Multi5</td><td>20Newsgroups</td><td>494</td><td>1 000</td><td>5</td></tr><tr><td><br />tr12</td><td>TREC</td><td>313</td><td>5 804</td><td>8</td></tr><tr><td><br />la2</td><td>LA Times (TREC) </td><td>3 075</td><td>31 472</td><td>6</td></tr><tr><td><br />k1a</td><td>WebACE</td><td>2 340</td><td>28 102</td><td>20</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="158"><b>实验2</b> 采用模拟数据集2D15、手写数字数据集MNIST、标准数据集waveform和forest来验证算法的有效性。数据集具体信息如表3所示。</p>
                </div>
                <div class="area_img" id="159">
                    <p class="img_tit"><b>表3 实验2数据集信息</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="159" border="1"><tr><td>数据集</td><td>大小</td><td>维数</td><td>类别</td></tr><tr><td>2D15</td><td>5 000</td><td>2</td><td>15</td></tr><tr><td><br />MNIST</td><td>5 000</td><td>21</td><td>3</td></tr><tr><td><br />waveform</td><td>581 012</td><td>54</td><td>7</td></tr><tr><td><br />forest</td><td>70 000</td><td>784</td><td>10</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="160" name="160">3.2 算法性能分析</h4>
                <div class="p1">
                    <p id="161">为了使6种算法具有可对比性, 在随机分块后每次运行算法时都使用相同的随机分块。为考察不同分块情况下本文算法的有效性, 分别取数据总量的5%、10%、25%、35%和50%这5种分块情况进行实验, 结果均为使用相同初始中心重复运行算法50次的结果。本文采用试错法选择模糊聚类中的参数, 经多次实验可得, wHFCLM中<i>m</i>的值取1.01, spHF (c+l) M中<i>k</i>的值取5, 实验结果保留两位小数。表4所示为未进行分快处理时FCM算法和HFCM算法的ARI值, 表5所示为6种增量聚类算法以及3种原始聚类方法的ARI值, 其中, 黑色字体表示最优结果。</p>
                </div>
                <div class="area_img" id="162">
                    <p class="img_tit"><b>表4 未进行分块处理时2种算法的ARI值对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="162" border="1"><tr><td><br />数据集</td><td>FCM算法</td><td>HFCM算法</td></tr><tr><td><br />Multi5</td><td>0.74</td><td>0.74</td></tr><tr><td><br />tr12</td><td>0.43</td><td>0.45</td></tr><tr><td><br />la2</td><td>0.26</td><td>0.50</td></tr><tr><td><br />k1a</td><td>0.30</td><td>0.38</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="163">
                    <p class="img_tit"><b>表5 9种聚类算法的ARI值对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="163" border="1"><tr><td>数据集</td><td>分块<br />比例/%</td><td>spFCM</td><td>spF (c+l) M</td><td>spHF (c+l) M</td><td>oFCM</td><td>oF (c+l) M</td><td>oHF (c+l) M</td><td>rseFCM</td><td>rseF (c+l) M</td><td>rseHF (c+l) M</td></tr><tr><td rowspan="5"><br />Multi5</td><td>5</td><td>0.15</td><td>0.20</td><td>0.44</td><td>0.00</td><td>0.00</td><td><b>0.50</b></td><td>0.08</td><td>0.12</td><td>0.23</td></tr><tr><td><br />10</td><td>0.30</td><td>0.33</td><td>0.42</td><td>0.00</td><td>0.00</td><td><b>0.67</b></td><td>0.14</td><td>0.18</td><td>0.28</td></tr><tr><td><br />25</td><td>0.52</td><td>0.53</td><td>0.46</td><td>0.36</td><td>0.38</td><td><b>0.65</b></td><td>0.31</td><td>0.33</td><td>0.39</td></tr><tr><td><br />35</td><td>0.53</td><td>0.54</td><td>0.51</td><td>0.46</td><td>0.50</td><td><b>0.66</b></td><td>0.38</td><td>0.42</td><td>0.48</td></tr><tr><td><br />50</td><td>0.59</td><td>0.60</td><td>0.62</td><td>0.58</td><td>0.60</td><td><b>0.70</b></td><td>0.52</td><td>0.55</td><td>0.61</td></tr><tr><td rowspan="5"><br />tr12</td><td>5</td><td>0.09</td><td>0.15</td><td>0.37</td><td>0.02</td><td>0.05</td><td><b>0.38</b></td><td>0.08</td><td>0.18</td><td>0.26</td></tr><tr><td><br />10</td><td>0.16</td><td>0.22</td><td>0.35</td><td>0.03</td><td>0.08</td><td><b>0.40</b></td><td>0.12</td><td>0.21</td><td>0.26</td></tr><tr><td><br />25</td><td>0.29</td><td>0.30</td><td>0.36</td><td>0.17</td><td>0.22</td><td><b>0.42</b></td><td>0.22</td><td>0.24</td><td>0.30</td></tr><tr><td><br />35</td><td>0.31</td><td>0.31</td><td>0.35</td><td>0.20</td><td>0.36</td><td><b>0.39</b></td><td>0.24</td><td>0.28</td><td>0.32</td></tr><tr><td><br />50</td><td>0.37</td><td>0.36</td><td>0.40</td><td>0.35</td><td>0.38</td><td>0.42</td><td>0.26</td><td>0.36</td><td><b>0.48</b></td></tr><tr><td rowspan="5"><br />la2</td><td>5</td><td>0.08</td><td>0.12</td><td>0.35</td><td>0.04</td><td>0.08</td><td><b>0.52</b></td><td>0.14</td><td>0.18</td><td>0.28</td></tr><tr><td><br />10</td><td>0.09</td><td>0.14</td><td>0.36</td><td>0.14</td><td>0.23</td><td><b>0.49</b></td><td>0.16</td><td>0.33</td><td>0.33</td></tr><tr><td><br />25</td><td>0.18</td><td>0.20</td><td>0.44</td><td>0.18</td><td>0.27</td><td><b>0.48</b></td><td>0.25</td><td>0.36</td><td>0.43</td></tr><tr><td><br />35</td><td>0.19</td><td>0.20</td><td>0.46</td><td>0.19</td><td>0.22</td><td><b>0.48</b></td><td>0.25</td><td>0.38</td><td>0.46</td></tr><tr><td><br />50</td><td>0.21</td><td>0.28</td><td>0.48</td><td>0.20</td><td>0.25</td><td><b>0.50</b></td><td>0.26</td><td>0.40</td><td>0.48</td></tr><tr><td rowspan="5"><br />k1a</td><td>5</td><td>0.05</td><td>0.13</td><td>0.35</td><td>0.04</td><td>0.10</td><td><b>0.51</b></td><td>0.09</td><td>0.12</td><td>0.25</td></tr><tr><td><br />10</td><td>0.15</td><td>0.20</td><td>0.33</td><td>0.14</td><td>0.15</td><td><b>0.47</b></td><td>0.13</td><td>0.15</td><td>0.29</td></tr><tr><td><br />25</td><td>0.21</td><td>0.23</td><td>0.36</td><td>0.27</td><td>0.22</td><td><b>0.40</b></td><td>0.17</td><td>0.23</td><td>0.34</td></tr><tr><td><br />35</td><td>0.24</td><td>0.25</td><td>0.35</td><td>0.27</td><td>0.33</td><td><b>0.36</b></td><td>0.23</td><td>0.28</td><td>0.34</td></tr><tr><td><br />50</td><td>0.27</td><td>0.28</td><td><b>0.37</b></td><td>0.26</td><td>0.32</td><td>0.36</td><td>0.25</td><td>0.32</td><td>0.35</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="164">根据表4、表5可以得出:</p>
                </div>
                <div class="p1">
                    <p id="165">1) spFCM、oFCM、rseFCM这3种扩展增量聚类算法在处理高维稀疏数据时, 基于改进wHFCLM增量聚类算法的ARI值几乎都高于基于FCM的增量聚类算法, 且oHF (c+l) M的聚类效果最佳。说明这些增量扩展算法保留了HFCM处理稀疏高维数据的优点, 并且能够增量处理数据。</p>
                </div>
                <div class="p1">
                    <p id="166">2) 采用一个合适的分块大小, 增量扩展的wHFCLM算法可能会比一次性加载所有数据到内存的处理方式取得的聚类效果更好。 oHF (c+l) M算法在分块大小为5%和50%时, 在la2数据集上比批处理方法聚类精度更高, 在k1a数据集上, 仅当分块大小为50%时低于批处理方法。可见, 通过将较大规模数据集分割成较小的数据块进行聚类, 可以获得更好的结果。因此, 与批处理方法相比, 分块增量式处理数据集不一定会影响聚类效果的有效性, 如果分块大小设置得当, 增量聚类算法甚至可能提高算法性能。</p>
                </div>
                <div class="p1">
                    <p id="167">3) 相比原始spFCM、oFCM和rseFCM算法, 增加了类间中心点之间相互影响的spF (c+l) M、oF (c+l) M和rseF (c+l) M增量聚类算法在4个数据集的大部分分块情况下聚类效果更好, 这表明考虑类间中心点的排斥力影响对提高聚类精度有效。</p>
                </div>
                <div class="p1">
                    <p id="168">为验证本文初始中心点选择方法的有效性, 在2D15、MNIST、waveform和forest 4个数据集上, 对随机初始化方法 (简称为-r) 和本文初始化方法 (简称为-b) 进行对比实验, 结果如图1所示。</p>
                </div>
                <div class="area_img" id="169">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201906012_169.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 spF (c+l) M 和oF (c+l) M在4个数据集上运用2种初始化方法效果对比" src="Detail/GetImg?filename=images/JSJC201906012_169.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图1 spF (c+l) M 和oF (c+l) M在4个数据集上运用2种初始化方法效果对比</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201906012_169.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="170">由图1可以看出, 本文初始中心点选择方法在多数情况下的聚类效果要优于随机初始化方法, 仅在个别数据集分块情况下和随机初始化方法效果相当。这说明本文方法能够在一定程度上避免初始中心点靠得太近的情况, 从而确保初始中心点选择的多样性。</p>
                </div>
                <h3 id="171" name="171" class="anchor-tag">4 结束语</h3>
                <div class="p1">
                    <p id="172">传统FCM算法无法高效处理稀疏高维数据集, 并且容易陷入局部最优。为此, 本文结合扩展增量聚类算法策略, 提出增量模糊聚类算法spHF (c+l) M、oHF (c+l) M和rseHF (c+l) M。实验结果表明, 这3种算法可以有效处理稀疏高维大规模数据集, 且合适的分块大小可以提高算法的效率与有效性。由于不同数据集之间具有结构多样性, 因此在实际中较难确定最优分块, 探索简单高效的分块方法将是下一步的研究方向。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Very large-scale data classification based on K-means clustering and multi-kernel SVM">

                                <b>[1]</b> TANG Tinglong, CHEN Shengyong, ZHAO Meng, et al.Very large-scale data classification based on K-means clustering and multi-kernel SVM[J].Soft Computing, 2018 (1) :1-9.
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Classification of data streams by incremental semi-supervised fuzzy clustering">

                                <b>[2]</b> CASTELLANO G, FANELLI A M.Classification of data streams by incremental semi-supervised fuzzy clustering[M]//PETROSINO A, LOIA V, PEDRYCZ W.Fuzzy logic and soft computing applications.Berlin, Germany:Springer, 2016:185-194.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Data mining with big data">

                                <b>[3]</b> WU Xidong, ZHU Xingquan, WU Gongqing, et al.Data mining with big data[J].IEEE Transactions on Knowledge and Data Engineering, 2014, 26 (1) :97-107.
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Incremental clustering for dynamic document databases">

                                <b>[4]</b> CAN F, DROCHAK N D I.Incremental clustering for dynamic document databases[C]//Proceedings of 1990 Symposium on Applied Computing.Washington D.C., USA:IEEE Press, 1990:61-67.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=HEBX200608045&amp;v=MTIwNDllUm9GaURoVkwvUExTakpkckc0SHRmTXA0OUJZWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVo=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b> 陈卓, 贺明霞, 刘相双.基于扩展凝聚点和网格的增量聚类算法[J].哈尔滨工业大学学报, 2006, 38 (8) :1382-1385.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSYJ201208018&amp;v=MDg5MTZxQnRHRnJDVVJMT2VaZVJvRmlEaFZML1BMejdTWkxHNEg5UE1wNDlFYklRS0RIODR2UjRUNmo1NE8zenE=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b> 孟凡荣, 李晓翠, 周勇.一种基于代表点的增量聚类算法[J].计算机应用研究, 2012, 29 (8) :2865-2867.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Extended fuzzy c-means with random sampling techniques for clustering large data">

                                <b>[7]</b> KOTHARI D, NARAYANAN S T, DEVI K K.Extended fuzzy c-means with random sampling techniques for clustering large data[J].International Journal of Innovative Research in Advanced Engineering, 2014, 1 (1) :1-4.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Online fuzzy C means">

                                <b>[8]</b> HORE P, HALL L O, GOLDGOF D B, et al.Online fuzzy c means[C]//Proceedings of NAFIPS’08.Washington D.C., USA:IEEE Press, 2008:1-5.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Single pass fuzzy c means">

                                <b>[9]</b> HORE P, HALL L, GOLDGOF D.Single pass fuzzy C means[C]//Proceedings of IEEE International Fuzzy Systems Conference.Washington D.C., USA:IEEE Press, 2007:1-7.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fuzzy c-Means Algorithms for Very Large Data">

                                <b>[10]</b> HAVENS T C, BEZDEK J C, LECKIE C, et al.Fuzzy c-means algorithms for very large data[J].IEEE Transactions on Fuzzy Systems, 2012, 20 (6) :1130-1146.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Incremental fuzzy clustering withmultiple medoids for large data">

                                <b>[11]</b> WANG Yangtao, CHEN Lihui, MEI Jianping.Incremental fuzzy clustering with multiple medoids for large data[J].IEEE Transactions on Fuzzy Systems, 2014, 22 (6) :1557-1568.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_12" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201601036&amp;v=MDExODBGckNVUkxPZVplUm9GaURoVkwvUEx6N0JiYkc0SDlmTXJvOUdZb1FLREg4NHZSNFQ2ajU0TzN6cXFCdEc=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[12]</b> 戴阳阳, 李朝锋, 徐华.初始点优化与参数自适应的密度聚类算法[J].计算机工程, 2016, 42 (1) :203-209.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_13" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SPQD&amp;filename=SPQD8F177F157213A84F2216E805F5E06BBC&amp;v=MDY1OTNGcG1hQnVIWWZPR1FsZkNwYlEzNU41Z3pMeTR3Szg9TmozYWFydk9IOWJMMlk1QVkra09EdzB4eTJBUjZENTdQWGZpcVdRd0RMS1NOOGpzQ09OdkZTaVdXcjdKSQ==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[13]</b> JIAO Runhai, LIU Shaolong, WEN Wen, et al.Incremental kernel fuzzy c-means with optimizing cluster center initialization and delivery[J].Kybernetes, 2016, 45 (8) :1273-1291.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_14" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSYJ201609030&amp;v=MDU1NzVoVkwvUEx6N1NaTEc0SDlmTXBvOUdaSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm9GaUQ=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[14]</b> 冯振华, 钱雪忠, 赵娜娜.Greedy DBSCAN:一种针对多密度聚类的DBSCAN改进算法[J].计算机应用研究, 2016, 33 (9) :2693-2696, 2700.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fuzzy (c+p)—means clustering and its application to a fuzzy rule-based classifier:Towards good generalization and good interpretability">

                                <b>[15]</b> LESKI J M.Fuzzy (c+p) -means clustering and its application to a fuzzy rule-based classifier:toward good generalization and good interpretability[J].IEEE Transactions on Fuzzy Systems, 2015, 23 (4) :802-812.
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_16" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011600738594&amp;v=MjgyOTFPZmJLN0h0RE5xWTlGWStnSENYVTlvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVVMbkpLRm9UYWhVPU5pZg==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[16]</b> DENG Zhaohong, CHOI K S, CHUNG F L, et al.Enhanced soft subspace clustering integrating within-cluster and between-cluster information[J].Pattern Recognition, 2010, 43 (3) :767-781.
                            </a>
                        </p>
                        <p id="35">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Distance-based clustering of CGH data">

                                <b>[17]</b> LIU Jun, MOHAMMED J, CARTER J, et al.Distance-based clustering of CGH data[J].Bioinformatics, 2006, 22 (16) :1971-1978.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJC201906012" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201906012&amp;v=MjM2NjM0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSb0ZpRGhWTC9QTHo3QmJiRzRIOWpNcVk5RVpvUUtESDg=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
