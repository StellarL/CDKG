<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637129054779337500%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJC201908035%26RESULT%3d1%26SIGN%3dV9DF473lYFKjs9bd6t%252fNGUhwuY4%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201908035&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201908035&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201908035&amp;v=MDQyMTJwNDlHWVlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJxRkNqbFZidk1MejdCYmJHNEg5ak0=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#79" data-title="0 概述 ">0 概述</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#83" data-title="1 相关工作 ">1 相关工作</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#89" data-title="2 语义表示与语义匹配 ">2 语义表示与语义匹配</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#91" data-title="2.1 语义表示">2.1 语义表示</a></li>
                                                <li><a href="#101" data-title="2.2 语义匹配">2.2 语义匹配</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#105" data-title="3 文本匹配模型 ">3 文本匹配模型</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#112" data-title="3.1 文本语义表示">3.1 文本语义表示</a></li>
                                                <li><a href="#115" data-title="3.2 匹配程度矩阵">3.2 匹配程度矩阵</a></li>
                                                <li><a href="#127" data-title="3.3 映射关系学习">3.3 映射关系学习</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#129" data-title="4 实验结果与分析 ">4 实验结果与分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#130" data-title="4.1 数据集">4.1 数据集</a></li>
                                                <li><a href="#132" data-title="4.2 评价标准">4.2 评价标准</a></li>
                                                <li><a href="#134" data-title="4.3 数据预处理">4.3 数据预处理</a></li>
                                                <li><a href="#137" data-title="4.4 实验过程与结果">4.4 实验过程与结果</a></li>
                                                <li><a href="#143" data-title="4.5 与其他模型的对比">4.5 与其他模型的对比</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#147" data-title="5 结束语 ">5 结束语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#107" data-title="&lt;b&gt;图1 文本匹配模型整体处理流程&lt;/b&gt;"><b>图1 文本匹配模型整体处理流程</b></a></li>
                                                <li><a href="#140" data-title="&lt;b&gt;图2 验证集上F1值随参数&lt;i&gt;K&lt;/i&gt;的变化情况&lt;/b&gt;"><b>图2 验证集上F1值随参数<i>K</i>的变化情况</b></a></li>
                                                <li><a href="#145" data-title="&lt;b&gt;表1 本文模型与BM25及深度文本匹配模型的对比&lt;/b&gt;"><b>表1 本文模型与BM25及深度文本匹配模型的对比</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="217">


                                    <a id="bibliography_1" title=" ROBERTSON S, ZARAGOZA H.The probabilistic relevance framework:BM25 and beyond[J].Foundations and Trends in Information Retrieval, 2009, 3 (4) :333-389." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The probabilistic relevance framework: BM25 and beyond">
                                        <b>[1]</b>
                                         ROBERTSON S, ZARAGOZA H.The probabilistic relevance framework:BM25 and beyond[J].Foundations and Trends in Information Retrieval, 2009, 3 (4) :333-389.
                                    </a>
                                </li>
                                <li id="219">


                                    <a id="bibliography_2" title=" BENGIO Y, DUCHARME R, VINCENT P, et al.A neural probabilistic language model[C]//Proceedings of Innovations in Machine Learning.Berlin, Germany:Springer, 2006:137-186." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A neural probabilistic language model">
                                        <b>[2]</b>
                                         BENGIO Y, DUCHARME R, VINCENT P, et al.A neural probabilistic language model[C]//Proceedings of Innovations in Machine Learning.Berlin, Germany:Springer, 2006:137-186.
                                    </a>
                                </li>
                                <li id="221">


                                    <a id="bibliography_3" title=" 庞亮, 兰艳艳, 徐君, 等.深度文本匹配综述[J].计算机学报, 2017, 40 (4) :985-1003." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJX201704014&amp;v=MTc1ODk5Yk1xNDlFWUlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJxRkNqbFZidlBMejdCZHJHNEg=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         庞亮, 兰艳艳, 徐君, 等.深度文本匹配综述[J].计算机学报, 2017, 40 (4) :985-1003.
                                    </a>
                                </li>
                                <li id="223">


                                    <a id="bibliography_4" title=" HUANG Posen, HE Xiaodong, GAO Jianfeng, et al.Learning deep structured semantic models for Web search using click through data[C]//Proceedings of ACM International Conference on Conference on Information and Knowledge Management.San Francisco, USA:ACM Press, 2013:2333-2338." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning deep structured semantic models for web search using clickthrough data">
                                        <b>[4]</b>
                                         HUANG Posen, HE Xiaodong, GAO Jianfeng, et al.Learning deep structured semantic models for Web search using click through data[C]//Proceedings of ACM International Conference on Conference on Information and Knowledge Management.San Francisco, USA:ACM Press, 2013:2333-2338.
                                    </a>
                                </li>
                                <li id="225">


                                    <a id="bibliography_5" title=" SHEN Yelong, HE Xiaodong, GAO Jianfeng, et al.Learning semantic representations using convolutional neural networks for Web search[C]//Proceedings of International Conference on World Wide Web.New York, USA:ACM Press, 2014:373-374." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning semantic representations using convolutional neural networks for web search">
                                        <b>[5]</b>
                                         SHEN Yelong, HE Xiaodong, GAO Jianfeng, et al.Learning semantic representations using convolutional neural networks for Web search[C]//Proceedings of International Conference on World Wide Web.New York, USA:ACM Press, 2014:373-374.
                                    </a>
                                </li>
                                <li id="227">


                                    <a id="bibliography_6" title=" HU Baotian, LU Zhengdong, LI Hang, et al.Convolutional neural network architectures for matching natural language sentences[C]//Proceedings of the 27th International Conference on Neural Information Processing Systems.New York, USA:ACM Press, 2015:2042-2050." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Convolutional Neural Network Architectures for Matching Natural Language Sentences">
                                        <b>[6]</b>
                                         HU Baotian, LU Zhengdong, LI Hang, et al.Convolutional neural network architectures for matching natural language sentences[C]//Proceedings of the 27th International Conference on Neural Information Processing Systems.New York, USA:ACM Press, 2015:2042-2050.
                                    </a>
                                </li>
                                <li id="229">


                                    <a id="bibliography_7" title=" QIU Xipeng, HUANG Xuanjing.Convolutional neural tensor network architecture for community-based question answering[C]//Proceedings of International Conference on Artificial Intelligence.Palo Alto, USA:AAAI Press, 2015:1305-1311." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Convolutional Neural Tensor Network Architecture for Community-Based Question Answering">
                                        <b>[7]</b>
                                         QIU Xipeng, HUANG Xuanjing.Convolutional neural tensor network architecture for community-based question answering[C]//Proceedings of International Conference on Artificial Intelligence.Palo Alto, USA:AAAI Press, 2015:1305-1311.
                                    </a>
                                </li>
                                <li id="231">


                                    <a id="bibliography_8" title=" PALANGI H, DENG Li, SHEN Yelong, et al.Deep sentence embedding using long short-term memory networks:analysis and application to information retrieval[J].IEEE/ACM Transactions on Audio Speech and Language Processing, 2015, 24 (4) :694-707." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM12E497787579973821021EC00BF1CD47&amp;v=MTI1NjJOdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQ3BiUTM1TnhpeExpNXhLOD1OaWZJWTdLNmE5WEZxSWhOWSs0SUJYVSt6QjRSNno5L1NRcVJyQkpIRDdQbk1iNllDTw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                         PALANGI H, DENG Li, SHEN Yelong, et al.Deep sentence embedding using long short-term memory networks:analysis and application to information retrieval[J].IEEE/ACM Transactions on Audio Speech and Language Processing, 2015, 24 (4) :694-707.
                                    </a>
                                </li>
                                <li id="233">


                                    <a id="bibliography_9" title=" YIN Wenpeng, SCH&#220;TZE H.MultiGranCNN:an architecture for general matching of text chunks on multiple levels of granularity[C]//Proceedings of Meeting of the Association for Computational Linguistics and the International Joint Conference on Natural Language Processing.Beijing, China:[s.n.], 2015:63-73." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multi Gran CNN:An Architecture for General Matching of Text Chunks on Multiple Levels of Granularity">
                                        <b>[9]</b>
                                         YIN Wenpeng, SCH&#220;TZE H.MultiGranCNN:an architecture for general matching of text chunks on multiple levels of granularity[C]//Proceedings of Meeting of the Association for Computational Linguistics and the International Joint Conference on Natural Language Processing.Beijing, China:[s.n.], 2015:63-73.
                                    </a>
                                </li>
                                <li id="235">


                                    <a id="bibliography_10" title=" SOCHER R, HUANG E H, PENNINGTON J, et al.Dynamic pooling and unfolding recursive autoencoders for paraphrase detection[J].Advances in Neural Information Processing Systems, 2011, 24:801-809." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection">
                                        <b>[10]</b>
                                         SOCHER R, HUANG E H, PENNINGTON J, et al.Dynamic pooling and unfolding recursive autoencoders for paraphrase detection[J].Advances in Neural Information Processing Systems, 2011, 24:801-809.
                                    </a>
                                </li>
                                <li id="237">


                                    <a id="bibliography_11" title=" WAN Shengxian, LAN Yanyan, GUO Jiafeng, et al.A deep architecture for semantic matching with multiple positional sentence representations[EB/OL].[2018-05-14].https://arxiv.org/abs/1511.08277." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A deep architecture for semantic matching with multiple positional sentence representations">
                                        <b>[11]</b>
                                         WAN Shengxian, LAN Yanyan, GUO Jiafeng, et al.A deep architecture for semantic matching with multiple positional sentence representations[EB/OL].[2018-05-14].https://arxiv.org/abs/1511.08277.
                                    </a>
                                </li>
                                <li id="239">


                                    <a id="bibliography_12" title=" WANG Mingxuan, LU Zhengdong, LI Hang, et al.Syntax based deep matching of short texts[C]//Proceedings of International Joint Conference on Artificial Intelligence.New York, USA:ACM Press, 2015:1354-1361." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Syntax-based deep matching of short texts.&amp;quot;">
                                        <b>[12]</b>
                                         WANG Mingxuan, LU Zhengdong, LI Hang, et al.Syntax based deep matching of short texts[C]//Proceedings of International Joint Conference on Artificial Intelligence.New York, USA:ACM Press, 2015:1354-1361.
                                    </a>
                                </li>
                                <li id="241">


                                    <a id="bibliography_13" title=" LU Zhengdong, LI Hang.A deep architecture for matching short texts[C]//Proceedings of International Conference on Neural Information Processing Systems.New York, USA:ACM Press, 2013:1367-1375." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=&amp;quot;A Deep Architecture for Matching Short Texts.&amp;quot;">
                                        <b>[13]</b>
                                         LU Zhengdong, LI Hang.A deep architecture for matching short texts[C]//Proceedings of International Conference on Neural Information Processing Systems.New York, USA:ACM Press, 2013:1367-1375.
                                    </a>
                                </li>
                                <li id="243">


                                    <a id="bibliography_14" title=" HU Baotian, LU Zhengdong, LI Hang, et al.Convolutional neural network architectures for matching natural language sentences[C]//Proceedings of International Conference on Neural Information Processing Systems.New York, USA:ACM Press, 2015:2042-2050." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Convolutional Neural Network Architectures for Matching Natural Language Sentences">
                                        <b>[14]</b>
                                         HU Baotian, LU Zhengdong, LI Hang, et al.Convolutional neural network architectures for matching natural language sentences[C]//Proceedings of International Conference on Neural Information Processing Systems.New York, USA:ACM Press, 2015:2042-2050.
                                    </a>
                                </li>
                                <li id="245">


                                    <a id="bibliography_15" title=" PANG Liang, LAN Yanyan, GUO Jiafeng, et al.Text matching as image reconginition[C]//Proceedings of the 30th AAAI Conference on Artificial Intelligence.Palo Alto, USA:AAAI Press, 2016:2793-2799." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Text Matching as Image Recognition">
                                        <b>[15]</b>
                                         PANG Liang, LAN Yanyan, GUO Jiafeng, et al.Text matching as image reconginition[C]//Proceedings of the 30th AAAI Conference on Artificial Intelligence.Palo Alto, USA:AAAI Press, 2016:2793-2799.
                                    </a>
                                </li>
                                <li id="247">


                                    <a id="bibliography_16" title=" WAN Shengxia, LAN Yanyan, XU Jun, et al.Match-SRNN:modeling the recursive matching structure with spatial RNN[J].Computers and Graphics, 2016, 28 (5) :731-745." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Match-SRNN:modeling the recursive matching structure with spatial RNN">
                                        <b>[16]</b>
                                         WAN Shengxia, LAN Yanyan, XU Jun, et al.Match-SRNN:modeling the recursive matching structure with spatial RNN[J].Computers and Graphics, 2016, 28 (5) :731-745.
                                    </a>
                                </li>
                                <li id="249">


                                    <a id="bibliography_17" title=" LIU Tieyan.Learning to rank for information retrieval[J].ACM SIGIR Forum, 2010, 41 (2) :904-914." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning to rank for information retrieval">
                                        <b>[17]</b>
                                         LIU Tieyan.Learning to rank for information retrieval[J].ACM SIGIR Forum, 2010, 41 (2) :904-914.
                                    </a>
                                </li>
                                <li id="251">


                                    <a id="bibliography_18" title=" PONTE J M, CROFT W B.A language modeling approach to information retrieval[C]//Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval.New York, USA:ACM Press, 1998:275-281." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A language modeling approach to information retrieval">
                                        <b>[18]</b>
                                         PONTE J M, CROFT W B.A language modeling approach to information retrieval[C]//Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval.New York, USA:ACM Press, 1998:275-281.
                                    </a>
                                </li>
                                <li id="253">


                                    <a id="bibliography_19" title=" BERGER A, LAFFERTY J.Information retrieval as statistical translation[C]//Proceedings of International ACM SIGIR Conference on Research and Development in Information Retrieval.New York, USA:ACM Press, 1999:222-229." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Information Retrieval as Statistical Translation">
                                        <b>[19]</b>
                                         BERGER A, LAFFERTY J.Information retrieval as statistical translation[C]//Proceedings of International ACM SIGIR Conference on Research and Development in Information Retrieval.New York, USA:ACM Press, 1999:222-229.
                                    </a>
                                </li>
                                <li id="255">


                                    <a id="bibliography_20" title=" CHOPRA S, HADSELL R, LECUN Y.Learning a similarity metric discriminatively, with application to face verification[C]//Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2005:539-546." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning a similarity metric discriminatively,with application to face verification">
                                        <b>[20]</b>
                                         CHOPRA S, HADSELL R, LECUN Y.Learning a similarity metric discriminatively, with application to face verification[C]//Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2005:539-546.
                                    </a>
                                </li>
                                <li id="257">


                                    <a id="bibliography_21" title=" MITRA B, DIAZ F, CRASWELL N.Learning to match using local and distributed representations of text for Web search[C]//Proceedings of the 26th International Conference on World Wide Web.Washington D.C., USA:IEEE Press, 2017:1291-1299." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning to match using local and distributed representations of text for web search">
                                        <b>[21]</b>
                                         MITRA B, DIAZ F, CRASWELL N.Learning to match using local and distributed representations of text for Web search[C]//Proceedings of the 26th International Conference on World Wide Web.Washington D.C., USA:IEEE Press, 2017:1291-1299.
                                    </a>
                                </li>
                                <li id="259">


                                    <a id="bibliography_22" title=" XIONG Chenyan, DAI Zhuyun, CALLAN J, et al.End-to-end neural ad-hoc ranking with kernel pooling[C]//Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval.New York, USA:ACM Press, 2017:55-64." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=End-to-end neural ad-hoc ranking with kernel pooling">
                                        <b>[22]</b>
                                         XIONG Chenyan, DAI Zhuyun, CALLAN J, et al.End-to-end neural ad-hoc ranking with kernel pooling[C]//Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval.New York, USA:ACM Press, 2017:55-64.
                                    </a>
                                </li>
                                <li id="261">


                                    <a id="bibliography_23" title=" DAI Zhuyun, XIONG Chenyan, CALLAN J, et al.Convolutional neural networks for soft-matching n-grams in ad-hoc search[C]//Proceedings of the 11th ACM International Conference on Web Search and Data Mining.New York, USA:ACM Press, 2018:126-134." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Convolutional neural networks for soft-matching n-grams in ad-hoc search">
                                        <b>[23]</b>
                                         DAI Zhuyun, XIONG Chenyan, CALLAN J, et al.Convolutional neural networks for soft-matching n-grams in ad-hoc search[C]//Proceedings of the 11th ACM International Conference on Web Search and Data Mining.New York, USA:ACM Press, 2018:126-134.
                                    </a>
                                </li>
                                <li id="263">


                                    <a id="bibliography_24" title=" PANG Liang, LAN Yanyan, GUO Jiafeng, et al.DeepRank:a new deep architecture for relevance ranking in information retrieval[C]//Proceedings of 2017 ACM on Conference on Information and Knowledge Management.New York, USA:ACM Press, 2017:257-266." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep Rank:A New Deep Architecture for Relevance Ranking in Information Retrieval">
                                        <b>[24]</b>
                                         PANG Liang, LAN Yanyan, GUO Jiafeng, et al.DeepRank:a new deep architecture for relevance ranking in information retrieval[C]//Proceedings of 2017 ACM on Conference on Information and Knowledge Management.New York, USA:ACM Press, 2017:257-266.
                                    </a>
                                </li>
                                <li id="265">


                                    <a id="bibliography_25" title=" GUO Jiafeng, FAN Yixing, AI Qingyao, et al.A deep relevance matching model for ad-hoc retrieval[C]//Proceedings of ACM International on Conference on Information and Knowledge Management.New York, USA:ACM Press, 2016:55-64." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Deep Relevance Matching Model for Ad-hoc Retrieval">
                                        <b>[25]</b>
                                         GUO Jiafeng, FAN Yixing, AI Qingyao, et al.A deep relevance matching model for ad-hoc retrieval[C]//Proceedings of ACM International on Conference on Information and Knowledge Management.New York, USA:ACM Press, 2016:55-64.
                                    </a>
                                </li>
                                <li id="267">


                                    <a id="bibliography_26" title=" YANG Liu, AI Qingyao, GUO Jiafeng, et al.aNMM:ranking short answer texts with attention-based neural matching model[C]//Proceedings of ACM International Conference on Information and Knowledge Management.New York, USA:ACM Press, 2016:287-296." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=aNMM:Ranking Short Answer Texts with Attention-Based Neural Matching Model">
                                        <b>[26]</b>
                                         YANG Liu, AI Qingyao, GUO Jiafeng, et al.aNMM:ranking short answer texts with attention-based neural matching model[C]//Proceedings of ACM International Conference on Information and Knowledge Management.New York, USA:ACM Press, 2016:287-296.
                                    </a>
                                </li>
                                <li id="269">


                                    <a id="bibliography_27" title=" SOWA J F.Extending and formalizing the framework for information systems architecture[J].IBM Systems Journal, 2010, 31 (3) :590-616." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Extending and formalizing the framework for information systems architecture">
                                        <b>[27]</b>
                                         SOWA J F.Extending and formalizing the framework for information systems architecture[J].IBM Systems Journal, 2010, 31 (3) :590-616.
                                    </a>
                                </li>
                                <li id="271">


                                    <a id="bibliography_28" title=" BANARESCU L, BONIAL C, CAI Shu, et al.Abstract meaning representation for sembanking[C]//Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse.Sofia, Bulgaria:[s.n.], 2013:178-186." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Abstract meaning representation for sembanking">
                                        <b>[28]</b>
                                         BANARESCU L, BONIAL C, CAI Shu, et al.Abstract meaning representation for sembanking[C]//Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse.Sofia, Bulgaria:[s.n.], 2013:178-186.
                                    </a>
                                </li>
                                <li id="273">


                                    <a id="bibliography_29" >
                                        <b>[29]</b>
                                     陆汝占.中文检索与汉语语义概念图表示[M].北京:清华大学出版社, 2009.</a>
                                </li>
                                <li id="275">


                                    <a id="bibliography_30" title=" FELLBAUM C, MILLER G.WordNet:an electronic lexical database[J].Computational Linguistics, 1998, 25 (2) :292-296." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=WordNet:an electronic lexical database">
                                        <b>[30]</b>
                                         FELLBAUM C, MILLER G.WordNet:an electronic lexical database[J].Computational Linguistics, 1998, 25 (2) :292-296.
                                    </a>
                                </li>
                                <li id="277">


                                    <a id="bibliography_31" title=" DONG Zhendong, DONG Qiang.HowNet——a hybrid language and knowledge resource[C]//Proceedings of International Conference on Natural Language Processing and Knowledge Engineering.Washington D.C., USA:IEEE Press, 2003:820-824." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=HowNet-A Hybrid Language and Knowledge Resource">
                                        <b>[31]</b>
                                         DONG Zhendong, DONG Qiang.HowNet——a hybrid language and knowledge resource[C]//Proceedings of International Conference on Natural Language Processing and Knowledge Engineering.Washington D.C., USA:IEEE Press, 2003:820-824.
                                    </a>
                                </li>
                                <li id="279">


                                    <a id="bibliography_32" title=" GRAVES A.Long short-term memory[M].Berlin, Germany:Springer, 2012." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Long short-term memory">
                                        <b>[32]</b>
                                         GRAVES A.Long short-term memory[M].Berlin, Germany:Springer, 2012.
                                    </a>
                                </li>
                                <li id="281">


                                    <a id="bibliography_33" title=" MIKOLOV T, CHEN Kai, CORRADO G, et al.Efficient estimation of word representations in vector space[EB/OL].[2018-05-14].https://arxiv.org/abs/1301.3781." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Efficient estimation of word representations in vector space">
                                        <b>[33]</b>
                                         MIKOLOV T, CHEN Kai, CORRADO G, et al.Efficient estimation of word representations in vector space[EB/OL].[2018-05-14].https://arxiv.org/abs/1301.3781.
                                    </a>
                                </li>
                                <li id="283">


                                    <a id="bibliography_34" title=" BOTEVA V, GHOLIPOUR D, SOKOLOV A, et al.A full-text learning to rank dataset for medical information retrieval[C]//Proceedings of European Conference on Information Retrieval.Berlin, Germany:Springer, 2016:716-722." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A full-text learning to rank dataset for medical information retrieval">
                                        <b>[34]</b>
                                         BOTEVA V, GHOLIPOUR D, SOKOLOV A, et al.A full-text learning to rank dataset for medical information retrieval[C]//Proceedings of European Conference on Information Retrieval.Berlin, Germany:Springer, 2016:716-722.
                                    </a>
                                </li>
                                <li id="285">


                                    <a id="bibliography_35" title=" HAN Hui, WANG Wenyuan, MAO Binghuan.Borderline-SMOTE:a new over-sampling method in imbalanced data sets learning[C]//Proceedings of International Conference on Advances in Intelligent Computing.Berlin, Germany:Springer, 2005:878-887." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Borderline-SMOTE:A new over-sampling method in imbalanced data sets learning">
                                        <b>[35]</b>
                                         HAN Hui, WANG Wenyuan, MAO Binghuan.Borderline-SMOTE:a new over-sampling method in imbalanced data sets learning[C]//Proceedings of International Conference on Advances in Intelligent Computing.Berlin, Germany:Springer, 2005:878-887.
                                    </a>
                                </li>
                                <li id="287">


                                    <a id="bibliography_36" title=" MANNING C D, SURDEANU M, BAUER J, et al.The Stanford coreNLP natural language processing toolkit[C]//Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics:System Demonstrations.Berlin, Germany:Springer, 2014:14-20." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The Stanford Core NLP Natural Language Processing Toolkit">
                                        <b>[36]</b>
                                         MANNING C D, SURDEANU M, BAUER J, et al.The Stanford coreNLP natural language processing toolkit[C]//Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics:System Demonstrations.Berlin, Germany:Springer, 2014:14-20.
                                    </a>
                                </li>
                                <li id="289">


                                    <a id="bibliography_37" title=" FAN Yixing, PANG Liang, HOU Jianpeng, et al.MatchZoo:a toolkit for deep text matching[EB/OL].[2018-04-13].https://arxiv.org/abs/1707.07270." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=MatchZoo:a toolkit for deep text matching">
                                        <b>[37]</b>
                                         FAN Yixing, PANG Liang, HOU Jianpeng, et al.MatchZoo:a toolkit for deep text matching[EB/OL].[2018-04-13].https://arxiv.org/abs/1707.07270.
                                    </a>
                                </li>
                                <li id="291">


                                    <a id="bibliography_38" title=" SRIVASTAVA N, HINTON G, KRIZHEVSKY A, et al.Dropout:a simple way to prevent neural networks from overfitting[J].Journal of Machine Learning Research, 2014, 15 (1) :1929-1958." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Dropout:A simple way to prevent neural networks from overfitting">
                                        <b>[38]</b>
                                         SRIVASTAVA N, HINTON G, KRIZHEVSKY A, et al.Dropout:a simple way to prevent neural networks from overfitting[J].Journal of Machine Learning Research, 2014, 15 (1) :1929-1958.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJC" target="_blank">计算机工程</a>
                2019,45(08),210-216+223 DOI:10.19678/j.issn.1000-3428.0051810            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于依存关系与神经网络的文本匹配模型</b></span>
 <span class="shoufa"></span>                                     </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%94%84%E5%8D%93&amp;code=40355801&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">甄卓</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%99%88%E7%8E%89%E6%B3%89&amp;code=08514665&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">陈玉泉</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%8A%E6%B5%B7%E4%BA%A4%E9%80%9A%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E5%B7%A5%E7%A8%8B%E7%B3%BB&amp;code=0054402&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">上海交通大学计算机科学与工程系</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>为增强文本匹配模型的文本语义捕捉能力并提高语义匹配准确度, 提出一种基于词嵌入与依存关系的文本匹配模型。构建融合词语义和词间依存关系的语义表示, 通过余弦均值卷积和K-Max池化操作获得描述两段文本各部分语义匹配程度的矩阵, 并采用长短期记忆网络学习匹配程度矩阵与真实匹配程度之间的映射关系。实验结果表明, 该模型的F1值为0.927 4, 相比BM25及深度文本匹配模型准确度更高。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">文本匹配;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%AF%AD%E4%B9%89%E5%8C%B9%E9%85%8D&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">语义匹配;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%BE%9D%E5%AD%98%E5%85%B3%E7%B3%BB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">依存关系;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%AF%8D%E5%B5%8C%E5%85%A5&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">词嵌入;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%BD%99%E5%BC%A6%E5%9D%87%E5%80%BC%E5%8D%B7%E7%A7%AF&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">余弦均值卷积;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=K-Max%E6%B1%A0%E5%8C%96&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">K-Max池化;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%95%BF%E7%9F%AD%E6%9C%9F%E8%AE%B0%E5%BF%86%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">长短期记忆网络;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    甄卓 (1988—) , 男, 硕士研究生, 主研方向为自然语言处理、文本匹配;;
                                </span>
                                <span>
                                    陈玉泉, 副教授。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-06-12</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金 (61673266);</span>
                    </p>
            </div>
                    <h1><b>Text Matching Model Based on Dependency Relation and Neural Network</b></h1>
                    <h2>
                    <span>ZHEN Zhuo</span>
                    <span>CHEN Yuquan</span>
            </h2>
                    <h2>
                    <span>Department of Computer Science and Engineering, Shanghai Jiaotong University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>In order to enhance the text semantic capture ability of text matching model and improve the semantic matching accuracy, a text matching model based on word embedding and dependency relation is proposed.It constructs the semantic representation of the fusion of word semantic and dependency relation between words, and obtains the matrix describing the semantic matching degree of each part of the two texts by cosine mean convolution and K-Max pooling operation.The Long-Short Term Memory (LSTM) network is used to learn the mapping relationship between the matching degree matrix and the true matching degree.Experimental results show that the F1 value of the model is 0.927 4, which is more accurate than the BM25 and deep text matching models.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=text%20matching&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">text matching;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=semantic%20matching&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">semantic matching;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=dependency%20relation&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">dependency relation;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=word%20embedding&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">word embedding;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=cosine%20mean%20convolution&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">cosine mean convolution;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=K-Max%20pooling&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">K-Max pooling;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Long-Short%20Term%20Memory%20(LSTM)%20network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Long-Short Term Memory (LSTM) network;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-06-12</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="79" name="79" class="anchor-tag">0 概述</h3>
                <div class="p1">
                    <p id="80">传统文本匹配模型 (如BM25<citation id="293" type="reference"><link href="217" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>) 是基于词袋模型的精确匹配模型, 该模型中的词精确匹配不能捕捉到词的语义相似性, 并且词袋模型忽略了词之间的关系, 对于用词相近但语义不同的句子, 难以区分其语义差异。</p>
                </div>
                <div class="p1">
                    <p id="81">词嵌入技术<citation id="294" type="reference"><link href="219" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>用语义空间中的稠密向量表示词, 语义相近的词, 其词向量也相似, 因此用词向量表示词, 可以表达词的语义。基于词嵌入技术生成了很多深度文本匹配模型。根据特征提取方式的不同, 将基于神经网络的文本匹配模型分为基于单语义表示的模型、基于多语义文档表示的模型和直接对匹配模式建模的模型<citation id="295" type="reference"><link href="221" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>。基于单语义文档表示的模型利用神经网络, 以一个高维稠密向量表示文档。在获得2个文档的向量表示后, 将向量的相似度作为文档的匹配程度。典型基于单语义表示的深度文本匹配模型包括使用全连接网络的深度语义结构模型 (Deep Semantic Structured Model, DSSM) <citation id="296" type="reference"><link href="223" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>、基于卷积神经网络的卷积深度语义结构模型 (ConvolutionalDeep Semantic Structured Model, CDSSM) <citation id="297" type="reference"><link href="225" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>、ARC-I<citation id="298" type="reference"><link href="227" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>和CNTN<citation id="299" type="reference"><link href="229" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>、基于循环神经网络的LSTM-RNN<citation id="300" type="reference"><link href="231" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>。基于多语义表示的模型综合考虑了文本的词、短语级别的局部表达特征和句子级别的全局表达特征。多粒度匹配可以弥补基于单语义表示的模型在压缩文本过程中产生的信息损失。多粒度卷积神经网络 (MultiGranCNN) <citation id="301" type="reference"><link href="233" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>、基于递归神经网络的uRAE<citation id="302" type="reference"><link href="235" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>和多视角循环神经网络 (MVLSTM) <citation id="303" type="reference"><link href="237" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>是此类模型的代表。直接对匹配模式建模的模型无需为文本构建单一或者多粒度的向量表示, 而是先使两段文本进行交互, 关注关键词、短语及其相对位置是否匹配, 进而判断文本匹配程度。典型的直接对匹配模式建模的模型包括DeepMatchTree<citation id="304" type="reference"><link href="239" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>、主题深度匹配模型 (DeepMatch) <citation id="305" type="reference"><link href="241" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>、卷积网络深度匹配模型 (ARC-II) <citation id="306" type="reference"><link href="243" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>、MatchPyramid<citation id="307" type="reference"><link href="245" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>和基于循环网络的Match-SRNN<citation id="308" type="reference"><link href="247" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="82">当前的深度文本匹配模型相比传统模型使用了词向量表示词, 可以更全面描述词语义, 同时关注词的相对位置, 可从中发现相似的结构, 从而提高匹配准确度。但是, 在此类模型中词的相对位置不能代表词间依存关系, 通常使用复杂的神经网络, 训练困难、解释性差, 并且难以扩展, 无法结合新的关于语义关系的研究成果。本文融合词语义和词间依存关系, 为文本生成语义内涵更丰富的语义表示, 进而实现更准确的语义匹配。</p>
                </div>
                <h3 id="83" name="83" class="anchor-tag">1 相关工作</h3>
                <div class="p1">
                    <p id="84">传统文本匹配模型大致可以分为:1) 基于人工提取特征的模型, 例如TF-IDF<citation id="309" type="reference"><link href="217" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>和Learning to rank<citation id="310" type="reference"><link href="249" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>, 由于特征通常基于精确匹配, 因此这些模型在词语义捕捉能力方面有所欠缺;2) 对后验概率进行建模的模型, 例如Language modelling<citation id="311" type="reference"><link href="251" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>和Translation models<citation id="312" type="reference"><link href="253" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>, 这类模型在计算过程中通常使用独立性假设, 即文本中出现的词相互独立, 从而失去捕捉词间依赖关系的能力。</p>
                </div>
                <div class="p1">
                    <p id="85">根据特征提取方式的不同<citation id="313" type="reference"><link href="221" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>, 将深度文本匹配模型大致分为3个类别:</p>
                </div>
                <div class="p1">
                    <p id="86">1) 基于单语义文本表示的模型。这类模型利用深度神经网络, 使用一个高维稠密向量表示文档。在获得2个文档的向量表示后, 以向量的相似度作为文档的匹配程度。DSSM<citation id="314" type="reference"><link href="223" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>针对查询项和文档的匹配程度进行建模, 其结构是典型的Siamese网络结构<citation id="315" type="reference"><link href="255" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>, 使用5层全连接神经网络对文本进行量化, 获得文本的向量表示, 然后使用余弦相似度决定文本匹配程度。但是, DSSM使用了词袋模型, 完全忽略了词之间序的关系, 且全连接网络参数太多, 以致难以优化。为解决上述问题, 在DSSM的基础上, 基于单词序列的CDSSM<citation id="316" type="reference"><link href="225" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>被提出, CDSSM使用卷积神经网络代替DSSM中的全连接层, 提升了匹配程度判断的准确度。ARC-I<citation id="317" type="reference"><link href="227" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>模型使用卷积神经网络进行文本匹配, 直接用卷积神经网络将待匹配的2个文档表达为定长的向量, 然后将2个文档向量拼接起来, 通过一个多层的全连接神经网络表示匹配程度。CNTN<citation id="318" type="reference"><link href="229" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>与ARC-I不同, 在使用卷积神经网络获得文档的表示向量后, 使用张量神经网络对2个文档的匹配程度进行建模。这些使用了卷积神经网络的模型虽然尝试对词间依赖关系进行建模, 但是依赖关系局限于滑动窗口内, 无法捕捉长距离的依赖关系。为解决卷积神经网络无法捕捉长距离依赖关系的问题, LSTM-RNN<citation id="319" type="reference"><link href="231" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>使用长短期记忆网络将查询和文档都表达为向量, 以向量余弦作为匹配程度的度量。为兼顾文本的局部表示和分布式表示, DUET<citation id="320" type="reference"><link href="257" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>用2个分离的神经网络学习上述2种表示, 2个神经网络训练过程完全同步。最终将2个神经网络的输出相结合, 得到文本的向量表示, 用于度量文本间的匹配程度。</p>
                </div>
                <div class="p1">
                    <p id="87">2) 基于多语义文本表示的模型。这类模型综合考虑文本的词、短语级别的局部表示和句子级别的全局表示。多粒度的匹配可以很大程度上弥补单语义模型在压缩文本过程中引起的信息损失。uRAE<citation id="321" type="reference"><link href="235" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>是可伸展递归自动编码器模型。该模型首先对文本进行句法分析, 构建句法树作为递归自动编码器的树状结构, 然后通过无监督训练, 使递归自动编码器可以对词、短语和句子进行编码, 再按照遍历语法树的顺序生成文本间的匹配程度矩阵。MultiGranCNN<citation id="322" type="reference"><link href="233" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>将句子分解为单词级别、短语级别、长短语级别和句子级别, 用卷积神经网络获得文本在各个级别的向量表示。在计算2个文本的匹配程度时, 使用不同级别的向量表达分别计算匹配程度, 然后进行动态最大值池化, 得到2个文本的最终匹配程度。MVLSTM<citation id="323" type="reference"><link href="237" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>为了克服长短期记忆网络存在的位置偏见, 从2个方向同时扫描文本, 在文本的每一个位置都得到2个表示, 即从文本开始位置到当前位置计算得到的表示和从文本结束位置逆向扫描到当前位置计算得到的表示。然后将2个表示相结合作为文本以当前位置为视角的向量表示, 文本也就被表示成多个视角向量表示的集合。判断两段文本的匹配程度时, 将各个视角的向量表示两两计算向量相似度, 得到一个相似度矩阵, 再连接动态最大值池化层和一个全连接网络, 得出2个文本的匹配程度。KNRM<citation id="324" type="reference"><link href="259" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>是一个端到端的匹配模型, 首先经过词嵌入层学习词向量, 然后经过翻译层, 构建查询中词和待匹配文档中词的转化矩阵, 再通过核池化层, 提取不同层次的软匹配特征, 最后通过Learning to rank层, 汇总各个层次的特征匹配程度, 得到最终的匹配结果。与KNRM不同的是, CONV-KRMM<citation id="325" type="reference"><link href="261" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>在生成转化矩阵之前, 先使用卷积神经网络生成基于<i>n</i>元模型的词向量, 然后交叉匹配查询和待匹配文档中的词向量, 生成转化矩阵。</p>
                </div>
                <div class="p1">
                    <p id="88">3) 直接对匹配模式建模的模型。这类模型无需为文本构建单一或者多粒度的向量表示, 而是使两段文本进行交互, 关注关键词和关键词的相对位置是否匹配, 进而捕获匹配结构, 给出匹配程度。DeepMatch<citation id="326" type="reference"><link href="241" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>分为局部匹配层和综合层2个部分。局部匹配层由多个不同的局部匹配模型组成, 输出2个文本的多种局部匹配结果。综合层是一个多层神经网络, 用于综合得到的各种局部匹配结果。DeepMatch在表示文本时使用词袋模型, 因此无法捕捉词间关系。为克服词袋模型导致的无法捕捉词间关系的问题<citation id="327" type="reference"><link href="239" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>, 做出以下改进: (1) 采用依存树作为文本的表示; (2) 在局部匹配时, 不考虑词集合间的匹配程度, 而是先根据文本的依存树是否包含有效子树对生成一个稀疏的表示, 作为局部匹配的依据。ARC-II<citation id="328" type="reference"><link href="243" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>首先用词向量序列表示文本, 进而使用滑动窗口得到词向量组序列, 以词向量组序列作为基本单元, 经过多次卷积和池化得到一个描述2个文本的匹配程度的向量, 再用一个多层神经网络作为综合层, 综合考察匹配程度向量各个维度的信息, 给出最终的匹配程度。MatchPyramid<citation id="329" type="reference"><link href="245" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>模型首先基于2个文本中词向量之间的相似度和词在句子中的空间位置, 构建一个匹配矩阵。然后文本匹配问题被视作匹配矩阵上的图像识别问题, 使用卷积神经网络判断文本的匹配程度。由于卷积神经网络只能捕捉窗口内词的位置关系, 因此Match-SRNN<citation id="330" type="reference"><link href="247" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>将捕捉词间关系能力更强的二维循环神经网络应用在匹配矩阵上, 增强了模型捕捉文本间结构相似性的能力。DeepRank<citation id="331" type="reference"><link href="263" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>与MatchPyramid和Match-SRNN类似, 也是先生成匹配矩阵, 然后使用卷积神经网络或者二维循环神经网络识别匹配矩阵的特征。但不同的是, DeepRank在构建匹配矩阵时, 并不是基于文本中词的相似性, 而是基于2个文本中出现的上下文相似性。DRMM<citation id="332" type="reference"><link href="265" rel="bibliography" /><sup>[<a class="sup">25</a>]</sup></citation>模型首先为查询和文档构建基于词向量的交互矩阵, 然后将查询中的每个词与文档的交互向量转化成柱状图, 每个柱状图经过前馈神经网络给出一个匹配程度, 再通过神经网络为查询中的词分配权重, 最后将查询中每个词对应的匹配程度做加权和, 得出最终匹配结果。aNMM<citation id="333" type="reference"><link href="267" rel="bibliography" /><sup>[<a class="sup">26</a>]</sup></citation>在构建基于词向量的匹配矩阵后, 先通过一个基于值共享的权重分配层, 为匹配矩阵的不同部分分配权重, 然后通过全连接前馈神经网络和一个注意力层, 给出最终的匹配结果。</p>
                </div>
                <h3 id="89" name="89" class="anchor-tag">2 语义表示与语义匹配</h3>
                <div class="p1">
                    <p id="90">人类匹配文本的过程可以分为2个阶段, 即文本语义理解阶段和文本语义匹配阶段。如果能够获得内涵丰富的文本语义表示, 则将提升文本匹配的准确度。</p>
                </div>
                <h4 class="anchor-tag" id="91" name="91">2.1 语义表示</h4>
                <div class="p1">
                    <p id="92">现有对于语义表示的研究, 如概念图<citation id="334" type="reference"><link href="269" rel="bibliography" /><sup>[<a class="sup">27</a>]</sup></citation>、抽象语义表示 (Abstract Meaning Representation, AMR) <citation id="335" type="reference"><link href="271" rel="bibliography" /><sup>[<a class="sup">28</a>]</sup></citation>和汉语语义概念图<citation id="336" type="reference"><link href="273" rel="bibliography" /><sup>[<a class="sup">29</a>]</sup></citation>, 都将语义表示成词语义和词之间关系组成的图 (树) 。另外, 语言知识库如WordNet<citation id="337" type="reference"><link href="275" rel="bibliography" /><sup>[<a class="sup">30</a>]</sup></citation>和HowNet<citation id="338" type="reference"><link href="277" rel="bibliography" /><sup>[<a class="sup">31</a>]</sup></citation>同时定义概念以及概念间可能的关系。因此, 若要准确全面地表达一段文本的语义, 需要同时考虑词语义和词间关系。</p>
                </div>
                <div class="p1">
                    <p id="93">人类在判断两段文本的语义是否匹配时, 首先会判断是否使用语义相近的词, 然后判断词之间的关系是否也相近。因此, 如果两段文本使用语义相近的词, 并且词之间的关系也比较相似, 则两段文本可能高度匹配。本文为文本构建满足以下要求的语义表示:</p>
                </div>
                <div class="p1">
                    <p id="94">1) 语义表示中要体现出文本中出现的词以及词之间的关系。</p>
                </div>
                <div class="p1">
                    <p id="95">2) 语义相近的词、相似的词间关系, 在语义表示中也相似。</p>
                </div>
                <div class="p1">
                    <p id="96">在将文本的语义解析为词和词间依存关系组成的图 (树) 结构后, 可以通过以下方式生成满足上述要求的语义表示:</p>
                </div>
                <div class="p1">
                    <p id="97">1) 按照统一的规则递归遍历图 (树) , 得到边的序列。</p>
                </div>
                <div class="p1">
                    <p id="98">2) 每一条边表示为2个端点词的连接。</p>
                </div>
                <div class="p1">
                    <p id="99">3) 每一个词用词向量表示。</p>
                </div>
                <div class="p1">
                    <p id="100">通过上述处理, 文本被表示成词间依存关系的序列。该语义表示既体现了文本中出现的词, 也体现了文本中出现的词间依存关系。词向量的使用保证了语义相近的词的表示相似, 而词间依存关系表示为2个端点词的词向量的连接, 保证了相似关系的向量表示也相似。通过统一的规则遍历图 (树) 结构, 既摆脱了语序的影响, 又可以保证语义相似的文本, 其词间依存关系序列中存在相近的子序列。</p>
                </div>
                <h4 class="anchor-tag" id="101" name="101">2.2 语义匹配</h4>
                <div class="p1">
                    <p id="102">本节基于语义表示两段文本的语义匹配程度。因为句子通常是表达完整语义的单位, 不失一般性, 先考虑如何描述一个句子和一段文本的语义匹配程度。借鉴图像处理领域常用的卷积方法, 以该句子的语义表示作为模板, 与该段文本的语义表示做卷积运算, 以获得该句子语义表示与文本语义表示各个部分的匹配程度。与图像不同的是, 文本的语义表示是词间依存关系的序列, 每一条关系是由端点词的词向量连接成的向量, 因此用向量余弦表达2个依存关系间的相似性, 用向量余弦的均值表达2个依存关系序列间的相似性, 将上述卷积运算称为余弦均值卷积。在文本匹配任务中, 可以选择两段文本中的一段作为查询, 顺序以查询中的各个句子作为模板, 与另一段文本做余弦均值卷积运算, 得到查询中的各个句子与另一段文本各个部分的匹配程度向量。</p>
                </div>
                <div class="p1">
                    <p id="103">经过余弦均值卷积得到的匹配程度向量存在以下问题:1) 匹配程度向量维度不统一, 这是因为一个句子的语义表示和一段文本的语义表示做余弦均值卷积, 得到的向量的长度取决于2个语义表示长度之差, 所以当查询中的句子长度不同时, 每个句子与文本的匹配程度向量长度不同, 不便于处理;2) 人类在判断一个句子和一段文本是否匹配时, 主要关注该句子是否与文本的某些部分高度匹配, 或者句子与文本的某些部分的匹配程度是否高于其他部分, 句子与这些部分的匹配程度是否达到预期。因此判断一个句子和一段文本是否匹配, 在经过卷积运算得到的匹配程度向量中, 只有最大的几个值有决定性的意义, 其他较小的值可以视作噪声。</p>
                </div>
                <div class="p1">
                    <p id="104">K-Max池化可以有效解决上述问题。对一个向量做K-Max池化是选择该向量中前<i>K</i>大的元素, 保留其先后顺序, 组成一个新的向量。这样一方面解决匹配程度向量维度不统一的问题, 另一方面保留了匹配程度向量中前<i>K</i>大的值, 有效过滤了噪声。经K-Max池化后再连接一个长短期记忆 (Long-Short Term Memory, LSTM) 网络<citation id="339" type="reference"><link href="279" rel="bibliography" /><sup>[<a class="sup">32</a>]</sup></citation>可以有监督地学习到匹配程度矩阵与真实匹配程度之间的映射关系。</p>
                </div>
                <h3 id="105" name="105" class="anchor-tag">3 文本匹配模型</h3>
                <div class="p1">
                    <p id="106">图1简要描述了本文提出的文本匹配模型的整体处理流程。为判断两段文本的匹配程度, 先将两段文本作为输入分别输入到模型中, 模型结合依存关系解析和词嵌入技术, 分别为两段文本构建语义表示。然后以两段文本的语义表示为基础, 经过余弦均值卷积和K-Max池化得到两段文本的匹配程度矩阵, 匹配程度矩阵描述了两段文本各个部分的匹配程度。最后通过训练好的长短期记忆网络, 模型最终返回两段文本的匹配程度。</p>
                </div>
                <div class="area_img" id="107">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201908035_107.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 文本匹配模型整体处理流程" src="Detail/GetImg?filename=images/JSJC201908035_107.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图1 文本匹配模型整体处理流程</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201908035_107.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="108">本文提出的文本匹配模型具体步骤如下:</p>
                </div>
                <div class="p1">
                    <p id="109">1) 为两段文本建立融合词语义和词间依存关系的语义表示。</p>
                </div>
                <div class="p1">
                    <p id="110">2) 通过余弦均值卷积和K-Max池化获得描述两段文本各个部分语义匹配程度矩阵。</p>
                </div>
                <div class="p1">
                    <p id="111">3) 通过长短期记忆网络将匹配程度矩阵映射为两段文本的匹配程度。</p>
                </div>
                <h4 class="anchor-tag" id="112" name="112">3.1 文本语义表示</h4>
                <div class="p1">
                    <p id="113">为获得内涵更丰富的文本语义表示, 需要从词语义和词间关系2个方面入手。本文使用词嵌入工具word2vec<citation id="340" type="reference"><link href="281" rel="bibliography" /><sup>[<a class="sup">33</a>]</sup></citation>获得每个词的词向量, 用词向量代表词语义和衡量词间语义相似度。虽然目前已有多种描述语义的图模型被提出, 如概念图<citation id="341" type="reference"><link href="269" rel="bibliography" /><sup>[<a class="sup">27</a>]</sup></citation>、抽象语义表示<citation id="342" type="reference"><link href="271" rel="bibliography" /><sup>[<a class="sup">28</a>]</sup></citation>和汉语语义概念图<citation id="343" type="reference"><link href="273" rel="bibliography" /><sup>[<a class="sup">29</a>]</sup></citation>, 但它们都缺乏成熟的自动分析标注技术, 而人工标注的成本又过高。在此情况下, 本文使用依存关系代表词间关系。一方面, 依存语法树描述了各个词间的依存关系, 可以反映句子各成分间的语义修饰关系, 而且可以获得长距离的搭配信息, 不受句子成分物理位置的干扰。另一反面, 依存分析已经有成熟的自动标注技术和工具可以使用, 省去了人工标注的成本。</p>
                </div>
                <div class="p1">
                    <p id="114">一段文本经过依存关系解析后, 按照统一的规则遍历依存关系树, 得到一个依存关系序列, 序列中的每一个元素就是2个词间的依存关系。因为一个依存关系可以由2个词唯一地确定, 所以本文将相应2个词的词向量顺序拼接, 作为一个依存关系的向量表示。这样一段文本就被唯一地表达成一个依存关系向量序列, 即拼接词向量序列。</p>
                </div>
                <h4 class="anchor-tag" id="115" name="115">3.2 匹配程度矩阵</h4>
                <div class="p1">
                    <p id="116">在获得两段文本的语义表示后, 需要判断两段文本各个部分的匹配程度。在图像处理领域, 卷积运算常被用来判断一个模板与图片各个位置的匹配程度。由于句子通常是表达语义的完整单位, 而且文本中常包含多个句子, 因此借鉴卷积神经网络的成功经验, 笔者尝试以一段文本中的各个句子作为模板, 通过余弦均值卷积来判断各个句子与另一段文本的各个部分的匹配程度, 进而得到一组匹配程度向量。综合考虑一段文本的各个句子与另一段文本的匹配程度, 得到两段文本各个部分匹配程度的描述。再经过K-Max池化, 规范数据, 过滤噪声, 得到描述2个文本各个部分匹配程度的匹配程度矩阵。</p>
                </div>
                <h4 class="anchor-tag" id="117" name="117">3.2.1 余弦均值卷积</h4>
                <div class="p1">
                    <p id="118">卷积运算的目标是获得模板与文本各个部分的匹配程度, 与某一部分的匹配程度越高, 卷积运算的返回结果越大。不同于图像处理中常用的卷积方式, 余弦均值卷积需要定义适合文本语义表示的卷积运算。假设作为模板的句子的语义表示是<b><i>s</i></b>, 长度为<i>n</i>, 即<b><i>s</i></b>是由<i>n</i>个依存关系向量构成的序列, 每一个依存关系向量由关系包含的2个词的词向量拼接而成。假设<b><i>t</i></b>是一段文本的语义表示中, 某一个位置起的长为<i>n</i>的子序列。通过以下公式计算<b><i>s</i></b>和<b><i>t</i></b>的余弦均值卷积:</p>
                </div>
                <div class="p1">
                    <p id="119" class="code-formula">
                        <mathml id="119"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>C</mi><mi>o</mi><mi>s</mi><mi>i</mi><mi>n</mi><mi>e</mi><mi>Μ</mi><mi>e</mi><mi>a</mi><mi>n</mi><mi>C</mi><mi>o</mi><mi>n</mi><mi>v</mi><mi>o</mi><mi>l</mi><mi>u</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">s</mi><mo>, </mo><mi mathvariant="bold-italic">t</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mn>1</mn><mi>n</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><mfrac><mrow><mi mathvariant="bold-italic">s</mi><msub><mrow></mrow><mi>i</mi></msub><mo>⋅</mo><mi mathvariant="bold-italic">t</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mrow><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">s</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">∥</mo><mo>×</mo><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">t</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">∥</mo></mrow></mfrac></mrow></mstyle></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="120">依次计算2个向量序列中对应位置的2个依存关系的向量余弦, 最后求余弦均值。如果<b><i>s</i></b>和<b><i>t</i></b>中相似的依存关系越多, 则余弦均值就越高。</p>
                </div>
                <div class="p1">
                    <p id="121">在定义上述运算后, 要描述模板和一段文本各个部分的匹配程度, 只需以1为步长, 依次计算模板与文本各个部分的余弦均值卷积。在此用<b><i>text</i></b><sub><i>k</i>, <i>l</i></sub>表示一段文本的语义表示从第<i>k</i>个元素开始到第<i>l</i>个元素为止的子序列, <b><i>text</i></b>的长度为<i>n</i>。并设<b><i>s</i></b>为一个句子的语义表示, 长度为<i>m</i>, 以此作为模板, 计算<b><i>s</i></b>与<b><i>text</i></b>各个部分的匹配程度, 得到一个长为<i>n</i>-<i>m</i>+1的匹配程度向量<b><i>sim</i> (<i>s</i>, <i>text</i></b>) 。匹配程度向量的第<i>i</i>个元素为:</p>
                </div>
                <div class="p1">
                    <p id="122"><b><i>sim</i> (<i>s</i>, <i>text</i></b>) <sub><i>i</i></sub>=<i>CosineMeanConvolution</i> (<b><i>s</i>, <i>text</i></b><sub><i>i</i>, <i>i</i>+<i>m</i>-1</sub>) </p>
                </div>
                <div class="p1">
                    <p id="123">如果被选作模板的文本包含若干个句子, 则卷积后就得到同样个数的匹配程度向量。但是因为匹配程度向量长度与句子语义表示的长度直接相关, 所以这些匹配程度向量长度并不一致。为了维度的统一和噪声去除, 需要进行池化操作。</p>
                </div>
                <h4 class="anchor-tag" id="124" name="124">3.2.2 K-Max池化</h4>
                <div class="p1">
                    <p id="125">经过余弦均值卷积得到的匹配程度向量存在以下问题:1) 匹配程度向量维度不统一, 这是因为一个句子的语义表示和一段文本的语义表示做余弦均值卷积, 得到的向量长度取决于2个语义表示长度的差, 所以当作为模板的文本中的各个句子长度不同时, 每个匹配程度向量的长度不同。2) 人类在判断一个句子和一段文本是否匹配时, 主要关注该句子是否与文本的某些部分高度匹配, 因此在匹配程度向量中, 只有较大的值对匹配程度的判断有重要意义, 而较小的值都可以视作噪声加以过滤。</p>
                </div>
                <div class="p1">
                    <p id="126">K-Max池化有效解决了上述问题。对一个向量进行K-Max池化就是选择该向量中前<i>K</i>大元素, 保持其先后顺序, 组成一个新的向量。K-Max池化一方面可以统一匹配程度向量的维度, 另一方面保留了对于匹配程度判断有重要意义的<i>K</i>个较大的值, 有效过滤掉了噪声。经过K-Max池化, 一个句子和一段文本的匹配程度向量就是一个<i>K</i>维的向量。如果作为模板的文本包含<i>n</i>个句子, 它与另外一段文本各个部分的匹配程度, 就可以用<i>n</i>个<i>K</i>维的匹配程度向量表达, 即一个<i>n</i>×<i>K</i>的矩阵, 称为匹配程度矩阵。如果一个句子和一段文本匹配程度较高, 那么经过K-Max池化得到的匹配程度向量中, 会有较多元素取比较大的值;同理, 一段文本如果与另一段文本匹配程度较高, 那么经过K-Max池化得到的匹配程度矩阵中, 会有较多元素取比较大的值。</p>
                </div>
                <h4 class="anchor-tag" id="127" name="127">3.3 映射关系学习</h4>
                <div class="p1">
                    <p id="128">经过K-Max池化, 本文获得了描述两段文本各个部分匹配程度的匹配程度矩阵。这个矩阵为<i>n</i>×<i>K</i>, 即匹配程度矩阵由<i>n</i>个<i>K</i>维的匹配程度向量组成。矩阵中的第<i>i</i>个向量, 是被选作模板的文本中的第<i>i</i>个句子与另一个文本的匹配程度向量。因此匹配程度矩阵也可以看作匹配程度向量组成的序列, 而且该序列长度不固定。为将不定长序列映射到单值, 需要有监督地训练一个长短期记忆网络来学习匹配程度向量序列与真实的匹配程度间的映射关系。如果文本匹配任务是分类任务, 则可在LSTM后连接逻辑回归模型;如果文本匹配任务是回归任务, 则可在LSTM后连接线性回归模型。</p>
                </div>
                <h3 id="129" name="129" class="anchor-tag">4 实验结果与分析</h3>
                <h4 class="anchor-tag" id="130" name="130">4.1 数据集</h4>
                <div class="p1">
                    <p id="131">文本匹配任务需要大量标注匹配程度的文本对数据, 才能通过有监督学习训练出高质量的回归或者分类模型。虽然现在已有很多组织公开了一些数据集, 但是大部分数据集并不公布原始文本, 而只公布文本对经过处理后得到的特征向量。这样的数据集可以用来训练基于特征的模型, 但却无法用来进行文本语义的挖掘和匹配。文献<citation id="344" type="reference">[<a class="sup">34</a>]</citation>构建了一个用于训练和测试文本匹配模型的数据集, 数据集由查询和文件对组成, 每一个查询和文件对关联了一个匹配等级, 并且所有的查询和文件都以全文本的形式提供。通过实验证明了数据集规模满足文本匹配任务的需要。</p>
                </div>
                <h4 class="anchor-tag" id="132" name="132">4.2 评价标准</h4>
                <div class="p1">
                    <p id="133">文献<citation id="345" type="reference">[<a class="sup">34</a>]</citation>将数据集划分为训练集、验证集和测试集, 分别用于训练模型、调整超参数和衡量模型的性能。数据集中匹配程度从低到高被设置成3个等级。观察数据集后发现, 数据集中实际只包含2个比较高的匹配等级的训练、验证和测试数据, 数据集退化成一个二分类的数据集。选取其中一个等级作为正类, 另一个等级作为负类, 用F1作为指标来评价文本匹配模型在测试集上的性能表现。</p>
                </div>
                <h4 class="anchor-tag" id="134" name="134">4.3 数据预处理</h4>
                <div class="p1">
                    <p id="135">在分析数据集后发现, 数据集中存在严重的数据不均衡现象, 即一类数据的比例显著高于另一类。直接使用原始数据集训练会使模型倾向于将所有的查询和文件对标注为占比高的一类。为提高模型的泛化性能, 需要解决数据不均衡问题。</p>
                </div>
                <div class="p1">
                    <p id="136">通常采用过采样、欠采样和数据合成方法调整不均衡的数据集。过采样方法是从少数类中选取样本, 复制后加入数据集, 以增加少数类样本的占比。欠采样方法则从数据集中剔除一部分多数类样本, 降低多数类样本的占比。数据合成方法, 如SMOTE<citation id="346" type="reference"><link href="285" rel="bibliography" /><sup>[<a class="sup">35</a>]</sup></citation>, 在样本空间中生成与少数类样本相近的样本, 加入数据集以提高少数类的占比。为在处理数据不均衡问题的过程中不会引起信息损失, 本文未采取剔除部分多数类样本的欠采样方法。数据合成方法, 如SMOTE, 依赖于样本间距离的计算以生成与少数类样本接近的样本, 但是使用匹配程度矩阵作为长短期记忆网络的输入, 不定长的输入不便于样本间距离的计算, 所以不适合使用数据合成方法。本文最终选择对少数类样本进行过采样并增加其样本的占比, 使少数类样本和多数类样本的比例维持在0.5左右。</p>
                </div>
                <h4 class="anchor-tag" id="137" name="137">4.4 实验过程与结果</h4>
                <div class="p1">
                    <p id="138">本文借鉴文献<citation id="347" type="reference">[<a class="sup">36</a>]</citation>对文本进行依存关系解析, 并使用word2vec<citation id="348" type="reference"><link href="281" rel="bibliography" /><sup>[<a class="sup">33</a>]</sup></citation>学习词向量。在生成文本的语义表示后, 经过余弦均值卷积和K-Max池化生成匹配程度矩阵, 输入一个双层的LSTM进行训练。为减小过拟合的概率, 在LSTM层间加入dropout<citation id="349" type="reference"><link href="269" rel="bibliography" /><sup>[<a class="sup">27</a>]</sup></citation>层。</p>
                </div>
                <div class="p1">
                    <p id="139">由于K-Max池化中的参数<i>K</i>对模型性能有重要影响, 因此选取不同<i>K</i>值在验证集上进行验证, 结果如图2所示。</p>
                </div>
                <div class="area_img" id="140">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201908035_140.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 验证集上F1值随参数K的变化情况" src="Detail/GetImg?filename=images/JSJC201908035_140.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图2 验证集上F1值随参数<i>K</i>的变化情况</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201908035_140.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="141">当<i>K</i>值较小时, 模型在验证集上的分类效果较差, F1值在低位波动。这是因为过小的<i>K</i>值, 使模型在K-Max池化的过程中去掉了大量对于匹配程度判断有价值的信息, 致使匹配程度矩阵无法准确描述两段文本各部分的匹配程度, 进而导致较差的分类效果。在<i>K</i>值持续增大, 从0增加至5的过程中, F1值呈上升趋势, 这就说明<i>K</i>的增大使K-Max池化保留了更多有价值的信息, 从而增强匹配程度矩阵描述匹配程度的能力, 提升匹配准确度。随着<i>K</i>继续增长, 模型在验证集上的F1值整体呈下降趋势。虽然随着<i>K</i>的增大, K-Max池化会保留更多关于匹配程度的信息, 但同时也保留了对匹配程度判断无意义的噪声, 导致模型分类能力下降。</p>
                </div>
                <div class="p1">
                    <p id="142"><i>K</i>代表了K-Max池化过滤信息的能力。越小的<i>K</i>使K-Max池化具有越强的过滤能力。过小的<i>K</i>使K-Max池化过滤掉大量对匹配程度判断有意义的信息, 使模型不能准确判断两段文本的匹配程度。越大的<i>K</i>使K-Max池化的过滤能力越弱。过大的<i>K</i>导致K-Max池化无法有效过滤噪声, 同样使模型无法对匹配程度做出准确判断。只有当<i>K</i>处于合适的区间时, 模型才可以准确评估文本间的语义匹配程度。在<i>K</i>取5的情况下, 本文模型在测试集上取得的查准率为95.06%, 查全率为90.49%, F1值为0.927 4。</p>
                </div>
                <h4 class="anchor-tag" id="143" name="143">4.5 与其他模型的对比</h4>
                <div class="p1">
                    <p id="144">本文选用9个文本匹配模型作对比, 在数据集上进行对比实验。选取的BM25<citation id="350" type="reference"><link href="217" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>是传统基于词精确匹配的模型, 基于单语义文本表示的深度文本匹配模型包括ARC-I<citation id="351" type="reference"><link href="227" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>、DSSM<citation id="352" type="reference"><link href="223" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>和CDSSM<citation id="353" type="reference"><link href="225" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>, 基于多语义文本表示的深度文本匹配模型包括KNRM<citation id="354" type="reference"><link href="259" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>和MVLSTM<citation id="355" type="reference"><link href="237" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>, 基于直接建模匹配的深度文本匹配模型包括ARC-II<citation id="356" type="reference"><link href="243" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>、DUET<citation id="357" type="reference"><link href="257" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>和MatchPyramid<citation id="358" type="reference"><link href="245" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>。本文借鉴文献<citation id="359" type="reference">[<a class="sup">37</a>]</citation>构建用于对比的深度文本匹配模型。实验结果如表1所示。</p>
                </div>
                <div class="area_img" id="145">
                    <p class="img_tit"><b>表1 本文模型与BM25及深度文本匹配模型的对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="145" border="1"><tr><td><br />模型</td><td>F1值</td></tr><tr><td><br />BM25模型</td><td>0.746 3</td></tr><tr><td><br />ARC-I模型</td><td>0.850 7</td></tr><tr><td><br />DSSM模型</td><td>0.800 5</td></tr><tr><td><br />CDSSM模型</td><td>0.800 7</td></tr><tr><td><br />KNRM模型</td><td>0.801 2</td></tr><tr><td><br />MVLSTM模型</td><td>0.586 2</td></tr><tr><td><br />ARC-II模型</td><td>0.834 1</td></tr><tr><td><br />DUET模型</td><td>0.845 6</td></tr><tr><td><br />MatchPyramid模型</td><td>0.794 0</td></tr><tr><td><br />本文模型</td><td>0.927 4</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="146">通过对比实验的结果可以发现, 本文模型显著优于传统的BM25模型和其他深度文本匹配模型, 说明综合考虑词语义和词间关系, 构建内涵丰富的语义表示, 对于文本语义匹配程度的判别起了很大的促进作用, 提升了分类效果。同时, 本文模型的神经网络部分只使用了一个带有dropout<citation id="360" type="reference"><link href="291" rel="bibliography" /><sup>[<a class="sup">38</a>]</sup></citation>的双层LSTM, 模型相比其他深度文本匹配模型更简洁, 解释性更强。</p>
                </div>
                <h3 id="147" name="147" class="anchor-tag">5 结束语</h3>
                <div class="p1">
                    <p id="148">传统文本匹配模型忽略了词间关系, 不能识别词间语义相似性。基于神经网络的文本匹配模型以词向量表示词间语义相似性, 不能有效利用词间依存关系, 模型可解释性差、复杂度高。为此, 本文提出一种新的文本匹配模型。综合词向量和依存关系, 为文本构建融合词语义和依存关系的语义表示。通过余弦均值卷积和K-Max池化, 建立描述文本间匹配程度的矩阵, 并使用LSTM将匹配程度矩阵映射为文本间的匹配程度。实验结果表明, 该模型匹配准确率较高。下一步将利用词义嵌入和消歧技术, 增强词向量对多义词的语义表达能力。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="217">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The probabilistic relevance framework: BM25 and beyond">

                                <b>[1]</b> ROBERTSON S, ZARAGOZA H.The probabilistic relevance framework:BM25 and beyond[J].Foundations and Trends in Information Retrieval, 2009, 3 (4) :333-389.
                            </a>
                        </p>
                        <p id="219">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A neural probabilistic language model">

                                <b>[2]</b> BENGIO Y, DUCHARME R, VINCENT P, et al.A neural probabilistic language model[C]//Proceedings of Innovations in Machine Learning.Berlin, Germany:Springer, 2006:137-186.
                            </a>
                        </p>
                        <p id="221">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJX201704014&amp;v=MTI2MThRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJxRkNqbFZidlBMejdCZHJHNEg5Yk1xNDlFWUk=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> 庞亮, 兰艳艳, 徐君, 等.深度文本匹配综述[J].计算机学报, 2017, 40 (4) :985-1003.
                            </a>
                        </p>
                        <p id="223">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning deep structured semantic models for web search using clickthrough data">

                                <b>[4]</b> HUANG Posen, HE Xiaodong, GAO Jianfeng, et al.Learning deep structured semantic models for Web search using click through data[C]//Proceedings of ACM International Conference on Conference on Information and Knowledge Management.San Francisco, USA:ACM Press, 2013:2333-2338.
                            </a>
                        </p>
                        <p id="225">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning semantic representations using convolutional neural networks for web search">

                                <b>[5]</b> SHEN Yelong, HE Xiaodong, GAO Jianfeng, et al.Learning semantic representations using convolutional neural networks for Web search[C]//Proceedings of International Conference on World Wide Web.New York, USA:ACM Press, 2014:373-374.
                            </a>
                        </p>
                        <p id="227">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Convolutional Neural Network Architectures for Matching Natural Language Sentences">

                                <b>[6]</b> HU Baotian, LU Zhengdong, LI Hang, et al.Convolutional neural network architectures for matching natural language sentences[C]//Proceedings of the 27th International Conference on Neural Information Processing Systems.New York, USA:ACM Press, 2015:2042-2050.
                            </a>
                        </p>
                        <p id="229">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Convolutional Neural Tensor Network Architecture for Community-Based Question Answering">

                                <b>[7]</b> QIU Xipeng, HUANG Xuanjing.Convolutional neural tensor network architecture for community-based question answering[C]//Proceedings of International Conference on Artificial Intelligence.Palo Alto, USA:AAAI Press, 2015:1305-1311.
                            </a>
                        </p>
                        <p id="231">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM12E497787579973821021EC00BF1CD47&amp;v=MzI0MjREN1BuTWI2WUNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkNwYlEzNU54aXhMaTV4Szg9TmlmSVk3SzZhOVhGcUloTlkrNElCWFUrekI0UjZ6OS9TUXFSckJKSA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b> PALANGI H, DENG Li, SHEN Yelong, et al.Deep sentence embedding using long short-term memory networks:analysis and application to information retrieval[J].IEEE/ACM Transactions on Audio Speech and Language Processing, 2015, 24 (4) :694-707.
                            </a>
                        </p>
                        <p id="233">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multi Gran CNN:An Architecture for General Matching of Text Chunks on Multiple Levels of Granularity">

                                <b>[9]</b> YIN Wenpeng, SCHÜTZE H.MultiGranCNN:an architecture for general matching of text chunks on multiple levels of granularity[C]//Proceedings of Meeting of the Association for Computational Linguistics and the International Joint Conference on Natural Language Processing.Beijing, China:[s.n.], 2015:63-73.
                            </a>
                        </p>
                        <p id="235">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection">

                                <b>[10]</b> SOCHER R, HUANG E H, PENNINGTON J, et al.Dynamic pooling and unfolding recursive autoencoders for paraphrase detection[J].Advances in Neural Information Processing Systems, 2011, 24:801-809.
                            </a>
                        </p>
                        <p id="237">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A deep architecture for semantic matching with multiple positional sentence representations">

                                <b>[11]</b> WAN Shengxian, LAN Yanyan, GUO Jiafeng, et al.A deep architecture for semantic matching with multiple positional sentence representations[EB/OL].[2018-05-14].https://arxiv.org/abs/1511.08277.
                            </a>
                        </p>
                        <p id="239">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Syntax-based deep matching of short texts.&amp;quot;">

                                <b>[12]</b> WANG Mingxuan, LU Zhengdong, LI Hang, et al.Syntax based deep matching of short texts[C]//Proceedings of International Joint Conference on Artificial Intelligence.New York, USA:ACM Press, 2015:1354-1361.
                            </a>
                        </p>
                        <p id="241">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=&amp;quot;A Deep Architecture for Matching Short Texts.&amp;quot;">

                                <b>[13]</b> LU Zhengdong, LI Hang.A deep architecture for matching short texts[C]//Proceedings of International Conference on Neural Information Processing Systems.New York, USA:ACM Press, 2013:1367-1375.
                            </a>
                        </p>
                        <p id="243">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Convolutional Neural Network Architectures for Matching Natural Language Sentences">

                                <b>[14]</b> HU Baotian, LU Zhengdong, LI Hang, et al.Convolutional neural network architectures for matching natural language sentences[C]//Proceedings of International Conference on Neural Information Processing Systems.New York, USA:ACM Press, 2015:2042-2050.
                            </a>
                        </p>
                        <p id="245">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Text Matching as Image Recognition">

                                <b>[15]</b> PANG Liang, LAN Yanyan, GUO Jiafeng, et al.Text matching as image reconginition[C]//Proceedings of the 30th AAAI Conference on Artificial Intelligence.Palo Alto, USA:AAAI Press, 2016:2793-2799.
                            </a>
                        </p>
                        <p id="247">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Match-SRNN:modeling the recursive matching structure with spatial RNN">

                                <b>[16]</b> WAN Shengxia, LAN Yanyan, XU Jun, et al.Match-SRNN:modeling the recursive matching structure with spatial RNN[J].Computers and Graphics, 2016, 28 (5) :731-745.
                            </a>
                        </p>
                        <p id="249">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning to rank for information retrieval">

                                <b>[17]</b> LIU Tieyan.Learning to rank for information retrieval[J].ACM SIGIR Forum, 2010, 41 (2) :904-914.
                            </a>
                        </p>
                        <p id="251">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A language modeling approach to information retrieval">

                                <b>[18]</b> PONTE J M, CROFT W B.A language modeling approach to information retrieval[C]//Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval.New York, USA:ACM Press, 1998:275-281.
                            </a>
                        </p>
                        <p id="253">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Information Retrieval as Statistical Translation">

                                <b>[19]</b> BERGER A, LAFFERTY J.Information retrieval as statistical translation[C]//Proceedings of International ACM SIGIR Conference on Research and Development in Information Retrieval.New York, USA:ACM Press, 1999:222-229.
                            </a>
                        </p>
                        <p id="255">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning a similarity metric discriminatively,with application to face verification">

                                <b>[20]</b> CHOPRA S, HADSELL R, LECUN Y.Learning a similarity metric discriminatively, with application to face verification[C]//Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2005:539-546.
                            </a>
                        </p>
                        <p id="257">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning to match using local and distributed representations of text for web search">

                                <b>[21]</b> MITRA B, DIAZ F, CRASWELL N.Learning to match using local and distributed representations of text for Web search[C]//Proceedings of the 26th International Conference on World Wide Web.Washington D.C., USA:IEEE Press, 2017:1291-1299.
                            </a>
                        </p>
                        <p id="259">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=End-to-end neural ad-hoc ranking with kernel pooling">

                                <b>[22]</b> XIONG Chenyan, DAI Zhuyun, CALLAN J, et al.End-to-end neural ad-hoc ranking with kernel pooling[C]//Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval.New York, USA:ACM Press, 2017:55-64.
                            </a>
                        </p>
                        <p id="261">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Convolutional neural networks for soft-matching n-grams in ad-hoc search">

                                <b>[23]</b> DAI Zhuyun, XIONG Chenyan, CALLAN J, et al.Convolutional neural networks for soft-matching n-grams in ad-hoc search[C]//Proceedings of the 11th ACM International Conference on Web Search and Data Mining.New York, USA:ACM Press, 2018:126-134.
                            </a>
                        </p>
                        <p id="263">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep Rank:A New Deep Architecture for Relevance Ranking in Information Retrieval">

                                <b>[24]</b> PANG Liang, LAN Yanyan, GUO Jiafeng, et al.DeepRank:a new deep architecture for relevance ranking in information retrieval[C]//Proceedings of 2017 ACM on Conference on Information and Knowledge Management.New York, USA:ACM Press, 2017:257-266.
                            </a>
                        </p>
                        <p id="265">
                            <a id="bibliography_25" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Deep Relevance Matching Model for Ad-hoc Retrieval">

                                <b>[25]</b> GUO Jiafeng, FAN Yixing, AI Qingyao, et al.A deep relevance matching model for ad-hoc retrieval[C]//Proceedings of ACM International on Conference on Information and Knowledge Management.New York, USA:ACM Press, 2016:55-64.
                            </a>
                        </p>
                        <p id="267">
                            <a id="bibliography_26" target="_blank" href="http://scholar.cnki.net/result.aspx?q=aNMM:Ranking Short Answer Texts with Attention-Based Neural Matching Model">

                                <b>[26]</b> YANG Liu, AI Qingyao, GUO Jiafeng, et al.aNMM:ranking short answer texts with attention-based neural matching model[C]//Proceedings of ACM International Conference on Information and Knowledge Management.New York, USA:ACM Press, 2016:287-296.
                            </a>
                        </p>
                        <p id="269">
                            <a id="bibliography_27" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Extending and formalizing the framework for information systems architecture">

                                <b>[27]</b> SOWA J F.Extending and formalizing the framework for information systems architecture[J].IBM Systems Journal, 2010, 31 (3) :590-616.
                            </a>
                        </p>
                        <p id="271">
                            <a id="bibliography_28" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Abstract meaning representation for sembanking">

                                <b>[28]</b> BANARESCU L, BONIAL C, CAI Shu, et al.Abstract meaning representation for sembanking[C]//Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse.Sofia, Bulgaria:[s.n.], 2013:178-186.
                            </a>
                        </p>
                        <p id="273">
                            <a id="bibliography_29" >
                                    <b>[29]</b>
                                 陆汝占.中文检索与汉语语义概念图表示[M].北京:清华大学出版社, 2009.
                            </a>
                        </p>
                        <p id="275">
                            <a id="bibliography_30" target="_blank" href="http://scholar.cnki.net/result.aspx?q=WordNet:an electronic lexical database">

                                <b>[30]</b> FELLBAUM C, MILLER G.WordNet:an electronic lexical database[J].Computational Linguistics, 1998, 25 (2) :292-296.
                            </a>
                        </p>
                        <p id="277">
                            <a id="bibliography_31" target="_blank" href="http://scholar.cnki.net/result.aspx?q=HowNet-A Hybrid Language and Knowledge Resource">

                                <b>[31]</b> DONG Zhendong, DONG Qiang.HowNet——a hybrid language and knowledge resource[C]//Proceedings of International Conference on Natural Language Processing and Knowledge Engineering.Washington D.C., USA:IEEE Press, 2003:820-824.
                            </a>
                        </p>
                        <p id="279">
                            <a id="bibliography_32" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Long short-term memory">

                                <b>[32]</b> GRAVES A.Long short-term memory[M].Berlin, Germany:Springer, 2012.
                            </a>
                        </p>
                        <p id="281">
                            <a id="bibliography_33" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Efficient estimation of word representations in vector space">

                                <b>[33]</b> MIKOLOV T, CHEN Kai, CORRADO G, et al.Efficient estimation of word representations in vector space[EB/OL].[2018-05-14].https://arxiv.org/abs/1301.3781.
                            </a>
                        </p>
                        <p id="283">
                            <a id="bibliography_34" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A full-text learning to rank dataset for medical information retrieval">

                                <b>[34]</b> BOTEVA V, GHOLIPOUR D, SOKOLOV A, et al.A full-text learning to rank dataset for medical information retrieval[C]//Proceedings of European Conference on Information Retrieval.Berlin, Germany:Springer, 2016:716-722.
                            </a>
                        </p>
                        <p id="285">
                            <a id="bibliography_35" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Borderline-SMOTE:A new over-sampling method in imbalanced data sets learning">

                                <b>[35]</b> HAN Hui, WANG Wenyuan, MAO Binghuan.Borderline-SMOTE:a new over-sampling method in imbalanced data sets learning[C]//Proceedings of International Conference on Advances in Intelligent Computing.Berlin, Germany:Springer, 2005:878-887.
                            </a>
                        </p>
                        <p id="287">
                            <a id="bibliography_36" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The Stanford Core NLP Natural Language Processing Toolkit">

                                <b>[36]</b> MANNING C D, SURDEANU M, BAUER J, et al.The Stanford coreNLP natural language processing toolkit[C]//Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics:System Demonstrations.Berlin, Germany:Springer, 2014:14-20.
                            </a>
                        </p>
                        <p id="289">
                            <a id="bibliography_37" target="_blank" href="http://scholar.cnki.net/result.aspx?q=MatchZoo:a toolkit for deep text matching">

                                <b>[37]</b> FAN Yixing, PANG Liang, HOU Jianpeng, et al.MatchZoo:a toolkit for deep text matching[EB/OL].[2018-04-13].https://arxiv.org/abs/1707.07270.
                            </a>
                        </p>
                        <p id="291">
                            <a id="bibliography_38" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Dropout:A simple way to prevent neural networks from overfitting">

                                <b>[38]</b> SRIVASTAVA N, HINTON G, KRIZHEVSKY A, et al.Dropout:a simple way to prevent neural networks from overfitting[J].Journal of Machine Learning Research, 2014, 15 (1) :1929-1958.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJC201908035" />
        <input id="dpi" type="hidden" value="600" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201908035&amp;v=MDQyMTJwNDlHWVlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJxRkNqbFZidk1MejdCYmJHNEg5ak0=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
