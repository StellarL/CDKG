<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637130603565900000%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJC201904027%26RESULT%3d1%26SIGN%3dfFF8ujISIhpp0QQMvPYv16EvdGg%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201904027&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201904027&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201904027&amp;v=MjU1NTJGeS9uVXJyQkx6N0JiYkc0SDlqTXE0OUhZNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm8=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#41" data-title="0 概述 ">0 概述</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#45" data-title="1 预备知识 ">1 预备知识</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#194" data-title="1.1 符号说明">1.1 符号说明</a></li>
                                                <li><a href="#54" data-title="1.2 集成学习">1.2 集成学习</a></li>
                                                <li><a href="#57" data-title="1.3 可容忍噪声的损失函数">1.3 可容忍噪声的损失函数</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#76" data-title="2 半监督学习框架 ">2 半监督学习框架</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#77" data-title="2.1 数据集构建">2.1 数据集构建</a></li>
                                                <li><a href="#81" data-title="2.2 弱模型训练">2.2 弱模型训练</a></li>
                                                <li><a href="#86" data-title="2.3 标记噪声建模">2.3 标记噪声建模</a></li>
                                                <li><a href="#100" data-title="2.4 风险最小化">2.4 风险最小化</a></li>
                                                <li><a href="#130" data-title="2.5 算法步骤和模型复杂度分析">2.5 算法步骤和模型复杂度分析</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#141" data-title="3 实验结果与分析 ">3 实验结果与分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#142" data-title="3.1 抽样方法的有效性分析">3.1 抽样方法的有效性分析</a></li>
                                                <li><a href="#155" data-title="3.2 NtLC-SSL实验分析">3.2 NtLC-SSL实验分析</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#172" data-title="4 结束语 ">4 结束语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#151" data-title="&lt;b&gt;表1 5个UCI基准数据集的参数&lt;/b&gt;"><b>表1 5个UCI基准数据集的参数</b></a></li>
                                                <li><a href="#153" data-title="&lt;b&gt;表2 2种方法在不同数据集上的抽样结果对比&lt;/b&gt;"><b>表2 2种方法在不同数据集上的抽样结果对比</b></a></li>
                                                <li><a href="#159" data-title="&lt;b&gt;图1 文献&lt;/b&gt;&lt;b&gt;算法生成的二维数据集&lt;/b&gt;"><b>图1 文献</b><b>算法生成的二维数据集</b></a></li>
                                                <li><a href="#160" data-title="&lt;b&gt;表3 生成数据与基准数据的分类准确率对比&lt;/b&gt;"><b>表3 生成数据与基准数据的分类准确率对比</b></a></li>
                                                <li><a href="#167" data-title="&lt;b&gt;图2 室内空间参考点示意&lt;/b&gt;"><b>图2 室内空间参考点示意</b></a></li>
                                                <li><a href="#170" data-title="&lt;b&gt;表4 各种算法在室内定位数据集上的分类准确率对比&lt;/b&gt;"><b>表4 各种算法在室内定位数据集上的分类准确率对比</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" title=" 张晨光, 张燕.半监督学习[M].北京:中国农业科学技术出版社, 2013." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CBBD&amp;filename=9787511614186000&amp;v=MTg0NzE5ZmJ2bktyaWZadTl1RkN2alU3aktJVnNkWEZxekdiYTVIOWZOcTQ1Tll1c1BEQk04enhVU21EZDlTSDduM3hF&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         张晨光, 张燕.半监督学习[M].北京:中国农业科学技术出版社, 2013.
                                    </a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" title=" NIGAM K, MCCALLUM A K, THRUN S, et al.Text classification from labeled and unlabeled documents using EM[J].Machine Learning, 2000, 38 (2/3) :103-134." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00001340068&amp;v=MjI2Mzc3cWVidWR0RkM3bFZMM0pKVmM9Tmo3QmFyTzRIdEhOckl0RlpPMEhZM2s1ekJkaDRqOTlTWHFScnhveGNNSDdS&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         NIGAM K, MCCALLUM A K, THRUN S, et al.Text classification from labeled and unlabeled documents using EM[J].Machine Learning, 2000, 38 (2/3) :103-134.
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title=" BLUM A, MITCHELL T.Combining labeled and unlabeled data with co-training[C]//Proceedings of the 11th Conference on Computational Learning Theory.New York, USA:ACM Press, 1998:92-100." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Combining Labeled and Unlabeled Data with Co-Training">
                                        <b>[3]</b>
                                         BLUM A, MITCHELL T.Combining labeled and unlabeled data with co-training[C]//Proceedings of the 11th Conference on Computational Learning Theory.New York, USA:ACM Press, 1998:92-100.
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" title=" JOACHIMS T.Transductive inference for text classifica-tion using support vector machines[C]//Proceedings of International Conference on Machine Learning.San Francisco, USA:Morgan Kaufmann Publishers, 1999:200-209." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Transductive inference for text classification using support vector machines">
                                        <b>[4]</b>
                                         JOACHIMS T.Transductive inference for text classifica-tion using support vector machines[C]//Proceedings of International Conference on Machine Learning.San Francisco, USA:Morgan Kaufmann Publishers, 1999:200-209.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" title=" ZHU X J, GHAHRAMANI Z, LAFFERTY J D.Semi-supervised learning using gaussian fields and harmonic functions[C]//Proceedings of the 12th International Conference on Machine Learning.Palo Alto, USA:AAAI Press, 2003:912-919." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Semi-supervised Learning using Gaussian Fields and Harmonic Functions">
                                        <b>[5]</b>
                                         ZHU X J, GHAHRAMANI Z, LAFFERTY J D.Semi-supervised learning using gaussian fields and harmonic functions[C]//Proceedings of the 12th International Conference on Machine Learning.Palo Alto, USA:AAAI Press, 2003:912-919.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" title=" ZHOU Z H.Ensemble methods:foundations and algorithms[M].London, UK:Taylor and Francis Group, 2012." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Ensemble methods:foundations and algorithms">
                                        <b>[6]</b>
                                         ZHOU Z H.Ensemble methods:foundations and algorithms[M].London, UK:Taylor and Francis Group, 2012.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" title=" FR&#201;NAY B, VERLEYSEN M.Classification in the presence of label noise:a survey[J].IEEE Transactions on Neural Networks and Learning Systems, 2014, 25 (5) :845-869." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Classification in the presence of label noise:A survey">
                                        <b>[7]</b>
                                         FR&#201;NAY B, VERLEYSEN M.Classification in the presence of label noise:a survey[J].IEEE Transactions on Neural Networks and Learning Systems, 2014, 25 (5) :845-869.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" title=" 刘康, 钱旭, 王自强.主动学习算法综述[J].计算机工程与应用, 2012, 48 (34) :1-4." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSGG201234001&amp;v=MTAwOTFQcTQ5RlpZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSb0Z5L25VcnJCTHo3TWFiRzRIOVA=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                         刘康, 钱旭, 王自强.主动学习算法综述[J].计算机工程与应用, 2012, 48 (34) :1-4.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" title=" BREIMAN L.Bagging predictors[J].Machine Learning, 1996, 24 (2) :123-140." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00001339482&amp;v=MjE0ODRSN3FlYnVkdEZDN2xWTDNKSlZjPU5qN0Jhck80SHRITnJJeE1ZT01OWTNrNXpCZGg0ajk5U1hxUnJ4b3hjTUg3&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                         BREIMAN L.Bagging predictors[J].Machine Learning, 1996, 24 (2) :123-140.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_10" title=" SCHAPIRE R E, FREUND Y, BARTLETT P, et al.Boosting the margin:a new explanation for the effectiveness of voting methods[C]//Proceedings of the 14th International Conference on Machine Learning.San Francisco, USA:Morgan Kaufmann Publishers, 1997:322-330." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Boosting the margin:A new explanation for the effectiveness of voting methods">
                                        <b>[10]</b>
                                         SCHAPIRE R E, FREUND Y, BARTLETT P, et al.Boosting the margin:a new explanation for the effectiveness of voting methods[C]//Proceedings of the 14th International Conference on Machine Learning.San Francisco, USA:Morgan Kaufmann Publishers, 1997:322-330.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_11" title=" MANWANI N, SASTRY P S.Noise tolerance under risk minimization[J].IEEE Transactions on Cybernetics, 2013, 43 (3) :1146-1151." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Noise tolerance under risk minimization">
                                        <b>[11]</b>
                                         MANWANI N, SASTRY P S.Noise tolerance under risk minimization[J].IEEE Transactions on Cybernetics, 2013, 43 (3) :1146-1151.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_12" title=" TEWARI A, BARTLETT P L.On the consistency of multiclass classification methods[C]//Proceedings of International Conference on Computational Learning Theory.Berlin, Germany:Springer, 2007:143-157." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=On the consistency of multiclass classification methods">
                                        <b>[12]</b>
                                         TEWARI A, BARTLETT P L.On the consistency of multiclass classification methods[C]//Proceedings of International Conference on Computational Learning Theory.Berlin, Germany:Springer, 2007:143-157.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_13" title=" SHAWE-TAYLOR J, BARTLETT P L, WILLIAMSON R C, et al.Structural risk minimization over data-dependent hierarchies[J].IEEE Transactions on Information Theory, 1998, 44 (5) :1926-1940." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Structural risk minimization over data-dependent hierarchies">
                                        <b>[13]</b>
                                         SHAWE-TAYLOR J, BARTLETT P L, WILLIAMSON R C, et al.Structural risk minimization over data-dependent hierarchies[J].IEEE Transactions on Information Theory, 1998, 44 (5) :1926-1940.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_14" title=" WESTON J, WATKINS C.Multi-class support vector machines[C]//Proceedings of European Symposia on Artificial Neural Networks.Brussels, Belgium:[s.n.], 1999:83-128." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multi-class support vector machines">
                                        <b>[14]</b>
                                         WESTON J, WATKINS C.Multi-class support vector machines[C]//Proceedings of European Symposia on Artificial Neural Networks.Brussels, Belgium:[s.n.], 1999:83-128.
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_15" title=" BARTLETT P L, JORDAN M I, MCAULIFFE J D.Convexity, classification, and risk bounds[J].Journal of the American Statistical Association, 2006, 101 (473) :138-156." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=STJD&amp;filename=STJD13062800006497&amp;v=MzA1MDNHZXJxUVRNbndaZVp1SHlqbVVMbklKMXdWYnhzPU5qbkJhcks3SHRmT3A0OUZaT3NKQ0hVK29CTVQ2VDRQUUgvaXJSZA==&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[15]</b>
                                         BARTLETT P L, JORDAN M I, MCAULIFFE J D.Convexity, classification, and risk bounds[J].Journal of the American Statistical Association, 2006, 101 (473) :138-156.
                                    </a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_16" title=" SZEGEDY C, IOFFE S, VANHOUCKE V, et al.Inception-v4, inception-resnet and the impact of residual connections on learning[C]//Proceedings of the 31st AAAI Conference on Artificial Intelligence.Palo Alto, USA:AAAI Press, 2017:4278-4284." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Inception-v4,Inception-ResNet and the Impact of Residual Connections on Learning">
                                        <b>[16]</b>
                                         SZEGEDY C, IOFFE S, VANHOUCKE V, et al.Inception-v4, inception-resnet and the impact of residual connections on learning[C]//Proceedings of the 31st AAAI Conference on Artificial Intelligence.Palo Alto, USA:AAAI Press, 2017:4278-4284.
                                    </a>
                                </li>
                                <li id="35">


                                    <a id="bibliography_17" title=" MALLAPRAGADA P K, JIN R, JAIN A K, et al.SemiBoost:boosting for semi-supervised learning[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2009, 31 (11) :2000-2014." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=SemiBoost: Boosting for semi-supervised learning">
                                        <b>[17]</b>
                                         MALLAPRAGADA P K, JIN R, JAIN A K, et al.SemiBoost:boosting for semi-supervised learning[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2009, 31 (11) :2000-2014.
                                    </a>
                                </li>
                                <li id="37">


                                    <a id="bibliography_18" title=" LI Y Y, SU L, CHEN J, et al.Semi-supervised question classification based on ensemble learning[C]//Proceedings of International Conference on Swarm Intelligence.Berlin, Germany:Springer, 2015:341-348." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Semi-supervised question classification based on ensemble learning">
                                        <b>[18]</b>
                                         LI Y Y, SU L, CHEN J, et al.Semi-supervised question classification based on ensemble learning[C]//Proceedings of International Conference on Swarm Intelligence.Berlin, Germany:Springer, 2015:341-348.
                                    </a>
                                </li>
                                <li id="39">


                                    <a id="bibliography_19" title=" GUYON I.Design of experiments for the NIPS 2003 variable selection benchmark[EB/OL].[2018-01-05].http://clopinet.com/isabelle/Projects/NIPS2003/Slides/ NIPS2003-Datasets.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Design of experiments for the NIPS 2003 variable selection benchmark">
                                        <b>[19]</b>
                                         GUYON I.Design of experiments for the NIPS 2003 variable selection benchmark[EB/OL].[2018-01-05].http://clopinet.com/isabelle/Projects/NIPS2003/Slides/ NIPS2003-Datasets.pdf.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJC" target="_blank">计算机工程</a>
                2019,45(04),157-162+168 DOI:10.19678/j.issn.1000-3428.0050398            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>噪声可容忍的标记组合半监督学习算法</b></span>
 <span class="shoufa"></span>                                     </h1>

            </div>
                        <h2>
                                <a href="javascript:;">林金钏</a>
                                <a href="javascript:;">艾浩军</a>
                </h2>
                    <h2>

                    <span>武汉大学计算机学院</span>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>针对传统机器学习方法在完成分类任务时多数存在人工标记成本较高、泛化能力较弱的问题, 提出一种标记组合半监督学习算法。基于集成学习的思想, 利用有标记数据训练多个弱模型并进行组合, 增强模型的泛化能力。对无标记数据进行预测, 生成有噪声的标记并组合建模。在风险最小化的框架下, 使模型收敛达到最优。实验结果表明, 在2种有监督场景下与现有的支持向量机、分类与回归树、神经网络等算法相比, 该算法具有较优的泛化能力。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">半监督学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">集成学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%A3%8E%E9%99%A9%E6%9C%80%E5%B0%8F%E5%8C%96&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">风险最小化;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">梯度下降;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">损失函数;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    林金钏 (1992—) , 男, 硕士研究生, 主研方向为迁移学习、复杂网络, E-mail:linjc0418@gmail.com;;
                                </span>
                                <span>
                                    艾浩军, 副教授。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-02-02</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家重点研发计划 (2016YFB0502201);</span>
                    </p>
            </div>
                    <h1><b>Noise Tolerant Label Combination Semi-supervised Learning Algorithm</b></h1>
                    <h2>
                    <span>LIN Jinchuan</span>
                    <span>AI Haojun</span>
            </h2>
                    <h2>
                    <span>School of Computer Science, Wuhan University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Traditional machine learning method always needs high cost manual marking process, and exhibits weak ability of generalization in classification task.In order to solve these problems, a label combination semi-supervised learning algorithm is proposed.Taking advantage of the principle of ensemble learning, the algorithm uses the labeled data to train multiple weak learners, and combine them to enhance the generalization ability.Predict the unlabeled data to generate noise labels, and then combine and model these noise labels to make the model more robust.Under the framework of risk minimization, the model converges to the optimal state.Experimental results show that, compared with some existing learning algorithms like Support Vector Machine (SVM) , Classification and Regression Tree (CART) , Neural Network (NN) , the algorithm has relatively good generalization ability.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=semi-supervised%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">semi-supervised learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=ensemble%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">ensemble learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=risk%20minimization&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">risk minimization;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=gradient%20descent&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">gradient descent;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=loss%20function&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">loss function;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-02-02</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="41" name="41" class="anchor-tag">0 概述</h3>
                <div class="p1">
                    <p id="42">随着计算技术、存储技术的快速发展, 计算机采集到的数据越来越多, 对这些数据的有效分析、挖掘和应用可极大地促进各领域的发展。机器学习是数据分析、挖掘和应用的重要基础。传统的机器学习主要针对监督学习的问题, 即对大量有标记的数据建模, 用训练好的模型预测未标记数据。在实际任务中, 可以很容易地获得未标记数据, 但是对这些数据进行标记需要大量的人力和物力。例如, 分析医学影像, 可以与医院合作获取大量的影像数据, 但是对这些影像中的症状进行标记需要专业医生来完成。如果只对少量的标记数据进行监督学习, 所得到的模型泛化能力较弱。半监督学习<citation id="174" type="reference"><link href="3" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>综合使用标记数据和未标记数据, 在一定程度上可以增强模型的泛化能力。</p>
                </div>
                <div class="p1">
                    <p id="43">目前, 半监督学习方法主要包括基于生成式模型的方法<citation id="175" type="reference"><link href="5" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>、协同训练方法<citation id="176" type="reference"><link href="7" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>、半监督SVM (Support Vector Machine) 方法<citation id="177" type="reference"><link href="9" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>、基于图的方法<citation id="178" type="reference"><link href="11" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>等。基于生成式模型的方法假设所有数据由相同分布产生, 将其转化为参数估计的问题, 用最大期望 (Expectation Maximization, EM) 算法进行计算。协同训练方法针对若干个视图进行相互学习, 不断将一个视图内最置信的未标记样本加入到另一个视图的标记样集中, 从而实现协同训练。半监督SVM方法通过调整SVM的超平面和未标记数据的标记指派, 在所有训练数据 (包括有标记和未标记数据) 上最大化间隔。基于图的方法用图表示整个数据集, 数据的分布信息和样本点之间的关系都包含在图结构中, 在图上进行标记信息的传递。</p>
                </div>
                <div class="p1">
                    <p id="44">本文结合集成学习<citation id="179" type="reference"><link href="13" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>和噪声感知<citation id="180" type="reference"><link href="15" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>分类方法, 提出一种噪声可容忍的标记组合半监督学习 (NtLC-SSL) 算法。对无标记数据集进行采样, 保证采样数据服从整体样本的统计规律, 使领域专家进行标记 (与主动学习<citation id="181" type="reference"><link href="17" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>类似) 。使用有标记数据训练若干弱模型, 对无标记数据进行预测, 生成标记数据集并对其噪声进行组合建模。最后将标记噪声模型嵌入传统的分类模型中进行训练。</p>
                </div>
                <h3 id="45" name="45" class="anchor-tag">1 预备知识</h3>
                <h4 class="anchor-tag" id="194" name="194">1.1 符号说明</h4>
                <div class="p1">
                    <p id="46">令随机变量<b><i>x</i></b>∈<sup><i>d</i></sup>, 其中, <i>d</i>为正整数, 表示数据的维度。二分类问题的类标签<i>y</i>∈{+1, -1}, 多分类问题的类标签<i>y</i>∈{1, 2, …, <i>k</i>}, <i>k</i>为正整数, 表示类别的个数。<b><i>x</i></b>和<i>y</i>的联合概率密度函数为<i>p</i> (<b><i>x</i></b>, <i>y</i>) 。在半监督学习中, 数据分为有标记数据 (<i>S</i><sub><i>L</i></sub>) 和无标记数据 (<i>S</i><sub><i>U</i></sub>) , 定义如下:</p>
                </div>
                <div class="p1">
                    <p id="48" class="code-formula">
                        <mathml id="48"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>S</mi><msub><mrow></mrow><mi>L</mi></msub><mo>=</mo><mrow><mo stretchy="false">{</mo><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><msubsup><mrow></mrow><mi>i</mi><mi>L</mi></msubsup><mo>, </mo><mi>y</mi><msubsup><mrow></mrow><mi>i</mi><mi>L</mi></msubsup><mo stretchy="false">}</mo></mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>n</mi><msub><mrow></mrow><mi>L</mi></msub></mrow></msubsup><mover><mstyle mathsize="140%" displaystyle="true"><mo>∼</mo></mstyle><mrow><mi>i</mi><mo>.</mo><mi>i</mi><mo>.</mo><mi>d</mi></mrow></mover><mi>p</mi><msub><mrow></mrow><mi>L</mi></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi>S</mi><msub><mrow></mrow><mi>U</mi></msub><mo>=</mo><mrow><mo stretchy="false">{</mo><mi mathvariant="bold-italic">x</mi><msubsup><mrow></mrow><mi>i</mi><mi>U</mi></msubsup><mo stretchy="false">}</mo></mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>n</mi><msub><mrow></mrow><mi>U</mi></msub></mrow></msubsup><mover><mstyle mathsize="140%" displaystyle="true"><mo>∼</mo></mstyle><mrow><mi>i</mi><mo>.</mo><mi>i</mi><mo>.</mo><mi>d</mi></mrow></mover><mi>p</mi><msub><mrow></mrow><mi>U</mi></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="49">其中, <i>n</i>=<i>n</i><sub><i>L</i></sub>+<i>n</i><sub><i>U</i></sub>, 表示数据集的数量, <b><i>X</i></b>={<b><i>x</i></b><sub><i>i</i></sub>}<mathml id="50"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup></mrow></math></mathml>, 表示样本点的集合。</p>
                </div>
                <div class="p1">
                    <p id="51">令函数f表示需要学习的模型, 函数l表示损失函数, 因此, 可以定义l的风险函数 (记为l-risk) 为:</p>
                </div>
                <div class="p1">
                    <p id="52">R<sub>l</sub> (f) =E<sub>p</sub>[l (f (<b><i>x</i></b>) , <i>y</i>) ]      (3) </p>
                </div>
                <div class="p1">
                    <p id="53">其中, <i>E</i>表示随机变量的期望, 其下标<i>p</i>表示随机变量<b><i>x</i></b>所服从的概率分布。</p>
                </div>
                <h4 class="anchor-tag" id="54" name="54">1.2 集成学习</h4>
                <div class="p1">
                    <p id="55">在机器学习中, 监督学习算法在一个给定的假设空间中搜索具有较好泛化能力的模型, 但是, 需要假设空间包含泛化能力强的模型, 这是机器学习中较难把握的一个问题。集成学习方法组合多个假设空间得到一个较优的模型, 即组合多个弱模型 (<i>Weak Learner</i>, 预测效果一般) 得到一个强模型 (<i>Strong Learner</i>, 泛化性能强) 。其组合的弱模型可以是同一类的 (同质集成学习) , 也可以是不同类的 (异质集成学习) 。</p>
                </div>
                <div class="p1">
                    <p id="56">典型的集成学习方法, 如<i>bagging</i><citation id="182" type="reference"><link href="19" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>、<i>boosting</i><citation id="183" type="reference"><link href="21" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>、随机森林, 都属于同质集成学习。异质集成学习方法, 如<i>stacking</i>、<i>blending</i>等, 在国际知名的比赛<i>KDD</i>和<i>Kaggle</i>中取得了较好的预测结果。为了提高集成学习的泛化性能, 需要保证对象之间的多样性, 例如, <i>bagging</i>方法通过样本扰动保证, 随机森林通过样本扰动和随机特征的选取来保证。</p>
                </div>
                <h4 class="anchor-tag" id="57" name="57">1.3 可容忍噪声的损失函数</h4>
                <div class="p1">
                    <p id="58">当数据集有噪声时, 无法保证训练所得模型的可靠性和可信度。因此, 在分类问题中需要标记有噪声场景。记有噪声的数据集为:</p>
                </div>
                <div class="p1">
                    <p id="59" class="code-formula">
                        <mathml id="59"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>S</mi><msub><mrow></mrow><mi>η</mi></msub><mo>=</mo><mo stretchy="false">{</mo><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mover><mstyle mathsize="140%" displaystyle="true"><mi>y</mi></mstyle><mrow><mspace width="0.25em" /><mo>^</mo></mrow></mover><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>, </mo><mi>i</mi><mo>=</mo><mn>1</mn><mo>, </mo><mn>2</mn><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>Ν</mi><mo stretchy="false">}</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="60">其中, <mathml id="61"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi>y</mi></mstyle><mrow><mspace width="0.25em" /><mo>^</mo></mrow></mover><msub><mrow></mrow><mi>i</mi></msub></mrow></math></mathml>是带噪声的标记, <i>y</i><sub><i>i</i></sub>为实际标记, 因此, 可以对噪声进行建模:</p>
                </div>
                <div class="p1">
                    <p id="62" class="code-formula">
                        <mathml id="62"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi>y</mi></mstyle><mrow><mspace width="0.25em" /><mo>^</mo></mrow></mover><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mrow><mo>{</mo><mrow><mtable><mtr><mtd columnalign="left"><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mtext>概</mtext><mtext>率</mtext><mtext>为</mtext><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>η</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd columnalign="left"><mi>j</mi><mo>, </mo><mi>j</mi><mo>∈</mo><mo stretchy="false">{</mo><mn>1</mn><mo>, </mo><mn>2</mn><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>k</mi><mo stretchy="false">}</mo><mtext>且</mtext><mi>j</mi><mo>≠</mo><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mtext>概</mtext><mtext>率</mtext><mtext>为</mtext><mover><mstyle mathsize="140%" displaystyle="true"><mi>η</mi></mstyle><mrow><mspace width="0.25em" /><mo>—</mo></mrow></mover><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub></mtd></mtr></mtable></mrow></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>5</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="63">其中, 对于任意的样本点<b><i>x</i></b><sub><i>i</i></sub>, 对应的实际标记为<i>y</i><sub><i>i</i></sub>, 有<mathml id="64"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>j</mi><mo>≠</mo><mi>i</mi></mrow></munder><mtext> </mtext></mstyle><mover><mstyle mathsize="140%" displaystyle="true"><mi>η</mi></mstyle><mrow><mspace width="0.25em" /><mo>—</mo></mrow></mover><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mi>η</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></math></mathml>。</p>
                </div>
                <div class="p1">
                    <p id="65"><b>定义1</b> (均匀噪声) 对于任意的样本点<b><i>x</i></b><sub><i>i</i></sub>, <i>η</i><sub><i>i</i></sub>=<i>η</i>且<mathml id="66"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi>η</mi></mstyle><mrow><mspace width="0.25em" /><mo>—</mo></mrow></mover><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mfrac><mi>η</mi><mrow><mi>k</mi><mo>-</mo><mn>1</mn></mrow></mfrac><mo>, </mo><mo>∀</mo><mi>j</mi><mo>≠</mo><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></math></mathml>, 其中, <i>η</i>为常数。</p>
                </div>
                <div class="p1">
                    <p id="67"><b>定义2</b> (不均匀噪声) 对于任意的样本点<b><i>x</i></b><sub><i>i</i></sub>, <i>η</i><sub><i>i</i></sub>和<mathml id="68"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi>η</mi></mstyle><mrow><mspace width="0.25em" /><mo>—</mo></mrow></mover><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow></math></mathml>都是<b><i>x</i></b><sub><i>i</i></sub>的函数。简单不均匀噪声是不均匀噪声的一种特殊情况, 即<mathml id="69"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi>η</mi></mstyle><mrow><mspace width="0.25em" /><mo>—</mo></mrow></mover><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mfrac><mrow><mi>η</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mrow><mi>k</mi><mo>-</mo><mn>1</mn></mrow></mfrac><mo>, </mo><mo>∀</mo><mi>j</mi><mo>≠</mo><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></math></mathml>。</p>
                </div>
                <div class="p1">
                    <p id="70">当数据集标记无噪声时, 针对特定的损失函数l, 它的风险函数用式 (3) 表示, 记为R<sub>l</sub> (f) , 令f<sup>*</sup>为R<sub>l</sub> (f) 的全局最小值。当数据集有噪声时, 其服从概率密度分布p<sub>η</sub>, 风险函数用式 (6) 表示, 令f<sup>*</sup><sub>η</sub>为R<mathml id="71"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>l</mi><mi>η</mi></msubsup></mrow></math></mathml> (f) 的全局最小值。</p>
                </div>
                <div class="p1">
                    <p id="72" class="code-formula">
                        <mathml id="72"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>R</mi><msubsup><mrow></mrow><mi>l</mi><mi>η</mi></msubsup><mo stretchy="false"> (</mo><mi>f</mi><mo stretchy="false">) </mo><mo>=</mo><mi>E</mi><msub><mrow></mrow><mrow><mi>p</mi><msub><mrow></mrow><mi>η</mi></msub></mrow></msub><mo stretchy="false">[</mo><mi>l</mi><mo stretchy="false"> (</mo><mi>f</mi><mtext> </mtext><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">) </mo><mo>, </mo><mover><mstyle mathsize="140%" displaystyle="true"><mi>y</mi></mstyle><mrow><mspace width="0.25em" /><mo>^</mo></mrow></mover><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>6</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="73">在损失函数l确定的情况下, 对模型进行风险最小化, 则模型对噪声可容忍需满足如下条件<citation id="184" type="reference"><link href="23" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>:</p>
                </div>
                <div class="p1">
                    <p id="74">P[f <sup>*</sup> (<b><i>x</i></b>) =<i>y</i>]=<i>P</i>[<i>f</i><sup>*</sup><sub><i>η</i></sub> (<b><i>x</i></b>) =<i>y</i>]      (7) </p>
                </div>
                <div class="p1">
                    <p id="75">此时, 损失函数<i>l</i>对噪声是可容忍的, 因此, 可以得到<i>f</i><sup>*</sup>=<i>f</i><sup>*</sup><sub><i>η</i></sub>。</p>
                </div>
                <h3 id="76" name="76" class="anchor-tag">2 半监督学习框架</h3>
                <h4 class="anchor-tag" id="77" name="77">2.1 数据集构建</h4>
                <div class="p1">
                    <p id="78">监督学习算法的有效性取决于是否能够采集到高质量的有标记数据集, 但是, 对数据集进行标记需耗费大量人力、物力。为了减少成本, 只对部分数据进行标记, 并结合剩下的无标记数据共同训练模型。因此, 需要有效地选择样本进行标记, 以增加模型的泛化性能。本文首先使用聚类的方法将数据分成若干个<i>cluster</i>, 然后进行分层抽样, 保证数据的统计规律性。</p>
                </div>
                <div class="p1">
                    <p id="79">对于一个分类任务, 首先需要明确类别的个数k。根据任务的需求进行数据采集, 采集到的样本数据都是无标记的, 样本数量为n。使用<i>k</i>-<i>means</i>算法将无标记数据聚合成k个<i>cluster</i>, 第i个<i>cluster</i>样本集合的数量记为n<sub>i</sub>。针对k个<i>cluster</i>进行分层抽样, 总共抽取m (m&lt;&lt;n) 个样本, 每个<i>cluster</i>按照比例进行简单随机抽样, 第i个<i>cluster</i>抽取的样本数量为mn<sub>i</sub>/n个。将所有随机抽取的样本标记为S<sub>L</sub>, 其余样本标记为S<sub>U</sub>。</p>
                </div>
                <div class="p1">
                    <p id="80">对于半监督学习的数据集, 需要保证标记数据的有效性。标签数据虽然少, 但其在统计上必须是无偏的。</p>
                </div>
                <h4 class="anchor-tag" id="81" name="81">2.2 弱模型训练</h4>
                <div class="p1">
                    <p id="82">在国际著名的数据挖掘竞赛<i>Kaggle</i>中, 集成的方法取得了很好的精度。可见, 集成学习方法能够在一定程度上增强模型的泛化能力。本文采用集成学习的思想, 在数据集S<sub>L</sub>上建立多个弱模型, 用各个弱模型对数据集S<sub>U</sub>进行预测。在训练弱模型时, 要保证模型的精度, 同时避免过拟合。因此, 在一般情况下, 要选择复杂度较低的模型, 或者使用正则化技术降低模型的复杂度。</p>
                </div>
                <div class="p1">
                    <p id="83">本文选用的弱模型采用不同的分类算法, 如<i>SVM</i>、随机森林等, 来保证各个算法的假设空间存在差异性, 从而增强弱模型的多样性<citation id="185" type="reference"><link href="13" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>, 提高模型的泛化能力。需要训练的弱模型数量记为T, 需要训练的弱模型集合记为<b><i>B</i></b>={<i>B</i><sub><i>i</i></sub>}<mathml id="84"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Τ</mi></msubsup></mrow></math></mathml>。将数据集S<sub>U</sub>作为测试集, 并用<b><i>B</i></b>中所有的弱模型对测试集进行预测。其中, 第<i>i</i>个弱模型<i>B</i><sub><i>i</i></sub>对测试集<i>S</i><sub><i>U</i></sub>的标记结果记为<i>Y</i><sup><i>B</i><sub><i>i</i></sub></sup>={<i>y</i><sup><i>B</i><sub><i>i</i></sub></sup><sub><i>j</i></sub>}<mathml id="85"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>n</mi><msub><mrow></mrow><mi>U</mi></msub></mrow></msubsup></mrow></math></mathml>。所有弱模型对测试集样本的标记结果记为Y<sup>B</sup>。由于弱模型的算法预测过程是独立的, 测试集的样本标记之间也是相互独立的。本文规定<b><i>B</i></b><sub><i>i</i></sub> (<b><i>x</i></b>) 表示第<i>i</i>个弱模型对样本<b><i>x</i></b>的预测标记, 同理, <b><i>B</i></b> (<b><i>x</i></b>) 表示所有弱模型对样本<b><i>x</i></b>的预测标记。</p>
                </div>
                <h4 class="anchor-tag" id="86" name="86">2.3 标记噪声建模</h4>
                <div class="p1">
                    <p id="87">在学习弱模型时, 使用少部分的有标记数据进行训练, 对大部分无标记数据集S<sub>U</sub>的预测精确度较低。因此, 弱模型集合<b><i>B</i></b>对<i>S</i><sub><i>U</i></sub>的预测结果集合<i>Y</i><sup><i>B</i></sup>存在错误标记, 本文定义该错误标记为标记噪声。</p>
                </div>
                <div class="p1">
                    <p id="88">根据第1.3节的描述, 噪声可以分为均匀噪声和不均匀噪声。为简化计算, 假设标记集合<i>Y</i><sup><i>B</i></sup>的标记噪声属于均匀噪声, 即对于每一个弱模型<i>B</i><sub><i>i</i></sub>, 都对应一个标记错误的概率, 记为<i>η</i><sup><i>B</i><sub><i>i</i></sub></sup>。根据第2.2节的算法过程, 数据集<i>S</i><sub><i>U</i></sub>的标记集合<i>Y</i><sup><i>B</i></sup>关于每一个弱模型的标记集合是相互独立的, 即<i>Y</i><sup><i>B</i><sub><i>i</i></sub></sup>, <i>i</i>=1, 2, …, <i>T</i>之间是相互独立的。</p>
                </div>
                <div class="p1">
                    <p id="89">对于二分类问题, 真实标记<i>y</i>∈{+1, -1}, <i>T</i>个弱模型所预测的标记为<i>y</i><sup><i>B</i></sup>。为了便于计算, 假设每个类别的概率是0.5, 则噪声模型为:</p>
                </div>
                <div class="p1">
                    <p id="90" class="code-formula">
                        <mathml id="90"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>p</mi><msub><mrow></mrow><mi>η</mi></msub><mo stretchy="false"> (</mo><mi>y</mi><msup><mrow></mrow><mi>B</mi></msup><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mstyle displaystyle="true"><munderover><mo>∏</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Τ</mi></munderover><mo stretchy="false"> (</mo></mstyle><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>η</mi><msup><mrow></mrow><mrow><mi>B</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msup><mo stretchy="false">) </mo><mi>Ι</mi><msub><mrow></mrow><mrow><mi>y</mi><mo>=</mo><mi>y</mi><msup><mrow></mrow><mrow><mi>B</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msup></mrow></msub><mo>+</mo><mi>η</mi><msup><mrow></mrow><mrow><mi>B</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msup><mi>Ι</mi><msub><mrow></mrow><mrow><mi>y</mi><mo>≠</mo><mi>y</mi><msup><mrow></mrow><mrow><mi>B</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msup></mrow></msub><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>8</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="91">其中, <i>I</i>为示性函数, 如果允许参数<i>η</i>∈<image href="images/JSJC201904027_092.jpg" type="" display="inline" placement="inline"><alt></alt></image><sup><i>T</i></sup>为随机变量, 则可以认为式 (8) 表示一族标记噪声的生成模型。</p>
                </div>
                <div class="p1">
                    <p id="93">以无标记数据集<i>S</i><sub><i>U</i></sub>为基础, 采用最大似然估计的方法对参数<i>η</i>进行计算, 可以具体化为求解式 (9) 所示的最优化问题:</p>
                </div>
                <div class="area_img" id="197">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJC201904027_19700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="97">其中, <b><i>B</i> (<i>x</i></b>) 表示所有弱模型对样本点<b><i>x</i></b>的预测标签, 是一个向量。该最优化问题可以使用基于梯度的方法来求解。</p>
                </div>
                <div class="p1">
                    <p id="98">对于多分类问题, 真实标记<i>y</i>使用one-hot方法进行编码。每个类别的概率为1/<i>k</i>, 因此, 噪声模型如式 (10) 所示, 求解参数<i>η</i>的方法保持不变。</p>
                </div>
                <div class="p1">
                    <p id="99" class="code-formula">
                        <mathml id="99"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>p</mi><msub><mrow></mrow><mi>η</mi></msub><mo stretchy="false"> (</mo><mi>y</mi><msup><mrow></mrow><mi>B</mi></msup><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mn>1</mn><mi>k</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∏</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Τ</mi></munderover><mo stretchy="false"> (</mo></mstyle><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>η</mi><msup><mrow></mrow><mrow><mi>B</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msup><mo stretchy="false">) </mo><mi>Ι</mi><msub><mrow></mrow><mrow><mi>y</mi><mo>=</mo><mi>y</mi><msup><mrow></mrow><mrow><mi>B</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msup></mrow></msub><mo>+</mo><mi>η</mi><msup><mrow></mrow><mrow><mi>B</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msup><mi>Ι</mi><msub><mrow></mrow><mrow><mi>y</mi><mo>≠</mo><mi>y</mi><msup><mrow></mrow><mrow><mi>B</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msup></mrow></msub><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>0</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <h4 class="anchor-tag" id="100" name="100">2.4 风险最小化</h4>
                <div class="p1">
                    <p id="101">通过基于梯度的方法算出参数η的估计值<mathml id="102"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi>η</mi></mstyle><mrow><mspace width="0.25em" /><mo>^</mo></mrow></mover></mrow></math></mathml>后, 噪声模型能够较精确地描述数据集S<sub>U</sub>的标签分布情况。本文利用训练好的噪声模型, 结合二分类算法<i>Logistics</i>回归 (多分类问题使用<i>Softmax</i>回归) 创建分类模型。</p>
                </div>
                <div class="p1">
                    <p id="103">引入损失函数分类可校正 (<i>classification calibrated</i>) <citation id="186" type="reference"><link href="25" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>方法, 即一个分类器在特定损失函数下的风险值足够小, 同时该分类器在0-1损失函数下的风险值也足够小。其与第1.3节所述的可容忍噪声损失函数相比本质上是一样的。</p>
                </div>
                <div class="p1">
                    <p id="104">二分类<i>Logistics</i>回归算法所使用的损失函数称为<i>Logistics</i>损失:</p>
                </div>
                <div class="p1">
                    <p id="105">L<sub><i>logistic</i></sub> (<b><i>x</i></b>, <i>y</i>) =ln (1+e<sup>-<i>y</i>·<i>f</i> (<b><i>x</i></b>) </sup>)      (11) </p>
                </div>
                <div class="p1">
                    <p id="106">其中, (<b><i>x</i></b>, <i>y</i>) 表示一个采样点的值及其对应的标记, <i>y</i>∈{+1, -1}, <i>f</i> (<b><i>x</i></b>) 是关于<b><i>x</i></b>的线性模型, 且<mathml id="107"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>f</mi><mo>:</mo><msup><mrow></mrow><mi>d</mi></msup><mo>→</mo><mo>, </mo></mrow></math></mathml>在一般情况下<i>f</i> (<b><i>x</i></b>) =<i>ω</i><sup>T</sup><b><i>x</i></b>+<i>b</i>。Logistics的风险函数可表示为:</p>
                </div>
                <div class="p1">
                    <p id="108"><i>R</i><sub>logistic</sub> (<b><i>x</i></b>, <i>y</i>) =<i>E</i>[<i>L</i><sub>logistic</sub> (<b><i>x</i></b>, <i>y</i>) ]      (12) </p>
                </div>
                <div class="p1">
                    <p id="109">其中, <i>R</i><sub>logistic</sub>是期望风险, 用来描述整个数据集上的损失。在实际应用中, 不可能采集到所有数据, 因此, 使用经验风险来代替期望风险, 根据大数定理, 当样本趋近无穷大时, 经验风险趋近于期望风险。因此, 为了找到期望风险最小的模型, 要在样本充分的情况下, 最小化经验风险。考虑到样本数量不充分, 可能会导致过拟合现象, 因此, 在经验风险中加入置信风险:</p>
                </div>
                <div class="p1">
                    <p id="110" class="code-formula">
                        <mathml id="110"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>R</mi><mo stretchy="false"> (</mo><mi>ω</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mn>1</mn><mrow><mi>n</mi><msub><mrow></mrow><mi>U</mi></msub></mrow></mfrac><mtext> </mtext><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi mathvariant="bold-italic">x</mi><mo>∈</mo><mi>S</mi><msub><mrow></mrow><mi>U</mi></msub></mrow></munder><mrow></mrow></mstyle></mrow><mtext> </mtext><mi>E</mi><msub><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>y</mi><msup><mrow></mrow><mi>B</mi></msup><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo><mo>∼</mo><mi>p</mi><msub><mrow></mrow><mi>η</mi></msub></mrow></msub><mo stretchy="false">[</mo><mrow><mi>ln</mi></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo>+</mo><mtext>e</mtext><msup><mrow></mrow><mrow><mo>-</mo><mi>ω</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">x</mi><mi>y</mi></mrow></msup><mo stretchy="false">) </mo><mo stretchy="false">|</mo><mi>y</mi><msup><mrow></mrow><mi>B</mi></msup><mo>=</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mi>B</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mo>+</mo><mi>α</mi><mrow><mo stretchy="false">∥</mo><mi>ω</mi><mo stretchy="false">∥</mo></mrow><msup><mrow></mrow><mn>2</mn></msup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>3</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="111">其中, ‖<i>ω</i>‖<sup>2</sup>为置信风险, 可减小模型的复杂度, 避免过拟合。参数<i>α</i>控制置信风险对整体风险的影响程度。</p>
                </div>
                <div class="p1">
                    <p id="112">根据风险最小化的原则<citation id="187" type="reference"><link href="27" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>, 求解式 (13) 的最小值, 计算过程如下:</p>
                </div>
                <div class="p1">
                    <p id="113" class="code-formula">
                        <mathml id="113"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi>ω</mi></mstyle><mrow><mspace width="0.25em" /><mo>^</mo></mrow></mover><mo>=</mo><mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mtext>a</mtext><mtext>r</mtext><mtext>g</mtext><mtext>m</mtext><mtext>i</mtext><mtext>n</mtext></mrow></mstyle><mi>ω</mi></munder></mrow><mtext> </mtext><mi>R</mi><mo stretchy="false"> (</mo><mi>ω</mi><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>4</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="114">式 (14) 只是一个基础的Logistics回归的变型, 因此, 可以使用梯度下降的方法求解参数<i>ω</i>。</p>
                </div>
                <div class="p1">
                    <p id="115">对于多分类的Softmax回归, 文献<citation id="198" type="reference">[<a class="sup">12</a>,<a class="sup">14</a>,<a class="sup">15</a>]</citation>证明在多分类的场景下, Logistics损失是分类可校正的。因此, Softmax回归的损失函数能够容忍标签的噪声。Softmax模型如下:</p>
                </div>
                <div class="p1">
                    <p id="116" class="code-formula">
                        <mathml id="116"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>δ</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mrow><mi>exp</mi><mo stretchy="false"> (</mo><mi>f</mi><mrow><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">) </mo></mrow><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><mtext>e</mtext></mstyle><mtext>x</mtext><mtext>p</mtext><mo stretchy="false"> (</mo><mi>f</mi><mrow><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">) </mo></mrow><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo></mrow></mfrac><mo>, </mo><mi>i</mi><mo>∈</mo><mo stretchy="false">{</mo><mn>1</mn><mo>, </mo><mn>2</mn><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>k</mi><mo stretchy="false">}</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>5</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="117">本文用one-hot方法对多分类问题的标签<i>y</i>进行编码, 所以<i>y</i>∈<image href="images/JSJC201904027_118.jpg" type="" display="inline" placement="inline"><alt></alt></image><sup><i>k</i></sup>, <i>f</i>:<image href="images/JSJC201904027_119.jpg" type="" display="inline" placement="inline"><alt></alt></image><sup><i>d</i></sup>→<image href="images/JSJC201904027_120.jpg" type="" display="inline" placement="inline"><alt></alt></image><sup><i>k</i></sup>, <i>f</i> (<b><i>x</i></b>) <sub><i>i</i></sub>表示<i>f</i> (<b><i>x</i></b>) 的第<i>i</i>个元素, <mathml id="121"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><mtext> </mtext></mstyle><mi>δ</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mn>1</mn></mrow></math></mathml>。定义<b><i>e</i></b><sub><i>j</i></sub>∈<image href="images/JSJC201904027_122.jpg" type="" display="inline" placement="inline"><alt></alt></image><sup><i>k</i></sup>, 当样本点<b><i>x</i></b>的标记<i>y</i>为<i>j</i>时, 如果<b><i>e</i></b><sub><i>j</i></sub>的第<i>i</i>个元素等于<i>j</i>, 则<b><i>e</i></b><sub><i>ji</i></sub>=1, 否则<b><i>e</i></b><sub><i>ji</i></sub>=0 (<i>y</i>和<b><i>e</i></b><sub><i>j</i></sub>是恒等关系) 。因此, 有Softmax的损失函数:</p>
                </div>
                <div class="p1">
                    <p id="123" class="code-formula">
                        <mathml id="123"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>L</mi><msub><mrow></mrow><mrow><mtext>S</mtext><mtext>o</mtext><mtext>f</mtext><mtext>t</mtext><mtext>m</mtext><mtext>a</mtext><mtext>x</mtext></mrow></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo><mo>=</mo><mi>l</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><mo>, </mo><mi mathvariant="bold-italic">e</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><mi mathvariant="bold-italic">e</mi></mstyle><msub><mrow></mrow><mrow><mi>j</mi><mi>i</mi></mrow></msub><mrow><mi>ln</mi></mrow><mfrac><mn>1</mn><mrow><mi>δ</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">) </mo></mrow></mfrac><mo>=</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mrow><mi>ln</mi></mrow><mfrac><mn>1</mn><mrow><mi>δ</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">) </mo></mrow></mfrac><mo>=</mo><mrow><mi>ln</mi></mrow><mfrac><mn>1</mn><mrow><mi>δ</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">) </mo><mo>⋅</mo><mi>y</mi></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>6</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="195">Softmax的风险函数R (ω) 为:</p>
                </div>
                <div class="area_img" id="196">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJC201904027_19600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="129">同样地, 将式 (17) 带入式 (14) , 用梯度下降的方法求解参数<i>ω</i>。对Logistics (Softmax) 模型训练完成就可对新的样本点进行预测。</p>
                </div>
                <h4 class="anchor-tag" id="130" name="130">2.5 算法步骤和模型复杂度分析</h4>
                <div class="p1">
                    <p id="131">本文<i>NtLC</i>-<i>SSL</i>算法过程如下:</p>
                </div>
                <div class="p1">
                    <p id="132"><b>算法</b> NtLC-SSL算法</p>
                </div>
                <div class="p1">
                    <p id="133"><b>输入</b> 经过数据清洗的无标记数据集<i>S</i></p>
                </div>
                <div class="p1">
                    <p id="134"><b>输出</b> Logistics模型 (Softmax模型) </p>
                </div>
                <div class="p1">
                    <p id="135">1) 使用k-means算法对数据集<i>S</i>进行聚类, 质心的个数为<i>k</i>。针对<i>k</i>个cluster进行分层抽样 (如第2.1节所述) 。最终生成标记数据集<i>S</i><sub><i>L</i></sub>和无标记数据集<i>S</i><sub><i>U</i></sub>。</p>
                </div>
                <div class="p1">
                    <p id="136">2) 取<i>S</i><sub><i>L</i></sub>为训练集, 单独训练模型集合<i>B</i>的每一个弱模型, 保证其泛化误差。取<i>S</i><sub><i>U</i></sub>为测试集, 输出标记数据集<i>Y</i><sup><i>B</i></sup>。</p>
                </div>
                <div class="p1">
                    <p id="137">3) 取数据集<i>Y</i><sup><i>B</i></sup>作为观测值集合, 使用基于梯度的方法训练标记噪声模型, 如式 (9) 所示。</p>
                </div>
                <div class="p1">
                    <p id="138">4) 将标记噪声模型嵌入Logistics模型 (Softmax模型) , 通过风险最小化框架进行训练。</p>
                </div>
                <div class="p1">
                    <p id="139">模型的复杂度决定其是否能够适用于复杂的数据集。简单的模型使用复杂的数据集进行训练, 会出现欠拟合的现象。例如, 谷歌提出的GoogLeNet模型<citation id="189" type="reference"><link href="33" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>, 具有强大的表达能力, 在ILSVRC2012比赛中, 测试误差为3.08%。因此, 需要通过分析NtLC-SSL的复杂度, 得出该模型的适用场景。</p>
                </div>
                <div class="p1">
                    <p id="140">在第2.4节中, 将训练好的噪声模型嵌入Logistics模型 (Softmax模型) 中, 然后通过梯度下降技术计算模型的参数, 最后输出Logistics模型 (Softmax模型) 。NtLC-SSL模型与Logistics模型 (Softmax模型) 的复杂度是类似的, 本质上均为广义线性模型。</p>
                </div>
                <h3 id="141" name="141" class="anchor-tag">3 实验结果与分析</h3>
                <h4 class="anchor-tag" id="142" name="142">3.1 抽样方法的有效性分析</h4>
                <div class="p1">
                    <p id="143">第2.1节所阐述的抽样方法, 是为了尽量保证弱模型的泛化性能, 抽取的样本要尽可能地符合样本总体的统计意义。本文考虑所抽取数据类条件概率的无偏性, 采用样本均值作为指标, 判断每个类别所对应的样本均值是否与实际总体样本均值近似, 即判断:</p>
                </div>
                <div class="p1">
                    <p id="144" class="code-formula">
                        <mathml id="144"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mn>1</mn><mrow><mi>n</mi><msub><mrow></mrow><mrow><mrow><mtext>s</mtext><mtext>a</mtext><mtext>m</mtext><mtext>p</mtext><mtext>l</mtext><mtext>e</mtext></mrow><msub><mrow></mrow><mi>i</mi></msub></mrow></msub></mrow></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>n</mi><msub><mrow></mrow><mrow><mrow><mtext>s</mtext><mtext>a</mtext><mtext>m</mtext><mtext>p</mtext><mtext>l</mtext><mtext>e</mtext></mrow><msub><mrow></mrow><mi>i</mi></msub></mrow></msub></mrow></munderover><mi mathvariant="bold-italic">x</mi></mstyle><msubsup><mrow></mrow><mi>j</mi><mrow><mrow><mtext>s</mtext><mtext>a</mtext><mtext>m</mtext><mtext>p</mtext><mtext>l</mtext><mtext>e</mtext></mrow><msub><mrow></mrow><mi>i</mi></msub></mrow></msubsup><mo>≈</mo><mfrac><mn>1</mn><mrow><mi>n</mi><msub><mrow></mrow><mrow><mi>C</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msub></mrow></mfrac><mtext> </mtext><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>n</mi><msub><mrow></mrow><mrow><mi>C</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msub></mrow></munderover><mrow></mrow></mstyle></mrow><mtext> </mtext><mi mathvariant="bold-italic">x</mi><msubsup><mrow></mrow><mi>j</mi><mrow><mi>C</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msubsup><mo>, </mo><mi>i</mi><mo>=</mo><mn>1</mn><mo>, </mo><mn>2</mn><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>k</mi><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>8</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="145">其中, <b><i>x</i></b><sup>sample<sub><i>i</i></sub></sup>表示所抽取的第<i>i</i>个类别的所有样本点, <b><i>x</i></b><sup><i>C</i><sub><i>i</i></sub></sup>表示总体样本第<i>i</i>个类别所有的样本点。计算所有类别样本均值之间的误差之和来衡量数据的无偏性:</p>
                </div>
                <div class="p1">
                    <p id="146" class="code-formula">
                        <mathml id="146"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>ε</mi><mo>=</mo><mfrac><mn>1</mn><mi>k</mi></mfrac><mtext> </mtext><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><mrow></mrow></mstyle></mrow><mrow><mo stretchy="false"> (</mo><mo stretchy="false">∥</mo><mover><mstyle mathsize="140%" displaystyle="true"><mi>X</mi></mstyle><mrow><mspace width="0.25em" /><mo>—</mo></mrow></mover><msub><mrow></mrow><mrow><mrow><mtext>s</mtext><mtext>a</mtext><mtext>m</mtext><mtext>p</mtext><mtext>l</mtext><mtext>e</mtext></mrow><msub><mrow></mrow><mi>i</mi></msub></mrow></msub><mo>-</mo><mover><mstyle mathsize="140%" displaystyle="true"><mi>X</mi></mstyle><mrow><mspace width="0.25em" /><mo>—</mo></mrow></mover><msub><mrow></mrow><mrow><mi>C</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msub><mo stretchy="false">∥</mo><mo stretchy="false">) </mo></mrow><msup><mrow></mrow><mn>2</mn></msup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>9</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="147">其中, <mathml id="148"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi>X</mi></mstyle><mrow><mspace width="0.25em" /><mo>—</mo></mrow></mover><msub><mrow></mrow><mrow><mrow><mtext>s</mtext><mtext>a</mtext><mtext>m</mtext><mtext>p</mtext><mtext>l</mtext><mtext>e</mtext></mrow><msub><mrow></mrow><mi>i</mi></msub></mrow></msub></mrow></math></mathml>表示所抽取的属于第i个类别的样本均值, <mathml id="149"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi>X</mi></mstyle><mrow><mspace width="0.25em" /><mo>—</mo></mrow></mover><msub><mrow></mrow><mrow><mi>C</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msub></mrow></math></mathml>表示总体样本中属于第<i>i</i>个类别的样本均值。‖<i>X</i>‖表示向量的二范数。</p>
                </div>
                <div class="p1">
                    <p id="150">本节实验使用UCI机器学习数据仓储中的若干个数据集作为基准数据集, 包括Iris、Wine、Breast Cancer、Segment、Handwritten Digits。表1为5个基准数据集的基本参数。</p>
                </div>
                <div class="area_img" id="151">
                    <p class="img_tit"><b>表1 5个UCI基准数据集的参数</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="151" border="1"><tr><td rowspan="2"><br />参数</td><td colspan="5"><br />基准数据集</td></tr><tr><td>Iris</td><td>Wine</td><td>Breast<br />Cancer</td><td>Segment</td><td>Handwritten<br />Digits</td></tr><tr><td>样本数量</td><td>150</td><td>178</td><td>569</td><td>2 310</td><td>5 620</td></tr><tr><td><br />特征数量</td><td>4</td><td>13</td><td>32</td><td>19</td><td>64</td></tr><tr><td><br />类别数量</td><td>3</td><td>3</td><td>2</td><td>7</td><td>10</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="152">将第2.1节描述的抽样方法用在5个数据集上, 由于数据集带有标记, 因此省略专家的标记过程。将本文抽样方法与对整体的简单随机抽样方法进行对比, SRS表示简单随机抽样, NHS表示本文NtLC-SSL分层抽样法。针对每个数据集进行100次独立实验, 对比结果如表2所示。</p>
                </div>
                <div class="area_img" id="153">
                    <p class="img_tit"><b>表2 2种方法在不同数据集上的抽样结果对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="153" border="1"><tr><td rowspan="2"><br />基准<br />数据集</td><td rowspan="2">抽样方法</td><td colspan="4"><br />抽样误差</td></tr><tr><td>最小值</td><td>最大值</td><td>均值</td><td>方差</td></tr><tr><td rowspan="2"><br />Iris</td><td>NHS方法</td><td>0.003 88</td><td>0.119 04</td><td>0.047 84</td><td>0.000 76</td></tr><tr><td><br />SRS方法</td><td>0.009 41</td><td>0.176 23</td><td>0.055 25</td><td>0.001 06</td></tr><tr><td rowspan="2"><br />Wine</td><td>NHS方法</td><td>0.142 44</td><td>0.690 45</td><td>0.301 55</td><td>0.007 88</td></tr><tr><td><br />SRS方法</td><td>0.159 94</td><td>0.976 80</td><td>0.312 79</td><td>0.013 79</td></tr><tr><td rowspan="2"><br />Breast <br />Cancer</td><td>NHS方法</td><td>0.057 07</td><td>0.560 95</td><td>0.214 26</td><td>0.009 58</td></tr><tr><td><br />SRS方法</td><td>0.046 43</td><td>0.584 60</td><td>0.230 06</td><td>0.013 64</td></tr><tr><td rowspan="2"><br />Segment</td><td>NHS方法</td><td>0.018 28</td><td>0.094 87</td><td>0.045 35</td><td>0.000 13</td></tr><tr><td><br />SRS方法</td><td>0.026 00</td><td>0.091 45</td><td>0.045 91</td><td>0.000 12</td></tr><tr><td rowspan="2"><br />Handwritten<br />Digits</td><td>NHS方法</td><td>0.331 03</td><td>0.740 03</td><td>0.507 83</td><td>0.006 29</td></tr><tr><td><br />SRS方法</td><td>0.382 84</td><td>0.805 41</td><td>0.554 23</td><td>0.007 01</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="154">从表2可以看出, 与SRS方法相比, NHS方法在平均水平下的抽样误差更小, 在数据集Iris、Wine、Breast Cancer、Segment、Handwritten Digits上分别减小了13.41%、3.6%、6.87%、1.22%、8.37%, 该方法更能抽取符合总体样本统计规律的数据。</p>
                </div>
                <h4 class="anchor-tag" id="155" name="155">3.2 NtLC-SSL实验分析</h4>
                <div class="p1">
                    <p id="156">为了分析NtLC-SSL算法的有效性, 本文采用的数据集是有标记数据集, 但在运行NtLC-SSL算法时忽略标记。引入SVM、分类与回归树 (Classification and Regression Tree, CART) 、神经网络 (Neural Network, NN) 等部分监督学习方法, 以及SemiBoost<citation id="190" type="reference"><link href="35" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>、Semi-Bagging<citation id="191" type="reference"><link href="37" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>等部分半监督集成学习方法进行对比实验。分析在监督学习情况下, NtLC-SSL算法的可行性和优越性。</p>
                </div>
                <h4 class="anchor-tag" id="157" name="157">3.2.1 生成数据和基准数据集的场景分析</h4>
                <div class="p1">
                    <p id="158">为了直观地反映算法的效果, 制定一个二分类的任务, 采用文献<citation id="192" type="reference">[<a class="sup">19</a>]</citation>算法生成一个二维特征的数据集, 如图1所示。NtLC-SSL算法得到的结果如图1中的直线所示。将NtLC-SSL算法应用在表1所述的基准数据集中, 并与常见的监督学习方法进行比较, 对比结果如表3所示, 表中加粗数值为最优值。</p>
                </div>
                <div class="area_img" id="159">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201904027_159.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 文献[19]算法生成的二维数据集" src="Detail/GetImg?filename=images/JSJC201904027_159.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图1 文献</b><citation id="193" type="reference">[<a class="sup">19</a>]</citation><b>算法生成的二维数据集</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201904027_159.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="160">
                    <p class="img_tit"><b>表3 生成数据与基准数据的分类准确率对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="160" border="1"><tr><td>数据集</td><td>NtLC-<br />SSL<br />算法</td><td>SVM<br />算法</td><td>CART<br />算法</td><td>NN<br />算法</td><td>Semi<br />Boost<br />算法</td><td>Semi-<br />Bagging<br />算法</td></tr><tr><td>Iris</td><td>0.923 3</td><td>0.936 7</td><td><b>0.926 7</b></td><td>0.916 7</td><td>0.908 7</td><td>0.857 6</td></tr><tr><td><br />Wine</td><td>0.903 0</td><td><b>0.983 1</b></td><td>0.949 1</td><td>0.981 7</td><td>0.893 7</td><td>0.812 9</td></tr><tr><td><br />Breast<br />Cancer</td><td>0.938 5</td><td>0.968 7</td><td>0.901 4</td><td><b>0.982 3</b></td><td>0.901 2</td><td>0.884 1</td></tr><tr><td><br />Segment</td><td>0.943 6</td><td>0.967 2</td><td>0.830 9</td><td><b>0.973 7</b></td><td>0.896 1</td><td>0.805 4</td></tr><tr><td><br />Handwritten<br />Digits</td><td>0.963 0</td><td><b>0.973 6</b></td><td>0.688 5</td><td>0.947 8</td><td>0.913 1</td><td>0.867 4</td></tr><tr><td><br />代码生成</td><td>0.912 0</td><td>0.924 0</td><td><b>0.968 0</b></td><td>0.908 0</td><td>0.886 3</td><td>0.856 5</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="161">从表3看出, NtLC-SSL算法的准确率不是最小的, 与最优算法相比, 差值为0.01～0.09。该算法在某些数据集上的准确率高于CART和NN算法, 差值也只有0.01～0.1。由此, 与SVM、CART和NN这3种算法相比, NtLC-SSL算法的泛化能力在可接受的范围内。与SemiBoost和Semi-Bagging算法相比, NtLC-SSL算法的准确率较高, 具有更优的泛化能力。</p>
                </div>
                <h4 class="anchor-tag" id="162" name="162">3.2.2 室内定位场景分析</h4>
                <div class="p1">
                    <p id="163">Wi-Fi指纹室内的定位方法分为2个阶段:</p>
                </div>
                <div class="p1">
                    <p id="164">1) 离线阶段, 在预选的参考点 (Reference Point, RP) 采集Wi-Fi接入点 (Access Point, AP) 的接收信号强度指示 (Received Signal Strength Indication, RSSI) 值, 创建无线图谱 (Radio Map, RM) 指纹库。</p>
                </div>
                <div class="p1">
                    <p id="165">2) 在线阶段, 用户实时采集AP的RSSI值, 通过模式识别匹配RM指纹库, 估计用户的位置。每个参考点作为一个类别, 在参考点上采集的样本都属于对应参考点的类别。</p>
                </div>
                <div class="p1">
                    <p id="166">在离线阶段, 需要花费大量的人力和时间来采集原始的Wi-Fi信号强度。图2所示是某个火车站的一层候车厅, 实心点代表参考点, 需要志愿者站在该处采集Wi-Fi信号并标记样本点, 持续时间为5 min。</p>
                </div>
                <div class="area_img" id="167">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201904027_167.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 室内空间参考点示意" src="Detail/GetImg?filename=images/JSJC201904027_167.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图2 室内空间参考点示意</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201904027_167.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="168">指纹库的创建要先规划参考点的分布, 才能进行Wi-Fi信号采集, 即数据集是先有类别, 后有样本值。因此, 本文NtLC-SSL算法创建数据集的方法应用于室内定位时需做适当修改, 但其中心思想不变, 即保证抽样数据的无偏性。具体地, 在带有标记<i>RP</i><sub><i>i</i></sub> (<i>i</i>=1, 2, …, <i>k</i>) 的参考点上进行Wi-Fi信号采集, 持续时间减小为10 s, 生成有标记数据集<i>S</i><sub><i>L</i></sub>。在所有参考点都采集完成后, 设计若干条经过所有参考点的路径, 沿着路径匀速行走并采集Wi-Fi信号强度, 生成无标记数据<i>S</i><sub><i>U</i></sub>。</p>
                </div>
                <div class="p1">
                    <p id="169">NtLC-SSL算法在基于Wi-Fi的室内定位数据集上的实验结果如表4所示。</p>
                </div>
                <div class="area_img" id="170">
                    <p class="img_tit"><b>表4 各种算法在室内定位数据集上的分类准确率对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="170" border="1"><tr><td><br />算法</td><td>准确率</td></tr><tr><td><br />NtLC-SSL算法</td><td>0.748 3</td></tr><tr><td><br />SVM算法</td><td>0.807 0</td></tr><tr><td><br />CART算法</td><td>0.529 0</td></tr><tr><td><br />NN算法</td><td>0.907 3</td></tr><tr><td><br />Semi Boost算法</td><td>0.687 5</td></tr><tr><td><br />Semi-Bagging算法</td><td>0.665 3</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="171">从表4可以看出, NtLC-SSL算法的准确率分别比NN算法和SVM算法低了0.159 0和0.058 7, 与CART算法相比高出0.219 3, 可见, NtLC-SSL算法的泛化能力相比CART算法和SVM算法, 具有一定的竞争力。与SemiBoosthe算法和Semi-Bagging算法相比, NtLC-SSL算法在准确率上有优势。这是由于参考点的数量代表类别的个数 (如图2所示) , 因此需要大量的样本和复杂的模型来对其进行建模, 使得NtLC-SSL算法的复杂度增加。</p>
                </div>
                <h3 id="172" name="172" class="anchor-tag">4 结束语</h3>
                <div class="p1">
                    <p id="173">为完成分类任务, 本文提出一种半监督学习的算法NtLC-SSL。利用集成学习的思想, 组合多个弱模型, 增强其泛化能力。对弱模型预测的标记噪声进行建模, 增强模型的健壮性。实验结果表明, 与现有的SVM、CART、NN等算法相比, NtLC-SSL算法具有较好的泛化能力。在某些分类问题中, 只需要标记抽取的样本即可取得与其他监督学习算法类似的性能, 且成本大幅降低。下一步将改进抽样方法, 考虑噪声不均匀的情况, 并在风险最小化的框架下改变模型的结构, 从而提高算法的精度, 拓展其适用范围。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CBBD&amp;filename=9787511614186000&amp;v=Mjk2MTVmWnU5dUZDdmpVN2pLSVZzZFhGcXpHYmE1SDlmTnE0NU5ZdXNQREJNOHp4VVNtRGQ5U0g3bjN4RTlmYnZuS3Jp&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> 张晨光, 张燕.半监督学习[M].北京:中国农业科学技术出版社, 2013.
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00001340068&amp;v=MjcxNjY5OVNYcVJyeG94Y01IN1I3cWVidWR0RkM3bFZMM0pKVmM9Tmo3QmFyTzRIdEhOckl0RlpPMEhZM2s1ekJkaDRq&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> NIGAM K, MCCALLUM A K, THRUN S, et al.Text classification from labeled and unlabeled documents using EM[J].Machine Learning, 2000, 38 (2/3) :103-134.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Combining Labeled and Unlabeled Data with Co-Training">

                                <b>[3]</b> BLUM A, MITCHELL T.Combining labeled and unlabeled data with co-training[C]//Proceedings of the 11th Conference on Computational Learning Theory.New York, USA:ACM Press, 1998:92-100.
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Transductive inference for text classification using support vector machines">

                                <b>[4]</b> JOACHIMS T.Transductive inference for text classifica-tion using support vector machines[C]//Proceedings of International Conference on Machine Learning.San Francisco, USA:Morgan Kaufmann Publishers, 1999:200-209.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Semi-supervised Learning using Gaussian Fields and Harmonic Functions">

                                <b>[5]</b> ZHU X J, GHAHRAMANI Z, LAFFERTY J D.Semi-supervised learning using gaussian fields and harmonic functions[C]//Proceedings of the 12th International Conference on Machine Learning.Palo Alto, USA:AAAI Press, 2003:912-919.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Ensemble methods:foundations and algorithms">

                                <b>[6]</b> ZHOU Z H.Ensemble methods:foundations and algorithms[M].London, UK:Taylor and Francis Group, 2012.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Classification in the presence of label noise:A survey">

                                <b>[7]</b> FRÉNAY B, VERLEYSEN M.Classification in the presence of label noise:a survey[J].IEEE Transactions on Neural Networks and Learning Systems, 2014, 25 (5) :845-869.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSGG201234001&amp;v=MDk4OTdyckJMejdNYWJHNEg5UFBxNDlGWllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJvRnkvblU=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b> 刘康, 钱旭, 王自强.主动学习算法综述[J].计算机工程与应用, 2012, 48 (34) :1-4.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00001339482&amp;v=MDk2NDl1ZHRGQzdsVkwzSkpWYz1OajdCYXJPNEh0SE5ySXhNWU9NTlkzazV6QmRoNGo5OVNYcVJyeG94Y01IN1I3cWVi&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b> BREIMAN L.Bagging predictors[J].Machine Learning, 1996, 24 (2) :123-140.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Boosting the margin:A new explanation for the effectiveness of voting methods">

                                <b>[10]</b> SCHAPIRE R E, FREUND Y, BARTLETT P, et al.Boosting the margin:a new explanation for the effectiveness of voting methods[C]//Proceedings of the 14th International Conference on Machine Learning.San Francisco, USA:Morgan Kaufmann Publishers, 1997:322-330.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Noise tolerance under risk minimization">

                                <b>[11]</b> MANWANI N, SASTRY P S.Noise tolerance under risk minimization[J].IEEE Transactions on Cybernetics, 2013, 43 (3) :1146-1151.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=On the consistency of multiclass classification methods">

                                <b>[12]</b> TEWARI A, BARTLETT P L.On the consistency of multiclass classification methods[C]//Proceedings of International Conference on Computational Learning Theory.Berlin, Germany:Springer, 2007:143-157.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Structural risk minimization over data-dependent hierarchies">

                                <b>[13]</b> SHAWE-TAYLOR J, BARTLETT P L, WILLIAMSON R C, et al.Structural risk minimization over data-dependent hierarchies[J].IEEE Transactions on Information Theory, 1998, 44 (5) :1926-1940.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multi-class support vector machines">

                                <b>[14]</b> WESTON J, WATKINS C.Multi-class support vector machines[C]//Proceedings of European Symposia on Artificial Neural Networks.Brussels, Belgium:[s.n.], 1999:83-128.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_15" target="_blank" href="/kcms/detail/detail.aspx?dbcode=STJD&amp;filename=STJD13062800006497&amp;v=MDM1OTZKMXdWYnhzPU5qbkJhcks3SHRmT3A0OUZaT3NKQ0hVK29CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVUxuSQ==&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[15]</b> BARTLETT P L, JORDAN M I, MCAULIFFE J D.Convexity, classification, and risk bounds[J].Journal of the American Statistical Association, 2006, 101 (473) :138-156.
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Inception-v4,Inception-ResNet and the Impact of Residual Connections on Learning">

                                <b>[16]</b> SZEGEDY C, IOFFE S, VANHOUCKE V, et al.Inception-v4, inception-resnet and the impact of residual connections on learning[C]//Proceedings of the 31st AAAI Conference on Artificial Intelligence.Palo Alto, USA:AAAI Press, 2017:4278-4284.
                            </a>
                        </p>
                        <p id="35">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=SemiBoost: Boosting for semi-supervised learning">

                                <b>[17]</b> MALLAPRAGADA P K, JIN R, JAIN A K, et al.SemiBoost:boosting for semi-supervised learning[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2009, 31 (11) :2000-2014.
                            </a>
                        </p>
                        <p id="37">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Semi-supervised question classification based on ensemble learning">

                                <b>[18]</b> LI Y Y, SU L, CHEN J, et al.Semi-supervised question classification based on ensemble learning[C]//Proceedings of International Conference on Swarm Intelligence.Berlin, Germany:Springer, 2015:341-348.
                            </a>
                        </p>
                        <p id="39">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Design of experiments for the NIPS 2003 variable selection benchmark">

                                <b>[19]</b> GUYON I.Design of experiments for the NIPS 2003 variable selection benchmark[EB/OL].[2018-01-05].http://clopinet.com/isabelle/Projects/NIPS2003/Slides/ NIPS2003-Datasets.pdf.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJC201904027" />
        <input id="dpi" type="hidden" value="600" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201904027&amp;v=MjU1NTJGeS9uVXJyQkx6N0JiYkc0SDlqTXE0OUhZNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm8=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
