<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637131280676525000%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJC201902045%26RESULT%3d1%26SIGN%3d0belY8vHnBHZsSuHjWU5n7DfNj0%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201902045&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201902045&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201902045&amp;v=MDYyODVMT2VaZVJuRnlqa1ZicklMejdCYmJHNEg5ak1yWTlCWVlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#83" data-title="0概述 ">0概述</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#198" data-title="1 PCANet ">1 PCANet</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#90" data-title="1.1 PCA卷积层">1.1 PCA卷积层</a></li>
                                                <li><a href="#95" data-title="1.2非线性处理层">1.2非线性处理层</a></li>
                                                <li><a href="#99" data-title="1.3特征池化层">1.3特征池化层</a></li>
                                                <li><a href="#102" data-title="1.4预训练卷积核">1.4预训练卷积核</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#157" data-title="3 实验与结果分析 ">3 实验与结果分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#160" data-title="3.1 网络参数选取">3.1 网络参数选取</a></li>
                                                <li><a href="#164" data-title="3.2 实验对比算法">3.2 实验对比算法</a></li>
                                                <li><a href="#172" data-title="3.3 实验结果">3.3 实验结果</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#196" data-title="4 结束语 ">4 结束语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#89" data-title="图1 PCANet结构示意图">图1 PCANet结构示意图</a></li>
                                                <li><a href="#125" data-title="图2 MS-PCANet-SPP结构示意图">图2 MS-PCANet-SPP结构示意图</a></li>
                                                <li><a href="#129" data-title="图3 空间金字塔池化示意图">图3 空间金字塔池化示意图</a></li>
                                                <li><a href="#159" data-title="图4 单行人样本检测结果">图4 单行人样本检测结果</a></li>
                                                <li><a href="#162" data-title="图5 不同卷积核尺寸对应的网络正确检测率">图5 不同卷积核尺寸对应的网络正确检测率</a></li>
                                                <li><a href="#175" data-title="表1 各算法在INRIA数据库中性能对比结果">表1 各算法在INRIA数据库中性能对比结果</a></li>
                                                <li><a href="#176" data-title="表2 各算法在NICTA数据库中性能对比结果">表2 各算法在NICTA数据库中性能对比结果</a></li>
                                                <li><a href="#185" data-title="图6 不同训练样本数目下各算法检测结果">图6 不同训练样本数目下各算法检测结果</a></li>
                                                <li><a href="#189" data-title="图7 部分自建数据集样本">图7 部分自建数据集样本</a></li>
                                                <li><a href="#220" data-title="表3 各算法在自建数据集中的性能对比结果">表3 各算法在自建数据集中的性能对比结果</a></li>
                                                <li><a href="#195" data-title="图8 不同数据库中算法检测结果">图8 不同数据库中算法检测结果</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="43">


                                    <a id="bibliography_1" title="苏志松, 李绍滋, 陈淑媛, 等.行人检测技术综述[J].电子学报, 2012, 40 (4) :814-820." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZXU201204030&amp;v=MjQ0MDllUm5GeWprVmJySUlUZlRlN0c0SDlQTXE0OUdaSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVo=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                        苏志松, 李绍滋, 陈淑媛, 等.行人检测技术综述[J].电子学报, 2012, 40 (4) :814-820.
                                    </a>
                                </li>
                                <li id="45">


                                    <a id="bibliography_2" title="DALAL N, TRIGGS B.Histograms of oriented gradients for human detection[C]//Proceedings of IEEEComputer Society Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEEPress, 2005:886-893." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Histograms of Oriented Gradients for Human Detection">
                                        <b>[2]</b>
                                        DALAL N, TRIGGS B.Histograms of oriented gradients for human detection[C]//Proceedings of IEEEComputer Society Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEEPress, 2005:886-893.
                                    </a>
                                </li>
                                <li id="47">


                                    <a id="bibliography_3" title="MU Y, YAN S, LIU Y, et al.Discriminative local binary binary pattems for pedestrian detection i personal album[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2008:1-8." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Discriminative local binary patternsfor pedestrian detection in personal album">
                                        <b>[3]</b>
                                        MU Y, YAN S, LIU Y, et al.Discriminative local binary binary pattems for pedestrian detection i personal album[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2008:1-8.
                                    </a>
                                </li>
                                <li id="49">


                                    <a id="bibliography_4" title="TUZEL O, PORIKLI F, MEER P.Pedestrian detection via classification on Riemannian manifolds[J].IEEETransactions on Pattem Analysis and Machine Intelligence, 2008, 30 (10) :1713-1727." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Pedestrian detection via classification on Riemannian manifolds">
                                        <b>[4]</b>
                                        TUZEL O, PORIKLI F, MEER P.Pedestrian detection via classification on Riemannian manifolds[J].IEEETransactions on Pattem Analysis and Machine Intelligence, 2008, 30 (10) :1713-1727.
                                    </a>
                                </li>
                                <li id="51">


                                    <a id="bibliography_5" title="VIOLA P, JONES M.Robust real-time face detection[J].International Journal of Computer Vision, 2004, 57 (2) :137-154." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002830860&amp;v=MjIxODVNSDdSN3FlYnVkdEZDSGxVNzdPSlY0PU5qN0Jhck80SHRIT3A0eEZiTzBQWTNrNXpCZGg0ajk5U1hxUnJ4b3hj&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                        VIOLA P, JONES M.Robust real-time face detection[J].International Journal of Computer Vision, 2004, 57 (2) :137-154.
                                    </a>
                                </li>
                                <li id="53">


                                    <a id="bibliography_6" >
                                        <b>[6]</b>
                                    HINTON G, SALAKHUTDINOV R.Reducing the dimensionality of data with neural networks[J].Science, 2006, 313 (5786) :304-304.</a>
                                </li>
                                <li id="55">


                                    <a id="bibliography_7" title="余凯, 贾磊, 陈雨强, 等.深度学习的昨天、今天和明天[J].计算机研究与发展, 2013, 50 (9) :1799-1804." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201309002&amp;v=Mjk5Njg2ajU0TzN6cXFCdEdGckNVUkxPZVplUm5GeWprVmJySUx5dlNkTEc0SDlMTXBvOUZab1FLREg4NHZSNFQ=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                        余凯, 贾磊, 陈雨强, 等.深度学习的昨天、今天和明天[J].计算机研究与发展, 2013, 50 (9) :1799-1804.
                                    </a>
                                </li>
                                <li id="57">


                                    <a id="bibliography_8" title="CHAN T H, JIA K, GAO S, et al.PCANet:a simple deep learning baseline for image classification?[J].IEEE Transactions on Image Processing, 2015, 24 (12) :5017-5032." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=PCANet:A simple deep learning baseline for image classification?">
                                        <b>[8]</b>
                                        CHAN T H, JIA K, GAO S, et al.PCANet:a simple deep learning baseline for image classification?[J].IEEE Transactions on Image Processing, 2015, 24 (12) :5017-5032.
                                    </a>
                                </li>
                                <li id="59">


                                    <a id="bibliography_9" title="GAO F, DONG J, LI B, et al.Automatic change detection in synthetic aperture radar images based on PCANet[J].IEEE Geoscience and Remote Sensing Letters, 2016, 13 (12) :1792-1796." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Automatic Change Detection in Synthetic Aperture Radar Imag-es Based on PCANet">
                                        <b>[9]</b>
                                        GAO F, DONG J, LI B, et al.Automatic change detection in synthetic aperture radar images based on PCANet[J].IEEE Geoscience and Remote Sensing Letters, 2016, 13 (12) :1792-1796.
                                    </a>
                                </li>
                                <li id="61">


                                    <a id="bibliography_10" title="WANG Z, BI Z, WANG C, et al.Traffic lights recognition based on PCANet[C]//Proceedings of Chinese Automation Congress.Washington D.C., USA:IEEE Press, 2016:559-564." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Traffic lights recognition based on PCANet">
                                        <b>[10]</b>
                                        WANG Z, BI Z, WANG C, et al.Traffic lights recognition based on PCANet[C]//Proceedings of Chinese Automation Congress.Washington D.C., USA:IEEE Press, 2016:559-564.
                                    </a>
                                </li>
                                <li id="63">


                                    <a id="bibliography_11" title="WANG S, CHEN L, ZHOU Z, et al.Human fall detection in surveillance video based on PCANet[J].Multimedia Tools and Applications, 2016, 75 (19) :11603-11613." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Human fall detection in surveillance video based on PCANet">
                                        <b>[11]</b>
                                        WANG S, CHEN L, ZHOU Z, et al.Human fall detection in surveillance video based on PCANet[J].Multimedia Tools and Applications, 2016, 75 (19) :11603-11613.
                                    </a>
                                </li>
                                <li id="65">


                                    <a id="bibliography_12" title="YANG J, YU K, GONG Y, et al.Linear spatial pyramid matching using sparse coding for image classification[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2009:1794-1801." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Linear spatial pyramid matching using sparse coding for image classification">
                                        <b>[12]</b>
                                        YANG J, YU K, GONG Y, et al.Linear spatial pyramid matching using sparse coding for image classification[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2009:1794-1801.
                                    </a>
                                </li>
                                <li id="67">


                                    <a id="bibliography_13" >
                                        <b>[13]</b>
                                    SERMANET P, LECUN Y.Traffic sign recognition with multi-scale convolutional networks[C]//Proceedings of International Joint Conference on Neural Networks.Washington D.C., USA:IEEE Press, 2011:2809-2813.</a>
                                </li>
                                <li id="69">


                                    <a id="bibliography_14" title="HOAI M.Regularized max pooling for image categorization[J].Journal of the British Institution of Radio Engineers, 2014, 14 (3) :94-100." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Regularized max pooling for image categorization">
                                        <b>[14]</b>
                                        HOAI M.Regularized max pooling for image categorization[J].Journal of the British Institution of Radio Engineers, 2014, 14 (3) :94-100.
                                    </a>
                                </li>
                                <li id="71">


                                    <a id="bibliography_15" title="顾凌云, 吕志文, 杨勇, 等.基于PCANet和SVM的谎言测试研究[J].电子学报, 2016, 44 (8) :1969-1973." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZXU201608028&amp;v=MDU3Njc0SDlmTXA0OUhiSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm5GeWprVmJySUlUZlRlN0c=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[15]</b>
                                        顾凌云, 吕志文, 杨勇, 等.基于PCANet和SVM的谎言测试研究[J].电子学报, 2016, 44 (8) :1969-1973.
                                    </a>
                                </li>
                                <li id="73">


                                    <a id="bibliography_16" >
                                        <b>[16]</b>
                                    CHANG C C, LIN C J.LIBSVM:A library for support vector machines[J].ACM Transactions on Intelligent Systems and Technology, 2011, 2 (3) :1-27.</a>
                                </li>
                                <li id="75">


                                    <a id="bibliography_17" title="OVERETT G, PETERSSON L.Boosting with multiple classifier families[C]//Proceedings of IEEE Intelligent Vehicles Symposium.Washington D.C., USA:IEEEPress, 2007:1039-1044." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Boosting with multiple classifier families">
                                        <b>[17]</b>
                                        OVERETT G, PETERSSON L.Boosting with multiple classifier families[C]//Proceedings of IEEE Intelligent Vehicles Symposium.Washington D.C., USA:IEEEPress, 2007:1039-1044.
                                    </a>
                                </li>
                                <li id="77">


                                    <a id="bibliography_18" title="SHAILAJA K, ANURADHA B.Effective face recognition using deep learning based linear discriminant classification[C]//Proceedings of IEEE International Conference on Computational Intelligence and Computing Research.Washington D.C., USA:IEEE Press, 2016:1-6." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Effective face recognition using deep learning based linear discriminant classification">
                                        <b>[18]</b>
                                        SHAILAJA K, ANURADHA B.Effective face recognition using deep learning based linear discriminant classification[C]//Proceedings of IEEE International Conference on Computational Intelligence and Computing Research.Washington D.C., USA:IEEE Press, 2016:1-6.
                                    </a>
                                </li>
                                <li id="79">


                                    <a id="bibliography_19" title="KRIZHEVSKY A, SUTSKEVER I, HINTON G.ImageNet classification with deep convolutional neural networks[C]//Proceedings of NIPS’12.Nevada, USA:Curran Associates InC., 2012:1097-1105." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=ImageNet classifcation with deep convolutional neural networks">
                                        <b>[19]</b>
                                        KRIZHEVSKY A, SUTSKEVER I, HINTON G.ImageNet classification with deep convolutional neural networks[C]//Proceedings of NIPS’12.Nevada, USA:Curran Associates InC., 2012:1097-1105.
                                    </a>
                                </li>
                                <li id="81">


                                    <a id="bibliography_20" title="WOJEK C, WALK S, SCHIELE B.Multi-cue onboard pedestrian detection[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2009:794-801." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multi-cue onboard pedestrian detection">
                                        <b>[20]</b>
                                        WOJEK C, WALK S, SCHIELE B.Multi-cue onboard pedestrian detection[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2009:794-801.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJC" target="_blank">计算机工程</a>
                2019,45(02),270-277 DOI:10.19678/j.issn.1000-3428.0049831            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm">多尺度空间金字塔池化PCANet的行人检测</span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%A4%8F%E8%83%A1%E4%BA%91&amp;code=37006539&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">夏胡云</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%8F%B6%E5%AD%A6%E4%B9%89&amp;code=16964408&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">叶学义</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%BD%97%E5%AE%B5%E6%99%97&amp;code=38675248&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">罗宵晗</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E9%B9%8F&amp;code=25549147&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王鹏</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%9D%AD%E5%B7%9E%E7%94%B5%E5%AD%90%E7%A7%91%E6%8A%80%E5%A4%A7%E5%AD%A6%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB%E4%B8%8E%E4%BF%A1%E6%81%AF%E5%AE%89%E5%85%A8%E5%AE%9E%E9%AA%8C%E5%AE%A4&amp;code=0073968&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">杭州电子科技大学模式识别与信息安全实验室</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>针对非理想条件下行人检测的性能和效率问题, 提出多尺度空间金字塔PCANet。将空间金字塔作为网络的特征池化层, 通过分层池化特征的方式获得图像的显著性特征, 并将底层特征和高层特征级联以获得样本的多尺度特征的向量表示, 输入SVM分类器。在INRIA和NICTA数据库中, 与HOG、CNN等算法进行行人检测对比实验, 结果表明, 该算法有更高的正确检测率、更低的漏检率和误检率。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%A1%8C%E4%BA%BA%E6%A3%80%E6%B5%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">行人检测;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%9E%B6%E6%9E%84&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度学习架构;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">主成分分析网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%A4%9A%E5%B0%BA%E5%BA%A6%E7%89%B9%E5%BE%81&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">多尺度特征;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%A9%BA%E9%97%B4%E9%87%91%E5%AD%97%E5%A1%94%E6%B1%A0%E5%8C%96&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">空间金字塔池化;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%98%BE%E8%91%97%E6%80%A7%E7%89%B9%E5%BE%81&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">显著性特征;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    夏胡云 (1994—) , 男, 硕士研究生, 主研方向为深度学习、计算机视觉;E-mail: xiahuyun043@ 126. com
;
                                </span>
                                <span>
                                    叶学义, 副教授、博士;
;
                                </span>
                                <span>
                                    罗宵晗、硕士研究生。
;
                                </span>
                                <span>
                                    王鹏, 硕士研究生。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2017-12-25</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金 (60802047, 60702018);</span>
                    </p>
            </div>
                    <h1>Pedestrian Detection Using Multi-scale Principal Component Analysis Network of Spatial Pyramid Pooling</h1>
                    <h2>
                    <span>XIA Huyun</span>
                    <span>YE Xueyi</span>
                    <span>LUO Xiaohan</span>
                    <span>WANG Peng</span>
            </h2>
                    <h2>
                    <span>Lab of Pattern Recognition and Information Security, Hangzhou Dianzi Universtiy</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Pedestrian detection is easily affected by non-ideal factors such as complex background, shooting angle and diversity of human body posture in natural environment. To solve this problem, this paper proposes a Multi-scale Principal Component Analysis Network of Spatial Pyramid Pooling (MS-PCANet-SPP) . The feature pooling layer using spatial pyramid pooling method extract the saliency features of the image. The multi-scale features of the input samples can be obtained by cascading the high-level and low-level features, which is input to the SVM classifer. The comparative experiments are performed in the INRIA and NICTA databases. Experimental results show that, compared with HOG, CNN and other algorithms, MS-PCANet-SPP has a higher detection rate, a lower miss rate, and a lower false positive rate.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=pedestrian%20detection&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">pedestrian detection;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=deep%20learning%20framework&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">deep learning framework;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Principal%20Component%20Analysis%20Network%20(PCANet)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Principal Component Analysis Network (PCANet) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=multi-scale%20feature&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">multi-scale feature;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=spatial%20pyramid%20pooling&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">spatial pyramid pooling;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=saliency%20feature&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">saliency feature;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2017-12-25</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="83" name="83" class="anchor-tag">0概述</h3>
                <div class="p1">
                    <p id="84">行人检测一直是计算机视觉和模式识别的研究热点和难点。目前行人检测基于统计分类的方法, 通过特征提取和分类与定位<citation id="221" type="reference"><link href="43" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>, 将行人检测转化成人与非人的二分类问题。从分类采用的特征来看, 目前行人检测的研究大致可以分为基于底层特征、基于混合特征、基于机器学习的特征3类方法。基于底层特征的方法利用梯度<citation id="222" type="reference"><link href="45" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>或者纹理<citation id="223" type="reference"><link href="47" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>等特征实现行人检测。该类方法只从单一角度描述行人特征, 容易受复杂背景、遮挡和光照等的影响。基于混合特征<citation id="224" type="reference"><link href="49" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>的方法利用各种不同特征从不同侧面刻画行人特征, 提高检测准确率。该方法会增加特征维度, 导致检测效率降低, 影响检测的实时性。基于机器学习的特征<citation id="225" type="reference"><link href="51" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>的方法利用学习网络, 采用统计分析的策略从大量样本中选择出判别能力较强的特征。该方法特征的选择与训练样本密切相关, 若样本集不具有代表性很难选择出好的特征。</p>
                </div>
                <div class="p1">
                    <p id="85">上述研究都试图将图像中行人区域的数据信息映射成语义信息, 其难点是对在非理想的自然环境下获得的图像实现稳定的映射。文献<citation id="226" type="reference">[<a class="sup">6</a>]</citation>提出深度学习的概念, 通过深度结构模拟人脑分层次学习的机制, 构建具有多隐层的机器学习模型来自下而上地学习输入数据的高层语义信息<citation id="227" type="reference"><link href="55" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>。相比于传统的机器学习方法, 其更能够刻画数据的丰富内在信息。文献<citation id="228" type="reference">[<a class="sup">8</a>]</citation>提出一种主成分分析网络 (Principal Component Analysis Network, PCANet) , 由主成分分析 (Principal Component Analysis, PCA) 、哈希运算及局部直方图3个基本操作构成。PCANet已经表现出优异的性能, 成为图像分类、识别领域中的一个研究热点<citation id="229" type="reference"><link href="59" rel="bibliography" /><link href="61" rel="bibliography" /><link href="63" rel="bibliography" /><sup>[<a class="sup">9</a>,<a class="sup">10</a>,<a class="sup">11</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="86">由于空间金字塔池化对形变目标具有较强鲁棒性<citation id="230" type="reference"><link href="65" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>以及多尺度卷积神经网络 (Convolutional Neural Netw ork, CNN) <citation id="231" type="reference"><link href="67" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>具有较高的分辨率, 本文在典型PCANet的基础上, 提出一种多尺度空间金字塔池化PCANet (Multi-scale Principal Component Analysis Netw ork of Spatial Pyramid Pooling, M S-PCANet-SPP) 。引入空间金字塔池化, 运用最大值池化方法逐层构建图像的描述子<citation id="232" type="reference"><link href="69" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>, 在整合并优化原始池化特征的同时, 捕获图像的结构化信息。将经空间金字塔池化得到的特征向量级联以提取图像的多尺度特征。</p>
                </div>
                <h3 id="198" name="198" class="anchor-tag">1 PCANet</h3>
                <div class="p1">
                    <p id="88">PCANet<citation id="233" type="reference"><link href="57" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>是一种深度特征提取网络, 由PCA卷积层、非线性处理层和特征池化层组成, 如图1所示。其中, I<sub>i</sub>为输入第i个样本, W<sub>l</sub><sup>1</sup> (l=1, 2, …, L<sub>1</sub>) 为PCA卷积层第1层的第l个卷积核, W<sub>λ</sub><sup>2</sup> (λ=1, 2, …, L<sub>2</sub>) 为PCA卷积层第2层的第λ个卷积核, L<sub>1</sub>和L<sub>2</sub>分别为第1层和第2层的卷积核数目, I<sub>i</sub><sup>l</sup> (l=1, 2, …, L<sub>1</sub>) 为I<sub>i</sub>与W<sub>l</sub><sup>1</sup>卷积的第l个输出, O<sub>i</sub><sup>l</sup> (l=1, 2, …, L<sub>1</sub>) 为I<sub>i</sub><sup>l</sup>与所有W<sub>l</sub><sup>2</sup>卷积得到的一组输出, 每组输出O<sub>i</sub><sup>l</sup>的数目为L<sub>2</sub>, T<sub>i</sub><sup>l</sup> (l=1, 2, …, L<sub>1</sub>) 为对O<sub>i</sub><sup>l</sup>中每组输出进行二值化加权得到的一幅“整型图”, f<sub>i</sub>为对所有T<sub>i</sub><sup>l</sup>进行局部直方图并且级联得到的网络的特征向量。在图1中, 符号*表示卷积, 红色块表示卷积核与该块进行卷积, 得到的卷积结果用黄色块表示 (彩色见电子版) 。给定N个不同的训练样本{I<sub>i</sub>}<sub>i</sub><sup>N</sup><sub>=1</sub>, 其中样本大小为m×n, 下文将对PCANet进行详细描述。</p>
                </div>
                <div class="area_img" id="89">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201902045_08900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 PCANet结构示意图" src="Detail/GetImg?filename=images/JSJC201902045_08900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 PCANet结构示意图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201902045_08900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="90" name="90">1.1 PCA卷积层</h4>
                <div class="p1">
                    <p id="91">PCANet采用2层卷积层提取输入样本的特征。文献<citation id="234" type="reference">[<a class="sup">8</a>]</citation>表明:当卷积层层数低于2时, 分类、识别的效果不好;当卷积层层数高于2时, 维度的增加导致计算量急剧增加;卷积层层数等于2是比较合适的。将输入样本I<sub>i</sub>与第1层卷积核卷积, 得到L<sub>1</sub>个卷积输出:</p>
                </div>
                <div class="area_img" id="92">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJC201902045_09200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="93">每一个卷积输出I<sub>i</sub><sup>l</sup>与第2层的所有卷积核W<sub>λ</sub><sup>2</sup>卷积得到L<sub>2</sub>个输出O<sub>i</sub><sup>l</sup>, 则在第2层, L<sub>1</sub>个输入I<sub>i</sub><sup>l</sup>将得到L<sub>1</sub>L<sub>2</sub>个输出:</p>
                </div>
                <div class="area_img" id="94">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJC201902045_09400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <h4 class="anchor-tag" id="95" name="95">1.2非线性处理层</h4>
                <div class="p1">
                    <p id="96">为加强样本特征表达性<citation id="235" type="reference"><link href="71" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>, 对图1中第2层的每个输出O<sub>i</sub><sup>l</sup>进行非线性处理, 使用赫维赛德阶跃函数H (x) 作为非线性处理函数二值化输出O<sub>i</sub><sup>l</sup>, 将O<sub>i</sub><sup>l</sup>中像素值大于0的置为1, 小于0的置为0。对二值化后的输出进行加权处理, 加权系数如下:</p>
                </div>
                <div class="area_img" id="97">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJC201902045_09700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="98">通过二值化加权处理使得O<sub>i</sub><sup>l</sup>中的每L<sub>2</sub>个输出得到一幅“整型图”。其中, “整型图”表示输出每个像素值的范围是[0, 2<sup>L2</sup>-1]之间的整数。因此, 每个输入样本I<sub>i</sub>将得到L<sub>1</sub>个“整型图”。</p>
                </div>
                <h4 class="anchor-tag" id="99" name="99">1.3特征池化层</h4>
                <div class="p1">
                    <p id="100">由于上述的非线性处理层得到的像素值为[0, 2<sup>L2-1</sup>]之间的整数, 而CNN中非线性处理层为前一层卷积层进行最大值或均值池化得到的像素值, 因此CNN的池化方式不适用于PCANet<citation id="236" type="reference"><link href="57" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>。文献<citation id="237" type="reference">[<a class="sup">8</a>]</citation>用局部直方图作为PCANet的特征池化层操作, 将每一个输出“整型图”T<sub>i</sub><sup>l</sup>分为B块, 对每块进行直方图统计, 并将B个直方图统计结果按列向量化, 记作Bhist (T<sub>i</sub><sup>l</sup>) 。级联L<sub>1</sub>个“整型图”所产生的向量得到输入样本I<sub>i</sub>的特征向量表示:</p>
                </div>
                <div class="area_img" id="101">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJC201902045_10100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <h4 class="anchor-tag" id="102" name="102">1.4预训练卷积核</h4>
                <div class="p1">
                    <p id="103">通过PCA预训练的方式获得PCANet中卷积核的值, 预训练的步骤为:</p>
                </div>
                <div class="p1">
                    <p id="199">步骤1对第i个输入样本I<sub>i</sub>, 以k<sub>1</sub>×k<sub>2</sub>大小的块在样本上滑动截取得到c= (m-k<sub>1</sub>+1) × (nk<sub>2</sub>+1) 个块, 将每个块向量化得到x<sub>i, 1</sub>, x<sub>i, 2</sub>, …, <image id="200" type="formula" href="images/JSJC201902045_20000.jpg" display="inline" placement="inline"><alt></alt></image><image id="200" type="formula" href="images/JSJC201902045_20001.jpg" display="inline" placement="inline"><alt></alt></image>, 其中, x<sub>i, j</sub>表示第i个样本I<sub>i</sub>的第j个块向量化后的向量。</p>
                </div>
                <div class="p1">
                    <p id="106">步骤2对x<sub>i, j</sub>进行去均值级联得到<image id="201" type="formula" href="images/JSJC201902045_20100.jpg" display="inline" placement="inline"><alt></alt></image><image id="201" type="formula" href="images/JSJC201902045_20101.jpg" display="inline" placement="inline"><alt></alt></image>, 其中, <image id="202" type="formula" href="images/JSJC201902045_20200.jpg" display="inline" placement="inline"><alt></alt></image>表示第i个输入样本预处理后的结果。</p>
                </div>
                <div class="p1">
                    <p id="107">步骤3对输入所有N个样本进行相同处理并且级联得到:</p>
                </div>
                <div class="area_img" id="108">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJC201902045_10800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="109">步骤4使用PCA算法对样本集X进行计算, PCA最小化重构误差的过程表达为:</p>
                </div>
                <div class="area_img" id="110">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJC201902045_11000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="111">其中, I<sub>L1</sub>为L<sub>1</sub>×L<sub>1</sub>的单位矩阵, V为满足PCA最小化重构误差的标准正交基。由PCA算法可知, 要解出PCA卷积层第1层卷积核W<sub>l</sub><sup>1</sup>首先需要计算X的协方差矩阵C:</p>
                </div>
                <div class="area_img" id="112">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJC201902045_11200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="203">计算协方差矩阵C对应的特征向量, 由PCA算法可知该特征向量即为所求的标准正交基V。式 (8) 中q<sub>L1</sub> (V) 表示提取V的前L<sub>1</sub>个主成分分量, 将每个主成分分量记作w<sup>1</sup><sub>l</sub> (l=1, 2, …, L<sub>1</sub>) , 式 (9) 将<image id="204" type="formula" href="images/JSJC201902045_20400.jpg" display="inline" placement="inline"><alt></alt></image>投影为矩阵<image id="205" type="formula" href="images/JSJC201902045_20500.jpg" display="inline" placement="inline"><alt></alt></image>, 得到L<sub>1</sub>个矩阵W<sup>1</sup><sub>l</sub>即为PCA卷积层第1层卷积核。</p>
                </div>
                <div class="p1">
                    <p id="206">步骤5对PCA卷积层第1层的所有输出I<sup>l</sup><sub>i</sub>滑动截取、向量化、去均值得到<image id="207" type="formula" href="images/JSJC201902045_20700.jpg" display="inline" placement="inline"><alt></alt></image><image id="207" type="formula" href="images/JSJC201902045_20701.jpg" display="inline" placement="inline"><alt></alt></image>, 则第l个卷积核输出结果去均值化的块表示为<image id="208" type="formula" href="images/JSJC201902045_20800.jpg" display="inline" placement="inline"><alt></alt></image>, 级联所有卷积核的输出块Y<sup>l</sup> (l=1, 2, …, L<sub>1</sub>) 得到:</p>
                </div>
                <div class="area_img" id="119">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJC201902045_11900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="120">步骤6重复步骤1～步骤5, 利用PCA算法对Y进行计算, 得到PCA卷积层第2层的卷积核:</p>
                </div>
                <div class="area_img" id="121">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJC201902045_12100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="122">文献<citation id="238" type="reference">[<a class="sup">8</a>]</citation>通过PCA卷积层、非线性处理层和特征池化层建立输入数据从底层特征到高层特征的映射, 提取输入样本的高层语义特征。但是, 只提取高层语义特征作为最终特征向量表示的方式忽视了数据的底层特征, 而底层特征包含更多的局部细节信息。文献<citation id="239" type="reference">[<a class="sup">13</a>]</citation>表明结合底层特征的局部细节信息和高层特征的抽象语义信息的样本多尺度的特征向量表示可有效减少由于多次卷积和池化操作导致的信息损失。同时, 文献<citation id="240" type="reference">[<a class="sup">12</a>]</citation>通过空间金字塔最大值池化方法对原始冗余特征进行提取, 对提取的特征向量进行聚合, 将不相关信息丢弃后保留图像中重要的信息, 使得网络具有更好的抗噪性和鲁棒性。</p>
                </div>
                <div class="area_img" id="123">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJC201902045_12300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="124">基于上述分析, 本文提出MS-PCANet-SPP, 由PCA卷积层、非线性处理层和特征池化层组成, 其结构如图2所示。类似于PCANet, 卷积层数为2, I<sub>i</sub>为输入样本, W<sup>1</sup><sub>l1</sub>和W<sup>2</sup><sub>l2</sub>分别为PCA卷积层第1层和第2层的卷积核, O<sub>i</sub><sup>1</sup> (l<sub>1</sub>) 和O<sub>i</sub><sup>2</sup> (l<sub>2</sub>) 分别为PCA卷积层第1层和第2层的卷积输出。其中, 黑色块表示对该块进行去均值化, 得到的结果用红色块表示;在每一层对红色块进行卷积, 卷积的结果用黄色块表示 (彩色见电子版) 。在非线性处理层, 对O<sub>i</sub><sup>1</sup> (l<sub>1</sub>) 进行非线性处理得到的“整型图”T<sub>i</sub><sup>1</sup> (l<sub>1</sub>) , 对O<sub>i</sub><sup>2</sup> (l<sub>2</sub>) 进行非线性处理得到的“整型图”T<sub>i</sub><sup>2</sup> (l<sub>2</sub>) 。在特征池化层引入空间金字塔池化策略逐层构建图像的特征描述子, f<sub>i, 1</sub>和f<sub>i, 2</sub>为每层“整型图”经过空间金字塔池化得到的特征向量, 将经过空间金字塔池化得到的不同尺度的特征向量级联以获得图像的多尺度特征, 即f<sub>i</sub>=[f<sub>i, 1</sub>;f<sub>i, 2</sub>]为级联f<sub>i, 1</sub>和f<sub>i, 2</sub>得到的特征向量。其中, f<sub>i, 1</sub>为f<sub>i</sub>的低维表示, 因此图2对向量进行倒置以表示f<sub>i, 1</sub>和f<sub>i, 2</sub>的这种级联关系。将f<sub>i</sub>=[f<sub>i, 1</sub>;f<sub>i, 2</sub>]送入支持向量机 (Support Vector Machine, SVM) 进行判断:输入样本图像中有无行人。</p>
                </div>
                <div class="area_img" id="125">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201902045_12500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 MS-PCANet-SPP结构示意图" src="Detail/GetImg?filename=images/JSJC201902045_12500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 MS-PCANet-SPP结构示意图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201902045_12500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="126">2.1多尺度特征及空间金字塔池化</p>
                </div>
                <div class="p1">
                    <p id="127">本文描述的多尺度特征表示的是输入样本从底层特征到高层特征的多尺度关系, 而空间金字塔表示的是空间像素级的多层次关系。如图3所示, U为局部直方图提取的特征向量集合, l∈L表示空间第l层级的划分, L是大于等于0的整数, 每层会有2<sup>l</sup>个子集, 即在横坐标以及纵坐标方向都被划分为2<sup>l</sup>个网格, 得到的图像块个数为2<sup>l</sup>×2<sup>l</sup>=4<sup>l</sup>个。分别在L层级中, 对特征向量运用池化算法, 将池化的函数表示为:</p>
                </div>
                <div class="area_img" id="128">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJC201902045_12800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="area_img" id="129">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201902045_12900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 空间金字塔池化示意图" src="Detail/GetImg?filename=images/JSJC201902045_12900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 空间金字塔池化示意图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201902045_12900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="130">由于均值池化容易丢失图像中目标的细节信息<citation id="241" type="reference"><link href="69" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>, 本文采用最大值函数作为空间函数。将从图像提取的特征向量对应的维度最大值表示图像对应的响应程度, 计算局部图像块编码的最大值的公式如下:</p>
                </div>
                <div class="area_img" id="131">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJC201902045_13100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="132">其中, z<sub>j</sub>为Z的第j个元素, u<sub>mj</sub>为矩阵U第m行第j列的元素, m为特征向量的个数。分别对金字塔每个层级的每个区域的特征使用最大值池化函数, 将所有的层级池化后的向量组合, 完成高层局部特征的表示。在局部直方图基础上引入空间金字塔的优点在于, 空间金字塔采用最大值池化函数对局部噪声有较强鲁棒性, 同时将图像特征向量在池化运算时用空间金字塔的形象表示, 可对图像的特征赋予位置信息, 使分类更加精确。随着划分的层级增加, 区域的密集性也会增加, 相应的向量维数将越来越大, 导致运算复杂度增加。实验结果表明, 当L≥3时计算复杂度较大且准确率并没有明显提升。因此, 本文使用L=2的空间金字塔模型, 即图像被划分为3个层级 (l=0, 1, 2) , 将3个层级池化后的特征向量拼接作为高层特征的特征向量表示f<sub>2</sub>。</p>
                </div>
                <div class="p1">
                    <p id="133">为引入多尺度策略, 在PCA卷积层对第1层输出进行二值化加权和空间金字塔池化操作提取输入样本底层特征f<sub>1</sub>, 将底层特征f<sub>1</sub>与高层特征f<sub>2</sub>级联, 使MS-PCANet-SPP既能提取到高层语义信息又能获得底层细节信息, 从而获得输入样本的多尺度特征表示。</p>
                </div>
                <div class="p1">
                    <p id="134">2.2算法描述</p>
                </div>
                <div class="p1">
                    <p id="135">M S-PCANet-SPP算法执行步骤如下:</p>
                </div>
                <div class="p1">
                    <p id="136">输入N张大小为m×n的行人图片样本集<image id="209" type="formula" href="images/JSJC201902045_20900.jpg" display="inline" placement="inline"><alt></alt></image></p>
                </div>
                <div class="p1">
                    <p id="138">输出特征向量f<sub>i</sub></p>
                </div>
                <div class="p1">
                    <p id="139">步骤1对样本集进行去均值化级联得到<image id="210" type="formula" href="images/JSJC201902045_21000.jpg" display="inline" placement="inline"><alt></alt></image><image id="210" type="formula" href="images/JSJC201902045_21001.jpg" display="inline" placement="inline"><alt></alt></image></p>
                </div>
                <div class="p1">
                    <p id="141">步骤2根据式 (9) 计算PCA卷积层第1层的卷积核W<sup>1</sup><sub>l1</sub>, 将卷积核与输入样本I<sub>i</sub>进行卷积, 得到第i个样本在第1层的L<sub>1</sub>个输出:</p>
                </div>
                <div class="area_img" id="142">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJC201902045_14200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="211">步骤3对输出O<sup>1</sup><sub>i</sub>进行二值化加权处理得到一幅“整型图”T<sup>1</sup><sub>i</sub>, 将T<sup>1</sup><sub>i</sub>分为B块, 提取每块直方图特征<image id="213" type="formula" href="images/JSJC201902045_21300.jpg" display="inline" placement="inline"><alt></alt></image>, u<sub>i</sub>表示每块直方图特征, 其维数为2<sup>L1</sup>。如2.1节所述, 将空间金字塔设为3层, 根据式 (13) 分层池化局部直方图得到池化后每层特征<image id="214" type="formula" href="images/JSJC201902045_21400.jpg" display="inline" placement="inline"><alt></alt></image><image id="214" type="formula" href="images/JSJC201902045_21401.jpg" display="inline" placement="inline"><alt></alt></image>, 级联3层特征得到输入样本I<sub>i</sub>的底层特征向量表示<image id="215" type="formula" href="images/JSJC201902045_21500.jpg" display="inline" placement="inline"><alt></alt></image></p>
                </div>
                <div class="p1">
                    <p id="212">步骤4对步骤2中输出O<sup>1</sup><sub>i</sub>最小化重构误差得到PCA卷积层第2层的卷积核<image id="216" type="formula" href="images/JSJC201902045_21600.jpg" display="inline" placement="inline"><alt></alt></image>, 将<image id="217" type="formula" href="images/JSJC201902045_21700.jpg" display="inline" placement="inline"><alt></alt></image>与O<sup>1</sup><sub>i</sub>卷积得到对应每个O<sup>1</sup><sub>i</sub> (l<sub>1</sub>) 的L<sub>2</sub>个输出O<sup>2</sup><sub>i</sub> (l<sub>1</sub>) 为:</p>
                </div>
                <div class="area_img" id="148">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJC201902045_14800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="149">步骤5根据式 (3) , 对每个O<sub>i</sub><sup>1</sup> (l<sub>1</sub>) 的L<sub>2</sub>个输出O<sub>i</sub><sup>2</sup> (l<sub>1</sub>) 进行计算得到一幅“整型图”, 则对于第2层L<sub>1</sub>个输入O<sub>i</sub><sup>1</sup>将得到L<sub>1</sub>幅“整型图”:</p>
                </div>
                <div class="area_img" id="150">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJC201902045_15000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="218">类似于步骤3特征池化过程, 对每幅“整型图”T<sup>2</sup><sub>i</sub>进行局部直方图, 空间金字塔池化得到输入样本I<sub>i</sub>的高层特征向量表示<image id="219" type="formula" href="images/JSJC201902045_21900.jpg" display="inline" placement="inline"><alt></alt></image></p>
                </div>
                <div class="p1">
                    <p id="153">步骤6级联底层特征f<sub>i, 1</sub>和高层特征f<sub>i, 2</sub>得到输入样本I<sub>i</sub>的多尺度特征表示f<sub>i</sub>=[f<sub>i, 1</sub>;f<sub>i, 2</sub>]。</p>
                </div>
                <div class="p1">
                    <p id="154">将特征向量f<sub>i</sub>输入SVM完成行人和非行人的判定。SVM分类函数为:</p>
                </div>
                <div class="area_img" id="155">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJC201902045_15500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="156">其中, x为SVM的输入, 即特征向量f<sub>i</sub>, θ为分类的SVM的权值矩阵。</p>
                </div>
                <h3 id="157" name="157" class="anchor-tag">3 实验与结果分析</h3>
                <div class="p1">
                    <p id="158">实验计算机配置为CPU 2.66 GHz, 内存24 GB, 采用Matlab R2014a平台, 使用Linlinear<citation id="242" type="reference"><link href="71" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>作为SVM分类器开发包, 选取目前使用最多的行人检测数据库INRIA<citation id="243" type="reference"><link href="45" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>和规模最大的静态行人数据库NICTA<citation id="244" type="reference"><link href="75" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>进行行人检测。在INRIA数据库中选择2 416张行人图片作为训练集正样本, 1 126张行人图片作为测试集正样本, 归一化大小为64×32;从1 218张高分辨率非行人图片中随机截取3 654张大小为64×32的图片作为训练集负样本, 从453张高分辨率非行人图片中随机截取1 812张大小为64×32的图像作为测试集负样本。NICTA数据库包含25 551张行人样本, 随机选取6 000张作为训练集正样本, 并且从剩余行人样本中选取6 000张作为测试集正样本, 归一化大小为80×32;从5 207张高分辨率非行人图片随机截取6 000张大小为80×32的图片作为训练集负样本, 6 000张大小为80×32的图片作为测试集样本。对图片中单行人样本进行检测时, 将除主要行人区域之外的其余行人视为背景考虑, 部分实验结果如图4所示。</p>
                </div>
                <div class="area_img" id="159">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201902045_15900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 单行人样本检测结果" src="Detail/GetImg?filename=images/JSJC201902045_15900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 单行人样本检测结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201902045_15900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="160" name="160">3.1 网络参数选取</h4>
                <div class="p1">
                    <p id="161">MS-PCANet-SPP的参数包括卷积核、局部直方图池化大小和空间金字塔池化层数等, 其中卷积核的选取对于PCANet及MS-PCANet-SPP性能有重要影响。卷积核的值采用1.4节中PCA预训练方式获得;卷积核数目采用文献<citation id="245" type="reference">[<a class="sup">8</a>]</citation>的结果, 即固定每一层卷积核数目L<sub>1</sub>=L<sub>2</sub>=8;卷积核的大小, 通过在INRIA和NICTA数据库上的实验结果评估其最佳值。卷积核的大小对正确检测率的影响如图5所示。</p>
                </div>
                <div class="area_img" id="162">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201902045_16200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 不同卷积核尺寸对应的网络正确检测率" src="Detail/GetImg?filename=images/JSJC201902045_16200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 不同卷积核尺寸对应的网络正确检测率  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201902045_16200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="163">由图5可知, 当卷积核大小为5×5时, 网络对INRIA和NICTA数据库有最佳的正确检测率。因此, 设置MS-PCANet-SPP的卷积核尺寸为5×5, 固定每一层卷积核数目为L<sub>1</sub>=L<sub>2</sub>=8。并且通过多组实验尝试, 兼顾检测时间及检测正确率, 本文将INRIA数据库局部直方图池化大小设为16×8, 池化方式设为非重叠池化, NICTA数据库局部直方图池化大小设为20×8, 池化方式设为非重叠池化。空间金字塔池化参数按文献<citation id="246" type="reference">[<a class="sup">12</a>]</citation>进行相同设置, 将池化层数设为3层, 从顶层到底层设置池化子区域为1×1, 2×2和4×4, 其中空间金字塔池化函数为最大值池化。</p>
                </div>
                <h4 class="anchor-tag" id="164" name="164">3.2 实验对比算法</h4>
                <div class="p1">
                    <p id="165">为了验证MS-PCANet-SPP的有效性, 将下列算法与MS-PCANet-SPP进行对比:</p>
                </div>
                <div class="p1">
                    <p id="166">1) HOG:HOG算法<citation id="247" type="reference"><link href="45" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>作为传统特征提取算法, 被公认为是最具有泛化能力的特征提取方法之一。</p>
                </div>
                <div class="p1">
                    <p id="167">2) DL-CLDRC:DL-CLDRC算法<citation id="248" type="reference"><link href="77" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>将深度学习运用于LDRC算法, 在人脸识别中取得较好的识别效果。</p>
                </div>
                <div class="p1">
                    <p id="168">3) CNN-2:本文设计2层的CNN<citation id="249" type="reference"><link href="79" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>对行人数据库进行检测, 其卷积核的大小为5×5, 第1层卷积核数目为10, 第2层卷积核数目为15, 选择Sigmoid激活函数作为非线性处理函数, 池化层池化方式选择均值池化, 将通过全连接层得到的特征向量用Softmax分类器进行分类输出。CNN-2采用批量训练方式, 设学习率0.1, 将数据集中每600张样本设为1批, 对每批样本进行200次迭代。</p>
                </div>
                <div class="p1">
                    <p id="169">4) PCANet:PCANet<citation id="250" type="reference"><link href="57" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>卷积核尺寸设为5×5, 每一层卷积核数目设为L<sub>1</sub>=L<sub>2</sub>=8。</p>
                </div>
                <div class="p1">
                    <p id="170">5) PCANet-1:在PCANet基础上提出仅使用一层PCA卷积层的PCANet作为训练样本的网络, 其卷积核尺寸设为5×5, 卷积核数目设为L<sub>1</sub>=8。</p>
                </div>
                <div class="p1">
                    <p id="171">6) PCANet-SPP1:在PCANet-1基础上增加空间金字塔池化, 其空间金字塔池化层数设为3层, 从顶层到底层设池化区域为1×1、2×2和4×4, 空间金字塔池化方式为最大值池化。</p>
                </div>
                <h4 class="anchor-tag" id="172" name="172">3.3 实验结果</h4>
                <h4 class="anchor-tag" id="173" name="173">3.3.1 在INRIA和NICTA数据库中实验结果</h4>
                <div class="p1">
                    <p id="174">上述算法在INRIA和NICTA数据库中进行行人检测, 其性能对比结果见表1和表2。其中, 正确检测率表示行人和非行人样本被正确检测出的百分比, 漏检率表示所有的行人样本中未被检测出行人的百分比, 误检率表示所有的非行人样本未被检测为非行人的百分比, 平均检测速度本文用每秒采样率表示, 即每秒能够检测的样本数量。</p>
                </div>
                <div class="area_img" id="175">
                                            <p class="img_tit">
                                                表1 各算法在INRIA数据库中性能对比结果
                                                
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201902045_17500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/JSJC201902045_17500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201902045_17500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表1 各算法在INRIA数据库中性能对比结果" src="Detail/GetImg?filename=images/JSJC201902045_17500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>
                <div class="area_img" id="176">
                                            <p class="img_tit">
                                                表2 各算法在NICTA数据库中性能对比结果
                                                
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201902045_17600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/JSJC201902045_17600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201902045_17600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表2 各算法在NICTA数据库中性能对比结果" src="Detail/GetImg?filename=images/JSJC201902045_17600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>
                <div class="p1">
                    <p id="177">根据表1和表2, 得出以下结论:</p>
                </div>
                <div class="p1">
                    <p id="178">1) HOG在2个数据库中均取得较高的正确检测率, 其正确检测率分别为97.07%和98.23%。</p>
                </div>
                <div class="p1">
                    <p id="179">2) CNN在所有算法中正确检测率最低, 仅为95.47%和97.24%。这是因为CNN使用基于梯度下降的学习方法进行监督训练, 导致在网络层数较少时对样本的泛化能力较差。</p>
                </div>
                <div class="p1">
                    <p id="180">3) PCANet的正确检测率要高于PCANet-1, 验证了PCANet作为一种深度特征提取网络, 其网络层数越多特征提取能力越强。</p>
                </div>
                <div class="p1">
                    <p id="181">4) PCANet-SPP1的正确检测率高于PCANet-1, 验证了本文提出的空间金字塔池化的有效性。</p>
                </div>
                <div class="p1">
                    <p id="182">5) M S-PCANet-SPP在2个数据库中的正确检测率最高, 并且漏检率和误检率也最低, 验证了本文算法的有效性。</p>
                </div>
                <div class="p1">
                    <p id="183">6) 由于M S-PCANet-SPP相比于其他算法进行了更多特征提取和分层池化, 导致网络计算复杂度增加, 因此MS-PCANet-SPP对于样本的平均检测速度最低。但是从行人检测的安全性考虑, 检测率重要性要优于检测速度, 并且由于计算机性能的提高, M S-PCANet-SPP基本可实现实时行人检测。</p>
                </div>
                <div class="p1">
                    <p id="184">为验证MS-PCANet-SPP的泛化性能和鲁棒性指标, 本文在INRIA数据库中随机选取6组样本作为训练集, 其中正样本与负样本比例设为1∶1, 样本数分别为1 000, 2 000, …, 6 000, 测试集数目固定不变。对每组样本进行训练并在测试集进行检测, 实验结果如图6 (a) 所示。同样, 图6 (b) 为对NICTA数据库的检测结果, 其训练集的样本数为3 000, 4 500, …, 12 000。</p>
                </div>
                <div class="area_img" id="185">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201902045_18500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 不同训练样本数目下各算法检测结果" src="Detail/GetImg?filename=images/JSJC201902045_18500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 不同训练样本数目下各算法检测结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201902045_18500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="186">由图6可得, 在INRIA和NICTA数据库中随着训练样本数量增加, MS-PCANet-SPP对测试集的正确检测率也相应提高, 算法的性能仍保持稳定, 验证了MS-PCANet-SPP具有较好的泛化性能和鲁棒性。</p>
                </div>
                <h4 class="anchor-tag" id="187" name="187">3.3.2 在自建数据集中实验结果</h4>
                <div class="p1">
                    <p id="188">为验证MS-PCANet-SPP在非理想的自然条件下行人检测的鲁棒性, 从INRIA、NICTA和TUD<citation id="251" type="reference"><link href="81" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>数据库中的测试集选取负责背景、部分遮挡、低分辨率低和远距离等的行人样本作为测试集, 建立专门针对非理想的自然条件下的行人测试数据集, 其中测试数据集正负样本数均为3 000张, 样本尺寸归一化为64×32, 部分测试样本如图7所示。</p>
                </div>
                <div class="area_img" id="189">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201902045_18900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 部分自建数据集样本" src="Detail/GetImg?filename=images/JSJC201902045_18900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 部分自建数据集样本  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201902045_18900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="190">表3为各算法在自建数据库中的性能比较结果。图8为各算法在NICTA、INRIA和自建数据库 (横坐标Our) 中的检测结果。3个数据库的训练集和测试集, 正负样本的比例均相同。由表3可知, M S-PCANet-SPP在复杂背景、部分遮挡、低分辨率等自然条件下正确检测率相比HOG、CNN、DL-CLDRC和PCANet等算法明显提高, 并且漏检率和误检率也更低。由图8可得, MS-PCANet-SPP在自建数据库中相比其他算法具有更低的下降幅度, 验证了MS-PCANet-SPP对非理想自然条件下的行人检测具有一定的鲁棒性。</p>
                </div>
                <div class="area_img" id="220">
                                            <p class="img_tit">
                                                表3 各算法在自建数据集中的性能对比结果
                                                
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201902045_22000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/JSJC201902045_22000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                <p class="img_note">%</p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201902045_22000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表3 各算法在自建数据集中的性能对比结果" src="Detail/GetImg?filename=images/JSJC201902045_22000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>
                <div class="area_img" id="195">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201902045_19500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 不同数据库中算法检测结果" src="Detail/GetImg?filename=images/JSJC201902045_19500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图8 不同数据库中算法检测结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201902045_19500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h3 id="196" name="196" class="anchor-tag">4 结束语</h3>
                <div class="p1">
                    <p id="197">行人检测需要解决在非理想条件下的准确性和鲁棒性的问题, 本文根据空间金字塔池化对形变目标具有较强鲁棒性, 同时受多尺度CNN级联不同尺度特征以使得网络既能提取到高层语义信息又能获得底层细节信息的启发, 提出一种面向行人检测的深度特征提取网络MS-PCANet-SPP。通过在原始池化层引入空间金字塔策略, 利用空间金字塔最大值池化方法对原始具有冗余的特征进行提取, 精简出其中重要的、区分性强的特征, 同时将不同尺度的经空间金字塔池化的特征向量级联构成以获得图像的多尺度特征描述, 使得MS-PCANet-SPP既能提取到高层语义信息又能获得底层细节信息。在INRIA和NICTA数据库中的对比实验结果表明, M S-PCANetSPP能有效提高行人检测的准确率, 正确检测率高于HOG、CNN和PCANet等算法, 在漏检率和误检率上也低于其他算法, 并且通过进一步实验验证本文提出的MS-PCANet-SPP是一个有效的深度特征提取网络, 对于复杂背景、拍摄角度以及人体姿态多样等非理想条件下的行人检测具有较强鲁棒性。同时, 由于计算机性能的提高, MS-PCANet-SPP基本能实现实时检测。下一步将在MS-PCANet-SPP的基础上对行人检测的显著性检测算法进行研究, 以提高算法的检测速度。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="43">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZXU201204030&amp;v=MjQzMzMzenFxQnRHRnJDVVJMT2VaZVJuRnlqa1ZicklJVGZUZTdHNEg5UE1xNDlHWklRS0RIODR2UjRUNmo1NE8=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b>苏志松, 李绍滋, 陈淑媛, 等.行人检测技术综述[J].电子学报, 2012, 40 (4) :814-820.
                            </a>
                        </p>
                        <p id="45">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Histograms of Oriented Gradients for Human Detection">

                                <b>[2]</b>DALAL N, TRIGGS B.Histograms of oriented gradients for human detection[C]//Proceedings of IEEEComputer Society Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEEPress, 2005:886-893.
                            </a>
                        </p>
                        <p id="47">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Discriminative local binary patternsfor pedestrian detection in personal album">

                                <b>[3]</b>MU Y, YAN S, LIU Y, et al.Discriminative local binary binary pattems for pedestrian detection i personal album[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2008:1-8.
                            </a>
                        </p>
                        <p id="49">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Pedestrian detection via classification on Riemannian manifolds">

                                <b>[4]</b>TUZEL O, PORIKLI F, MEER P.Pedestrian detection via classification on Riemannian manifolds[J].IEEETransactions on Pattem Analysis and Machine Intelligence, 2008, 30 (10) :1713-1727.
                            </a>
                        </p>
                        <p id="51">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002830860&amp;v=MjE5OTF4Y01IN1I3cWVidWR0RkNIbFU3N09KVjQ9Tmo3QmFyTzRIdEhPcDR4RmJPMFBZM2s1ekJkaDRqOTlTWHFScnhv&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b>VIOLA P, JONES M.Robust real-time face detection[J].International Journal of Computer Vision, 2004, 57 (2) :137-154.
                            </a>
                        </p>
                        <p id="53">
                            <a id="bibliography_6" >
                                    <b>[6]</b>
                                HINTON G, SALAKHUTDINOV R.Reducing the dimensionality of data with neural networks[J].Science, 2006, 313 (5786) :304-304.
                            </a>
                        </p>
                        <p id="55">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201309002&amp;v=MTQ5ODJDVVJMT2VaZVJuRnlqa1ZicklMeXZTZExHNEg5TE1wbzlGWm9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnI=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b>余凯, 贾磊, 陈雨强, 等.深度学习的昨天、今天和明天[J].计算机研究与发展, 2013, 50 (9) :1799-1804.
                            </a>
                        </p>
                        <p id="57">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=PCANet:A simple deep learning baseline for image classification?">

                                <b>[8]</b>CHAN T H, JIA K, GAO S, et al.PCANet:a simple deep learning baseline for image classification?[J].IEEE Transactions on Image Processing, 2015, 24 (12) :5017-5032.
                            </a>
                        </p>
                        <p id="59">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Automatic Change Detection in Synthetic Aperture Radar Imag-es Based on PCANet">

                                <b>[9]</b>GAO F, DONG J, LI B, et al.Automatic change detection in synthetic aperture radar images based on PCANet[J].IEEE Geoscience and Remote Sensing Letters, 2016, 13 (12) :1792-1796.
                            </a>
                        </p>
                        <p id="61">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Traffic lights recognition based on PCANet">

                                <b>[10]</b>WANG Z, BI Z, WANG C, et al.Traffic lights recognition based on PCANet[C]//Proceedings of Chinese Automation Congress.Washington D.C., USA:IEEE Press, 2016:559-564.
                            </a>
                        </p>
                        <p id="63">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Human fall detection in surveillance video based on PCANet">

                                <b>[11]</b>WANG S, CHEN L, ZHOU Z, et al.Human fall detection in surveillance video based on PCANet[J].Multimedia Tools and Applications, 2016, 75 (19) :11603-11613.
                            </a>
                        </p>
                        <p id="65">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Linear spatial pyramid matching using sparse coding for image classification">

                                <b>[12]</b>YANG J, YU K, GONG Y, et al.Linear spatial pyramid matching using sparse coding for image classification[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2009:1794-1801.
                            </a>
                        </p>
                        <p id="67">
                            <a id="bibliography_13" >
                                    <b>[13]</b>
                                SERMANET P, LECUN Y.Traffic sign recognition with multi-scale convolutional networks[C]//Proceedings of International Joint Conference on Neural Networks.Washington D.C., USA:IEEE Press, 2011:2809-2813.
                            </a>
                        </p>
                        <p id="69">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Regularized max pooling for image categorization">

                                <b>[14]</b>HOAI M.Regularized max pooling for image categorization[J].Journal of the British Institution of Radio Engineers, 2014, 14 (3) :94-100.
                            </a>
                        </p>
                        <p id="71">
                            <a id="bibliography_15" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZXU201608028&amp;v=MTY4NTFuRnlqa1ZicklJVGZUZTdHNEg5Zk1wNDlIYklRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVI=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[15]</b>顾凌云, 吕志文, 杨勇, 等.基于PCANet和SVM的谎言测试研究[J].电子学报, 2016, 44 (8) :1969-1973.
                            </a>
                        </p>
                        <p id="73">
                            <a id="bibliography_16" >
                                    <b>[16]</b>
                                CHANG C C, LIN C J.LIBSVM:A library for support vector machines[J].ACM Transactions on Intelligent Systems and Technology, 2011, 2 (3) :1-27.
                            </a>
                        </p>
                        <p id="75">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Boosting with multiple classifier families">

                                <b>[17]</b>OVERETT G, PETERSSON L.Boosting with multiple classifier families[C]//Proceedings of IEEE Intelligent Vehicles Symposium.Washington D.C., USA:IEEEPress, 2007:1039-1044.
                            </a>
                        </p>
                        <p id="77">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Effective face recognition using deep learning based linear discriminant classification">

                                <b>[18]</b>SHAILAJA K, ANURADHA B.Effective face recognition using deep learning based linear discriminant classification[C]//Proceedings of IEEE International Conference on Computational Intelligence and Computing Research.Washington D.C., USA:IEEE Press, 2016:1-6.
                            </a>
                        </p>
                        <p id="79">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=ImageNet classifcation with deep convolutional neural networks">

                                <b>[19]</b>KRIZHEVSKY A, SUTSKEVER I, HINTON G.ImageNet classification with deep convolutional neural networks[C]//Proceedings of NIPS’12.Nevada, USA:Curran Associates InC., 2012:1097-1105.
                            </a>
                        </p>
                        <p id="81">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multi-cue onboard pedestrian detection">

                                <b>[20]</b>WOJEK C, WALK S, SCHIELE B.Multi-cue onboard pedestrian detection[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2009:794-801.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJC201902045" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201902045&amp;v=MDYyODVMT2VaZVJuRnlqa1ZicklMejdCYmJHNEg5ak1yWTlCWVlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
