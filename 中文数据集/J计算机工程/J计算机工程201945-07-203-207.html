<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637130772485118750%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJC201907032%26RESULT%3d1%26SIGN%3dYCXMtT7rMs%252bQ1Ja7UiHWdbjFk9g%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201907032&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201907032&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201907032&amp;v=MjU0OThyS0x6N0JiYkc0SDlqTXFJOUdab1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm9GQ2prVXI=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRnRMbEw2TkJZUHBuNHB3SXVIRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#41" data-title="0 概述 ">0 概述</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#47" data-title="1 基于FPN的人群计数 ">1 基于FPN的人群计数</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#48" data-title="1.1 密度图">1.1 密度图</a></li>
                                                <li><a href="#59" data-title="1.2 网络结构">1.2 网络结构</a></li>
                                                <li><a href="#67" data-title="1.3 网络结构优化">1.3 网络结构优化</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#71" data-title="2 实验结果与分析 ">2 实验结果与分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#73" data-title="2.1 评价标准">2.1 评价标准</a></li>
                                                <li><a href="#76" data-title="2.2 ShanghaiTech数据集">2.2 ShanghaiTech数据集</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#91" data-title="3 结束语 ">3 结束语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#64" data-title="&lt;b&gt;图1 全卷积神经网络结构&lt;/b&gt;"><b>图1 全卷积神经网络结构</b></a></li>
                                                <li><a href="#66" data-title="&lt;b&gt;图2 特征金字塔结构&lt;/b&gt;"><b>图2 特征金字塔结构</b></a></li>
                                                <li><a href="#83" data-title="&lt;b&gt;表1 多尺度特征图融合前后算法性能对比&lt;/b&gt;"><b>表1 多尺度特征图融合前后算法性能对比</b></a></li>
                                                <li><a href="#85" data-title="&lt;b&gt;表2 ShanghaiTech上各算法性能对比&lt;/b&gt;"><b>表2 ShanghaiTech上各算法性能对比</b></a></li>
                                                <li><a href="#87" data-title="&lt;b&gt;图3 两组测试图片的真实密度图和估计密度图对比&lt;/b&gt;"><b>图3 两组测试图片的真实密度图和估计密度图对比</b></a></li>
                                                <li><a href="#89" data-title="&lt;b&gt;图4 Part_A图片真实人数和估计人数对比&lt;/b&gt;"><b>图4 Part_A图片真实人数和估计人数对比</b></a></li>
                                                <li><a href="#90" data-title="&lt;b&gt;图5 Part_B图片真实人数和估计人数对比&lt;/b&gt;"><b>图5 Part_B图片真实人数和估计人数对比</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="111">


                                    <a id="bibliography_1" title=" LIN Shengfu, CHEN J Y, CHAO Huangxin.Estimation of number of people in crowded scenes using perspective transformation[J].IEEE Transactions on Systems, 2001, 31 (6) :645-654." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Estimation of number of people in crowded scenes using perspective transformation">
                                        <b>[1]</b>
                                         LIN Shengfu, CHEN J Y, CHAO Huangxin.Estimation of number of people in crowded scenes using perspective transformation[J].IEEE Transactions on Systems, 2001, 31 (6) :645-654.
                                    </a>
                                </li>
                                <li id="113">


                                    <a id="bibliography_2" title=" WU Bo, NEVATIA R.Detection of multiple, partially occluded humans in a single image by Bayesian combination of edgelet part detectors[C]//Proceedings of the 10th IEEE International Conference on Computer Vision.Washington D.C., USA:IEEE Press, 2005:90-97." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Detection of multiple,partially occluded humans in a single image by Bayesian combination of edgelet part detectors">
                                        <b>[2]</b>
                                         WU Bo, NEVATIA R.Detection of multiple, partially occluded humans in a single image by Bayesian combination of edgelet part detectors[C]//Proceedings of the 10th IEEE International Conference on Computer Vision.Washington D.C., USA:IEEE Press, 2005:90-97.
                                    </a>
                                </li>
                                <li id="115">


                                    <a id="bibliography_3" title=" LI Min, ZHANG Zhaoxiang, HUANG Kaiqi, et al.Estimating the number of people in crowded scenes by mid based foreground segmentation and head-shoulder detection[C]//Proceedings of the 19th International Conference on Pattern Recognition.Tampa, USA:IEEE Press, 2008:1-4." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Es-timating the Number of People in Crowded Scenesby MID Based Foreground Segmentation and Head-shoulder Detection">
                                        <b>[3]</b>
                                         LI Min, ZHANG Zhaoxiang, HUANG Kaiqi, et al.Estimating the number of people in crowded scenes by mid based foreground segmentation and head-shoulder detection[C]//Proceedings of the 19th International Conference on Pattern Recognition.Tampa, USA:IEEE Press, 2008:1-4.
                                    </a>
                                </li>
                                <li id="117">


                                    <a id="bibliography_4" title=" ZHAO Tao, NEVATIA R, WU Bo.Segmentation and tracking of multiple humans in crowded environments[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2008, 30 (7) :1198-1211." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Segmentation and tracking of multiple humans in crowded environments">
                                        <b>[4]</b>
                                         ZHAO Tao, NEVATIA R, WU Bo.Segmentation and tracking of multiple humans in crowded environments[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2008, 30 (7) :1198-1211.
                                    </a>
                                </li>
                                <li id="119">


                                    <a id="bibliography_5" title=" GE W, COLLINS R T.Marked point processes for crowd counting[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.San Francisco, USA:IEEE Press, 2009:2913-2920." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Marked point processes for crowdcounting">
                                        <b>[5]</b>
                                         GE W, COLLINS R T.Marked point processes for crowd counting[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.San Francisco, USA:IEEE Press, 2009:2913-2920.
                                    </a>
                                </li>
                                <li id="121">


                                    <a id="bibliography_6" title=" WANG Meng, WANG Xiaogang.Automatic adaptation of a generic pedestrian detector to a specific traffic scene[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Colorado Springs, USA:IEEE Press, 2011:3401-3408." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Automatic adaptation of a generic pedestrian detector to a specific traffic scene">
                                        <b>[6]</b>
                                         WANG Meng, WANG Xiaogang.Automatic adaptation of a generic pedestrian detector to a specific traffic scene[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Colorado Springs, USA:IEEE Press, 2011:3401-3408.
                                    </a>
                                </li>
                                <li id="123">


                                    <a id="bibliography_7" title=" 李云波, 唐斯琪, 周星宇, 等.可伸缩模块化CNN人群计数方法[J].计算机科学, 2018, 45 (8) :17-21, 40." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJA201808006&amp;v=MDA3ODI3QmI3RzRIOW5NcDQ5RllvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSb0ZDamtVcnJLTHo=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRnRMbEw2TkJZUHBuNHB3SXVIRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                         李云波, 唐斯琪, 周星宇, 等.可伸缩模块化CNN人群计数方法[J].计算机科学, 2018, 45 (8) :17-21, 40.
                                    </a>
                                </li>
                                <li id="125">


                                    <a id="bibliography_8" title=" CHAN A B, LIANG Z S J, Vasconcelos N.Privacy preserving crowd monitoring:counting people without people models or tracking[C]//Proceedings of the 10th IEEE International Conference on Computer Vision and Pattern Recognition.Anchorage, USA:IEEE Press, 2008:1-7." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Privacy preserving crowd monitoring: Counting people without people models or tracking">
                                        <b>[8]</b>
                                         CHAN A B, LIANG Z S J, Vasconcelos N.Privacy preserving crowd monitoring:counting people without people models or tracking[C]//Proceedings of the 10th IEEE International Conference on Computer Vision and Pattern Recognition.Anchorage, USA:IEEE Press, 2008:1-7.
                                    </a>
                                </li>
                                <li id="127">


                                    <a id="bibliography_9" title=" CHEN K, LOY C C, GONG S, et al.Feature mining for localised crowd counting[C]//Proceedings of BMVC’12.London, UK:[s.n.], 2012:3." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Feature mining for localised crowd counting">
                                        <b>[9]</b>
                                         CHEN K, LOY C C, GONG S, et al.Feature mining for localised crowd counting[C]//Proceedings of BMVC’12.London, UK:[s.n.], 2012:3.
                                    </a>
                                </li>
                                <li id="129">


                                    <a id="bibliography_10" title=" LEMPITSKY V, ZISSERMAN A.Learning to count objects in images[C]//Proceedings of Advances in Neural Information Processing Systems.Vancouver, Canada:[s.n.], 2010:1324-1332." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning to count objects in images">
                                        <b>[10]</b>
                                         LEMPITSKY V, ZISSERMAN A.Learning to count objects in images[C]//Proceedings of Advances in Neural Information Processing Systems.Vancouver, Canada:[s.n.], 2010:1324-1332.
                                    </a>
                                </li>
                                <li id="131">


                                    <a id="bibliography_11" title=" LONG J, SHELHAMER E, DARRELL T.Fully convolutional networks for semantic segmentation[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Boston, USA:IEEE Press, 2015:3431-3440." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fully convolutional networks for semantic segmentation">
                                        <b>[11]</b>
                                         LONG J, SHELHAMER E, DARRELL T.Fully convolutional networks for semantic segmentation[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Boston, USA:IEEE Press, 2015:3431-3440.
                                    </a>
                                </li>
                                <li id="133">


                                    <a id="bibliography_12" title=" MAGGIORI E, TARABALKA Y, CHARPIAT G, et al.Convolutional neural networks for large-scale remote-sensing image classification[J].IEEE Transactions on Geoscience and Remote Sensing, 2017, 55 (2) :645-657." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Convolutional Neural Networks for Large-Scale Remote-Sensing Image Classification">
                                        <b>[12]</b>
                                         MAGGIORI E, TARABALKA Y, CHARPIAT G, et al.Convolutional neural networks for large-scale remote-sensing image classification[J].IEEE Transactions on Geoscience and Remote Sensing, 2017, 55 (2) :645-657.
                                    </a>
                                </li>
                                <li id="135">


                                    <a id="bibliography_13" title=" ZHANG Cong, LI Hongsheng, WANG Xiaogang, et al.Cross-scene crowd counting via deep convolutional neural networks[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Boston, USA:IEEE Press, 2015:833-841." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Cross-scene crowd counting via deep convolutional neural networks">
                                        <b>[13]</b>
                                         ZHANG Cong, LI Hongsheng, WANG Xiaogang, et al.Cross-scene crowd counting via deep convolutional neural networks[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Boston, USA:IEEE Press, 2015:833-841.
                                    </a>
                                </li>
                                <li id="137">


                                    <a id="bibliography_14" title=" ZHANG Yingying, ZHOU Desan, CHEN Siqin, et al.Single-image crowd counting via multi-column convolutional neural network[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Las Vegas, USA:IEEE Press, 2016:589-597." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=C.:Single-image crowd counting via multi-column convolutional neural network">
                                        <b>[14]</b>
                                         ZHANG Yingying, ZHOU Desan, CHEN Siqin, et al.Single-image crowd counting via multi-column convolutional neural network[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Las Vegas, USA:IEEE Press, 2016:589-597.
                                    </a>
                                </li>
                                <li id="139">


                                    <a id="bibliography_15" title=" 吴淑窈, 刘希庚, 胡昌振, 等.基于卷积神经网络人群计数的研究与实现[J].科教导刊, 2017 (9) :16-17." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=KJDS201709009&amp;v=MDY5ODM0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSb0ZDamtVcnJLTGlmUGZiRzRIOWJNcG85RmJZUUtESDg=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRnRMbEw2TkJZUHBuNHB3SXVIRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[15]</b>
                                         吴淑窈, 刘希庚, 胡昌振, 等.基于卷积神经网络人群计数的研究与实现[J].科教导刊, 2017 (9) :16-17.
                                    </a>
                                </li>
                                <li id="141">


                                    <a id="bibliography_16" title=" 唐斯琪, 陶蔚, 张梁梁, 等.一种多列特征图融合的深度人群计数算法[J].郑州大学学报 (理学版) , 2018, 50 (2) :1-6." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZZDZ201802012&amp;v=MDc0MzRqa1VycktQemZQZExHNEg5bk1yWTlFWm9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJvRkM=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRnRMbEw2TkJZUHBuNHB3SXVIRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[16]</b>
                                         唐斯琪, 陶蔚, 张梁梁, 等.一种多列特征图融合的深度人群计数算法[J].郑州大学学报 (理学版) , 2018, 50 (2) :1-6.
                                    </a>
                                </li>
                                <li id="143">


                                    <a id="bibliography_17" title=" MARSDEN M, McGUINESS K, LITTLE S, et al.Fully convolutional crowd counting on highly congested scenes[EB/OL].[2018-05-20].https://www.researchgate.net/publication." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fully convolutional crowd counting on highly congested scenes">
                                        <b>[17]</b>
                                         MARSDEN M, McGUINESS K, LITTLE S, et al.Fully convolutional crowd counting on highly congested scenes[EB/OL].[2018-05-20].https://www.researchgate.net/publication.
                                    </a>
                                </li>
                                <li id="145">


                                    <a id="bibliography_18" title=" SINDAGI V A, PATEL V M.Cnn-based cascaded multi-task learning of high-level prior and density estimation for crowd counting[C]//Proceedings of IEEE International Conference on Advanced Video and Signal Based Surveillance.Lecce, Italy:IEEE Press, 2017:1-6." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=CNN-Based cascaded multi-task learning of high-level prior and density estimation for crowd counting">
                                        <b>[18]</b>
                                         SINDAGI V A, PATEL V M.Cnn-based cascaded multi-task learning of high-level prior and density estimation for crowd counting[C]//Proceedings of IEEE International Conference on Advanced Video and Signal Based Surveillance.Lecce, Italy:IEEE Press, 2017:1-6.
                                    </a>
                                </li>
                                <li id="147">


                                    <a id="bibliography_19" title=" LIN T Y, DOLLAR P, GIRSHICK R, et al.Feature pyramid networks for object detection[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Honolulu, USA:[s.n.], 2017:4-7." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Feature pyramid networks for object detection">
                                        <b>[19]</b>
                                         LIN T Y, DOLLAR P, GIRSHICK R, et al.Feature pyramid networks for object detection[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Honolulu, USA:[s.n.], 2017:4-7.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJC" target="_blank">计算机工程</a>
                2019,45(07),203-207 DOI:10.19678/j.issn.1000-3428.0050951            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于特征金字塔网络的人群计数算法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%A9%AC%E7%9A%93&amp;code=39301046&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRnRMbEw2TkJZUHBuNHB3SXVIRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">马皓</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%AE%B7%E4%BF%9D%E7%BE%A4&amp;code=09581513&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRnRMbEw2TkJZUHBuNHB3SXVIRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">殷保群</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BD%AD%E6%80%9D%E5%87%A1&amp;code=39301047&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRnRMbEw2TkJZUHBuNHB3SXVIRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">彭思凡</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E7%A7%91%E5%AD%A6%E6%8A%80%E6%9C%AF%E5%A4%A7%E5%AD%A6%E8%87%AA%E5%8A%A8%E5%8C%96%E7%B3%BB&amp;code=0002522&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRnRMbEw2TkJZUHBuNHB3SXVIRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国科学技术大学自动化系</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>由于单张图片人群计数存在严重的人群遮挡和尺度变化问题, 导致人群计数算法性能明显下降。为此, 提出一种基于特征金字塔网络对图片进行人群计数的算法, 并给出能够处理任意图片分辨率的全卷积网络。将特征金字塔网络应用到人群计数中, 通过逐层融合网络中不同尺度的特征图来解决图片中的上述问题。在人群计数数据库ShanghaiTech上对网络模型进行训练和性能评测, 结果表明, 与当前主流的人群计数算法相比, 该算法具有更高的鲁棒性和准确性。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%BA%BA%E7%BE%A4%E8%AE%A1%E6%95%B0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRnRMbEw2TkJZUHBuNHB3SXVIRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">人群计数;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRnRMbEw2TkJZUHBuNHB3SXVIRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">卷积神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%89%B9%E5%BE%81%E9%87%91%E5%AD%97%E5%A1%94&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRnRMbEw2TkJZUHBuNHB3SXVIRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">特征金字塔;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%AF%86%E5%BA%A6%E5%9B%BE&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRnRMbEw2TkJZUHBuNHB3SXVIRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">密度图;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%B9%B3%E5%9D%87%E7%BB%9D%E5%AF%B9%E8%AF%AF%E5%B7%AE&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRnRMbEw2TkJZUHBuNHB3SXVIRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">平均绝对误差;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    马皓 (1993—) , 男, 硕士研究生, 主研方向为人群计数算法, E-mail: tmac01@ mail. ustc. edu. cn;;
                                </span>
                                <span>
                                    殷保群, 教授;;
                                </span>
                                <span>
                                    彭思凡, 硕士研究生。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-03-27</p>

                    <p>

                            <b>基金：</b>
                                                        <span>总装预研基金 (61403120201);</span>
                    </p>
            </div>
                    <h1><b>Crowd Counting Algorithm Based on Feature Pyramid Network</b></h1>
                    <h2>
                    <span>MA Hao</span>
                    <span>YIN Baoqun</span>
                    <span>PENG Sifan</span>
            </h2>
                    <h2>
                    <span>Department of Automation, University of Science and Technology of China</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>The single-picture crowd count has a sharp decline in performance due to severe population occlusion and scale changes.Therefore, this paper proposes an algorithm for crowd counting pictures, and gives a Full Convolution Network (FCN) capable of processing the resolution of any picture.The scale change and occlusion problems in the picture are solved by applying the feature pyramid network to the crowd count.The network model is trained and evaluated in the crowd counting database ShanghaiTech, results show that the algorithm has good robustness and accuracy compared with the current mainstream crowd counting algorithm.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=crowd%20counting&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRnRMbEw2TkJZUHBuNHB3SXVIRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">crowd counting;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Convolutional%20Neural%20Network%20(CNN)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRnRMbEw2TkJZUHBuNHB3SXVIRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Convolutional Neural Network (CNN) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=feature%20pyramid&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRnRMbEw2TkJZUHBuNHB3SXVIRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">feature pyramid;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=density%20map&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRnRMbEw2TkJZUHBuNHB3SXVIRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">density map;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Mean%20Absolute%20Error%20(MAE)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRnRMbEw2TkJZUHBuNHB3SXVIRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Mean Absolute Error (MAE) ;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-03-27</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="41" name="41" class="anchor-tag">0 概述</h3>
                <div class="p1">
                    <p id="42">近年来, 由于缺乏有效的人群管控措施, 人群踩踏事件时有发生。对人群管理和公共安全而言, 从图片中准确地估计人群总人数已经变得越来越重要。人群计数是一个充满挑战的任务, 人群图片, 尤其是高密度人群图片, 存在着严重的人群重叠问题和尺度变化问题。</p>
                </div>
                <div class="p1">
                    <p id="43">现有人群计数算法大致可以分为2类:基于检测的计数和基于回归的计数。基于检测的算法采用目标检测器把图片中每一个人物进行定位, 然后累加定位出来的目标, 便得到人群总数<citation id="149" type="reference"><link href="111" rel="bibliography" /><link href="113" rel="bibliography" /><link href="115" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>,<a class="sup">3</a>]</sup></citation>。该方法假定图片中的每一个人都能够被检测到, 并且这样的计算方式需要消耗大量的计算资源<citation id="150" type="reference"><link href="117" rel="bibliography" /><link href="119" rel="bibliography" /><sup>[<a class="sup">4</a>,<a class="sup">5</a>]</sup></citation>。然而, 当图片中人群遮挡严重和背景复杂时, 算法的性能便会急剧下降<citation id="151" type="reference"><link href="121" rel="bibliography" /><link href="123" rel="bibliography" /><sup>[<a class="sup">6</a>,<a class="sup">7</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="44">基于回归的算法直接映射人群图片到总人数。文献<citation id="152" type="reference">[<a class="sup">8</a>]</citation>提取一系列特征然后使用高斯回归过程得到总人数。文献<citation id="153" type="reference">[<a class="sup">9</a>]</citation>提出一种用于人群计数的多输出回归模型。文献<citation id="154" type="reference">[<a class="sup">10</a>]</citation>提出一个基于密度图回归的算法, 通过对密度图积分得到总人数。</p>
                </div>
                <div class="p1">
                    <p id="45">近年来, 深度学习方法被应用到了许多计算机视觉任务中, 如语义分割<citation id="155" type="reference"><link href="131" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>和图像分类<citation id="156" type="reference"><link href="133" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>, 并且取得了较好的性能。文献<citation id="157" type="reference">[<a class="sup">13</a>]</citation>将深度学习方法应用到人群计数领域, 提出利用卷积神经网络 (Convolutional Neural Network, CNN) 计算不同场景下的人数, 与传统方法相比在计数准确性上有了明显提升。由于单列CNN只含有一种尺度的感受野, 难以处理尺度变化问题, 研究者们提出一系列的多列CNN、多输入CNN和多任务学习CNN结构。文献<citation id="161" type="reference">[<a class="sup">14</a>,<a class="sup">15</a>]</citation>均采用了三列CNN结构, 每一列网络中包含不同的卷积核尺寸, 通过融合三列网络提取的不同尺度特征解决图片中人的尺度变化问题。文献<citation id="158" type="reference">[<a class="sup">16</a>]</citation>在多列CNN的基础上加入了特征图融合, 并结合高级语义特征和底层细节特征回归密度图, 进一步提升了多列CNN处理尺度变化问题的能力。文献<citation id="159" type="reference">[<a class="sup">17</a>]</citation>提出一种单列多输入CNN, 将图片调整为不同比例输入到网络中, 通过平均各比例图片的结果得到最终人数。文献<citation id="160" type="reference">[<a class="sup">18</a>]</citation>学习图片总人数和密度等级, 将密度等级分类产生的全局信息合并到密度图生成网络中解决尺度变化问题。</p>
                </div>
                <div class="p1">
                    <p id="46">本文提出一种能够处理任意分辨率图片的单列CNN结构, 通过逐层融合网络中不同尺度的特征图来解决人群图片中的尺度变化问题, 并在人群计数数据库ShanghaiTech上进行网络训练和性能测试。</p>
                </div>
                <h3 id="47" name="47" class="anchor-tag">1 基于FPN的人群计数</h3>
                <h4 class="anchor-tag" id="48" name="48">1.1 密度图</h4>
                <div class="p1">
                    <p id="49">在人群计数任务中, 训练卷积神经网络有2种选择。一种是使用人群总数作为数据标签, 另一种则是使用密度图。与人群总数相比, 密度图能够提供人群分布信息, 这些信息对于一些实际应用具有较高的价值。因此, 本文通过训练CNN得到输入图片和其对应的密度图之间的映射关系。</p>
                </div>
                <div class="p1">
                    <p id="50">由于人群计数数据库只提供了人头标记的坐标点, 在网络训练之前首先需要生成训练图片的密度图。对于图片中坐标为<i>x</i><sub><i>i</i></sub>的人头标记点, 可以将其表示为<mathml id="51"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>δ</mi><mrow><mo> (</mo><mrow><mi>x</mi><mo>-</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mo>) </mo></mrow></mrow></math></mathml>。那么, 一张具有N个人头标记点的图片可以表示为:</p>
                </div>
                <div class="p1">
                    <p id="52" class="code-formula">
                        <mathml id="52"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Η</mi><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mi>δ</mi></mstyle><mo stretchy="false"> (</mo><mi>x</mi><mo>-</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="53">然后使用高斯核滤波器G<sub>σ</sub>与该方程进行卷积得到密度方程, 如式 (2) 所示。</p>
                </div>
                <div class="p1">
                    <p id="54" class="code-formula">
                        <mathml id="54"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>F</mi><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mi>δ</mi></mstyle><mo stretchy="false"> (</mo><mi>x</mi><mo>-</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>*</mo><mi>G</mi><msub><mrow></mrow><mi>σ</mi></msub><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="55">对于人群均匀分布的图片, 可以使用自适应高斯滤波器<citation id="162" type="reference"><link href="139" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>进行卷积, 将图片的视角扭曲考虑到密度图生成中。此情况下的密度图方程可以表示为:</p>
                </div>
                <div class="p1">
                    <p id="56" class="code-formula">
                        <mathml id="56"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>F</mi><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mi>δ</mi></mstyle><mo stretchy="false"> (</mo><mi>x</mi><mo>-</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>*</mo><mi>G</mi><msub><mrow></mrow><mrow><mi>σ</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msub><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mo>, </mo><mi>σ</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mi>β</mi><mover accent="true"><mrow><mi>d</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mo stretchy="true">¯</mo></mover><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="57">其中, <mathml id="58"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mrow><mi>d</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mo stretchy="true">¯</mo></mover></mrow></math></mathml>为标记点<i>x</i><sub><i>i</i></sub>与其最近的<i>k</i>个人头之间的平均距离, 而<i>β</i>根据经验取为0.3。</p>
                </div>
                <h4 class="anchor-tag" id="59" name="59">1.2 网络结构</h4>
                <div class="p1">
                    <p id="60">在本文方法中, 网络最终需要生成与输入图片相对应的密度图, 而非一个分类的标签。因此, 本文的网络结构采用了一个全卷积神经网络, 不使用全连接层。由于缺少全连接层, 使得本文的网络结构对输入图片的尺寸没有限制, 能够处理任意尺寸的图片。</p>
                </div>
                <div class="p1">
                    <p id="61">人群图片面临着人物尺度变化问题, 这限制了人群计数算法的性能提升。当前大多数算法使用多列CNN来提取多种尺度的人物特征, 从而解决尺度变化问题。但是, 这些多列CNN普遍采用了最高层的特征图回归产生密度图, 而高层的特征图在经过逐层抽象表达和池化层的下采样后, 会丢失较多的细节信息甚至过滤掉一些尺度较小的目标, 造成算法对小目标的计数能力较差。</p>
                </div>
                <div class="p1">
                    <p id="62">文献<citation id="163" type="reference">[<a class="sup">19</a>]</citation>提出了解决多尺度问题的目标检测算法, 该算法通过特征金字塔网络融合了多层次的特征, 改进了CNN网络的特征提取, 在小目标检测上取得较大进步。受到此算法的启发, 本文将特征金字塔网络应用到人群计数当中解决尺度变化问题。在CNN中, 连续的卷积层中间会穿插一些最大池化层进行下采样, 特征图在经过池化层处理之后分辨率会缩小为原来的1/2, 网络在前向传播过程中会产生多种分辨率的特征图, 这些不同分辨率的特征图便形成了特征金字塔结构。而在CNN中特征是逐层抽象表达的, 于是在特征金字塔结构中越高层的特征图分辨率越低越接近语义特征, 而越底层的特征图分辨率越高含有更多的细节信息。同时, 不同深度的特征图对应的感受野也不相同, 底层特征图感受野小更容易提取小尺度目标特征, 而高层特征图感受野大更容易提取大尺度目标特征。本文通过对CNN产生的特征金字塔自顶向下逐层融合不同分辨率的特征图, 构建具有高级语义特征和细节信息以及具有不同尺度目标特征的特征图, 使用融合之后的特征图回归产生密度图来适应图片中人物的尺度变化。</p>
                </div>
                <div class="p1">
                    <p id="63">图1展示了本文的网络结构, 此网络的前9层为网络的基础部分。这部分结构与VGG-16的前9层相同, 其中包含2个最大池化层。</p>
                </div>
                <div class="area_img" id="64">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201907032_064.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRnRMbEw2TkJZUHBuNHB3SXVIRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 全卷积神经网络结构" src="Detail/GetImg?filename=images/JSJC201907032_064.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRnRMbEw2TkJZUHBuNHB3SXVIRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图1 全卷积神经网络结构</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201907032_064.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRnRMbEw2TkJZUHBuNHB3SXVIRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="65">本文把网络中具有相同分辨率特征图的网络层归入同一个网络阶段, 在每个网络阶段中会含有多个特征图, 而在一个网络阶段中最后一层的输出特征图是该网络阶段最抽象的表达, 于是将每个网络阶段中最后一层的输出特征图作为该阶段的特征图。对于本文的结构可以将网络的基础部分分为3个网络阶段, 这3个网络阶段的特征图便形成了一个3层的特征金字塔结构, 如图2所示。在网络前向传播形成该特征金字塔结构之后, 使用一种自上而下的方法进行侧边连接, 将所有尺度的特征图构建高级语义特征, 最终回归生成密度图。从图1可以看出, 本文的后续网络结构中包含了2个侧边连接。侧边连接的具体实现方法如下:对当前网络阶段的特征图进行反卷积, 使得特征图分辨率变为原来的2倍, 而对前一阶段的特征图进行1*1卷积使两阶段特征图的深度相同, 最后将前后两阶段的特征图进行像素级相加。</p>
                </div>
                <div class="area_img" id="66">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201907032_066.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRnRMbEw2TkJZUHBuNHB3SXVIRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 特征金字塔结构" src="Detail/GetImg?filename=images/JSJC201907032_066.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRnRMbEw2TkJZUHBuNHB3SXVIRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图2 特征金字塔结构</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201907032_066.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRnRMbEw2TkJZUHBuNHB3SXVIRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="67" name="67">1.3 网络结构优化</h4>
                <div class="p1">
                    <p id="68">本文采用随机梯度下降算法进行网络优化, 使用欧氏距离作为目标函数, 计算预测得到的密度图与真实密度图之间的差距。损失函数的定义如式 (4) 所示。</p>
                </div>
                <div class="p1">
                    <p id="69" class="code-formula">
                        <mathml id="69"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><mo stretchy="false"> (</mo><mi>Θ</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mn>1</mn><mrow><mn>2</mn><mi>Ν</mi></mrow></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mrow><mo stretchy="false">∥</mo><mi>F</mi><mo stretchy="false"> (</mo><mi>X</mi><msub><mrow></mrow><mi>i</mi></msub><mo>;</mo><mi>Θ</mi><mo stretchy="false">) </mo><mo>-</mo><mi>F</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">∥</mo></mrow></mstyle><msup><mrow></mrow><mn>2</mn></msup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="70">其中, <i>Θ</i>是网络参数, <i>N</i>是训练集图片总数, 而<i>X</i><sub><i>i</i></sub>是第<i>i</i>张图片, <i>F</i> (<i>X</i><sub><i>i</i></sub>;<i>Θ</i>) 、<i>F</i><sub><i>i</i></sub>分别表示第<i>i</i>张图片的估计密度图和真实密度图。</p>
                </div>
                <h3 id="71" name="71" class="anchor-tag">2 实验结果与分析</h3>
                <div class="p1">
                    <p id="72">本文在具有挑战性的ShanghaiTech数据库中训练并评测所提出的人群计数算法。实验结果表明, 本文方法在准确性和鲁棒性上超过了一些当前的最优算法。而网络结构的训练均在Caffe<citation id="164" type="reference"><link href="143" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>架构上面进行。</p>
                </div>
                <h4 class="anchor-tag" id="73" name="73">2.1 评价标准</h4>
                <div class="p1">
                    <p id="74">与文献<citation id="165" type="reference">[<a class="sup">12</a>,<a class="sup">14</a>,<a class="sup">15</a>]</citation>相同, 本文使用平均绝对误差 (Mean Absolute Error, MAE) 和平均平方误差 (Mean Squared Error, MSE) 评价不同算法的计数性能。MAE和MSE的定义如式 (5) 、式 (6) 所示。</p>
                </div>
                <div class="p1">
                    <p id="75" class="code-formula">
                        <mathml id="75"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>Μ</mi><mi>A</mi><mi>E</mi><mo>=</mo><mfrac><mn>1</mn><mi>Ν</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mrow><mrow><mo>|</mo><mrow><mi>z</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><msup><mi>z</mi><mo>′</mo></msup><msub><mrow></mrow><mi>i</mi></msub></mrow><mo>|</mo></mrow></mrow></mstyle><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>5</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi>Μ</mi><mi>S</mi><mi>E</mi><mo>=</mo><msqrt><mrow><mfrac><mn>1</mn><mi>Ν</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mrow><mo stretchy="false"> (</mo><mi>z</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><msup><mi>z</mi><mo>′</mo></msup><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo></mrow></mstyle><msup><mrow></mrow><mn>2</mn></msup></mrow></msqrt><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>6</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <h4 class="anchor-tag" id="76" name="76">2.2 ShanghaiTech数据集</h4>
                <div class="p1">
                    <p id="77">作为目前最大规模的人群计数数据集, Shanghai Tech包含了1 198张图片, 总标记人数达到了330 165人。该数据集由两部分组成:Part_A和Part_B。其中, Part_A有482张从网络抓取的图片组成, 而Part_B则包含了716张在上海街头拍摄的图片。无论是Part_A还是Part_B, 它们都分为训练集和测试集。Part_A中的300张作为训练集, 剩下的182张作为测试集。在Part_B中训练集和测试集图片分别为400张和316张。本文分别在Part_A和Part_B中评价算法性能。</p>
                </div>
                <h4 class="anchor-tag" id="78" name="78">2.2.1 模型训练</h4>
                <div class="p1">
                    <p id="79">为了保证网络训练有足够的人群图片, 本文采用了数据增强方法增加训练集中的样本数目。从每一张训练图片中截取9张图片, 每一张图片尺寸都为原图片尺寸的1/4, 并且将截取得到的图片翻转得到其对称的图片。这样, 从每一张训练图片中能够得到18张图片。在密度图生成方面, 由于Part_A人群密度高且人群分布较为均匀, 因此能够采用自适应高斯核滤波器生成密度图。而对于Part_B, 人群相对稀疏, 可采用高斯核滤波器生成密度图。</p>
                </div>
                <div class="p1">
                    <p id="80">在实验实现方面, 本文使用了SGD算法进行模型训练, 学习率为1×10<sup>-6</sup>, 动量为0.9, 权重衰减为0.000 5。在网络权值初始化方面, 使用了标准差为0.01高斯分布初始化网络权值。Part_A上网络训练用时约为10 h, 测试时间约为40 s;而Part_B上网络训练用时约为6 h, 测试时间约为90 s。</p>
                </div>
                <h4 class="anchor-tag" id="81" name="81">2.2.2 实验结果</h4>
                <div class="p1">
                    <p id="82">为了验证所采用的特征金字塔网络处理尺度变化问题的有效性, 本文使用VGG-16的前9层加上一个1*1卷积层构成一个没有进行多尺度特征图融合的基础网络, 并将本文提出的特征金字塔网络和基础网络分别在ShanghaiTech上进行了训练和性能测试, 2个网络的训练参数设置均相同。表1是基础网络和特征金字塔网络在ShanghaiTech数据集上的性能比较结果, 可以看到, 相比于基础网络, 特征金字塔网络在Part_A和Part_B 2个数据集上的计数准确性和鲁棒性均有明显提升, 表明使用特征金字塔网络融合多尺度特征图能够有效处理人群图片中尺度变化问题。</p>
                </div>
                <div class="area_img" id="83">
                                            <p class="img_tit">
                                                <b>表1 多尺度特征图融合前后算法性能对比</b>
                                                
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201907032_08300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRnRMbEw2TkJZUHBuNHB3SXVIRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/JSJC201907032_08300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRnRMbEw2TkJZUHBuNHB3SXVIRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                <p class="img_note"> %</p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201907032_08300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRnRMbEw2TkJZUHBuNHB3SXVIRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表1 多尺度特征图融合前后算法性能对比" src="Detail/GetImg?filename=images/JSJC201907032_08300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRnRMbEw2TkJZUHBuNHB3SXVIRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>
                <div class="p1">
                    <p id="84">在验证特征金字塔网络能够有效处理尺度变化问题之后, 本文将所提出算法在ShanghaiTech上的计数准确性和鲁棒性与文献<citation id="166" type="reference">[<a class="sup">14</a>,<a class="sup">15</a>,<a class="sup">16</a>,<a class="sup">17</a>,<a class="sup">18</a>]</citation>的6种算法进行比较, 比较结果如表2所示。从表2可以看出, 相比于这6种算法, 本文算法在Part_A和Part_B 2个数据集上都得到了最佳的MAE和MSE, 尤其是在人群密度较大的Part_A上本文的算法在计数准确性和鲁棒性方面要明显优于这6种算法。这表明本文算法具有优异的计数准确性和稳定性。</p>
                </div>
                <div class="area_img" id="85">
                                            <p class="img_tit">
                                                <b>表2 ShanghaiTech上各算法性能对比</b>
                                                
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201907032_08500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRnRMbEw2TkJZUHBuNHB3SXVIRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/JSJC201907032_08500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRnRMbEw2TkJZUHBuNHB3SXVIRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                <p class="img_note">%</p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201907032_08500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRnRMbEw2TkJZUHBuNHB3SXVIRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表2 ShanghaiTech上各算法性能对比" src="Detail/GetImg?filename=images/JSJC201907032_08500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRnRMbEw2TkJZUHBuNHB3SXVIRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>
                <div class="p1">
                    <p id="86">图3分别展示了两组测试图片的估计密度图和真实密度图, 可以看到本文的网络结构能够较为准确地反映图片中人群分布情况。</p>
                </div>
                <div class="area_img" id="87">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201907032_087.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRnRMbEw2TkJZUHBuNHB3SXVIRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 两组测试图片的真实密度图和估计密度图对比" src="Detail/GetImg?filename=images/JSJC201907032_087.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRnRMbEw2TkJZUHBuNHB3SXVIRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图3 两组测试图片的真实密度图和估计密度图对比</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201907032_087.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRnRMbEw2TkJZUHBuNHB3SXVIRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="88">本文还比较了在Part_A和Part_B 2个数据集上由本文方法估计得到的人数与真实人数, 结果分别如图4和图5所示。为了便于比较, 测试集按照人数递增的顺序分为10组, 每一组的平均人数作为该组所对应的计数结果。除了第10组分别包含20张和37张图片外, 其余的每一组图片中Part_A和Part_B分别包含18张和31张图片。图4展示了在Part_A上的比较结果, 在图4中可以看出, 在大多数测试图片中本文方法都能够准确估计人数, 而在200人以下的图片中出现估计人数过多, 在800人以上的图片中估计人数少于真实人数, 原因在于Part_A中这2种密度的图片较少, 造成了网络结构对这类情况的训练不足。从图5的结果可以看出, 本文方法在Part_B中的各种密度图片上预测人数都十分接近真实值, 这表明本文的方法具有良好的鲁棒性和准确性。</p>
                </div>
                <div class="area_img" id="89">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201907032_089.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRnRMbEw2TkJZUHBuNHB3SXVIRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 Part_A图片真实人数和估计人数对比" src="Detail/GetImg?filename=images/JSJC201907032_089.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRnRMbEw2TkJZUHBuNHB3SXVIRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图4 Part_A图片真实人数和估计人数对比</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201907032_089.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRnRMbEw2TkJZUHBuNHB3SXVIRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="90">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201907032_090.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRnRMbEw2TkJZUHBuNHB3SXVIRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 Part_B图片真实人数和估计人数对比" src="Detail/GetImg?filename=images/JSJC201907032_090.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRnRMbEw2TkJZUHBuNHB3SXVIRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图5 Part_B图片真实人数和估计人数对比</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201907032_090.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRnRMbEw2TkJZUHBuNHB3SXVIRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h3 id="91" name="91" class="anchor-tag">3 结束语</h3>
                <div class="p1">
                    <p id="92">本文提出一个基于特征金字塔的全卷积神经网络, 该方法融合多尺度特征进行密度图回归, 并结合高级语义特征和底层特征, 改进了现有算法的特征提取。本文在大规模数据集ShanghaiTech的Part_A和Part_B上分别进行网络训练和性能测试, 结果表明, 该算法具有良好的稳定性和鲁棒性。然而, 当前的人群计数算法多数将研究重点放在提高算法的计数准确性上, 忽略了网络预测的密度图质量, 因此下一步将研究在保证算法准确性的同时, 如何生成高质量的密度图。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="111">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Estimation of number of people in crowded scenes using perspective transformation">

                                <b>[1]</b> LIN Shengfu, CHEN J Y, CHAO Huangxin.Estimation of number of people in crowded scenes using perspective transformation[J].IEEE Transactions on Systems, 2001, 31 (6) :645-654.
                            </a>
                        </p>
                        <p id="113">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Detection of multiple,partially occluded humans in a single image by Bayesian combination of edgelet part detectors">

                                <b>[2]</b> WU Bo, NEVATIA R.Detection of multiple, partially occluded humans in a single image by Bayesian combination of edgelet part detectors[C]//Proceedings of the 10th IEEE International Conference on Computer Vision.Washington D.C., USA:IEEE Press, 2005:90-97.
                            </a>
                        </p>
                        <p id="115">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Es-timating the Number of People in Crowded Scenesby MID Based Foreground Segmentation and Head-shoulder Detection">

                                <b>[3]</b> LI Min, ZHANG Zhaoxiang, HUANG Kaiqi, et al.Estimating the number of people in crowded scenes by mid based foreground segmentation and head-shoulder detection[C]//Proceedings of the 19th International Conference on Pattern Recognition.Tampa, USA:IEEE Press, 2008:1-4.
                            </a>
                        </p>
                        <p id="117">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Segmentation and tracking of multiple humans in crowded environments">

                                <b>[4]</b> ZHAO Tao, NEVATIA R, WU Bo.Segmentation and tracking of multiple humans in crowded environments[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2008, 30 (7) :1198-1211.
                            </a>
                        </p>
                        <p id="119">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Marked point processes for crowdcounting">

                                <b>[5]</b> GE W, COLLINS R T.Marked point processes for crowd counting[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.San Francisco, USA:IEEE Press, 2009:2913-2920.
                            </a>
                        </p>
                        <p id="121">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Automatic adaptation of a generic pedestrian detector to a specific traffic scene">

                                <b>[6]</b> WANG Meng, WANG Xiaogang.Automatic adaptation of a generic pedestrian detector to a specific traffic scene[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Colorado Springs, USA:IEEE Press, 2011:3401-3408.
                            </a>
                        </p>
                        <p id="123">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJA201808006&amp;v=MjgxMzRqa1VycktMejdCYjdHNEg5bk1wNDlGWW9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJvRkM=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRnRMbEw2TkJZUHBuNHB3SXVIRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b> 李云波, 唐斯琪, 周星宇, 等.可伸缩模块化CNN人群计数方法[J].计算机科学, 2018, 45 (8) :17-21, 40.
                            </a>
                        </p>
                        <p id="125">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Privacy preserving crowd monitoring: Counting people without people models or tracking">

                                <b>[8]</b> CHAN A B, LIANG Z S J, Vasconcelos N.Privacy preserving crowd monitoring:counting people without people models or tracking[C]//Proceedings of the 10th IEEE International Conference on Computer Vision and Pattern Recognition.Anchorage, USA:IEEE Press, 2008:1-7.
                            </a>
                        </p>
                        <p id="127">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Feature mining for localised crowd counting">

                                <b>[9]</b> CHEN K, LOY C C, GONG S, et al.Feature mining for localised crowd counting[C]//Proceedings of BMVC’12.London, UK:[s.n.], 2012:3.
                            </a>
                        </p>
                        <p id="129">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning to count objects in images">

                                <b>[10]</b> LEMPITSKY V, ZISSERMAN A.Learning to count objects in images[C]//Proceedings of Advances in Neural Information Processing Systems.Vancouver, Canada:[s.n.], 2010:1324-1332.
                            </a>
                        </p>
                        <p id="131">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fully convolutional networks for semantic segmentation">

                                <b>[11]</b> LONG J, SHELHAMER E, DARRELL T.Fully convolutional networks for semantic segmentation[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Boston, USA:IEEE Press, 2015:3431-3440.
                            </a>
                        </p>
                        <p id="133">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Convolutional Neural Networks for Large-Scale Remote-Sensing Image Classification">

                                <b>[12]</b> MAGGIORI E, TARABALKA Y, CHARPIAT G, et al.Convolutional neural networks for large-scale remote-sensing image classification[J].IEEE Transactions on Geoscience and Remote Sensing, 2017, 55 (2) :645-657.
                            </a>
                        </p>
                        <p id="135">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Cross-scene crowd counting via deep convolutional neural networks">

                                <b>[13]</b> ZHANG Cong, LI Hongsheng, WANG Xiaogang, et al.Cross-scene crowd counting via deep convolutional neural networks[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Boston, USA:IEEE Press, 2015:833-841.
                            </a>
                        </p>
                        <p id="137">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=C.:Single-image crowd counting via multi-column convolutional neural network">

                                <b>[14]</b> ZHANG Yingying, ZHOU Desan, CHEN Siqin, et al.Single-image crowd counting via multi-column convolutional neural network[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Las Vegas, USA:IEEE Press, 2016:589-597.
                            </a>
                        </p>
                        <p id="139">
                            <a id="bibliography_15" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=KJDS201709009&amp;v=MDYyOTZVcnJLTGlmUGZiRzRIOWJNcG85RmJZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSb0ZDams=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRnRMbEw2TkJZUHBuNHB3SXVIRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[15]</b> 吴淑窈, 刘希庚, 胡昌振, 等.基于卷积神经网络人群计数的研究与实现[J].科教导刊, 2017 (9) :16-17.
                            </a>
                        </p>
                        <p id="141">
                            <a id="bibliography_16" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZZDZ201802012&amp;v=MDg2MTVxcUJ0R0ZyQ1VSTE9lWmVSb0ZDamtVcnJLUHpmUGRMRzRIOW5Nclk5RVpvUUtESDg0dlI0VDZqNTRPM3o=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRnRMbEw2TkJZUHBuNHB3SXVIRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[16]</b> 唐斯琪, 陶蔚, 张梁梁, 等.一种多列特征图融合的深度人群计数算法[J].郑州大学学报 (理学版) , 2018, 50 (2) :1-6.
                            </a>
                        </p>
                        <p id="143">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fully convolutional crowd counting on highly congested scenes">

                                <b>[17]</b> MARSDEN M, McGUINESS K, LITTLE S, et al.Fully convolutional crowd counting on highly congested scenes[EB/OL].[2018-05-20].https://www.researchgate.net/publication.
                            </a>
                        </p>
                        <p id="145">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=CNN-Based cascaded multi-task learning of high-level prior and density estimation for crowd counting">

                                <b>[18]</b> SINDAGI V A, PATEL V M.Cnn-based cascaded multi-task learning of high-level prior and density estimation for crowd counting[C]//Proceedings of IEEE International Conference on Advanced Video and Signal Based Surveillance.Lecce, Italy:IEEE Press, 2017:1-6.
                            </a>
                        </p>
                        <p id="147">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Feature pyramid networks for object detection">

                                <b>[19]</b> LIN T Y, DOLLAR P, GIRSHICK R, et al.Feature pyramid networks for object detection[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Honolulu, USA:[s.n.], 2017:4-7.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJC201907032" />
        <input id="dpi" type="hidden" value="600" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRnRMbEw2TkJZUHBuNHB3SXVIRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201907032&amp;v=MjU0OThyS0x6N0JiYkc0SDlqTXFJOUdab1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm9GQ2prVXI=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRnRMbEw2TkJZUHBuNHB3SXVIRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
