<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637131279320118750%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJC201902048%26RESULT%3d1%26SIGN%3d1HBHBr%252fI%252f88ShIO%252fAiET1RkCCUA%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201902048&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201902048&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201902048&amp;v=Mjg1MjM0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSbkZ5amtXci9OTHo3QmJiRzRIOWpNclk5QmJJUUtESDg=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#63" data-title="0概述 ">0概述</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#68" data-title="1 井漏类型诊断数据预处理 ">1 井漏类型诊断数据预处理</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#70" data-title="1.1 缺值数据补全">1.1 缺值数据补全</a></li>
                                                <li><a href="#72" data-title="1.2 数据离散化">1.2 数据离散化</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#74" data-title="2 AFIV-ID3算法 ">2 AFIV-ID3算法</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#107" data-title="3 实验结果与分析 ">3 实验结果与分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#108" data-title="3.1 标准数据集验证">3.1 标准数据集验证</a></li>
                                                <li><a href="#142" data-title="3.2 井漏数据实验">3.2 井漏数据实验</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#151" data-title="4 结束语 ">4 结束语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#113" data-title="表1 气候因素表">表1 气候因素表</a></li>
                                                <li><a href="#128" data-title="图1 ID3算法构建的决策树">图1 ID3算法构建的决策树</a></li>
                                                <li><a href="#129" data-title="图2 AFIV-ID3算法构建的决策树">图2 AFIV-ID3算法构建的决策树</a></li>
                                                <li><a href="#135" data-title="表2 4种算法分类精确度结果对比">表2 4种算法分类精确度结果对比</a></li>
                                                <li><a href="#136" data-title="图3 4种算法分类精确度对比柱状图">图3 4种算法分类精确度对比柱状图</a></li>
                                                <li><a href="#139" data-title="图4 UCI数据集分类精确度对比雷达图">图4 UCI数据集分类精确度对比雷达图</a></li>
                                                <li><a href="#140" data-title="表3 UCI数据集分类精确度结果对比">表3 UCI数据集分类精确度结果对比</a></li>
                                                <li><a href="#145" data-title="表4 井漏类型诊断数据片段">表4 井漏类型诊断数据片段</a></li>
                                                <li><a href="#147" data-title="表5 井漏类型诊断数据分类结果对比">表5 井漏类型诊断数据分类结果对比</a></li>
                                                <li><a href="#148" data-title="图5 井漏类型诊断数据分类结果对比折线图">图5 井漏类型诊断数据分类结果对比折线图</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="33">


                                    <a id="bibliography_1" title="蔡汶君.基于神经网络融合技术的钻井井漏诊断模型研究[D].成都:西南石油大学, 2014." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=1014415698.nh&amp;v=MjgyNTFuRnlqa1dyL05WRjI2R3JlNUc5ZkZwNUViUElRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVI=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                        蔡汶君.基于神经网络融合技术的钻井井漏诊断模型研究[D].成都:西南石油大学, 2014.
                                    </a>
                                </li>
                                <li id="35">


                                    <a id="bibliography_2" title="徐哲, 李建, 王兵, 等.基于贝叶斯网络的钻井井漏问题研究[J].石油天然气学报, 2013, 35 (12) :125-129." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JHSX201312024&amp;v=MjIyNTdyL05MeVhZZHJHNEg5TE5yWTlIWUlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJuRnlqa1c=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                        徐哲, 李建, 王兵, 等.基于贝叶斯网络的钻井井漏问题研究[J].石油天然气学报, 2013, 35 (12) :125-129.
                                    </a>
                                </li>
                                <li id="37">


                                    <a id="bibliography_3" title="QUINLAN J R.Induction of decision trees[J].Machine Learning, 1986, 1 (1) :81-106." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00001337810&amp;v=MTMyOTZidWR0RkNIbFU3N0JJRnM9Tmo3QmFyTzRIdEhOckl4Q2JPb1BZM2s1ekJkaDRqOTlTWHFScnhveGNNSDdSN3Fl&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                        QUINLAN J R.Induction of decision trees[J].Machine Learning, 1986, 1 (1) :81-106.
                                    </a>
                                </li>
                                <li id="39">


                                    <a id="bibliography_4" title="WAGACHA P W.Induction of decision trees[EB/OL].[2017-11-28].http://erepository.uonbi.ac.ke/bitstream/handle/11295/44263/decisionTrees.pdf?sequence=1." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Induction of decision trees">
                                        <b>[4]</b>
                                        WAGACHA P W.Induction of decision trees[EB/OL].[2017-11-28].http://erepository.uonbi.ac.ke/bitstream/handle/11295/44263/decisionTrees.pdf?sequence=1.
                                    </a>
                                </li>
                                <li id="41">


                                    <a id="bibliography_5" title="韩松来, 张辉, 周华平.基于关联度函数的决策树分类算法[J].计算机应用, 2005, 25 (11) :2655-2657." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY200511062&amp;v=MTczNzR6cXFCdEdGckNVUkxPZVplUm5GeWprV3IvTkx6N0JkN0c0SHRUTnJvOURab1FLREg4NHZSNFQ2ajU0TzM=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                        韩松来, 张辉, 周华平.基于关联度函数的决策树分类算法[J].计算机应用, 2005, 25 (11) :2655-2657.
                                    </a>
                                </li>
                                <li id="43">


                                    <a id="bibliography_6" title="韩松来.基于关联度函数的决策树分类算法研究[D].长沙:国防科学技术大学, 2005." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=2006126645.nh&amp;v=MDc2MTVrV3IvTlYxMjdHTEs2R05mSXFwRWJQSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm5GeWo=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                        韩松来.基于关联度函数的决策树分类算法研究[D].长沙:国防科学技术大学, 2005.
                                    </a>
                                </li>
                                <li id="45">


                                    <a id="bibliography_7" title="LUO H, CHEN Y, ZHANG W.An improved ID3algorithm based on attribute importance-weighted[C]//Proceedings of the 2nd International Workshop on Database Technology and Applications.Washington D.C., USA:IEEE Press, 2010:1-4." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=An improved ID3algorithm based on attribute importance-weighted">
                                        <b>[7]</b>
                                        LUO H, CHEN Y, ZHANG W.An improved ID3algorithm based on attribute importance-weighted[C]//Proceedings of the 2nd International Workshop on Database Technology and Applications.Washington D.C., USA:IEEE Press, 2010:1-4.
                                    </a>
                                </li>
                                <li id="47">


                                    <a id="bibliography_8" title="陆秋, 程小辉.基于属性相似度的决策树算法[J].计算机工程, 2009, 35 (6) :82-84." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC200906030&amp;v=MTQyODl0ak1xWTlHWklRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJuRnlqa1dyL05MejdCYmJHNEg=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                        陆秋, 程小辉.基于属性相似度的决策树算法[J].计算机工程, 2009, 35 (6) :82-84.
                                    </a>
                                </li>
                                <li id="49">


                                    <a id="bibliography_9" title="胡学钢, 李楠.基于属性重要度的随机决策树学习算法[J].合肥工业大学学报 (自然科学版) , 2007, 30 (6) :681-685." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=HEFE200706005&amp;v=MjU3MTE0TzN6cXFCdEdGckNVUkxPZVplUm5GeWprV3IvTkxTak5hN0c0SHRiTXFZOUZZWVFLREg4NHZSNFQ2ajU=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                        胡学钢, 李楠.基于属性重要度的随机决策树学习算法[J].合肥工业大学学报 (自然科学版) , 2007, 30 (6) :681-685.
                                    </a>
                                </li>
                                <li id="51">


                                    <a id="bibliography_10" title="张琳, 陈燕, 李桃迎, 等.决策树分类算法研究[J].计算机工程, 2011, 37 (13) :66-67." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201113021&amp;v=MjI1OTh0R0ZyQ1VSTE9lWmVSbkZ5amtXci9OTHo3QmJiRzRIOUROckk5SFpZUUtESDg0dlI0VDZqNTRPM3pxcUI=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[10]</b>
                                        张琳, 陈燕, 李桃迎, 等.决策树分类算法研究[J].计算机工程, 2011, 37 (13) :66-67.
                                    </a>
                                </li>
                                <li id="53">


                                    <a id="bibliography_11" title="郝胜轩, 宋宏, 周晓锋.基于近邻噪声处理的KNN缺失数据填补算法[J].计算机仿真, 2014, 31 (7) :264-268." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJZ201407060&amp;v=MDQzODBMejdCZExHNEg5WE1xSTlEWklRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJuRnlqa1dyL04=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[11]</b>
                                        郝胜轩, 宋宏, 周晓锋.基于近邻噪声处理的KNN缺失数据填补算法[J].计算机仿真, 2014, 31 (7) :264-268.
                                    </a>
                                </li>
                                <li id="55">


                                    <a id="bibliography_12" title="王小巍, 蒋玉明.决策树ID3算法的分析与改进[J].计算机工程与设计, 2011, 32 (9) :3069-3072." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=SJSJ201109038&amp;v=MjU5MTN5amtXci9OTmlmWVpMRzRIOURNcG85R2JJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSbkY=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[12]</b>
                                        王小巍, 蒋玉明.决策树ID3算法的分析与改进[J].计算机工程与设计, 2011, 32 (9) :3069-3072.
                                    </a>
                                </li>
                                <li id="57">


                                    <a id="bibliography_13" title="郑捷.机器学习算法原理与编程实践[M].北京:电子工业出版社, 2015." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CBBD&amp;filename=9787121273674000&amp;v=MzEyMjJ2c1U3L0pLVjRSWEZxekdiSzZIOVBMcklsQ1lPc1BEQk04enhVU21EZDlTSDduM3hFOWZidm5LcmlmWnU5dUZD&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[13]</b>
                                        郑捷.机器学习算法原理与编程实践[M].北京:电子工业出版社, 2015.
                                    </a>
                                </li>
                                <li id="59">


                                    <a id="bibliography_14" title="周志华.机器学习[M].北京:清华大学出版社, 2016." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CBBD&amp;filename=9787302423287000&amp;v=MDg2MjY5dUZDdnNVNy9KS1Y0UlhGcXpHYkM0SE5YT3JJMU5ZK3NQREJNOHp4VVNtRGQ5U0g3bjN4RTlmYnZuS3JpZlp1&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[14]</b>
                                        周志华.机器学习[M].北京:清华大学出版社, 2016.
                                    </a>
                                </li>
                                <li id="61">


                                    <a id="bibliography_15" title="温雪岩, 陈家男, 景维鹏, 等.面向不平衡数据集分类模型的优化研究[J].计算机工程, 2018, 44 (4) :268-273, 293." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201804043&amp;v=Mjc5MjdUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJuRnlqa1dyL05MejdCYmJHNEg5bk1xNDlCWjRRS0RIODR2UjQ=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[15]</b>
                                        温雪岩, 陈家男, 景维鹏, 等.面向不平衡数据集分类模型的优化研究[J].计算机工程, 2018, 44 (4) :268-273, 293.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJC" target="_blank">计算机工程</a>
                2019,45(02),290-295 DOI:10.19678/j.issn.1000-3428.0049846            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm">基于优化ID3的井漏类型分类算法</span>
 <span class="shoufa"></span>                                     </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%8E%E5%BB%BA&amp;code=10905320&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">李建</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E4%BB%98%E5%B0%8F%E6%96%8C&amp;code=38617607&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">付小斌</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%90%B4%E5%AA%9B%E5%AA%9B&amp;code=25253079&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">吴媛媛</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E8%A5%BF%E5%8D%97%E7%9F%B3%E6%B2%B9%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E5%AD%A6%E9%99%A2&amp;code=0184187&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">西南石油大学计算机科学学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>决策树算法用于井漏分类时, 由于井漏数据离散化后多值属性占比较大, 且具有多值偏向的缺点, 分类效果不理想。为此, 提出一种基于改进ID3的AFIV-ID3算法。在ID3的基础上引入属性重要度计算新的信息熵, 属性重要度大小由决策者依靠先验或领域知识决定。在信息增益计算中加入关联度函数比, 对信息增益值做出修正。AFIV-ID3算法克服了ID3多值偏向的缺点, 提高了数据中重要属性的权重, 从而提升井漏类型分类精度。4组UCI数据集和真实井漏数据测试结果表明, 该算法的分类精度优于ID3和C4. 5算法, 并能够将人工经验法不稳定的分类精度提高至约72. 23%。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%BA%95%E6%BC%8F%E7%B1%BB%E5%9E%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">井漏类型;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=ID3%E7%AE%97%E6%B3%95&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">ID3算法;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%85%B3%E8%81%94%E5%BA%A6%E5%87%BD%E6%95%B0%E6%AF%94&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">关联度函数比;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%B1%9E%E6%80%A7%E9%87%8D%E8%A6%81%E5%BA%A6&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">属性重要度;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%A4%9A%E5%80%BC%E5%81%8F%E5%90%91&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">多值偏向;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    李建 (1960—) , 男, 教授, 主研方向为数据仓库、数据挖掘;E-mail: felixo0@ 163. com
;
                                </span>
                                <span>
                                    *付小斌 (通信作者) , 硕士研究生;
;
                                </span>
                                <span>
                                    吴媛媛, 硕士研究生。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2017-12-26</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家科技重大专项 (2016ZX05020-006);</span>
                    </p>
            </div>
                    <h1>Classification Algorithm of Well Leakage Type Based on Optimized ID3</h1>
                    <h2>
                    <span>LI Jian</span>
                    <span>FU Xiaobin</span>
                    <span>WU Yuanyuan</span>
            </h2>
                    <h2>
                    <span>School of Computer Science, Southwest Petroleum University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>When the decision tree algorithm is used in well leakage classification, the classification effect is not satisfactory because of the large proportion of multi-valued attributes after the well leakage data is discretized, and because the algorithm has the shortcoming of multi-value bias. Therefore, an improved AFIV-ID3 algorithm based on ID3 is proposed. On the basis of ID3, attribute importance is introduced to calculate new information entropy. Attribute importance is determined by the decision maker depending on prior knowledge or domain knowledge. The association function ratio is added to the information gain calculation to modify the information gain value. The AFIV-ID3 algorithm overcomes the shortcoming of ID3 multi-value bias, improves the weight of important attributes in the data, and effectively improves the classification accuracy of well leakage type. The test results of four UCI data sets and real well leakage data show that the classification accuracy of this algorithm is better than that of ID3 and C4. 5 algorithm, and the unstable classification accuracy of artificial experience method can be improved to about 72. 23%.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=well%20leakage%20type&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">well leakage type;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=ID3%20algorithm&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">ID3 algorithm;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=association%20function%20ratio&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">association function ratio;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=attribute%20importance&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">attribute importance;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=multi-value%20bias&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">multi-value bias;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2017-12-26</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="63" name="63" class="anchor-tag">0概述</h3>
                <div class="p1">
                    <p id="64">井漏问题在石油钻采过程中极为常见, 该问题会对石油钻采安全和钻采效率产生巨大的影响。井漏的类型基本分为渗透性井漏、裂缝性井漏、溶洞性井漏和破裂性井漏。钻井过程中井漏类型种类多、数据庞杂, 传统以施工人员经验为判断依据的方法很难高效准确地判断井漏类型<citation id="158" type="reference"><link href="33" rel="bibliography" /><link href="35" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>]</sup></citation>。在施工现场准确判断井漏类型并及时采取相应措施进行堵漏, 具有重要的现实意义。在进行井漏分类时, 分类器需要满足易解释、易操作和精度高等要求。在众多的分类算法中, 朴素贝叶斯算法在数据属性较多或者属性间相关性较大时效率不及决策树算法, KNN算法与神经网络算法输出结果可解释性差, 遗传算法需要大量的时间进行训练以获得较高的精确度。与这些算法相比, 决策树算法具有易解释、效率高、分类精度高等优点, 在处理井漏类型判断问题时具有较大的优势, 而且决策树算法在实际工程中应用广泛, 因此, 本文选用决策树算法处理井漏分类问题。</p>
                </div>
                <div class="p1">
                    <p id="65">ID3算法是一种以信息论为基础, 以信息熵和信息增益为衡量标准的分类预测算法<citation id="159" type="reference"><link href="37" rel="bibliography" /><link href="39" rel="bibliography" /><sup>[<a class="sup">3</a>,<a class="sup">4</a>]</sup></citation>。该算法通过计算每一个属性所对应的信息增益, 且遵循信息增益高的属性是好属性的原则生成决策树, 但其存在多值偏向的缺点。通过信息增益所选取的属性在一些情况下并不一定是最优属性, 属性取值数量与属性是否重要之间没有必然联系, 这使得ID3算法在分类时可能会归纳出不正确的知识。</p>
                </div>
                <div class="p1">
                    <p id="66">文献<citation id="164" type="reference">[<a class="sup">5</a>,<a class="sup">6</a>]</citation>使用关联度函数构建决策树, 其分类能力与ID3算法相当, 但由于摒弃了信息增益, 因此对分类精度的提升程度有限。文献<citation id="160" type="reference">[<a class="sup">7</a>]</citation>提出属性重要度, 但在改进算法中只单纯引入属性重要度使得算法分类精度不能有效地提高。文献<citation id="161" type="reference">[<a class="sup">8</a>]</citation>以属性相似度为切入点进行算法优化, 改进的算法精度有明显提升。文献<citation id="162" type="reference">[<a class="sup">9</a>]</citation>将属性重要度与随机决策树相结合, 解决了RDT中因属性的完全随机选择所导致的分类正确率不稳定问题。文献<citation id="163" type="reference">[<a class="sup">10</a>]</citation>引入重要性参数和属性取值数量参数, 然而因为这些参数的取值太过细密, 所以不利于实际操作。由于井漏类型数据的复杂性, 以上研究均不能高效地应用于井漏类型分类问题中。</p>
                </div>
                <div class="p1">
                    <p id="67">本文利用属性重要度值和关联度函数比改进信息增益的计算方式, 提出一种AFIV-ID3 (Association Function and Attribute-importance Value ID3) 算法。其中, 关联度函数为文献<citation id="165" type="reference">[<a class="sup">5</a>]</citation>算法中的核心函数, 将其划归为每个属性的关联度函数比, 并结合属性重要度值修正信息增益的计算方式。</p>
                </div>
                <h3 id="68" name="68" class="anchor-tag">1 井漏类型诊断数据预处理</h3>
                <div class="p1">
                    <p id="69">井漏类型数据较为杂乱, 在日常数据记录过程中会出现漏记、误记、重复记录情况, 在预处理过程中需要将数据中的缺值数据补全、剔除冗余实例、删除异常实例及离散化连续属性。冗余属性及异常实例通过对比及经验进行筛除, 缺失数据使用KNN补全法进行补全, 数据离散化使用K-means算法。</p>
                </div>
                <h4 class="anchor-tag" id="70" name="70">1.1 缺值数据补全</h4>
                <div class="p1">
                    <p id="71">在处理井漏类型数据时, 本文使用KNN补全法对缺值数据进行补全。该方法与KNN分类算法一致的是:在实例间距离最近的两者最为相似的基础上, 寻找完整实例空间中与缺失实例最为相似的K个实例。KNN补全法的优点是不需要对每一个缺值的实例单独建立模型, 可处理具有多个缺失值的属性, 且数据的相关结构也被考虑在内<citation id="166" type="reference"><link href="53" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>。</p>
                </div>
                <h4 class="anchor-tag" id="72" name="72">1.2 数据离散化</h4>
                <div class="p1">
                    <p id="73">ID3算法只能处理离散数据, 因此, 在数据预处理阶段需要将连续型的数据转化为离散数据。对连续属性A采用K-means算法进行离散, 首先随机选择该属性上的k个值作为初始中心;然后分别计算其他值到这些簇的距离, 选择距离最小的簇加入, 并通过加权平均的方法计算每个簇的中心值, 以此类推, 将剩余的值分别加入各簇得到A的k个聚类;最后选择1到k-1个簇中的最大值作为标准对A进行划分, 设k-1个值为{v<sub>1</sub>, v<sub>2</sub>, …, v<sub>k-1</sub>}, 那么A可分为k个区间{a≤v<sub>1</sub>, v<sub>1</sub>&lt;a≤v<sub>2</sub>, …, v<sub>k-1</sub>&lt;a}, 至此属性A就得到了k个属性值<citation id="167" type="reference"><link href="55" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>。</p>
                </div>
                <h3 id="74" name="74" class="anchor-tag">2 AFIV-ID3算法</h3>
                <div class="p1">
                    <p id="75">AFIV-ID3算法在ID3算法信息增益计算方式的基础上, 引入属性重要度值和关联度函数比以修正信息增益, 从而更加合理地构建决策树并进行分类。</p>
                </div>
                <div class="p1">
                    <p id="76">信息熵能够用来度量包含不同特征的数据样本与类别的不确定性, 变量的不确定性越大所对应的熵值也越大, 将其清晰化所需要的信息量也相应的越多。以信息熵的下降速度为标准选取测试属性, 这为决策树的划分提供了最重要的依据。</p>
                </div>
                <div class="p1">
                    <p id="77">设在数据集合S中有s个数据样本, 类别标签有m种取值, 则定义m个类C<sub>i</sub> (i=1, 2, …, m) 。根据上述条件假设s<sub>i</sub>是C<sub>i</sub>中的样本数, 于是对一个已有的完整样本进行分类时, 信息熵计算如下:</p>
                </div>
                <div class="area_img" id="78">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJC201902048_07800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="79">其中, p<sub>i</sub>表示任意样本属于C<sub>i</sub>的概率, 且用<image id="153" type="formula" href="images/JSJC201902048_15300.jpg" display="inline" placement="inline"><alt></alt></image>进行估计。</p>
                </div>
                <div class="p1">
                    <p id="80">设属性A有v个不同的值{a<sub>1</sub>, a<sub>2</sub>, …, a<sub>v</sub>}, A可以将S划分成相同数量的子集{S<sub>1</sub>, S<sub>2</sub>, …, S<sub>v</sub>}。其中, S<sub>j</sub>包含S中的一些样本:它们在A上具有值a<sub>j</sub>, 如果选A做测试属性, 即最优划分属性, 那么这些子集就是S节点生长出来的决策树分支。设s<sub>ij</sub>是子集S<sub>j</sub>中类C<sub>j</sub>的样本数, 由A划分成子集的熵或期望信息如式 (2) 所示。</p>
                </div>
                <div class="area_img" id="81">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJC201902048_08100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="83">其中, <image id="154" type="formula" href="images/JSJC201902048_15400.jpg" display="inline" placement="inline"><alt></alt></image>是第j个子集的权, 且等于子集 (即A值为a<sub>j</sub>) 的样本个数除以S的样本总数。属性A信息熵值越小, 子集划分的纯度越高。对于给定的子集S<sub>j</sub>, 其熵为:</p>
                </div>
                <div class="area_img" id="84">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJC201902048_08400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="85">其中, <image id="155" type="formula" href="images/JSJC201902048_15500.jpg" display="inline" placement="inline"><alt></alt></image>表示S<sub>j</sub>中的样本属于类C<sub>i</sub>的概率。</p>
                </div>
                <div class="p1">
                    <p id="86">通过计算信息增益来确定决策树分支的划分。用Gain (A) 表示信息增益, 那么在A上的分支获得的信息增益为:</p>
                </div>
                <div class="area_img" id="87">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJC201902048_08700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="88">信息增益是因知道属性A的值而导致的熵的期望压缩<citation id="168" type="reference"><link href="57" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="89">AFIV-ID3算法引入属性重要度值和关联度函数比, 并使用新的方法计算信息增益, 解决了ID3算法多值偏向的缺点, 从而构建出更加合理的决策树。</p>
                </div>
                <div class="p1">
                    <p id="90">在式 (2) 中引入属性重要度值w (0≤w&lt;1) , 其大小由决策者根据经验和领域知识决定。属性A的属性重要度值表述如下:</p>
                </div>
                <div class="area_img" id="91">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJC201902048_09100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="92">其中, p<sub>i</sub>表示属性A属于决策属性C<sub>i</sub>的概率。</p>
                </div>
                <div class="p1">
                    <p id="93">属性的重要度在熟知的数据集中较容易确定, 在实际应用时可以根据经验给出相应的属性重要度值。若在一个数据集中将实际比较重要的属性赋予较少的价值时, 可以增大其属性重要度, 这样计算的属性信息增益会相应地增大, 以便于更好地建树, 于是得到E' (A) :</p>
                </div>
                <div class="area_img" id="94">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJC201902048_09400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="95">设A是数据集S的属性, C是S的决策属性。A和C之间的关联度函数可以表示如下:</p>
                </div>
                <div class="area_img" id="96">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJC201902048_09600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="97">其中, x<sub>ij</sub> (j=1, 2) 表示数据集S的属性A取第i个值, 决策属性C采用第j个值的样本个数, n是属性A的取值个数。</p>
                </div>
                <div class="p1">
                    <p id="98">式 (7) 理解为:设有属性“发烧”, 其值域为{Yes, No}, 有决策属性“感冒”, 其值域为{Yes, No}, 那么, x<sub>11</sub>表示发烧取值为Yes, 感冒取值为Yes的样本数量, x<sub>12</sub>表示发烧取值为Yes, 感冒取值为No的样本数量, x<sub>21</sub>表示发烧取值为No, 感冒取值为Yes的样本数量, x<sub>22</sub>表示发烧取值为No, 感冒取值为No的样本数量。则AF (发烧) 为:</p>
                </div>
                <div class="area_img" id="99">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJC201902048_09900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="100">其中, n为属性发烧的取值数目, 此处n=2, |x<sub>11</sub>-x<sub>12</sub>|为在发烧情况下, 感冒样本数和不感冒样本数之间的差异, 差异越大说明发烧和感冒之间的关联越大, 反之则越小。</p>
                </div>
                <div class="p1">
                    <p id="101">遵循上述关联度函数的规范, 假设有i个属性和每个属性对应的关联度函数值, 分别表示为AF (1) , AF (2) , …, AF (i) , 于是得到每一个属性的关联度函数值比:</p>
                </div>
                <div class="area_img" id="102">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJC201902048_10200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="103">其中, 0&lt;A≤i。</p>
                </div>
                <div class="p1">
                    <p id="104">基于式 (6) 和式 (9) 可以将式 (4) 修改为:</p>
                </div>
                <div class="area_img" id="105">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJC201902048_10500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="106">G ain'可以作为构造决策树步骤中的新标准, 在构造决策树时选Gain'值最大的属性作为划分属性, 对各属性重复以上计算步骤, 即可得最终决策树。</p>
                </div>
                <h3 id="107" name="107" class="anchor-tag">3 实验结果与分析</h3>
                <h4 class="anchor-tag" id="108" name="108">3.1 标准数据集验证</h4>
                <div class="p1">
                    <p id="109">标准数据集的验证主要分为2个步骤:</p>
                </div>
                <div class="p1">
                    <p id="110">1) 利用提出的信息增益计算方式计算得到修正过的信息增益, 并以其来构建决策树, 同时判断新的决策树是否有效地克服了多值偏向的缺点。</p>
                </div>
                <div class="p1">
                    <p id="111">2) 首先使用AFIV-ID3算法与文献<citation id="169" type="reference">[<a class="sup">5</a>]</citation>提出的算法作对比, 然后再使用标准UCI数据集做分类测试, 并与ID3、C4.5算法作对比。</p>
                </div>
                <div class="p1">
                    <p id="112">步骤1 AFIV-ID3算法获得重新计算的信息增益Gain', Gain'被用来构建新的决策树。本文使用表1所示的气候因素表进行决策树构建测试。实验将表1数据集各属性的属性重要度值设置为w (天气) =0.25, w (湿度) =0.20, w (气温) =w (大风) =0。</p>
                </div>
                <div class="area_img" id="113">
                                            <p class="img_tit">
                                                表1 气候因素表
                                                
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201902048_11300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/JSJC201902048_11300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201902048_11300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表1 气候因素表" src="Detail/GetImg?filename=images/JSJC201902048_11300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>
                <div class="p1">
                    <p id="114">以表1气候因素表为例, 各属性的关联度函数比计算如下:</p>
                </div>
                <div class="area_img" id="115">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJC201902048_11500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="116">从而可以得到:</p>
                </div>
                <div class="area_img" id="117">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJC201902048_11700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="118">该数据集包含10个训练样例, 其中正例占<image id="156" type="formula" href="images/JSJC201902048_15600.jpg" display="inline" placement="inline"><alt></alt></image>, 反例占<image id="157" type="formula" href="images/JSJC201902048_15700.jpg" display="inline" placement="inline"><alt></alt></image>, 于是可以得出:</p>
                </div>
                <div class="area_img" id="119">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJC201902048_11900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="120">以表1中的天气属性为例, 它有3个可能的取值:{晴, 雨, 云}。若使用该属性进行划分, 则有:</p>
                </div>
                <div class="area_img" id="121">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJC201902048_12100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="area_img" id="121">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJC201902048_12101.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="123">类似地, 计算出其他属性的信息增益分别为:</p>
                </div>
                <div class="area_img" id="124">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJC201902048_12400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="125">从上文计算可以看出, 天气属性的信息增益最大, 于是它被选为划分属性。当天气属性为晴时, 该节点所包含样例编号为{1, 3, 4, 5}, 在此基础上计算出各属性的信息增益分别为:</p>
                </div>
                <div class="area_img" id="126">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJC201902048_12600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="127">此时湿度属性取得了最大信息增益, 因此以该属性作为划分属性。类似地, 对每个分支节点进行上述操作, 即可得最终决策树。图1、图2分别为ID3算法和AFIV-ID3算法构建的决策树<citation id="170" type="reference"><link href="59" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>。</p>
                </div>
                <div class="area_img" id="128">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201902048_12800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 ID3算法构建的决策树" src="Detail/GetImg?filename=images/JSJC201902048_12800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 ID3算法构建的决策树  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201902048_12800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="129">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201902048_12900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 AFIV-ID3算法构建的决策树" src="Detail/GetImg?filename=images/JSJC201902048_12900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 AFIV-ID3算法构建的决策树  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201902048_12900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="130">由图1和图2可知, AFIV-ID3算法有效地解决了ID3算法多值偏向的缺点, 且针对同一数据集构建的决策树, AFIV-ID3得到的叶子结点更少, 决策树更简单, 因此构造更加合理。</p>
                </div>
                <div class="p1">
                    <p id="131">步骤2首先将AFIV-ID3算法与文献<citation id="171" type="reference">[<a class="sup">5</a>]</citation>算法作分类对比, 实验选取文献<citation id="172" type="reference">[<a class="sup">5</a>]</citation>使用的tic-tac-toe和glass数据集, 原有测试结果以文献<citation id="173" type="reference">[<a class="sup">5</a>]</citation>为准;然后选取4组标准UCI数据集进行测试, 分别为zoo、iris、nursery、car, 分类比较函数选取F-measure值:</p>
                </div>
                <div class="area_img" id="132">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJC201902048_13200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="133">其中, P表示精确率, R表示召回率, β表示为参数, 当β=1时即为F1-measure公式<citation id="174" type="reference"><link href="61" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="134">按照文献<citation id="175" type="reference">[<a class="sup">5</a>]</citation>实验, 将测试集和训练集比例按照2∶1随机划分, 进行分类测试。表2和图3为4种算法在相同情况下的实验对比结果。</p>
                </div>
                <div class="area_img" id="135">
                                            <p class="img_tit">
                                                表2 4种算法分类精确度结果对比
                                                
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201902048_13500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/JSJC201902048_13500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                <p class="img_note">%</p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201902048_13500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表2 4种算法分类精确度结果对比" src="Detail/GetImg?filename=images/JSJC201902048_13500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>
                <div class="area_img" id="136">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201902048_13600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 4种算法分类精确度对比柱状图" src="Detail/GetImg?filename=images/JSJC201902048_13600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 4种算法分类精确度对比柱状图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201902048_13600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="137">该实验结果表明, 在同等情况下, AFIV-ID3算法的分类精度高于经典的ID3算法、C4.5算法和文献<citation id="176" type="reference">[<a class="sup">5</a>]</citation>算法。</p>
                </div>
                <div class="p1">
                    <p id="138">然后, 实验将4组标准UCI数据集均以3∶2的比例划分为训练集和测试集, 并与ID3和C4.5算法作对比。在测试前, 数据集zoo的属性重要度值设置为w (hair) =0.25, w (tail) =0.25, w (egg) =0.20, w (aquatic) =0.20;数据集iris不进行设置;数据集nursery设置为w (health) =0.25, w (parents) =0.20, w (finance) =0.20;car数据集设置为w (safety) =0.30, w (person) =0.10。UCI数据集实验结果如图4和表3所示。</p>
                </div>
                <div class="area_img" id="139">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201902048_13900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 UCI数据集分类精确度对比雷达图" src="Detail/GetImg?filename=images/JSJC201902048_13900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 UCI数据集分类精确度对比雷达图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201902048_13900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="140">
                                            <p class="img_tit">
                                                表3 UCI数据集分类精确度结果对比
                                                
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201902048_14000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/JSJC201902048_14000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                <p class="img_note">%</p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201902048_14000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表3 UCI数据集分类精确度结果对比" src="Detail/GetImg?filename=images/JSJC201902048_14000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>
                <div class="p1">
                    <p id="141">4组标准UCI数据集的测试结果表明, AFIV-ID3算法能够在一定程度上提升数据集的分类精度, 且分类效果较理想。</p>
                </div>
                <h4 class="anchor-tag" id="142" name="142">3.2 井漏数据实验</h4>
                <div class="p1">
                    <p id="143">经验法和传统分类算法对井漏类型分类结果都不理想, AFIV-ID3算法在本文选定的标准数据集上表现较为优秀, 因此使用该算法对井漏类型进行分类。</p>
                </div>
                <div class="p1">
                    <p id="144">井漏类型诊断数据是在日常钻井过程中收集的数据, 数据量较为庞大且存在噪声。表4为补全的井漏类型诊断数据片段, 其中属性分别为:漏失通道性质参数FPC, 地层孔隙压力P<sub>孔</sub>, 立管压力P<sub>立</sub>, 套管压力P<sub>套</sub>, 钻井液密度ρ, 钻井液粘度FV, 钻井液静切力τ<sub>0</sub>, 泵压P<sub>泵</sub>, 钻具外径D<sub>1</sub>, 井眼直径D<sub>2</sub>, 下钻速度v<sub>钻</sub>, 钻井液排量Q, TYPE{0-正常、无漏失, 1-渗透性漏失, 2-裂缝性漏失, 3-溶洞性漏失, 4-破裂性漏失}。</p>
                </div>
                <div class="area_img" id="145">
                                            <p class="img_tit">
                                                表4 井漏类型诊断数据片段
                                                
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201902048_14500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/JSJC201902048_14500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201902048_14500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表4 井漏类型诊断数据片段" src="Detail/GetImg?filename=images/JSJC201902048_14500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>
                <div class="p1">
                    <p id="146">将表4的数据离散化后, 使用AFIV-ID3算法进行分类并与传统算法、文献<citation id="177" type="reference">[<a class="sup">5</a>]</citation>算法作对比。在井漏数据的属性中FPC、FV、τ<sub>0</sub>及钻井液排量Q对判断井漏类型起至关重要的作用, 但若直接用其构造决策树并分类, 效果并不理想, 因此实验对上述4个属性赋予属性重要度值:w (FPC) =0.20, w (FV) =w (τ<sub>0</sub>) =w (Q) =0.10。为更加精准地对结果进行评价, 在分类时设置10%的交叉验证, 实验结果如表5和图5所示。</p>
                </div>
                <div class="area_img" id="147">
                                            <p class="img_tit">
                                                表5 井漏类型诊断数据分类结果对比
                                                
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201902048_14700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/JSJC201902048_14700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                <p class="img_note">%</p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201902048_14700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表5 井漏类型诊断数据分类结果对比" src="Detail/GetImg?filename=images/JSJC201902048_14700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>
                <div class="area_img" id="148">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201902048_14800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 井漏类型诊断数据分类结果对比折线图" src="Detail/GetImg?filename=images/JSJC201902048_14800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 井漏类型诊断数据分类结果对比折线图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201902048_14800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="149">由表5和图5可以看出, 经验法得到的分类结果极不稳定, ID3算法精度较低, C4.5算法精度也稍低于AFIV-ID3, AFIV-ID3算法精度平均约为72.23%, 明显高于传统算法。实验中原始ID3算法和文献<citation id="178" type="reference">[<a class="sup">5</a>]</citation>算法的分类效果很不理想, 主要是因为在井漏类型数据预处理后, 为保证数据的准确性, 离散化的数据取值依然较多。由于ID3算法存在多值偏向的问题, 导致这些多值数据在构造决策树时会构造出不利于分类的决策树, 文献<citation id="179" type="reference">[<a class="sup">5</a>]</citation>算法因忽略信息增益, 导致分类精确度不高。AFIV-ID3算法很好地解决了该问题, 因此精度有了明显的提高。</p>
                </div>
                <div class="p1">
                    <p id="150">井漏类型分类具有重要的现实意义, 但由于原始井漏数据不利于分类, 因此在实现分类前需要做大量的数据预处理工作。因为井漏数据本身也存在一些问题, 即使使用AFIV-ID3算法对井漏类型数据进行分类, 分类精度依旧不是很高, 不过相对于传统的经验法, AFIV-ID3算法得到的井漏类型分类更为准确。</p>
                </div>
                <h3 id="151" name="151" class="anchor-tag">4 结束语</h3>
                <div class="p1">
                    <p id="152">为高效合理地对井漏类型诊断数据进行分类, 本文提出AFIV-ID3算法。该算法在原始计算信息增益的基础上引入属性重要度值和关联度函数比, 以计算新的信息增益。利用新的信息增益来构建决策树, 解决ID3算法多值偏向问题, 同时数据集中较少却重要的属性也有了一定的体现。在标准UCI数据集和井既能构建出更加合理的决策树, 又能有效地提升对数漏类型诊断数据集的实验结果表明, AFIV-ID3算法据的分类精度, 在实际井漏类型判断中, 该算法的分类结果相对于经验判断更加具有参考价值。下一步将研究如何进一步提高该算法的分类性能, 并将其应用于更多的领域。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="33">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=1014415698.nh&amp;v=MDk2NzhRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJuRnlqa1dyL05WRjI2R3JlNUc5ZkZwNUViUEk=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b>蔡汶君.基于神经网络融合技术的钻井井漏诊断模型研究[D].成都:西南石油大学, 2014.
                            </a>
                        </p>
                        <p id="35">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JHSX201312024&amp;v=MTcxMjg2ajU0TzN6cXFCdEdGckNVUkxPZVplUm5GeWprV3IvTkx5WFlkckc0SDlMTnJZOUhZSVFLREg4NHZSNFQ=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b>徐哲, 李建, 王兵, 等.基于贝叶斯网络的钻井井漏问题研究[J].石油天然气学报, 2013, 35 (12) :125-129.
                            </a>
                        </p>
                        <p id="37">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00001337810&amp;v=MDM1OTN0SE5ySXhDYk9vUFkzazV6QmRoNGo5OVNYcVJyeG94Y01IN1I3cWVidWR0RkNIbFU3N0JJRnM9Tmo3QmFyTzRI&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b>QUINLAN J R.Induction of decision trees[J].Machine Learning, 1986, 1 (1) :81-106.
                            </a>
                        </p>
                        <p id="39">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Induction of decision trees">

                                <b>[4]</b>WAGACHA P W.Induction of decision trees[EB/OL].[2017-11-28].http://erepository.uonbi.ac.ke/bitstream/handle/11295/44263/decisionTrees.pdf?sequence=1.
                            </a>
                        </p>
                        <p id="41">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY200511062&amp;v=MDczNzBSbkZ5amtXci9OTHo3QmQ3RzRIdFROcm85RFpvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmU=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b>韩松来, 张辉, 周华平.基于关联度函数的决策树分类算法[J].计算机应用, 2005, 25 (11) :2655-2657.
                            </a>
                        </p>
                        <p id="43">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=2006126645.nh&amp;v=MjI5OTBmSXFwRWJQSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm5GeWprV3IvTlYxMjdHTEs2R04=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b>韩松来.基于关联度函数的决策树分类算法研究[D].长沙:国防科学技术大学, 2005.
                            </a>
                        </p>
                        <p id="45">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=An improved ID3algorithm based on attribute importance-weighted">

                                <b>[7]</b>LUO H, CHEN Y, ZHANG W.An improved ID3algorithm based on attribute importance-weighted[C]//Proceedings of the 2nd International Workshop on Database Technology and Applications.Washington D.C., USA:IEEE Press, 2010:1-4.
                            </a>
                        </p>
                        <p id="47">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC200906030&amp;v=MzExOTRqa1dyL05MejdCYmJHNEh0ak1xWTlHWklRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJuRnk=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b>陆秋, 程小辉.基于属性相似度的决策树算法[J].计算机工程, 2009, 35 (6) :82-84.
                            </a>
                        </p>
                        <p id="49">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=HEFE200706005&amp;v=MDcwNjllUm5GeWprV3IvTkxTak5hN0c0SHRiTXFZOUZZWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVo=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b>胡学钢, 李楠.基于属性重要度的随机决策树学习算法[J].合肥工业大学学报 (自然科学版) , 2007, 30 (6) :681-685.
                            </a>
                        </p>
                        <p id="51">
                            <a id="bibliography_10" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201113021&amp;v=MjMzMDg2ajU0TzN6cXFCdEdGckNVUkxPZVplUm5GeWprV3IvTkx6N0JiYkc0SDlETnJJOUhaWVFLREg4NHZSNFQ=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[10]</b>张琳, 陈燕, 李桃迎, 等.决策树分类算法研究[J].计算机工程, 2011, 37 (13) :66-67.
                            </a>
                        </p>
                        <p id="53">
                            <a id="bibliography_11" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJZ201407060&amp;v=Mjg0ODZPZVplUm5GeWprV3IvTkx6N0JkTEc0SDlYTXFJOURaSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkw=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[11]</b>郝胜轩, 宋宏, 周晓锋.基于近邻噪声处理的KNN缺失数据填补算法[J].计算机仿真, 2014, 31 (7) :264-268.
                            </a>
                        </p>
                        <p id="55">
                            <a id="bibliography_12" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=SJSJ201109038&amp;v=MTQ1ODFpZllaTEc0SDlETXBvOUdiSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm5GeWprV3IvTk4=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[12]</b>王小巍, 蒋玉明.决策树ID3算法的分析与改进[J].计算机工程与设计, 2011, 32 (9) :3069-3072.
                            </a>
                        </p>
                        <p id="57">
                            <a id="bibliography_13" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CBBD&amp;filename=9787121273674000&amp;v=MDM4NDN6R2JLNkg5UExySWxDWU9zUERCTTh6eFVTbURkOVNIN24zeEU5ZmJ2bktyaWZadTl1RkN2c1U3L0pLVjRSWEZx&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[13]</b>郑捷.机器学习算法原理与编程实践[M].北京:电子工业出版社, 2015.
                            </a>
                        </p>
                        <p id="59">
                            <a id="bibliography_14" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CBBD&amp;filename=9787302423287000&amp;v=MjQ4MTl1RkN2c1U3L0pLVjRSWEZxekdiQzRITlhPckkxTlkrc1BEQk04enhVU21EZDlTSDduM3hFOWZidm5LcmlmWnU5&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[14]</b>周志华.机器学习[M].北京:清华大学出版社, 2016.
                            </a>
                        </p>
                        <p id="61">
                            <a id="bibliography_15" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201804043&amp;v=MjYyNDVSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm5GeWprV3IvTkx6N0JiYkc0SDluTXE0OUJaNFFLREg4NHY=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[15]</b>温雪岩, 陈家男, 景维鹏, 等.面向不平衡数据集分类模型的优化研究[J].计算机工程, 2018, 44 (4) :268-273, 293.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJC201902048" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201902048&amp;v=Mjg1MjM0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSbkZ5amtXci9OTHo3QmJiRzRIOWpNclk5QmJJUUtESDg=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="4" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
