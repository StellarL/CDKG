<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637130615416368750%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJC201904041%26RESULT%3d1%26SIGN%3dK63vF0beLjTxf5N87yHxEBMzgm0%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201904041&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201904041&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201904041&amp;v=MTk2NjhaZVJvRnkvZ1U3M1BMejdCYmJHNEg5ak1xNDlCWllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2U=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#45" data-title="0 概述 ">0 概述</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#50" data-title="1 显著目标检测算法 ">1 显著目标检测算法</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#53" data-title="1.1 目标语义检测模型">1.1 目标语义检测模型</a></li>
                                                <li><a href="#66" data-title="1.2 显著目标优化">1.2 显著目标优化</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#79" data-title="2 实验结果与分析 ">2 实验结果与分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#80" data-title="2.1 评价指标与数据集">2.1 评价指标与数据集</a></li>
                                                <li><a href="#83" data-title="2.2 结果分析">2.2 结果分析</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#102" data-title="3 结束语 ">3 结束语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#52" data-title="&lt;b&gt;图1 显著目标检测框架&lt;/b&gt;"><b>图1 显著目标检测框架</b></a></li>
                                                <li><a href="#56" data-title="&lt;b&gt;图2 目标语义检测的FCNN结构&lt;/b&gt;"><b>图2 目标语义检测的FCNN结构</b></a></li>
                                                <li><a href="#65" data-title="&lt;b&gt;图3 不同层输出的部分特征图对比&lt;/b&gt;"><b>图3 不同层输出的部分特征图对比</b></a></li>
                                                <li><a href="#77" data-title="&lt;b&gt;图4 语义显著图、最终显著图的对比效果&lt;/b&gt;"><b>图4 语义显著图、最终显著图的对比效果</b></a></li>
                                                <li><a href="#89" data-title="&lt;b&gt;图5 10种算法在不同数据集上的PR曲线对比&lt;/b&gt;"><b>图5 10种算法在不同数据集上的PR曲线对比</b></a></li>
                                                <li><a href="#93" data-title="&lt;b&gt;表1 10种算法在3个数据集上的F-measure对比&lt;/b&gt;"><b>表1 10种算法在3个数据集上的F-measure对比</b></a></li>
                                                <li><a href="#98" data-title="&lt;b&gt;表2 10种算法在3个数据集上的&lt;i&gt;MAE&lt;/i&gt;值对比&lt;/b&gt;"><b>表2 10种算法在3个数据集上的<i>MAE</i>值对比</b></a></li>
                                                <li><a href="#100" data-title="&lt;b&gt;图6 10种算法显著性检测结果对比&lt;/b&gt;"><b>图6 10种算法显著性检测结果对比</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" title=" SHEHNAZ M, NAVEEN N.An object recognition algorithm with structure-guided saliency detection and SVM classifier[C]//Proceedings of 2015 International Conference on Power, Instrumentation, Control and Computing.Washington D.C., USA:IEEE Press, 2015:1-4." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=An object recognition algorithm with structure-guided saliency detection and SVM classifier">
                                        <b>[1]</b>
                                         SHEHNAZ M, NAVEEN N.An object recognition algorithm with structure-guided saliency detection and SVM classifier[C]//Proceedings of 2015 International Conference on Power, Instrumentation, Control and Computing.Washington D.C., USA:IEEE Press, 2015:1-4.
                                    </a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" title=" ZHANG F, DU B, ZHANG L.Saliency-guided unsupervised feature learning for scene classification[J].IEEE Tran-sactions on Geoscience and Remote Sensing, 2015, 53 (4) :2175-2184." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Saliency-guided unsupervised feature learning for scene classification">
                                        <b>[2]</b>
                                         ZHANG F, DU B, ZHANG L.Saliency-guided unsupervised feature learning for scene classification[J].IEEE Tran-sactions on Geoscience and Remote Sensing, 2015, 53 (4) :2175-2184.
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title=" ZHAO R, OUYANG W, WANG X.Unsupervised salience learning for person re-identification[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2013:3586-3593." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Unsupervised salience learning for person re-identification">
                                        <b>[3]</b>
                                         ZHAO R, OUYANG W, WANG X.Unsupervised salience learning for person re-identification[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2013:3586-3593.
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" title=" GUO D, TANG J, CUI Y, et al.Saliency-based content-aware lifestyle image mosaics[J].Journal of Visual Communication and Image Representation, 2015, 26:192-199." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14121200075626&amp;v=MjQ0ODRmYks4SDlQTnJZOUZaT3dLQ240L29CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVUxuSUoxc1VhQlU9TmlmTw==&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                         GUO D, TANG J, CUI Y, et al.Saliency-based content-aware lifestyle image mosaics[J].Journal of Visual Communication and Image Representation, 2015, 26:192-199.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" title=" 冯松鹤, 郎丛妍, 须德.一种融合图学习与区域显著性分析的图像检索算法[J].电子学报, 2011, 39 (10) :2288-2294." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZXU201110014&amp;v=MDgyODFUZlRlN0c0SDlETnI0OUVZSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm9GeS9nVTczUEk=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                         冯松鹤, 郎丛妍, 须德.一种融合图学习与区域显著性分析的图像检索算法[J].电子学报, 2011, 39 (10) :2288-2294.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" title=" SHEN X, WU Y.A unified approach to salient object detection via low rank matrix recovery[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2012:853-860." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A unified approach to salient object detection via low rank matrix recovery">
                                        <b>[6]</b>
                                         SHEN X, WU Y.A unified approach to salient object detection via low rank matrix recovery[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2012:853-860.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" title=" ZHU W, LIANG S, WEI Y, et al.Saliency optimization from robust background detection[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2014:2814-2821." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Saliency optimization from robust background detection">
                                        <b>[7]</b>
                                         ZHU W, LIANG S, WEI Y, et al.Saliency optimization from robust background detection[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2014:2814-2821.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" title=" 袁巧, 程艳芬, 陈先桥.多先验特征与综合对比度的图像显著性检测[J].中国图象图形学报, 2018, 23 (2) :239-248." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZGTB201802009&amp;v=MDQ3MzQvZ1U3M1BQeXJmYkxHNEg5bk1yWTlGYllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJvRnk=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                         袁巧, 程艳芬, 陈先桥.多先验特征与综合对比度的图像显著性检测[J].中国图象图形学报, 2018, 23 (2) :239-248.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" title=" WANG L, LU H, RUAN X, et al.Deep networks for saliency detection via local estimation and global search[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2015:3183-3192." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep networks for saliency detection via localestimation and global search">
                                        <b>[9]</b>
                                         WANG L, LU H, RUAN X, et al.Deep networks for saliency detection via local estimation and global search[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2015:3183-3192.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_10" title=" 高东东, 张新生.基于空间卷积神经网络模型的图像显著性检测[J].计算机工程, 2018, 44 (5) :246-251." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201805040&amp;v=MDAwNTN5L2dVNzNQTHo3QmJiRzRIOW5NcW85QlpJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSb0Y=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[10]</b>
                                         高东东, 张新生.基于空间卷积神经网络模型的图像显著性检测[J].计算机工程, 2018, 44 (5) :246-251.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_11" title=" KIM J, PAVLOVIC V.A shape preserving approach for salient object detection using convolutional neural networks[C]//Proceedings of the 23rd International Con-ference on Pattern Recognition.Washington D.C., USA:IEEE Press, 2016:609-614." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A shape preserving approach for salient object detection using convolutional neural networks">
                                        <b>[11]</b>
                                         KIM J, PAVLOVIC V.A shape preserving approach for salient object detection using convolutional neural networks[C]//Proceedings of the 23rd International Con-ference on Pattern Recognition.Washington D.C., USA:IEEE Press, 2016:609-614.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_12" title=" LONG J, SHELHAMER E, DARRELL T.Fully convolutional networks for semantic segmentation[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2015:3431-3440." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fully convolutional netw orks for semantic segmentation">
                                        <b>[12]</b>
                                         LONG J, SHELHAMER E, DARRELL T.Fully convolutional networks for semantic segmentation[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2015:3431-3440.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_13" title=" LIU T, SUN J, ZHENG N N, et al.Learning to detect a salient object[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2007:1-8." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning to detect a salient ob-ject">
                                        <b>[13]</b>
                                         LIU T, SUN J, ZHENG N N, et al.Learning to detect a salient object[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2007:1-8.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_14" title=" YANG C, ZHANG L, LU H, et al.Saliency detection via graph-based manifold ranking[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2013:3166-3173." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Saliency detection via graph-based manifold ranking">
                                        <b>[14]</b>
                                         YANG C, ZHANG L, LU H, et al.Saliency detection via graph-based manifold ranking[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2013:3166-3173.
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_15" title=" YAN Q, XU L, SHI J, et al.Hierarchical saliency detec-tion[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2013:1155-1162." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Hierarchical saliency detection">
                                        <b>[15]</b>
                                         YAN Q, XU L, SHI J, et al.Hierarchical saliency detec-tion[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2013:1155-1162.
                                    </a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_16" title=" ALPERT S, GALUN M, BASRI R, et al.Image segmentation by probabilistic bottom-up aggregation and cue integration[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2012, 34 (2) :315-327." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image Segmentation by Probabilistic Bottom-Up Aggregation and Cue Integration">
                                        <b>[16]</b>
                                         ALPERT S, GALUN M, BASRI R, et al.Image segmentation by probabilistic bottom-up aggregation and cue integration[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2012, 34 (2) :315-327.
                                    </a>
                                </li>
                                <li id="35">


                                    <a id="bibliography_17" title=" ACHANTA R, S&#220;SSTRUNK S.Saliency detection using maximum symmetric surround[C]//Proceedings of IEEE International Conference on Image Processing.Washington D.C., USA:IEEE Press, 2010:2653-2656." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Saliency detection using maxi-mum symmetric surround">
                                        <b>[17]</b>
                                         ACHANTA R, S&#220;SSTRUNK S.Saliency detection using maximum symmetric surround[C]//Proceedings of IEEE International Conference on Image Processing.Washington D.C., USA:IEEE Press, 2010:2653-2656.
                                    </a>
                                </li>
                                <li id="37">


                                    <a id="bibliography_18" title=" MARGOLIN R, TAL A, ZELNIK-MANOR L.What makes a patch distinct[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2013:1139-1146." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=What makes a patch distinct?">
                                        <b>[18]</b>
                                         MARGOLIN R, TAL A, ZELNIK-MANOR L.What makes a patch distinct[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2013:1139-1146.
                                    </a>
                                </li>
                                <li id="39">


                                    <a id="bibliography_19" title=" YANG C, ZHANG L, LU H.Graph-regularized saliency detection with convex-hull-based center prior [J].IEEE Signal Processing Letters, 2013, 20 (7) :637-640." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Graph-regularized Saliency Detection withConvex-hull-based Center Prior">
                                        <b>[19]</b>
                                         YANG C, ZHANG L, LU H.Graph-regularized saliency detection with convex-hull-based center prior [J].IEEE Signal Processing Letters, 2013, 20 (7) :637-640.
                                    </a>
                                </li>
                                <li id="41">


                                    <a id="bibliography_20" title=" JIANG B, ZHANG L, LU H, et al.Saliency detection via absorbing markov chain[C]//Proceedings of IEEE International Conference on Computer Vision.Washington D.C., USA:IEEE Press, 2013:1665-1672." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Saliency Detection via Absorbing Markov Chain">
                                        <b>[20]</b>
                                         JIANG B, ZHANG L, LU H, et al.Saliency detection via absorbing markov chain[C]//Proceedings of IEEE International Conference on Computer Vision.Washington D.C., USA:IEEE Press, 2013:1665-1672.
                                    </a>
                                </li>
                                <li id="43">


                                    <a id="bibliography_21" title=" JIANG H, WANG J, YUAN Z, et al.Salient object detection:a discriminative regional feature integration approach [C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2013:2083-2090." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Salient object detection:A discriminative regional feature integration approach">
                                        <b>[21]</b>
                                         JIANG H, WANG J, YUAN Z, et al.Salient object detection:a discriminative regional feature integration approach [C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2013:2083-2090.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJC" target="_blank">计算机工程</a>
                2019,45(04),248-253 DOI:10.19678/j.issn.1000-3428.0052350            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>语义信息引导下的显著目标检测算法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%82%96%E9%94%8B&amp;code=10931532&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">肖锋</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%8E%E8%8C%B9%E5%A8%9C&amp;code=41128563&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">李茹娜</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E8%A5%BF%E5%AE%89%E5%B7%A5%E4%B8%9A%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0201951&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">西安工业大学计算机科学与工程学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>针对现有显著性检测方法在凸显目标完整性和背景噪声抑制方面性能较差的问题, 提出一种显著目标检测算法。构建改进的全卷积神经网络, 捕获图像中的语义信息, 生成高层语义初步显著图。利用语义知识引导流形排序进行优化, 实现显著目标的边缘传播。融合不同尺度下的显著图, 完成显著目标检测。在ECSSD、DUT-OMRON、SED2数据集上进行实验, 结果表明, 与最大对称环绕、主成分分析等算法相比, 该算法检测出的显著目标更加完整, 在复杂场景下检测结果鲁棒性更好。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%98%BE%E8%91%97%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">显著目标检测;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%AF%AD%E4%B9%89%E4%BF%A1%E6%81%AF&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">语义信息;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B5%81%E5%BD%A2%E6%8E%92%E5%BA%8F&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">流形排序;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%85%A8%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">全卷积神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%9B%AE%E6%A0%87%E6%84%9F%E7%9F%A5&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">目标感知;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    肖锋 (1976—) , 男, 教授、博士, 主研方向为图像处理、视觉注意机制、场景语义理解、智能信息处理, E-mail:xffriends@163.com;;
                                </span>
                                <span>
                                    李茹娜, 硕士研究生。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-08-08</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金 (61572392, 61671362);</span>
                                <span>陕西省自然科学基础研究面上项目 (2017JC2-08);</span>
                    </p>
            </div>
                    <h1><b>Salient Object Detection Algorithm Under Guidance of Semantic Information</b></h1>
                    <h2>
                    <span>XIAO Feng</span>
                    <span>LI Runa</span>
            </h2>
                    <h2>
                    <span>School of Computer Science and Engineering, Xi'an Technological University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>In view of the poor performance of the existing saliency detection method in object highlighting and background noise suppression, a salient object detection algorithm is proposed.An improved Fully Convolutional Neural Network (FCNN) is constructed to capture the semantic information in the image, which can generate the high-level semantic preliminary saliency maps.The semantic knowledge is used to optimize the Manifold Ranking (MR) , and then achieve the edge propagation of salient object.The saliency maps under different scales are fused to realize salient object detection.Experiments are carried out on ECSSD, DUT-OMRON and SED2 datasets, the results show that compared with algorithms such as Maximum Symmetric Surround (MSS) and Principal Component Analysis (PCA) , the salient objects detected by the algorithm in this paper are more complete and the detection results are more robust in complex scenes.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=salient%20object%20detection&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">salient object detection;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=semantic%20information&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">semantic information;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Manifold%20Ranking%20(MR)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Manifold Ranking (MR) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Fully%20Convolutional%20Neural%20Network%20(FCNN)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Fully Convolutional Neural Network (FCNN) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=target%20perception&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">target perception;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-08-08</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="45" name="45" class="anchor-tag">0 概述</h3>
                <div class="p1">
                    <p id="46">模拟生物视觉注意机制的显著性检测技术作为计算机视觉具有代表性的技术之一, 旨在发现和定位图像中与人类感知相一致的区域, 从而有效地服务于多种视觉任务, 如目标检测与识别<citation id="104" type="reference"><link href="3" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>、场景分类<citation id="105" type="reference"><link href="5" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>、行人再识别<citation id="106" type="reference"><link href="7" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>、照片拼接<citation id="107" type="reference"><link href="9" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>、图像检索<citation id="108" type="reference"><link href="11" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>等。虽然该技术已取得很大进步, 但在复杂场景下仍面临较大的挑战。</p>
                </div>
                <div class="p1">
                    <p id="47">现有的显著目标检测技术一般通过图像底层特征和先验知识进行建模。底层特征主要包括颜色、纹理、亮度、方向等, 先验知识包括颜色先验、中心先验、背景先验、边界先验、对比度先验等。文献<citation id="130" type="reference">[<a class="sup">6</a>]</citation>在低秩矩阵重建的基础上, 融入中心先验、颜色先验和人脸检测以提高显著性检测的结果。文献<citation id="131" type="reference">[<a class="sup">7</a>]</citation>将图像边界上的区域假定为背景区域, 通过对比其他区域与边界区域的差异大小来衡量显著性。文献<citation id="132" type="reference">[<a class="sup">8</a>]</citation>在全局对比度特征和局部对比度特征的基础上, 融合多种先验知识来计算显著性。然而, 手动选择特征进行整合需要精细的设计, 不具有完备性, 不能完整地突出显著性区域。各种先验知识在特定场景下可能会失效, 当算法应用于不同类型的图像时, 性能不稳定, 使用范围较为局限。此外, 现有检测算法缺少高层语义知识, 当场景足够复杂或显著目标和背景对比度低时, 检测结果不够准确。</p>
                </div>
                <div class="p1">
                    <p id="48">卷积神经网络 (Convolutional Neural Network, CNN) 能够自动学习图像层次化的抽象特征, 更有利于图像的理解和数据本质的表达。文献<citation id="133" type="reference">[<a class="sup">9</a>]</citation>从局部估计和全局搜索的角度设计了2个神经网络来进行显著区域检测。文献<citation id="134" type="reference">[<a class="sup">10</a>]</citation>提出一个空间卷积特征学习的模型来进行显著性检测。文献<citation id="135" type="reference">[<a class="sup">11</a>]</citation>将由卷积神经网络驱动的形状预测与中、低层图像信息相结合, 实现显著目标检测。图像中引起视觉注意的目标具有丰富的语义信息, 如人们首先会注意到场景中的行人、车辆、动植物等。将高层语义知识应用于显著目标检测, 模拟人对目标物体的感知, 能使显著性检测的结果更加准确。</p>
                </div>
                <div class="p1">
                    <p id="49">近年来, 语义分割领域发展迅猛, 受全卷积神经网络 (Fully Convolutional Neural Network, FCNN) <citation id="115" type="reference"><link href="25" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>的启发, 本文提出一种语义引导下的显著目标检测算法。采用数据驱动的方式捕捉显著目标的潜在语义特性及其与背景间的差异, 提取图像中的语义对象, 得到粗粒度的显著图。针对语义显著图中目标边缘区域模糊的问题, 利用目标感知的结果引导流形排序 (Manifold Ranking, MR) <citation id="116" type="reference"><link href="27" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>来进行优化, 得到具有清晰边缘的显著图。最后对各部分显著图进行融合, 得到细粒度显著图。</p>
                </div>
                <h3 id="50" name="50" class="anchor-tag">1 显著目标检测算法</h3>
                <div class="p1">
                    <p id="51">语义信息引导下的显著目标检测模型框架如图1所示, 其主要包括利用FCNN生成语义显著图和采用流形排序算法优化2个阶段。</p>
                </div>
                <div class="area_img" id="52">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201904041_052.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 显著目标检测框架" src="Detail/GetImg?filename=images/JSJC201904041_052.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图1 显著目标检测框架</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201904041_052.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="53" name="53">1.1 目标语义检测模型</h4>
                <h4 class="anchor-tag" id="54" name="54">1.1.1 网络结构</h4>
                <div class="p1">
                    <p id="55">CNN具有自动学习图像特征的能力, 被广泛应用于图像分类、图像检测与识别等任务中。为了有效解决像素级的预测问题, 将传统CNN转化为FCNN, 使得网络可以获取任意大小的输入图像。构造图2所示的FCNN结构, 网络训练可以直接将输入图像映射为具有语义特征的显著图。</p>
                </div>
                <div class="area_img" id="56">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201904041_056.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 目标语义检测的FCNN结构" src="Detail/GetImg?filename=images/JSJC201904041_056.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图2 目标语义检测的FCNN结构</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201904041_056.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="57">在图2中, 网络前13层类似于VGG-16, 包含13个卷积层, 卷积核大小为3×3, 每个卷积层带有线性修正单元ReLU, 共经过5次池化操作。Conv6和Conv7的卷积核大小分别为7×7和1×1, 然后在其后接一个1×1大小的卷积核用于输出预测显著得分图。由于网络中存在5个空间池化层, 对于一幅输入大小为<i>W</i>×<i>H</i>的图像, 经过5次卷积核大小为2×2、步长为2的池化操作后, 其特征映射大小为<mathml id="58"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mi>W</mi><mo>×</mo><mi>Η</mi></mrow><mrow><mn>2</mn><msup><mrow></mrow><mn>5</mn></msup></mrow></mfrac></mrow></math></mathml>。为了解决网络输出分辨率低的问题, 添加反卷积操作来对上一层输出的特征图进行上采样。根据上层输出特征图的大小, 将反卷积操作的卷积核大小设置为63×63, 步长为31, 以便获得与输入图像具有相同分辨率的显著图。</p>
                </div>
                <h4 class="anchor-tag" id="59" name="59">1.1.2 网络训练与测试</h4>
                <div class="p1">
                    <p id="60">利用数据集中的原始图像和对应的标注图像进行有监督学习。给定训练集样本 (<i>I</i>, <i>G</i>) , 其中, <i>I</i>为原始图像, 大小为<i>h</i>×<i>w</i>×3, <i>G</i>为<i>I</i>对应的手工标注真值图<i>G</i>∈{0, 1}<sup><i>h</i>×<i>w</i></sup>。训练过程中的参数<i>θ</i>通过式 (1) 的损失函数来学习。</p>
                </div>
                <div class="p1">
                    <p id="61" class="code-formula">
                        <mathml id="61"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>ℓ</mi><mo stretchy="false"> (</mo><mi>Ρ</mi><mo>, </mo><mi>G</mi><mo stretchy="false">) </mo><mo>=</mo><mo>-</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>h</mi><mo>×</mo><mi>w</mi></mrow></munderover><mo stretchy="false"> (</mo></mstyle><mi>g</mi><msub><mrow></mrow><mi>i</mi></msub><mrow><mi>ln</mi></mrow><mtext> </mtext><mi>p</mi><msub><mrow></mrow><mi>i</mi></msub><mo>+</mo><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>g</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mrow><mi>ln</mi></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>p</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="62">其中, <i>p</i><sub><i>i</i></sub>为经过网络模型后的输出图像像素<i>i</i>属于标签1的概率, (1-<i>p</i><sub><i>i</i></sub>) 为像素<i>i</i>属于标签0的概率。</p>
                </div>
                <div class="p1">
                    <p id="63">网络的训练在MSRA数据集上进行。在训练过程中, 采用随机梯度下降法优化整个网络, 学习率设置为10<sup>-6</sup>, 权重衰减系数为0.000 5, 动量因子为0.99。为了提高网络的泛化性能, 设置dropout操作, 在权重更新过程中, 以0.5的概率丢弃一些神经元。将训练好的VGG网络权重加载到本文模型中, 并随机初始化新增加的卷积层权重。</p>
                </div>
                <div class="p1">
                    <p id="64">在测试时, 输入任意尺寸图像, 网络会输出相同大小的显著目标图。对网络在不同层次输出的部分特征图进行可视化, 其对比效果如图3所示。从图3可以看出较浅的卷积层输出的特征图能够学习到颜色、边缘、纹理等底层特征, 随着网络的加深, 提取的抽象特征逐渐向目标语义信息靠近。</p>
                </div>
                <div class="area_img" id="65">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201904041_065.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 不同层输出的部分特征图对比" src="Detail/GetImg?filename=images/JSJC201904041_065.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图3 不同层输出的部分特征图对比</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201904041_065.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="66" name="66">1.2 显著目标优化</h4>
                <div class="p1">
                    <p id="67">通过网络训练得到的高层语义信息, 对目标物体的定位比较准确, 但损失了部分细节信息, 导致目标边缘模糊。为了解决上述问题, 本文在目标感知结果的基础上, 采用MR方法进一步优化, 将超像素分割的结果映射到初始显著图上, 得到边缘信息更精确的显著图。</p>
                </div>
                <div class="p1">
                    <p id="68">MR采用半监督学习方法, 利用数据之间的内在流形结构进行排序以实现分类。对于输入图像, 使用简单的线性迭代聚类 (Simple Linear Iterative Clustering, SLIC) 算法进行过分割, 得到超像素集合。以超像素为节点构建无向加权图<i>G</i>, 其构图原则是:超像素<i>p</i><sub><i>i</i></sub>与其邻接超像素建立一条无向边, 再向外扩展2层, 即与其邻接超像素的邻接超像素建立一条无向边, 图像的4个边界的超像素两两相连。图<i>G</i>的边由关联矩阵<b><i>W</i></b>来保存, 计算公式如下:</p>
                </div>
                <div class="p1">
                    <p id="69" class="code-formula">
                        <mathml id="69"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi mathvariant="bold-italic">W</mi><mo>=</mo><mrow><mo>{</mo><mrow><mtable><mtr><mtd columnalign="left"><mi>w</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>, </mo><mi>p</mi><msub><mrow></mrow><mi>i</mi></msub><mtext>和</mtext><mi>p</mi><msub><mrow></mrow><mi>j</mi></msub><mtext>之</mtext><mtext>间</mtext><mtext>有</mtext><mtext>边</mtext></mtd></mtr><mtr><mtd columnalign="left"><mn>0</mn><mo>, </mo><mtext>其</mtext><mtext>他</mtext></mtd></mtr></mtable></mrow></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi>w</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mrow><mi>exp</mi></mrow><mrow><mo> (</mo><mrow><mo>-</mo><mfrac><mn>1</mn><mrow><mi>σ</mi><msup><mrow></mrow><mn>2</mn></msup></mrow></mfrac><mo stretchy="false">∥</mo><mi>c</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mi>c</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">∥</mo></mrow><mo>) </mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="70">其中, <i>w</i><sub><i>ij</i></sub>为2个节点之间的权重, <i>c</i><sub><i>i</i></sub>和<i>c</i><sub><i>j</i></sub>为2个节点的超像素在CIE Lab颜色空间中的均值, ‖·‖计算2节点在CIE Lab颜色空间中的距离, <i>σ</i>为控制权重的系数, 本文取10。计算图<i>G</i>的度矩阵<b><i>D</i></b>=diag{<i>d</i><sub>11</sub>, <i>d</i><sub>22</sub>, …, <i>d</i><sub><i>nn</i></sub>}, 其中, <mathml id="71"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>d</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>i</mi></mrow></msub><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>j</mi></munder><mi>w</mi></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow></math></mathml>。</p>
                </div>
                <div class="p1">
                    <p id="72">不同于文献<citation id="136" type="reference">[<a class="sup">13</a>]</citation>前景查询种子的设置, 本文前景查询种子来自语义显著图, 选择初始显著图中显著度高的超像素作为前景查询节点, 具体方法是:判断超像素区域中像素的平均显著值是否大于整幅图像的平均显著值, 若大于, 则超像素标记为1, 否则, 标记为0, 从而得到查询种子<b><i>Y</i></b>=[<i>y</i><sub>1</sub>, <i>y</i><sub>2</sub>, …, <i>y</i><sub><i>n</i></sub>]<sup>T</sup>。获得的前景种子尽可能完整地覆盖了显著目标, 然后通过排序计算, 对显著目标的边缘传播进行优化。通过式 (4) 计算排序得分。</p>
                </div>
                <div class="p1">
                    <p id="73"><b><i>f</i></b><sup>*</sup>= (<b><i>D</i></b>-<i>α</i><b><i>W</i></b>) <sup>-1</sup><i>y</i>      (4) </p>
                </div>
                <div class="p1">
                    <p id="74"><i>p</i><sub><i>i</i></sub>和<i>p</i><sub><i>j</i></sub>的排序得分 <b><i>f</i></b><sub><i>i</i></sub>和 <b><i>f</i></b><sub><i>j</i></sub>越接近, 表示2个超像素间的相似性越大。对排序得分进行归一化, 再将超像素的显著值分配给每个像素可得到优化后的显著图。</p>
                </div>
                <div class="p1">
                    <p id="75">在超像素分割时, 由于显著目标的大小不确定, 不同尺度的超像素会对检测结果产生不同的影响。本文在3个尺度上进行超像素分割, 对应超像素的大小分别为200、300和400, 最终经过流形排序得到3幅显著图<i>S</i><sub>1</sub>、<i>S</i><sub>2</sub>和<i>S</i><sub>3</sub>, 通过式 (5) 进行融合, 最终的显著图如图4所示。</p>
                </div>
                <div class="p1">
                    <p id="76"><i>S</i>=<i>S</i><sub>1</sub>+<i>S</i><sub>2</sub>+<i>S</i><sub>3</sub>      (5) </p>
                </div>
                <div class="area_img" id="77">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201904041_077.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 语义显著图、最终显著图的对比效果" src="Detail/GetImg?filename=images/JSJC201904041_077.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图4 语义显著图、最终显著图的对比效果</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201904041_077.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="78">从图4可以看出, 本文设计的网络能够正确预测显著目标位置, 这对于MR前景种子的选择起指导性作用, 可从一定程度上降低前景估计带来的负面影响。通过MR方法得到的显著目标包含丰富的边缘信息, 如图4 (c) 中的第1幅图像, 花瓣的边缘得到了明显增强。同时, 网络输出显著图图像不完整的情况得到改善, 如图4 (c) 中的第2幅图像, 整个显著目标更加凸显。</p>
                </div>
                <h3 id="79" name="79" class="anchor-tag">2 实验结果与分析</h3>
                <h4 class="anchor-tag" id="80" name="80">2.1 评价指标与数据集</h4>
                <div class="p1">
                    <p id="81">为了定量评估本文方法的性能, 将其在3个广泛使用的数据集上与9种经典的显著性检测方法进行比较, 通过准确率-召回率 (Precision-Recall, PR) 曲线、F值 (F-measure) 和平均绝对误差值 (Mean Absolute Error, MAE) 3种客观评价指标来评测本文算法与其他算法的性能。实验环境的GPU为GTX1080 8 GB显存, 使用Caffe深度学习框架, 编程环境为Python3和Matlab2016a。</p>
                </div>
                <div class="p1">
                    <p id="82">实验中使用的评价数据集为DUT-OMRON<citation id="118" type="reference"><link href="27" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>、ECSSD<citation id="119" type="reference"><link href="29" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>、SED2<citation id="120" type="reference"><link href="31" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>, 3个数据集分别包含5 168幅、1 000幅、100幅图像和对应像素级标注图。具体来说, DUT-OMRON数据集中图像场景语义多样, 图像背景极其复杂;ECSSD数据集是对CSSD数据集的扩充, 包含大量具有语义特征且场景复杂的图像;SED2数据集通常在一幅图像中包含2个显著目标。</p>
                </div>
                <h4 class="anchor-tag" id="83" name="83">2.2 结果分析</h4>
                <div class="p1">
                    <p id="84">为了验证本文方法的有效性和优越性, 将其与MSS (Maximum Symmetric Surround) <citation id="121" type="reference"><link href="33" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>、PCA (Principal Component Analysis) <sup></sup><citation id="122" type="reference"><link href="35" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>、GR (Graph-Regularized) <citation id="123" type="reference"><link href="37" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>、RBD<citation id="124" type="reference"><link href="15" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>、HS<citation id="125" type="reference"><link href="29" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>、MC<citation id="126" type="reference"><link href="39" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>、GMR<citation id="127" type="reference"><link href="27" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>、DRFI<citation id="128" type="reference"><link href="41" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>、MDF<citation id="129" type="reference"><link href="43" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation> 9种显著性检测算法进行比较。具体评价指标如下:</p>
                </div>
                <h4 class="anchor-tag" id="85" name="85">1) PR曲线</h4>
                <div class="p1">
                    <p id="86">设置0～255的灰度值作为阈值, 对显著图进行分割, 用得到的二值图与手工标注的真值图进行比较, 利用式 (6) 和式 (7) 计算出准确率 (<i>P</i>) 和召回率 (<i>R</i>) , 绘制得到PR曲线。</p>
                </div>
                <div class="p1">
                    <p id="87" class="code-formula">
                        <mathml id="87"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>Ρ</mi><mo>=</mo><mfrac><mrow><mstyle displaystyle="true"><mo>∑</mo><mi>S</mi></mstyle><msub><mrow></mrow><mi>g</mi></msub><mo>×</mo><mi>S</mi><msub><mrow></mrow><mi>d</mi></msub></mrow><mrow><mi>S</mi><msub><mrow></mrow><mi>d</mi></msub></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>6</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi>R</mi><mo>=</mo><mfrac><mrow><mstyle displaystyle="true"><mo>∑</mo><mi>S</mi></mstyle><msub><mrow></mrow><mi>g</mi></msub><mo>×</mo><mi>S</mi><msub><mrow></mrow><mi>d</mi></msub></mrow><mrow><mi>S</mi><msub><mrow></mrow><mi>g</mi></msub></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>7</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="88">其中, <i>S</i><sub><i>d</i></sub>表示显著图经过阈值分割后的灰度值, <i>S</i><sub><i>g</i></sub>表示真值图的灰度值。10种方法在3个数据集上的PR曲线如图5所示。总体来说, 本文方法得到的PR曲线具有更高的准确率, 因为深度卷积网络能更好地捕获高层语义信息。</p>
                </div>
                <div class="area_img" id="89">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201904041_089.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 10种算法在不同数据集上的PR曲线对比" src="Detail/GetImg?filename=images/JSJC201904041_089.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图5 10种算法在不同数据集上的PR曲线对比</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201904041_089.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="90" name="90">2) F-measure</h4>
                <div class="p1">
                    <p id="91">运用F-measure来综合准确率和召回率, 使两者之间保持均衡, 进一步验证本文方法的有效性。F-measure的计算选用自适应阈值分割方法, 阈值选取由待测显著图平均灰度值的2倍确定, 计算得到图像的平均准确率和平均召回率, 从而根据式 (8) 计算得到F-measure值。表1为不同算法在3个数据集上的F-measure对比值, F-measure值越大表明性能越好。其他9种对比算法中在ECSSD、DUT-OMRON数据集上效果最好的是MDF算法, 在SED2数据集上效果最好的是RBD算法, 与对比方法中最先进的算法相比, 本文算法在这3个数据集上的F-measure值分别提高了3.0%、9.2%和1.5%。</p>
                </div>
                <div class="p1">
                    <p id="92" class="code-formula">
                        <mathml id="92"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtext>F</mtext><mo>-</mo><mtext>m</mtext><mtext>e</mtext><mtext>a</mtext><mtext>s</mtext><mtext>u</mtext><mtext>r</mtext><mtext>e</mtext><mo>=</mo><mfrac><mrow><mo stretchy="false"> (</mo><mn>1</mn><mo>+</mo><mi>β</mi><msup><mrow></mrow><mn>2</mn></msup><mo stretchy="false">) </mo><mi>Ρ</mi><mo>×</mo><mi>R</mi></mrow><mrow><mi>β</mi><msup><mrow></mrow><mn>2</mn></msup><mo>×</mo><mi>Ρ</mi><mo>+</mo><mi>R</mi></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>8</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="area_img" id="93">
                    <p class="img_tit"><b>表1 10种算法在3个数据集上的F-measure对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="93" border="1"><tr><td>算法</td><td>ECSSD数据集</td><td>DUT-OMRON数据集</td><td>SED2数据集</td></tr><tr><td><br />MSS算法</td><td>0.525</td><td>0.462</td><td>0.733</td></tr><tr><td><br />PCA算法</td><td>0.622</td><td>0.554</td><td>0.748</td></tr><tr><td><br />GR算法</td><td>0.583</td><td>0.544</td><td>0.704</td></tr><tr><td><br />RBD算法</td><td>0.680</td><td>0.580</td><td>0.797</td></tr><tr><td><br />HS算法</td><td>0.657</td><td>0.595</td><td>0.718</td></tr><tr><td><br />MC算法</td><td>0.641</td><td>0.603</td><td>0.681</td></tr><tr><td><br />GMR算法</td><td>0.662</td><td>0.591</td><td>0.789</td></tr><tr><td><br />DRFI算法</td><td>0.733</td><td>0.552</td><td>0.725</td></tr><tr><td><br />MDF算法</td><td>0.803</td><td>0.644</td><td>0.786</td></tr><tr><td><br />本文算法</td><td>0.827</td><td>0.703</td><td>0.809</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="94" name="94">3) 平均绝对误差</h4>
                <div class="p1">
                    <p id="95">为了进行更全面的比较, 采用<i>MAE</i>作为另一个评价标准来反映显著图与真值图的相似程度, 计算公式如下:</p>
                </div>
                <div class="p1">
                    <p id="96" class="code-formula">
                        <mathml id="96"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Μ</mi><mi>A</mi><mi>E</mi><mo>=</mo><mfrac><mn>1</mn><mrow><mi>W</mi><mo>×</mo><mi>Η</mi></mrow></mfrac><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>W</mi></munderover><mrow></mrow></mstyle></mrow><mtext> </mtext><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>Η</mi></munderover><mrow></mrow></mstyle></mrow><mrow><mo>|</mo><mrow><mi>S</mi><mo stretchy="false"> (</mo><mi>i</mi><mo>, </mo><mi>j</mi><mo stretchy="false">) </mo><mo>-</mo><mi>G</mi><mo stretchy="false"> (</mo><mi>i</mi><mo>, </mo><mi>j</mi><mo stretchy="false">) </mo></mrow><mo>|</mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>9</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="97">其中, <i>W</i>和<i>H</i>分别代表图像的宽度和高度, <i>S</i> (<i>i</i>, <i>j</i>) 代表显著图对应的像素值, <i>G</i> (<i>i</i>, <i>j</i>) 代表真值图对应的像素值, <i>MAE</i>值越小, 表明显著图越接近于真值图。表2为10种算法在3个数据集上<i>MAE</i>值的比较结果。从表2可以看出, 本文算法的<i>MAE</i>值和MDF算法最为接近, 并且小于其他算法, 说明本文算法可以对背景信息进行有效抑制。</p>
                </div>
                <div class="area_img" id="98">
                    <p class="img_tit"><b>表2 10种算法在3个数据集上的<i>MAE</i>值对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="98" border="1"><tr><td>算法</td><td>ECSSD数据集</td><td>DUT-OMRON数据集</td><td>SED2数据集</td></tr><tr><td>MSS算法</td><td>0.245</td><td>0.197</td><td>0.192</td></tr><tr><td><br />PCA算法</td><td>0.248</td><td>0.206</td><td>0.201</td></tr><tr><td><br />GR算法</td><td>0.285</td><td>0.249</td><td>0.197</td></tr><tr><td><br />RBD算法</td><td>0.173</td><td>0.181</td><td>0.130</td></tr><tr><td><br />HS算法</td><td>0.228</td><td>0.227</td><td>0.154</td></tr><tr><td><br />MC算法</td><td>0.204</td><td>0.186</td><td>0.182</td></tr><tr><td><br />GMR算法</td><td>0.189</td><td>0.189</td><td>0.163</td></tr><tr><td><br />DRFI算法</td><td>0.171</td><td>0.155</td><td>0.137</td></tr><tr><td><br />MDF算法</td><td>0.108</td><td>0.092</td><td>0.117</td></tr><tr><td><br />本文算法</td><td>0.098</td><td>0.088</td><td>0.101<br /></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="99">为了更为直观地进行评价, 本文选取几幅有代表性的图片进行实验, 并与其他算法进行显著图的比较, 结果如图6所示。第1幅图的显著目标与图像边界相连, 由于GMR算法采用边界先验, 其对于显著目标处于图像边界时的检测不够完整。GR算法虽然检测到显著目标, 但是第1幅图像的检测结果中包含了大量背景信息。本文算法克服了边界先验的缺陷, 同时有效地抑制了背景信息。第2幅、第3幅图像中显著目标和背景部分对比度低, MDF算法可有效提取显著目标区域, 其他几种算法出现大面积的背景噪声, 本文算法有效利用了语义信息, 检测结果相对较好。第4幅图像包含小尺度显著目标, MDF算法出现误检, 本文算法检测结果更为完整。第5幅～第7幅图像场景极其复杂, 除MDF算法外, 其他几种对比算法得到的显著图离散背景信息凸显, 对于第5幅图像, MDF算法检测的显著目标不完整, 本文算法在这几幅图像中检测结果最优。总体来说, 本文算法得到的显著图和手工标注的真值图更为接近, 避免了传统算法对于先验知识的依赖性, 在高亮显著目标的同时, 有效地抑制了背景信息, 在复杂场景下的检测结果鲁棒性更好。</p>
                </div>
                <div class="area_img" id="100">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201904041_100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 10种算法显著性检测结果对比" src="Detail/GetImg?filename=images/JSJC201904041_100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图6 10种算法显著性检测结果对比</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201904041_100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="101">由于深度学习方法需要GPU加速进行网络训练, 因此本文算法的时间复杂度<i>O</i> (<i>n</i>) 由网络提取和流形排序2个部分构成, 运行速度低于一些高效算法。对于流形排序部分, 时间主要消耗在不同尺度超像素的排序上, 随着超像素个数的增加, 相应的排序次数增加。然而, 综合上述评价指标和视觉效果, 本文算法仍具有优势。</p>
                </div>
                <h3 id="102" name="102" class="anchor-tag">3 结束语</h3>
                <div class="p1">
                    <p id="103">本文提出一种语义信息引导下的显著目标检测算法, 突出高层语义信息在显著性检测中的重要性。通过设计FCNN结构提取图像的语义显著图, 在进行MR算法优化时, 有效利用高层语义信息来指导排序。在DUT-OMRON、ECSSD、SED2数据集上, 与其他9种算法进行对比, 结果表明, 本文算法能够有效地利用语义信息, 抑制背景信息, 且其在复杂场景下能保持较好的鲁棒性。下一步将对网络结构进行优化, 充分利用图像的层次化信息以完成显著性检测任务。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=An object recognition algorithm with structure-guided saliency detection and SVM classifier">

                                <b>[1]</b> SHEHNAZ M, NAVEEN N.An object recognition algorithm with structure-guided saliency detection and SVM classifier[C]//Proceedings of 2015 International Conference on Power, Instrumentation, Control and Computing.Washington D.C., USA:IEEE Press, 2015:1-4.
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Saliency-guided unsupervised feature learning for scene classification">

                                <b>[2]</b> ZHANG F, DU B, ZHANG L.Saliency-guided unsupervised feature learning for scene classification[J].IEEE Tran-sactions on Geoscience and Remote Sensing, 2015, 53 (4) :2175-2184.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Unsupervised salience learning for person re-identification">

                                <b>[3]</b> ZHAO R, OUYANG W, WANG X.Unsupervised salience learning for person re-identification[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2013:3586-3593.
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14121200075626&amp;v=MDE5OTlUNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVVMbklKMXNVYUJVPU5pZk9mYks4SDlQTnJZOUZaT3dLQ240L29CTQ==&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b> GUO D, TANG J, CUI Y, et al.Saliency-based content-aware lifestyle image mosaics[J].Journal of Visual Communication and Image Representation, 2015, 26:192-199.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZXU201110014&amp;v=MjY0MThRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJvRnkvZ1U3M1BJVGZUZTdHNEg5RE5yNDlFWUk=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b> 冯松鹤, 郎丛妍, 须德.一种融合图学习与区域显著性分析的图像检索算法[J].电子学报, 2011, 39 (10) :2288-2294.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A unified approach to salient object detection via low rank matrix recovery">

                                <b>[6]</b> SHEN X, WU Y.A unified approach to salient object detection via low rank matrix recovery[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2012:853-860.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Saliency optimization from robust background detection">

                                <b>[7]</b> ZHU W, LIANG S, WEI Y, et al.Saliency optimization from robust background detection[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2014:2814-2821.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZGTB201802009&amp;v=Mjc5Mzh0R0ZyQ1VSTE9lWmVSb0Z5L2dVNzNQUHlyZmJMRzRIOW5Nclk5RmJZUUtESDg0dlI0VDZqNTRPM3pxcUI=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b> 袁巧, 程艳芬, 陈先桥.多先验特征与综合对比度的图像显著性检测[J].中国图象图形学报, 2018, 23 (2) :239-248.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep networks for saliency detection via localestimation and global search">

                                <b>[9]</b> WANG L, LU H, RUAN X, et al.Deep networks for saliency detection via local estimation and global search[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2015:3183-3192.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_10" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201805040&amp;v=MDY5MDBMejdCYmJHNEg5bk1xbzlCWklRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJvRnkvZ1U3M1A=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[10]</b> 高东东, 张新生.基于空间卷积神经网络模型的图像显著性检测[J].计算机工程, 2018, 44 (5) :246-251.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A shape preserving approach for salient object detection using convolutional neural networks">

                                <b>[11]</b> KIM J, PAVLOVIC V.A shape preserving approach for salient object detection using convolutional neural networks[C]//Proceedings of the 23rd International Con-ference on Pattern Recognition.Washington D.C., USA:IEEE Press, 2016:609-614.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fully convolutional netw orks for semantic segmentation">

                                <b>[12]</b> LONG J, SHELHAMER E, DARRELL T.Fully convolutional networks for semantic segmentation[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2015:3431-3440.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning to detect a salient ob-ject">

                                <b>[13]</b> LIU T, SUN J, ZHENG N N, et al.Learning to detect a salient object[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2007:1-8.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Saliency detection via graph-based manifold ranking">

                                <b>[14]</b> YANG C, ZHANG L, LU H, et al.Saliency detection via graph-based manifold ranking[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2013:3166-3173.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Hierarchical saliency detection">

                                <b>[15]</b> YAN Q, XU L, SHI J, et al.Hierarchical saliency detec-tion[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2013:1155-1162.
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image Segmentation by Probabilistic Bottom-Up Aggregation and Cue Integration">

                                <b>[16]</b> ALPERT S, GALUN M, BASRI R, et al.Image segmentation by probabilistic bottom-up aggregation and cue integration[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2012, 34 (2) :315-327.
                            </a>
                        </p>
                        <p id="35">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Saliency detection using maxi-mum symmetric surround">

                                <b>[17]</b> ACHANTA R, SÜSSTRUNK S.Saliency detection using maximum symmetric surround[C]//Proceedings of IEEE International Conference on Image Processing.Washington D.C., USA:IEEE Press, 2010:2653-2656.
                            </a>
                        </p>
                        <p id="37">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=What makes a patch distinct?">

                                <b>[18]</b> MARGOLIN R, TAL A, ZELNIK-MANOR L.What makes a patch distinct[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2013:1139-1146.
                            </a>
                        </p>
                        <p id="39">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Graph-regularized Saliency Detection withConvex-hull-based Center Prior">

                                <b>[19]</b> YANG C, ZHANG L, LU H.Graph-regularized saliency detection with convex-hull-based center prior [J].IEEE Signal Processing Letters, 2013, 20 (7) :637-640.
                            </a>
                        </p>
                        <p id="41">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Saliency Detection via Absorbing Markov Chain">

                                <b>[20]</b> JIANG B, ZHANG L, LU H, et al.Saliency detection via absorbing markov chain[C]//Proceedings of IEEE International Conference on Computer Vision.Washington D.C., USA:IEEE Press, 2013:1665-1672.
                            </a>
                        </p>
                        <p id="43">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Salient object detection:A discriminative regional feature integration approach">

                                <b>[21]</b> JIANG H, WANG J, YUAN Z, et al.Salient object detection:a discriminative regional feature integration approach [C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2013:2083-2090.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJC201904041" />
        <input id="dpi" type="hidden" value="600" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201904041&amp;v=MTk2NjhaZVJvRnkvZ1U3M1BMejdCYmJHNEg5ak1xNDlCWllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2U=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
