<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637130533337686250%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJC201905040%26RESULT%3d1%26SIGN%3d%252ff1LJagPAj5PeiNQquGCW23Y9Kk%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201905040&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201905040&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201905040&amp;v=MDg5NDhIOWpNcW85QlpJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSb0Z5M21WcjNQTHo3QmJiRzQ=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#61" data-title="0 概述 ">0 概述</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#66" data-title="1 多层次多粒度的时空特征提取 ">1 多层次多粒度的时空特征提取</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#69" data-title="1.1 基于级联SRU的时域金字塔构建">1.1 基于级联SRU的时域金字塔构建</a></li>
                                                <li><a href="#120" data-title="1.2 多层次多粒度特征">1.2 多层次多粒度特征</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#134" data-title="2 实验结果与分析 ">2 实验结果与分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#136" data-title="2.1 实验设置">2.1 实验设置</a></li>
                                                <li><a href="#139" data-title="2.2 结果分析">2.2 结果分析</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#152" data-title="3 结束语 ">3 结束语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#68" data-title="&lt;b&gt;图1 多层次多粒度的时空特征提取网络结构&lt;/b&gt;"><b>图1 多层次多粒度的时空特征提取网络结构</b></a></li>
                                                <li><a href="#71" data-title="&lt;b&gt;图2 时域金字塔构建流程&lt;/b&gt;"><b>图2 时域金字塔构建流程</b></a></li>
                                                <li><a href="#92" data-title="&lt;b&gt;图3 视频划分&lt;/b&gt;"><b>图3 视频划分</b></a></li>
                                                <li><a href="#114" data-title="&lt;b&gt;图4 级联SRU池化&lt;/b&gt;"><b>图4 级联SRU池化</b></a></li>
                                                <li><a href="#133" data-title="&lt;b&gt;图5 金字塔特征聚合&lt;/b&gt;"><b>图5 金字塔特征聚合</b></a></li>
                                                <li><a href="#142" data-title="&lt;b&gt;表1 金字塔层数对分类精度的影响&lt;/b&gt;"><b>表1 金字塔层数对分类精度的影响</b></a></li>
                                                <li><a href="#146" data-title="&lt;b&gt;表2 不同池化方法分类精度对比结果&lt;/b&gt;"><b>表2 不同池化方法分类精度对比结果</b></a></li>
                                                <li><a href="#150" data-title="&lt;b&gt;表3 不同方法的分类精度对比结果&lt;/b&gt;"><b>表3 不同方法的分类精度对比结果</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" title=" HE Ning, CAO Jiaheng, SONG Lin.Scale space histogram of oriented gradients for human detection[C]//Proceedings of International Symposium on Information Science and Engineering.Washington D.C., USA:IEEE Computer Society, 2008:167-170." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Scale space histogram of oriented gradients for human detection">
                                        <b>[1]</b>
                                         HE Ning, CAO Jiaheng, SONG Lin.Scale space histogram of oriented gradients for human detection[C]//Proceedings of International Symposium on Information Science and Engineering.Washington D.C., USA:IEEE Computer Society, 2008:167-170.
                                    </a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" title=" WANG Heng, KL&#196;SER A, SCHMID C, et al.Dense trajectories and motion boundary descriptors for action recognition[J].International Journal of Computer Vision, 2013, 103 (1) :60-79." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD130502008880&amp;v=MDc2NDJVU21EZDlTSDduM3hFOWZidm5LcmlmWnU5dUZDdmpVN3JMSlZ3VE5qN0Jhcks3SHRUTXJZOUZiT01IREJNOHp4&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         WANG Heng, KL&#196;SER A, SCHMID C, et al.Dense trajectories and motion boundary descriptors for action recognition[J].International Journal of Computer Vision, 2013, 103 (1) :60-79.
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title=" JI Shuiwang, YANG Ming, YU Kai, et al.3D convolutional neural networks for human action recognition[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2012, 35 (1) :221-231." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=3D Convolutional Neural Networks for Human Action Recognition">
                                        <b>[3]</b>
                                         JI Shuiwang, YANG Ming, YU Kai, et al.3D convolutional neural networks for human action recognition[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2012, 35 (1) :221-231.
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" title=" KARPATHY A, TODERICI G, SHETTY S, et al.Large-scale video classification with convolutional neural networks[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Computer Society, 2014:1725-1732." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Largescale video classification with convolutional neural networks">
                                        <b>[4]</b>
                                         KARPATHY A, TODERICI G, SHETTY S, et al.Large-scale video classification with convolutional neural networks[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Computer Society, 2014:1725-1732.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" title=" SIMONYAN K, ZISSERMAN A.Two-stream convolutional networks for action recognition in videos[EB/OL].[2018-02-10].https://arxiv.org/pdf/1406.2199.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Two-stream convolutional networks for action recognition in videos">
                                        <b>[5]</b>
                                         SIMONYAN K, ZISSERMAN A.Two-stream convolutional networks for action recognition in videos[EB/OL].[2018-02-10].https://arxiv.org/pdf/1406.2199.pdf.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" title=" WU Zuxuan, WANG Xi, JIANG Yugang, et al.Modeling spatial-temporal clues in a hybrid deep learning framework for video classification[C]//Proceedings of the 23rd ACM International Conference on Multimedia.New York, USA:ACM Press, 2015:461-470." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Modeling spatial-temporal clues in a hybrid deep learning framework for video classification">
                                        <b>[6]</b>
                                         WU Zuxuan, WANG Xi, JIANG Yugang, et al.Modeling spatial-temporal clues in a hybrid deep learning framework for video classification[C]//Proceedings of the 23rd ACM International Conference on Multimedia.New York, USA:ACM Press, 2015:461-470.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" title=" DONAHUE J, HENDRICKS L A, GUADARRAMA S, et al.Long-term recurrent convolutional networks for visual recognition and description[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2015:2625-2634." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Long-term recurrent convolutional networks for visual recognition and description">
                                        <b>[7]</b>
                                         DONAHUE J, HENDRICKS L A, GUADARRAMA S, et al.Long-term recurrent convolutional networks for visual recognition and description[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2015:2625-2634.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" title=" NGJ Y H, HAUSKNECHT M, VIJAYANARASIMHAN S, et al.Beyond short snippets:deep networks for video classification[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2015:4694-4702." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Beyond short snippets:Deep networks for video classification">
                                        <b>[8]</b>
                                         NGJ Y H, HAUSKNECHT M, VIJAYANARASIMHAN S, et al.Beyond short snippets:deep networks for video classification[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2015:4694-4702.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" title=" WANG Peng, CAO Yuanzhouhan, SHEN Chunhua, et al.Temporal pyramid pooling based convolutional neural network for action recognition[J].IEEE Transactions on Circuits and Systems for Video Technology, 2017, 27 (12) :2613-2622." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Temporal Pyramid Pooling Based Convolutional Neural Networks for Action Recognition">
                                        <b>[9]</b>
                                         WANG Peng, CAO Yuanzhouhan, SHEN Chunhua, et al.Temporal pyramid pooling based convolutional neural network for action recognition[J].IEEE Transactions on Circuits and Systems for Video Technology, 2017, 27 (12) :2613-2622.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_10" title=" ZHU Jiagang, ZOU Wei, ZHU Zheng.End-to-end video-level representation learning for action recognition [EB/OL].[2018-02-10].https://arxiv.org/pdf/1711.04161.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=End-to-end video-level representation learning for action recognition">
                                        <b>[10]</b>
                                         ZHU Jiagang, ZOU Wei, ZHU Zheng.End-to-end video-level representation learning for action recognition [EB/OL].[2018-02-10].https://arxiv.org/pdf/1711.04161.pdf.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_11" title=" WANG Yilin, WANG Suhang, TANG Jiliang, et al.Hierarchical attention network for action recognition in videos [EB/OL].[2018-02-10].https://arxiv.org/pdf/1607.06416.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Hierarchical attention network for action recognition in videos">
                                        <b>[11]</b>
                                         WANG Yilin, WANG Suhang, TANG Jiliang, et al.Hierarchical attention network for action recognition in videos [EB/OL].[2018-02-10].https://arxiv.org/pdf/1607.06416.pdf.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_12" title=" YAN Shiyang, SMITH J S, LU Wenjin, et al.CHAM:action recognition using convolutional hierarchical attention model[EB/OL].[2018-02-10].https://arxiv.org/pdf/1705.03146.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=CHAM:action recognition using convolutional hierarchical attention model">
                                        <b>[12]</b>
                                         YAN Shiyang, SMITH J S, LU Wenjin, et al.CHAM:action recognition using convolutional hierarchical attention model[EB/OL].[2018-02-10].https://arxiv.org/pdf/1705.03146.pdf.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_13" title=" YAN Shiyang, SMITH J S, LU Wenjin, et al.Hierarchical multi-scale attention networks for action recognition[J].Signal Processing Image Communication, 2018, 61:73-84." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESC18FBFAB36D3DEC8D9E1910AE20D4A0F&amp;v=MDE2MzhmQ3BiUTM1TjVod2J1NndxOD1OaWZPZmNDNUZxZSsyZjQzWisxN0R3aE12QjVuNDBwOFFYN2kzV2MzZWNhUU5McnBDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbA==&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[13]</b>
                                         YAN Shiyang, SMITH J S, LU Wenjin, et al.Hierarchical multi-scale attention networks for action recognition[J].Signal Processing Image Communication, 2018, 61:73-84.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_14" title=" LEI Tao, ZHANG Yu.Training RNNs as fast as CNNs[EB/OL].[2018-02-10].https://arxiv.org/pdf/1709.02755v2.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Training RNNs as fast as CNNs">
                                        <b>[14]</b>
                                         LEI Tao, ZHANG Yu.Training RNNs as fast as CNNs[EB/OL].[2018-02-10].https://arxiv.org/pdf/1709.02755v2.pdf.
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_15" title=" WANG Limin, XIONG Yuanjun, WANG Zhe, et al.Temporal segment networks:towards good practices for deep action recognition[C]//Proceedings of European Conference on Computer Vision.Berlin, Germany:Springer, 2016:20-36." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Temporal segment networks:towards good practices for deep action recognition">
                                        <b>[15]</b>
                                         WANG Limin, XIONG Yuanjun, WANG Zhe, et al.Temporal segment networks:towards good practices for deep action recognition[C]//Proceedings of European Conference on Computer Vision.Berlin, Germany:Springer, 2016:20-36.
                                    </a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_16" title=" IOFFE S, SZEGEDY C.Batch normalization:accelerating deep network training by reducing internal covariate shift[EB/OL].[2018-02-10].https://arxiv.org/pdf/1502.03167.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Batch normalization:accelerating deep network training by reducing internal covariate shift">
                                        <b>[16]</b>
                                         IOFFE S, SZEGEDY C.Batch normalization:accelerating deep network training by reducing internal covariate shift[EB/OL].[2018-02-10].https://arxiv.org/pdf/1502.03167.pdf.
                                    </a>
                                </li>
                                <li id="35">


                                    <a id="bibliography_17" title=" HE Kaiming, ZHANG Xiangyu, REN Shaoqing, et al.Spatial pyramid pooling in deep convolutional networks for visual recognition[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2015, 37 (9) :1904-1916." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Spatial pyramid pooling in deep convolutional networks for visual recognition">
                                        <b>[17]</b>
                                         HE Kaiming, ZHANG Xiangyu, REN Shaoqing, et al.Spatial pyramid pooling in deep convolutional networks for visual recognition[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2015, 37 (9) :1904-1916.
                                    </a>
                                </li>
                                <li id="37">


                                    <a id="bibliography_18" title=" ZAGORUYKO S, KOMODAKIS N.Paying more attention to attention:improving the performance of convolutional neural networks via attention transfer[EB/OL].[2018-02-10].https://arxiv.org/pdf/1612.03928.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Paying more attention to attention:improving the performance of convolutional neural networks via attention transfer">
                                        <b>[18]</b>
                                         ZAGORUYKO S, KOMODAKIS N.Paying more attention to attention:improving the performance of convolutional neural networks via attention transfer[EB/OL].[2018-02-10].https://arxiv.org/pdf/1612.03928.pdf.
                                    </a>
                                </li>
                                <li id="39">


                                    <a id="bibliography_19" title=" LIN Weiyao, MI Yang, WU Jianxin, et al.Action recognition with coarse-to-fine deep feature integration and asynchronous fusion[EB/OL].[2018-02-10].https://arxiv.org/pdf/1711.07430.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Action recognition with coarse-to-fine deep feature integration and asynchronous fusion">
                                        <b>[19]</b>
                                         LIN Weiyao, MI Yang, WU Jianxin, et al.Action recognition with coarse-to-fine deep feature integration and asynchronous fusion[EB/OL].[2018-02-10].https://arxiv.org/pdf/1711.07430.pdf.
                                    </a>
                                </li>
                                <li id="41">


                                    <a id="bibliography_20" title=" KETKAR N.Deep learning with Python[M].Berkeley, USA:Apress, 2017:195-208." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep learning with Python">
                                        <b>[20]</b>
                                         KETKAR N.Deep learning with Python[M].Berkeley, USA:Apress, 2017:195-208.
                                    </a>
                                </li>
                                <li id="43">


                                    <a id="bibliography_21" title=" THUMOS challenge:action recognition with a large number of classes [EB/OL].[2018-02-10].http://crcv.ucf.edu/ICCV13-Action-Workshop/." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=THUMOS challenge:action recognition with a large number of classes">
                                        <b>[21]</b>
                                         THUMOS challenge:action recognition with a large number of classes [EB/OL].[2018-02-10].http://crcv.ucf.edu/ICCV13-Action-Workshop/.
                                    </a>
                                </li>
                                <li id="45">


                                    <a id="bibliography_22" title=" ZACH C, POCK T, BISCHOF H.A duality based approach for realtime tv-L1 optical flow[C]//Proceedings of Joint Pattern Recognition Symposium.Berlin, Germany:Springer, 2007:214-223." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A duality based approach for realtime TV-L 1 optical flow">
                                        <b>[22]</b>
                                         ZACH C, POCK T, BISCHOF H.A duality based approach for realtime tv-L1 optical flow[C]//Proceedings of Joint Pattern Recognition Symposium.Berlin, Germany:Springer, 2007:214-223.
                                    </a>
                                </li>
                                <li id="47">


                                    <a id="bibliography_23" title=" BRADSKI G.The opencv library[J].Journal of Software Tools, 2000, 25:120-125." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The opencv library">
                                        <b>[23]</b>
                                         BRADSKI G.The opencv library[J].Journal of Software Tools, 2000, 25:120-125.
                                    </a>
                                </li>
                                <li id="49">


                                    <a id="bibliography_24" title=" DENG Jia, DONG Wei, SOCHER R, et al.ImageNet:a large-scale hierarchical image database[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, Washington D.C., USA:IEEE Press, 2009:248-255." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=ImageNet:A largescale hierarchical image database">
                                        <b>[24]</b>
                                         DENG Jia, DONG Wei, SOCHER R, et al.ImageNet:a large-scale hierarchical image database[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, Washington D.C., USA:IEEE Press, 2009:248-255.
                                    </a>
                                </li>
                                <li id="51">


                                    <a id="bibliography_25" title=" WANG Heng, SCHMID C.Action recognition with improved trajectories[C]//Proceedings of IEEE International Conference on Computer Vision.Washington D.C., USA:IEEE Computer Society, 2014:3551-3558." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Action recognition with improved trajectories">
                                        <b>[25]</b>
                                         WANG Heng, SCHMID C.Action recognition with improved trajectories[C]//Proceedings of IEEE International Conference on Computer Vision.Washington D.C., USA:IEEE Computer Society, 2014:3551-3558.
                                    </a>
                                </li>
                                <li id="53">


                                    <a id="bibliography_26" title=" WANG Limin, QIAO Yu, TANG Xiaoou.MoFAP:a multi-level representation for action recognition [J].International Journal of Computer Vision, 2016, 119 (3) :254-271." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=MoFAP:Amulti-level representation for action recognition">
                                        <b>[26]</b>
                                         WANG Limin, QIAO Yu, TANG Xiaoou.MoFAP:a multi-level representation for action recognition [J].International Journal of Computer Vision, 2016, 119 (3) :254-271.
                                    </a>
                                </li>
                                <li id="55">


                                    <a id="bibliography_27" title=" WANG Limin, QIAO Yu, TANG Xiaoou.Action recognition with trajectory-pooled deep-convolutional descriptors[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2015:4305-4314." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Action recognition with trajectory-pooled deep-convolutional descriptors">
                                        <b>[27]</b>
                                         WANG Limin, QIAO Yu, TANG Xiaoou.Action recognition with trajectory-pooled deep-convolutional descriptors[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2015:4305-4314.
                                    </a>
                                </li>
                                <li id="57">


                                    <a id="bibliography_28" title=" TRAN D, BOURDEV L, FERGUS R, et al.Learning spatiotemporal features with 3D convolutional networks[C]//Proceedings of IEEE International Conference on Computer Vision.Washington D.C., USA:IEEE Computer Society, 2015:4489-4497." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning spatiotemporal features with 3d convolutional networks">
                                        <b>[28]</b>
                                         TRAN D, BOURDEV L, FERGUS R, et al.Learning spatiotemporal features with 3D convolutional networks[C]//Proceedings of IEEE International Conference on Computer Vision.Washington D.C., USA:IEEE Computer Society, 2015:4489-4497.
                                    </a>
                                </li>
                                <li id="59">


                                    <a id="bibliography_29" title=" DIBA A, SHARMA V, GOOL L V.Deep temporal linear encoding networks[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2017:1541-1550." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep temporal linear encoding networks">
                                        <b>[29]</b>
                                         DIBA A, SHARMA V, GOOL L V.Deep temporal linear encoding networks[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2017:1541-1550.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJC" target="_blank">计算机工程</a>
                2019,45(05),243-248 DOI:10.19678/j.issn.1000-3428.0050768            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于SRU的时域金字塔构建方法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%99%BA%E6%B4%AA%E6%AC%A3&amp;code=38796085&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">智洪欣</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E4%BA%8E%E6%B4%AA%E6%B6%9B&amp;code=21432900&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">于洪涛</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%8E%E9%82%B5%E6%A2%85&amp;code=21303258&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">李邵梅</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%AB%98%E8%B6%85&amp;code=31691972&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">高超</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%9B%BD%E5%AE%B6%E6%95%B0%E5%AD%97%E4%BA%A4%E6%8D%A2%E7%B3%BB%E7%BB%9F%E5%B7%A5%E7%A8%8B%E6%8A%80%E6%9C%AF%E7%A0%94%E7%A9%B6%E4%B8%AD%E5%BF%83&amp;code=0291391&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">国家数字交换系统工程技术研究中心</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>现有基于时域金字塔的特征提取方法不能学习视频帧和视频段各自之间的时间依赖性信息以及未充分利用视频时域的分层结构信息, 造成视频分类特征提取不充分。为此, 提出一种基于SRU的多层次多粒度时空域深度特征提取方法。利用卷积神经网络提取视频的低、中、高3个层次的帧特征, 构建时域金字塔, 同时采用级联SRU学习视频时间依赖性和时域的分层结构特征, 通过聚合3个层次的时域金字塔得到视频的多层次多粒度全局特征。在数据集UCF101和HMDB51上的实验结果表明, 与DTPP方法、TLE方法相比, 该方法提取的特征具有较好的表征能力和鲁棒性。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%97%B6%E5%9F%9F%E9%87%91%E5%AD%97%E5%A1%94&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">时域金字塔;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%97%B6%E9%97%B4%E4%BE%9D%E8%B5%96%E6%80%A7&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">时间依赖性;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%88%86%E5%B1%82%E7%BB%93%E6%9E%84&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">分层结构;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%A4%9A%E5%B1%82%E6%AC%A1%E5%A4%9A%E7%B2%92%E5%BA%A6&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">多层次多粒度;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%BA%A7%E8%81%94SRU&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">级联SRU;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    智洪欣 (1987—) , 男, 硕士研究生, 主研方向为计算机视觉、视频处理;E-mail: newzhx@ yeah. net;
                                </span>
                                <span>
                                    于洪涛, 研究员、博士;;
                                </span>
                                <span>
                                    李邵梅, 副研究员、博士。;
                                </span>
                                <span>
                                    高超, 副研究员、博士。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-03-14</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金 (61601513);</span>
                    </p>
            </div>
                    <h1><b>Temporal Pyramid Construction Method Based on SRU</b></h1>
                    <h2>
                    <span>ZHI Hongxin</span>
                    <span>YU Hongtao</span>
                    <span>LI Shaomei</span>
                    <span>GAO Chao</span>
            </h2>
                    <h2>
                    <span>China National Digital Switching System Engineering and Technological R & D Center</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Existing feature extraction methods based on temporal pyramid cannot learn the time dependence between video frames and video segments and the hierarchical structure information of video temporal is not fully utilized, resulting in insufficient video classification feature extraction.Therefore, a multi-level multi-granularity spatial-temporal depth feature extraction method based on SRU is proposed.The Convolutional Neural Network (CNN) is used to extract the low, medium and high frame features of the video, and the temporal pyramid is constructed.At the same time, the cascade SRU is used to learn the temporal characteristics of video time and the hierarchical structure of the temporal, and the three-level temporal pyramid is aggregated to obtain the multi-level and multi-granular global features of the video.Experimental results in the datasets UCF101 and HMDB51 show that compared with DTPP method and TLE method, this method has better characterization ability and robustness.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=temporal%20pyramid&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">temporal pyramid;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=time%20dependence&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">time dependence;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=hierarchical%20structure&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">hierarchical structure;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=multi-level%20multi-granularity&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">multi-level multi-granularity;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=cascad%20Simple%20Recurrent%20Units%20(SRU)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">cascad Simple Recurrent Units (SRU) ;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-03-14</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="61" name="61" class="anchor-tag">0 概述</h3>
                <div class="p1">
                    <p id="62">视频分类是计算机视觉的一个重要研究方向, 在视频监管、行为分析、人工智能等领域中具有重要作用。因此, 视频分类问题受到研究者的广泛关注。</p>
                </div>
                <div class="p1">
                    <p id="63">特征提取是视频分类的核心环节。传统视频分类方法主要依靠手工设计特征, 如梯度方向直方图 (Histograms of Oriented Gradients, HOG) <citation id="154" type="reference"><link href="3" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>和稠密轨迹 (Dense Trajectories, DT) <citation id="155" type="reference"><link href="5" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>, 但表征能力有限, 且视频分类精度较差。深度学习能够有效地提取到较高语义层次的特征, 被广泛应用于视频分类任务中, 其通过反向传播和端到端学习机制, 自动学习适合分类任务的特征。视频结构包含空域信息和时域信息, 如何有效地提取时域特征是视频分类任务中的难点之一。文献<citation id="157" type="reference">[<a class="sup">3</a>,<a class="sup">4</a>]</citation>提出3D卷积网络, 使用堆叠视频帧作为神经网络的输入, 把卷积神经网络 (Convolutional Neural Network, CNN) 扩展到时域。文献<citation id="156" type="reference">[<a class="sup">5</a>]</citation>提出双流网络, 分别把视频帧和相应的光流图作为网络输入来提取空域信息和时域信息。文献<citation id="158" type="reference">[<a class="sup">6</a>,<a class="sup">7</a>,<a class="sup">8</a>]</citation>使用长短时记忆 (Long-Short Term Memory, LSTM) 网络捕获视频中的长时时域信息。</p>
                </div>
                <div class="p1">
                    <p id="64">上述方法提取的时域特征不能应对视频中场景变化速度、物体运动速度不同等造成的时域多尺度问题, 因此文献<citation id="159" type="reference">[<a class="sup">9</a>,<a class="sup">10</a>]</citation>通过在CNN网络中引入时域金字塔池化层 (Temporal Pyramid Pooling Layer, TPPL) 学习多粒度时域特征, 但在构建时域金字塔过程中忽略了视频帧和视频段各自之间的时间依赖性。此外, 视频的时域结构是分层的, 低层时域信息变化较快, 高层时域信息变化较慢, 充分利用时域的分层结构信息, 有利于提取表征能力和鲁棒性更强的时域特征<citation id="160" type="reference"><link href="23" rel="bibliography" /><link href="25" rel="bibliography" /><link href="27" rel="bibliography" /><sup>[<a class="sup">11</a>,<a class="sup">12</a>,<a class="sup">13</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="65">基于上述分析, 本文对原有时域金字塔构建方法进行改进, 利用级联SRU (Simple Recurrent Units) <citation id="161" type="reference"><link href="29" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>构建时域金字塔。根据SRU的特征聚合能力, 对视频帧特征进行池化, 通过级联结构学习视频帧和视频段各自之间的时间依赖性信息以及视频时域的分层结构信息。在此基础上, 在CNN中提取低、中、高3个层次的帧特征来构建时域金字塔, 并将其聚合为视频的多层次多粒度特征。</p>
                </div>
                <h3 id="66" name="66" class="anchor-tag">1 多层次多粒度的时空特征提取</h3>
                <div class="p1">
                    <p id="67">本文方法网络如图1所示。可以看出, 该网络分为结构相同的空域流和时域流2个子网络, 分别把RGB帧和堆叠的光流图作为子网络的输入。考虑到视频帧之间的冗余性与计算消耗, 本文采取与文献<citation id="162" type="reference">[<a class="sup">15</a>]</citation>相同的策略:把视频均匀地划分为<i>T</i>段, 从每个视频段中随机采样出一帧<b><i>x</i></b><sub><i>i</i></sub>, 然后把采样出的视频帧{<b><i>x</i></b><sub>1</sub>, <b><i>x</i></b><sub>2</sub>, …, <b><i>x</i></b><sub><i>T</i></sub>}和相应的光流图分别作为网络的输入。本文使用BN-Inception<citation id="163" type="reference"><link href="33" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>作为基础CNN, 分别从网络中提取每帧的低、中、高3个层次的帧特征, 利用提出的时域金字塔构建算法, 分别使用这3个层次的帧特征构建金字塔, 然后把上述3个金字塔聚合为视频的多层次多粒度特征并进行分类, 融合时域和空域网络得分得到网络的最后结果。</p>
                </div>
                <div class="area_img" id="68">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201905040_068.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 多层次多粒度的时空特征提取网络结构" src="Detail/GetImg?filename=images/JSJC201905040_068.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图1 多层次多粒度的时空特征提取网络结构</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201905040_068.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="69" name="69">1.1 基于级联SRU的时域金字塔构建</h4>
                <div class="p1">
                    <p id="70">本文提出的时域金字塔构建算法流程如图2所示。根据金字塔的层数<i>N</i>对视频特征进行分段, 然后通过级联SRU进行池化, 学习时间依赖性和时域分层结构, 进而把所有特征拼接起来得到金字塔。</p>
                </div>
                <div class="area_img" id="71">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201905040_071.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 时域金字塔构建流程" src="Detail/GetImg?filename=images/JSJC201905040_071.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图2 时域金字塔构建流程</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201905040_071.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="72" name="72">1.1.1 视频分段算法</h4>
                <div class="p1">
                    <p id="73">本文采用类似空域金字塔<citation id="164" type="reference"><link href="35" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>选择池化核大小和步长的算法对视频进行分段, 金字塔第<i>n</i>层, 共有2<sup><i>n</i>-1</sup>个特征, 该层对应的池化核大小和池化步长计算方法分别为:</p>
                </div>
                <div class="p1">
                    <p id="74"><i>kernel</i>_<i>h</i>=<image href="images/JSJC201905040_075.jpg" type="" display="inline" placement="inline"><alt></alt></image><mathml id="76"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mi>h</mi><mi>e</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi></mrow><mi>n</mi></mfrac></mrow></math></mathml><image href="images/JSJC201905040_077.jpg" type="" display="inline" placement="inline"><alt></alt></image></p>
                </div>
                <div class="p1">
                    <p id="78"><i>kernel</i>_<i>w</i>=<image href="images/JSJC201905040_079.jpg" type="" display="inline" placement="inline"><alt></alt></image><mathml id="80"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mi>w</mi><mi>i</mi><mi>d</mi><mi>t</mi><mi>h</mi></mrow><mi>n</mi></mfrac></mrow></math></mathml><image href="images/JSJC201905040_081.jpg" type="" display="inline" placement="inline"><alt></alt></image> (1) </p>
                </div>
                <div class="p1">
                    <p id="82"><i>stride</i>_<i>h</i>=<image href="images/JSJC201905040_083.jpg" type="" display="inline" placement="inline"><alt></alt></image><mathml id="84"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mi>h</mi><mi>e</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi></mrow><mi>n</mi></mfrac></mrow></math></mathml><image href="images/JSJC201905040_085.jpg" type="" display="inline" placement="inline"><alt></alt></image></p>
                </div>
                <div class="p1">
                    <p id="86"><i>stride</i>_<i>w</i>=<image href="images/JSJC201905040_087.jpg" type="" display="inline" placement="inline"><alt></alt></image><mathml id="88"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mi>w</mi><mi>i</mi><mi>d</mi><mi>t</mi><mi>h</mi></mrow><mi>n</mi></mfrac></mrow></math></mathml><image href="images/JSJC201905040_089.jpg" type="" display="inline" placement="inline"><alt></alt></image> (2) </p>
                </div>
                <div class="p1">
                    <p id="90">其中, <i>height</i>和<i>width</i>分别为特征图的高度和宽度, 「·⎤表示向上取整, ⎣·」表示向下取整。</p>
                </div>
                <div class="p1">
                    <p id="91">文献<citation id="165" type="reference">[<a class="sup">17</a>]</citation>的池化核作用在特征图上, 在对边界进行池化时, 可以进行填充。然而, 本文使用级联SRU进行池化, 且不能填充, 因此将其算法进行修改, 如图3所示。</p>
                </div>
                <div class="area_img" id="92">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201905040_092.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 视频划分" src="Detail/GetImg?filename=images/JSJC201905040_092.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图3 视频划分</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201905040_092.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="93">在金字塔第<i>n</i>层, 将视频帧划分为2<sup><i>n</i>-1</sup>段, 前2<sup><i>n</i>-1</sup>-1段每段包含帧数为:<image href="images/JSJC201905040_094.jpg" type="" display="inline" placement="inline"><alt></alt></image><mathml id="95"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mi>Τ</mi><mrow><mn>2</mn><msup><mrow></mrow><mrow><mi>n</mi><mo>-</mo><mn>1</mn></mrow></msup></mrow></mfrac></mrow></math></mathml><image href="images/JSJC201905040_096.jpg" type="" display="inline" placement="inline"><alt></alt></image> (如果<image href="images/JSJC201905040_097.jpg" type="" display="inline" placement="inline"><alt></alt></image><mathml id="98"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mi>Τ</mi><mrow><mn>2</mn><msup><mrow></mrow><mrow><mi>n</mi><mo>-</mo><mn>1</mn></mrow></msup></mrow></mfrac></mrow></math></mathml><image href="images/JSJC201905040_099.jpg" type="" display="inline" placement="inline"><alt></alt></image> (2<sup><i>n</i>-1</sup>-1) &gt;<i>T</i>, 则取<image href="images/JSJC201905040_100.jpg" type="" display="inline" placement="inline"><alt></alt></image><mathml id="101"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mi>Τ</mi><mrow><mn>2</mn><msup><mrow></mrow><mrow><mi>n</mi><mo>-</mo><mn>1</mn></mrow></msup></mrow></mfrac></mrow></math></mathml><image href="images/JSJC201905040_102.jpg" type="" display="inline" placement="inline"><alt></alt></image>) , 其中, <i>T</i>是视频帧特征总数, 最后一段视频帧数为:<i>T</i>-<image href="images/JSJC201905040_103.jpg" type="" display="inline" placement="inline"><alt></alt></image><mathml id="104"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mi>Τ</mi><mrow><mn>2</mn><msup><mrow></mrow><mrow><mi>n</mi><mo>-</mo><mn>1</mn></mrow></msup></mrow></mfrac></mrow></math></mathml><image href="images/JSJC201905040_105.jpg" type="" display="inline" placement="inline"><alt></alt></image> (2<sup><i>n</i>-1</sup>-1) , 如果其包含的帧数小于3帧, 那么将其合并到上一段中。</p>
                </div>
                <h4 class="anchor-tag" id="106" name="106">1.1.2 SRU算法</h4>
                <div class="p1">
                    <p id="107">本文时域金字塔构建算法的核心是充分捕获视频的时间依赖性和时域分层结构。级联LSTM网络是捕获这些信息的常用方法, 但结构较复杂, 且训练速度慢, 在构建金字塔过程中需要同时使用多个LSTM网络, 增加了网络的复杂度和训练时间。</p>
                </div>
                <div class="p1">
                    <p id="108">基于上述原因, 本文采用可以并行运算的SRU构建时域金字塔。SRU优化了网络结构, 主要由遗忘门、重置门和记忆单元3部分组成。其计算方法为:</p>
                </div>
                <div class="p1">
                    <p id="109" class="code-formula">
                        <mathml id="109"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi mathvariant="bold-italic">x</mi></mstyle><mrow><mspace width="0.25em" /><mo>∼</mo></mrow></mover><msub><mrow></mrow><mi>t</mi></msub></mrow><mo>=</mo><mi mathvariant="bold-italic">W</mi><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>t</mi></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi mathvariant="bold-italic">f</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mi>σ</mi><mrow><mo> (</mo><mrow><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mi>f</mi></msub><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>t</mi></msub><mo>+</mo><mi mathvariant="bold-italic">b</mi><msub><mrow></mrow><mi>f</mi></msub></mrow><mo>) </mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi mathvariant="bold-italic">r</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mi>σ</mi><mrow><mo> (</mo><mrow><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mi>r</mi></msub><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>t</mi></msub><mo>+</mo><mi mathvariant="bold-italic">b</mi><msub><mrow></mrow><mi>r</mi></msub></mrow><mo>) </mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>5</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi mathvariant="bold-italic">c</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mi mathvariant="bold-italic">f</mi><msub><mrow></mrow><mi>t</mi></msub><mo>⊙</mo><mi mathvariant="bold-italic">c</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>+</mo><mrow><mo> (</mo><mrow><mn>1</mn><mo>-</mo><mi mathvariant="bold-italic">f</mi><msub><mrow></mrow><mi>t</mi></msub></mrow><mo>) </mo></mrow><mo>⊙</mo><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi mathvariant="bold-italic">x</mi></mstyle><mrow><mspace width="0.25em" /><mo>∼</mo></mrow></mover><msub><mrow></mrow><mi>t</mi></msub></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>6</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi mathvariant="bold-italic">h</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mi mathvariant="bold-italic">r</mi><msub><mrow></mrow><mi>t</mi></msub><mo>⊙</mo><mi>g</mi><mrow><mo> (</mo><mrow><mi mathvariant="bold-italic">c</mi><msub><mrow></mrow><mi>t</mi></msub></mrow><mo>) </mo></mrow><mo>+</mo><mrow><mo> (</mo><mrow><mn>1</mn><mo>-</mo><mi mathvariant="bold-italic">r</mi><msub><mrow></mrow><mi>t</mi></msub></mrow><mo>) </mo></mrow><mo>⊙</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>t</mi></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>7</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="110">其中, <b><i>f</i></b><sub><i>t</i></sub>、<b><i>r</i></b><sub><i>t</i></sub>、<b><i>c</i></b><sub><i>t</i></sub>分别代表遗忘门、重置门、记忆单元, <mathml id="111"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>g</mi><mrow><mo> (</mo><mo>⋅</mo><mo>) </mo></mrow></mrow></math></mathml>表示激活函数, 本节选用<i>ReLU</i>函数。由于<i>SRU</i>的计算过程不需要隐藏状态参与, 因此当前时间可以通过并行计算加快速度。</p>
                </div>
                <h4 class="anchor-tag" id="112" name="112">1.1.3 级联<i>SRU</i>池化</h4>
                <div class="p1">
                    <p id="113">本文提出的级联<i>SRU</i>池化算法如图4所示。对于金字塔的第n层, 根据视频分段算法对视频进行划分, 然后将每个视频段中的视频帧特征依据先后顺序分别输入到级联的<i>SRU</i>网络结构中进行池化。第1层<i>SRU</i>网络把视频帧卷积特征作为输入进行池化, 学习视频帧之间的时间依赖性。第2层<i>SRU</i>把每个视频段中最后一帧的第1层<i>SRU</i>输出作为输入, 并且把所有视频段联系起来进行二级池化, 学习视频段之间的时间依赖性, 把最后一个视频段的第2层<i>SRU</i>的输出作为该金字塔层的特征。第1层<i>SRU</i>学习变化较快的视频帧特征, 第2层<i>SRU</i>学习变化较慢的视频段特征, 这正是视频时域分层结构的特点。因此, 级联<i>SRU</i>池化同时学习了视频帧和视频段之间的时间依赖性以及视频时域分层结构信息。</p>
                </div>
                <div class="area_img" id="114">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201905040_114.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 级联SRU池化" src="Detail/GetImg?filename=images/JSJC201905040_114.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图4 级联SRU池化</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201905040_114.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="115">通过上述方法获得金字塔每层特征<b><i>P</i></b><mathml id="116"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>n</mi><mi>i</mi></msubsup></mrow></math></mathml>, i∈{L, M, H}, n∈N。由于金字塔第1层的划分只有一个视频段, 因此只使用一层<i>SRU</i>。金字塔每层的视频划分粒度不同, 捕获特征的时域尺度不同, 把所有层拼接起来得到时域多尺度金字塔<b><i>P</i></b><sup><i>i</i></sup>=[<b><i>P</i></b><mathml id="117"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>1</mn><mi>i</mi></msubsup></mrow></math></mathml>, <b><i>P</i></b><mathml id="118"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>2</mn><mi>i</mi></msubsup></mrow></math></mathml>, …, <b><i>P</i></b><mathml id="119"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>Ν</mi><mi>i</mi></msubsup></mrow></math></mathml>], <i>i</i>∈{<i>L</i>, <i>M</i>, <i>H</i>}。此金字塔特征只有<i>N</i>×<i>D</i>维, 远小于现有算法的特征维度。因此本文提出的金字塔构建算法在捕获更丰富信息的同时, 能有效减小金字塔特征的维度, 使网络更稳定。</p>
                </div>
                <h4 class="anchor-tag" id="120" name="120">1.2 多层次多粒度特征</h4>
                <div class="p1">
                    <p id="121">在<i>CNN</i>中高层包含较丰富的语义信息, 而低层主要关注边界、颜色等特征, 即不同<i>CNN</i>层关注的区域不同<citation id="166" type="reference"><link href="37" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>, 因此, 同时提取低层特征和高层特征能够增强特征的鲁棒性。</p>
                </div>
                <div class="p1">
                    <p id="122">本文分别从<i>BN</i>-<i>Inception</i>网络的3～5阶段的最后一层inception_3c/output, inception_4e/output和inception_5b/output这3层提取每帧的低、中、高3个层次的帧特征。不同阶段的特征通道数量分别为576、1 056和1 024, 经过全局池化后的特征向量维度不同, 因此, 本文分别在inception_3c/output和inception_4e/output层后添加一个反卷积和卷积层, 使每个层次的特征有相同维度的特征通道。最后得到3个层次的帧特征分别为{<b><i>f</i></b><mathml id="123"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>1</mn><mi>L</mi></msubsup></mrow></math></mathml>, <b><i>f</i></b><mathml id="124"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>2</mn><mi>L</mi></msubsup></mrow></math></mathml>, …, <b><i>f</i></b><mathml id="125"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>Τ</mi><mi>L</mi></msubsup></mrow></math></mathml>}、{<b><i>f</i></b><mathml id="126"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>1</mn><mi>Μ</mi></msubsup></mrow></math></mathml>, <b><i>f</i></b><sup><i>M</i></sup><sub>2</sub>, …, <b><i>f</i></b><mathml id="127"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>Τ</mi><mi>Μ</mi></msubsup></mrow></math></mathml>}和{<b><i>f</i></b><mathml id="128"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>1</mn><mi>Η</mi></msubsup></mrow></math></mathml>, <b><i>f</i></b><mathml id="129"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>2</mn><mi>Η</mi></msubsup></mrow></math></mathml>, …, <b><i>f</i></b><mathml id="130"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>Τ</mi><mi>Η</mi></msubsup></mrow></math></mathml>}, 其中, <b><i>f</i></b>∈<image href="images/JSJC201905040_131.jpg" type="" display="inline" placement="inline"><alt></alt></image><sup>1×<i>D</i></sup>, <i>D</i>是特征维度。同时利用时域金字塔构建算法, 分别使用这3个层次的帧特征构建金字塔<b><i>P</i></b><sup><i>L</i></sup>、<b><i>P</i></b><sup><i>M</i></sup>和<b><i>P</i></b><sup><i>H</i></sup>。</p>
                </div>
                <div class="p1">
                    <p id="132">在得到3个层次的金字塔特征以后, 对其进行特征聚合得到视频的多层次多粒度特征。本文利用“Coarse-to-Fine”方法<citation id="167" type="reference"><link href="39" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>进行聚合, 如图5所示, 3个层次的金字塔特征分别输入到3个SRU单元中进行聚合。在训练过程中, 分别在每个SRU单元后接一个全连接层和softmax层进行分类, 把3个分类器的损失加起来进行反向传播, 使网络自动学习3个特征之间相对重要性。在测试阶段, 去掉第1个和第2个SRU单元后的分类器, 把第3个SRU单元后的分类器结果作为网络的最终结果。</p>
                </div>
                <div class="area_img" id="133">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201905040_133.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 金字塔特征聚合" src="Detail/GetImg?filename=images/JSJC201905040_133.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图5 金字塔特征聚合</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201905040_133.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h3 id="134" name="134" class="anchor-tag">2 实验结果与分析</h3>
                <div class="p1">
                    <p id="135">本文基于pytorch<citation id="168" type="reference"><link href="41" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>构建网络, 在NVIDIA M40 GPU服务器上实现提出的视频分类方法, 通过实验讨论了金字塔层次和金字塔特征融合方法对分类精度的影响, 并与传统池化方法进行对比。</p>
                </div>
                <h4 class="anchor-tag" id="136" name="136">2.1 实验设置</h4>
                <div class="p1">
                    <p id="137">本文分别在UCF101和HMDB51这2个数据集上进行实验, 数据集UCF101中的视频主要来源于YouTuBe, 包含13 320个视频, 总时长达27 h, 共101类, 分为人-物互动、身体动作、人-人互动、弹奏乐器和运动5种。数据集HMDB51中的视频主要来自电影和网络视频, 共有6 766个视频, 分为51类, 并且此数据集由至少2名观察者同时进行人工标注。本文遵从THUMOS13<citation id="169" type="reference"><link href="43" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>验证方法, 分别在数据集的3个split上进行训练和测试, 最后取测试结果的平均值作为最后结果。</p>
                </div>
                <div class="p1">
                    <p id="138">本文网络以RGB帧和相应的光流图作为输入, 采用TV-L1<citation id="170" type="reference"><link href="45" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>算法, 使用基于GPU加速的OpenCV库<citation id="171" type="reference"><link href="47" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>提取相应的光流图。在训练过程中使用批量随机梯度下降 (Stochastic Gradient Descent, SGD) 算法优化网络。由于构建金字塔过程中视频分段的需要, 本文在测试和训练阶段均把视频均匀地分为<i>T</i>=25段, 从每段中随机地抽取出1帧和相应的10张光流图作为网络的输入。在训练过程中, 受GPU显存的限制, 批尺寸设置为64。空域流使用在ImageNet<citation id="172" type="reference"><link href="49" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>上预训练的网络模型, 初始学习率设置为0.001, 每训练2 000次衰减至1/10, 最大迭代次数设置为4 500。为使网络训练更加稳定, 在训练过程中使用梯度裁剪方法, <i>clip</i>-<i>gradients</i>设置为40。时域流使用在空域流上训练的网络模型作为预训练模型, 初始学习率设置为0.005, 迭代到12 000次和18 000次以后每次衰减至1/10, 最大迭代次数设置为20 000, 2个子网络的权重衰减率设置为1×10<sup>-2</sup>, 冲量设置为0.9。</p>
                </div>
                <h4 class="anchor-tag" id="139" name="139">2.2 结果分析</h4>
                <h4 class="anchor-tag" id="140" name="140">2.2.1 金字塔层次选择</h4>
                <div class="p1">
                    <p id="141">在数据集UCF101 split 1上进行实验, 本文实验只选取每帧最深层的CNN特征来建构时域金字塔, 结果如表1所示。</p>
                </div>
                <div class="area_img" id="142">
                    <p class="img_tit"><b>表1 金字塔层数对分类精度的影响</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="142" border="1"><tr><td rowspan="2"><br />金字塔<br />层数</td><td rowspan="2">每层视频<br />分段数</td><td colspan="3"><br />精度/%</td></tr><tr><td><br />空域流</td><td>时域流</td><td>双流网络</td></tr><tr><td>1</td><td>1</td><td>88.4</td><td>88.7</td><td>94.8</td></tr><tr><td><br />2</td><td>1、2</td><td>90.4</td><td>89.2</td><td>95.2</td></tr><tr><td><br />3</td><td>1、2、4</td><td>90.6</td><td>90.1</td><td>95.4</td></tr><tr><td><br />4</td><td>1、2、4、8</td><td>90.2</td><td>89.7</td><td>94.7</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="143">从表1可以看出, 随着金字塔层数和时域尺度的增多, 分类精度逐渐提升。当金字塔只有一层时, 提出的方法的分类精度为94.8%;当金字塔层数为3时, 分类精度达到最大, 为95.4%;当金字塔层数继续增多时, 分类精度反而下降。因此, 空域流和时域流的金字塔层数设置为3。</p>
                </div>
                <h4 class="anchor-tag" id="144" name="144">2.2.2 金字塔特征的融合方法选择</h4>
                <div class="p1">
                    <p id="145">本节在UCF101 split 1上进行实验, 不同池化方法的分类性能如表2所示。</p>
                </div>
                <div class="area_img" id="146">
                    <p class="img_tit"><b>表2 不同池化方法分类精度对比结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note">%</p>
                    <table id="146" border="1"><tr><td>方法</td><td>空域流</td><td>时域流</td><td>双流网络</td></tr><tr><td><br />最大池化方法</td><td>91.2</td><td>92.3</td><td>95.6</td></tr><tr><td><br />平均池化方法</td><td>90.6</td><td>91.7</td><td>95.3</td></tr><tr><td><br />级联SRU方法</td><td>91.6</td><td>92.7</td><td>95.8</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="147">从表2可以看出, 最大池化方法比平均池化方法性能要好, 而级联SRU方法的分类精度最高。级联SRU在特征融合过程中, 逐级计算每个层次的误差并反向传播, 从而对特征融合进行持续优化。实验结果表明, 该优化过程能够提高分类精度, 而其他2种方法没有优化特征融合环节。</p>
                </div>
                <h4 class="anchor-tag" id="148" name="148">2.2.3 方法对比</h4>
                <div class="p1">
                    <p id="149">通过上述实验确定金字塔层数和多层金字塔特征融合方法以后, 本节在UCF101和HMDB51这3个split上进行了实验, 对3个split上的结果求均值并与传统方法进行对比, 结果如表3所示。</p>
                </div>
                <div class="area_img" id="150">
                    <p class="img_tit"><b>表3 不同方法的分类精度对比结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note">%</p>
                    <table id="150" border="1"><tr><td><br />方法</td><td>UCF101</td><td>HMDB51</td></tr><tr><td><br />Two-stream<sup>[5]</sup></td><td>88.0</td><td>59.4</td></tr><tr><td><br />DTPP<sup>[10]</sup></td><td>95.8</td><td>74.8</td></tr><tr><td><br />TSN (7 seg) <sup>[16]</sup></td><td>94.9</td><td>71.0</td></tr><tr><td><br />IDT<sup>[25]</sup></td><td>85.9</td><td>57.2</td></tr><tr><td><br />MoFAP<sup>[26]</sup></td><td>88.3</td><td>61.7</td></tr><tr><td><br />TDD<sup>[27]</sup></td><td>90.3</td><td>63.2</td></tr><tr><td><br />C3D<sup>[28]</sup></td><td>85.2</td><td>—</td></tr><tr><td><br />TLE<sup>[29]</sup></td><td>95.6</td><td>71.1</td></tr><tr><td><br />本文方法</td><td>96.1</td><td>75.3</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="151">从表3可以看出, 本文方法有较好的分类精度:在UCF101上为96.1%, 比DTPP高0.3%, 比TLE高0.5%;在HMDB51上为75.3%, 比DTPP高0.5%, 而比TLE高4.2%。本文方法的性能在数据集HMDB51上比TLE方法有大幅提高。数据集HMDB51比UCF101复杂, 并且动作信息是数据集HMDB51中的重要信息。实验结果表明, 本文方法能够较好地捕获视频中的时域信息。</p>
                </div>
                <h3 id="152" name="152" class="anchor-tag">3 结束语</h3>
                <div class="p1">
                    <p id="153">本文提出一种基于视频分类的时域金字塔构建方法。从CNN网络中提取每帧低、中、高3个层次的特征, 利用视频分段方法对其特征进行划分。根据每个层次的帧特征, 通过级联SRU网络结构进行池化, 然后学习视频帧以及视频段之间的时间依赖性和视频时域分层结构信息, 并构建金字塔, 融合金字塔特征, 从而得到视频多层次多粒度特征, 最终对其进行分类。实验结果表明, 该方法能取得较好的分类结果。下一步将在时域金字塔构建算法中加入视频边界检测机制, 使分段结果更加合理。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Scale space histogram of oriented gradients for human detection">

                                <b>[1]</b> HE Ning, CAO Jiaheng, SONG Lin.Scale space histogram of oriented gradients for human detection[C]//Proceedings of International Symposium on Information Science and Engineering.Washington D.C., USA:IEEE Computer Society, 2008:167-170.
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD130502008880&amp;v=MDU4ODJyTEpWd1ROajdCYXJLN0h0VE1yWTlGYk9NSERCTTh6eFVTbURkOVNIN24zeEU5ZmJ2bktyaWZadTl1RkN2alU3&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> WANG Heng, KLÄSER A, SCHMID C, et al.Dense trajectories and motion boundary descriptors for action recognition[J].International Journal of Computer Vision, 2013, 103 (1) :60-79.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=3D Convolutional Neural Networks for Human Action Recognition">

                                <b>[3]</b> JI Shuiwang, YANG Ming, YU Kai, et al.3D convolutional neural networks for human action recognition[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2012, 35 (1) :221-231.
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Largescale video classification with convolutional neural networks">

                                <b>[4]</b> KARPATHY A, TODERICI G, SHETTY S, et al.Large-scale video classification with convolutional neural networks[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Computer Society, 2014:1725-1732.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Two-stream convolutional networks for action recognition in videos">

                                <b>[5]</b> SIMONYAN K, ZISSERMAN A.Two-stream convolutional networks for action recognition in videos[EB/OL].[2018-02-10].https://arxiv.org/pdf/1406.2199.pdf.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Modeling spatial-temporal clues in a hybrid deep learning framework for video classification">

                                <b>[6]</b> WU Zuxuan, WANG Xi, JIANG Yugang, et al.Modeling spatial-temporal clues in a hybrid deep learning framework for video classification[C]//Proceedings of the 23rd ACM International Conference on Multimedia.New York, USA:ACM Press, 2015:461-470.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Long-term recurrent convolutional networks for visual recognition and description">

                                <b>[7]</b> DONAHUE J, HENDRICKS L A, GUADARRAMA S, et al.Long-term recurrent convolutional networks for visual recognition and description[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2015:2625-2634.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Beyond short snippets:Deep networks for video classification">

                                <b>[8]</b> NGJ Y H, HAUSKNECHT M, VIJAYANARASIMHAN S, et al.Beyond short snippets:deep networks for video classification[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2015:4694-4702.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Temporal Pyramid Pooling Based Convolutional Neural Networks for Action Recognition">

                                <b>[9]</b> WANG Peng, CAO Yuanzhouhan, SHEN Chunhua, et al.Temporal pyramid pooling based convolutional neural network for action recognition[J].IEEE Transactions on Circuits and Systems for Video Technology, 2017, 27 (12) :2613-2622.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=End-to-end video-level representation learning for action recognition">

                                <b>[10]</b> ZHU Jiagang, ZOU Wei, ZHU Zheng.End-to-end video-level representation learning for action recognition [EB/OL].[2018-02-10].https://arxiv.org/pdf/1711.04161.pdf.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Hierarchical attention network for action recognition in videos">

                                <b>[11]</b> WANG Yilin, WANG Suhang, TANG Jiliang, et al.Hierarchical attention network for action recognition in videos [EB/OL].[2018-02-10].https://arxiv.org/pdf/1607.06416.pdf.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=CHAM:action recognition using convolutional hierarchical attention model">

                                <b>[12]</b> YAN Shiyang, SMITH J S, LU Wenjin, et al.CHAM:action recognition using convolutional hierarchical attention model[EB/OL].[2018-02-10].https://arxiv.org/pdf/1705.03146.pdf.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_13" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESC18FBFAB36D3DEC8D9E1910AE20D4A0F&amp;v=MTc2MDJwOFFYN2kzV2MzZWNhUU5McnBDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZDcGJRMzVONWh3YnU2d3E4PU5pZk9mY0M1RnFlKzJmNDNaKzE3RHdoTXZCNW40MA==&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[13]</b> YAN Shiyang, SMITH J S, LU Wenjin, et al.Hierarchical multi-scale attention networks for action recognition[J].Signal Processing Image Communication, 2018, 61:73-84.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Training RNNs as fast as CNNs">

                                <b>[14]</b> LEI Tao, ZHANG Yu.Training RNNs as fast as CNNs[EB/OL].[2018-02-10].https://arxiv.org/pdf/1709.02755v2.pdf.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Temporal segment networks:towards good practices for deep action recognition">

                                <b>[15]</b> WANG Limin, XIONG Yuanjun, WANG Zhe, et al.Temporal segment networks:towards good practices for deep action recognition[C]//Proceedings of European Conference on Computer Vision.Berlin, Germany:Springer, 2016:20-36.
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Batch normalization:accelerating deep network training by reducing internal covariate shift">

                                <b>[16]</b> IOFFE S, SZEGEDY C.Batch normalization:accelerating deep network training by reducing internal covariate shift[EB/OL].[2018-02-10].https://arxiv.org/pdf/1502.03167.pdf.
                            </a>
                        </p>
                        <p id="35">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Spatial pyramid pooling in deep convolutional networks for visual recognition">

                                <b>[17]</b> HE Kaiming, ZHANG Xiangyu, REN Shaoqing, et al.Spatial pyramid pooling in deep convolutional networks for visual recognition[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2015, 37 (9) :1904-1916.
                            </a>
                        </p>
                        <p id="37">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Paying more attention to attention:improving the performance of convolutional neural networks via attention transfer">

                                <b>[18]</b> ZAGORUYKO S, KOMODAKIS N.Paying more attention to attention:improving the performance of convolutional neural networks via attention transfer[EB/OL].[2018-02-10].https://arxiv.org/pdf/1612.03928.pdf.
                            </a>
                        </p>
                        <p id="39">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Action recognition with coarse-to-fine deep feature integration and asynchronous fusion">

                                <b>[19]</b> LIN Weiyao, MI Yang, WU Jianxin, et al.Action recognition with coarse-to-fine deep feature integration and asynchronous fusion[EB/OL].[2018-02-10].https://arxiv.org/pdf/1711.07430.pdf.
                            </a>
                        </p>
                        <p id="41">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep learning with Python">

                                <b>[20]</b> KETKAR N.Deep learning with Python[M].Berkeley, USA:Apress, 2017:195-208.
                            </a>
                        </p>
                        <p id="43">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=THUMOS challenge:action recognition with a large number of classes">

                                <b>[21]</b> THUMOS challenge:action recognition with a large number of classes [EB/OL].[2018-02-10].http://crcv.ucf.edu/ICCV13-Action-Workshop/.
                            </a>
                        </p>
                        <p id="45">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A duality based approach for realtime TV-L 1 optical flow">

                                <b>[22]</b> ZACH C, POCK T, BISCHOF H.A duality based approach for realtime tv-L1 optical flow[C]//Proceedings of Joint Pattern Recognition Symposium.Berlin, Germany:Springer, 2007:214-223.
                            </a>
                        </p>
                        <p id="47">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The opencv library">

                                <b>[23]</b> BRADSKI G.The opencv library[J].Journal of Software Tools, 2000, 25:120-125.
                            </a>
                        </p>
                        <p id="49">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=ImageNet:A largescale hierarchical image database">

                                <b>[24]</b> DENG Jia, DONG Wei, SOCHER R, et al.ImageNet:a large-scale hierarchical image database[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, Washington D.C., USA:IEEE Press, 2009:248-255.
                            </a>
                        </p>
                        <p id="51">
                            <a id="bibliography_25" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Action recognition with improved trajectories">

                                <b>[25]</b> WANG Heng, SCHMID C.Action recognition with improved trajectories[C]//Proceedings of IEEE International Conference on Computer Vision.Washington D.C., USA:IEEE Computer Society, 2014:3551-3558.
                            </a>
                        </p>
                        <p id="53">
                            <a id="bibliography_26" target="_blank" href="http://scholar.cnki.net/result.aspx?q=MoFAP:Amulti-level representation for action recognition">

                                <b>[26]</b> WANG Limin, QIAO Yu, TANG Xiaoou.MoFAP:a multi-level representation for action recognition [J].International Journal of Computer Vision, 2016, 119 (3) :254-271.
                            </a>
                        </p>
                        <p id="55">
                            <a id="bibliography_27" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Action recognition with trajectory-pooled deep-convolutional descriptors">

                                <b>[27]</b> WANG Limin, QIAO Yu, TANG Xiaoou.Action recognition with trajectory-pooled deep-convolutional descriptors[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2015:4305-4314.
                            </a>
                        </p>
                        <p id="57">
                            <a id="bibliography_28" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning spatiotemporal features with 3d convolutional networks">

                                <b>[28]</b> TRAN D, BOURDEV L, FERGUS R, et al.Learning spatiotemporal features with 3D convolutional networks[C]//Proceedings of IEEE International Conference on Computer Vision.Washington D.C., USA:IEEE Computer Society, 2015:4489-4497.
                            </a>
                        </p>
                        <p id="59">
                            <a id="bibliography_29" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep temporal linear encoding networks">

                                <b>[29]</b> DIBA A, SHARMA V, GOOL L V.Deep temporal linear encoding networks[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2017:1541-1550.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJC201905040" />
        <input id="dpi" type="hidden" value="600" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201905040&amp;v=MDg5NDhIOWpNcW85QlpJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSb0Z5M21WcjNQTHo3QmJiRzQ=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
