<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637126180932583750%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJC201910007%26RESULT%3d1%26SIGN%3dqg%252bg13LKbzhBuHSh3eoVOV9udJM%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201910007&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201910007&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201910007&amp;v=MTkwNDlqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSdEZ5cm1Xci9OTHo3QmJiRzRIOWpOcjQ5Rlk0UUtESDg0dlI0VDY=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEVzU2dLNE9nS3dzSERwV3ppOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#35" data-title="0 概述 ">0 概述</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#38" data-title="1 相关工作 ">1 相关工作</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#42" data-title="2 矩阵乘算法 ">2 矩阵乘算法</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#52" data-title="3 加速器设计 ">3 加速器设计</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#61" data-title="3.1 PU结构">3.1 PU结构</a></li>
                                                <li><a href="#80" data-title="3.2 定制AXI Master接口模块">3.2 定制AXI Master接口模块</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#82" data-title="4 矩阵乘在加速器上的映射 ">4 矩阵乘在加速器上的映射</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#83" data-title="4.1 映射算法">4.1 映射算法</a></li>
                                                <li><a href="#88" data-title="4.2 分块调度">4.2 分块调度</a></li>
                                                <li><a href="#93" data-title="4.3 不同规模的矩阵乘">4.3 不同规模的矩阵乘</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#97" data-title="5 实验结果与分析 ">5 实验结果与分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#98" data-title="5.1 实验设置">5.1 实验设置</a></li>
                                                <li><a href="#102" data-title="5.2 性能分析">5.2 性能分析</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#118" data-title="6 结束语 ">6 结束语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#54" data-title="&lt;b&gt;图1 加速器架构&lt;/b&gt;"><b>图1 加速器架构</b></a></li>
                                                <li><a href="#63" data-title="&lt;b&gt;图2 PU结构&lt;/b&gt;"><b>图2 PU结构</b></a></li>
                                                <li><a href="#72" data-title="&lt;b&gt;图3 PE结构&lt;/b&gt;"><b>图3 PE结构</b></a></li>
                                                <li><a href="#85" data-title="&lt;b&gt;图4 分块矩阵乘过程&lt;/b&gt;"><b>图4 分块矩阵乘过程</b></a></li>
                                                <li><a href="#87" data-title="&lt;b&gt;图5 rank-1更新操作过程&lt;/b&gt;"><b>图5 rank-1更新操作过程</b></a></li>
                                                <li><a href="#92" data-title="&lt;b&gt;图6 列、行向量相乘在处理单元上的映射过程&lt;/b&gt;"><b>图6 列、行向量相乘在处理单元上的映射过程</b></a></li>
                                                <li><a href="#100" data-title="&lt;b&gt;表1 开发板硬件参数设置&lt;/b&gt;"><b>表1 开发板硬件参数设置</b></a></li>
                                                <li><a href="#106" data-title="&lt;b&gt;表2 bare-metal环境下2种矩阵乘性能对比结果&lt;/b&gt;"><b>表2 bare-metal环境下2种矩阵乘性能对比结果</b></a></li>
                                                <li><a href="#108" data-title="&lt;b&gt;表3 bare-metal环境下1个PU配置加速器的资源使用情况&lt;/b&gt;"><b>表3 bare-metal环境下1个PU配置加速器的资源使用情况</b></a></li>
                                                <li><a href="#110" data-title="&lt;b&gt;表4 bare-metal环境下1个PU配置加速器的带宽需求情况&lt;/b&gt;"><b>表4 bare-metal环境下1个PU配置加速器的带宽需求情况</b></a></li>
                                                <li><a href="#114" data-title="&lt;b&gt;表5 Linux环境下2种矩阵乘性能对比结果&lt;/b&gt;"><b>表5 Linux环境下2种矩阵乘性能对比结果</b></a></li>
                                                <li><a href="#115" data-title="&lt;b&gt;表6 Linux环境下3个PU配置加速器的资源使用情况&lt;/b&gt;"><b>表6 Linux环境下3个PU配置加速器的资源使用情况</b></a></li>
                                                <li><a href="#116" data-title="&lt;b&gt;表7 Linux环境下3个PU配置加速器的带宽需求情况&lt;/b&gt;"><b>表7 Linux环境下3个PU配置加速器的带宽需求情况</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" title=" JACOB B,KLIGYS S,CHEN Bo,et al.Quantization and training of neural networks for efficient integer-arithmetic-only inference[EB/OL].[2018-07-20].https://arxiv.org/pdf/1712.05877.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Quantization and training of neural networks for efficient integer-arithmetic-only inference">
                                        <b>[1]</b>
                                         JACOB B,KLIGYS S,CHEN Bo,et al.Quantization and training of neural networks for efficient integer-arithmetic-only inference[EB/OL].[2018-07-20].https://arxiv.org/pdf/1712.05877.pdf.
                                    </a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" title=" DETTMERS T.8-bit approximations for parallelism in deep learning[EB/OL].[2018-07-20].https://arxiv.org/pdf/1511.04561.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=8-bit approximations for parallelism in deep learning">
                                        <b>[2]</b>
                                         DETTMERS T.8-bit approximations for parallelism in deep learning[EB/OL].[2018-07-20].https://arxiv.org/pdf/1511.04561.pdf.
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title=" GYSEL P,MOTAMEDI M,GHIASI S.Hardware-oriented approximation of convolutional neural networks[EB/OL].[2018-07-20].https://arxiv.org/pdf/1604.03168.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Hardware-oriented approximation of convolutional neural networks">
                                        <b>[3]</b>
                                         GYSEL P,MOTAMEDI M,GHIASI S.Hardware-oriented approximation of convolutional neural networks[EB/OL].[2018-07-20].https://arxiv.org/pdf/1604.03168.pdf.
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" title=" HAN Song,MAO Huizi,DALLY W J.Deep compression:compressing deep neural networks with pruning,trained quantization and huffman coding[EB/OL].[2018-07-20].https://arxiv.org/pdf/1510.00149.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep compression:compressing deep neural networks with pruning trained quantization and huffman coding">
                                        <b>[4]</b>
                                         HAN Song,MAO Huizi,DALLY W J.Deep compression:compressing deep neural networks with pruning,trained quantization and huffman coding[EB/OL].[2018-07-20].https://arxiv.org/pdf/1510.00149.pdf.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" title=" NARANG S,DIAMOS G.An update to DeepBench with a focus on deep learning inference[EB/OL].[2018-07-20].https://svail.github.io/DeepBench-update." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=An update to DeepBench with a focus on deep learning inference">
                                        <b>[5]</b>
                                         NARANG S,DIAMOS G.An update to DeepBench with a focus on deep learning inference[EB/OL].[2018-07-20].https://svail.github.io/DeepBench-update.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" title=" JANG J,CHOI S,PRASANNA V K K.Area and time efficient implementations of matrix multiplication on FPGAs[C]//Proceedings of IEEE International Conference on Field-programmable Technology.Washington D.C.,USA:IEEE Press,2002:93-100." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Area and time efficient implementation of matrix multiplication on FPGAs">
                                        <b>[6]</b>
                                         JANG J,CHOI S,PRASANNA V K K.Area and time efficient implementations of matrix multiplication on FPGAs[C]//Proceedings of IEEE International Conference on Field-programmable Technology.Washington D.C.,USA:IEEE Press,2002:93-100.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" title=" CAMPBELL S J,KHATRI S P.Resource and delay efficient matrix multiplication using newer FPGA devices[C]//Proceedings of the 16th ACM Great Lakes Symposium on VLSI.New York,USA:ACM Press,2006:308-311." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Resource and delay effi-cient matrix multiplication using newer FPGA devices">
                                        <b>[7]</b>
                                         CAMPBELL S J,KHATRI S P.Resource and delay efficient matrix multiplication using newer FPGA devices[C]//Proceedings of the 16th ACM Great Lakes Symposium on VLSI.New York,USA:ACM Press,2006:308-311.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" title=" EL-ATFY R,DESSOUKY M A,EL-GHITANI H.Accelerating matrix multiplication on FPGAs[C]//Proceedings of the 2nd International Design and Test Workshop.Washington D.C.,USA:IEEE Press,2007:203-204." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Accelerating matrix multiplication on FPGAs">
                                        <b>[8]</b>
                                         EL-ATFY R,DESSOUKY M A,EL-GHITANI H.Accelerating matrix multiplication on FPGAs[C]//Proceedings of the 2nd International Design and Test Workshop.Washington D.C.,USA:IEEE Press,2007:203-204.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" title=" DAVE N,FLEMING K,KING M,et al.Hardware accele-ration of matrix multiplication on a Xilinx FPGA[C]//Proceedings of IEEE/ACM International Conference on Formal Methods and Models for Codesign.Washington D.C.,USA:IEEE Press,2007:97-100." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Hardware accele-ration of matrix multiplication on a Xilinx FPGA">
                                        <b>[9]</b>
                                         DAVE N,FLEMING K,KING M,et al.Hardware accele-ration of matrix multiplication on a Xilinx FPGA[C]//Proceedings of IEEE/ACM International Conference on Formal Methods and Models for Codesign.Washington D.C.,USA:IEEE Press,2007:97-100.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_10" title=" 田翔,周凡,陈耀武,等.基于FPGA的实时双精度浮点矩阵乘法器设计[J].浙江大学学报(工学版),2008,42(9):1611-1615." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZDZC200809028&amp;v=MDc4Mzh0R0ZyQ1VSTE9lWmVSdEZ5cm1Xci9NUHluUmJiRzRIdG5NcG85SGJJUUtESDg0dlI0VDZqNTRPM3pxcUI=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEVzU2dLNE9nS3dzSERwV3ppOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[10]</b>
                                         田翔,周凡,陈耀武,等.基于FPGA的实时双精度浮点矩阵乘法器设计[J].浙江大学学报(工学版),2008,42(9):1611-1615.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_11" title=" 张婷.嵌入式环境下浮点矩阵乘法的FPGA加速关键技术研究[D].长沙:湖南大学,2013." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=1014168361.nh&amp;v=MDEyMDVySytGdExLcnBFYlBJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSdEZ5cm1Xci9NVkYyNkc=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEVzU2dLNE9nS3dzSERwV3ppOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[11]</b>
                                         张婷.嵌入式环境下浮点矩阵乘法的FPGA加速关键技术研究[D].长沙:湖南大学,2013.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_12" title=" 马邺晨,李醒飞.用于导航解算的矩阵运算硬件加速器设计[J].计算机工程,2014,40(8):259-263." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201408050&amp;v=MjY2ODVMT2VaZVJ0RnlybVdyL01MejdCYmJHNEg5WE1wNDlBWklRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEVzU2dLNE9nS3dzSERwV3ppOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[12]</b>
                                         马邺晨,李醒飞.用于导航解算的矩阵运算硬件加速器设计[J].计算机工程,2014,40(8):259-263.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_13" title=" Intel Corporation.Intel Xeon Phi delivers competitive performance for deep learning and getting better fast[EB/OL].[2018-07-20].https://software.intel.com/en-us/articles/intel-xeon-phi-delivers-competitive-performance-for-deep-learningand-getting-better-fast." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Intel Xeon Phi delivers competitive performance for deep learning and getting better fast">
                                        <b>[13]</b>
                                         Intel Corporation.Intel Xeon Phi delivers competitive performance for deep learning and getting better fast[EB/OL].[2018-07-20].https://software.intel.com/en-us/articles/intel-xeon-phi-delivers-competitive-performance-for-deep-learningand-getting-better-fast.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_14" title=" MOSS D J M,KRISHNAN S,NURVITADHI E,et al.A customizable matrix multiplication framework for the Intel HARPv2 Xeon+ FPGA platform:a deep learning case study[C]//Proceedings of 2018 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays.New York,USA:ACM Press,2018:107-116." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A customizable matrix multiplication framework for the Intel HARPv2 Xeon+ FPGA platform:a deep learning case study">
                                        <b>[14]</b>
                                         MOSS D J M,KRISHNAN S,NURVITADHI E,et al.A customizable matrix multiplication framework for the Intel HARPv2 Xeon+ FPGA platform:a deep learning case study[C]//Proceedings of 2018 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays.New York,USA:ACM Press,2018:107-116.
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_15" title=" KRIZHEVSKY A,SUTSKEVER I,HINTON G E.ImageNet classification with deep convolutional neural networks[C]//Proceedings of the 25th International Conference on Neural Information Processing Systems.New York,USA:ACM Press,2012:1097-1105." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image Net classification with deep convolutional neural networks">
                                        <b>[15]</b>
                                         KRIZHEVSKY A,SUTSKEVER I,HINTON G E.ImageNet classification with deep convolutional neural networks[C]//Proceedings of the 25th International Conference on Neural Information Processing Systems.New York,USA:ACM Press,2012:1097-1105.
                                    </a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_16" title=" Xillybus.Xillybus host application programming guide for Linux[EB/OL].[2018-07-20].http://xillybus.com/downloads/doc/xillybus_host_programming_guide_linux.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Xillybus host application programming guide for Linux">
                                        <b>[16]</b>
                                         Xillybus.Xillybus host application programming guide for Linux[EB/OL].[2018-07-20].http://xillybus.com/downloads/doc/xillybus_host_programming_guide_linux.pdf.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJC" target="_blank">计算机工程</a>
                2019,45(10),40-45 DOI:10.19678/j.issn.1000-3428.0052372            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>面向深度学习推理的矩阵乘法加速器设计</b></span>
 <span class="shoufa"></span>                                     </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%86%89%E5%BE%B7%E6%88%90&amp;code=40276363&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEVzU2dLNE9nS3dzSERwV3ppOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">冉德成</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%90%B4%E4%B8%9C&amp;code=35352347&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEVzU2dLNE9nS3dzSERwV3ppOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">吴东</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%92%B1%E7%A3%8A&amp;code=35352343&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEVzU2dLNE9nS3dzSERwV3ppOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">钱磊</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%95%B0%E5%AD%A6%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%85%88%E8%BF%9B%E8%AE%A1%E7%AE%97%E5%9B%BD%E5%AE%B6%E9%87%8D%E7%82%B9%E5%AE%9E%E9%AA%8C%E5%AE%A4&amp;code=0199248&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEVzU2dLNE9nS3dzSERwV3ppOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">数学工程与先进计算国家重点实验室</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>为满足深度学习推理中对不同规模矩阵乘法的计算需求,提出一种基于Zynq SoC平台的整数矩阵乘法加速器。采用基于总线广播的并行结构,充分利用片上数据的重用性并最小化中间累加结果的移动范围,以降低外部DRAM的访问需求。通过动态调整矩阵分块的大小,使加速器在计算形状不规则的矩阵乘时保持较高效率。实验结果表明,在DeepBench测试基准下,该加速器可对双核ARM Cortex-A9 CPU的矩阵乘运算实现8.4倍的加速效果。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%95%B4%E6%95%B0%E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEVzU2dLNE9nS3dzSERwV3ppOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">整数矩阵乘法;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8A%A0%E9%80%9F%E5%99%A8&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEVzU2dLNE9nS3dzSERwV3ppOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">加速器;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8F%AF%E7%BC%96%E7%A8%8B%E7%89%87%E4%B8%8A%E7%B3%BB%E7%BB%9F&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEVzU2dLNE9nS3dzSERwV3ppOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">可编程片上系统;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8E%A8%E7%90%86&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEVzU2dLNE9nS3dzSERwV3ppOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度学习推理;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%88%86%E5%9D%97%E6%96%B9%E6%A1%88&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEVzU2dLNE9nS3dzSERwV3ppOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">分块方案;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=DeepBench%E6%B5%8B%E8%AF%95&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEVzU2dLNE9nS3dzSERwV3ppOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">DeepBench测试;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    冉德成(1989—),男,硕士研究生,主研方向为可重构加速计算;E-mail:dcran@foxmail.com;
                                </span>
                                <span>
                                    吴东,研究员、博士;;
                                </span>
                                <span>
                                    钱磊,工程师、硕士。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-08-10</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金(61732010);</span>
                    </p>
            </div>
                    <h1><b>Design of Matrix Multiplication Accelerator for Deep Learning Inference</b></h1>
                    <h2>
                    <span>RAN Decheng</span>
                    <span>WU Dong</span>
                    <span>QIAN Lei</span>
            </h2>
                    <h2>
                    <span>State Key Laboratory of Mathematical Engineering and Advanced Computing</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>An integer matrix multiplication accelerator based on Zynq SoC platform is proposed to satisfy the computing requirements of matrix multiplication of different sizes in deep learning inference.The parallel architecture based on bus broadcasting makes full use of the reusability of on chip data and minimizes the moving range of intermediate cumulative result to reduce the access requirement of external DRAM.By dynamically adjusting the size of matrix blocks,the accelerator can maintain high efficiency in calculating matrix multiplication with irregular shape.Experimental results show that under DeepBench test benchmark,the accelerator can achieve 8.4 times acceleration effect for matrix multiplication of dual-core ARM Cortex-A9 CPU.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=integer%20matrix%20multiplication&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEVzU2dLNE9nS3dzSERwV3ppOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">integer matrix multiplication;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=accelerator&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEVzU2dLNE9nS3dzSERwV3ppOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">accelerator;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=programmable%20System%20on%20Chip(SoC)&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEVzU2dLNE9nS3dzSERwV3ppOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">programmable System on Chip(SoC);</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=deep%20learning%20inference&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEVzU2dLNE9nS3dzSERwV3ppOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">deep learning inference;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=blocking%20scheme&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEVzU2dLNE9nS3dzSERwV3ppOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">blocking scheme;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=DeepBench%20test&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEVzU2dLNE9nS3dzSERwV3ppOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">DeepBench test;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-08-10</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="35" name="35" class="anchor-tag">0 概述</h3>
                <div class="p1">
                    <p id="36">在移动端部署深度学习模型需要高效精确的推理方案<citation id="121" type="reference"><link href="3" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>。通用矩阵乘法是深度神经网络(Deep Neural Network,DNN)的核心,也是深度学习推理计算中比较耗时的部分。实现卷积计算的一种主流方法是将其转换为通用矩阵乘计算。此外,循环神经网络(Recurrent Neural Network,RNN)中也包含大量通用矩阵乘计算。因此,矩阵乘法的加速设计对边缘设备中的深度学习推理具有重要意义。</p>
                </div>
                <div class="p1">
                    <p id="37">目前,商用的深度学习应用大多使用32 bit单精度浮点数进行训练和推理。研究显示,使用更低的数值精度执行深度学习推理可以保持相同的准确性,如图像分类等很多应用只需8 bit整数或更低定点数值精度即可取得可接受的推理准确性<citation id="123" type="reference"><link href="5" rel="bibliography" /><link href="7" rel="bibliography" /><link href="9" rel="bibliography" /><sup>[<a class="sup">2</a>,<a class="sup">3</a>,<a class="sup">4</a>]</sup></citation>。百度在深度学习基准测试工具DeepBench<citation id="122" type="reference"><link href="11" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>中将深度推理的最低精度需求确定为8 bit相乘和32 bit累加,本文以该精度要求作为设计参照,基于低成本的片上可编程系统,设计并实现一种整数矩阵乘法加速器。针对不同规模矩阵乘法的计算需求,充分利用数据的重用性,采用在片上保存中间累加结果的方式,使加速器具备高性能的同时降低外部DRAM的访问需求。对于形状不规则的矩阵乘,采取矩阵分块可变的设计,使加速器可以采用灵活的分块方案,从而提高其性能。</p>
                </div>
                <h3 id="38" name="38" class="anchor-tag">1 相关工作</h3>
                <div class="p1">
                    <p id="39">文献<citation id="124" type="reference">[<a class="sup">6</a>]</citation>指出,在对矩阵乘法器进行设计时只需固定带宽,其所需的存储单元大小随矩阵规模的增大而成比例增长。文献<citation id="125" type="reference">[<a class="sup">7</a>]</citation>提出一种基于广播的二维阵列并行矩阵乘法器结构,但该乘法器将输入、输出矩阵都存储在片上,使得所需的存储空间随着矩阵维数的增加而增加,因此,其难以处理规模较大的矩阵。文献<citation id="126" type="reference">[<a class="sup">8</a>]</citation>对上述矩阵乘法器进行改进,将输入矩阵存储在片上,每个输出矩阵分块在计算完成后传输至外部存储器,降低了存储空间需求,但其仍不能处理大规模矩阵。文献<citation id="127" type="reference">[<a class="sup">9</a>]</citation>设计一种软硬件协同的定点矩阵乘加速结构,其采用多处理单元并行的方式,每个处理单元负责一个矩阵分块相乘,通过片上通信网络实现数据加载。该矩阵乘加速器优化了与外部DRAM间的通信,取得了较好的加速效果,但其仅支持对维数为64倍数的矩阵进行计算。</p>
                </div>
                <div class="p1">
                    <p id="40">在浮点矩阵乘加速方面,文献<citation id="129" type="reference">[<a class="sup">10</a>,<a class="sup">11</a>]</citation>设计基于广播的二维阵列并行结构浮点矩阵乘加速器,并取得了较好的加速效果,但该加速器的高带宽要求限制了其性能提升。文献<citation id="128" type="reference">[<a class="sup">12</a>]</citation>提出一种基于FPGA/SOPC的脉动阵列矩阵乘结构,但该结构仅能对较小规模的矩阵乘计算进行测试。</p>
                </div>
                <div class="p1">
                    <p id="41">与传统通用矩阵乘不同,深度学习应用中的矩阵乘计算包含大量规模小且形状不规则的矩阵乘<citation id="130" type="reference"><link href="27" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>。文献<citation id="131" type="reference">[<a class="sup">14</a>]</citation>提出一种基于Intel HARPv2 Xeon与FPGA平台的可定制矩阵乘计算框架。该框架针对深度学习计算进行优化,采用运行时可配置的存储交错机制,优化前小规模矩阵的计算效率低于20%,而优化后计算效率提升了2.7倍以上;对于AlexNet卷积神经网络<citation id="132" type="reference"><link href="31" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>,优化后各层计算取得了1.3倍～4倍的性能提升。该研究基于面向数据中心的高端CPU+FPGA平台,支持深度学习训练和推理,但难以应用于低成本的边缘端设备。</p>
                </div>
                <h3 id="42" name="42" class="anchor-tag">2 矩阵乘算法</h3>
                <div class="p1">
                    <p id="43">在本文中,(子)矩阵用大写字母表示,标量用希腊小写字母表示。矩阵乘<i><b>C</b></i>=<i><b>AB</b></i>可以表示为:</p>
                </div>
                <div class="p1">
                    <p id="44" class="code-formula">
                        <mathml id="44"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>γ</mi><msub><mrow></mrow><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>Ν</mi><mo>-</mo><mn>1</mn></mrow></munderover><mi>α</mi></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mo>,</mo><mi>k</mi></mrow></msub><mo>×</mo><mi>β</mi><msub><mrow></mrow><mrow><mi>k</mi><mo>,</mo><mi>j</mi></mrow></msub><mo>,</mo><mn>0</mn><mo>≤</mo><mi>i</mi><mo>&lt;</mo><mi>Μ</mi><mo>,</mo><mn>0</mn><mo>≤</mo><mi>j</mi><mo>&lt;</mo><mi>R</mi></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="45">矩阵<i><b>A</b></i>、<i><b>B</b></i>和<i><b>C</b></i>的大小分别为<i>M</i>×<i>N</i>、<i>N</i>×<i>R</i>和<i>M</i>×<i>R</i>。上述计算表达式可由算法1描述。</p>
                </div>
                <div class="p1">
                    <p id="46"><b>算法1</b> 矩阵乘算法</p>
                </div>
                <div class="p1">
                    <p id="47">1.for i ← 1 to M do</p>
                </div>
                <div class="p1">
                    <p id="48">2.for j ← 1 to R do</p>
                </div>
                <div class="p1">
                    <p id="49">3.for k ← 1 to N do</p>
                </div>
                <div class="p1">
                    <p id="50">4.γ<sub>i,j</sub>←γ<sub>i,j</sub>+α<sub>i,k</sub>×β<sub>k,j</sub></p>
                </div>
                <div class="p1">
                    <p id="51">在算法1中,3个循环的位置可以交换。根据<i>k</i>循环位置的不同,矩阵乘可分为内积、中间积和外积3种方式。</p>
                </div>
                <h3 id="52" name="52" class="anchor-tag">3 加速器设计</h3>
                <div class="p1">
                    <p id="53">本文加速器架构如图1所示。</p>
                </div>
                <div class="area_img" id="54">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201910007_054.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEVzU2dLNE9nS3dzSERwV3ppOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 加速器架构" src="Detail/GetImg?filename=images/JSJC201910007_054.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEVzU2dLNE9nS3dzSERwV3ppOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图1 加速器架构</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201910007_054.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEVzU2dLNE9nS3dzSERwV3ppOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="55">加速器主要由以下5个模块构成:</p>
                </div>
                <div class="p1">
                    <p id="56">1)ARM Cortex-A9 CPU:运行在CPU上的程序控制任务执行模块,通过通用接口与加速器的控制器进行通信。</p>
                </div>
                <div class="p1">
                    <p id="57">2)控制器:负责接收CPU发出的任务信息,将任务分发到处理单元(Processing Unit,PU)执行模块,并将数据传输需求发送到定制AXI Master接口。</p>
                </div>
                <div class="p1">
                    <p id="58">3)定制AXI Master接口:负责所有的外部DRAM数据访问,包括矩阵<i><b>A</b></i>、<i><b>B</b></i>读入和矩阵<i><b>C</b></i>写出。</p>
                </div>
                <div class="p1">
                    <p id="59">4)定制AXI Lite Slave接口:负责CPU与控制器之间的通信。</p>
                </div>
                <div class="p1">
                    <p id="60">5)处理单元PU:负责分块层次的矩阵计算。</p>
                </div>
                <h4 class="anchor-tag" id="61" name="61">3.1 PU结构</h4>
                <div class="p1">
                    <p id="62">PU负责分块层次的运算,总体结构如图2所示,其主要包括<i>n</i><sub><i>r</i></sub>×<i>n</i><sub><i>r</i></sub>大小的由处理引擎(Processing Engine,PE)组成的二维阵列和矩阵<i><b>A</b></i>、<i><b>B</b></i>数据缓存。为便于说明,本文以<i>n</i><sub><i>r</i></sub>×<i>n</i><sub><i>r</i></sub>=4×4规模的PU为例进行讨论。</p>
                </div>
                <div class="area_img" id="63">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201910007_063.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEVzU2dLNE9nS3dzSERwV3ppOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 PU结构" src="Detail/GetImg?filename=images/JSJC201910007_063.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEVzU2dLNE9nS3dzSERwV3ppOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图2 PU结构</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201910007_063.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEVzU2dLNE9nS3dzSERwV3ppOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="64" name="64">3.1.1 基本操作</h4>
                <div class="p1">
                    <p id="65">假定矩阵<i><b>C</b></i>、<i><b>A</b></i>和<i><b>B</b></i>都存放到片上,大小分别为4×4、4×<i>n</i>和<i>n</i>×4,<i><b>C</b></i>=<i><b>AB</b></i>的计算可表示为:</p>
                </div>
                <div class="p1">
                    <p id="66" class="code-formula">
                        <mathml id="66"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>(</mo><mrow><mtable><mtr><mtd><mi>γ</mi><msub><mrow></mrow><mrow><mn>0</mn><mo>,</mo><mn>0</mn></mrow></msub></mtd><mtd><mi>γ</mi><msub><mrow></mrow><mrow><mn>0</mn><mo>,</mo><mn>1</mn></mrow></msub></mtd><mtd><mi>γ</mi><msub><mrow></mrow><mrow><mn>0</mn><mo>,</mo><mn>2</mn></mrow></msub></mtd><mtd><mi>γ</mi><msub><mrow></mrow><mrow><mn>0</mn><mo>,</mo><mn>3</mn></mrow></msub></mtd></mtr><mtr><mtd><mi>γ</mi><msub><mrow></mrow><mrow><mn>1</mn><mo>,</mo><mn>0</mn></mrow></msub></mtd><mtd><mi>γ</mi><msub><mrow></mrow><mrow><mn>1</mn><mo>,</mo><mn>1</mn></mrow></msub></mtd><mtd><mi>γ</mi><msub><mrow></mrow><mrow><mn>1</mn><mo>,</mo><mn>2</mn></mrow></msub></mtd><mtd><mi>γ</mi><msub><mrow></mrow><mrow><mn>1</mn><mo>,</mo><mn>3</mn></mrow></msub></mtd></mtr><mtr><mtd><mi>γ</mi><msub><mrow></mrow><mrow><mn>2</mn><mo>,</mo><mn>0</mn></mrow></msub></mtd><mtd><mi>γ</mi><msub><mrow></mrow><mrow><mn>2</mn><mo>,</mo><mn>1</mn></mrow></msub></mtd><mtd><mi>γ</mi><msub><mrow></mrow><mrow><mn>2</mn><mo>,</mo><mn>2</mn></mrow></msub></mtd><mtd><mi>γ</mi><msub><mrow></mrow><mrow><mn>2</mn><mo>,</mo><mn>3</mn></mrow></msub></mtd></mtr><mtr><mtd><mi>γ</mi><msub><mrow></mrow><mrow><mn>3</mn><mo>,</mo><mn>0</mn></mrow></msub></mtd><mtd><mi>γ</mi><msub><mrow></mrow><mrow><mn>3</mn><mo>,</mo><mn>1</mn></mrow></msub></mtd><mtd><mi>γ</mi><msub><mrow></mrow><mrow><mn>3</mn><mo>,</mo><mn>2</mn></mrow></msub></mtd><mtd><mi>γ</mi><msub><mrow></mrow><mrow><mn>3</mn><mo>,</mo><mn>3</mn></mrow></msub></mtd></mtr></mtable></mrow><mo>)</mo></mrow><mo>+</mo><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi mathvariant="bold-italic">i</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>n</mi><mo>-</mo><mn>1</mn></mrow></munderover><mrow></mrow></mstyle><mrow><mo>(</mo><mtable columnalign="left"><mtr><mtd><mi>α</mi><msub><mrow></mrow><mrow><mn>0</mn><mo>,</mo><mi>i</mi></mrow></msub></mtd></mtr><mtr><mtd><mi>α</mi><msub><mrow></mrow><mrow><mn>1</mn><mo>,</mo><mi>i</mi></mrow></msub></mtd></mtr><mtr><mtd><mi>α</mi><msub><mrow></mrow><mrow><mn>2</mn><mo>,</mo><mi>i</mi></mrow></msub></mtd></mtr><mtr><mtd><mi>α</mi><msub><mrow></mrow><mrow><mn>3</mn><mo>,</mo><mi>i</mi></mrow></msub></mtd></mtr></mtable><mo>)</mo></mrow><mo stretchy="false">(</mo><mi>β</mi><msub><mrow></mrow><mrow><mi>i</mi><mo>,</mo><mn>0</mn></mrow></msub><mtext> </mtext><mi>β</mi><msub><mrow></mrow><mrow><mi>i</mi><mo>,</mo><mn>1</mn></mrow></msub><mtext> </mtext><mi>β</mi><msub><mrow></mrow><mrow><mi>i</mi><mo>,</mo><mn>2</mn></mrow></msub><mtext> </mtext><mi>β</mi><msub><mrow></mrow><mrow><mi>i</mi><mo>,</mo><mn>3</mn></mrow></msub><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="67"><i><b>C</b></i>的第<i>i</i>次更新操作为:</p>
                </div>
                <div class="p1">
                    <p id="68" class="code-formula">
                        <mathml id="68"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>(</mo><mtable columnalign="left"><mtr><mtd><mi>γ</mi><msub><mrow></mrow><mrow><mn>0</mn><mo>,</mo><mn>0</mn></mrow></msub><mo>+</mo><mi>α</mi><msub><mrow></mrow><mrow><mn>0</mn><mo>,</mo><mi>i</mi></mrow></msub><mi>β</mi><msub><mrow></mrow><mrow><mi>i</mi><mo>,</mo><mn>0</mn></mrow></msub><mtext> </mtext><mi>γ</mi><msub><mrow></mrow><mrow><mn>0</mn><mo>,</mo><mn>1</mn></mrow></msub><mo>+</mo><mi>α</mi><msub><mrow></mrow><mrow><mn>0</mn><mo>,</mo><mn>1</mn></mrow></msub><mi>β</mi><msub><mrow></mrow><mrow><mi>i</mi><mo>,</mo><mn>1</mn></mrow></msub><mtext> </mtext><mo>⋯</mo><mtext> </mtext><mi>γ</mi><msub><mrow></mrow><mrow><mn>0</mn><mo>,</mo><mn>3</mn></mrow></msub><mo>+</mo><mi>α</mi><msub><mrow></mrow><mrow><mn>0</mn><mo>,</mo><mi>i</mi></mrow></msub><mi>β</mi><msub><mrow></mrow><mrow><mi>i</mi><mo>,</mo><mn>3</mn></mrow></msub></mtd></mtr><mtr><mtd><mi>γ</mi><msub><mrow></mrow><mrow><mn>1</mn><mo>,</mo><mn>0</mn></mrow></msub><mo>+</mo><mi>α</mi><msub><mrow></mrow><mrow><mn>1</mn><mo>,</mo><mi>i</mi></mrow></msub><mi>β</mi><msub><mrow></mrow><mrow><mi>i</mi><mo>,</mo><mn>0</mn></mrow></msub><mtext> </mtext><mi>γ</mi><msub><mrow></mrow><mrow><mn>1</mn><mo>,</mo><mn>1</mn></mrow></msub><mo>+</mo><mi>α</mi><msub><mrow></mrow><mrow><mn>1</mn><mo>,</mo><mn>1</mn></mrow></msub><mi>β</mi><msub><mrow></mrow><mrow><mi>i</mi><mo>,</mo><mn>1</mn></mrow></msub><mtext> </mtext><mo>⋯</mo><mtext> </mtext><mi>γ</mi><msub><mrow></mrow><mrow><mn>1</mn><mo>,</mo><mn>3</mn></mrow></msub><mo>+</mo><mi>α</mi><msub><mrow></mrow><mrow><mn>1</mn><mo>,</mo><mi>i</mi></mrow></msub><mi>β</mi><msub><mrow></mrow><mrow><mi>i</mi><mo>,</mo><mn>3</mn></mrow></msub></mtd></mtr><mtr><mtd><mtext> </mtext><mtext> </mtext><mo>⋮</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mo>⋮</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mo>⋮</mo></mtd></mtr><mtr><mtd><mi>γ</mi><msub><mrow></mrow><mrow><mn>3</mn><mo>,</mo><mn>0</mn></mrow></msub><mo>+</mo><mi>α</mi><msub><mrow></mrow><mrow><mn>3</mn><mo>,</mo><mi>i</mi></mrow></msub><mi>β</mi><msub><mrow></mrow><mrow><mi>i</mi><mo>,</mo><mn>0</mn></mrow></msub><mtext> </mtext><mi>γ</mi><msub><mrow></mrow><mrow><mn>3</mn><mo>,</mo><mn>1</mn></mrow></msub><mo>+</mo><mi>α</mi><msub><mrow></mrow><mrow><mn>3</mn><mo>,</mo><mn>1</mn></mrow></msub><mi>β</mi><msub><mrow></mrow><mrow><mi>i</mi><mo>,</mo><mn>1</mn></mrow></msub><mtext> </mtext><mo>⋯</mo><mtext> </mtext><mi>γ</mi><msub><mrow></mrow><mrow><mn>3</mn><mo>,</mo><mn>3</mn></mrow></msub><mo>+</mo><mi>α</mi><msub><mrow></mrow><mrow><mn>3</mn><mo>,</mo><mi>i</mi></mrow></msub><mi>β</mi><msub><mrow></mrow><mrow><mi>i</mi><mo>,</mo><mn>3</mn></mrow></msub></mtd></mtr></mtable><mo>)</mo></mrow></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="69">将<i><b>C</b></i>的每次更新操作称为rank-1更新。矩阵<i><b>A</b></i>和<i><b>B</b></i>的相乘过程如下:<i><b>A</b></i>矩阵的第1列通过行总线广播到PE阵列,<i><b>B</b></i>矩阵的第1行通过列总线广播到PE阵列,每个PE进行乘加计算,更新后存储在其内部的<i><b>C</b></i>矩阵中作为中间结果。以此类推,经过<i>n</i>次rank-1更新后得到<i><b>C</b></i>矩阵。</p>
                </div>
                <h4 class="anchor-tag" id="70" name="70">3.1.2 PE结构</h4>
                <div class="p1">
                    <p id="71">PE是基本的运算单元,主要包括乘加部件、本地存储器(用于存储矩阵<i><b>C</b></i>的中间结果)、行列通信接口以及Cast模块,其结构如图3所示。</p>
                </div>
                <div class="area_img" id="72">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201910007_072.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEVzU2dLNE9nS3dzSERwV3ppOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 PE结构" src="Detail/GetImg?filename=images/JSJC201910007_072.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEVzU2dLNE9nS3dzSERwV3ppOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图3 PE结构</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201910007_072.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEVzU2dLNE9nS3dzSERwV3ppOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="73">PE接收行广播总线上的矩阵<i><b>A</b></i>元素值<i>α</i><sub><i>i</i></sub><sub>,</sub><sub><i>j</i></sub>和列广播总线上的矩阵<i><b>B</b></i>元素值<i>β</i><sub><i>k</i></sub><sub>,</sub><sub><i>j</i></sub>,并从本地读取矩阵<i><b>C</b></i>的中间结果<i>γ</i><sub><i>i</i></sub><sub>,</sub><sub><i>j</i></sub>,计算<i>γ</i><sub><i>i</i></sub><sub>,</sub><sub><i>j</i></sub>+=<i>α</i><sub><i>i</i></sub><sub>,</sub><sub><i>k</i></sub>×<i>β</i><sub><i>k</i></sub><sub>,</sub><sub><i>j</i></sub>,将<i>γ</i><sub><i>i</i></sub><sub>,</sub><sub><i>j</i></sub>存储到本地,完成一次矩阵<i><b>C</b></i>中间结果的更新操作。</p>
                </div>
                <div class="p1">
                    <p id="74">多路选择器MUX的作用是在计算一个新的矩阵<i><b>C</b></i>元素时,将第一次累加值设为0。</p>
                </div>
                <div class="p1">
                    <p id="75">Cast模块用于内部32 bit累加结果到8 bit输出的转换,其根据具体深度神经网络量化方法而定。在得到矩阵<i><b>C</b></i>的最终分块结果后,通过列输出总线写到外部DRAM中。</p>
                </div>
                <h4 class="anchor-tag" id="76" name="76">3.1.3 矩阵数据缓存</h4>
                <div class="p1">
                    <p id="77">矩阵<i><b>A</b></i>、<i><b>B</b></i>数据缓存采用乒乓存储设计,将存储空间分为2个部分,同时进行计算和读取新数据,以保证计算效率。</p>
                </div>
                <h4 class="anchor-tag" id="78" name="78">3.1.4 PU控制逻辑</h4>
                <div class="p1">
                    <p id="79">PU的控制逻辑采用分布式结构,每个PE有相同的控制器,执行预定的通信、存储和计算操作。PU的全局控制和握手信号仅限于粗粒度地对PE进行启动和停止,而无需设置PE的内部状态。</p>
                </div>
                <h4 class="anchor-tag" id="80" name="80">3.2 定制AXI Master接口模块</h4>
                <div class="p1">
                    <p id="81">由于矩阵存储在外部DRAM中,因此高效的片内外通信对加速器的性能有重要影响。本文设计定制AXI Master接口连接到DDR存储控制器以进行数据传输,接口模块的读写地址信息由控制器发送,该方式具有较高的传输效率,且不影响CPU的工作。</p>
                </div>
                <h3 id="82" name="82" class="anchor-tag">4 矩阵乘在加速器上的映射</h3>
                <h4 class="anchor-tag" id="83" name="83">4.1 映射算法</h4>
                <div class="p1">
                    <p id="84">对矩阵进行分块,将<i><b>A</b></i>矩阵划分为大小为<i>S</i><sub><i>i</i></sub>×<i>N</i>的分块<i><b>PA</b></i><sub><i>i</i></sub>,<i><b>B</b></i>矩阵划分为大小为<i>N</i>×<i>S</i><sub><i>j</i></sub>的分块<i><b>PB</b></i><sub><i>j</i></sub>。<i><b>PA</b></i><sub><i>i</i></sub>与<i><b>PB</b></i><sub><i>j</i></sub>相乘得到大小为<i>S</i><sub><i>i</i></sub>×<i>S</i><sub><i>j</i></sub>的<i><b>C</b></i>的分块<i><b>PC</b></i><sub><i>i</i></sub><sub>,</sub><sub><i>j</i></sub>。以串行方式执行<i><b>PA</b></i><sub><i>i</i></sub>与<i><b>PB</b></i><sub><i>j</i></sub>的相乘操作,即得到整个<i><b>C</b></i>矩阵。分块矩阵乘过程如图4所示。</p>
                </div>
                <div class="area_img" id="85">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201910007_085.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEVzU2dLNE9nS3dzSERwV3ppOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 分块矩阵乘过程" src="Detail/GetImg?filename=images/JSJC201910007_085.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEVzU2dLNE9nS3dzSERwV3ppOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图4 分块矩阵乘过程</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201910007_085.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEVzU2dLNE9nS3dzSERwV3ppOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="86"><i><b>PA</b></i><sub><i>i</i></sub>与<i><b>PB</b></i><sub><i>j</i></sub>相乘采用rank-1更新方案,如图5所示。矩阵<i><b>A</b></i>的分块<i><b>PA</b></i><sub><i>i</i></sub>可看作<i>N</i>个<i>S</i><sub><i>i</i></sub>×1的列向量<i><b>PA</b></i><sub><i>i</i></sub><sub>,</sub><sub><i>k</i></sub>,矩阵<i><b>B</b></i>的分块<i><b>PB</b></i><sub><i>j</i></sub>可看作<i>N</i>个1×<i>S</i><sub><i>j</i></sub>的行向量<i><b>PB</b></i><sub><i>j</i></sub><sub>,</sub><sub><i>k</i></sub>。相应的列、行向量相乘得到矩阵<i><b>C</b></i>的分块<i><b>PC</b></i><sub><i>i</i></sub><sub>,</sub><sub><i>j</i></sub>的一层,<i>N</i>层相加得到<i><b>PC</b></i><sub><i>i</i></sub><sub>,</sub><sub><i>j</i></sub>的最终结果。</p>
                </div>
                <div class="area_img" id="87">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201910007_087.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEVzU2dLNE9nS3dzSERwV3ppOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 rank-1更新操作过程" src="Detail/GetImg?filename=images/JSJC201910007_087.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEVzU2dLNE9nS3dzSERwV3ppOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图5 rank-1更新操作过程</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201910007_087.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEVzU2dLNE9nS3dzSERwV3ppOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="88" name="88">4.2 分块调度</h4>
                <div class="p1">
                    <p id="89">分块是将大规模矩阵乘计算映射到固定规模加速器的常用方式。分块的大小有<i>S</i><sub><i>i</i></sub>和<i>S</i><sub><i>j</i></sub> 2个参数。文献<citation id="133" type="reference">[<a class="sup">7</a>,<a class="sup">8</a>]</citation>设置<i>S</i><sub><i>i</i></sub>=<i>S</i><sub><i>j</i></sub>=<i>n</i><sub><i>r</i></sub>。此时,矩阵<i><b>A</b></i>、<i><b>B</b></i>的每个元素均重用<i>n</i><sub><i>r</i></sub>次,每个周期的带宽需求是2×<i>n</i><sub><i>r</i></sub>个元素,矩阵<i><b>C</b></i>的存储需求是<i>n</i><sub><i>r</i></sub>×<i>n</i><sub><i>r</i></sub>个中间结果。因此,上述分块方案的优点是存储空间需求较低,但同时引起了带宽要求随阵列规模的增大而线性增长的问题。采用该方案,在阵列规模增大到某值时,带宽要求会成为计算性能提升的制约因素。</p>
                </div>
                <div class="p1">
                    <p id="90">为充分重用片上数据,本文对上述方案进行改进,设置<i>S</i><sub><i>i</i></sub>=<i>μ</i>×<i>n</i><sub><i>r</i></sub>,<i>S</i><sub><i>j</i></sub>=<i>ν</i>×<i>n</i><sub><i>r</i></sub>。此时,矩阵<i><b>A</b></i>的每个元素重用<i>ν</i>×<i>n</i><sub><i>r</i></sub>次,矩阵<i><b>B</b></i>的每个元素重用<i>μ</i>×<i>n</i><sub><i>r</i></sub>次,每个周期的带宽需求是<mathml id="120"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mi>n</mi><msub><mrow></mrow><mi>r</mi></msub></mrow><mi>ν</mi></mfrac><mo>+</mo><mfrac><mrow><mi>n</mi><msub><mrow></mrow><mi>r</mi></msub></mrow><mi>μ</mi></mfrac></mrow></math></mathml>个元素,矩阵<i><b>C</b></i>的存储需求是<i>μ</i>×<i>ν</i>×<i>n</i><sub><i>r</i></sub>×<i>n</i><sub><i>r</i></sub>个中间结果。在计算过程中,矩阵<i><b>C</b></i>的中间结果存储在片上,计算完成后才传输至片外,这能够最小化中间结果数据的移动需求。根据片上存储资源和外部带宽情况,选择合适的<i>S</i><sub><i>i</i></sub>和<i>S</i><sub><i>j</i></sub>,可以提高系统性能并降低外部DRAM的访问需求。</p>
                </div>
                <div class="p1">
                    <p id="91">对应本文分块方案,需要采用分时计算的方式,在<i>n</i><sub><i>r</i></sub>×<i>n</i><sub><i>r</i></sub>阵列上完成<i>S</i><sub><i>i</i></sub>×1的列向量<i><b>PA</b></i><sub><i>i</i></sub><sub>,</sub><sub><i>k</i></sub>与1×<i>S</i><sub><i>j</i></sub>的行向量<i><b>PB</b></i><sub><i>j</i></sub><sub>,</sub><sub><i>k</i></sub>相乘,分时计算调度如图6所示。将<i><b>PA</b></i><sub><i>i</i></sub><sub>,</sub><sub><i>k</i></sub>分为大小为<i>n</i><sub><i>r</i></sub>×1的分块,将<i><b>PB</b></i><sub><i>j</i></sub><sub>,</sub><sub><i>k</i></sub>分为大小为1×<i>n</i><sub><i>r</i></sub>的分块,首先固定<i><b>PA</b></i><sub><i>i</i></sub><sub>,</sub><sub><i>k</i></sub>的一个分块,依次与<i>S</i><sub><i>j</i></sub>/<i>n</i><sub><i>r</i></sub>个<i><b>PB</b></i><sub><i>j</i></sub><sub>,</sub><sub><i>k</i></sub>的分块相乘,然后固定<i><b>PA</b></i><sub><i>i</i></sub><sub>,</sub><sub><i>k</i></sub>的下一个分块,该过程共需(<i>S</i><sub><i>i</i></sub>/<i>n</i><sub><i>r</i></sub>)×(<i>S</i><sub><i>j</i></sub>/<i>n</i><sub><i>r</i></sub>)次分块相乘,最终完成列、行向量相乘。</p>
                </div>
                <div class="area_img" id="92">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201910007_092.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEVzU2dLNE9nS3dzSERwV3ppOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 列、行向量相乘在处理单元上的映射过程" src="Detail/GetImg?filename=images/JSJC201910007_092.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEVzU2dLNE9nS3dzSERwV3ppOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图6 列、行向量相乘在处理单元上的映射过程</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201910007_092.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEVzU2dLNE9nS3dzSERwV3ppOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="93" name="93">4.3 不同规模的矩阵乘</h4>
                <div class="p1">
                    <p id="94">深度学习中的矩阵乘计算既包括规模较大的矩阵乘,也存在某一维度值较小的矩阵乘。若将<i>S</i><sub><i>i</i></sub>和<i>S</i><sub><i>j</i></sub>设置为较小的值,可高效执行不同规模的矩阵乘计算。相应地,片上数据重用次数少,带宽要求高,也会引起功耗的增加。若将<i>S</i><sub><i>i</i></sub>和<i>S</i><sub><i>j</i></sub>设置为较大的值,则在执行某一维度值较小的矩阵乘时,效率会大幅下降。</p>
                </div>
                <div class="p1">
                    <p id="95">在初始设计中,编译时即确定分块大小。虽然在执行大矩阵相乘时,计算部件接近峰值性能,但在处理小规模矩阵时,性能出现了明显下降。为此,本文在矩阵乘运行时支持动态调整分块大小,具体为:处理器支持分块大小可变,矩阵分块由运行在CPU上的程序实现,程序将分块计算参数等信息发送至控制器,处理器只负责按照控制器分发的任务信息执行分块层次的矩阵相乘。该设计优先保证计算性能,其次尽可能降低带宽需求,从而降低功耗。</p>
                </div>
                <div class="p1">
                    <p id="96">在处理器内部设有分块计算参数的存储地址,控制器将分块任务信息(包括<i>S</i><sub><i>i</i></sub>和<i>S</i><sub><i>j</i></sub>)发送至处理器,处理器根据分块任务信息控制有限状态机的状态转换以完成计算,然后通知控制器发送下一个分块任务信息。</p>
                </div>
                <h3 id="97" name="97" class="anchor-tag">5 实验结果与分析</h3>
                <h4 class="anchor-tag" id="98" name="98">5.1 实验设置</h4>
                <div class="p1">
                    <p id="99">本文实验平台为ZedBoard开发板,开发环境为Xilinx Vivado 2017.3,开发板硬件参数设置如表1所示。</p>
                </div>
                <div class="area_img" id="100">
                    <p class="img_tit"><b>表1 开发板硬件参数设置</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="100" border="1"><tr><td><br />参数</td><td>设置</td></tr><tr><td><br />平台</td><td>Avnet Zedboard</td></tr><tr><td><br />芯片</td><td>Xilinx Zynq XC7Z020 SoC</td></tr><tr><td><br />处理器</td><td>2 ARM Cortex-A9@667 MHz</td></tr><tr><td><br />可编程逻辑</td><td>Artix-7</td></tr><tr><td><br />存储器</td><td>512 MB DDR3@533 MHz</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="101">本文在bare-metal和Linux 2种环境下测试加速器的性能。加速器包括1个～3个PU,PU配置如下:RAM<sub><i>A</i></sub>/RAM<sub><i>B</i></sub>使用1个36 KB BRAM,设计成双缓冲工作方式,每个缓冲区大小为256×64 bit。PU阵列规模为8×8,PE内部存储器RAM<sub><i>C</i></sub>使用1个18 KB BRAM,为512×32 bit。在该PU配置下,片上存储空间对分块大小的约束为:<i>S</i><sub><i>i</i></sub>≤2 048,<i>S</i><sub><i>j</i></sub>≤2 048,<i>S</i><sub><i>i</i></sub><i>S</i><sub><i>j</i></sub>≤32 768。</p>
                </div>
                <h4 class="anchor-tag" id="102" name="102">5.2 性能分析</h4>
                <div class="p1">
                    <p id="103">使用DeepBench中用来测试边缘端设备通用矩阵乘性能的数据集对本文加速器进行测试。该数据集由深度学习中应用广泛的通用矩阵乘计算组成,包括规模小且形状不规则的矩阵乘。DeepBench调用Google公司的gemmlowp低精度矩阵乘库进行整数矩阵乘测试,精度为8 bit相乘和32 bit累加。</p>
                </div>
                <h4 class="anchor-tag" id="104" name="104">5.2.1 bare-metal环境下的测试结果</h4>
                <div class="p1">
                    <p id="105">在bare-metal环境下,本文加速器配置1个PU,对其与gemmlowp库在双核ARM Cortex-A9 CPU上的性能进行对比,结果如表2所示,其中,双核Cortex-A9 CPU的工作频率为667 MHz。从表2可以看出,与2 ARM Cortex-A9 CPU相比,本文加速器平均达到了2.79倍的加速比,且其效率保持在85%以上。加速器共有64个PE,每个PE有1个乘加单元,工作频率为100 MHz,理论计算性能峰值为12.8 GOPS。序号2、3、5和7测试的实测性能与峰值性能比值达到97.4%以上,实测性能与峰值性能的差距主要来源于在分块计算开始前的数据传输时间和分块计算结束后矩阵<i><b>C</b></i>分块写到DRAM的时间,PU乘加单元停止工作。序号4和6测试的实测性能与峰值性能比值分别为90.6%和93.0%,效率下降的原因是<i>N</i>的值较小,使传输时间占总时间的比例增加。序号1测试的实测性能与峰值性能比值为85.6%,效率下降的主要原因是<i>M</i>值较小且不是8的倍数,导致传输无效数据的时间过长。</p>
                </div>
                <div class="area_img" id="106">
                                            <p class="img_tit">
                                                <b>表2 bare-metal环境下2种矩阵乘性能对比结果</b>
                                                
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201910007_10600.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEVzU2dLNE9nS3dzSERwV3ppOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/JSJC201910007_10600.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEVzU2dLNE9nS3dzSERwV3ppOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201910007_10600.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEVzU2dLNE9nS3dzSERwV3ppOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表2 bare-metal环境下2种矩阵乘性能对比结果" src="Detail/GetImg?filename=images/JSJC201910007_10600.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEVzU2dLNE9nS3dzSERwV3ppOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>
                <div class="p1">
                    <p id="107">1个PU配置的加速器资源占用情况如表3所示,LUT为查找表,FF为触发器。为便于分析和调试,实验中配置了3个AXI Master接口模块区分矩阵<i><b>A</b></i>、<i><b>B</b></i>和<i><b>C</b></i>数据,增加了资源消耗。在实际应用中,1个AXI Master接口模块即可满足通信需求。</p>
                </div>
                <div class="area_img" id="108">
                    <p class="img_tit"><b>表3 bare-metal环境下1个PU配置加速器的资源使用情况</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="108" border="1"><tr><td>资源</td><td>DSP</td><td>BRAM</td><td>LUT</td><td>FF</td></tr><tr><td>使用</td><td>64</td><td>37</td><td>17 809</td><td>12 114</td></tr><tr><td><br />可用</td><td>220</td><td>140</td><td>53 200</td><td>106 400</td></tr><tr><td><br />使用率/%</td><td>29.09</td><td>26.43</td><td>33.48</td><td>11.39</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="109">本文测试涉及的矩阵乘相应带宽需求如表4所示。由于输入、输出分开,而输入带宽需求小于输出带宽需求,因此最大带宽需求为输出带宽800 MB/s。输出数据量远小于输入数据量,因此,虽然输出带宽需求较高,但输出时间占总时间的比例很小,只要满足输入带宽需求就可取得较高的计算效率。</p>
                </div>
                <div class="area_img" id="110">
                    <p class="img_tit"><b>表4 bare-metal环境下1个PU配置加速器的带宽需求情况</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="110" border="1"><tr><td>序号</td><td><i>M</i></td><td><i>N</i></td><td><i>R</i></td><td>输入带宽/<br />(MB·s<sup>-1</sup>)</td><td>输出带宽/<br />(MB·s<sup>-1</sup>)</td></tr><tr><td>1</td><td>35</td><td>2 048</td><td>700</td><td>169</td><td>800</td></tr><tr><td><br />2</td><td>128</td><td>1 280</td><td>1 500</td><td>75</td><td>800</td></tr><tr><td><br />3</td><td>176</td><td>1 408</td><td>1 500</td><td>72</td><td>800</td></tr><tr><td><br />4</td><td>3 072</td><td>128</td><td>1 500</td><td>75</td><td>800</td></tr><tr><td><br />5</td><td>3 072</td><td>1 024</td><td>1 500</td><td>75</td><td>800</td></tr><tr><td><br />6</td><td>4 224</td><td>176</td><td>1 500</td><td>75</td><td>800</td></tr><tr><td><br />7</td><td>5 124</td><td>2 048</td><td>700</td><td>75</td><td>800</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="111" name="111">5.2.2 Linux环境下的测试结果</h4>
                <div class="p1">
                    <p id="112">在Linux环境下,本文选用Xillybus<citation id="134" type="reference"><link href="33" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>作为加速器和ARM处理器之间的数据传输方案。Xillybus由IP核和相应的Linux驱动组成,在ZedBoard开发板上可提供最大速率约为300 MB/s的数据传输能力。Xillybus的IP核与加速器之间通过FIFO连接。</p>
                </div>
                <div class="p1">
                    <p id="113">加速器配置1个～3个PU,其与2 ARM Cortex-A9 CPU的性能对比结果如表5所示。3个PU配置加速器的资源使用和带宽需求情况分别如表6、表7所示。从表5可以看出,针对不同规模的矩阵乘,与2 ARM Cortex-A9 CPU相比,加速器达到了2.92倍～7.23倍的加速比。在1个PU配置下,加速器性能相对于bare-metal环境下的相同配置出现了不同程度的下降,主要原因是Xillybus提供的带宽不能满足加速器的需求以及存在数据传输延迟。在序号4和6的测试中,<i>N</i>值较小,传输时间占总时间的比例较大,带宽受限和数据传输延迟引起了较大程度的性能下降。2个PU配置的加速器相对于1个PU配置,有1.18倍～1.98倍的性能提升,3个PU配置的加速器相对于1个PU配置,有1.19倍～2.79倍的性能提升。</p>
                </div>
                <div class="area_img" id="114">
                    <p class="img_tit"><b>表5 Linux环境下2种矩阵乘性能对比结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="114" border="1"><tr><td rowspan="2">序号</td><td rowspan="2"><i>M</i></td><td rowspan="2"><i>N</i></td><td rowspan="2"><i>R</i></td><td colspan="4"><br />计算速度/GOPS</td></tr><tr><td>2 ARM Cortex-<br />A9 CPU</td><td>加速器<br />(1PU)</td><td>加速器<br />(2PU)</td><td>加速器<br />(3PU)</td></tr><tr><td>1</td><td>35</td><td>2 048</td><td>700</td><td>3.816</td><td>9.775</td><td>11.490</td><td>11.634</td></tr><tr><td><br />2</td><td>128</td><td>1 280</td><td>1 500</td><td>4.668</td><td>11.173</td><td>21.272</td><td>29.397</td></tr><tr><td><br />3</td><td>176</td><td>1 408</td><td>1 500</td><td>5.009</td><td>11.183</td><td>21.234</td><td>28.819</td></tr><tr><td><br />4</td><td> 3 072</td><td>128</td><td>1 500</td><td>3.751</td><td>6.251</td><td>10.013</td><td>10.965</td></tr><tr><td><br />5</td><td>3 072</td><td>1 024</td><td>1 500</td><td>4.580</td><td>11.011</td><td>21.483</td><td>30.765</td></tr><tr><td><br />6</td><td>4 224</td><td>176</td><td>1 500</td><td>4.224</td><td>7.296</td><td>12.997</td><td>13.169</td></tr><tr><td><br />7</td><td>5 124</td><td>2 048</td><td>700</td><td>4.314</td><td>11.538</td><td>22.855</td><td>31.179</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="115">
                    <p class="img_tit"><b>表6 Linux环境下3个PU配置加速器的资源使用情况</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="115" border="1"><tr><td>资源</td><td>DSP</td><td>BRAM</td><td>LUT</td><td>FF</td></tr><tr><td>使用</td><td>192</td><td>119.50</td><td>21 464</td><td>21 396</td></tr><tr><td><br />可用</td><td>220</td><td>140</td><td>53 200</td><td>106 400</td></tr><tr><td><br />使用率/%</td><td>87.27</td><td>85.36</td><td>40.35</td><td>20.11</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="116">
                    <p class="img_tit"><b>表7 Linux环境下3个PU配置加速器的带宽需求情况</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="116" border="1"><tr><td>序号</td><td><i>M</i></td><td><i>N</i></td><td><i>R</i></td><td>输入带宽/<br />(MB·s<sup>-1</sup>)</td><td>输出带宽/<br />(MB·s<sup>-1</sup>)</td></tr><tr><td>1</td><td>35</td><td>2 048</td><td>700</td><td>507</td><td>2 400</td></tr><tr><td><br />2</td><td>128</td><td>1 280</td><td>1 500</td><td>225</td><td>2 400</td></tr><tr><td><br />3</td><td>176</td><td>1 408</td><td>1 500</td><td>216</td><td>2 400</td></tr><tr><td><br />4</td><td>3 072</td><td>128</td><td>1 500</td><td>225</td><td>2 400</td></tr><tr><td><br />5</td><td>3 072</td><td>1 024</td><td>1 500</td><td>225</td><td>2 400</td></tr><tr><td><br />6</td><td>4 224</td><td>176</td><td>1 500</td><td>225</td><td>2 400</td></tr><tr><td><br />7</td><td>5 124</td><td>2 048</td><td>700</td><td>225</td><td>2 400</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="117">由于本文的定制AXI Master接口模块暂不支持多PU同时工作,因此bare-metal环境下只进行了1个PU配置的加速器测试。对比bare-metal和Linux 2种环境下的测试结果,bare-metal环境下数据传输延迟较小,如果定制AXI Master接口模块能够支持多PU同时工作,Zynq 7020上可部署3个PU,使加速比达到8.4。在Linux环境下,可通过替换更高性能的数据传输模块进一步提升加速器的性能。</p>
                </div>
                <h3 id="118" name="118" class="anchor-tag">6 结束语</h3>
                <div class="p1">
                    <p id="119">针对深度学习推理中对不同规模整数矩阵乘法的计算需求,本文设计一种基于Zynq SoC平台的整数矩阵乘法加速器。量化深度神经网络时以整数而非浮点运算进行推理,能够大幅降低功耗并缩短网络延迟。本文的研究成果对在低成本的片上可编程系统中实现实时神经网络推理具有参考意义。下一步将优化该加速器的时序设计以提高其工作频率。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Quantization and training of neural networks for efficient integer-arithmetic-only inference">

                                <b>[1]</b> JACOB B,KLIGYS S,CHEN Bo,et al.Quantization and training of neural networks for efficient integer-arithmetic-only inference[EB/OL].[2018-07-20].https://arxiv.org/pdf/1712.05877.pdf.
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=8-bit approximations for parallelism in deep learning">

                                <b>[2]</b> DETTMERS T.8-bit approximations for parallelism in deep learning[EB/OL].[2018-07-20].https://arxiv.org/pdf/1511.04561.pdf.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Hardware-oriented approximation of convolutional neural networks">

                                <b>[3]</b> GYSEL P,MOTAMEDI M,GHIASI S.Hardware-oriented approximation of convolutional neural networks[EB/OL].[2018-07-20].https://arxiv.org/pdf/1604.03168.pdf.
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep compression:compressing deep neural networks with pruning trained quantization and huffman coding">

                                <b>[4]</b> HAN Song,MAO Huizi,DALLY W J.Deep compression:compressing deep neural networks with pruning,trained quantization and huffman coding[EB/OL].[2018-07-20].https://arxiv.org/pdf/1510.00149.pdf.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=An update to DeepBench with a focus on deep learning inference">

                                <b>[5]</b> NARANG S,DIAMOS G.An update to DeepBench with a focus on deep learning inference[EB/OL].[2018-07-20].https://svail.github.io/DeepBench-update.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Area and time efficient implementation of matrix multiplication on FPGAs">

                                <b>[6]</b> JANG J,CHOI S,PRASANNA V K K.Area and time efficient implementations of matrix multiplication on FPGAs[C]//Proceedings of IEEE International Conference on Field-programmable Technology.Washington D.C.,USA:IEEE Press,2002:93-100.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Resource and delay effi-cient matrix multiplication using newer FPGA devices">

                                <b>[7]</b> CAMPBELL S J,KHATRI S P.Resource and delay efficient matrix multiplication using newer FPGA devices[C]//Proceedings of the 16th ACM Great Lakes Symposium on VLSI.New York,USA:ACM Press,2006:308-311.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Accelerating matrix multiplication on FPGAs">

                                <b>[8]</b> EL-ATFY R,DESSOUKY M A,EL-GHITANI H.Accelerating matrix multiplication on FPGAs[C]//Proceedings of the 2nd International Design and Test Workshop.Washington D.C.,USA:IEEE Press,2007:203-204.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Hardware accele-ration of matrix multiplication on a Xilinx FPGA">

                                <b>[9]</b> DAVE N,FLEMING K,KING M,et al.Hardware accele-ration of matrix multiplication on a Xilinx FPGA[C]//Proceedings of IEEE/ACM International Conference on Formal Methods and Models for Codesign.Washington D.C.,USA:IEEE Press,2007:97-100.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_10" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZDZC200809028&amp;v=MDA2MjVSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUnRGeXJtV3IvTVB5blJiYkc0SHRuTXBvOUhiSVFLREg4NHY=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEVzU2dLNE9nS3dzSERwV3ppOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[10]</b> 田翔,周凡,陈耀武,等.基于FPGA的实时双精度浮点矩阵乘法器设计[J].浙江大学学报(工学版),2008,42(9):1611-1615.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_11" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=1014168361.nh&amp;v=MDQ4MzE0TzN6cXFCdEdGckNVUkxPZVplUnRGeXJtV3IvTVZGMjZHcksrRnRMS3JwRWJQSVFLREg4NHZSNFQ2ajU=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEVzU2dLNE9nS3dzSERwV3ppOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[11]</b> 张婷.嵌入式环境下浮点矩阵乘法的FPGA加速关键技术研究[D].长沙:湖南大学,2013.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_12" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201408050&amp;v=MzE3ODJDVVJMT2VaZVJ0RnlybVdyL01MejdCYmJHNEg5WE1wNDlBWklRS0RIODR2UjRUNmo1NE8zenFxQnRHRnI=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEVzU2dLNE9nS3dzSERwV3ppOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[12]</b> 马邺晨,李醒飞.用于导航解算的矩阵运算硬件加速器设计[J].计算机工程,2014,40(8):259-263.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Intel Xeon Phi delivers competitive performance for deep learning and getting better fast">

                                <b>[13]</b> Intel Corporation.Intel Xeon Phi delivers competitive performance for deep learning and getting better fast[EB/OL].[2018-07-20].https://software.intel.com/en-us/articles/intel-xeon-phi-delivers-competitive-performance-for-deep-learningand-getting-better-fast.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A customizable matrix multiplication framework for the Intel HARPv2 Xeon+ FPGA platform:a deep learning case study">

                                <b>[14]</b> MOSS D J M,KRISHNAN S,NURVITADHI E,et al.A customizable matrix multiplication framework for the Intel HARPv2 Xeon+ FPGA platform:a deep learning case study[C]//Proceedings of 2018 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays.New York,USA:ACM Press,2018:107-116.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image Net classification with deep convolutional neural networks">

                                <b>[15]</b> KRIZHEVSKY A,SUTSKEVER I,HINTON G E.ImageNet classification with deep convolutional neural networks[C]//Proceedings of the 25th International Conference on Neural Information Processing Systems.New York,USA:ACM Press,2012:1097-1105.
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Xillybus host application programming guide for Linux">

                                <b>[16]</b> Xillybus.Xillybus host application programming guide for Linux[EB/OL].[2018-07-20].http://xillybus.com/downloads/doc/xillybus_host_programming_guide_linux.pdf.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJC201910007" />
        <input id="dpi" type="hidden" value="600" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEVzU2dLNE9nS3dzSERwV3ppOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201910007&amp;v=MTkwNDlqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSdEZ5cm1Xci9OTHo3QmJiRzRIOWpOcjQ5Rlk0UUtESDg0dlI0VDY=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEVzU2dLNE9nS3dzSERwV3ppOD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
