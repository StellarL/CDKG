<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637129944606993750%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJC201907043%26RESULT%3d1%26SIGN%3datvl4hwS%252ffAKTJcV66QXYqRzLwg%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201907043&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201907043&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201907043&amp;v=MDc2NDFIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJwRkN2a1U3N0lMejdCYmJHNEg5ak1xSTlCWjRRS0Q=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9wS0dTT3lHTXAzN1YxWUJuaTVBbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#37" data-title="0 概述 ">0 概述</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#41" data-title="1 基于超像素的快速代价聚集 ">1 基于超像素的快速代价聚集</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#46" data-title="1.1 AD-Census代价匹配">1.1 AD-Census代价匹配</a></li>
                                                <li><a href="#60" data-title="1.2 超像素最小生成树">1.2 超像素最小生成树</a></li>
                                                <li><a href="#73" data-title="1.3 基于超像素最小生成树的代价聚集">1.3 基于超像素最小生成树的代价聚集</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#103" data-title="2 实验结果与分析 ">2 实验结果与分析</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#116" data-title="3 结束语 ">3 结束语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#43" data-title="&lt;b&gt;图1 基于超像素的快速代价聚集流程&lt;/b&gt;"><b>图1 基于超像素的快速代价聚集流程</b></a></li>
                                                <li><a href="#66" data-title="&lt;b&gt;图2 SLIC超像素分割效果&lt;/b&gt;"><b>图2 SLIC超像素分割效果</b></a></li>
                                                <li><a href="#72" data-title="&lt;b&gt;图3 超像素最小生成树示意图&lt;/b&gt;"><b>图3 超像素最小生成树示意图</b></a></li>
                                                <li><a href="#77" data-title="&lt;b&gt;图4 超像素代价聚集过程&lt;/b&gt;"><b>图4 超像素代价聚集过程</b></a></li>
                                                <li><a href="#106" data-title="&lt;b&gt;表1 4种算法误匹配率对比&lt;/b&gt;"><b>表1 4种算法误匹配率对比</b></a></li>
                                                <li><a href="#108" data-title="&lt;b&gt;图5 Middlebury测试集上的立体匹配结果&lt;/b&gt;"><b>图5 Middlebury测试集上的立体匹配结果</b></a></li>
                                                <li><a href="#110" data-title="&lt;b&gt;图6 自然场景下的立体匹配结果&lt;/b&gt;"><b>图6 自然场景下的立体匹配结果</b></a></li>
                                                <li><a href="#112" data-title="&lt;b&gt;图7 双目测距示例&lt;/b&gt;"><b>图7 双目测距示例</b></a></li>
                                                <li><a href="#114" data-title="&lt;b&gt;图8 5种算法耗时情况对比&lt;/b&gt;"><b>图8 5种算法耗时情况对比</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="142">


                                    <a id="bibliography_1" title=" SUN Jian, SHUM Heung Yeung, ZHENG Nanning.Stereo matching using belief propagation[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2003, 25 (7) :787-800." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Stereo matching using belief propagation">
                                        <b>[1]</b>
                                         SUN Jian, SHUM Heung Yeung, ZHENG Nanning.Stereo matching using belief propagation[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2003, 25 (7) :787-800.
                                    </a>
                                </li>
                                <li id="144">


                                    <a id="bibliography_2" title=" HIRSCHMULLER H.Accurate and efficient stereo processing by semi-global matching and mutual information[C]//Proceedings of 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Computer Society, 2005:807-814." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Accurate and efficient stereo processing by semi-global matching and mutual information">
                                        <b>[2]</b>
                                         HIRSCHMULLER H.Accurate and efficient stereo processing by semi-global matching and mutual information[C]//Proceedings of 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Computer Society, 2005:807-814.
                                    </a>
                                </li>
                                <li id="146">


                                    <a id="bibliography_3" title=" 王新艳, 潘巍, 王月莲, 等.基于全局差错能量函数的立体匹配算法[J].计算机工程, 2017, 43 (7) :244-249." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201707042&amp;v=MzEzMTR2a1U3N0lMejdCYmJHNEg5Yk1xSTlCWm9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJwRkM=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9wS0dTT3lHTXAzN1YxWUJuaTVBbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         王新艳, 潘巍, 王月莲, 等.基于全局差错能量函数的立体匹配算法[J].计算机工程, 2017, 43 (7) :244-249.
                                    </a>
                                </li>
                                <li id="148">


                                    <a id="bibliography_4" title=" 王年, 范益政, 鲍文霞, 等.基于图割的图像匹配算法[J].电子学报, 2006, 34 (2) :232-236." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZXU200602008&amp;v=MDE4OTh0R0ZyQ1VSTE9lWmVScEZDdmtVNzdJSVRmVGU3RzRIdGZNclk5RmJJUUtESDg0dlI0VDZqNTRPM3pxcUI=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9wS0dTT3lHTXAzN1YxWUJuaTVBbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                         王年, 范益政, 鲍文霞, 等.基于图割的图像匹配算法[J].电子学报, 2006, 34 (2) :232-236.
                                    </a>
                                </li>
                                <li id="150">


                                    <a id="bibliography_5" title=" 王召月, 陈丽芳.基于mean-shift全局立体匹配方法[J].计算机工程与科学, 2017, 39 (7) :1333-1337." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJK201707020&amp;v=MjIyNzdJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVScEZDdmtVNzdJTHo3QlpiRzRIOWJNcUk5SFo=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9wS0dTT3lHTXAzN1YxWUJuaTVBbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                         王召月, 陈丽芳.基于mean-shift全局立体匹配方法[J].计算机工程与科学, 2017, 39 (7) :1333-1337.
                                    </a>
                                </li>
                                <li id="152">


                                    <a id="bibliography_6" title=" HONG Li, CHEN George.Segment-based stereo matching using graph cuts[C]//Proceedings of 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Computer Society, 2004:74-81." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Segment-based Stereo Matching using Graph Cuts”">
                                        <b>[6]</b>
                                         HONG Li, CHEN George.Segment-based stereo matching using graph cuts[C]//Proceedings of 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Computer Society, 2004:74-81.
                                    </a>
                                </li>
                                <li id="154">


                                    <a id="bibliography_7" title=" SCHARSTEIN D, SZELISKI R.A taxonomy and evaluation of dense two-frame stereo correspondence algorithms[J].International Journal of Computer Vision, 2002, 47 (1/2/3) :7-42." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002830724&amp;v=MTAzMDROajdCYXJPNEh0SE9wNHhGWStrTFkzazV6QmRoNGo5OVNYcVJyeG94Y01IN1I3cWVidWR0RkMvbVVMN0lJVjQ9&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9wS0dTT3lHTXAzN1YxWUJuaTVBbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                         SCHARSTEIN D, SZELISKI R.A taxonomy and evaluation of dense two-frame stereo correspondence algorithms[J].International Journal of Computer Vision, 2002, 47 (1/2/3) :7-42.
                                    </a>
                                </li>
                                <li id="156">


                                    <a id="bibliography_8" title=" PORIKLI F.Constant time O (1) bilateral filtering[C]//Proceedings of International Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2008:1-8." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Constant time O (1)bilateral filtering">
                                        <b>[8]</b>
                                         PORIKLI F.Constant time O (1) bilateral filtering[C]//Proceedings of International Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2008:1-8.
                                    </a>
                                </li>
                                <li id="158">


                                    <a id="bibliography_9" title=" HE Kaiming, SUN Jian, TANG Xiao’ou.Guided image filtering[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2013, 35 (6) :1397-1409." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Guided image filtering">
                                        <b>[9]</b>
                                         HE Kaiming, SUN Jian, TANG Xiao’ou.Guided image filtering[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2013, 35 (6) :1397-1409.
                                    </a>
                                </li>
                                <li id="160">


                                    <a id="bibliography_10" title=" LU Jiangbo, SHI Keyang, MIN Dongbo, et al.Cross-based local multipoint filtering[C]//Proceedings of International Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2012:430-437." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Cross-based local multipoint filtering">
                                        <b>[10]</b>
                                         LU Jiangbo, SHI Keyang, MIN Dongbo, et al.Cross-based local multipoint filtering[C]//Proceedings of International Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2012:430-437.
                                    </a>
                                </li>
                                <li id="162">


                                    <a id="bibliography_11" title=" YANG Qingxiong.A non-local cost aggregation method for stereo matching[C]//Proceedings of International Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2012:1402-1409." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A non-local cost aggregation method for stereo matching">
                                        <b>[11]</b>
                                         YANG Qingxiong.A non-local cost aggregation method for stereo matching[C]//Proceedings of International Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2012:1402-1409.
                                    </a>
                                </li>
                                <li id="164">


                                    <a id="bibliography_12" title=" MEI Xing, SUN Xun, DONG Weiming, et al.Segment-tree based cost aggregation for stereo matching[C]//Proceedings of International Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2013:313-320." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Segment-tree based cost aggregation for stereo matching">
                                        <b>[12]</b>
                                         MEI Xing, SUN Xun, DONG Weiming, et al.Segment-tree based cost aggregation for stereo matching[C]//Proceedings of International Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2013:313-320.
                                    </a>
                                </li>
                                <li id="166">


                                    <a id="bibliography_13" title=" ZBONTAR J, LECUN Y.Stereo matching by training a convolutional neural network to compare image patches[J].Journal of Machine Learning Research, 2016, 17 (1) :2287-2318." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Stereo matching by training a convolutional neural network to compare image patches">
                                        <b>[13]</b>
                                         ZBONTAR J, LECUN Y.Stereo matching by training a convolutional neural network to compare image patches[J].Journal of Machine Learning Research, 2016, 17 (1) :2287-2318.
                                    </a>
                                </li>
                                <li id="168">


                                    <a id="bibliography_14" title=" LI Lincheng, YU Xin, ZHANG Shunli, et al.3D cost aggregation with multiple minimum spanning trees for stereo matching[J].Applied Optics, 2017, 56 (12) :3411-3420." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=3Dcost aggregation with multiple minimum spanning trees for stereo matching">
                                        <b>[14]</b>
                                         LI Lincheng, YU Xin, ZHANG Shunli, et al.3D cost aggregation with multiple minimum spanning trees for stereo matching[J].Applied Optics, 2017, 56 (12) :3411-3420.
                                    </a>
                                </li>
                                <li id="170">


                                    <a id="bibliography_15" title=" LEE S, LEE J H, LIM J, et al.Robust stereo matching using adaptive random walk with restart algorithm[J].Image and Vision Computing, 2015, 37:1-11." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES15122800102749&amp;v=MTgwMTBqTEkxOFVheEk9TmlmT2ZiSzlIOVBPcDQ5Rlplc05DM2d3b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VTA==&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9wS0dTT3lHTXAzN1YxWUJuaTVBbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[15]</b>
                                         LEE S, LEE J H, LIM J, et al.Robust stereo matching using adaptive random walk with restart algorithm[J].Image and Vision Computing, 2015, 37:1-11.
                                    </a>
                                </li>
                                <li id="172">


                                    <a id="bibliography_16" title=" ACHANTA R, SHAJI A, SMITH K, Et al.SLIC superpixels compared to state-of-the-art superpixel methods[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2012, 34 (11) :2274-2282." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=SLIC Superpixels Compared to State-of-the-Art Superpixel Methods">
                                        <b>[16]</b>
                                         ACHANTA R, SHAJI A, SMITH K, Et al.SLIC superpixels compared to state-of-the-art superpixel methods[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2012, 34 (11) :2274-2282.
                                    </a>
                                </li>
                                <li id="174">


                                    <a id="bibliography_17" title=" MEI Xing, SUN Xun, ZHOU Mingcai, et al.On building an accurate stereo matching system on graphics hardware[C]//Proceedings of International Conference on Computer Vision Workshops.Washington D.C., USA:IEEE Press, 2011:467-474." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=On building an accurate stereo matching system on graphics hardware">
                                        <b>[17]</b>
                                         MEI Xing, SUN Xun, ZHOU Mingcai, et al.On building an accurate stereo matching system on graphics hardware[C]//Proceedings of International Conference on Computer Vision Workshops.Washington D.C., USA:IEEE Press, 2011:467-474.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJC" target="_blank">计算机工程</a>
                2019,45(07),268-272+281 DOI:10.19678/j.issn.1000-3428.0050790            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>改进的代价聚集快速立体匹配算法</b></span>
 <span class="shoufa"></span>                                     </h1>

            </div>
                        <h2>
                                <a href="javascript:;">杨罡</a>
                                <a href="javascript:;">晋涛</a>
                                <a href="javascript:;">王大伟</a>
                                <a href="javascript:;">曹京津</a>
                                <a href="javascript:;">张娜</a>
                                <a href="javascript:;">严碧武</a>
                                <a href="javascript:;">李涛</a>
                                <a href="javascript:;">程远</a>
                </h2>
                    <h2>

                    <span>国网山西省电力科学研究院</span>
                    <span>国网电力科学研究院武汉南瑞有限责任公司</span>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>针对立体匹配中的代价聚集问题, 提出一种改进的代价聚集算法。对图像进行超像素分割并建立最小生成树, 采用树形滤波器进行代价聚集, 生成超像素代价。利用权重融合初始像素代价和超像素代价, 得到最终的像素代价。实验结果表明, 与最小生成树、线段树等算法相比, 该算法的时间复杂度较低, 且生成的视差图具有良好的边缘保持特性。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%BB%A3%E4%BB%B7%E8%81%9A%E9%9B%86&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9wS0dTT3lHTXAzN1YxWUJuaTVBbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">代价聚集;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%B6%85%E5%83%8F%E7%B4%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9wS0dTT3lHTXAzN1YxWUJuaTVBbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">超像素;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%9C%80%E5%B0%8F%E7%94%9F%E6%88%90%E6%A0%91&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9wS0dTT3lHTXAzN1YxWUJuaTVBbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">最小生成树;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%A0%91%E5%BD%A2%E6%BB%A4%E6%B3%A2%E5%99%A8&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9wS0dTT3lHTXAzN1YxWUJuaTVBbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">树形滤波器;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%AB%8B%E4%BD%93%E5%8C%B9%E9%85%8D&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9wS0dTT3lHTXAzN1YxWUJuaTVBbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">立体匹配;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    杨罡 (1983—) , 男, 高级工程师, 主研方向为图像处理、高压线路智能监测研究, E-mail: yanggang9800@ 163. com;;
                                </span>
                                <span>
                                    晋涛, 高级工程师;;
                                </span>
                                <span>
                                    王大伟, 工程师;;
                                </span>
                                <span>
                                    曹京津, 工程师;;
                                </span>
                                <span>
                                    张娜, 工程师;;
                                </span>
                                <span>
                                    严碧武, 高级工程师;;
                                </span>
                                <span>
                                    李涛, 工程师;;
                                </span>
                                <span>
                                    程远, 助理工程师。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-03-15</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国网山西电力2017年重点研究项目 (52053016000v);</span>
                    </p>
            </div>
                    <h1><b>Improved Cost Aggregation Algorithm for Fast Stereo Matching</b></h1>
                    <h2>
                    <span>YANG Gang</span>
                    <span>JIN Tao</span>
                    <span>WANG Dawei</span>
                    <span>CAO Jingjin</span>
                    <span>ZHANG Na</span>
                    <span>YAN Biwu</span>
                    <span>LI Tao</span>
                    <span>CHENG Yuan</span>
            </h2>
                    <h2>
                    <span>State Grid Shanxi Electric Power Research Institute</span>
                    <span>Wuhan Nanrui Limited Company of State Grid Electric Power Research Institute</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>For the cost aggregation problem in stereo matching, an improved cost aggregation algorithm is proposed.The image is segmented by superpixel and the Minimum Spanning Tree (MST) is established.Then, use tree filter for cost aggregation to generate superpixel cost.The final pixel cost is obtained by weighting the initial pixel cost and superpixel cost.Experimental results show that compared with MST, Segmcat-Tree (ST) and other methods, the proposed algorithm has lower time complexity, and can get disparity map with good edge preserve features.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=cost%20aggregation&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9wS0dTT3lHTXAzN1YxWUJuaTVBbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">cost aggregation;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=superpixel&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9wS0dTT3lHTXAzN1YxWUJuaTVBbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">superpixel;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Minimum%20Spanning%20Tree%20(MST)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9wS0dTT3lHTXAzN1YxWUJuaTVBbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Minimum Spanning Tree (MST) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=tree%20filter&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9wS0dTT3lHTXAzN1YxWUJuaTVBbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">tree filter;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=stereo%20matching&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9wS0dTT3lHTXAzN1YxWUJuaTVBbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">stereo matching;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-03-15</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="37" name="37" class="anchor-tag">0 概述</h3>
                <div class="p1">
                    <p id="38">在计算机视觉领域, 立体匹配技术一直是研究的热点和难点。现有的立体匹配方法主要分为局部匹配和全局匹配2类。全局立体匹配方法通过建立能量方程并对其进行最小化来获取视差, 采用的技术主要有置信度传播<citation id="176" type="reference"><link href="142" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>、动态规划<citation id="178" type="reference"><link href="144" rel="bibliography" /><link href="146" rel="bibliography" /><sup>[<a class="sup">2</a>,<a class="sup">3</a>]</sup></citation>、图割<citation id="179" type="reference"><link href="148" rel="bibliography" /><link href="150" rel="bibliography" /><link href="152" rel="bibliography" /><sup>[<a class="sup">4</a>,<a class="sup">5</a>,<a class="sup">6</a>]</sup></citation>等。虽然全局匹配方法能获得很好的视差, 但是其复杂度较高, 应用性不强。局部立体匹配方法只考虑像素邻域范围内的像素点, 与全局立体匹配方法相比复杂度较低。文献<citation id="177" type="reference">[<a class="sup">7</a>]</citation>把局部匹配方法分为代价计算、代价聚集、视差求取、视差求精4个步骤, 其中, 代价聚集被认为是局部立体匹配方法的核心。</p>
                </div>
                <div class="p1">
                    <p id="39">文献<citation id="180" type="reference">[<a class="sup">8</a>]</citation>利用<i>O</i> (1) 复杂度的双边滤波器进行代价聚合, 因其受制于快速双边滤波时的量化手段, 算法的运行结果并不好。文献<citation id="181" type="reference">[<a class="sup">9</a>]</citation>提出将原始的彩色图作为导向图来进行导向滤波代价聚集, 由于其假设视差数据在结构上和导向图相似, 因此该方法的效果比双边滤波更好。文献<citation id="182" type="reference">[<a class="sup">10</a>]</citation>依据像素相似度, 对每个像素建立支撑区域, 其代价聚集取得了良好的效果。基于窗口或支撑区域的方法, 其待聚集区域在本质上是局部的, 文献<citation id="185" type="reference">[<a class="sup">11</a>,<a class="sup">12</a>]</citation>提出非局部代价聚集的思想, 将图像像素点看作图的节点, 像素点的颜色差异作为边的权重, 来建立最小生成树。该方法考虑了图像中全局像素对于待聚集像素的影响, 因此取得了较好的效果。文献<citation id="183" type="reference">[<a class="sup">13</a>]</citation>将卷积神经网络应用于代价聚集, 而文献<citation id="184" type="reference">[<a class="sup">14</a>]</citation>采用多棵最小分离树来进行3D代价聚集, 虽然两者都取得了良好的效果, 但其依赖于数据驱动或强大的硬件设备, 时间成本较高。</p>
                </div>
                <div class="p1">
                    <p id="40">文献<citation id="186" type="reference">[<a class="sup">15</a>]</citation>以超像素作为节点进行随机游走, 本文将其与文献<citation id="187" type="reference">[<a class="sup">11</a>,<a class="sup">12</a>]</citation>采用的非局部代价聚集方法相结合, 提出一种改进的代价聚集算法。通过简单线性迭代聚集 (Simple Linear Iterative Cluster, SLIC) 算法进行超像素分割, 构建超像素最小生成树, 计算超像素代价。以一定的权重融合初始像素代价和超像素代价, 采用winner take all进行视差后处理, 得到立体匹配结果。</p>
                </div>
                <h3 id="41" name="41" class="anchor-tag">1 基于超像素的快速代价聚集</h3>
                <div class="p1">
                    <p id="42">本文提出的代价聚集方法流程如图1所示。</p>
                </div>
                <div class="area_img" id="43">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201907043_043.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9wS0dTT3lHTXAzN1YxWUJuaTVBbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 基于超像素的快速代价聚集流程" src="Detail/GetImg?filename=images/JSJC201907043_043.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9wS0dTT3lHTXAzN1YxWUJuaTVBbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图1 基于超像素的快速代价聚集流程</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201907043_043.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9wS0dTT3lHTXAzN1YxWUJuaTVBbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="44">应用SLIC算法生成图像的超像素。利用克鲁斯卡尔算法生成超像素最小生成树, 通过树形滤波器生成超像素代价。基于权重融合AD-Census代价匹配得到的像素代价<i>C</i><sup>0</sup><sub>PIXEL</sub> (<i>u</i>, <i>v</i>, <i>d</i>) 和超像素代价<i>C</i><sup>1</sup><sub>SUPERPIXEL</sub> (sup <i>u</i>, sup <i>v</i>, <i>d</i>) , 生成像素代价<i>C</i><mathml id="45"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mtext>Ρ</mtext><mtext>Ι</mtext><mtext>X</mtext><mtext>E</mtext><mtext>L</mtext></mrow><mn>1</mn></msubsup></mrow></math></mathml> (<i>u</i>, <i>v</i>, <i>d</i>) , 应用winner take all策略得到视差数据。</p>
                </div>
                <h4 class="anchor-tag" id="46" name="46">1.1 AD-Census代价匹配</h4>
                <div class="p1">
                    <p id="47">采用AD-Census代价计算的方法生成初始像素代价<i>C</i><mathml id="48"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mtext>Ρ</mtext><mtext>Ι</mtext><mtext>X</mtext><mtext>E</mtext><mtext>L</mtext></mrow><mn>0</mn></msubsup></mrow></math></mathml> (<i>u</i>, <i>v</i>, <i>d</i>) 。AD-Census代价由2个部分构成:绝对误差 (Absolute Difference, AD) 变换和Census变换。</p>
                </div>
                <div class="p1">
                    <p id="49">AD变换通过计算双目图像中2个像素的颜色差异得到匹配代价, 计算过程如下:</p>
                </div>
                <div class="p1">
                    <p id="50" class="code-formula">
                        <mathml id="50"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>C</mi><msub><mrow></mrow><mrow><mtext>A</mtext><mtext>D</mtext></mrow></msub><mo stretchy="false"> (</mo><mi>u</mi><mo>, </mo><mi>v</mi><mo>, </mo><mi>d</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mn>1</mn><mn>3</mn></mfrac><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mi>R</mi><mo>, </mo><mi>G</mi><mo>, </mo><mi>B</mi></mrow></munder><mo stretchy="false">|</mo></mstyle><mi>Ι</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mtext>L</mtext><mtext>e</mtext><mtext>f</mtext><mtext>t</mtext></mrow></msubsup><mo stretchy="false"> (</mo><mi>u</mi><mo>, </mo><mi>v</mi><mo stretchy="false">) </mo><mo>-</mo><mi>Ι</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mtext>R</mtext><mtext>i</mtext><mtext>g</mtext><mtext>h</mtext><mtext>t</mtext></mrow></msubsup><mo stretchy="false"> (</mo><mi>i</mi><mo>-</mo><mi>d</mi><mo>, </mo><mi>v</mi><mo stretchy="false">) </mo><mo stretchy="false">|</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="51">Census变换是一种非参数变换。首先对待匹配双目图像进行Census变换, 然后比较双目图像中2个像素的Census编码, 得到汉明距离并将其作为匹配代价。本文采用5×5的窗口对图像进行Census变换。任一像素点 (<i>u</i>, <i>v</i>) 的Census变换通过衡量它与周边像素的亮度差异得到, 过程如下:</p>
                </div>
                <div class="p1">
                    <p id="52" class="code-formula">
                        <mathml id="52"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>C</mi><mi>e</mi><mi>n</mi><mi>s</mi><mi>u</mi><mi>s</mi><mo stretchy="false"> (</mo><mi>u</mi><mo>, </mo><mi>v</mi><mo stretchy="false">) </mo><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mo>⊗</mo></mstyle><mrow><mo stretchy="false"> (</mo><mi>m</mi><mo>, </mo><mi>n</mi><mo stretchy="false">) </mo><mo>∈</mo><mi>Ν</mi><mo stretchy="false"> (</mo><mi>u</mi><mo>, </mo><mi>v</mi><mo stretchy="false">) </mo></mrow></munder><mi>ε</mi><mo stretchy="false"> (</mo><mi>Ι</mi><mo stretchy="false"> (</mo><mi>m</mi><mo>, </mo><mi>n</mi><mo stretchy="false">) </mo><mo>, </mo><mi>Ι</mi><mo stretchy="false"> (</mo><mi>u</mi><mo>, </mo><mi>v</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi>ε</mi><mo stretchy="false"> (</mo><mi>a</mi><mo>, </mo><mi>b</mi><mo stretchy="false">) </mo><mo>=</mo><mrow><mo>{</mo><mrow><mtable><mtr><mtd><mn>1</mn><mo>, </mo><mspace width="0.25em" /><mi>b</mi><mo>&gt;</mo><mi>a</mi></mtd></mtr><mtr><mtd><mn>0</mn><mo>, </mo><mspace width="0.25em" /><mi>b</mi><mo>≤</mo><mi>a</mi></mtd></mtr></mtable></mrow></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="53">通过异或操作比较双目图像中2个像素的二进制串, 从而生成汉明距离。对于5×5的窗口, 汉明距离为0代表2个像素的相似度最高, 汉明距离为25代表其相似度最低。由此可以得到Census变换的代价计算公式:</p>
                </div>
                <div class="p1">
                    <p id="54"><i>C</i><sub>CENSUS</sub> (<i>u</i>, <i>v</i>, <i>d</i>) =<i>Ham</i> (<i>Census</i><sup>Left</sup> (<i>u</i>, <i>v</i>) , <i>Census</i><sup>Right</sup> (<i>u</i>-<i>d</i>, <i>v</i>) )      (4) </p>
                </div>
                <div class="p1">
                    <p id="56">文献<citation id="188" type="reference">[<a class="sup">13</a>]</citation>提到Census变换对于噪声和光照变换的鲁棒性高, 而对于重复纹理区域以及弱纹理区域的匹配效果较差。而AD变换的特性恰好能够和Census变换进行互补, 因此可以通过级联AD与Census组成AD-Census算法, 从而获得鲁棒性更高的初始像素代价, 具体如下:</p>
                </div>
                <div class="p1">
                    <p id="57"><i>C</i><sup>0</sup><sub>PIXEL</sub> (<i>u</i>, <i>v</i>, <i>d</i>) =<i>p</i> (<i>C</i><sub>CENSUS</sub> (<i>u</i>, <i>v</i>, <i>d</i>) , <i>λ</i><sub>CENSUS</sub>) +<i>p</i> (<i>C</i><sub>AD</sub> (<i>u</i>, <i>v</i>, <i>d</i>) , <i>λ</i><sub>AD</sub>)      (5) </p>
                </div>
                <div class="p1">
                    <p id="59">其中, <i>λ</i><sub>CENSUS</sub>和<i>λ</i><sub>AD</sub>是融合系数, <i>p</i>代表代价和融合系数相乘。</p>
                </div>
                <h4 class="anchor-tag" id="60" name="60">1.2 超像素最小生成树</h4>
                <div class="p1">
                    <p id="61">通过AD-Census方法计算得到的像素代价中包含噪声, 因此需要通过代价聚集步骤来降低匹配的模糊性和代价中的噪声。代价聚集方法基于相似的区域具有相似的视差这一特征, 传统的代价聚集方法都是针对像素代价计算进行的, 其复杂度依赖于像素数目。文献<citation id="189" type="reference">[<a class="sup">15</a>]</citation>引入超像素分割的思想, 将相似的像素组成超像素, 由于超像素数目远小于像素数目, 因此超像素代价聚集计算比像素代价聚集计算的速度更快。文献<citation id="190" type="reference">[<a class="sup">15</a>]</citation>通过像素代价构造超像素代价, 并重启随机游走算法对其进行迭代优化。随机游走过程需要进行多次迭代, 并且超像素构成的大型邻接稀疏矩阵每次都要参与运算, 因此, 算法的时间性能和空间复杂度都较差。本文算法基于超像素分割生成超像素最小生成树, 并通过树形滤波器进行代价聚集。在代价聚集的过程中, 只需要进行少量计算就可获得代价聚集结果。</p>
                </div>
                <div class="p1">
                    <p id="62">采用超像素进行代价聚集是基于2个方面的考虑。首先, 超像素按照相似度对图像进行分割, 超像素代价<i>C</i><mathml id="63"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mtext>S</mtext><mtext>U</mtext><mtext>Ρ</mtext><mtext>E</mtext><mtext>R</mtext><mtext>Ρ</mtext><mtext>Ι</mtext><mtext>X</mtext><mtext>E</mtext><mtext>L</mtext></mrow><mn>0</mn></msubsup></mrow></math></mathml> (sup <i>u</i>, sup <i>v</i>, <i>d</i>) 通过属于同一区域的像素代价<i>C</i><mathml id="64"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mtext>Ρ</mtext><mtext>Ι</mtext><mtext>X</mtext><mtext>E</mtext><mtext>L</mtext></mrow><mn>0</mn></msubsup></mrow></math></mathml> (<i>u</i>, <i>v</i>, <i>d</i>) 生成, 因此, 它的抗噪鲁棒性比像素代价<i>C</i><sup>0</sup><sub>PIXEL</sub> (<i>u</i>, <i>v</i>, <i>d</i>) 高。其次, 超像素的数量远小于像素的数量, 其代价聚集的时间大幅缩短, 同时也节约了内存。</p>
                </div>
                <div class="p1">
                    <p id="65">本文采用SLIC算法<citation id="191" type="reference"><link href="172" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>进行超像素分割, 将一幅图像分割为若干小块, 且小块中像素的相似度很高, 超像素实质上代表一块图像区域。图2所示为对一幅450像素×375像素的图像进行超像素分割后的效果, 该图被分割为200个超像素。</p>
                </div>
                <div class="area_img" id="66">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201907043_066.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9wS0dTT3lHTXAzN1YxWUJuaTVBbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 SLIC超像素分割效果" src="Detail/GetImg?filename=images/JSJC201907043_066.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9wS0dTT3lHTXAzN1YxWUJuaTVBbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图2 SLIC超像素分割效果</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201907043_066.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9wS0dTT3lHTXAzN1YxWUJuaTVBbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="67">由图2可以看出, 图像中450×375=168 750个像素被划分到200个超像素中, 传统的代价聚集方法需要在168 750个像素上进行<i>D</i>次, 现在只需在200个超像素上进行<i>D</i>次。所生成的超像素的颜色、亮度以及超像素代价如下:</p>
                </div>
                <div class="p1">
                    <p id="68" class="code-formula">
                        <mathml id="68"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mi>C</mi><mi>o</mi><mi>l</mi><mi>o</mi><mi>r</mi><mo stretchy="false"> (</mo><mi>sup</mi><mspace width="0.25em" /><mi>u</mi><mo>, </mo><mi>sup</mi><mspace width="0.25em" /><mi>v</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mo stretchy="false"> (</mo><mi>u</mi><mo>, </mo><mi>v</mi><mo stretchy="false">) </mo><mo>∈</mo><mo stretchy="false"> (</mo><mi>sup</mi><mspace width="0.25em" /><mi>u</mi><mo>, </mo><mi>sup</mi><mspace width="0.25em" /><mi>v</mi><mo stretchy="false">) </mo></mrow></munder><mi>C</mi></mstyle><mi>o</mi><mi>l</mi><mi>o</mi><mi>r</mi><mo stretchy="false"> (</mo><mi>u</mi><mo>, </mo><mi>v</mi><mo stretchy="false">) </mo></mrow><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mo stretchy="false"> (</mo><mi>u</mi><mo>, </mo><mi>v</mi><mo stretchy="false">) </mo><mo>∈</mo><mo stretchy="false"> (</mo><mi>sup</mi><mspace width="0.25em" /><mi>u</mi><mo>, </mo><mi>sup</mi><mspace width="0.25em" /><mi>v</mi><mo stretchy="false">) </mo></mrow></munder><mn>1</mn></mstyle></mrow></mfrac></mtd></mtr><mtr><mtd><mi>Ι</mi><mi>n</mi><mi>t</mi><mi>e</mi><mi>n</mi><mi>s</mi><mi>i</mi><mi>t</mi><mi>y</mi><mo stretchy="false"> (</mo><mi>sup</mi><mspace width="0.25em" /><mi>u</mi><mo>, </mo><mi>sup</mi><mspace width="0.25em" /><mi>v</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mo stretchy="false"> (</mo><mi>u</mi><mo>, </mo><mi>v</mi><mo stretchy="false">) </mo><mo>∈</mo><mo stretchy="false"> (</mo><mi>sup</mi><mspace width="0.25em" /><mi>u</mi><mo>, </mo><mi>sup</mi><mspace width="0.25em" /><mi>v</mi><mo stretchy="false">) </mo></mrow></munder><mi>Ι</mi></mstyle><mi>n</mi><mi>t</mi><mi>e</mi><mi>n</mi><mi>s</mi><mi>i</mi><mi>t</mi><mi>y</mi><mo stretchy="false"> (</mo><mi>u</mi><mo>, </mo><mi>v</mi><mo stretchy="false">) </mo></mrow><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mo stretchy="false"> (</mo><mi>u</mi><mo>, </mo><mi>v</mi><mo stretchy="false">) </mo><mo>∈</mo><mo stretchy="false"> (</mo><mi>sup</mi><mspace width="0.25em" /><mi>u</mi><mo>, </mo><mi>sup</mi><mspace width="0.25em" /><mi>v</mi><mo stretchy="false">) </mo></mrow></munder><mn>1</mn></mstyle></mrow></mfrac></mtd></mtr><mtr><mtd><mi>C</mi><msubsup><mrow></mrow><mrow><mtext>S</mtext><mtext>U</mtext><mtext>Ρ</mtext><mtext>E</mtext><mtext>R</mtext><mtext>Ρ</mtext><mtext>Ι</mtext><mtext>X</mtext><mtext>E</mtext><mtext>L</mtext></mrow><mn>0</mn></msubsup><mo stretchy="false"> (</mo><mi>sup</mi><mspace width="0.25em" /><mi>u</mi><mo>, </mo><mi>sup</mi><mspace width="0.25em" /><mi>v</mi><mo>, </mo><mi>d</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mo stretchy="false"> (</mo><mi>u</mi><mo>, </mo><mi>v</mi><mo stretchy="false">) </mo><mo>∈</mo><mo stretchy="false"> (</mo><mi>sup</mi><mspace width="0.25em" /><mi>u</mi><mo>, </mo><mi>sup</mi><mspace width="0.25em" /><mi>v</mi><mo stretchy="false">) </mo></mrow></munder><mi>C</mi></mstyle><msubsup><mrow></mrow><mrow><mtext>Ρ</mtext><mtext>Ι</mtext><mtext>X</mtext><mtext>E</mtext><mtext>L</mtext></mrow><mn>0</mn></msubsup><mo stretchy="false"> (</mo><mi>u</mi><mo>, </mo><mi>v</mi><mo>, </mo><mi>d</mi><mo stretchy="false">) </mo></mrow><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mo stretchy="false"> (</mo><mi>u</mi><mo>, </mo><mi>v</mi><mo stretchy="false">) </mo><mo>∈</mo><mo stretchy="false"> (</mo><mi>sup</mi><mspace width="0.25em" /><mi>u</mi><mo>, </mo><mi>sup</mi><mspace width="0.25em" /><mi>v</mi><mo stretchy="false">) </mo></mrow></munder><mn>1</mn></mstyle></mrow></mfrac></mtd></mtr></mtable></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>6</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="69">为了生成超像素最小生成树, 可以把超像素看作是图, 将超像素点看作图的一个节点, 相邻2个超像素构成边, 代表2个超像素的相邻关系。边上的值为超像素之间的亮度差值, 差值越小, 超像素的相似度越高。其计算过程如下:</p>
                </div>
                <div class="p1">
                    <p id="70">Δ<i>I</i><sub><i>AB</i></sub>=<i>Intensity</i> (<i>A</i>) -<i>Intensity</i> (<i>B</i>)      (7) </p>
                </div>
                <div class="p1">
                    <p id="71">将亮度差值作为边的权重, 构造最小生成树, 如图3所示。2个超像素在图中只有一条连通的路径, 该路径是连接这2个超像素的所有路径中边权重最小的, 代表这条路径中的超像素具有最好的相似度并且超像素间不易跨越边缘。利用该路径组成的超像素进行代价聚集, 聚集后的视差空间能够较好地保持边缘特性。</p>
                </div>
                <div class="area_img" id="72">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201907043_072.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9wS0dTT3lHTXAzN1YxWUJuaTVBbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 超像素最小生成树示意图" src="Detail/GetImg?filename=images/JSJC201907043_072.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9wS0dTT3lHTXAzN1YxWUJuaTVBbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图3 超像素最小生成树示意图</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201907043_072.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9wS0dTT3lHTXAzN1YxWUJuaTVBbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="73" name="73">1.3 基于超像素最小生成树的代价聚集</h4>
                <div class="p1">
                    <p id="74">文献<citation id="192" type="reference">[<a class="sup">11</a>]</citation>将整幅图像中的像素作为图的节点, 像素间为4邻域或8邻域, 构建最小生成树。文献<citation id="193" type="reference">[<a class="sup">12</a>]</citation>在局部区域建立多棵最小生成树。与文献<citation id="194" type="reference">[<a class="sup">11</a>,<a class="sup">12</a>]</citation>相比, 本文利用超像素最小生成树建立代价聚集时, 由于超像素数目远小于像素数目, 因此其速度较快。同时, 本文加入超像素作为匹配基础, 所以最终生成的像素代价<i>C</i><sup>1</sup><sub>PIXEL</sub> (<i>u</i>, <i>v</i>, <i>d</i>) 由超像素代价<i>C</i><mathml id="75"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mtext>S</mtext><mtext>U</mtext><mtext>Ρ</mtext><mtext>E</mtext><mtext>R</mtext><mtext>Ρ</mtext><mtext>Ι</mtext><mtext>X</mtext><mtext>E</mtext><mtext>L</mtext></mrow><mn>1</mn></msubsup></mrow></math></mathml> (sup <i>u</i>, sup <i>v</i>, <i>d</i>) 和像素代价<i>C</i><mathml id="76"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mtext>Ρ</mtext><mtext>Ι</mtext><mtext>X</mtext><mtext>E</mtext><mtext>L</mtext></mrow><mn>0</mn></msubsup></mrow></math></mathml> (<i>u</i>, <i>v</i>, <i>d</i>) 组成。对生成的超像素最小生成树, 通过2次遍历最小生成树进行代价聚集。初始时需要从最小生成树的叶子节点, 依据超像素的孩子节点自底向上更新其代价值, 使超像素在代价聚集时受到其底端节点的影响。当第1次遍历到达最小生成树的根节点后, 进行自顶向下的代价更新, 从而增加父节点对子节点代价聚集的影响。图4展示了基于超像素最小生成树的代价聚集过程。</p>
                </div>
                <div class="area_img" id="77">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201907043_077.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9wS0dTT3lHTXAzN1YxWUJuaTVBbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 超像素代价聚集过程" src="Detail/GetImg?filename=images/JSJC201907043_077.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9wS0dTT3lHTXAzN1YxWUJuaTVBbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图4 超像素代价聚集过程</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201907043_077.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9wS0dTT3lHTXAzN1YxWUJuaTVBbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="78">在代价聚集时, 将超像素之间的高斯权重作为相邻超像素对当前超像素的贡献权重, 计算过程如下:</p>
                </div>
                <div class="p1">
                    <p id="79" class="code-formula">
                        <mathml id="79"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>G</mi><mi>a</mi><mi>u</mi><mi>s</mi><mi>s</mi><mo stretchy="false"> (</mo><mi>A</mi><mo>, </mo><mi>B</mi><mo stretchy="false">) </mo><mo>=</mo><mrow><mi>exp</mi></mrow><mrow><mo> (</mo><mrow><mo>-</mo><mfrac><mrow><mi>Ι</mi><mi>n</mi><mi>t</mi><mi>e</mi><mi>n</mi><mi>s</mi><mi>i</mi><mi>t</mi><mi>y</mi><mo stretchy="false"> (</mo><mi>A</mi><mo stretchy="false">) </mo><mo>-</mo><mi>Ι</mi><mi>n</mi><mi>t</mi><mi>e</mi><mi>n</mi><mi>s</mi><mi>i</mi><mi>t</mi><mi>y</mi><mo stretchy="false"> (</mo><mi>B</mi><mo stretchy="false">) </mo></mrow><mrow><mn>2</mn><mi>s</mi><mi>i</mi><mi>g</mi><mi>m</mi><mi>a</mi><msup><mrow></mrow><mn>2</mn></msup></mrow></mfrac></mrow><mo>) </mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>8</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="80">其中, <i>A</i>、<i>B</i>代表2个超像素, <i>sigma</i>代表方差, exp代表指数, <i>Gauss</i> (<i>A</i>, <i>B</i>) 为超像素之间的高斯权重。</p>
                </div>
                <div class="p1">
                    <p id="81">对于超像素<i>A</i>, 在其自底向上的代价聚集过程中, 超像素的代价更新公式如下:</p>
                </div>
                <div class="p1">
                    <p id="82" class="code-formula">
                        <mathml id="82"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>C</mi><msubsup><mrow></mrow><mrow><mtext>S</mtext><mtext>U</mtext><mtext>Ρ</mtext><mtext>E</mtext><mtext>R</mtext><mtext>Ρ</mtext><mtext>Ι</mtext><mtext>X</mtext><mtext>E</mtext><mtext>L</mtext></mrow><mo>↑</mo></msubsup><mo stretchy="false"> (</mo><mi>A</mi><mo>, </mo><mi>d</mi><mo stretchy="false">) </mo><mo>=</mo><mi>C</mi><msubsup><mrow></mrow><mrow><mtext>S</mtext><mtext>U</mtext><mtext>Ρ</mtext><mtext>E</mtext><mtext>R</mtext><mtext>Ρ</mtext><mtext>Ι</mtext><mtext>X</mtext><mtext>E</mtext><mtext>L</mtext></mrow><mn>0</mn></msubsup><mo stretchy="false"> (</mo><mi>A</mi><mo>, </mo><mi>d</mi><mo stretchy="false">) </mo><mo>+</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>Q</mi><mo>∈</mo><mi>Ν</mi><mo stretchy="false"> (</mo><mi>A</mi><mo stretchy="false">) </mo></mrow></munder><mi>G</mi></mstyle><mi>a</mi><mi>u</mi><mi>s</mi><mi>s</mi><mo stretchy="false"> (</mo><mi>A</mi><mo>, </mo><mi>Q</mi><mo stretchy="false">) </mo><mo>×</mo><mi>C</mi><msubsup><mrow></mrow><mrow><mtext>S</mtext><mtext>U</mtext><mtext>Ρ</mtext><mtext>E</mtext><mtext>R</mtext><mtext>Ρ</mtext><mtext>Ι</mtext><mtext>X</mtext><mtext>E</mtext><mtext>L</mtext></mrow><mn>0</mn></msubsup><mo stretchy="false"> (</mo><mi>Q</mi><mo>, </mo><mi>d</mi><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>9</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="83">其中, <i>N</i> (<i>A</i>) 表示超像素<i>A</i>的邻域。</p>
                </div>
                <div class="p1">
                    <p id="84">在进行自顶向下的代价聚集之后, 超像素代价如下:</p>
                </div>
                <div class="p1">
                    <p id="85"><i>C</i><mathml id="86"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mtext>S</mtext><mtext>U</mtext><mtext>Ρ</mtext><mtext>E</mtext><mtext>R</mtext><mtext>Ρ</mtext><mtext>Ι</mtext><mtext>X</mtext><mtext>E</mtext><mtext>L</mtext></mrow><mn>1</mn></msubsup></mrow></math></mathml> (<i>A</i>, <i>d</i>) =<i>Gauss</i> (<i>Pr</i> (<i>A</i>) , <i>A</i>) ×<i>C</i><mathml id="88"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mtext>S</mtext><mtext>U</mtext><mtext>Ρ</mtext><mtext>E</mtext><mtext>R</mtext><mtext>Ρ</mtext><mtext>Ι</mtext><mtext>X</mtext><mtext>E</mtext><mtext>L</mtext></mrow><mn>1</mn></msubsup></mrow></math></mathml> (<i>Pr</i> (<i>A</i>) , <i>d</i>) +<i>C</i><sup>↑</sup><sub>SUPERPIXEL</sub> (<i>A</i>, <i>d</i>) × (1-<i>Gauss</i><sup>2</sup> (<i>Pr</i> (<i>A</i>) , <i>A</i>) )      (10) </p>
                </div>
                <div class="p1">
                    <p id="90">其中, <i>Pr</i> (<i>A</i>) 代表超像素<i>A</i>的父节点。</p>
                </div>
                <div class="p1">
                    <p id="91">在代价更新的过程中, 超像素节点都直接或间接对其他超像素节点的代价更新有影响, 由于其计算过程只需要进行少量的加、减、乘运算, 因此能高效地进行代价聚集。得到超像素代价<i>C</i><mathml id="92"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mtext>S</mtext><mtext>U</mtext><mtext>Ρ</mtext><mtext>E</mtext><mtext>R</mtext><mtext>Ρ</mtext><mtext>Ι</mtext><mtext>X</mtext><mtext>E</mtext><mtext>L</mtext></mrow><mn>1</mn></msubsup></mrow></math></mathml> (sup <i>u</i>, sup <i>v</i>, <i>d</i>) 之后, 利用式 (11) 将其与初始代价<i>C</i><mathml id="93"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mtext>Ρ</mtext><mtext>Ι</mtext><mtext>X</mtext><mtext>E</mtext><mtext>L</mtext></mrow><mn>0</mn></msubsup></mrow></math></mathml> (<i>u</i>, <i>v</i>, <i>d</i>) 进行融合, 最终得到像素代价<i>C</i><mathml id="94"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mtext>Ρ</mtext><mtext>Ι</mtext><mtext>X</mtext><mtext>E</mtext><mtext>L</mtext></mrow><mn>1</mn></msubsup></mrow></math></mathml> (<i>u</i>, <i>v</i>, <i>d</i>) :</p>
                </div>
                <div class="p1">
                    <p id="95"><i>C</i><mathml id="96"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mtext>Ρ</mtext><mtext>Ι</mtext><mtext>X</mtext><mtext>E</mtext><mtext>L</mtext></mrow><mn>1</mn></msubsup></mrow></math></mathml> (<i>u</i>, <i>v</i>, <i>d</i>) =<i>λ</i><sub>SUPERPIXEL</sub><i>C</i><mathml id="97"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mtext>S</mtext><mtext>U</mtext><mtext>Ρ</mtext><mtext>E</mtext><mtext>R</mtext><mtext>Ρ</mtext><mtext>Ι</mtext><mtext>X</mtext><mtext>E</mtext><mtext>L</mtext></mrow><mn>1</mn></msubsup></mrow></math></mathml> (sup <i>u</i>, sup <i>v</i>, <i>d</i>) +<i>λ</i><sub>PIXEL</sub><i>C</i><mathml id="99"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mtext>Ρ</mtext><mtext>Ι</mtext><mtext>X</mtext><mtext>E</mtext><mtext>L</mtext></mrow><mn>0</mn></msubsup></mrow></math></mathml> (<i>u</i>, <i>v</i>, <i>d</i>)      (11) </p>
                </div>
                <div class="p1">
                    <p id="100">其中, (<i>u</i>, <i>v</i>) ∈ (sup <i>u</i>, sup <i>v</i>) 。</p>
                </div>
                <div class="p1">
                    <p id="101">利用winner take all策略<citation id="195" type="reference"><link href="154" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>对像素代价<i>C</i><mathml id="102"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mtext>Ρ</mtext><mtext>Ι</mtext><mtext>X</mtext><mtext>E</mtext><mtext>L</mtext></mrow><mn>1</mn></msubsup></mrow></math></mathml> (<i>u</i>, <i>v</i>, <i>d</i>) 进行计算, 得到视差, 并采用文献<citation id="196" type="reference">[<a class="sup">17</a>]</citation>的视差后处理方法对视差求精。</p>
                </div>
                <h3 id="103" name="103" class="anchor-tag">2 实验结果与分析</h3>
                <div class="p1">
                    <p id="104">为验证本文算法的优越性, 在Intel corei5-6400 2.7 GHz的CPU、内存8 GB的Windows平台下使用C++实现算法。由于本文算法在CPU端具有很好的并行性, 在实际编码过程中利用Intel TBB技术对算法进行加速, 其在求取分辨率为640像素×480像素, 视差等级为30的立体视觉对的视差时, 速度可达30 frame/s。实验选取Middlebury测试集中的Tsukuba、Venus、Teddy、Cones作为算法测试集, 将误匹配率作为测试指标。同时, 将本文算法的实验结果与ST算法<citation id="197" type="reference"><link href="164" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>、MST算法<citation id="198" type="reference"><link href="162" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>、ARW算法<citation id="199" type="reference"><link href="170" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>进行比较。为验证本文算法在实际场景中的处理效果及其时间优越性, 还对自然场景下拍摄的分辨率为640像素×480像素、最大视差为30的立体视觉对进行测试。</p>
                </div>
                <div class="p1">
                    <p id="105">在利用Middlebury测试集进行评测时, 选取误差阈值为1个像素的非遮挡区域nonocc进行比较, 结果如表1所示。由表1可以看出, 在立体匹配时, 本文算法的误匹配率稍差于MST算法和ST算法。这是由于本文算法的代价聚集基于超像素进行, 而MST算法和ST算法则以像素为基础进行代价聚集。与ARW算法相比, 本文算法的误匹配率较优。这是因为本文算法利用树形滤波器对超像素进行代价聚集, 而ARW算法以随机游走方式进行代价聚集。同时, 本文算法包含良好的分割信息, 相较于ARW算法, 其保边性能更好, 因此评价系数的提升较为显著。</p>
                </div>
                <div class="area_img" id="106">
                    <p class="img_tit"><b>表1 4种算法误匹配率对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"> %</p>
                    <table id="106" border="1"><tr><td>测试集</td><td>ST算法</td><td>MST算法</td><td>ARW算法</td><td>本文算法</td></tr><tr><td>Tsukuba</td><td>1.25</td><td>1.47</td><td>4.10</td><td>1.52</td></tr><tr><td><br />Vensus</td><td>0.20</td><td>0.25</td><td>2.28</td><td>0.36</td></tr><tr><td><br />Teddy</td><td>6.00</td><td>6.01</td><td>7.97</td><td>6.22</td></tr><tr><td><br />Cones</td><td>2.77</td><td>2.87</td><td>6.79</td><td>2.94</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="107">本文算法对于Tsukuba、Venus、Teddy、Cones测试集生成的视差伪彩图如图5所示。</p>
                </div>
                <div class="area_img" id="108">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201907043_108.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9wS0dTT3lHTXAzN1YxWUJuaTVBbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 Middlebury测试集上的立体匹配结果" src="Detail/GetImg?filename=images/JSJC201907043_108.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9wS0dTT3lHTXAzN1YxWUJuaTVBbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图5 Middlebury测试集上的立体匹配结果</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201907043_108.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9wS0dTT3lHTXAzN1YxWUJuaTVBbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="109">与Middlebury测试集的立体视觉对相比, 自然场景下拍摄的立体视觉对的处理难度更大。本文算法对于自然场景下拍摄的图片也表现出较好的立体匹配效果, 如图6所示。</p>
                </div>
                <div class="area_img" id="110">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201907043_110.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9wS0dTT3lHTXAzN1YxWUJuaTVBbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 自然场景下的立体匹配结果" src="Detail/GetImg?filename=images/JSJC201907043_110.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9wS0dTT3lHTXAzN1YxWUJuaTVBbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图6 自然场景下的立体匹配结果</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201907043_110.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9wS0dTT3lHTXAzN1YxWUJuaTVBbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="111">基于本文算法在自然场景下的优越性能, 结合合理的相机选型, 可实现精确物体测距、背景虚化等应用。图7展示了本文算法用于物体测距和测尺寸的效果。得到的测量结果为:高<i>H</i>=1 747.66 mm, 与相机的距离<i>D</i>=1 503.77 mm, 与实际值 (<i>H</i>=1 745.00 mm, <i>D</i>=1 548.00 mm) 较为接近。</p>
                </div>
                <div class="area_img" id="112">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201907043_112.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9wS0dTT3lHTXAzN1YxWUJuaTVBbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 双目测距示例" src="Detail/GetImg?filename=images/JSJC201907043_112.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9wS0dTT3lHTXAzN1YxWUJuaTVBbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图7 双目测距示例</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201907043_112.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9wS0dTT3lHTXAzN1YxWUJuaTVBbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="113">本文算法在保证处理效果的前提下, 使用TBB技术对其进行加速。图8给出5种算法处理立体视觉对的耗时情况, 其中, FA是未经过加速的本文算法, FA-TBB是采用TBB技术加速后的本文算法。</p>
                </div>
                <div class="area_img" id="114">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201907043_114.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9wS0dTT3lHTXAzN1YxWUJuaTVBbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 5种算法耗时情况对比" src="Detail/GetImg?filename=images/JSJC201907043_114.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9wS0dTT3lHTXAzN1YxWUJuaTVBbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图8 5种算法耗时情况对比</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201907043_114.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9wS0dTT3lHTXAzN1YxWUJuaTVBbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="115">由图8可以看出, ARW算法由于使用随机游走进行迭代求解, 其速度最慢。ST算法、MST算法的速度可达2 frame/s。而本文算法在未经优化的情况下处理速度为13 frame/s, 经过TBB加速后可达30 frame/s, 其能在保证处理效果的前提下, 提高速度。</p>
                </div>
                <h3 id="116" name="116" class="anchor-tag">3 结束语</h3>
                <div class="p1">
                    <p id="117">本文提出一种改进的代价聚集快速立体匹配算法。通过AD-Census进行初始像素代价计算, 并利用SLIC进行超像素分割, 构建超像素初始代价。以超像素之间的亮度差异为权重生成超像素最小生成树, 通过树形滤波器进行超像素代价聚集, 再以一定的权重将其与初始像素代价融合。采用winner take all进行视差后处理, 生成视差图。实验结果表明, 该算法能够取得较好的立体匹配效果, 大幅减小时间代价。下一步将考虑用递归非全局保持滤波器改进超像素代价聚集过程, 加快算法速度。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="142">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Stereo matching using belief propagation">

                                <b>[1]</b> SUN Jian, SHUM Heung Yeung, ZHENG Nanning.Stereo matching using belief propagation[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2003, 25 (7) :787-800.
                            </a>
                        </p>
                        <p id="144">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Accurate and efficient stereo processing by semi-global matching and mutual information">

                                <b>[2]</b> HIRSCHMULLER H.Accurate and efficient stereo processing by semi-global matching and mutual information[C]//Proceedings of 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Computer Society, 2005:807-814.
                            </a>
                        </p>
                        <p id="146">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201707042&amp;v=MDAyMTR6cXFCdEdGckNVUkxPZVplUnBGQ3ZrVTc3SUx6N0JiYkc0SDliTXFJOUJab1FLREg4NHZSNFQ2ajU0TzM=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9wS0dTT3lHTXAzN1YxWUJuaTVBbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> 王新艳, 潘巍, 王月莲, 等.基于全局差错能量函数的立体匹配算法[J].计算机工程, 2017, 43 (7) :244-249.
                            </a>
                        </p>
                        <p id="148">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZXU200602008&amp;v=Mjk4MTVrVTc3SUlUZlRlN0c0SHRmTXJZOUZiSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUnBGQ3Y=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9wS0dTT3lHTXAzN1YxWUJuaTVBbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b> 王年, 范益政, 鲍文霞, 等.基于图割的图像匹配算法[J].电子学报, 2006, 34 (2) :232-236.
                            </a>
                        </p>
                        <p id="150">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJK201707020&amp;v=MTk0NjdUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJwRkN2a1U3N0lMejdCWmJHNEg5Yk1xSTlIWklRS0RIODR2UjQ=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9wS0dTT3lHTXAzN1YxWUJuaTVBbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b> 王召月, 陈丽芳.基于mean-shift全局立体匹配方法[J].计算机工程与科学, 2017, 39 (7) :1333-1337.
                            </a>
                        </p>
                        <p id="152">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Segment-based Stereo Matching using Graph Cuts”">

                                <b>[6]</b> HONG Li, CHEN George.Segment-based stereo matching using graph cuts[C]//Proceedings of 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Computer Society, 2004:74-81.
                            </a>
                        </p>
                        <p id="154">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002830724&amp;v=MDY4NzVoNGo5OVNYcVJyeG94Y01IN1I3cWVidWR0RkMvbVVMN0lJVjQ9Tmo3QmFyTzRIdEhPcDR4Rlkra0xZM2s1ekJk&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9wS0dTT3lHTXAzN1YxWUJuaTVBbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b> SCHARSTEIN D, SZELISKI R.A taxonomy and evaluation of dense two-frame stereo correspondence algorithms[J].International Journal of Computer Vision, 2002, 47 (1/2/3) :7-42.
                            </a>
                        </p>
                        <p id="156">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Constant time O (1)bilateral filtering">

                                <b>[8]</b> PORIKLI F.Constant time O (1) bilateral filtering[C]//Proceedings of International Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2008:1-8.
                            </a>
                        </p>
                        <p id="158">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Guided image filtering">

                                <b>[9]</b> HE Kaiming, SUN Jian, TANG Xiao’ou.Guided image filtering[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2013, 35 (6) :1397-1409.
                            </a>
                        </p>
                        <p id="160">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Cross-based local multipoint filtering">

                                <b>[10]</b> LU Jiangbo, SHI Keyang, MIN Dongbo, et al.Cross-based local multipoint filtering[C]//Proceedings of International Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2012:430-437.
                            </a>
                        </p>
                        <p id="162">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A non-local cost aggregation method for stereo matching">

                                <b>[11]</b> YANG Qingxiong.A non-local cost aggregation method for stereo matching[C]//Proceedings of International Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2012:1402-1409.
                            </a>
                        </p>
                        <p id="164">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Segment-tree based cost aggregation for stereo matching">

                                <b>[12]</b> MEI Xing, SUN Xun, DONG Weiming, et al.Segment-tree based cost aggregation for stereo matching[C]//Proceedings of International Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2013:313-320.
                            </a>
                        </p>
                        <p id="166">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Stereo matching by training a convolutional neural network to compare image patches">

                                <b>[13]</b> ZBONTAR J, LECUN Y.Stereo matching by training a convolutional neural network to compare image patches[J].Journal of Machine Learning Research, 2016, 17 (1) :2287-2318.
                            </a>
                        </p>
                        <p id="168">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=3Dcost aggregation with multiple minimum spanning trees for stereo matching">

                                <b>[14]</b> LI Lincheng, YU Xin, ZHANG Shunli, et al.3D cost aggregation with multiple minimum spanning trees for stereo matching[J].Applied Optics, 2017, 56 (12) :3411-3420.
                            </a>
                        </p>
                        <p id="170">
                            <a id="bibliography_15" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES15122800102749&amp;v=MDk5NzJUTW53WmVadUh5am1VTGpMSTE4VWF4ST1OaWZPZmJLOUg5UE9wNDlGWmVzTkMzZ3dvQk1UNlQ0UFFIL2lyUmRHZXJxUQ==&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9wS0dTT3lHTXAzN1YxWUJuaTVBbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[15]</b> LEE S, LEE J H, LIM J, et al.Robust stereo matching using adaptive random walk with restart algorithm[J].Image and Vision Computing, 2015, 37:1-11.
                            </a>
                        </p>
                        <p id="172">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=SLIC Superpixels Compared to State-of-the-Art Superpixel Methods">

                                <b>[16]</b> ACHANTA R, SHAJI A, SMITH K, Et al.SLIC superpixels compared to state-of-the-art superpixel methods[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2012, 34 (11) :2274-2282.
                            </a>
                        </p>
                        <p id="174">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=On building an accurate stereo matching system on graphics hardware">

                                <b>[17]</b> MEI Xing, SUN Xun, ZHOU Mingcai, et al.On building an accurate stereo matching system on graphics hardware[C]//Proceedings of International Conference on Computer Vision Workshops.Washington D.C., USA:IEEE Press, 2011:467-474.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJC201907043" />
        <input id="dpi" type="hidden" value="600" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcTdWa2FjcW9wS0dTT3lHTXAzN1YxWUJuaTVBbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201907043&amp;v=MDc2NDFIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJwRkN2a1U3N0lMejdCYmJHNEg5ak1xSTlCWjRRS0Q=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9wS0dTT3lHTXAzN1YxWUJuaTVBbz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
