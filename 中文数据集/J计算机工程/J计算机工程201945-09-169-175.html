<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637128034580655000%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJC201909027%26RESULT%3d1%26SIGN%3dm5uXBKZB%252fBw01HvjDaGlZMcImgI%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201909027&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201909027&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201909027&amp;v=MDkyNThRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJyRnkvaFZiM0FMejdCYmJHNEg5ak1wbzlIWTQ=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTg2cEJVZU4zZUYvSE5NMVVZMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#49" data-title="0 概述 ">0 概述</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#51" data-title="1 相关工作 ">1 相关工作</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#55" data-title="2 SRM神经元梯度下降学习算法 ">2 SRM神经元梯度下降学习算法</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#56" data-title="2.1 SRM神经元模型">2.1 SRM神经元模型</a></li>
                                                <li><a href="#65" data-title="2.2 多脉冲神经元梯度下降学习算法">2.2 多脉冲神经元梯度下降学习算法</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#73" data-title="3 带延迟学习的梯度下降算法 ">3 带延迟学习的梯度下降算法</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#88" data-title="4 实验结果与分析 ">4 实验结果与分析</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#113" data-title="5 结束语 ">5 结束语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#92" data-title="&lt;b&gt;图1 离线方式下2种算法的学习精度对比结果&lt;/b&gt;"><b>图1 离线方式下2种算法的学习精度对比结果</b></a></li>
                                                <li><a href="#93" data-title="&lt;b&gt;图2 在线方式下2种算法的学习精度对比结果&lt;/b&gt;"><b>图2 在线方式下2种算法的学习精度对比结果</b></a></li>
                                                <li><a href="#96" data-title="&lt;b&gt;图3 3种算法精度对比结果1&lt;/b&gt;"><b>图3 3种算法精度对比结果1</b></a></li>
                                                <li><a href="#98" data-title="&lt;b&gt;图4 MSGDB-D1算法学习结束时的突触延迟时间1&lt;/b&gt;"><b>图4 MSGDB-D1算法学习结束时的突触延迟时间1</b></a></li>
                                                <li><a href="#99" data-title="&lt;b&gt;图5 MSGDB-D2算法学习结束时的突触延迟时间1&lt;/b&gt;"><b>图5 MSGDB-D2算法学习结束时的突触延迟时间1</b></a></li>
                                                <li><a href="#103" data-title="&lt;b&gt;图6 3种算法精度对比结果2&lt;/b&gt;"><b>图6 3种算法精度对比结果2</b></a></li>
                                                <li><a href="#104" data-title="&lt;b&gt;图7 MSGDB-D1算法学习结束时突触延迟时间2&lt;/b&gt;"><b>图7 MSGDB-D1算法学习结束时突触延迟时间2</b></a></li>
                                                <li><a href="#105" data-title="&lt;b&gt;图8 MSGDB-D2算法学习结束时突触延迟时间2&lt;/b&gt;"><b>图8 MSGDB-D2算法学习结束时突触延迟时间2</b></a></li>
                                                <li><a href="#106" data-title="&lt;b&gt;图9 3种算法精度对比结果3&lt;/b&gt;"><b>图9 3种算法精度对比结果3</b></a></li>
                                                <li><a href="#107" data-title="&lt;b&gt;图10 MSGDB-D1算法学习结束时突触延迟时间3&lt;/b&gt;"><b>图10 MSGDB-D1算法学习结束时突触延迟时间3</b></a></li>
                                                <li><a href="#108" data-title="&lt;b&gt;图11 MSGDB-D2算法学习结束时突触延迟时间3&lt;/b&gt;"><b>图11 MSGDB-D2算法学习结束时突触延迟时间3</b></a></li>
                                                <li><a href="#110" data-title="&lt;b&gt;图12 3种算法精度对比结果4&lt;/b&gt;"><b>图12 3种算法精度对比结果4</b></a></li>
                                                <li><a href="#111" data-title="&lt;b&gt;图13 MSGDB-D1算法学习结束时突触延迟时间4&lt;/b&gt;"><b>图13 MSGDB-D1算法学习结束时突触延迟时间4</b></a></li>
                                                <li><a href="#112" data-title="&lt;b&gt;图14 MSGDB-D2算法学习结束时突触延迟时间4&lt;/b&gt;"><b>图14 MSGDB-D2算法学习结束时突触延迟时间4</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="143">


                                    <a id="bibliography_1" title=" PONULAK F,KASINSKI A.Introduction to spiking neural networks:information processing,learning and applications [J].Acta Neurobiologiae Experimentalis,2011,71(4):409-433." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Introduction to spiking neural networks: Information processing, learning and applications">
                                        <b>[1]</b>
                                         PONULAK F,KASINSKI A.Introduction to spiking neural networks:information processing,learning and applications [J].Acta Neurobiologiae Experimentalis,2011,71(4):409-433.
                                    </a>
                                </li>
                                <li id="145">


                                    <a id="bibliography_2" title=" BOHTE S M,KOK J N,LA POUTR’E J A.Error-backpropagation in temporally encoded networks of spiking neurons[J].Neurocomputing,2002,48(1/4):17-37." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501914521&amp;v=MjA1NjBvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVVMcklKMW9TYUJvPU5pZk9mYks3SHRETnFvOUViZW9MQ1g0NA==&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTg2cEJVZU4zZUYvSE5NMVVZMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         BOHTE S M,KOK J N,LA POUTR’E J A.Error-backpropagation in temporally encoded networks of spiking neurons[J].Neurocomputing,2002,48(1/4):17-37.
                                    </a>
                                </li>
                                <li id="147">


                                    <a id="bibliography_3" title=" MCKENNOCH S,LIU D,BUSHNELL L G.Fast modifications of the spikeprop algorithm [C]//Proceedings of International Joint Conference on Neural Networks.Washington D.C.,USA:IEEE Press,2006:3970-3977." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fast modifications of the SpikeProp algorithm">
                                        <b>[3]</b>
                                         MCKENNOCH S,LIU D,BUSHNELL L G.Fast modifications of the spikeprop algorithm [C]//Proceedings of International Joint Conference on Neural Networks.Washington D.C.,USA:IEEE Press,2006:3970-3977.
                                    </a>
                                </li>
                                <li id="149">


                                    <a id="bibliography_4" title=" SILVA S M,RUANO A E.Application of levenberg-marquardt method to the training of spiking neural networks[C]//Proceedings of the International Conference on Neural Networks and Brain.Washington D.C.,USA:IEEE Press,2005:1354-1358." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Application of LevenbergMarquardt method to the training of spiking neural networks">
                                        <b>[4]</b>
                                         SILVA S M,RUANO A E.Application of levenberg-marquardt method to the training of spiking neural networks[C]//Proceedings of the International Conference on Neural Networks and Brain.Washington D.C.,USA:IEEE Press,2005:1354-1358.
                                    </a>
                                </li>
                                <li id="151">


                                    <a id="bibliography_5" title=" SCHRAUWEN B,CAMPENHOUT J V.Extending spike-prop[C]//Proceedings of International Joint Conference on Neural Networks.Washington D.C.,USA:IEEE Press,2004:471-476." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Extending spike-prop">
                                        <b>[5]</b>
                                         SCHRAUWEN B,CAMPENHOUT J V.Extending spike-prop[C]//Proceedings of International Joint Conference on Neural Networks.Washington D.C.,USA:IEEE Press,2004:471-476.
                                    </a>
                                </li>
                                <li id="153">


                                    <a id="bibliography_6" title=" BOOIJ O,NGUYEN H T.A gradient descent rule for multiple spiking neurons emitting multiple spikes[J].Information Processing Letters,2005,95(6):552-558." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011601422048&amp;v=MDQ3NjRmYks3SHRETnFZOUVZT2tOREhneG9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVUxySUoxb1NhQm89TmlmTw==&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTg2cEJVZU4zZUYvSE5NMVVZMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                         BOOIJ O,NGUYEN H T.A gradient descent rule for multiple spiking neurons emitting multiple spikes[J].Information Processing Letters,2005,95(6):552-558.
                                    </a>
                                </li>
                                <li id="155">


                                    <a id="bibliography_7" title=" XU Yan,ZENG Xiaoqin,HAN Lixin,et al.A supervised multi-spike learning algorithm based on gradient descent for spiking neural networks[J].Neural Networks,2013,43(4):99-113." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13050900043406&amp;v=MTM4MDhmYks3SHRUTXBvOUZaTzhNQ0h3L29CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVUxySUoxb1NhQm89TmlmTw==&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTg2cEJVZU4zZUYvSE5NMVVZMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                         XU Yan,ZENG Xiaoqin,HAN Lixin,et al.A supervised multi-spike learning algorithm based on gradient descent for spiking neural networks[J].Neural Networks,2013,43(4):99-113.
                                    </a>
                                </li>
                                <li id="157">


                                    <a id="bibliography_8" title=" 徐彦.基于梯度下降的脉冲神经元在线学习方法[J].计算机工程,2015,41(12):150-155,160." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201512029&amp;v=MDQ2MDBGckNVUkxPZVplUnJGeS9oVmIzQUx6N0JiYkc0SDlUTnJZOUhiWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEc=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTg2cEJVZU4zZUYvSE5NMVVZMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                         徐彦.基于梯度下降的脉冲神经元在线学习方法[J].计算机工程,2015,41(12):150-155,160.
                                    </a>
                                </li>
                                <li id="159">


                                    <a id="bibliography_9" title=" XU Yan,YANG Jing,ZHONG Shuiming.An online supervised learning method based on gradient descent for spiking neurons[J].Neural Networks,2017,93:7-20." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESB3B9362AB34B38D6DC3A58C2C625E162&amp;v=MjQwMDR3N3k1d3FBPU5pZk9mY0c3Yk5qUHFZMDBGdWdMZm44eHV4Qm5tVHdNVFhlUnJtRXplN2ZoUkx5ZENPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkNwYlEzNU4xaA==&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTg2cEJVZU4zZUYvSE5NMVVZMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                         XU Yan,YANG Jing,ZHONG Shuiming.An online supervised learning method based on gradient descent for spiking neurons[J].Neural Networks,2017,93:7-20.
                                    </a>
                                </li>
                                <li id="161">


                                    <a id="bibliography_10" title=" FLORIAN R V.The chronotron:a neuron that learns to fire temporally precise spike patterns [EB/OL].[2018-05-21].https://www.ncbi.nlm.nih.gov/pubmed/22879876." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The chronotron:a neuron that learns to fire temporally precise spike patterns">
                                        <b>[10]</b>
                                         FLORIAN R V.The chronotron:a neuron that learns to fire temporally precise spike patterns [EB/OL].[2018-05-21].https://www.ncbi.nlm.nih.gov/pubmed/22879876.
                                    </a>
                                </li>
                                <li id="163">


                                    <a id="bibliography_11" title=" VICTOR J D,PURPURA K P.Nature and precision of temporal coding in visual cortex:a metric-space analysis[J].Journal of Neurophysiology,1996,76(2):1310-1326." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Nature and precision of temporal coding in visual cortex: a metric-space analysis">
                                        <b>[11]</b>
                                         VICTOR J D,PURPURA K P.Nature and precision of temporal coding in visual cortex:a metric-space analysis[J].Journal of Neurophysiology,1996,76(2):1310-1326.
                                    </a>
                                </li>
                                <li id="165">


                                    <a id="bibliography_12" title=" MARKRAM H,TSODYKS M.Redistribution of synaptic efficacy between neocortical pyramidal neurons[J].Nature,1996,382(6594):807-810." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Redistribution of synaptic efficacy between neocortical pyramidal neurons">
                                        <b>[12]</b>
                                         MARKRAM H,TSODYKS M.Redistribution of synaptic efficacy between neocortical pyramidal neurons[J].Nature,1996,382(6594):807-810.
                                    </a>
                                </li>
                                <li id="167">


                                    <a id="bibliography_13" title=" PONULAK F,KASINSKI A.Supervised learning in spiking neural networks with ReSuMe:sequence learning,classification and spike shifting[J].Neural Computation,2010,22(2):467-510." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJBK&amp;filename=SJBK15090500013038&amp;v=MDEyNTBNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VTHJJSjFvU2FCbz1OaWZKWmJLOUh0ak1xbzlGWk9vTURIOHhvQg==&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTg2cEJVZU4zZUYvSE5NMVVZMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[13]</b>
                                         PONULAK F,KASINSKI A.Supervised learning in spiking neural networks with ReSuMe:sequence learning,classification and spike shifting[J].Neural Computation,2010,22(2):467-510.
                                    </a>
                                </li>
                                <li id="169">


                                    <a id="bibliography_14" title=" 徐彦,杨静.脉冲神经元序列学习方法的影响因素研究[J].计算机工程,2015,41(11):194-201." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201511035&amp;v=MTI2NjF6N0JiYkc0SDlUTnJvOUdZWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUnJGeS9oVmIzQUw=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTg2cEJVZU4zZUYvSE5NMVVZMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[14]</b>
                                         徐彦,杨静.脉冲神经元序列学习方法的影响因素研究[J].计算机工程,2015,41(11):194-201.
                                    </a>
                                </li>
                                <li id="171">


                                    <a id="bibliography_15" title=" XU Yan,ZENG Xiaoqin,ZHONG Shuiming.A new supervised learning algorithm for spiking neurons[J].Neural Computation,2013,25(6):1472-1511." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJBK&amp;filename=SJBK15090500013411&amp;v=MTY4MTA5RlpPb01DSDA0b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VTHJJSjFvU2FCbz1OaWZKWmJLOUh0ak1xbw==&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTg2cEJVZU4zZUYvSE5NMVVZMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[15]</b>
                                         XU Yan,ZENG Xiaoqin,ZHONG Shuiming.A new supervised learning algorithm for spiking neurons[J].Neural Computation,2013,25(6):1472-1511.
                                    </a>
                                </li>
                                <li id="173">


                                    <a id="bibliography_16" title=" MOHEMMED A,SCHLIEBS S,MATSUDA S,et al.Training spiking neural networks to associate spatio-temporal input-output spike patterns[J].Neurocomputing,2013,107(4):3-10." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501913275&amp;v=MDcyMTd0RE5xbzlFYmVvTURuczhvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVVMcklKMW9TYUJvPU5pZk9mYks3SA==&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTg2cEJVZU4zZUYvSE5NMVVZMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[16]</b>
                                         MOHEMMED A,SCHLIEBS S,MATSUDA S,et al.Training spiking neural networks to associate spatio-temporal input-output spike patterns[J].Neurocomputing,2013,107(4):3-10.
                                    </a>
                                </li>
                                <li id="175">


                                    <a id="bibliography_17" title=" YU Qiang,TANG Huajin,TAN K C,et al.Precise-spike-driven synaptic plasticity:learning hetero-association of spatiotemporal spike patterns [EB/OL].[2018-05-21].https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0078318." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Precise-spike-driven synaptic plasticity:learning hetero-association of spatiotemporal spike patterns">
                                        <b>[17]</b>
                                         YU Qiang,TANG Huajin,TAN K C,et al.Precise-spike-driven synaptic plasticity:learning hetero-association of spatiotemporal spike patterns [EB/OL].[2018-05-21].https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0078318.
                                    </a>
                                </li>
                                <li id="177">


                                    <a id="bibliography_18" title=" LIN Xianghong,ZHANG Ning,WANG Xiangwen.An online supervised learning algorithm based on nonlinear spike train kernels [C]//Proceedings of International Conference on Intelligent Computing.Berlin,Germany:Springer,2015:106-115." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=An online supervised learning algorithm based on nonlinear spike train kernels">
                                        <b>[18]</b>
                                         LIN Xianghong,ZHANG Ning,WANG Xiangwen.An online supervised learning algorithm based on nonlinear spike train kernels [C]//Proceedings of International Conference on Intelligent Computing.Berlin,Germany:Springer,2015:106-115.
                                    </a>
                                </li>
                                <li id="179">


                                    <a id="bibliography_19" title=" GARDNER B,GR&#220;NING A.Supervised learning in spiking neural networks for precise temporal encoding[EB/OL].[2018-05-21].https://www.ncbi.nlm.nih.gov/pubmed/27532262." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Supervised learning in spiking neural networks for precise temporal encoding">
                                        <b>[19]</b>
                                         GARDNER B,GR&#220;NING A.Supervised learning in spiking neural networks for precise temporal encoding[EB/OL].[2018-05-21].https://www.ncbi.nlm.nih.gov/pubmed/27532262.
                                    </a>
                                </li>
                                <li id="181">


                                    <a id="bibliography_20" title=" 徐彦,熊迎军,杨静.脉冲神经元脉冲序列学习方法综述[J].计算机应用,2018,38(6):1527-1534.." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201806002&amp;v=MTc1MDJDVVJMT2VaZVJyRnkvaFZiM0FMejdCZDdHNEg5bk1xWTlGWm9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnI=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTg2cEJVZU4zZUYvSE5NMVVZMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[20]</b>
                                         徐彦,熊迎军,杨静.脉冲神经元脉冲序列学习方法综述[J].计算机应用,2018,38(6):1527-1534..
                                    </a>
                                </li>
                                <li id="183">


                                    <a id="bibliography_21" title=" XU Bo,GONG Yubing,WANG Baoying.Delay-induced firing behavior and transitions in adaptive neuronal networks with two types of synapses[J].Science China Chemistry,2013,56 (2):222-229." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD130409020623&amp;v=MjI3ODJ3Y05qN0Jhcks3SHRYTXBvOUhaTzBORHhNOHp4VVNtRGQ5U0g3bjN4RTlmYnZuS3JpZlp1OXVGQ3ZnVTdqTUps&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTg2cEJVZU4zZUYvSE5NMVVZMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[21]</b>
                                         XU Bo,GONG Yubing,WANG Baoying.Delay-induced firing behavior and transitions in adaptive neuronal networks with two types of synapses[J].Science China Chemistry,2013,56 (2):222-229.
                                    </a>
                                </li>
                                <li id="185">


                                    <a id="bibliography_22" title=" SCHRAUWEN B,CAMPENHOUT J V.Extending SpikeProp [C]//Proceedings of IEEE International Joint Conference on Neural Network.Washington D.C.,USA:IEEE Press,2004:471-475." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Extending spikeprop">
                                        <b>[22]</b>
                                         SCHRAUWEN B,CAMPENHOUT J V.Extending SpikeProp [C]//Proceedings of IEEE International Joint Conference on Neural Network.Washington D.C.,USA:IEEE Press,2004:471-475.
                                    </a>
                                </li>
                                <li id="187">


                                    <a id="bibliography_23" title=" TAHERKHANI A,BELATRECHE A,LI Yuhua,et al.DL-ReSuMe:a delay learning-based remote supervised method for spiking neurons[C]//Proceedings of International Conference on Neural Information Processing.Berlin,Germany:Springer,2015:190-197." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=DL-ReSuMe:a delay learning-based remote supervised method for spiking neurons">
                                        <b>[23]</b>
                                         TAHERKHANI A,BELATRECHE A,LI Yuhua,et al.DL-ReSuMe:a delay learning-based remote supervised method for spiking neurons[C]//Proceedings of International Conference on Neural Information Processing.Berlin,Germany:Springer,2015:190-197.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJC" target="_blank">计算机工程</a>
                2019,45(09),169-175 DOI:10.19678/j.issn.1000-3428.0052234            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>带延迟调整的脉冲神经元梯度下降学习算法</b></span>
 <span class="shoufa"></span>                                     </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%A8%E9%9D%99&amp;code=42806577&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTg2cEJVZU4zZUYvSE5NMVVZMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">杨静</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BE%90%E5%BD%A6&amp;code=33396760&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTg2cEJVZU4zZUYvSE5NMVVZMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">徐彦</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%B5%B5%E6%AC%A3&amp;code=42806578&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTg2cEJVZU4zZUYvSE5NMVVZMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">赵欣</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%8C%97%E4%BA%AC%E5%B8%88%E8%8C%83%E5%A4%A7%E5%AD%A6%E7%8F%A0%E6%B5%B7%E5%88%86%E6%A0%A1%E7%AE%A1%E7%90%86%E5%AD%A6%E9%99%A2&amp;code=1749784&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTg2cEJVZU4zZUYvSE5NMVVZMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">北京师范大学珠海分校管理学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%8D%97%E4%BA%AC%E5%86%9C%E4%B8%9A%E5%A4%A7%E5%AD%A6%E4%BF%A1%E6%81%AF%E7%A7%91%E6%8A%80%E5%AD%A6%E9%99%A2&amp;code=0167914&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTg2cEJVZU4zZUYvSE5NMVVZMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">南京农业大学信息科技学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>脉冲神经元有监督学习算法通过梯度下降法调整神经元的突触权值,但目标学习序列长度的增加会降低其精度并延长学习周期。为此,提出一种带延迟调整的梯度下降学习算法。将每个突触的延迟作为学习参数,在学习过程中调整权值,同时对突触的延迟时间进行梯度下降调整,从而使神经元激发出目标脉冲序列。实验结果表明,该算法在不增加算法复杂度的情况下,能够提高神经元学习复杂脉冲序列的能力,且收敛速度较快。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%84%89%E5%86%B2%E7%A5%9E%E7%BB%8F%E5%85%83&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTg2cEJVZU4zZUYvSE5NMVVZMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">脉冲神经元;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%84%89%E5%86%B2%E5%BA%8F%E5%88%97&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTg2cEJVZU4zZUYvSE5NMVVZMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">脉冲序列;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTg2cEJVZU4zZUYvSE5NMVVZMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">梯度下降;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%A5%9E%E7%BB%8F%E5%85%83%E7%AA%81%E8%A7%A6&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTg2cEJVZU4zZUYvSE5NMVVZMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">神经元突触;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%BB%B6%E8%BF%9F%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTg2cEJVZU4zZUYvSE5NMVVZMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">延迟学习;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    杨静(1980—),女,副教授、博士,主研方向为神经网络、模式识别;E-mail:yangjing3000@ sina. com;
                                </span>
                                <span>
                                    徐彦,讲师、博士;;
                                </span>
                                <span>
                                    赵欣,副教授、博士。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-07-27</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金(61503031);</span>
                                <span>广东省科技计划基金(2016A040403029);</span>
                                <span>广东省哲学社会科学“十二五”规划学科共建项目(GD15XGL02);</span>
                    </p>
            </div>
                    <h1><b>Gradient Descent Learning Algorithm for Spiking Neuron with Delay Adjustment</b></h1>
                    <h2>
                    <span>YANG Jing</span>
                    <span>XU Yan</span>
                    <span>ZHAO Xin</span>
            </h2>
                    <h2>
                    <span>School of Management,Zhuhai Campus,Beijing Normal University</span>
                    <span>College of Information Science and Technology,Nanjing Agricultural University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>The spiking neuron supervised learning algorithm adjusts the synaptic weight of the neuron by gradient descent method,but the accuracy gets low and the learning period gets long as the length of the target learning sequence increases.Therefore,a gradient descent learning algorithm with delay adjustment is proposed.The delay of each synapse is used as a learning parameter to adjust the weight during the learning process,and at the same time,the gradient delay adjustment of the synaptic delay time is performed to make neurons trigger a target spiking sequence.Experimental results show that the proposed algorithm can improve the ability of neurons to learn complex spiking sequences without increasing the complexity of the algorithm,and the convergence speed is faster.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=spiking%20neuron&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTg2cEJVZU4zZUYvSE5NMVVZMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">spiking neuron;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=spiking%20sequence&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTg2cEJVZU4zZUYvSE5NMVVZMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">spiking sequence;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=gradient%20descent&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTg2cEJVZU4zZUYvSE5NMVVZMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">gradient descent;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=neuron%20synapse&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTg2cEJVZU4zZUYvSE5NMVVZMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">neuron synapse;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=delay%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTg2cEJVZU4zZUYvSE5NMVVZMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">delay learning;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-07-27</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="49" name="49" class="anchor-tag">0 概述</h3>
                <div class="p1">
                    <p id="50">脉冲神经网络(Spiking Neural Network,SNN)被称为第三代神经网络,是一种基于连续时间的多层全连接前馈网络,其输入和输出均为脉冲时间序列,通过自身的学习机制来调整网络参数,从而在网络的输入和输出之间建立一种特定的映射关系。在神经元内部,通过计算输入脉冲改变神经元内部状态值,即膜电位,当内部状态达到或超过设定的阙值时,神经元会激发一个脉冲。脉冲神经元模型主要有脉冲反应模型(Spike Response Model,SRM)、漏积分(Leaky Integrate and Fire,LIF)模型和HH(Hodgkin-Huxley)模型。其中,SRM模型具有显式的膜电位表达式。本文基于有监督学习,提出一种带延迟学习的梯度下降算法。以SRM为研究对象,通过改进的梯度下降算法调整权值和延迟,以提高脉冲神经元的学习精度。</p>
                </div>
                <h3 id="51" name="51" class="anchor-tag">1 相关工作</h3>
                <div class="p1">
                    <p id="52">有监督学习算法被广泛应用于梯度下降学习方法。该算法通过梯度下降法最小化误差函数,使网络能够正确的输出,但需要计算误差函数的偏导数,要求网络的激活函数连续且必须可微。然而脉冲神经元的输出并不是其内部状态的值,而是神经元在运行过程中所激发的各个脉冲时刻,因此脉冲神经元不能根据输出对其内部状态进行求导。文献<citation id="189" type="reference">[<a class="sup">2</a>]</citation>提出一种基于梯度下降的脉冲神经网络有监督学习方法SpikeProp,在输出脉冲激发时刻的一个小范围中,通过假设神经元的内部状态是时间线性函数,以解决对神经元内部状态的求导问题。在此基础上,国内外学者提出带动量的反向传播SpikeProp的变形算法,如QuickProp、Rprop、Levenberg-Marquardt BP等<citation id="194" type="reference"><link href="147" rel="bibliography" /><link href="149" rel="bibliography" /><link href="151" rel="bibliography" /><sup>[<a class="sup">3</a>,<a class="sup">4</a>,<a class="sup">5</a>]</sup></citation>,但只能处理单脉冲的情况,即输入层、隐层和输出层神经元仅激发一个脉冲。文献<citation id="190" type="reference">[<a class="sup">6</a>]</citation>提出一种基于多脉冲的反向传播学习算法,但只有隐层神经元可以激发多脉冲。文献<citation id="191" type="reference">[<a class="sup">7</a>]</citation>提出一种可以处理各层输出均为多脉冲情况的梯度下降有监督学习方法MSGDB,该方法假设实际输出与期望输出有相同个数的脉冲,并据此构造误差函数,在神经元运行完成后得到所有的实际输出脉冲序列后调整权值,属于离线学习方式。在线学习方法<citation id="195" type="reference"><link href="157" rel="bibliography" /><link href="159" rel="bibliography" /><sup>[<a class="sup">8</a>,<a class="sup">9</a>]</sup></citation>在神经元激发出脉冲时,根据当前激发和对应的期望激发构造实时误差函数,并调整权值最小化误差函数。该方法比离线方法的学习性能强。文献<citation id="192" type="reference">[<a class="sup">10</a>]</citation>提出一种基于梯度下降的有监督学习算法,该算法采用一种改进的VP度量<citation id="193" type="reference"><link href="163" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>来衡量实际输出脉冲序列和目标脉冲序列之间的距离。</p>
                </div>
                <div class="p1">
                    <p id="53">文献<citation id="196" type="reference">[<a class="sup">12</a>]</citation>提出一种基于脉冲时间依赖可塑性(Spike-Timing-Dependent-Plasticity,STDP)规则<citation id="197" type="reference"><link href="165" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>,该规则是一种根据实际激发的脉冲以及输入脉冲自发调整突触作用大小的学习方法。基于此规则,研究者提出一种称为ReSuMe (Remote Supervised Method)<citation id="198" type="reference"><link href="167" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>的脉冲神经元学习算法,其权值更新分为两部分:在期望输出时间点上权值的增强和在实际输出时间点上的权值减弱。ReSuMe方法具有较高的学习精度且窗口函数独立于神经元的内部状态表达式,因此其可以适用于多种神经元模型,具有较好的适用性<citation id="199" type="reference"><link href="169" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>。I-learning方法<citation id="200" type="reference"><link href="161" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>依据STDP规则调整权值,但该方法以在线方式运行,更符合生物神经元的行为特性。文献<citation id="201" type="reference">[<a class="sup">15</a>]</citation>提出一种基于感知机规则的脉冲神经元学习算法,通过将有监督学习转换为两分类问题,进而利用感知机规则来解决分类问题。SPAN (Spike Pattern Association Neuron) 学习方法<citation id="202" type="reference"><link href="173" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>基于Windrow-Hoff规则。该方法采用卷积方式将离散的脉冲序列时刻转换为所对应的实数值并将其代入Widrow-Hoff规则得到权值调整公式。另一种类似的方法PSD (Precise-Spike-Driven)<citation id="203" type="reference"><link href="175" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>也是采用卷积方式来转换脉冲序列,该方法基于LIF神经元模型并且只对输入脉冲进行卷积。文献<citation id="204" type="reference">[<a class="sup">18</a>]</citation>提出一种基于STK (Spike Train Kernels)的在线学习算法,该算法利用卷积核与一个非线性函数相结合完成对输入脉冲序列的转换。文献<citation id="205" type="reference">[<a class="sup">19</a>]</citation>提出基于随机的脉冲序列学习方法,该方法将期望与实际输出脉冲之间的误差通过滤镜函数处理后,调整权值以减少滤镜作用后的误差。文献<citation id="206" type="reference">[<a class="sup">20</a>]</citation>指出,基于梯度下降的方法具有较高的学习精度,但由于多脉冲学习之间的互相干扰,权值调整速率需要控制在一个较小的范围内,这导致算法需要较多的学习周期,收敛速度较慢。</p>
                </div>
                <div class="p1">
                    <p id="54">以上学习算法均只对神经元的突触权值进行调整,并没有涉及到突触延迟的调整。而生物学证据表明:在突触前神经元与突触后神经元之间的突触在传递信号时存在时间延迟,这些时间延迟对于神经元的激发和转换行为的影响较大<citation id="207" type="reference"><link href="183" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>。文献<citation id="208" type="reference">[<a class="sup">22</a>]</citation>对SpikeProp算法进行改进,增加了延迟的梯度下降调整,然而该算法仅适用于单脉冲的情况。文献<citation id="209" type="reference">[<a class="sup">23</a>]</citation>提出一种基于ReSuMe的带延迟学习的启发式神经元学习算法DL-ReSuMe,其中采用的神经元模型为LIF。神经元初始延迟被设为0,在延迟调整过程中,有两类突触需要被加上延迟,分别是激励突触(权值为正)和抑制突触(权值为负)。实验结果表明,该算法相比ReSuMe算法提高了10%的学习精度且收敛速度较快。</p>
                </div>
                <h3 id="55" name="55" class="anchor-tag">2 SRM神经元梯度下降学习算法</h3>
                <h4 class="anchor-tag" id="56" name="56">2.1 SRM神经元模型</h4>
                <div class="p1">
                    <p id="57">本文神经元模型为脉冲反应模型,神经元接收由多个突触传输的一串离散脉冲序列,这些脉冲在到达神经元后会使神经元的内部状态值发生变化。神经元的内部状态值由2个部分组成:第1部分由输入脉冲产生的突触后电位(Postsynaptic Potential,PSP)与权值<i>w</i><sub><i>i</i></sub>共同决定的,第2部分是不应期函数<i>η</i>(<i>t</i>)。其计算公式如下:</p>
                </div>
                <div class="p1">
                    <p id="58" class="code-formula">
                        <mathml id="58"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>u</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mtext> </mtext></mstyle><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mtable><mtr><mtd><mi>t</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mo stretchy="false">(</mo><mi>g</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>∈</mo><mi>G</mi><msub><mrow></mrow><mi>i</mi></msub></mtd></mtr><mtr><mtd><mi>t</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mo stretchy="false">(</mo><mi>g</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>+</mo><mi>d</mi><msub><mrow></mrow><mi>i</mi></msub><mo>&gt;</mo><mi>t</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>f</mi><mi>r</mi><mo stretchy="false">)</mo></mrow></msup><mo>+</mo><mi>R</mi><msub><mrow></mrow><mi>a</mi></msub></mtd></mtr></mtable></mrow></munder><mi>w</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mi>ε</mi><mo stretchy="false">(</mo><mi>t</mi><mo>-</mo><mi>t</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mo stretchy="false">(</mo><mi>g</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>-</mo><mi>d</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo>+</mo><mi>η</mi><mo stretchy="false">(</mo><mi>t</mi><mo>-</mo><mi>t</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>f</mi><mi>r</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="59">其中,<i>N</i>表示神经元的输入突触个数,<i>t</i><mathml id="115"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mrow><mo stretchy="false">(</mo><mi>g</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow></math></mathml>表示第i个突触的第g个脉冲到达神经元的时间,d<sub>i</sub>表示第i个突触的延迟,t<sup>(fr)</sup>表示神经元当前运行时刻t之前的最近一次激发时刻,R<sub>a</sub>表示绝对不应期长度,使突触输入在最近一次脉冲激发时刻t<sup>(fr)</sup>～t<sup>(fr)</sup>+R<sub>a</sub>时间内对神经元内部状态影响无效。反应函数有多种形式,本文选用一种较常用的反应函数,其表达式为:</p>
                </div>
                <div class="p1">
                    <p id="60" class="code-formula">
                        <mathml id="60"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>ε</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo>=</mo><mrow><mo>{</mo><mrow><mtable><mtr><mtd columnalign="left"><mfrac><mi>t</mi><mi>τ</mi></mfrac><mi>exp</mi><mrow><mo>(</mo><mrow><mn>1</mn><mo>-</mo><mfrac><mi>t</mi><mi>τ</mi></mfrac></mrow><mo>)</mo></mrow><mo>,</mo><mspace width="0.25em" /><mi>t</mi><mo>&gt;</mo><mn>0</mn></mtd></mtr><mtr><mtd columnalign="left"><mn>0</mn><mo>,</mo><mspace width="0.25em" /><mi>t</mi><mo>≤</mo><mn>0</mn></mtd></mtr></mtable></mrow></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="61">其中,τ是决定反应函数形态的时间延迟常数。</p>
                </div>
                <div class="p1">
                    <p id="62">当神经元不断接收输入脉冲时,其内部状态值会逐渐增加直到超过特定阙值ϑ,神经元会激发出一个脉冲,且内部状态值立即降为0。绝对不应期长度R<sub>a</sub>使神经元在[t<sup>(fr)</sup>,t<sup>(fr)</sup>+R<sub>a</sub>]时间段内不会连续激发,这个时间段被称为绝对不应期。随后神经元会进入一个相对不容易再次激发脉冲的时期,这个时间段称为相对不应期。相对不应期对内部状态值的影响由η(t)函数描述。</p>
                </div>
                <div class="p1">
                    <p id="63" class="code-formula">
                        <mathml id="63"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>η</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo>=</mo><mrow><mo>{</mo><mrow><mtable><mtr><mtd columnalign="left"><mo>-</mo><mn>2</mn><mtext>ϑ</mtext><mtext>e</mtext><msup><mrow></mrow><mrow><mo>-</mo><mfrac><mi>t</mi><mrow><mi>τ</mi><msub><mrow></mrow><mi>R</mi></msub></mrow></mfrac></mrow></msup><mo>,</mo><mspace width="0.25em" /><mi>t</mi><mo>&gt;</mo><mn>0</mn></mtd></mtr><mtr><mtd columnalign="left"><mn>0</mn><mo>,</mo><mspace width="0.25em" /><mi>t</mi><mo>≤</mo><mn>0</mn></mtd></mtr></mtable></mrow></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>3</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="64">其中,τ<sub>R</sub>是决定不应期函数性态的延迟常数。可以看出,η(t)是一个恒负的函数,其作用是当神经元有激发时,会产生一个较大的负值,以降低神经元的内部状态值,使其难以连续激发,离上次激发时间越远,抑制作用会越小。</p>
                </div>
                <h4 class="anchor-tag" id="65" name="65">2.2 多脉冲神经元梯度下降学习算法</h4>
                <div class="p1">
                    <p id="66">多脉冲序列学习算法通过梯度下降方法最小化根据实际输出和期望输出所构造的误差函数。由于脉冲神经元的输出是一个脉冲序列,因此当神经元有多个脉冲输出时,神经元的实际脉冲输出个数可能和期望输出的不相等。为解决该矛盾,通常采用动态选取脉冲的方法。MSGDB有2种运行方式:在线和离线学习方式。在离线学习方式中,权值调整发生于神经元已激发出所有的脉冲后。假设神经元的期望输出由<mathml id="116"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi>F</mi></mstyle><mrow><mspace width="0.25em" /><mo>^</mo></mrow></mover></mrow></math></mathml>个脉冲组成的序列为<mathml id="117"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi>Τ</mi></mstyle><mrow><mspace width="0.25em" /><mo>^</mo></mrow></mover><mo>=</mo><mo stretchy="false">{</mo><mover><mstyle mathsize="140%" displaystyle="true"><mi>t</mi></mstyle><mrow><mspace width="0.25em" /><mo>^</mo></mrow></mover><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><mo>,</mo><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi>t</mi></mstyle><mrow><mspace width="0.25em" /><mo>^</mo></mrow></mover><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></msup><mo>,</mo><mo>⋯</mo><mo>,</mo></mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi>t</mi></mstyle><mrow><mspace width="0.25em" /><mo>^</mo></mrow></mover><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mover><mstyle mathsize="140%" displaystyle="true"><mi>F</mi></mstyle><mrow><mspace width="0.25em" /><mo>^</mo></mrow></mover><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">}</mo></mrow></math></mathml>,而实际输出为<i>T</i>={<i>t</i><sup>(1)</sup>,<i>t</i><sup>(2)</sup>,…,<i>t</i><sup>(</sup><i>F</i>)},则误差函数定义为:</p>
                </div>
                <div class="p1">
                    <p id="67" class="code-formula">
                        <mathml id="67"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>E</mi><mo>=</mo><mrow><mo>{</mo><mrow><mtable><mtr><mtd><mfrac><mn>1</mn><mn>2</mn></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>f</mi><mo>=</mo><mn>1</mn></mrow><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi>F</mi></mstyle><mrow><mspace width="0.25em" /><mo>^</mo></mrow></mover></mrow></munderover><mrow><mo stretchy="false">(</mo><mi>t</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">)</mo></mrow></msup><mo>-</mo><mover><mstyle mathsize="140%" displaystyle="true"><mi>t</mi></mstyle><mrow><mspace width="0.25em" /><mo>^</mo></mrow></mover><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo></mrow></mstyle><msup><mrow></mrow><mn>2</mn></msup><mo>,</mo><mi>F</mi><mo>≥</mo><mover><mstyle mathsize="140%" displaystyle="true"><mi>F</mi></mstyle><mrow><mspace width="0.25em" /><mo>^</mo></mrow></mover></mtd></mtr><mtr><mtd><mfrac><mn>1</mn><mn>2</mn></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>f</mi><mo>=</mo><mn>1</mn></mrow><mi>F</mi></munderover><mrow><mo stretchy="false">(</mo><mi>t</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">)</mo></mrow></msup><mo>-</mo><mover><mstyle mathsize="140%" displaystyle="true"><mi>t</mi></mstyle><mrow><mspace width="0.25em" /><mo>^</mo></mrow></mover><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo></mrow></mstyle><msup><mrow></mrow><mn>2</mn></msup><mo>,</mo><mi>F</mi><mo>&lt;</mo><mover><mstyle mathsize="140%" displaystyle="true"><mi>F</mi></mstyle><mrow><mspace width="0.25em" /><mo>^</mo></mrow></mover></mtd></mtr></mtable></mrow></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>4</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="68">在线学习方式中,每当神经元激发出一个脉冲就基于此输出立刻调整权值。假设当前神经元在时刻<i>t</i><sup>(</sup><i>f</i>)激发出第<i>f</i>个脉冲,而相应的第<i>f</i>个期望输出为<mathml id="118"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi>t</mi></mstyle><mrow><mspace width="0.25em" /><mo>^</mo></mrow></mover><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">)</mo></mrow></msup></mrow></math></mathml>,则误差函数定义为:</p>
                </div>
                <div class="p1">
                    <p id="69" class="code-formula">
                        <mathml id="69"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>E</mi><mo>=</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mrow><mo stretchy="false">(</mo><mi>t</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">)</mo></mrow></msup><mo>-</mo><mover><mstyle mathsize="140%" displaystyle="true"><mi>t</mi></mstyle><mrow><mspace width="0.25em" /><mo>^</mo></mrow></mover><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo></mrow><msup><mrow></mrow><mn>2</mn></msup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>5</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="70">根据以上构造的误差函数,基于梯度下降的调权规则为:</p>
                </div>
                <div class="p1">
                    <p id="71" class="code-formula">
                        <mathml id="71"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtext>Δ</mtext><mi>w</mi><mo>=</mo><mo>-</mo><mi>α</mi><mo>∇</mo><mi>E</mi><mo>=</mo><mo>-</mo><mi>α</mi><mfrac><mrow><mo>∂</mo><mi>E</mi></mrow><mrow><mo>∂</mo><mi>w</mi></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>6</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="72">其中,导数∂<i>E</i>/∂<i>w</i>的求解需要利用求导的链式法则,并假设神经元的内部状态在当前输出时刻附近为线性函数。</p>
                </div>
                <h3 id="73" name="73" class="anchor-tag">3 带延迟学习的梯度下降算法</h3>
                <div class="p1">
                    <p id="74">时间延迟是生物神经系统的固有特性之一,在神经元的脉冲激发以及神经元之间的信息传递中具有重要的作用。脉冲神经元的时间延迟在神经元的运行过程中会随着内部状态的变化而变化,促进神经元的同步、异步。脉冲神经元的各个输入突触在时间<i>t</i><sub><i>i</i></sub>传递过来的脉冲经过延迟<i>d</i><sub><i>i</i></sub>后到达神经元的时间变为<i>t</i><sub><i>i</i></sub>+<i>d</i><sub><i>i</i></sub>,再与各突触的权值共同决定神经元的内部状态值。带延迟学习的梯度下降算法依据脉冲神经元的此生物特征,在每一次学习中调整权值和延迟,且调整幅度通过梯度下降计算。</p>
                </div>
                <div class="p1">
                    <p id="75">以离线学习方式为例,延迟学习中神经元的第<i>i</i>个突触延迟<i>d</i><sub><i>i</i></sub>的调整规则为:</p>
                </div>
                <div class="p1">
                    <p id="76" class="code-formula">
                        <mathml id="76"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mtext>Δ</mtext><mi>d</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mo>-</mo><mi>α</mi><msub><mrow></mrow><mi>d</mi></msub><mfrac><mrow><mo>∂</mo><mi>E</mi></mrow><mrow><mo>∂</mo><mi>d</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></mfrac><mo>=</mo><mo>-</mo><mfrac><mrow><mi>α</mi><msub><mrow></mrow><mi>d</mi></msub></mrow><mn>2</mn></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>f</mi><mo>=</mo><mn>1</mn></mrow><mi>F</mi></munderover><mrow><mfrac><mrow><mo>∂</mo><mo stretchy="false">(</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">)</mo></mrow></msup><mo>-</mo><mover><mstyle mathsize="140%" displaystyle="true"><mi>t</mi></mstyle><mrow><mspace width="0.25em" /><mo>^</mo></mrow></mover><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo></mrow><msup><mrow></mrow><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><mrow><mo>∂</mo><mi>d</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></mfrac></mrow></mstyle><mo>=</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mo>-</mo><mfrac><mrow><mi>α</mi><msub><mrow></mrow><mi>d</mi></msub></mrow><mn>2</mn></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>f</mi><mo>=</mo><mn>1</mn></mrow><mi>F</mi></munderover><mrow><mfrac><mrow><mo>∂</mo><mo stretchy="false">(</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">)</mo></mrow></msup><mo>-</mo><mover><mstyle mathsize="140%" displaystyle="true"><mi>t</mi></mstyle><mrow><mspace width="0.25em" /><mo>^</mo></mrow></mover><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo></mrow><msup><mrow></mrow><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><mrow><mo>∂</mo><mi>t</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">)</mo></mrow></msup></mrow></mfrac></mrow></mstyle><mfrac><mrow><mo>∂</mo><mi>t</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">)</mo></mrow></msup></mrow><mrow><mo>∂</mo><mi>d</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></mfrac><mo>=</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mo>-</mo><mi>α</mi><msub><mrow></mrow><mi>d</mi></msub><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>f</mi><mo>=</mo><mn>1</mn></mrow><mi>F</mi></munderover><mo stretchy="false">(</mo></mstyle><mi>t</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">)</mo></mrow></msup><mo>-</mo><mover><mstyle mathsize="140%" displaystyle="true"><mi>t</mi></mstyle><mrow><mspace width="0.25em" /><mo>^</mo></mrow></mover><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo><mfrac><mrow><mo>∂</mo><mi>t</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">)</mo></mrow></msup></mrow><mrow><mo>∂</mo><mi>d</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>7</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="77">其中,<i>α</i><sub><i>d</i></sub>为延迟的调整速率。可以看出,与权值梯度下降的结果类似,第<i>i</i>个突触延迟<i>d</i><sub><i>i</i></sub>的调整幅度主要由实际输出脉冲和期望输出脉冲之差与∂<i>t</i><sup>(</sup><i>f</i>)/∂<i>d</i><sub><i>i</i></sub>决定,该偏导数采用权值梯度下降的递归方法计算。</p>
                </div>
                <div class="p1">
                    <p id="78" class="code-formula">
                        <mathml id="78"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mfrac><mrow><mo>∂</mo><mi>t</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">)</mo></mrow></msup></mrow><mrow><mo>∂</mo><mi>d</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></mfrac><mo>=</mo><mfrac><mrow><mo>∂</mo><mi>t</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">)</mo></mrow></msup></mrow><mrow><mo>∂</mo><mi>u</mi><mo stretchy="false">(</mo><mi>t</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo></mrow></mfrac><mrow><mo>(</mo><mrow><mfrac><mrow><mo>∂</mo><mi>u</mi><mo stretchy="false">(</mo><mi>t</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo></mrow><mrow><mo>∂</mo><mi>d</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></mfrac><mo>+</mo><mfrac><mrow><mo>∂</mo><mi>u</mi><mo stretchy="false">(</mo><mi>t</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo></mrow><mrow><mo>∂</mo><mi>t</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>f</mi><mo>-</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup></mrow></mfrac><mfrac><mrow><mo>∂</mo><mi>t</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>f</mi><mo>-</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup></mrow><mrow><mo>∂</mo><mi>d</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></mfrac></mrow><mo>)</mo></mrow></mtd></mtr><mtr><mtd><mfrac><mrow><mo>∂</mo><mi>t</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>f</mi><mo>-</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup></mrow><mrow><mo>∂</mo><mi>d</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></mfrac><mo>=</mo><mfrac><mrow><mo>∂</mo><mi>t</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>f</mi><mo>-</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup></mrow><mrow><mo>∂</mo><mi>u</mi><mo stretchy="false">(</mo><mi>t</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>f</mi><mo>-</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo></mrow></mfrac><mrow><mo>(</mo><mrow><mfrac><mrow><mo>∂</mo><mi>u</mi><mo stretchy="false">(</mo><mi>t</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>f</mi><mo>-</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo></mrow><mrow><mo>∂</mo><mi>d</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></mfrac><mo>+</mo><mfrac><mrow><mo>∂</mo><mi>u</mi><mo stretchy="false">(</mo><mi>t</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>f</mi><mo>-</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo></mrow><mrow><mo>∂</mo><mi>t</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>f</mi><mo>-</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></msup></mrow></mfrac><mfrac><mrow><mo>∂</mo><mi>t</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>f</mi><mo>-</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></msup></mrow><mrow><mo>∂</mo><mi>d</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></mfrac></mrow><mo>)</mo></mrow></mtd></mtr><mtr><mtd><mspace width="0.25em" /><mo>⋮</mo></mtd></mtr><mtr><mtd><mfrac><mrow><mo>∂</mo><mi>t</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup></mrow><mrow><mo>∂</mo><mi>d</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></mfrac><mo>=</mo><mfrac><mrow><mo>∂</mo><mi>t</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup></mrow><mrow><mo>∂</mo><mi>u</mi><mo stretchy="false">(</mo><mi>t</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo></mrow></mfrac><mrow><mo>(</mo><mrow><mfrac><mrow><mo>∂</mo><mi>u</mi><mo stretchy="false">(</mo><mi>t</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo></mrow><mrow><mo>∂</mo><mi>d</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></mfrac></mrow><mo>)</mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>8</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="79">该递归过程与延迟相关的仅是其中的偏导数∂<i>u</i>(<i>t</i><sup>(</sup><i>f</i>))/∂<i>d</i><sub><i>i</i></sub>的计算,其余各部分的计算均可以直接利用权值调整过程的中间结果。神经元内部状态值<i>u</i>(<i>t</i><sup>(</sup><i>f</i>))中含有反应函数,而反应函数<i>ε</i>(<i>t</i>)并不是连续可微函数,求导时需分段考虑。</p>
                </div>
                <div class="p1">
                    <p id="80" class="code-formula">
                        <mathml id="80"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mo>∂</mo><mi>ε</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow><mrow><mo>∂</mo><mi>t</mi></mrow></mfrac><mo>→</mo><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd></mtd></mtr><mtr><mtd><mtable><mtr><mtd columnalign="left"><mtext>存</mtext><mtext>在</mtext><mo>,</mo><mi>t</mi><mo>&gt;</mo><mn>0</mn></mtd></mtr><mtr><mtd columnalign="left"><mtext>不</mtext><mtext>存</mtext><mtext>在</mtext><mo>,</mo><mi>t</mi><mo>=</mo><mn>0</mn></mtd></mtr><mtr><mtd columnalign="left"><mn>0</mn><mo>,</mo><mi>t</mi><mo>&lt;</mo><mn>0</mn></mtd></mtr></mtable></mtd></mtr><mtr><mtd></mtd></mtr></mtable></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>9</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="81">当<i>t</i>=0时,反应函数对<i>t</i>的导数不存在,在梯度下降的计算过程中只考虑神经元突触前输入脉冲,即满足条件<i>t</i><sup>(</sup><i>f</i>)≠<i>t</i><mathml id="119"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mrow><mo stretchy="false">(</mo><mi>g</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow></math></mathml>+<i>d</i><sub><i>i</i></sub>。当<i>t</i>≤0时,<i>ε</i>(<i>t</i>)=0,在选取参加神经元内部状态值计算的有效输入脉冲时加入条件<i>t</i><mathml id="120"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mrow><mo stretchy="false">(</mo><mi>g</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow></math></mathml>+<i>d</i><sub><i>i</i></sub>&lt;<i>t</i><sup>(</sup><i>f</i>),该条件不影响内部状态值计算,且能够避免导数不存在的情况。式(8)中内部状态值对于各个延迟的偏导数计算过程如下:</p>
                </div>
                <div class="p1">
                    <p id="82" class="code-formula">
                        <mathml id="82"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mtext> </mtext><mtext> </mtext><mfrac><mrow><mo>∂</mo><mtext> </mtext><mi>u</mi><mo stretchy="false">(</mo><mi>t</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo></mrow><mrow><mo>∂</mo><mtext> </mtext><mi>d</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></mfrac><mo>=</mo><mfrac><mrow><mo>∂</mo><mo stretchy="false">(</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mtable><mtr><mtd><mi>t</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mo stretchy="false">(</mo><mi>g</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>∈</mo><mi>G</mi><msub><mrow></mrow><mi>i</mi></msub></mtd></mtr><mtr><mtd><mi>t</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>f</mi><mo>-</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><mo>+</mo><mi>R</mi><msub><mrow></mrow><mi>a</mi></msub><mo>&lt;</mo><mi>t</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mo stretchy="false">(</mo><mi>g</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>+</mo><mi>d</mi><msub><mrow></mrow><mi>i</mi></msub><mo>&lt;</mo><mi>t</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">)</mo></mrow></msup></mtd></mtr></mtable></mrow></munder><mi>w</mi></mstyle></mrow></mstyle><msub><mrow></mrow><mi>i</mi></msub><mi>ε</mi><mo stretchy="false">(</mo><mi>t</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">)</mo></mrow></msup><mo>-</mo><mi>t</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mo stretchy="false">(</mo><mi>g</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>-</mo><mi>d</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo>+</mo><mi>η</mi><mo stretchy="false">(</mo><mi>t</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">)</mo></mrow></msup><mo>-</mo><mi>t</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>f</mi><mo>-</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><mrow><mo>∂</mo><mtext> </mtext><mi>d</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></mfrac><mo>=</mo><mi>w</mi><msub><mrow></mrow><mi>i</mi></msub><mfrac><mrow><mo>∂</mo><mo stretchy="false">(</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mtable><mtr><mtd><mi>t</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mo stretchy="false">(</mo><mi>g</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>∈</mo><mi>G</mi><msub><mrow></mrow><mi>i</mi></msub></mtd></mtr><mtr><mtd><mi>t</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>f</mi><mo>-</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><mo>+</mo><mi>R</mi><msub><mrow></mrow><mi>a</mi></msub><mo>&lt;</mo><mi>t</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mo stretchy="false">(</mo><mi>g</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>+</mo><mi>d</mi><msub><mrow></mrow><mi>i</mi></msub><mo>&lt;</mo><mi>t</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">)</mo></mrow></msup></mtd></mtr></mtable></mrow></munder><mi>ε</mi></mstyle><mo stretchy="false">(</mo><mi>t</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">)</mo></mrow></msup><mo>-</mo><mi>t</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mo stretchy="false">(</mo><mi>g</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>-</mo><mi>d</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><mrow><mo>∂</mo><mtext> </mtext><mi>d</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></mfrac><mo>=</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mi>w</mi><msub><mrow></mrow><mi>i</mi></msub><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mtable><mtr><mtd><mi>t</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mo stretchy="false">(</mo><mi>g</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>∈</mo><mi>G</mi><msub><mrow></mrow><mi>i</mi></msub></mtd></mtr><mtr><mtd><mi>t</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>f</mi><mo>-</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><mo>+</mo><mi>R</mi><msub><mrow></mrow><mi>a</mi></msub><mo>&lt;</mo><mi>t</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mo stretchy="false">(</mo><mi>g</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>+</mo><mi>d</mi><msub><mrow></mrow><mi>i</mi></msub><mo>&lt;</mo><mi>t</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">)</mo></mrow></msup></mtd></mtr></mtable></mrow></munder><mrow><mrow><mo>(</mo><mrow><mfrac><mrow><mo stretchy="false">(</mo><mi>t</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">)</mo></mrow></msup><mo>-</mo><mi>t</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mo stretchy="false">(</mo><mi>g</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>-</mo><mi>d</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><mrow><mi>τ</mi><msup><mrow></mrow><mn>2</mn></msup></mrow></mfrac><mtext>e</mtext><msup><mrow></mrow><mrow><mrow><mo>(</mo><mrow><mn>1</mn><mo>-</mo><mfrac><mrow><mo stretchy="false">(</mo><mi>t</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">)</mo></mrow></msup><mo>-</mo><mi>t</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mo stretchy="false">(</mo><mi>g</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>-</mo><mi>d</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><mi>τ</mi></mfrac></mrow><mo>)</mo></mrow></mrow></msup><mo>-</mo><mfrac><mrow><mtext>e</mtext><msup><mrow></mrow><mrow><mrow><mo>(</mo><mrow><mn>1</mn><mo>-</mo><mfrac><mrow><mo stretchy="false">(</mo><mi>t</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">)</mo></mrow></msup><mo>-</mo><mi>t</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mo stretchy="false">(</mo><mi>g</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>-</mo><mi>d</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><mi>τ</mi></mfrac></mrow><mo>)</mo></mrow></mrow></msup></mrow><mi>τ</mi></mfrac></mrow><mo>)</mo></mrow></mrow></mstyle><mo>=</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mi>w</mi><msub><mrow></mrow><mi>i</mi></msub><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mtable><mtr><mtd><mi>t</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mo stretchy="false">(</mo><mi>g</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>∈</mo><mi>G</mi><msub><mrow></mrow><mi>i</mi></msub></mtd></mtr><mtr><mtd><mi>t</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>f</mi><mo>-</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><mo>+</mo><mi>R</mi><msub><mrow></mrow><mi>a</mi></msub><mo>&lt;</mo><mi>t</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mo stretchy="false">(</mo><mi>g</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>+</mo><mi>d</mi><msub><mrow></mrow><mi>i</mi></msub><mo>&lt;</mo><mi>t</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">)</mo></mrow></msup></mtd></mtr></mtable></mrow></munder><mi>ε</mi></mstyle><mo stretchy="false">(</mo><mi>t</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">)</mo></mrow></msup><mo>-</mo><mi>t</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mo stretchy="false">(</mo><mi>g</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>-</mo><mi>d</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo>⋅</mo><mrow><mo>(</mo><mrow><mfrac><mn>1</mn><mi>τ</mi></mfrac><mo>-</mo><mfrac><mn>1</mn><mrow><mi>t</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">)</mo></mrow></msup><mo>-</mo><mi>t</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mo stretchy="false">(</mo><mi>g</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>-</mo><mi>d</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></mfrac></mrow><mo>)</mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>0</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="83">∂<i>t</i><sup>(</sup><i>f</i>)/∂<i>u</i>(<i>t</i><sup>(</sup><i>f</i>))和∂<i>u</i>(<i>t</i><sup>(</sup><i>f</i>))/∂<i>t</i><sup>(</sup><i>f</i>-1)的计算方法如下:</p>
                </div>
                <div class="p1">
                    <p id="84" class="code-formula">
                        <mathml id="84"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mfrac><mrow><mo>∂</mo><mi>t</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">)</mo></mrow></msup></mrow><mrow><mo>∂</mo><mi>u</mi><mo stretchy="false">(</mo><mi>t</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo></mrow></mfrac><mo>=</mo><mfrac><mrow><mo>-</mo><mn>1</mn></mrow><mrow><mo>∂</mo><mi>u</mi><mo stretchy="false">(</mo><mi>t</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo><mo>/</mo><mo>∂</mo><mi>t</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">)</mo></mrow></msup></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>1</mn><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mfrac><mrow><mo>∂</mo><mi>u</mi><mo stretchy="false">(</mo><mi>t</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo></mrow><mrow><mo>∂</mo><mi>t</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">)</mo></mrow></msup></mrow></mfrac><mo>=</mo></mtd></mtr><mtr><mtd><mrow><mo>{</mo><mrow><mtable><mtr><mtd columnalign="left"><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mrow></mrow></mstyle></mrow><mtext> </mtext><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mtable><mtr><mtd><mi>t</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mo stretchy="false">(</mo><mi>g</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>∈</mo><mi>G</mi><msub><mrow></mrow><mi>i</mi></msub></mtd></mtr><mtr><mtd><mi>t</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mo stretchy="false">(</mo><mi>g</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>+</mo><mi>d</mi><msub><mrow></mrow><mi>i</mi></msub><mo>&lt;</mo><mi>t</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup></mtd></mtr></mtable></mrow></munder><mrow></mrow></mstyle></mrow><mi>w</mi><msub><mrow></mrow><mi>i</mi></msub><mi>ε</mi><mo stretchy="false">(</mo><mi>t</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><mo>-</mo><mi>t</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mo stretchy="false">(</mo><mi>g</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>-</mo><mi>d</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo>×</mo></mtd></mtr><mtr><mtd columnalign="left"><mtext> </mtext><mtext> </mtext><mspace width="0.25em" /><mrow><mo>(</mo><mrow><mfrac><mn>1</mn><mrow><mi>t</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><mo>-</mo><mi>t</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mo stretchy="false">(</mo><mi>g</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>-</mo><mi>d</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></mfrac><mo>-</mo><mfrac><mn>1</mn><mi>τ</mi></mfrac></mrow><mo>)</mo></mrow><mo>,</mo><mi>f</mi><mo>=</mo><mn>1</mn></mtd></mtr><mtr><mtd columnalign="left"><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mrow></mrow></mstyle></mrow><mtext> </mtext><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mtable><mtr><mtd><mi>t</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mo stretchy="false">(</mo><mi>g</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>∈</mo><mi>G</mi><msub><mrow></mrow><mi>i</mi></msub></mtd></mtr><mtr><mtd><mi>t</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>f</mi><mo>-</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><mo>+</mo><mi>R</mi><msub><mrow></mrow><mi>a</mi></msub><mo>&lt;</mo><mi>t</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mo stretchy="false">(</mo><mi>g</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>+</mo><mi>d</mi><msub><mrow></mrow><mi>i</mi></msub><mo>&lt;</mo><mi>t</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">)</mo></mrow></msup></mtd></mtr></mtable></mrow></munder><mrow></mrow></mstyle></mrow><mi>w</mi><msub><mrow></mrow><mi>i</mi></msub><mi>ε</mi><mo stretchy="false">(</mo><mi>t</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">)</mo></mrow></msup><mo>-</mo><mi>t</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mo stretchy="false">(</mo><mi>g</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>-</mo><mi>d</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo>×</mo></mtd></mtr><mtr><mtd columnalign="left"><mtext> </mtext><mrow><mo>(</mo><mrow><mfrac><mn>1</mn><mrow><mi>t</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">)</mo></mrow></msup><mo>-</mo><mi>t</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mo stretchy="false">(</mo><mi>g</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>-</mo><mi>d</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></mfrac><mo>-</mo><mfrac><mn>1</mn><mi>τ</mi></mfrac></mrow><mo>)</mo></mrow><mo>-</mo><mfrac><mn>1</mn><mrow><mi>τ</mi><msub><mrow></mrow><mi>R</mi></msub></mrow></mfrac><mi>η</mi><mo stretchy="false">(</mo><mi>t</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">)</mo></mrow></msup><mo>-</mo><mi>t</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>f</mi><mo>-</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo><mo>,</mo><mtext> </mtext><mi>f</mi><mo>&gt;</mo><mn>1</mn></mtd></mtr></mtable></mrow></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>2</mn><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mfrac><mrow><mo>∂</mo><mi>u</mi><mo stretchy="false">(</mo><mi>t</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo></mrow><mrow><mo>∂</mo><mi>t</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>f</mi><mo>-</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup></mrow></mfrac><mo>=</mo><mi>η</mi><mo stretchy="false">(</mo><mi>t</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">)</mo></mrow></msup><mo>-</mo><mi>t</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>f</mi><mo>-</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo><mo>/</mo><mi>τ</mi><msub><mrow></mrow><mi>R</mi></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>3</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="85">对于在线学习方式,一旦神经元激发出一个输出脉冲时就立刻调整相应的权值和延迟。当神经元在<i>t</i><sup>(</sup><i>f</i>)时刻激发出第<i>f</i>个脉冲且相对应的第<i>f</i>个期望脉冲为<mathml id="121"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi>t</mi></mstyle><mrow><mspace width="0.25em" /><mo>^</mo></mrow></mover><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">)</mo></mrow></msup></mrow></math></mathml>,延迟调整如式(14)所示,其余与离线方式相同。延迟的调整幅度计算公式如下:</p>
                </div>
                <div class="p1">
                    <p id="86" class="code-formula">
                        <mathml id="86"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mtext>Δ</mtext><mi>d</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mo>-</mo><mi>α</mi><msub><mrow></mrow><mi>d</mi></msub><mfrac><mrow><mo>∂</mo><mi>E</mi></mrow><mrow><mo>∂</mo><mi>d</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></mfrac><mo>=</mo><mo>-</mo><mfrac><mrow><mi>α</mi><msub><mrow></mrow><mi>d</mi></msub></mrow><mn>2</mn></mfrac><mfrac><mrow><mo>∂</mo><mo stretchy="false">(</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">)</mo></mrow></msup><mo>-</mo><mover><mstyle mathsize="140%" displaystyle="true"><mi>t</mi></mstyle><mrow><mspace width="0.25em" /><mo>^</mo></mrow></mover><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo></mrow><msup><mrow></mrow><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><mrow><mo>∂</mo><mi>d</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></mfrac><mo>=</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mo>-</mo><mfrac><mrow><mi>α</mi><msub><mrow></mrow><mi>d</mi></msub></mrow><mn>2</mn></mfrac><mfrac><mrow><mo>∂</mo><mo stretchy="false">(</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">)</mo></mrow></msup><mo>-</mo><mover><mstyle mathsize="140%" displaystyle="true"><mi>t</mi></mstyle><mrow><mspace width="0.25em" /><mo>^</mo></mrow></mover><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo></mrow><msup><mrow></mrow><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><mrow><mo>∂</mo><mi>t</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">)</mo></mrow></msup></mrow></mfrac><mfrac><mrow><mo>∂</mo><mi>t</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">)</mo></mrow></msup></mrow><mrow><mo>∂</mo><mi>d</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></mfrac><mo>=</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mi>α</mi><msub><mrow></mrow><mi>d</mi></msub><mo stretchy="false">(</mo><mover><mstyle mathsize="140%" displaystyle="true"><mi>t</mi></mstyle><mrow><mspace width="0.25em" /><mo>^</mo></mrow></mover><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">)</mo></mrow></msup><mo>-</mo><mi>t</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo><mfrac><mrow><mo>∂</mo><mi>t</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">)</mo></mrow></msup></mrow><mrow><mo>∂</mo><mi>d</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>4</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="87">在生物神经系统中,神经元的延迟均为正数,通过延迟梯度下降调整后,部分突触的延迟可能出现负值,与生物学基础并不相符,有2种解决方案:一种是在延迟调整过程中加入非负限制,当延迟调整使该突触的延迟小于0时,则取消此次延迟调整以保证延迟的非负性,但在每一轮的学习过程中只能调整部分延迟,学习效果会受到影响;另外一种是在模型中忽略该限制,允许负值延迟,与突触性质在模拟脉冲神经网络模型中的改变类似,生物神经元的输入突触有激励突触和抑制突触两类(正值权值和负值权值),且在运行过程中其类型不会发生变化。而现有研究中多数采用可变的突触权值,以提高脉冲神经网络的学习性能。突触的时间延迟被认为是传统神经网络中的偏置,正值延迟表示该脉冲输入要延后到达,负值延迟表示该输入要提前到达。基于此,为获取更好的学习能力,对神经元的延迟性质进行修改。在延迟梯度下降调整后,神经元突触的延迟不是离散的整数值,而是更为精确的小数值,这使神经元的输入脉冲到达时间不再是整数的时间点。由于脉冲神经元的运行时间连续,为便于计算机模拟,人为将神经元运行时间离散化,一般取精度为1 ms,这并不违背生物神经元的行为特征。另外,在延迟梯度下降时,可以直接利用权值梯度下降的中间结果,因此该算法没有大幅增加算法的复杂度。在下文中,MSGDB-D1代表无正负限制的带延迟的梯度下降算法,MSGDB-D2代表非负带延迟的梯度下降算法。</p>
                </div>
                <h3 id="88" name="88" class="anchor-tag">4 实验结果与分析</h3>
                <div class="p1">
                    <p id="89">为验证本文提出的带延迟学习的梯度下降算法,通过实验验证在不同输入脉冲和不同期望脉冲序列下算法的学习效果。部分参数设置为:<i>τ</i>=7 ms,<i>τ</i><sub><i>R</i></sub>=80 ms,ϑ=1,<i>R</i><sub><i>a</i></sub>=1 ms,神经元的输入和输出分别为给定激发频率的Poisson过程产生的脉冲序列,<i>F</i><sub>in</sub>和<i>F</i><sub>out</sub>分别代表脉冲序列的输入和输出激发频率,<i>r</i><sub><i>w</i></sub>和<i>r</i><sub><i>d</i></sub>代表权值和延迟的学习速率,<i>T</i><sub><i>t</i></sub>代表神经元的运行时间。具体实验描述如下:</p>
                </div>
                <div class="p1">
                    <p id="90">1)验证在线和离线学习方式的区别。神经元有200个输入突触,<i>T</i><sub><i>t</i></sub>=200 ms,<i>F</i><sub>in</sub>=5 Hz,<i>F</i><sub>out</sub>=80 Hz,<i>r</i><sub><i>w</i></sub>=0.2×10<sup>-2</sup>,<i>r</i><sub><i>d</i></sub>=1,突触的初始权值为均匀分布于区间(0,0.2)中的随机数。图1、图2给出MSGDB和MSGDB-D1这2种算法在离线方式和在线方式下的学习精度对比结果。</p>
                </div>
                <div class="area_img" id="92">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201909027_092.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTg2cEJVZU4zZUYvSE5NMVVZMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 离线方式下2种算法的学习精度对比结果" src="Detail/GetImg?filename=images/JSJC201909027_092.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTg2cEJVZU4zZUYvSE5NMVVZMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图1 离线方式下2种算法的学习精度对比结果</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201909027_092.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTg2cEJVZU4zZUYvSE5NMVVZMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="93">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201909027_093.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTg2cEJVZU4zZUYvSE5NMVVZMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 在线方式下2种算法的学习精度对比结果" src="Detail/GetImg?filename=images/JSJC201909027_093.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTg2cEJVZU4zZUYvSE5NMVVZMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图2 在线方式下2种算法的学习精度对比结果</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201909027_093.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTg2cEJVZU4zZUYvSE5NMVVZMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="94">从图1、图2可以看出,在线学习方式下的MSGDB-D1算法仅需50步就学会了目标序列,而离线方式下2种算法需要更多的步数,在线方式明显优于离线方式。无论离线还是在线学习,带延迟学习的梯度下降学习算法均优于不带延迟的学习算法。因此,下文实验均为在线学习方式。</p>
                </div>
                <div class="p1">
                    <p id="95">2)验证非负延迟限制对算法的影响。神经元的输入突触个数为200,<i>T</i><sub><i>t</i></sub>=100 ms,<i>F</i><sub>out</sub>=100 Hz,神经元的输入脉冲在运行时间内均匀分布,突触的初始权值为均匀分布在区间(0,0.45)中的随机数,这种初始权值的设置使初始神经元激发脉冲的个数与期望激发脉冲个数相似。图3给出MSGDB算法及2种带延迟调整的梯度下降算法的精度对比结果。可以看出,2种带延迟调整的算法精度均明显高于MSGDB算法。其中MSGDB-D1算法只需30步就收敛于精度1,优于MSGDB-D2算法。</p>
                </div>
                <div class="area_img" id="96">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201909027_096.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTg2cEJVZU4zZUYvSE5NMVVZMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 3种算法精度对比结果1" src="Detail/GetImg?filename=images/JSJC201909027_096.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTg2cEJVZU4zZUYvSE5NMVVZMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图3 3种算法精度对比结果1</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201909027_096.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTg2cEJVZU4zZUYvSE5NMVVZMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="97">图4和图5分别显示在学习结束时2种带延迟调整算法的延迟大小。其中,权值调整速率<i>r</i><sub><i>w</i></sub>=0.03,延迟调整速率<i>r</i><sub><i>d</i></sub>=1。从图4可以看出,MSGDB-D1算法的延迟有正有负,对于神经元行为,正值延迟表示该算法尽可能降低该突触输入对神经元的影响,负值延迟说明算法尽可能增大该突触输入的作用。从图5可以看出,MSGDB-D2算法限制延迟为非负,导致有些突触在学习结束时的延迟变得非常大,这在一定程度上破坏了梯度下降的结果,因此效果差于MSGDB-D1算法。</p>
                </div>
                <div class="area_img" id="98">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201909027_098.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTg2cEJVZU4zZUYvSE5NMVVZMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 MSGDB-D1算法学习结束时的突触延迟时间1" src="Detail/GetImg?filename=images/JSJC201909027_098.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTg2cEJVZU4zZUYvSE5NMVVZMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图4 MSGDB-D1算法学习结束时的突触延迟时间1</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201909027_098.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTg2cEJVZU4zZUYvSE5NMVVZMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="99">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201909027_099.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTg2cEJVZU4zZUYvSE5NMVVZMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 MSGDB-D2算法学习结束时的突触延迟时间1" src="Detail/GetImg?filename=images/JSJC201909027_099.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTg2cEJVZU4zZUYvSE5NMVVZMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图5 MSGDB-D2算法学习结束时的突触延迟时间1</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201909027_099.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTg2cEJVZU4zZUYvSE5NMVVZMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="100">3)验证当期望输出序列中脉冲个数与神经元初始输出中脉冲个数相差较大时,延迟调整是否仍然能有效提高学习效果。这组实验中包含2个子实验:第1个将初始权值设为(0,0.15)中的随机数,第2个设为(0,1)中的随机数,其余参数与第2组实验相同。这2种初始权值的设置可以分别使神经元初始的激发脉冲个数约为期望输出的一半和两倍左右。图6给出3种梯度下降算法在第1个子实验中精度对比结果。图7、图8分别为MSGDB-D1算法和MSGDB-D2算法在第1个子实验中学习结束时突触延迟。图9给出3种梯度下降算法在第2个子实验中精度对比结果。图10、图11分别为MSGDB-D1算法和MSGDB-D2算法在第2个子实验中学习结束时突触延迟。可以看出,在初始激发个数多于或少于期望激发个数的情况下,2种带延迟的梯度下降算法均能大幅提高学习效果。在图9中,2种带延迟的梯度下降算法的结果非常接近,MSGDB-D1算法在15步时收敛到精度1,而MSGDB-D2算法需要18步收敛到精度1。观察2种算法结束时的延迟,可以看出,MSGDB-D1算法结束时大部分延迟均为正数,只有少部分为负值,说明期望脉冲输出大部分在神经元实际输出,这是导致MSGDB-D1算法和MSGDB-D2两种算法的精度非常接近的主要原因。</p>
                </div>
                <div class="area_img" id="103">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201909027_103.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTg2cEJVZU4zZUYvSE5NMVVZMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 3种算法精度对比结果2" src="Detail/GetImg?filename=images/JSJC201909027_103.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTg2cEJVZU4zZUYvSE5NMVVZMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图6 3种算法精度对比结果2</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201909027_103.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTg2cEJVZU4zZUYvSE5NMVVZMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="104">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201909027_104.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTg2cEJVZU4zZUYvSE5NMVVZMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 MSGDB-D1算法学习结束时突触延迟时间2" src="Detail/GetImg?filename=images/JSJC201909027_104.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTg2cEJVZU4zZUYvSE5NMVVZMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图7 MSGDB-D1算法学习结束时突触延迟时间2</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201909027_104.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTg2cEJVZU4zZUYvSE5NMVVZMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="105">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201909027_105.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTg2cEJVZU4zZUYvSE5NMVVZMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 MSGDB-D2算法学习结束时突触延迟时间2" src="Detail/GetImg?filename=images/JSJC201909027_105.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTg2cEJVZU4zZUYvSE5NMVVZMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图8 MSGDB-D2算法学习结束时突触延迟时间2</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201909027_105.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTg2cEJVZU4zZUYvSE5NMVVZMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="106">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201909027_106.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTg2cEJVZU4zZUYvSE5NMVVZMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图9 3种算法精度对比结果3" src="Detail/GetImg?filename=images/JSJC201909027_106.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTg2cEJVZU4zZUYvSE5NMVVZMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图9 3种算法精度对比结果3</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201909027_106.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTg2cEJVZU4zZUYvSE5NMVVZMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="107">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201909027_107.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTg2cEJVZU4zZUYvSE5NMVVZMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图10 MSGDB-D1算法学习结束时突触延迟时间3" src="Detail/GetImg?filename=images/JSJC201909027_107.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTg2cEJVZU4zZUYvSE5NMVVZMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图10 MSGDB-D1算法学习结束时突触延迟时间3</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201909027_107.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTg2cEJVZU4zZUYvSE5NMVVZMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="108">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201909027_108.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTg2cEJVZU4zZUYvSE5NMVVZMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图11 MSGDB-D2算法学习结束时突触延迟时间3" src="Detail/GetImg?filename=images/JSJC201909027_108.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTg2cEJVZU4zZUYvSE5NMVVZMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图11 MSGDB-D2算法学习结束时突触延迟时间3</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201909027_108.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTg2cEJVZU4zZUYvSE5NMVVZMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="109">第3组实验较前两组实验更为复杂,将神经元突触个数增加到400个,<i>T</i><sub><i>t</i></sub>=400 ms,<i>F</i><sub>in</sub>=20 Hz,<i>F</i><sub>out</sub>=60 Hz,<i>r</i><sub><i>w</i></sub>=1.5×10<sup>-4</sup>,<i>r</i><sub><i>d</i></sub>=0.3。初始权值设为区间(0,0.05)的均匀分布随机数,结果如图12～图14所示,可以看出,当序列变长且输入为多脉冲时,原始的梯度下降算法精度明显下降,在700步时仍只能达到0.75左右的精度,然而MSGDB-D2算法可以达到0.82,MSGDB-D1算法最高达到0.93。这说明在学习任务更为复杂的情况下,带延迟的梯度下降法仍然可以达到较好的效果。由于输入突触个数比较多,因此采用较小的学习速率,可以降低各输出脉冲之间的相互干扰。</p>
                </div>
                <div class="area_img" id="110">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201909027_110.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTg2cEJVZU4zZUYvSE5NMVVZMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图12 3种算法精度对比结果4" src="Detail/GetImg?filename=images/JSJC201909027_110.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTg2cEJVZU4zZUYvSE5NMVVZMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图12 3种算法精度对比结果4</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201909027_110.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTg2cEJVZU4zZUYvSE5NMVVZMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="111">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201909027_111.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTg2cEJVZU4zZUYvSE5NMVVZMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图13 MSGDB-D1算法学习结束时突触延迟时间4" src="Detail/GetImg?filename=images/JSJC201909027_111.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTg2cEJVZU4zZUYvSE5NMVVZMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图13 MSGDB-D1算法学习结束时突触延迟时间4</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201909027_111.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTg2cEJVZU4zZUYvSE5NMVVZMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="112">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201909027_112.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTg2cEJVZU4zZUYvSE5NMVVZMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图14 MSGDB-D2算法学习结束时突触延迟时间4" src="Detail/GetImg?filename=images/JSJC201909027_112.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTg2cEJVZU4zZUYvSE5NMVVZMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图14 MSGDB-D2算法学习结束时突触延迟时间4</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201909027_112.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTg2cEJVZU4zZUYvSE5NMVVZMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h3 id="113" name="113" class="anchor-tag">5 结束语</h3>
                <div class="p1">
                    <p id="114">本文提出一种带延迟学习的梯度下降脉冲神经元算法,在每一轮学习过程中,通过梯度下降算法调整神经元的输入延迟和权值。在离线学习和在线学习方式中,利用权值调整部分中间结果。实验结果表明,该算法能大幅提升梯度下降的学习精度,收敛速度较高,且MSGDB-D1算法的学习精度高于MSGDB-D2算法。下一步可将延迟学习应用于脉冲神经网络,以降低算法的复杂度。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="143">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Introduction to spiking neural networks: Information processing, learning and applications">

                                <b>[1]</b> PONULAK F,KASINSKI A.Introduction to spiking neural networks:information processing,learning and applications [J].Acta Neurobiologiae Experimentalis,2011,71(4):409-433.
                            </a>
                        </p>
                        <p id="145">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501914521&amp;v=MDU3MTRmT2ZiSzdIdEROcW85RWJlb0xDWDQ0b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VTHJJSjFvU2FCbz1OaQ==&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTg2cEJVZU4zZUYvSE5NMVVZMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> BOHTE S M,KOK J N,LA POUTR’E J A.Error-backpropagation in temporally encoded networks of spiking neurons[J].Neurocomputing,2002,48(1/4):17-37.
                            </a>
                        </p>
                        <p id="147">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fast modifications of the SpikeProp algorithm">

                                <b>[3]</b> MCKENNOCH S,LIU D,BUSHNELL L G.Fast modifications of the spikeprop algorithm [C]//Proceedings of International Joint Conference on Neural Networks.Washington D.C.,USA:IEEE Press,2006:3970-3977.
                            </a>
                        </p>
                        <p id="149">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Application of LevenbergMarquardt method to the training of spiking neural networks">

                                <b>[4]</b> SILVA S M,RUANO A E.Application of levenberg-marquardt method to the training of spiking neural networks[C]//Proceedings of the International Conference on Neural Networks and Brain.Washington D.C.,USA:IEEE Press,2005:1354-1358.
                            </a>
                        </p>
                        <p id="151">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Extending spike-prop">

                                <b>[5]</b> SCHRAUWEN B,CAMPENHOUT J V.Extending spike-prop[C]//Proceedings of International Joint Conference on Neural Networks.Washington D.C.,USA:IEEE Press,2004:471-476.
                            </a>
                        </p>
                        <p id="153">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011601422048&amp;v=MTc1NzVQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVUxySUoxb1NhQm89TmlmT2ZiSzdIdEROcVk5RVlPa05ESGd4b0JNVDZUNA==&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTg2cEJVZU4zZUYvSE5NMVVZMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b> BOOIJ O,NGUYEN H T.A gradient descent rule for multiple spiking neurons emitting multiple spikes[J].Information Processing Letters,2005,95(6):552-558.
                            </a>
                        </p>
                        <p id="155">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13050900043406&amp;v=MTYxMDRVTHJJSjFvU2FCbz1OaWZPZmJLN0h0VE1wbzlGWk84TUNIdy9vQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbQ==&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTg2cEJVZU4zZUYvSE5NMVVZMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b> XU Yan,ZENG Xiaoqin,HAN Lixin,et al.A supervised multi-spike learning algorithm based on gradient descent for spiking neural networks[J].Neural Networks,2013,43(4):99-113.
                            </a>
                        </p>
                        <p id="157">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201512029&amp;v=MjI0ODhaZVJyRnkvaFZiM0FMejdCYmJHNEg5VE5yWTlIYllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2U=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTg2cEJVZU4zZUYvSE5NMVVZMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b> 徐彦.基于梯度下降的脉冲神经元在线学习方法[J].计算机工程,2015,41(12):150-155,160.
                            </a>
                        </p>
                        <p id="159">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESB3B9362AB34B38D6DC3A58C2C625E162&amp;v=Mjg1NzdyN0pJRnBtYUJ1SFlmT0dRbGZDcGJRMzVOMWh3N3k1d3FBPU5pZk9mY0c3Yk5qUHFZMDBGdWdMZm44eHV4Qm5tVHdNVFhlUnJtRXplN2ZoUkx5ZENPTnZGU2lXVw==&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTg2cEJVZU4zZUYvSE5NMVVZMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b> XU Yan,YANG Jing,ZHONG Shuiming.An online supervised learning method based on gradient descent for spiking neurons[J].Neural Networks,2017,93:7-20.
                            </a>
                        </p>
                        <p id="161">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The chronotron:a neuron that learns to fire temporally precise spike patterns">

                                <b>[10]</b> FLORIAN R V.The chronotron:a neuron that learns to fire temporally precise spike patterns [EB/OL].[2018-05-21].https://www.ncbi.nlm.nih.gov/pubmed/22879876.
                            </a>
                        </p>
                        <p id="163">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Nature and precision of temporal coding in visual cortex: a metric-space analysis">

                                <b>[11]</b> VICTOR J D,PURPURA K P.Nature and precision of temporal coding in visual cortex:a metric-space analysis[J].Journal of Neurophysiology,1996,76(2):1310-1326.
                            </a>
                        </p>
                        <p id="165">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Redistribution of synaptic efficacy between neocortical pyramidal neurons">

                                <b>[12]</b> MARKRAM H,TSODYKS M.Redistribution of synaptic efficacy between neocortical pyramidal neurons[J].Nature,1996,382(6594):807-810.
                            </a>
                        </p>
                        <p id="167">
                            <a id="bibliography_13" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJBK&amp;filename=SJBK15090500013038&amp;v=MjEzNTZaYks5SHRqTXFvOUZaT29NREg4eG9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVUxySUoxb1NhQm89TmlmSg==&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTg2cEJVZU4zZUYvSE5NMVVZMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[13]</b> PONULAK F,KASINSKI A.Supervised learning in spiking neural networks with ReSuMe:sequence learning,classification and spike shifting[J].Neural Computation,2010,22(2):467-510.
                            </a>
                        </p>
                        <p id="169">
                            <a id="bibliography_14" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201511035&amp;v=MjgwOTlLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUnJGeS9oVmIzQUx6N0JiYkc0SDlUTnJvOUdZWVE=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTg2cEJVZU4zZUYvSE5NMVVZMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[14]</b> 徐彦,杨静.脉冲神经元序列学习方法的影响因素研究[J].计算机工程,2015,41(11):194-201.
                            </a>
                        </p>
                        <p id="171">
                            <a id="bibliography_15" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJBK&amp;filename=SJBK15090500013411&amp;v=MTQzNDNTYUJvPU5pZkpaYks5SHRqTXFvOUZaT29NQ0gwNG9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVUxySUoxbw==&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTg2cEJVZU4zZUYvSE5NMVVZMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[15]</b> XU Yan,ZENG Xiaoqin,ZHONG Shuiming.A new supervised learning algorithm for spiking neurons[J].Neural Computation,2013,25(6):1472-1511.
                            </a>
                        </p>
                        <p id="173">
                            <a id="bibliography_16" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501913275&amp;v=MDM2MDM9TmlmT2ZiSzdIdEROcW85RWJlb01EbnM4b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VTHJJSjFvU2FCbw==&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTg2cEJVZU4zZUYvSE5NMVVZMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[16]</b> MOHEMMED A,SCHLIEBS S,MATSUDA S,et al.Training spiking neural networks to associate spatio-temporal input-output spike patterns[J].Neurocomputing,2013,107(4):3-10.
                            </a>
                        </p>
                        <p id="175">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Precise-spike-driven synaptic plasticity:learning hetero-association of spatiotemporal spike patterns">

                                <b>[17]</b> YU Qiang,TANG Huajin,TAN K C,et al.Precise-spike-driven synaptic plasticity:learning hetero-association of spatiotemporal spike patterns [EB/OL].[2018-05-21].https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0078318.
                            </a>
                        </p>
                        <p id="177">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=An online supervised learning algorithm based on nonlinear spike train kernels">

                                <b>[18]</b> LIN Xianghong,ZHANG Ning,WANG Xiangwen.An online supervised learning algorithm based on nonlinear spike train kernels [C]//Proceedings of International Conference on Intelligent Computing.Berlin,Germany:Springer,2015:106-115.
                            </a>
                        </p>
                        <p id="179">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Supervised learning in spiking neural networks for precise temporal encoding">

                                <b>[19]</b> GARDNER B,GRÜNING A.Supervised learning in spiking neural networks for precise temporal encoding[EB/OL].[2018-05-21].https://www.ncbi.nlm.nih.gov/pubmed/27532262.
                            </a>
                        </p>
                        <p id="181">
                            <a id="bibliography_20" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201806002&amp;v=MTEwOTgzQUx6N0JkN0c0SDluTXFZOUZab1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUnJGeS9oVmI=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTg2cEJVZU4zZUYvSE5NMVVZMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[20]</b> 徐彦,熊迎军,杨静.脉冲神经元脉冲序列学习方法综述[J].计算机应用,2018,38(6):1527-1534..
                            </a>
                        </p>
                        <p id="183">
                            <a id="bibliography_21" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD130409020623&amp;v=MzE2MDN1RkN2Z1U3ak1KbHdjTmo3QmFySzdIdFhNcG85SFpPME5EeE04enhVU21EZDlTSDduM3hFOWZidm5LcmlmWnU5&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTg2cEJVZU4zZUYvSE5NMVVZMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[21]</b> XU Bo,GONG Yubing,WANG Baoying.Delay-induced firing behavior and transitions in adaptive neuronal networks with two types of synapses[J].Science China Chemistry,2013,56 (2):222-229.
                            </a>
                        </p>
                        <p id="185">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Extending spikeprop">

                                <b>[22]</b> SCHRAUWEN B,CAMPENHOUT J V.Extending SpikeProp [C]//Proceedings of IEEE International Joint Conference on Neural Network.Washington D.C.,USA:IEEE Press,2004:471-475.
                            </a>
                        </p>
                        <p id="187">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=DL-ReSuMe:a delay learning-based remote supervised method for spiking neurons">

                                <b>[23]</b> TAHERKHANI A,BELATRECHE A,LI Yuhua,et al.DL-ReSuMe:a delay learning-based remote supervised method for spiking neurons[C]//Proceedings of International Conference on Neural Information Processing.Berlin,Germany:Springer,2015:190-197.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJC201909027" />
        <input id="dpi" type="hidden" value="600" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTg2cEJVZU4zZUYvSE5NMVVZMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201909027&amp;v=MDkyNThRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJyRnkvaFZiM0FMejdCYmJHNEg5ak1wbzlIWTQ=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTg2cEJVZU4zZUYvSE5NMVVZMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
