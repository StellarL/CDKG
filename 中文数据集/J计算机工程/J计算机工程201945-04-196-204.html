<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637130603999962500%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJC201904033%26RESULT%3d1%26SIGN%3dhjaYQ%252bxqNSVccVxq8ZThRVXqQ2M%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201904033&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201904033&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201904033&amp;v=MTYzNTMzenFxQnRHRnJDVVJMT2VaZVJvRnkvbFc3dk1MejdCYmJHNEg5ak1xNDlHWjRRS0RIODR2UjRUNmo1NE8=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#51" data-title="0 概述 ">0 概述</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#55" data-title="1 基于W-HHT的EEG特征提取 ">1 基于W-HHT的EEG特征提取</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#58" data-title="1.1 特征提取">1.1 特征提取</a></li>
                                                <li><a href="#111" data-title="1.2 训练与识别">1.2 训练与识别</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#141" data-title="2 实验与结果分析 ">2 实验与结果分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#142" data-title="2.1 &lt;i&gt;EEG&lt;/i&gt;情感数据的提取">2.1 <i>EEG</i>情感数据的提取</a></li>
                                                <li><a href="#152" data-title="2.2 结果分析">2.2 结果分析</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#166" data-title="3 结束语 ">3 结束语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#57" data-title="&lt;b&gt;图1 基于EEG的情感识别总体流程&lt;/b&gt;"><b>图1 基于EEG的情感识别总体流程</b></a></li>
                                                <li><a href="#63" data-title="&lt;b&gt;图2 EEG预处理流程&lt;/b&gt;"><b>图2 EEG预处理流程</b></a></li>
                                                <li><a href="#70" data-title="&lt;b&gt;图3 上包络线和下包络线&lt;/b&gt;"><b>图3 上包络线和下包络线</b></a></li>
                                                <li><a href="#79" data-title="&lt;b&gt;图4 EMD分解得到的IMF&lt;/b&gt;"><b>图4 EMD分解得到的IMF</b></a></li>
                                                <li><a href="#104" data-title="&lt;b&gt;图5 Hilbert谱图&lt;/b&gt;"><b>图5 Hilbert谱图</b></a></li>
                                                <li><a href="#107" data-title="&lt;b&gt;图6 Hilbert能量谱&lt;/b&gt;"><b>图6 Hilbert能量谱</b></a></li>
                                                <li><a href="#114" data-title="&lt;b&gt;图7 C-R-SVM混合情感识别模型&lt;/b&gt;"><b>图7 C-R-SVM混合情感识别模型</b></a></li>
                                                <li><a href="#121" data-title="&lt;b&gt;图8 LSTM结构&lt;/b&gt;"><b>图8 LSTM结构</b></a></li>
                                                <li><a href="#144" data-title="&lt;b&gt;图9 Emotiv电极位置&lt;/b&gt;"><b>图9 Emotiv电极位置</b></a></li>
                                                <li><a href="#149" data-title="&lt;b&gt;图10 EEG情感诱发流程&lt;/b&gt;"><b>图10 EEG情感诱发流程</b></a></li>
                                                <li><a href="#154" data-title="&lt;b&gt;图11 各频带能量地形图&lt;/b&gt;"><b>图11 各频带能量地形图</b></a></li>
                                                <li><a href="#155" data-title="&lt;b&gt;图12 各频带的瞬时能量&lt;/b&gt;"><b>图12 各频带的瞬时能量</b></a></li>
                                                <li><a href="#157" data-title="&lt;b&gt;图13 同一实验不同分类器的混淆矩阵&lt;/b&gt;"><b>图13 同一实验不同分类器的混淆矩阵</b></a></li>
                                                <li><a href="#158" data-title="&lt;b&gt;表1 不同情感状态在不同频带下的平均识别率&lt;/b&gt;"><b>表1 不同情感状态在不同频带下的平均识别率</b></a></li>
                                                <li><a href="#160" data-title="&lt;b&gt;图14 不同通道位置的选择&lt;/b&gt;"><b>图14 不同通道位置的选择</b></a></li>
                                                <li><a href="#162" data-title="&lt;b&gt;图15 不同分类器&lt;i&gt;γ&lt;/i&gt;频带的平均识别率&lt;/b&gt;"><b>图15 不同分类器<i>γ</i>频带的平均识别率</b></a></li>
                                                <li><a href="#164" data-title="&lt;b&gt;表2 不同方法识别率比较&lt;/b&gt;"><b>表2 不同方法识别率比较</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" title=" CHU S, NARAYANAN S, KUO C C J.Environmental sound recognition using MP-based features[C]//Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing.Washington D.C., USA:IEEE Press, 2008:1-4." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Environmental sound recognition using MP-based features">
                                        <b>[1]</b>
                                         CHU S, NARAYANAN S, KUO C C J.Environmental sound recognition using MP-based features[C]//Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing.Washington D.C., USA:IEEE Press, 2008:1-4.
                                    </a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" title=" CANNON W B.The James-Lange theory of emotions:a critical examination and an alternative theory[J].The American Journal of Psychology, 1987, 100 (3/4) :567-586." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJST&amp;filename=SJSTD6F4115A3957E02901E45BFAF8F7926F&amp;v=MDk4NjErSUtDd2s1elI4VDYwcDVUUTJVM1dROUQ3V2RSN3pwQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQ3BiUTM1TjVodzdpM3hLdz1OaWZZZXNlK2FOWE5yb28wWg==&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         CANNON W B.The James-Lange theory of emotions:a critical examination and an alternative theory[J].The American Journal of Psychology, 1987, 100 (3/4) :567-586.
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title=" 陆文娟.基于脑电信号的情感识别研究[D].南京:南京邮电大学, 2017." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=1017859112.nh&amp;v=MTcwMjJDVVJMT2VaZVJvRnkvbFc3dk1WRjI2R2J1OUY5RE5yWkViUElRS0RIODR2UjRUNmo1NE8zenFxQnRHRnI=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         陆文娟.基于脑电信号的情感识别研究[D].南京:南京邮电大学, 2017.
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" title=" BAJAJ V, PACHORI R B.Detection of human emotions using features based on the multiwavelet transform of EEG signals[M].Berlin, Germany:Springer, 2015:215-240." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Detection of human emotions using features based on the multiwavelet transform of EEG signals">
                                        <b>[4]</b>
                                         BAJAJ V, PACHORI R B.Detection of human emotions using features based on the multiwavelet transform of EEG signals[M].Berlin, Germany:Springer, 2015:215-240.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" title=" ARNAU-GONZ&#193;LEZ P, AREVALILLO-HERR&#193;EZ M, RAMZAN N.Fusing highly dimensional energy and connectivity features to identify affective states from EEG signals[J].Neurocomputing, 2017, 244:81-89." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESEFA829C6BAC61CA72A1ED921C81D7469&amp;v=MTcxNzVtYUJ1SFlmT0dRbGZDcGJRMzVONWh3N2kzeEt3PU5pZk9mY2JPYjluT3B2eERGcHA4Q24xS3ZoRVJtejRJUEhiZ3JXRTllTWFUUWJ5V0NPTnZGU2lXV3I3SklGcA==&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                         ARNAU-GONZ&#193;LEZ P, AREVALILLO-HERR&#193;EZ M, RAMZAN N.Fusing highly dimensional energy and connectivity features to identify affective states from EEG signals[J].Neurocomputing, 2017, 244:81-89.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" title=" LIN Y P, WANG C H, JUNG T P.EEG-based emotion recognition in music listening[J].IEEE Transactions on Biomedical Engineering, 2010, 57 (7) :1798-1806." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=EEG-based emotion recognition in music listening">
                                        <b>[6]</b>
                                         LIN Y P, WANG C H, JUNG T P.EEG-based emotion recognition in music listening[J].IEEE Transactions on Biomedical Engineering, 2010, 57 (7) :1798-1806.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" title=" KARLSSON L, LOUTFI A.Sleep stage classification using unsupervised feature learning[J].Advances in Artificial Neural Systems, 2012." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJHD&amp;filename=SJHD130524016570&amp;v=MTQxMjNuS3JpZlp1OXVGQ3ZqVTdqSUtGb1FOaWZEYXJLN0h0VE9xNDlFWXU0SURCTTh6eFVTbURkOVNIN24zeEU5ZmJ2&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                         KARLSSON L, LOUTFI A.Sleep stage classification using unsupervised feature learning[J].Advances in Artificial Neural Systems, 2012.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" title=" LI K, LI X, ZHANG Y.Affective state recognition from EEG with deep belief networks[C]//Proceedings of IEEE International Conference on Bioinformatics and Biomedicine.Washington D.C., USA:IEEE Press, 2013:305-310." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Affective state recognition from EEG with deep belief networks">
                                        <b>[8]</b>
                                         LI K, LI X, ZHANG Y.Affective state recognition from EEG with deep belief networks[C]//Proceedings of IEEE International Conference on Bioinformatics and Biomedicine.Washington D.C., USA:IEEE Press, 2013:305-310.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" title=" 吴琛, 周瑞忠.Hilbert-Huang变换在提取地震信号动力特性中的应用[J].地震工程与工程振动, 2006, 26 (5) :41-46." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DGGC200605006&amp;v=MjQzMjdUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJvRnkvbFc3dk1JU3JNYmJHNEh0Zk1xbzlGWW9RS0RIODR2UjQ=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                         吴琛, 周瑞忠.Hilbert-Huang变换在提取地震信号动力特性中的应用[J].地震工程与工程振动, 2006, 26 (5) :41-46.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_10" title=" PETRANTONAKIS P C, HADJILEONTIADIS L J.A novel emotion elicitation index using frontal brain asymmetry for enhanced EEG-based emotion recognition[J].IEEE Transactions on Information Technology in Biomedicine, 2011, 15 (5) :737-46." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Novel Emotion Elicitation Index Using Frontal Brain Asymmetry for Enhanced EEG-Based Emotion Recognition">
                                        <b>[10]</b>
                                         PETRANTONAKIS P C, HADJILEONTIADIS L J.A novel emotion elicitation index using frontal brain asymmetry for enhanced EEG-based emotion recognition[J].IEEE Transactions on Information Technology in Biomedicine, 2011, 15 (5) :737-46.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_11" title=" XU Y L, CHEN J.Characterizing nonstationary wind speed using empirical mode decomposition[J].Journal of Structural Engineering, 2004, 130 (6) :912-920." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCE&amp;filename=SJCE13092200024344&amp;v=MjcxNjYvaXJSZEdlcnFRVE1ud1plWnVIeWptVUxuSUoxNGNiaFk9TmlmSWE3SzdIdGpPclk5RlpPa0xEM2c5b0JNVDZUNFBRSA==&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[11]</b>
                                         XU Y L, CHEN J.Characterizing nonstationary wind speed using empirical mode decomposition[J].Journal of Structural Engineering, 2004, 130 (6) :912-920.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_12" title=" PENG Z K, TSE P W, CHU F L.An improved Hilbert-Huang transform and its application in vibration signal analysis[J].Journal of Sound and Vibration, 2005, 286 (1/2) :187-205." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011601010830&amp;v=MjE1MTA4NW9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVUxuSUoxNGNiaFk9TmlmT2ZiSzdIdEROcVk5RVpPb1BCSA==&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[12]</b>
                                         PENG Z K, TSE P W, CHU F L.An improved Hilbert-Huang transform and its application in vibration signal analysis[J].Journal of Sound and Vibration, 2005, 286 (1/2) :187-205.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_13" title=" 赵双乐.浅析小波变换理论及其应用[J].科技风, 2014 (15) :106-106." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=KJFT201415095&amp;v=MzE1NDBGckNVUkxPZVplUm9GeS9sVzd2TUxpZk5lckc0SDlYTnFvOU1ZWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEc=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[13]</b>
                                         赵双乐.浅析小波变换理论及其应用[J].科技风, 2014 (15) :106-106.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_14" title=" 石瑞敏, 杨兆建.基于改进EMD的多绳摩擦提升机载荷信息特征提取[J].煤炭学报, 2014, 39 (4) :782-788." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MTXB201404033&amp;v=MDA5NDNVUkxPZVplUm9GeS9sVzd2TUtEblRiTEc0SDlYTXE0OUdaNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckM=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[14]</b>
                                         石瑞敏, 杨兆建.基于改进EMD的多绳摩擦提升机载荷信息特征提取[J].煤炭学报, 2014, 39 (4) :782-788.
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_15" title=" BADSHAH A M, AHMAD J, RAHIM N, et al.Speech emotion recognition from spectrograms with deep convolutional neural network[C]//Proceedings of International Conference on Platform Technology and Service.Washington D.C., USA:IEEE Press, 2017:1-5." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Speech emotion recognition from spectrograms with deep convolutional neural network">
                                        <b>[15]</b>
                                         BADSHAH A M, AHMAD J, RAHIM N, et al.Speech emotion recognition from spectrograms with deep convolutional neural network[C]//Proceedings of International Conference on Platform Technology and Service.Washington D.C., USA:IEEE Press, 2017:1-5.
                                    </a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_16" >
                                        <b>[16]</b>
                                     BENGIO Y, SIMARD P, FRASCONI P.Learning long-term dependencies with gradient descent is difficult[J].IEEE Transactions on Neural Networks, 1994, 5 (2) :157-166.</a>
                                </li>
                                <li id="35">


                                    <a id="bibliography_17" title=" 李幼军, 黄佳进, 王海渊.基于SAE和LSTM RNN的多模态生理信号融合和情感识别研究[J].通信学报, 2017, 38 (12) :109-120." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=TXXB201712011&amp;v=MTA3Nzh0R0ZyQ1VSTE9lWmVSb0Z5L2xXN3ZNTVRYVGJMRzRIOWJOclk5RVpZUUtESDg0dlI0VDZqNTRPM3pxcUI=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[17]</b>
                                         李幼军, 黄佳进, 王海渊.基于SAE和LSTM RNN的多模态生理信号融合和情感识别研究[J].通信学报, 2017, 38 (12) :109-120.
                                    </a>
                                </li>
                                <li id="37">


                                    <a id="bibliography_18" title=" CORTES C, VAPNIK V.Support-vector networks[J].Machine Learning, 1995, 20 (3) :273-297." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00001339251&amp;v=MDI0MTM3cWVidWR0RkM3bFZML0FKRm89Tmo3QmFyTzRIdEhOckl4TVp1NE9ZM2s1ekJkaDRqOTlTWHFScnhveGNNSDdS&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[18]</b>
                                         CORTES C, VAPNIK V.Support-vector networks[J].Machine Learning, 1995, 20 (3) :273-297.
                                    </a>
                                </li>
                                <li id="39">


                                    <a id="bibliography_19" title=" NIE D, WANG X W, SHI L C, et al.EEG-based emotion recognition during watching movies[C]//Proceedings of International IEEE/EMBS Conference on Neural Engineering.Washington D.C., USA:IEEE Press, 2011:667-670." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=EEG-based emotion recognition during watching movies">
                                        <b>[19]</b>
                                         NIE D, WANG X W, SHI L C, et al.EEG-based emotion recognition during watching movies[C]//Proceedings of International IEEE/EMBS Conference on Neural Engineering.Washington D.C., USA:IEEE Press, 2011:667-670.
                                    </a>
                                </li>
                                <li id="41">


                                    <a id="bibliography_20" title=" PAUL S, MAZUMDER A, GHOSH P, et al.EEG based emotion recognition system using MFDFA as feature extractor[C]//Proceedings of International Conference on Robotics, Automation, Control and Embedded Systems.Washington D.C., USA:IEEE Press, 2015:1-5." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=EEG based emotion recognition system using MFDFA as feature extractor">
                                        <b>[20]</b>
                                         PAUL S, MAZUMDER A, GHOSH P, et al.EEG based emotion recognition system using MFDFA as feature extractor[C]//Proceedings of International Conference on Robotics, Automation, Control and Embedded Systems.Washington D.C., USA:IEEE Press, 2015:1-5.
                                    </a>
                                </li>
                                <li id="43">


                                    <a id="bibliography_21" title=" ZHENG W L, LU B L.Investigating critical frequency bands and channels for EEG-based emotion recognition with deep neural networks[J].IEEE Transactions on Autonomous Mental Development, 2015, 7 (3) :162-175." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Investigating Critical Frequency Bands and Channels for EEG-based Emotion Recognition with Deep Neural Networks">
                                        <b>[21]</b>
                                         ZHENG W L, LU B L.Investigating critical frequency bands and channels for EEG-based emotion recognition with deep neural networks[J].IEEE Transactions on Autonomous Mental Development, 2015, 7 (3) :162-175.
                                    </a>
                                </li>
                                <li id="45">


                                    <a id="bibliography_22" title=" LIU S, MENG J, ZHANG D, et al.Emotion recognition based on EEG changes in movie viewing[C]//Proceedings of International IEEE/EMBS Conference on Neural Engineering.Washington D.C., USA:IEEE Press, 2015:1036-1039." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Emotion recognition based on EEG changes in movie viewing">
                                        <b>[22]</b>
                                         LIU S, MENG J, ZHANG D, et al.Emotion recognition based on EEG changes in movie viewing[C]//Proceedings of International IEEE/EMBS Conference on Neural Engineering.Washington D.C., USA:IEEE Press, 2015:1036-1039.
                                    </a>
                                </li>
                                <li id="47">


                                    <a id="bibliography_23" title=" AFTANAS L I, REVA N V, VARLAMOV A A, et al.Analysis of evoked EEG synchronization and desynchronization in conditions of emotional activation in humans:temporal and topographic characteristics[J].Neuroscience and Behavioral Physiology, 2004, 34 (8) :859-67." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00001444699&amp;v=MzIwNDJIN1I3cWVidWR0RkM3bFZML0FKRm89Tmo3QmFyTzRIdEhOcTR0Qll1SUdZM2s1ekJkaDRqOTlTWHFScnhveGNN&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[23]</b>
                                         AFTANAS L I, REVA N V, VARLAMOV A A, et al.Analysis of evoked EEG synchronization and desynchronization in conditions of emotional activation in humans:temporal and topographic characteristics[J].Neuroscience and Behavioral Physiology, 2004, 34 (8) :859-67.
                                    </a>
                                </li>
                                <li id="49">


                                    <a id="bibliography_24" title=" JENKE R, PEER A, BUSS M.Feature extraction and selection for emotion recognition from EEG[J].IEEE Transactions on Affective Computing, 2017, 5 (3) :327-339." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Feature extraction and selection for emotion recognition from EEG">
                                        <b>[24]</b>
                                         JENKE R, PEER A, BUSS M.Feature extraction and selection for emotion recognition from EEG[J].IEEE Transactions on Affective Computing, 2017, 5 (3) :327-339.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJC" target="_blank">计算机工程</a>
                2019,45(04),196-204 DOI:10.19678/j.issn.1000-3428.0050486            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于脑电信号瞬时能量的情感识别方法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%99%88%E7%94%B0&amp;code=01964170&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">陈田</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%99%88%E5%8D%A0%E5%88%9A&amp;code=41107131&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">陈占刚</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%A2%81%E6%99%93%E8%BE%89&amp;code=22192088&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">袁晓辉</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%9E%A0%E6%80%9D%E8%88%AA&amp;code=41496744&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">鞠思航</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E4%BB%BB%E7%A6%8F%E7%BB%A7&amp;code=27168503&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">任福继</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%90%88%E8%82%A5%E5%B7%A5%E4%B8%9A%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%B8%8E%E4%BF%A1%E6%81%AF%E5%AD%A6%E9%99%A2&amp;code=0083575&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">合肥工业大学计算机与信息学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%90%88%E8%82%A5%E5%B7%A5%E4%B8%9A%E5%A4%A7%E5%AD%A6%E6%83%85%E6%84%9F%E8%AE%A1%E7%AE%97%E4%B8%8E%E5%85%88%E8%BF%9B%E6%99%BA%E8%83%BD%E6%9C%BA%E5%99%A8%E5%AE%89%E5%BE%BD%E7%9C%81%E9%87%8D%E7%82%B9%E5%AE%9E%E9%AA%8C%E5%AE%A4&amp;code=0062244&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">合肥工业大学情感计算与先进智能机器安徽省重点实验室</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%8C%97%E5%BE%B7%E5%85%8B%E8%90%A8%E6%96%AF%E5%B7%9E%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%B8%8E%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0071649&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">北德克萨斯州大学计算机与工程学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%BE%B7%E5%B2%9B%E5%A4%A7%E5%AD%A6%E5%B7%A5%E5%AD%A6%E9%83%A8&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">德岛大学工学部</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>希尔伯特-黄变换 (HHT) 是一种处理脑电信号 (EEG) 的有效方法, 包括经验模态分解 (EMD) 和Hilbert变换2个部分。但EMD无法分解包含低能量的信号, 且在低频区域会产生不良的本征模态函数。为消除EMD的弊端, 提出一种小波包变换 (WPT) 和HHT相结合的EEG处理方法。采用WPT将EEG分解成一组窄带信号, 通过HHT得到Hilbert能量谱, 求出平均瞬时能量作为EEG特征并封装成特征矩阵。将特征矩阵通过卷积神经网络 (CNN) 、递归神经网络 (RNN) 、支持向量机 (SVM) 组成的混合情感识别模型进行训练与分类。实验结果表明, 该方法对高兴、悲伤、平静、恐惧4种情感的平均识别率为86.22%, 最优识别率为93.45%。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%84%91%E7%94%B5%E4%BF%A1%E5%8F%B7&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">脑电信号;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%83%85%E6%84%9F%E8%AF%86%E5%88%AB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">情感识别;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%B8%8C%E5%B0%94%E4%BC%AF%E7%89%B9-%E9%BB%84%E5%8F%98%E6%8D%A2&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">希尔伯特-黄变换;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">卷积神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%80%92%E5%BD%92%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">递归神经网络;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    陈田 (1974—) , 女, 副教授, 主研方向为情感计算, E-mail:ct@hfut.edu.cn;;
                                </span>
                                <span>
                                    陈占刚, 硕士研究生;;
                                </span>
                                <span>
                                    袁晓辉, 副教授;;
                                </span>
                                <span>
                                    鞠思航, 硕士研究生;;
                                </span>
                                <span>
                                    任福继, 教授。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-02-11</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金重点项目 (61432004) ;国家自然科学基金 (61204046, 61474035);</span>
                                <span>国家留学基金 (201706695016);</span>
                    </p>
            </div>
                    <h1><b>Emotion Recognition Method Based on Instantaneous Energy of Electroencephalography</b></h1>
                    <h2>
                    <span>CHEN Tian</span>
                    <span>CHEN Zhan'gang</span>
                    <span>YUAN Xiaohui</span>
                    <span>JU Sihang</span>
                    <span>REN Fuji</span>
            </h2>
                    <h2>
                    <span>School of Computer and Information, Hefei University of Technology</span>
                    <span>Anhui Province Key Laboratory of Affective Computing and Advanced Intelligent Machine, Hefei University of Technology</span>
                    <span>School of Computer and Engineering, University of North Texas</span>
                    <span>Faculty of Engineering, The University of Tokushima</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Hilbert-Huang Transform (HHT) is an effective method to deal with Electroencephalography (EEG) that includes two parts:Empirical Mode Decomposition (EMD) and Hilbert transform.However, the EMD cannot decompose a signal of low energy and will produce bad Intrinsic Mode Functions (IMF) in the low frequency region.To eliminate the effects of EMD, this paper proposes an EEG processing method which combines Wavelet Packet Transform (WPT) and HHT.Firstly, the EEG is decomposed into a set of narrow-band signals by WPT, the Hilbert energy spectrum of EEG is obtained by HHT, and the average value of the instantaneous energies is calculated as the EEG feature and packaged into a feature matrix.The feature matrix is trained and classified by mixed emotion recognition model which consists of Convolutional Neural Network (CNN) , Recurrent Neural Network (RNN) , and Support Vector Machine (SVM) .Experimental results show that the average recognition rate and the best recognition rate of the four emotions which are happiness, sadness, calmness, and fear are 86.22% and 93.45%.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Electroencephalography%20(EEG)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Electroencephalography (EEG) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=emotion%20recognition&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">emotion recognition;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Hilbert-Huang%20Transform%20(HHT)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Hilbert-Huang Transform (HHT) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Convolutional%20Neural%20Network%20(CNN)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Convolutional Neural Network (CNN) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Recurrent%20Neural%20Network%20(RNN)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Recurrent Neural Network (RNN) ;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-02-11</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="51" name="51" class="anchor-tag">0 概述</h3>
                <div class="p1">
                    <p id="52">自然人机交互技术是当前计算机应用技术研究的重要方向, 情感的自动识别是实现自然人机交互的关键技术之一<citation id="168" type="reference"><link href="3" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>。越来越多的便携式生理信号采集设备的出现, 使得基于脑电信号 (EEG) 的情感识别研究成为可能。心理和生理学研究显示, 外显生理活动与内隐心理行为之间有着密切关系, 内隐行为主要包括感觉、知觉、注意、记忆、思维、情绪和意志等<citation id="169" type="reference"><link href="5" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>。随着情感计算研究领域的发展, EEG、心电信号 (ECG) 、肌电信号 (EMG) 等生理信号逐渐被应用。研究发现, 使用EEG进行情感识别的效果较为理想<citation id="170" type="reference"><link href="7" rel="bibliography" /><link href="9" rel="bibliography" /><sup>[<a class="sup">3</a>,<a class="sup">4</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="53">基于EEG的情感识别已经成为情感识别领域重要的研究对象之一。目前, 国际上基于机器学习和神经网络的情感识别方法在EEG研究领域也取得了一些成果。文献<citation id="171" type="reference">[<a class="sup">5</a>]</citation>通过计算不同频带的信号特征, 经特征筛选找到合适的特征子集, 使用SVM训练与分类获得较好的分类效果。文献<citation id="172" type="reference">[<a class="sup">6</a>]</citation>采用SVM将EEG的情感状态分为4类, 其平均分类精确度为82.29%, 并验证了大脑的额叶和颞叶是情感的主要识别区域。文献<citation id="173" type="reference">[<a class="sup">7</a>]</citation>应用深度信念网络 (Deep Belief Network, DBN) 和隐马尔可夫模型 (Hidden Markov Model, HMM) , 采用多模态情感识别的方法取得了比传统的特征分类更好的结果。文献<citation id="174" type="reference">[<a class="sup">8</a>]</citation>提出一种基于深度神经网络的EEG情感状态识别模型, 进一步提高了分类效果。</p>
                </div>
                <div class="p1">
                    <p id="54">近年来, 希尔伯特-黄变换 (Hilbert-Huang Transform, HHT) 被应用于信号处理领域<citation id="175" type="reference"><link href="19" rel="bibliography" /><link href="21" rel="bibliography" /><sup>[<a class="sup">9</a>,<a class="sup">10</a>]</sup></citation>, 并取得了较大的进展。该方法对非稳定信号的处理有很好的效果, 将一个信号分解成一组近似余弦波的信号 (周期、振幅不固定) 和一个趋势函数, 可以有效地去除噪音, 得到信号的时频特征。HHT的经验模态分解 (Empirical Mode Decomposition, EMD) 步骤不能分解包含低能量的信号并且在低频区域会产生不良的本征模态函数 (Intrinsic Mode Function, IMF) 。为此, 本文采用一种小波包变换 (Wavelet Packet Transform, WPT) 和HHT相结合的EEG处理方法, 即W-HHT方法, 并且通过CNN、RNN与SVM组成的混合情感识别模型 (C-R-SVM) 进行训练与分类。</p>
                </div>
                <h3 id="55" name="55" class="anchor-tag">1 基于W-HHT的EEG特征提取</h3>
                <div class="p1">
                    <p id="56">早期的EEG情感识别方法需要基于广泛的领域知识, 设计和提取来自单个或多个信道信号的一系列特征, 但无法充分利用不同信道间的相关信息。本文方法可以有效解决这些问题, 其总体流程如图1所示。使用经过筛选的视频片段诱发受试者的情感, 提取特定情感状态下的EEG数据。对EEG数据进行预处理, 主要包括截取有效时长的数据片段和通过快速独立成分分析 (Fast Independent Component Analysis, FastICA) 去噪。使用W-HHT方法处理EEG, 求出Hilbert能量谱并计算出瞬时能量作为特征。通过混合C-R-SVM情感识别模型进行训练与分类。</p>
                </div>
                <div class="area_img" id="57">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201904033_057.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 基于EEG的情感识别总体流程" src="Detail/GetImg?filename=images/JSJC201904033_057.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图1 基于EEG的情感识别总体流程</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201904033_057.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="58" name="58">1.1 特征提取</h4>
                <div class="p1">
                    <p id="59">EEG属于非平稳信号, HHT作为一种非平稳信号的处理方法, 在避免复杂数学运算的同时, 可以分析信号频率随时间的变化规律, 已应用于海洋、地震和机械振动信号分析等领域<citation id="176" type="reference"><link href="23" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>。但EMD在分解信号时存在一些弊端<citation id="177" type="reference"><link href="25" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>:</p>
                </div>
                <div class="p1">
                    <p id="60">1) EMD不能分解包含低能量的信号, 在低频区域会产生不良的IMF, 并且所产生的IMF并不完全有效, 从而造成实验结果存在误差。</p>
                </div>
                <div class="p1">
                    <p id="61">2) 获得的第一个IMF覆盖的频率范围过宽, 会产生虚假模态的问题。</p>
                </div>
                <div class="p1">
                    <p id="62">因此, 本文使用WPT对EEG进行预处理, 将其分解成一组窄带信号, 其预处理流程如图2所示。</p>
                </div>
                <div class="area_img" id="63">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201904033_063.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 EEG预处理流程" src="Detail/GetImg?filename=images/JSJC201904033_063.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图2 EEG预处理流程</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201904033_063.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="64">由图2可见, 预处理流程包括4个部分:</p>
                </div>
                <h4 class="anchor-tag" id="65" name="65">1) WPT求取窄带信号</h4>
                <div class="p1">
                    <p id="66">WPT继承和发展了短时傅里叶变换局部化的思想, 并克服了窗口大小不随频率变化的缺点, 是对信号进行时频处理和分析的理想工具<citation id="178" type="reference"><link href="27" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>。为解决上述EMD的弊端, 本文在对EEG进行HHT之前, 应用WPT对EEG进行预处理, 将其分解成一组窄带信号, 并在此基础上对分解后的信号进行HHT处理, 很大程度上解决了上述EMD自身的缺点。</p>
                </div>
                <h4 class="anchor-tag" id="67" name="67">2) EMD分解EEG</h4>
                <div class="p1">
                    <p id="68">EMD是一种对非线性和非稳态信号进行时频分析的方法, 分解过程直观, 适应性强<citation id="179" type="reference"><link href="19" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>。由EMD分解出的信号称为IMF, 其中IMF满足以下2个条件: (1) 在整个数据段内, 极值点的个数和过零点的个数必须相等或最多相差一个; (2) 在任意时刻, 由局部极大值点形成的上包络线和由局部极小值点形成的下包络线的平均值为0。求取IMF的流程为:</p>
                </div>
                <div class="p1">
                    <p id="69"><b>步骤1</b> 寻找窄带信号<i>x</i> (<i>t</i>) 的极大值点和极小值点, 通过三次样条差值求取极大值点的上、下包络线, 如图3所示。</p>
                </div>
                <div class="area_img" id="70">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201904033_070.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 上包络线和下包络线" src="Detail/GetImg?filename=images/JSJC201904033_070.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图3 上包络线和下包络线</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201904033_070.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="71"><b>步骤2</b> 求取上、下包络线的平均值, 即<i>m</i><sub>1</sub> (<i>t</i>) 。</p>
                </div>
                <div class="p1">
                    <p id="72"><b>步骤3</b> 将<i>x</i> (<i>t</i>) 减去<i>m</i><sub>1</sub> (<i>t</i>) 得到<i>h</i><sub>1</sub> (<i>t</i>) , 判断<i>h</i><sub>1</sub> (<i>t</i>) 是否满足IMF上述2个条件。如不满足, 将<i>h</i><sub>1</sub> (<i>t</i>) 作为原始信号重复上述步骤, 直到满足IMF的上述2个条件为止, 将得到的第一个IMF记为<i>c</i><sub>1</sub> (<i>t</i>) 。</p>
                </div>
                <div class="p1">
                    <p id="73"><b>步骤4</b> 将<i>c</i><sub>1</sub> (<i>t</i>) 从原始信号<i>x</i> (<i>t</i>) 中分离出来得到<i>r</i><sub>1</sub> (<i>t</i>) , 判断<i>r</i><sub>1</sub> (<i>t</i>) 是否可以分解。不可以分解则结束;可以分解则将<i>r</i><sub>1</sub> (<i>t</i>) 作为原始信号继续分解, 重复循环上述步骤, 得到一组IMF。最后剩余部分计为<i>r</i><sub><i>n</i></sub> (<i>t</i>) , 如果<i>r</i><sub><i>n</i></sub> (<i>t</i>) 不能分解, 则分解结束。</p>
                </div>
                <div class="p1">
                    <p id="74">通过EMD后原始信号<i>x</i> (<i>t</i>) 可以表示为:</p>
                </div>
                <div class="p1">
                    <p id="75" class="code-formula">
                        <mathml id="75"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>x</mi><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>c</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo><mo>+</mo><mi>r</mi><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="76">其中, <i>c</i><sub><i>i</i></sub> (<i>t</i>) 即为IMF, <i>r</i><sub><i>n</i></sub> (<i>t</i>) 为不能继续分解的残余信号。</p>
                </div>
                <h4 class="anchor-tag" id="77" name="77">3) IMF筛选</h4>
                <div class="p1">
                    <p id="78">通过EMD可以获得10个～12个IMF, 如图4所示。</p>
                </div>
                <div class="area_img" id="79">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201904033_079.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 EMD分解得到的IMF" src="Detail/GetImg?filename=images/JSJC201904033_079.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图4 EMD分解得到的IMF</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201904033_079.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="80">方差贡献率通常用来表示每个IMF的相对重要性<citation id="180" type="reference"><link href="29" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>。为确定有效的IMF个数, 计算IMF的累积方差贡献率, 令每个IMF的方差为<i>mse</i> (<i>i</i>) , 其值为每个IMF平方的均值减去均值的平方, 计算公式如下:</p>
                </div>
                <div class="p1">
                    <p id="81" class="code-formula">
                        <mathml id="81"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>m</mi><mi>s</mi><mi>e</mi><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mn>1</mn><mi>n</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><mrow><mo>|</mo><mrow><mi>c</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false"> (</mo><mi>k</mi><mtext>Δ</mtext><mi>t</mi><mo stretchy="false">) </mo></mrow><mo>|</mo></mrow></mrow></mstyle><msup><mrow></mrow><mn>2</mn></msup><mo>-</mo><mrow><mo stretchy="false">[</mo><mfrac><mn>1</mn><mi>n</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><mrow><mo>|</mo><mrow><mi>c</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false"> (</mo><mi>k</mi><mtext>Δ</mtext><mi>t</mi><mo stretchy="false">) </mo></mrow><mo>|</mo></mrow></mrow></mstyle><mo stretchy="false">]</mo></mrow><msup><mrow></mrow><mn>2</mn></msup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="82">令每个IMF的方差贡献率为<i>mse</i>′ (<i>i</i>) , 则:</p>
                </div>
                <div class="p1">
                    <p id="83" class="code-formula">
                        <mathml id="83"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>m</mi><mi>s</mi><msup><mi>e</mi><mo>′</mo></msup><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mrow><mi>m</mi><mi>s</mi><mi>e</mi><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mi>m</mi></mstyle><mi>s</mi><mi>e</mi><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="84">计算发现, 前4个IMF的累计方差贡献率达到了95%以上, 这是由于前4个IMF基本包含了信号绝大部分重要信息, 因此本文选择前4个IMF进行实验。</p>
                </div>
                <h4 class="anchor-tag" id="85" name="85">4) Hilbert变换求瞬时能量</h4>
                <div class="p1">
                    <p id="86">对IMF进行Hilbert变换求取瞬时能量特征。对式 (1) 中每个固有的IMF, 即<i>c</i><sub><i>i</i></sub> (<i>t</i>) 做Hilbert变换得:</p>
                </div>
                <div class="p1">
                    <p id="87" class="code-formula">
                        <mathml id="87"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Y</mi><mo stretchy="false">[</mo><mi>c</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mo>=</mo><mi>Ρ</mi><mfrac><mn>1</mn><mtext>π</mtext></mfrac><mstyle displaystyle="true"><mrow><msubsup><mo>∫</mo><mrow><mo>-</mo><mi>∞</mi></mrow><mi>∞</mi></msubsup><mrow><mfrac><mrow><mi>c</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo></mrow><mrow><mi>t</mi><mo>-</mo><mi>τ</mi></mrow></mfrac></mrow></mrow></mstyle><mtext>d</mtext><mi>τ</mi><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="88">其中, <i>P</i>是柯西主值。构造解析信号<i>Z</i><sub><i>i</i></sub> (<i>t</i>) :</p>
                </div>
                <div class="p1">
                    <p id="89"><i>Z</i><sub><i>i</i></sub> (<i>t</i>) =<i>c</i><sub><i>i</i></sub> (<i>t</i>) +<i>nY</i>[<i>c</i><sub><i>i</i></sub> (<i>t</i>) ]=<i>a</i><sub><i>i</i></sub> (<i>t</i>) e<sup><i>nθ</i><sub><i>i</i></sub> (<i>t</i>) </sup>      (5) </p>
                </div>
                <div class="p1">
                    <p id="90"><i>Z</i><sub><i>i</i></sub> (<i>t</i>) 是一个复数值信号, 包含了IMF中所有的信息。从式 (5) 可以确定IMF的瞬时振幅<i>a</i><sub><i>i</i></sub> (<i>t</i>) 和瞬时相位<i>θ</i><sub><i>i</i></sub> (<i>t</i>) :</p>
                </div>
                <div class="p1">
                    <p id="91" class="code-formula">
                        <mathml id="91"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>a</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo><mo>=</mo><msqrt><mrow><mi>c</mi><msubsup><mrow></mrow><mi>i</mi><mn>2</mn></msubsup><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo><mo>+</mo><mi>Y</mi><msup><mrow></mrow><mn>2</mn></msup><mo stretchy="false">[</mo><mi>c</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo><mo stretchy="false">]</mo></mrow></msqrt><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>6</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi>θ</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo><mo>=</mo><mrow><mi>arctan</mi></mrow><mo stretchy="false">{</mo><mi>Y</mi><mo stretchy="false">[</mo><mi>c</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mo>/</mo><mi>c</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo><mo stretchy="false">}</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>7</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="92">进而求出信号的瞬时频率<i>ω</i><sub><i>i</i></sub> (<i>t</i>) :</p>
                </div>
                <div class="p1">
                    <p id="93"><i>ω</i><sub><i>i</i></sub> (<i>t</i>) =d<i>θ</i><sub><i>i</i></sub> (<i>t</i>) /d<i>t</i>      (8) </p>
                </div>
                <div class="p1">
                    <p id="94">则窄带信号<i>x</i> (<i>t</i>) 可表示为:</p>
                </div>
                <div class="p1">
                    <p id="95"><i>x</i> (<i>t</i>) =Re<mathml id="96"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>a</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo><mtext>e</mtext><msup><mrow></mrow><mrow><mi>n</mi><mi>θ</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo></mrow></msup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>9</mn><mo stretchy="false">) </mo></mrow></math></mathml></p>
                </div>
                <div class="p1">
                    <p id="97">可变形为:</p>
                </div>
                <div class="p1">
                    <p id="98"><i>x</i> (<i>t</i>) =Re<mathml id="99"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>a</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo><mtext>e</mtext><msup><mrow></mrow><mrow><mi>n</mi><mstyle displaystyle="true"><mrow><mo>∫</mo><mi>ω</mi></mrow></mstyle><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo><mtext>d</mtext><mi>t</mi></mrow></msup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>0</mn><mo stretchy="false">) </mo></mrow></math></mathml></p>
                </div>
                <div class="p1">
                    <p id="100">式 (10) 省略了残余函数<i>r</i><sub><i>n</i></sub> (<i>t</i>) , 即通过EMD不能继续分解的信号。Hilbert时频谱, 简称Hilbert谱<i>H</i> (<i>ω</i>, <i>t</i>) , 计算公式如下:</p>
                </div>
                <div class="p1">
                    <p id="101"><i>H</i> (<i>ω</i>, <i>t</i>) =Re<mathml id="102"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>a</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo><mtext>e</mtext><msup><mrow></mrow><mrow><mi>n</mi><mstyle displaystyle="true"><mrow><mo>∫</mo><mi>ω</mi></mrow></mstyle><msub><mrow></mrow><mi>i</mi></msub><mtext>d</mtext><mi>t</mi></mrow></msup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml></p>
                </div>
                <div class="p1">
                    <p id="103">图5即为得到的Hilbert谱图。</p>
                </div>
                <div class="area_img" id="104">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201904033_104.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 Hilbert谱图" src="Detail/GetImg?filename=images/JSJC201904033_104.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图5 Hilbert谱图</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201904033_104.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="105">由于每个IMF的频率和幅值都是时间的函数, 因此Hilbert谱<i>H</i> (<i>ω</i>, <i>t</i>) 描述了信号幅值随时间和频率的分布情况。<i>H</i><sup>2</sup> (<i>ω</i>, <i>t</i>) 和<mathml id="106"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>|</mo><mrow><mi>x</mi><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo></mrow><mo>|</mo></mrow><msup><mrow></mrow><mn>2</mn></msup></mrow></math></mathml>都可以看作信号的能量密度, 将其称为<i>Hilbert</i>能量谱, <i>IMF</i>的<i>Hilbert</i>能量谱如图6所示。</p>
                </div>
                <div class="area_img" id="107">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201904033_107.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 Hilbert能量谱" src="Detail/GetImg?filename=images/JSJC201904033_107.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图6 Hilbert能量谱</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201904033_107.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="108">进而可求出EEG的瞬时能量<i>E</i> (<i>t</i>) , 即:</p>
                </div>
                <div class="p1">
                    <p id="109"><i>E</i> (<i>t</i>) =∫<sup>∞</sup><sub>-∞</sub><i>H</i><sup>2</sup> (<i>ω</i>, <i>t</i>) d<i>ω</i>      (12) </p>
                </div>
                <div class="p1">
                    <p id="110">其中, <i>ω</i>是频率。在求得瞬时能量后, 将每个IMF平均分为10部分, 求取每部分的平均瞬时能量作为特征值, 组成40×<i>N</i>的特征矩阵 (<i>N</i>为所选脑波仪通道个数) , 作为混合情感识别模型的输入。</p>
                </div>
                <h4 class="anchor-tag" id="111" name="111">1.2 训练与识别</h4>
                <h4 class="anchor-tag" id="112" name="112">1.2.1 情感识别框架总体结构</h4>
                <div class="p1">
                    <p id="113">本文采用<i>CNN</i>与<i>RNN</i>相结合的混合深度学习模型来对数据进行训练, 该模型将<i>CNN</i>强大的数据处理能力与<i>RNN</i>顺序处理数据的良好性能相结合, 可实现对<i>EEG</i>数据的有效训练。其中, <i>CNN</i>为7层结构:1个输入层, 2个卷积层<i>C</i>1和<i>C</i>2, 2个池化层<i>S</i>1和<i>S</i>2, 1个全连层和1输出层。在<i>RNN</i>中使用长短期记忆网络 (<i>Long Short</i>-<i>Term Memory</i>, <i>LSTM</i>) 作为单元, 可有效处理时间序列中间隔和延迟相对较长的序列。<i>SVM</i>作为分类器对训练后的情感特征进行分类。<i>C</i>-<i>R</i>-<i>SVM</i>混合情感识别模型的结构如图7所示。</p>
                </div>
                <div class="area_img" id="114">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201904033_114.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 C-R-SVM混合情感识别模型" src="Detail/GetImg?filename=images/JSJC201904033_114.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图7 C-R-SVM混合情感识别模型</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201904033_114.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="115" name="115">1.2.2 卷积神经网络</h4>
                <div class="p1">
                    <p id="116">CNN具有可以池化、权值共享和感受野等优点, 为神经网络处理具有网格结构的数据提供了可能, 一般由一个或多个卷积层组成。卷积是将卷积滤波器应用于原始2D数据的过程, 卷积阶段的特点包括稀疏连接和参数共享, 参数共享机制大大减少了传统全连接神经网络中权重参数的数量, 降低了参数存储的需求。CNN在计算机视觉、语音识别和自然语言处理等领域取得了许多成果<citation id="181" type="reference"><link href="31" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>, 但应用于EEG情感识别领域才刚刚起步。</p>
                </div>
                <div class="p1">
                    <p id="117">本文将卷积层C1的滤波器大小设置为14×1, 目的是在一个特定的尺度下挖掘通道之间的相关特征。经过反复的卷积运算后得到多个特征映射, 本阶段将卷积滤波器的个数设置为8, 即提取8种不同的相关信息。第1个卷积层后是一个2×1的池化层, 用来汇聚交叉通道的信息。C2被设定为有16个大小为1×1卷积滤波器, 有助于进一步融合前面8个特征图的信息, 之后是一个2×1的池化层, 用来进行信息聚合。最后的全连接层用来确保特征序列输入到RNN之前数据变为一维特征向量。</p>
                </div>
                <h4 class="anchor-tag" id="118" name="118">1.2.3 递归神经网络</h4>
                <div class="p1">
                    <p id="119">在与RNN连接后, 混合模型获得了学习时间序列的能力和连续的建模能力。RNN的能力依赖于它的递归结构, 可以用相同或不同长度的序列来对上下文信息进行建模, 这对情绪的评估与识别起着重要作用。</p>
                </div>
                <div class="p1">
                    <p id="120">传统的RNN难以训练, 面临着“逆向传播中的梯度消失或爆炸”的挑战<citation id="182" type="reference"><link href="33" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>。因此, 本文使用了LSTM, 它结合了“门”机制, 是一种时间递归神经网络, 适合于处理和预测时间序列中间隔和延迟相对较长的序列, 已在图像识别、语音识别和机器翻译等领域取得了成功<citation id="183" type="reference"><link href="35" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>。本文采用LSTM作为RNN的一个单元, 将训练数据表示为展开链的形式, 每条链表示一个从CNN输出的正在处理的数据的时间步长。LSTM的结构如图8所示, 主要由忘记门、输入门和输出门3部分组成。</p>
                </div>
                <div class="area_img" id="121">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201904033_121.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 LSTM结构" src="Detail/GetImg?filename=images/JSJC201904033_121.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图8 LSTM结构</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201904033_121.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="122">1) 忘记门决定了什么信息应该被忘记, 其中, <b><i>h</i></b><sub><i>t</i>-1</sub>是上一个LSTM单元, 记录了上一个神经元的状态, <b><i>x</i></b><sub><i>t</i></sub>是当前输入, <b><i>W</i></b><sub><i>f</i></sub>是门的权重参数, <b><i>b</i></b><sub><i>f</i></sub>是忘记门的偏置项, 输出向量 <b><i>f</i></b><sub><i>t</i></sub>在激活函数Sigmoid的作用下缩放到0～1, 计算公式如下:</p>
                </div>
                <div class="p1">
                    <p id="123"><b><i>f</i></b><sub><i>t</i></sub>=<i>σ</i> (<b><i>W</i></b><sub><i>f</i></sub>·[<b><i>h</i></b><sub><i>t</i>-1</sub>, <b><i>x</i></b><sub><i>t</i></sub>]+<b><i>b</i></b><sub><i>f</i></sub>)      (13) </p>
                </div>
                <div class="p1">
                    <p id="124">按元素乘法计算<b><i>C</i></b><sub><i>t</i>-1</sub><b><i>f</i></b><sub><i>t</i></sub>, 以决定上一个神经元的哪些信息可以保留。如果结果为0, 表示全舍弃;如果结果为1, 表示全保留。</p>
                </div>
                <div class="p1">
                    <p id="125">2) 输入门决定了信息的存储, 主要分2步:首先, 输入门决定更新哪些值<b><i>i</i></b><sub><i>t</i></sub>;然后, tanh层生成了一个新的候选向量<mathml id="126"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mrow><mi mathvariant="bold-italic">C</mi><msub><mrow></mrow><mi>t</mi></msub></mrow><mo stretchy="true">¯</mo></mover></mrow></math></mathml>, 其计算公式如下:</p>
                </div>
                <div class="p1">
                    <p id="127"><b><i>i</i></b><sub><i>t</i></sub>=<i>σ</i> (<b><i>W</i></b><sub><i>i</i></sub>·[<b><i>h</i></b><sub><i>t</i>-1</sub>, <b><i>x</i></b><sub><i>t</i></sub>]+<b><i>b</i></b><sub><i>i</i></sub>)      (14) </p>
                </div>
                <div class="p1">
                    <p id="128" class="code-formula">
                        <mathml id="128"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mrow><mi mathvariant="bold-italic">C</mi><msub><mrow></mrow><mi>t</mi></msub></mrow><mo stretchy="true">¯</mo></mover><mo>=</mo><mrow><mi>tanh</mi></mrow><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mi>c</mi></msub><mo>⋅</mo><mo stretchy="false">[</mo><mi mathvariant="bold-italic">h</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>, </mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">]</mo><mo>+</mo><mi mathvariant="bold-italic">b</mi><msub><mrow></mrow><mi>c</mi></msub><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>5</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="129">其中, <b><i>b</i></b><sub><i>i</i></sub>是输入门的偏置项, <b><i>b</i></b><sub><i>c</i></sub>是新记忆的偏置项, <b><i>W</i></b><sub><i>i</i></sub>、<b><i>W</i></b><sub><i>c</i></sub>为对应的权重矩阵。将2个值按元素乘法计算<mathml id="130"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mrow><mi mathvariant="bold-italic">C</mi><msub><mrow></mrow><mi>t</mi></msub></mrow><mo stretchy="true">¯</mo></mover><mo>⋅</mo><mi mathvariant="bold-italic">i</mi><msub><mrow></mrow><mi>t</mi></msub></mrow></math></mathml>, 即为更新的记忆。</p>
                </div>
                <div class="p1">
                    <p id="131">当前单元的状态<b><i>C</i></b><sub><i>t</i></sub>是原始信息和当前更新信息的组合, 即:</p>
                </div>
                <div class="p1">
                    <p id="132" class="code-formula">
                        <mathml id="132"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">C</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mi mathvariant="bold-italic">C</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>⋅</mo><mi mathvariant="bold-italic">f</mi><msub><mrow></mrow><mi>t</mi></msub><mo>+</mo><mover><mstyle mathsize="140%" displaystyle="true"><mi mathvariant="bold-italic">C</mi></mstyle><mrow><mspace width="0.25em" /><mo>—</mo></mrow></mover><msub><mrow></mrow><mi>t</mi></msub><mo>⋅</mo><mi mathvariant="bold-italic">i</mi><msub><mrow></mrow><mi>t</mi></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>6</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="133">3) 输出层门决定了当前输出, 通过向量<i>O</i><sub><i>t</i></sub>和tanh层来决定输出<b><i>h</i></b><sub><i>t</i></sub>, 计算公式如下:</p>
                </div>
                <div class="p1">
                    <p id="134"><b><i>O</i></b><sub><i>t</i></sub>=<i>σ</i> (<b><i>W</i></b><sub><i>o</i></sub>·[<b><i>h</i></b><sub><i>t</i>-1</sub>, <b><i>x</i></b><sub><i>t</i></sub>]+<b><i>b</i></b><sub><i>o</i></sub>)      (17) </p>
                </div>
                <div class="p1">
                    <p id="135"><b><i>h</i></b><sub><i>t</i></sub>=tanh (<b><i>C</i></b><sub><i>t</i></sub>) ·<b><i>O</i></b><sub><i>t</i></sub>      (18) </p>
                </div>
                <div class="p1">
                    <p id="136">其中, <b><i>b</i></b><sub><i>o</i></sub>是输出门的偏置项。</p>
                </div>
                <h4 class="anchor-tag" id="137" name="137">1.2.4 支持向量机</h4>
                <div class="p1">
                    <p id="138">本文模型在对数据进行训练后选择使用<i>SVM</i>对训练结果进行分类。<i>SVM</i>是一种常用且分类有效的一种分类器, 可以在高维空间中构建最优分离超平面, 对新样本进行分类<citation id="184" type="reference"><link href="37" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>。为获得更好的情感识别结果, 将训练过的特征向量输入到<i>SVM</i>中进行分类, 并选用高斯核函数, 计算公式如下:</p>
                </div>
                <div class="p1">
                    <p id="139" class="code-formula">
                        <mathml id="139"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>k</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mrow><mi>exp</mi></mrow><mrow><mo> (</mo><mrow><mfrac><mrow><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>j</mi></msub></mrow><mo>|</mo></mrow></mrow><mrow><mn>2</mn><mi>σ</mi><msup><mrow></mrow><mn>2</mn></msup></mrow></mfrac></mrow><mo>) </mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>9</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="140">其中, σ (σ&gt;0) 为高斯核的带宽, 可通过调控σ的大小实现最优分类结果。高斯核函数是一种局部性强的核函数, 可以将一个样本映射到一个更高维的空间, 与样本大小无关, 而且相对于其他核函数使用参数较少。在使用<i>SVM</i>进行分类时, 通过5-折交叉验证法和网格搜索得到最优的惩罚参数C来对训练集进行训练, 本文设置C=1。由于<i>SVM</i>是一种二分类分类器, 本文将高兴、悲伤、平静、恐惧4种情感的标签依次设置为1、2、3、4。以其中1类作为正样本, 其他3类作为负样本实现两两分类。因此, 在模型后面需要4个<i>SVM</i>, 每个<i>SVM</i>对应着一个正负样本的二分类。</p>
                </div>
                <h3 id="141" name="141" class="anchor-tag">2 实验与结果分析</h3>
                <h4 class="anchor-tag" id="142" name="142">2.1 <i>EEG</i>情感数据的提取</h4>
                <div class="p1">
                    <p id="143">本文使用16导的<i>Emotiv</i>脑波仪 (频率128 <i>Hz</i>) 提取悲伤、平静、高兴、恐惧4种不同情感状态下的<i>EEG</i>数据, 16导脑波仪的电极位置如图9所示。</p>
                </div>
                <div class="area_img" id="144">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201904033_144.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图9 Emotiv电极位置" src="Detail/GetImg?filename=images/JSJC201904033_144.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图9 Emotiv电极位置</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201904033_144.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="145" name="145">1) 受试者</h4>
                <div class="p1">
                    <p id="146">选择10名健康且自愿参加的受试者进行实验 (5男, 5女, 年龄:20岁～25岁) 。</p>
                </div>
                <h4 class="anchor-tag" id="147" name="147">2) 情感诱发</h4>
                <div class="p1">
                    <p id="148">为更好地诱发受试者的特定情感, 选择具有不同情感色彩的视频作为实验素材, 以确保视频的有效性。实验进行5次, 每次从准备的200个视频中选择20个未曾使用过的视频进行实验。其中, 每种情感状态通过5个视频片段诱发, 每个片段时长在3 min～5 min, EEG情感数据提取的流程如图10所示。实验在安静的房间里进行, 受试者坐在距离电脑约60 cm处。每个视频片段开始前有5 s的数字倒计时, 作为视频开始播放的提示。视频播放结束后要求受试者对该视频的情感诱发程度做一个评估, 将受试者的自我评估作为情感识别的真实标签。每段视频结束后有30 s的休息时间, 尽可能在下一段视频开始前消除上一段视频的影响。为确保实验的有效性, 按照悲伤、平静、高兴、恐惧的顺序让每个受试者观看视频进行实验。在整个实验中, 每个受试者看了100个视频, 每种情感的视频25个。</p>
                </div>
                <div class="area_img" id="149">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201904033_149.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图10 EEG情感诱发流程" src="Detail/GetImg?filename=images/JSJC201904033_149.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图10 EEG情感诱发流程</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201904033_149.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="150" name="150">3) 数据预处理</h4>
                <div class="p1">
                    <p id="151">根据受试者的评价, 选取情感诱发最有效的时长为30 s的数据进行实验。为验证不同频带的EEG特性, 通过小波分解将EEG分解为<i>θ</i> ([4, 8]) 、<i>α</i> ([9, 13]) 、 <i>β</i> ([14, 30]) 、<i>γ</i> ([31, ∞) ) 4个频带 (单位:Hz) 。由于眼电信号对EEG的产生影响较大, 本文采用FastICA去噪, 去除眼电信号的干扰。FastICA是一种基于负熵判断的不动点算法, 主要分为3个步骤: (1) 对信号进行去均值和白化处理; (2) 对处理后的信号依据负熵判据来寻找解混矩阵; (3) 由信号的混解算法实现独立分量的分离。</p>
                </div>
                <h4 class="anchor-tag" id="152" name="152">2.2 结果分析</h4>
                <div class="p1">
                    <p id="153">由于不是所有的EEG频率范围和电极位置都带有重要的情感信息, 为了选取有效的频带和电极位置, 本文系统比较了不同频带和不同电极位置的性能。通过图11可以观察到, 情感活动主要集中在 <i>β</i>频带和<i>γ</i>频带, 且主要分布在额叶和颞叶。图12表示了4种波形的瞬时能量谱图, 可以看出<i>γ</i>频带的瞬时能量随时间变化最为强烈, 其次是 <i>β</i>频带, 而<i>θ</i>频带的瞬时能量随时间变化最弱。<i>α</i>频带主要在睡眠和闭目时出现, 对于情感活动反应不强烈。因此, 通过不同频带瞬时能量的比较也可证明 <i>β</i>频带和<i>γ</i>频带与情感活动最相关。</p>
                </div>
                <div class="area_img" id="154">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201904033_154.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图11 各频带能量地形图" src="Detail/GetImg?filename=images/JSJC201904033_154.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图11 各频带能量地形图</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201904033_154.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="155">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201904033_155.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图12 各频带的瞬时能量" src="Detail/GetImg?filename=images/JSJC201904033_155.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图12 各频带的瞬时能量</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201904033_155.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="156">图13为不同分类器的混淆矩阵, 显示了分类器对不同情感的识别强弱。混淆矩阵的列代表预测类, 行代表实际输出类。可以看出, 积极的情绪可以被高精度地识别, 而消极情绪难以识别。从图13可以看出, KNN (参数<i>k</i>=3, <i>p</i>=2) 对积极情感有较高的分类准确度, 而对消极情感识别效果较差。相对于KNN, SVM (惩罚系数<i>C</i>=1) 可以更准确地识别消极情感。而C-R-SVM不仅对积极的情绪有很高的分类准确度, 而且可以有效识别消极情感。表1为不同情感状态在不同频带下的平均识别率, 可以观察到选用 <i>β</i>和<i>γ</i>频带在情感识别上可以得到较好的识别结果, 这也证明了上述结论。</p>
                </div>
                <div class="area_img" id="157">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201904033_157.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图13 同一实验不同分类器的混淆矩阵" src="Detail/GetImg?filename=images/JSJC201904033_157.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图13 同一实验不同分类器的混淆矩阵</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201904033_157.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="158">
                    <p class="img_tit"><b>表1 不同情感状态在不同频带下的平均识别率</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note">%</p>
                    <table id="158" border="1"><tr><td><br />情感状态</td><td><i>θ</i>频带</td><td><i>α</i>频带</td><td><i>β</i>频带</td><td><i>γ</i>频带</td></tr><tr><td><br />高兴</td><td>57.16</td><td>63.60</td><td>78.86</td><td>89.56</td></tr><tr><td><br />悲伤</td><td>58.03</td><td>62.52</td><td>77.25</td><td>85.85</td></tr><tr><td><br />平静</td><td>56.58</td><td>61.73</td><td>75.32</td><td>88.82</td></tr><tr><td><br />恐惧</td><td>54.58</td><td>59.84</td><td>72.25</td><td>80.64</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="159">为确定有效的电极位置, 根据上述情感活动主要集中在 <i>β</i>和<i>γ</i>频带, 且主要在额叶和颞叶的结论, 选择不同的电极位置进行实验, 如图14所示。实验1:选择电极AF3、AF4、F7、F8、F3、F4、T7、T8, 如图14 (a) 所示。实验2:选择电极F7、F8、F3、F4、FC5、FC6、T7、T8, 如图14 (b) 所示。实验3:选择电极AF3、AF4、F7、F8、FC5、FC6、T7、T8, 如图14 (c) 所示。实验4:选择电极AF3、AF4、F7、F8、F3、F4、FC5、FC6、T7、T8, 如图14 (d) 所示。实验5:选择全部通道位置。</p>
                </div>
                <div class="area_img" id="160">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201904033_160.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图14 不同通道位置的选择" src="Detail/GetImg?filename=images/JSJC201904033_160.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图14 不同通道位置的选择</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201904033_160.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="161">图15展示了选择不同电极位置在不同分类器下<i>γ</i>频带的平均识别率。通过对比可看出, 实验1采用C-R-SVM、SVM和KNN 3种不同分类器得到的平均识别率优于实验2～实验4, 平均识别率分别为86.22%、83.25%、78.84%。实验5效果最差并存在较大误差, 这是由于人的情感活动在特定的区域才会活动强烈。因此, 不是所有电极位置都适合进行情感分类, 选择正确的脑部区域和电极位置不仅可以降低计算复杂度, 还可以提高识别精度<citation id="188" type="reference"><link href="39" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>。</p>
                </div>
                <div class="area_img" id="162">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201904033_162.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图15 不同分类器γ频带的平均识别率" src="Detail/GetImg?filename=images/JSJC201904033_162.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图15 不同分类器<i>γ</i>频带的平均识别率</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201904033_162.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="163">目前关于EEG情感识别的研究备受关注, 表2是本文方法与其他方法情感平均识别率和最优识别率的对比。文献<citation id="189" type="reference">[<a class="sup">6</a>]</citation>使用功率谱密度 (Power Spectral Density, PSD) 作为EEG特征, 在SVM下得到的平均识别率和最优识别率分别为82.29%、85.35%。文献<citation id="190" type="reference">[<a class="sup">20</a>]</citation>使用多重去趋势波动分析 (Multifractal Detrended Fluctuation Analysis, MFDFA) 提取EEG特征, 在SVM下得到的平均识别率和最优识别率分别为84.50%、91.62%。文献<citation id="191" type="reference">[<a class="sup">21</a>]</citation>使用差分熵 (Differential Entropy, DE) 作为EEG特征, 用DBN训练和分类得到的平均识别率和最优识别率分别为86.08%、92.35%。文献<citation id="192" type="reference">[<a class="sup">22</a>]</citation>使用不对称指数 (Asymmetric Index, AI) 作为特征, 得到的平均识别率和最优识别率分别为85.39%、90.36%。而本文方法得到的平均识别率和最优识别率分别为86.22%、93.45%, 效果优于其他方法。</p>
                </div>
                <div class="area_img" id="164">
                    <p class="img_tit"><b>表2 不同方法识别率比较</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note">%</p>
                    <table id="164" border="1"><tr><td><br />方法</td><td>分类器</td><td>平均识别率</td><td>最优识别率</td></tr><tr><td><br />文献[5]方法</td><td>SVM</td><td>68.00</td><td>69.60</td></tr><tr><td><br />文献[6]方法</td><td>SVM</td><td>82.29</td><td>85.35</td></tr><tr><td><br />文献[7]方法</td><td>DBN</td><td>72.20</td><td>79.90</td></tr><tr><td><br />文献[20]方法</td><td>SVM</td><td>84.50</td><td>91.62</td></tr><tr><td><br />文献[21]方法</td><td>DBN</td><td>86.08</td><td>92.35</td></tr><tr><td><br />文献[22]方法</td><td>SVM</td><td>85.39</td><td>90.36</td></tr><tr><td><br />本文方法</td><td>C-R-SVM</td><td>86.22</td><td>93.45</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="165">实验证明W-HHT是一种有效的EEG处理方法, 并且C-R-SVM混合情感识别模型可以有效地对情感数据训练与识别。同时, 实验也证明了并非所有的电极位置和EEG频带都适合进行情感识别, 确定合适的频带和电极位置可有效提高情感识别结果。</p>
                </div>
                <h3 id="166" name="166" class="anchor-tag">3 结束语</h3>
                <div class="p1">
                    <p id="167">情感与EEG有直接的相关性。本文采用W-HHT算法处理EEG, 使用C-R-SVM情感模型进行训练与识别。实验结果表明, 该方法获得情感识别率的平均值和最优值分别为86.22%、93.45%, 人脑的情感活动主要分布在额叶和颞叶, 并且<i>γ</i>频带最适合用于情感识别, 这与文献<citation id="193" type="reference">[<a class="sup">23</a>,<a class="sup">24</a>]</citation>的结果相吻合。下一步将对同一个体在不同情况下相同情感的EEG信号以及不同个体相同情感的EEG信号展开研究。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Environmental sound recognition using MP-based features">

                                <b>[1]</b> CHU S, NARAYANAN S, KUO C C J.Environmental sound recognition using MP-based features[C]//Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing.Washington D.C., USA:IEEE Press, 2008:1-4.
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJST&amp;filename=SJSTD6F4115A3957E02901E45BFAF8F7926F&amp;v=MDQyMjh4S3c9TmlmWWVzZSthTlhOcm9vMForSUtDd2s1elI4VDYwcDVUUTJVM1dROUQ3V2RSN3pwQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQ3BiUTM1TjVodzdpMw==&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> CANNON W B.The James-Lange theory of emotions:a critical examination and an alternative theory[J].The American Journal of Psychology, 1987, 100 (3/4) :567-586.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=1017859112.nh&amp;v=MjQ1MDI4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm9GeS9sVzd2TVZGMjZHYnU5RjlETnJaRWJQSVFLREg=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> 陆文娟.基于脑电信号的情感识别研究[D].南京:南京邮电大学, 2017.
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Detection of human emotions using features based on the multiwavelet transform of EEG signals">

                                <b>[4]</b> BAJAJ V, PACHORI R B.Detection of human emotions using features based on the multiwavelet transform of EEG signals[M].Berlin, Germany:Springer, 2015:215-240.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESEFA829C6BAC61CA72A1ED921C81D7469&amp;v=MDYwMzRpV1dyN0pJRnBtYUJ1SFlmT0dRbGZDcGJRMzVONWh3N2kzeEt3PU5pZk9mY2JPYjluT3B2eERGcHA4Q24xS3ZoRVJtejRJUEhiZ3JXRTllTWFUUWJ5V0NPTnZGUw==&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b> ARNAU-GONZÁLEZ P, AREVALILLO-HERRÁEZ M, RAMZAN N.Fusing highly dimensional energy and connectivity features to identify affective states from EEG signals[J].Neurocomputing, 2017, 244:81-89.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=EEG-based emotion recognition in music listening">

                                <b>[6]</b> LIN Y P, WANG C H, JUNG T P.EEG-based emotion recognition in music listening[J].IEEE Transactions on Biomedical Engineering, 2010, 57 (7) :1798-1806.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJHD&amp;filename=SJHD130524016570&amp;v=MzE3NjF1NElEQk04enhVU21EZDlTSDduM3hFOWZidm5LcmlmWnU5dUZDdmpVN2pJS0ZvUU5pZkRhcks3SHRUT3E0OUVZ&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b> KARLSSON L, LOUTFI A.Sleep stage classification using unsupervised feature learning[J].Advances in Artificial Neural Systems, 2012.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Affective state recognition from EEG with deep belief networks">

                                <b>[8]</b> LI K, LI X, ZHANG Y.Affective state recognition from EEG with deep belief networks[C]//Proceedings of IEEE International Conference on Bioinformatics and Biomedicine.Washington D.C., USA:IEEE Press, 2013:305-310.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DGGC200605006&amp;v=Mjk5NjVSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm9GeS9sVzd2TUlTck1iYkc0SHRmTXFvOUZZb1FLREg4NHY=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b> 吴琛, 周瑞忠.Hilbert-Huang变换在提取地震信号动力特性中的应用[J].地震工程与工程振动, 2006, 26 (5) :41-46.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Novel Emotion Elicitation Index Using Frontal Brain Asymmetry for Enhanced EEG-Based Emotion Recognition">

                                <b>[10]</b> PETRANTONAKIS P C, HADJILEONTIADIS L J.A novel emotion elicitation index using frontal brain asymmetry for enhanced EEG-based emotion recognition[J].IEEE Transactions on Information Technology in Biomedicine, 2011, 15 (5) :737-46.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_11" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCE&amp;filename=SJCE13092200024344&amp;v=MzAzNzFRVE1ud1plWnVIeWptVUxuSUoxNGNiaFk9TmlmSWE3SzdIdGpPclk5RlpPa0xEM2c5b0JNVDZUNFBRSC9pclJkR2VycQ==&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[11]</b> XU Y L, CHEN J.Characterizing nonstationary wind speed using empirical mode decomposition[J].Journal of Structural Engineering, 2004, 130 (6) :912-920.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_12" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011601010830&amp;v=MTk1NzBqbVVMbklKMTRjYmhZPU5pZk9mYks3SHRETnFZOUVaT29QQkg4NW9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeQ==&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[12]</b> PENG Z K, TSE P W, CHU F L.An improved Hilbert-Huang transform and its application in vibration signal analysis[J].Journal of Sound and Vibration, 2005, 286 (1/2) :187-205.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_13" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=KJFT201415095&amp;v=MDY4NDhIOVhOcW85TVlZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSb0Z5L2xXN3ZNTGlmTmVyRzQ=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[13]</b> 赵双乐.浅析小波变换理论及其应用[J].科技风, 2014 (15) :106-106.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_14" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MTXB201404033&amp;v=MjUxNjg2ajU0TzN6cXFCdEdGckNVUkxPZVplUm9GeS9sVzd2TUtEblRiTEc0SDlYTXE0OUdaNFFLREg4NHZSNFQ=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[14]</b> 石瑞敏, 杨兆建.基于改进EMD的多绳摩擦提升机载荷信息特征提取[J].煤炭学报, 2014, 39 (4) :782-788.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Speech emotion recognition from spectrograms with deep convolutional neural network">

                                <b>[15]</b> BADSHAH A M, AHMAD J, RAHIM N, et al.Speech emotion recognition from spectrograms with deep convolutional neural network[C]//Proceedings of International Conference on Platform Technology and Service.Washington D.C., USA:IEEE Press, 2017:1-5.
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_16" >
                                    <b>[16]</b>
                                 BENGIO Y, SIMARD P, FRASCONI P.Learning long-term dependencies with gradient descent is difficult[J].IEEE Transactions on Neural Networks, 1994, 5 (2) :157-166.
                            </a>
                        </p>
                        <p id="35">
                            <a id="bibliography_17" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=TXXB201712011&amp;v=MjAyNTh0R0ZyQ1VSTE9lWmVSb0Z5L2xXN3ZNTVRYVGJMRzRIOWJOclk5RVpZUUtESDg0dlI0VDZqNTRPM3pxcUI=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[17]</b> 李幼军, 黄佳进, 王海渊.基于SAE和LSTM RNN的多模态生理信号融合和情感识别研究[J].通信学报, 2017, 38 (12) :109-120.
                            </a>
                        </p>
                        <p id="37">
                            <a id="bibliography_18" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00001339251&amp;v=Mjc5OTBrNXpCZGg0ajk5U1hxUnJ4b3hjTUg3UjdxZWJ1ZHRGQzdsVkwvQUpGbz1OajdCYXJPNEh0SE5ySXhNWnU0T1kz&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[18]</b> CORTES C, VAPNIK V.Support-vector networks[J].Machine Learning, 1995, 20 (3) :273-297.
                            </a>
                        </p>
                        <p id="39">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=EEG-based emotion recognition during watching movies">

                                <b>[19]</b> NIE D, WANG X W, SHI L C, et al.EEG-based emotion recognition during watching movies[C]//Proceedings of International IEEE/EMBS Conference on Neural Engineering.Washington D.C., USA:IEEE Press, 2011:667-670.
                            </a>
                        </p>
                        <p id="41">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=EEG based emotion recognition system using MFDFA as feature extractor">

                                <b>[20]</b> PAUL S, MAZUMDER A, GHOSH P, et al.EEG based emotion recognition system using MFDFA as feature extractor[C]//Proceedings of International Conference on Robotics, Automation, Control and Embedded Systems.Washington D.C., USA:IEEE Press, 2015:1-5.
                            </a>
                        </p>
                        <p id="43">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Investigating Critical Frequency Bands and Channels for EEG-based Emotion Recognition with Deep Neural Networks">

                                <b>[21]</b> ZHENG W L, LU B L.Investigating critical frequency bands and channels for EEG-based emotion recognition with deep neural networks[J].IEEE Transactions on Autonomous Mental Development, 2015, 7 (3) :162-175.
                            </a>
                        </p>
                        <p id="45">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Emotion recognition based on EEG changes in movie viewing">

                                <b>[22]</b> LIU S, MENG J, ZHANG D, et al.Emotion recognition based on EEG changes in movie viewing[C]//Proceedings of International IEEE/EMBS Conference on Neural Engineering.Washington D.C., USA:IEEE Press, 2015:1036-1039.
                            </a>
                        </p>
                        <p id="47">
                            <a id="bibliography_23" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00001444699&amp;v=MDU5MzM3cWVidWR0RkM3bFZML0FKRm89Tmo3QmFyTzRIdEhOcTR0Qll1SUdZM2s1ekJkaDRqOTlTWHFScnhveGNNSDdS&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[23]</b> AFTANAS L I, REVA N V, VARLAMOV A A, et al.Analysis of evoked EEG synchronization and desynchronization in conditions of emotional activation in humans:temporal and topographic characteristics[J].Neuroscience and Behavioral Physiology, 2004, 34 (8) :859-67.
                            </a>
                        </p>
                        <p id="49">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Feature extraction and selection for emotion recognition from EEG">

                                <b>[24]</b> JENKE R, PEER A, BUSS M.Feature extraction and selection for emotion recognition from EEG[J].IEEE Transactions on Affective Computing, 2017, 5 (3) :327-339.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJC201904033" />
        <input id="dpi" type="hidden" value="600" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201904033&amp;v=MTYzNTMzenFxQnRHRnJDVVJMT2VaZVJvRnkvbFc3dk1MejdCYmJHNEg5ak1xNDlHWjRRS0RIODR2UjRUNmo1NE8=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="1" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
