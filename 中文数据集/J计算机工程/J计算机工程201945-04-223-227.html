<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637130615172306250%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJC201904037%26RESULT%3d1%26SIGN%3d4AOhqWiH2c8Lm4sWQLkdr4T%252bv6U%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201904037&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201904037&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201904037&amp;v=MjU0ODFyQ1VSTE9lWmVSb0Z5L2dVci9PTHo3QmJiRzRIOWpNcTQ5R1k0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0Y=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#53" data-title="0 概述 ">0 概述</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#56" data-title="1 相关理论 ">1 相关理论</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#58" data-title="1.1 生成对抗网络">1.1 生成对抗网络</a></li>
                                                <li><a href="#67" data-title="1.2 条件生成对抗网络">1.2 条件生成对抗网络</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#74" data-title="2 基于cGAN的咬翼片图像分割方法 ">2 基于cGAN的咬翼片图像分割方法</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#75" data-title="2.1 损失函数">2.1 损失函数</a></li>
                                                <li><a href="#90" data-title="2.2 模型结构">2.2 模型结构</a></li>
                                                <li><a href="#95" data-title="2.3 数据增强">2.3 数据增强</a></li>
                                                <li><a href="#97" data-title="2.4 网络结构">2.4 网络结构</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#105" data-title="3 实验结果与分析 ">3 实验结果与分析</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#115" data-title="4 结束语 ">4 结束语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#92" data-title="&lt;b&gt;图1 生成器 (&lt;i&gt;G&lt;/i&gt;) 的训练过程&lt;/b&gt;"><b>图1 生成器 (<i>G</i>) 的训练过程</b></a></li>
                                                <li><a href="#94" data-title="&lt;b&gt;图2 判别器 (&lt;i&gt;D&lt;/i&gt;) 的训练过程&lt;/b&gt;"><b>图2 判别器 (<i>D</i>) 的训练过程</b></a></li>
                                                <li><a href="#99" data-title="&lt;b&gt;表1 生成器 (&lt;i&gt;G&lt;/i&gt;) 的网络结构&lt;/b&gt;"><b>表1 生成器 (<i>G</i>) 的网络结构</b></a></li>
                                                <li><a href="#100" data-title="&lt;b&gt;表2 判别器 (&lt;i&gt;D&lt;/i&gt;) 的网络结构&lt;/b&gt;"><b>表2 判别器 (<i>D</i>) 的网络结构</b></a></li>
                                                <li><a href="#104" data-title="&lt;b&gt;图3 生成器网络结构&lt;/b&gt;"><b>图3 生成器网络结构</b></a></li>
                                                <li><a href="#107" data-title="&lt;b&gt;图4 训练样本与分割标签图像&lt;/b&gt;"><b>图4 训练样本与分割标签图像</b></a></li>
                                                <li><a href="#109" data-title="&lt;b&gt;图5 基于cGAN与U-Net的咬翼片X射线图像分割结果&lt;/b&gt;"><b>图5 基于cGAN与U-Net的咬翼片X射线图像分割结果</b></a></li>
                                                <li><a href="#111" data-title="&lt;b&gt;表3 测试集上不同部位分割的性能比较结果&lt;/b&gt;"><b>表3 测试集上不同部位分割的性能比较结果</b></a></li>
                                                <li><a href="#113" data-title="&lt;b&gt;表4 测试集上2个分割模型的性能比较结果&lt;/b&gt;"><b>表4 测试集上2个分割模型的性能比较结果</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" title=" AL-AMRI S S, KALYANKAR N V.Image segmentation by using threshold techniques[EB/OL].[2018-01-16].http://www.arXiv preprint arXiv:1005." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image segmentation by using threshold techniques">
                                        <b>[1]</b>
                                         AL-AMRI S S, KALYANKAR N V.Image segmentation by using threshold techniques[EB/OL].[2018-01-16].http://www.arXiv preprint arXiv:1005.
                                    </a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" title=" KANG J K.Moving region segmentation using parse motion cue from a moving camera[C]//Proceedings of the 12th International Conference Intelligent Autonomous Systems.Berlin, Germany:Springer, 2012:257-264." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Moving region segmentation using parse motion cue from a moving camera">
                                        <b>[2]</b>
                                         KANG J K.Moving region segmentation using parse motion cue from a moving camera[C]//Proceedings of the 12th International Conference Intelligent Autonomous Systems.Berlin, Germany:Springer, 2012:257-264.
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title=" GAUR P.Recognition of 2D barcode images using edge detection and morphological operation[EB/OL].[2018-01-16].https://ijcsmc.com/docs/papers/April2 014/V3I4201499b59.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Recognition of 2D barcode images using edge detection and morphological operation">
                                        <b>[3]</b>
                                         GAUR P.Recognition of 2D barcode images using edge detection and morphological operation[EB/OL].[2018-01-16].https://ijcsmc.com/docs/papers/April2 014/V3I4201499b59.pdf.
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" title=" KINGMA D P, MAX W.Auto-encoding variational Bayes[EB/OL].[2018-01-16].https://arxiv.org/abs/1312.6114." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Auto-encoding variational Bayes">
                                        <b>[4]</b>
                                         KINGMA D P, MAX W.Auto-encoding variational Bayes[EB/OL].[2018-01-16].https://arxiv.org/abs/1312.6114.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" title=" GOODFELLOW I J, POUGET-ABADIE J, MIRZA M, et al.Generative adversarial nets[EB/OL].[2018-01-16].https://arxiv.org/abs/1406.2661." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Generative adversarial networks">
                                        <b>[5]</b>
                                         GOODFELLOW I J, POUGET-ABADIE J, MIRZA M, et al.Generative adversarial nets[EB/OL].[2018-01-16].https://arxiv.org/abs/1406.2661.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" title=" LEDIG C, THEIS L, HUSZAR F, et al.Photo-realistic single image super-resolution using a generative adversarial network[EB/OL].[2018-01-16].https://arxiv.org/abs/1609.04802." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Photo-realistic single image super-resolution using a generative adversarial network">
                                        <b>[6]</b>
                                         LEDIG C, THEIS L, HUSZAR F, et al.Photo-realistic single image super-resolution using a generative adversarial network[EB/OL].[2018-01-16].https://arxiv.org/abs/1609.04802.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" title=" HU Y P, GIBSON E, VERCAUTEREN T, et al.Intraoperative organ motion models with an ensemble of conditional generative adversarial networks[C]//Proceedings of International Conference on Medical Image Computing and Computer-assisted Intervention.Berlin, Germany:Springer, 2017:368-376." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Intraoperative organ motion models with an ensemble of conditional generative adversarial networks">
                                        <b>[7]</b>
                                         HU Y P, GIBSON E, VERCAUTEREN T, et al.Intraoperative organ motion models with an ensemble of conditional generative adversarial networks[C]//Proceedings of International Conference on Medical Image Computing and Computer-assisted Intervention.Berlin, Germany:Springer, 2017:368-376.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" title=" HU Y P, GIBSON E, LEE L L.Freehand ultrasound image simulation with spatially conditioned generative adversarial networks[C]//Proceedings of International Workshop on Reconstruction and Analysis of Moving Body Organs.Berlin, Germany:Springer, 2017:105-115." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Freehand ultrasound image simulation with spatially conditioned generative adversarial networks">
                                        <b>[8]</b>
                                         HU Y P, GIBSON E, LEE L L.Freehand ultrasound image simulation with spatially conditioned generative adversarial networks[C]//Proceedings of International Workshop on Reconstruction and Analysis of Moving Body Organs.Berlin, Germany:Springer, 2017:105-115.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" title=" KOHL S, BONEKAMP D, SCHLEMMER H.Adversarial networks for the detection of aggressive prostate cancer[EB/OL].[2018-01-16].https://arxiv.org/abs/1702.08014." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Adversarial networks for the detection of aggressive prostate cancer">
                                        <b>[9]</b>
                                         KOHL S, BONEKAMP D, SCHLEMMER H.Adversarial networks for the detection of aggressive prostate cancer[EB/OL].[2018-01-16].https://arxiv.org/abs/1702.08014.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_10" title=" TAN P L, EVANS R W, MORGAN M V.Caries, bitewings, and treatment decisions[J].Australian Dental Journal, 2002, 47 (2) :138-141." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJWD&amp;filename=SJWD00000227378&amp;v=MjgzNzFIWTNrNXpCZGg0ajk5U1hxUnJ4b3hjTUg3UjdxZWJ1ZHRGQzdsVkxySklGZz1OaWZjYXJPNEh0SE1yWTFDWit3&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[10]</b>
                                         TAN P L, EVANS R W, MORGAN M V.Caries, bitewings, and treatment decisions[J].Australian Dental Journal, 2002, 47 (2) :138-141.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_11" title=" 臧晶, 宋凯.基于多分辨率分析理论的口腔CT图像的增强[J].沈阳电力高等专科学校学报, 2004, 6 (3) :48-50." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=SYDL200403017&amp;v=MDUxNjhIdFhNckk5RVk0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSb0Z5L2dVci9PTmpUUFlyRzQ=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[11]</b>
                                         臧晶, 宋凯.基于多分辨率分析理论的口腔CT图像的增强[J].沈阳电力高等专科学校学报, 2004, 6 (3) :48-50.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_12" title=" 杨玲, 王中科, 王云鹏.牙缝约束下模糊连接法对单颗牙齿的分割[J].计算机工程与设计, 2009, 30 (21) :5031-5036." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=SJSJ200921069&amp;v=MTY2MDJDVVJMT2VaZVJvRnkvZ1VyL09OaWZZWkxHNEh0ak9ybzlEYllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnI=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[12]</b>
                                         杨玲, 王中科, 王云鹏.牙缝约束下模糊连接法对单颗牙齿的分割[J].计算机工程与设计, 2009, 30 (21) :5031-5036.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_13" title=" LIN P L, HUANG P W, CHO Y S, et al.An automatic and effective tooth isolation method for dental radiographs[J].Opto-electronics Review, 2013, 21 (1) :126-136." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD130122003664&amp;v=MTk3ODJLN0h0RE9yWTlGWiswSkNCTTh6eFVTbURkOVNIN24zeEU5ZmJ2bktyaWZadTl1RkN2alU3ak5JVjRTTmo3QmFy&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[13]</b>
                                         LIN P L, HUANG P W, CHO Y S, et al.An automatic and effective tooth isolation method for dental radiographs[J].Opto-electronics Review, 2013, 21 (1) :126-136.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_14" title=" 莫宏兵, 赵刚.基于细菌模糊聚类算法的牙齿图像分割技术研究[J].黑龙江医药科学, 2014, 34 (1) :108-109." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=KXJY201101078&amp;v=MzAzOTJPM3pxcUJ0R0ZyQ1VSTE9lWmVSb0Z5L2dVci9PTGpYQmQ3RzRIOURNcm85Q2JJUUtESDg0dlI0VDZqNTQ=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[14]</b>
                                         莫宏兵, 赵刚.基于细菌模糊聚类算法的牙齿图像分割技术研究[J].黑龙江医药科学, 2014, 34 (1) :108-109.
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_15" title=" RONNEBERGER O, FISCHER P, BROX T.Dental X-ray image segmentation using a U-shaped deep convolutional network[EB/OL].[2018-01-16].http://www-o.ntust.edu.tw/～cweiwang/ISBI2015/challen ge2/isbi2015_Ronneberger.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Dental X-ray image segmentation using a U-shaped deep convolutional network">
                                        <b>[15]</b>
                                         RONNEBERGER O, FISCHER P, BROX T.Dental X-ray image segmentation using a U-shaped deep convolutional network[EB/OL].[2018-01-16].http://www-o.ntust.edu.tw/～cweiwang/ISBI2015/challen ge2/isbi2015_Ronneberger.pdf.
                                    </a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_16" title=" SUTSKEVER I, JOZEFOWICZ R, GREGOR K, et al.Towards principled unsupervised learning[EB/OL].[2018-01-16].https://arxiv.org/abs/1511.06440" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Towards principled unsupervised learning">
                                        <b>[16]</b>
                                         SUTSKEVER I, JOZEFOWICZ R, GREGOR K, et al.Towards principled unsupervised learning[EB/OL].[2018-01-16].https://arxiv.org/abs/1511.06440
                                    </a>
                                </li>
                                <li id="35">


                                    <a id="bibliography_17" title=" GOODFELLOW I, BENGIO Y, COURVILLE A.Deep learning[M].Cambridge, UK:MIT Press, 2016." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep learning">
                                        <b>[17]</b>
                                         GOODFELLOW I, BENGIO Y, COURVILLE A.Deep learning[M].Cambridge, UK:MIT Press, 2016.
                                    </a>
                                </li>
                                <li id="37">


                                    <a id="bibliography_18" title=" RATLIff L J, BURDEN S A, SASTRY S S.Characterization and computation of local Nash equilibria in continuous games[C]//Proceedings of the 51st Annual Allerton Con-ference on Communication, Control, and Computing.Monticello, USA:IEEE Press, 2013:917-924." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Characterization and computation of local Nash equilibria in continuous games">
                                        <b>[18]</b>
                                         RATLIff L J, BURDEN S A, SASTRY S S.Characterization and computation of local Nash equilibria in continuous games[C]//Proceedings of the 51st Annual Allerton Con-ference on Communication, Control, and Computing.Monticello, USA:IEEE Press, 2013:917-924.
                                    </a>
                                </li>
                                <li id="39">


                                    <a id="bibliography_19" title=" MIRZA M, OSINDERO S.Conditional generative adversarial nets[EB/OL].[2018-01-16].https://arxiv.org/abs/1411.1784." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Conditional generative adversarial nets">
                                        <b>[19]</b>
                                         MIRZA M, OSINDERO S.Conditional generative adversarial nets[EB/OL].[2018-01-16].https://arxiv.org/abs/1411.1784.
                                    </a>
                                </li>
                                <li id="41">


                                    <a id="bibliography_20" title=" RADFORD A, METZ L, CHINTALA S.Unsupervised representation learning with deep convolutional generative adversarial networks[EB/OL].[2018-01-16].https://arxiv.org/abs/1511.06434." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Unsupervised representation learning with deep convolutional generative adversarial networks[Online]">
                                        <b>[20]</b>
                                         RADFORD A, METZ L, CHINTALA S.Unsupervised representation learning with deep convolutional generative adversarial networks[EB/OL].[2018-01-16].https://arxiv.org/abs/1511.06434.
                                    </a>
                                </li>
                                <li id="43">


                                    <a id="bibliography_21" title=" KINGMA D, BA J.Adam:a method for stochastic optimization[EB/OL].[2018-01-16].https://arxiv.org/abs/1412.6980v8." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Adam:amethodforstochasticoptimization[OL]">
                                        <b>[21]</b>
                                         KINGMA D, BA J.Adam:a method for stochastic optimization[EB/OL].[2018-01-16].https://arxiv.org/abs/1412.6980v8.
                                    </a>
                                </li>
                                <li id="45">


                                    <a id="bibliography_22" title=" SALIMANS T, GOODFELLOW I, ZAREMBA W, et al.Improved techniques for training gans[EB/OL].[2018-01-16].https://arxiv.org/abs/1606.03498." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Improved techniques for training gans">
                                        <b>[22]</b>
                                         SALIMANS T, GOODFELLOW I, ZAREMBA W, et al.Improved techniques for training gans[EB/OL].[2018-01-16].https://arxiv.org/abs/1606.03498.
                                    </a>
                                </li>
                                <li id="47">


                                    <a id="bibliography_23" title=" GULRAJANI I, AHMED F, ARJOVSKY M, et al.Improved training of wasserstein GANs[EB/OL].[2018-01-16].https://arxiv.org/abs/1704.00028v3." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Improved training of wasserstein GANs">
                                        <b>[23]</b>
                                         GULRAJANI I, AHMED F, ARJOVSKY M, et al.Improved training of wasserstein GANs[EB/OL].[2018-01-16].https://arxiv.org/abs/1704.00028v3.
                                    </a>
                                </li>
                                <li id="49">


                                    <a id="bibliography_24" title=" RONNEBERGER O, FISCHER P, BROX T.U-net:convolutional networks for biomedical image segmenta-tion[C]//Proceedings of MICCAI’15.Berlin, Germany:Springer, 2015:234-241." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=U-Net:Convolutional Networks for Biomedical Image Segmentation">
                                        <b>[24]</b>
                                         RONNEBERGER O, FISCHER P, BROX T.U-net:convolutional networks for biomedical image segmenta-tion[C]//Proceedings of MICCAI’15.Berlin, Germany:Springer, 2015:234-241.
                                    </a>
                                </li>
                                <li id="51">


                                    <a id="bibliography_25" title=" PATHAK D, KRAHENBUHL P, DONAHUE J, et al.Context encoders:feature learning by inpainting[EB/OL].[2018-01-16].https://arxiv.org/abs/1604.07379." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Context encoders:feature learning by inpainting">
                                        <b>[25]</b>
                                         PATHAK D, KRAHENBUHL P, DONAHUE J, et al.Context encoders:feature learning by inpainting[EB/OL].[2018-01-16].https://arxiv.org/abs/1604.07379.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJC" target="_blank">计算机工程</a>
                2019,45(04),223-227 DOI:10.19678/j.issn.1000-3428.0050445            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于条件生成对抗网络的咬翼片图像分割</b></span>
 <span class="shoufa"></span>                                     </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%92%8B%E8%8A%B8&amp;code=09140808&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">蒋芸</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%B0%AD%E5%AE%81&amp;code=39136091&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">谭宁</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BC%A0%E6%B5%B7&amp;code=15344374&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">张海</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BD%AD%E5%A9%B7%E5%A9%B7&amp;code=39136092&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">彭婷婷</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E8%A5%BF%E5%8C%97%E5%B8%88%E8%8C%83%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0012645&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">西北师范大学计算机科学与工程学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>现有基于U型网络 (U-Net) 的咬翼片图像分割方法将咬翼片X射线图像分割成龋齿、牙釉质、牙本质、牙髓、牙冠、修复体和牙根管7个部分, 但分割准确率偏低。为此, 提出一种改进的咬翼片图像分割方法, 将条件生成对抗网络与U-Net相结合对咬翼片进行分割, 使判别器与生成器相互优化, 获得具有更多上下文信息的分割特征图。实验结果表明, 改进方法的Dice系数相比U-Net方法提升了0.133, 分割准确率更高。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">生成对抗网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">图像分割;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=U%E5%9E%8B%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">U型网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">数据增强;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    蒋芸 (1970—) , 女, 教授、博士, 主研方向为图像分割、数据挖掘、粗糙集理论及应用;;
                                </span>
                                <span>
                                    谭宁, 硕士研究生, E-mail:tanning2315@126.com。;
                                </span>
                                <span>
                                    张海, 硕士研究生。;
                                </span>
                                <span>
                                    彭婷婷, 硕士研究生。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-02-07</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金 (61163036);</span>
                                <span>甘肃省自然科学基金 (1606RJZA047);</span>
                                <span>甘肃省高校研究生导师科研项目 (1201-16);</span>
                                <span>西北师范大学第三期“知识与创新工程”科研骨干项目 (nwnu-kjcxgc-03-67);</span>
                    </p>
            </div>
                    <h1><b>Bitewing Radiography Image Segmentation Based on Conditional Generative Adversarial Network</b></h1>
                    <h2>
                    <span>JIANG Yun</span>
                    <span>TAN Ning</span>
                    <span>ZHANG Hai</span>
                    <span>PENG Tingting</span>
            </h2>
                    <h2>
                    <span>School of Computer Science and Engineering, Northwest Normal University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>The existing bitewing radiography image segmentation method based on U-Net divides the X-ray image of the bitewing radiography into caries, enamel, dentin, pulp, crown, prosthesis and root canal, but the segmentation accuracy is low.So, this paper proposes an improved method to segmentation bitewing radiograpy images.The conditional Generative Adversarial Network (cGAN) combined with U-Net to segmentation the bitewing radiograpy images.It optimizes the discriminator and the generator to obtain a segmentation feature map with more context information.Experimental results show that the Dice coefficient of the improved method is improved by 0.133 compared to the U-Net method, and the segmentation accuracy is higher.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Generative%20Adversarial%20Network%20(GAN)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Generative Adversarial Network (GAN) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=image%20segmentation&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">image segmentation;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=deep%20leaning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">deep leaning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=U-Net&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">U-Net;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=data%20enhancement&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">data enhancement;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-02-07</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="53" name="53" class="anchor-tag">0 概述</h3>
                <div class="p1">
                    <p id="54">图像分割是指根据灰度、颜色、纹理和形状等特征将图像划分成若干互不交迭的区域, 并使这些特征在同一区域内呈现出相似性, 而在不同区域间呈现出明显的差异性。常用的图像分割方法有阈值分割<citation id="117" type="reference"><link href="3" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>、区域分割<citation id="118" type="reference"><link href="5" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>、边缘检测分割<citation id="119" type="reference"><link href="7" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>等。医学图像分割一直是生物医学工程领域的研究热点, 对于人体的不同组织和器官的分割问题有不同方法。近年来, 无监督学习成为研究热点, 变分自编码器<citation id="120" type="reference"><link href="9" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>、生成对抗网络 (Generative Adversarial Network, GAN) <citation id="121" type="reference"><link href="11" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>等无监督模型受到越来越多的关注。在人工智能高速发展的时代, GAN的提出不仅满足了相关领域的研究和应用需求, 也带来了新的发展动力。特别是在图像和视觉领域中对GAN的研究和应用较为广泛, 已经可以通过随机数字生成人脸、从低分辨率图像生成高分辨率图像<citation id="122" type="reference"><link href="13" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>等。此外, GAN已经开始逐渐应用到医学图像处理中, 例如通过术前磁共振图像直接生成病人特定的超声探头, 从而诱导前列腺运动的模型<citation id="123" type="reference"><link href="15" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>, 在超声探头的空间位置上有条件地对解剖学图像进行采样<citation id="124" type="reference"><link href="17" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>, 检测恶性前列腺癌<citation id="125" type="reference"><link href="19" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>等问题的研究中。</p>
                </div>
                <div class="p1">
                    <p id="55">由于龋齿会破坏牙齿结构传播细菌性疾病, 因此特别是对于儿童, 龋齿会影响儿童的发育, 对颌面部造成影响, 龋洞会破坏牙胚。牙科医生主要根据咬翼片X射线图像来诊断和治疗龋齿。自动化龋齿病变检测技术为牙科医生提供潜在的诊断数据, 并有助于识别各种疾病的迹象<citation id="126" type="reference"><link href="21" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>。目前, 牙科鉴定过程为手工进行, 非常耗时且主观性差。为解决该问题, 文献<citation id="127" type="reference">[<a class="sup">11</a>]</citation>提出基于小波变换的图像多分辨率梯度信息的牙齿图像分割方法。文献<citation id="128" type="reference">[<a class="sup">12</a>]</citation>在牙缝约束下利用模糊连接法对单颗牙齿进行分割。文献<citation id="129" type="reference">[<a class="sup">13</a>]</citation>建立一种自动且高效的牙齿隔离方法, 能够实现上下颌分离、单齿隔离、过分割验证和下分割检测。文献<citation id="130" type="reference">[<a class="sup">14</a>]</citation>研究了一种基于细菌优化的模糊聚类算法, 用于牙齿图像分割。文献<citation id="131" type="reference">[<a class="sup">15</a>]</citation>使用U-Net对咬翼片进行分割, 将咬翼片X射线图像分割成龋齿、牙釉质、牙本质、牙髓、牙冠、修复体和牙根管7个部分, 但是分割准确率偏低。本文在上述研究的基础上, 引入条件生成对抗网络对咬翼片X射线进行分割, 由于判别器有无限的建模能力, 因此不断对生成器分割结果与标准分割图像的差异进行判断, 生成器和判别器互相对各自的模型进行优化, 使得生成器分割结果无限接近标准分割图像。</p>
                </div>
                <h3 id="56" name="56" class="anchor-tag">1 相关理论</h3>
                <div class="p1">
                    <p id="57">近年来, 生成对抗网络受到了越来越多研究人员的广泛关注, 成为无监督<citation id="132" type="reference"><link href="31" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>复杂概率分布学习的主流方法。生成器 (<i>G</i>) 和判别器 (<i>D</i>) 采用当前热门的深度神经网络<citation id="133" type="reference"><link href="33" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>。</p>
                </div>
                <h4 class="anchor-tag" id="58" name="58">1.1 生成对抗网络</h4>
                <div class="p1">
                    <p id="59">生成对抗网络由2个神经网络组成, 包含生成器 (<i>G</i>) 和判别器 (<i>D</i>) 。生成器 (<i>G</i>) 的目的是生成近似真实数据分布<i>p</i><sub>data</sub>的样本来欺骗判别器 (<i>D</i>) , 使判别器 (<i>D</i>) 无法区分输入的数据来自真实数据<i>p</i><sub>data</sub>还是生成器 (<i>G</i>) 。判别器 (<i>D</i>) 的目的是正确区分数据样本来自真实的数据分布<i>p</i><sub>data</sub>还是生成器 (<i>G</i>) , 当输入的数据来自真实的数据分布<i>p</i><sub>data</sub>时, 判别器 (<i>D</i>) 的目标是使输出的概率<i>D</i> (<i>x</i>) ≈1, 当输入的数据来自生成器 (<i>G</i>) 时, 判别器 (<i>D</i>) 的目标是使<i>D</i> (<i>G</i> (<i>z</i>) ) ≈0, 同时生成器 (<i>G</i>) 的目标是使<i>D</i> (<i>G</i> (<i>z</i>) ) ≈1。2个网络在一个极小-极大的游戏中相互迭代、相互竞争优化各自的网络参数, 使目标函数达到那什均衡<citation id="134" type="reference"><link href="35" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>, 当收敛时, 期望<i>p</i><sub>data</sub>=<i>p</i><sub><i>g</i></sub>, 其中<i>p</i><sub><i>g</i></sub>是生成器 (<i>G</i>) 生成的数据样本分布。本文使用以下目标函数优化生成器的参数 (<i>θ</i><sub><i>G</i></sub>) 和判别器的参数 (<i>θ</i><sub><i>D</i></sub>) :</p>
                </div>
                <div class="p1">
                    <p id="60"><mathml id="61"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mi>D</mi></munder></mrow><mtext> </mtext><mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mi>G</mi></munder></mrow><mo stretchy="false"> (</mo><mi>θ</mi><msub><mrow></mrow><mi>G</mi></msub><mo>, </mo><mi>θ</mi><msub><mrow></mrow><mi>D</mi></msub><mo stretchy="false">) </mo><mo>=</mo></mrow></math></mathml><image href="images/JSJC201904037_062.jpg" type="" display="inline" placement="inline"><alt></alt></image><sub><i>x</i>～<i>p</i><sub>data</sub></sub>[lg <i>D</i> (<i>x</i>) ]+<image href="images/JSJC201904037_064.jpg" type="" display="inline" placement="inline"><alt></alt></image><sub><i>z</i>～<i>p</i><sub><i>z</i></sub> (<i>z</i>) </sub>[lg (1-<i>D</i> (<i>G</i> (<i>z</i>) ) ) ]      (1) </p>
                </div>
                <div class="p1">
                    <p id="65">其中, <i>x</i>取自于真实的数据分布<i>p</i><sub>data</sub>, <i>z</i>取自于先验分布<i>p</i><sub><i>z</i></sub> (<i>z</i>) (例如正态分布) , <image href="images/JSJC201904037_066.jpg" type="" display="inline" placement="inline"><alt></alt></image> (·) 表示计算期望值。</p>
                </div>
                <h4 class="anchor-tag" id="67" name="67">1.2 条件生成对抗网络</h4>
                <div class="p1">
                    <p id="68">条件生成对抗网络 (condition GAN, cGAN) <citation id="135" type="reference"><link href="37" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>是生成器 (<i>G</i>) 和判别器 (<i>D</i>) 都输入一个额外的信息<i>y</i>作为条件, <i>y</i>可以是任何类型的辅助信息 (例如手写体, <i>y</i>可以表示为数字类别) 。额外辅助信息<i>y</i>与样本数据<i>x</i>进行拼接后输入到生成器 (<i>G</i>) 和判别器 (<i>D</i>) , 使生成对抗网络扩展为条件模型, 目标函数转换为:</p>
                </div>
                <div class="p1">
                    <p id="69"><mathml id="70"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mi>G</mi></munder></mrow><mtext> </mtext><mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mi>D</mi></munder></mrow><mtext> </mtext><mtext>L</mtext><mo stretchy="false"> (</mo><mi>D</mi><mo>, </mo><mi>G</mi><mo stretchy="false">) </mo><mo>=</mo></mrow></math></mathml><image href="images/JSJC201904037_071.jpg" type="" display="inline" placement="inline"><alt></alt></image><sub><i>x</i>～<i>p</i><sub>data</sub> (<i>x</i>) </sub>[lg <i>D</i> (<i>x</i>|<i>y</i>) ]+<image href="images/JSJC201904037_073.jpg" type="" display="inline" placement="inline"><alt></alt></image><sub><i>z</i>～<i>p</i><sub><i>z</i></sub> (<i>z</i>) </sub>[lg (1-<i>D</i> (<i>G</i> (<i>x</i>|<i>y</i>) ) ) ]      (2) </p>
                </div>
                <h3 id="74" name="74" class="anchor-tag">2 基于cGAN的咬翼片图像分割方法</h3>
                <h4 class="anchor-tag" id="75" name="75">2.1 损失函数</h4>
                <div class="p1">
                    <p id="76">生成对抗网络的生成模型需要学习一个从随机噪声z到输出图像y之间的映射关系, G:z→y。条件生成对抗网络<citation id="136" type="reference"><link href="37" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>需要学习图片x和随机噪声z到输出图像y之间的映射, G:{x, z}→y。</p>
                </div>
                <div class="p1">
                    <p id="77">实验中所用的条件生成对抗网络的目标函数描述如下:</p>
                </div>
                <div class="p1">
                    <p id="78">L<sub>cGAN</sub> (<i>G</i>, <i>D</i>) =<image href="images/JSJC201904037_079.jpg" type="" display="inline" placement="inline"><alt></alt></image><sub><i>x</i>～<i>p</i><sub>data</sub> (<i>x</i>) , <i>y</i></sub>[lg <i>D</i> (<i>x</i>, <i>y</i>) ]+<image href="images/JSJC201904037_081.jpg" type="" display="inline" placement="inline"><alt></alt></image><sub><i>x</i>～<i>p</i><sub>data</sub> (<i>x</i>) , <i>z</i>～<i>p</i><sub><i>z</i></sub> (<i>z</i>) </sub>[lg (1-<i>D</i> (<i>x</i>, <i>G</i> (<i>x</i>, <i>z</i>) ) ) ]      (3) </p>
                </div>
                <div class="p1">
                    <p id="82">其中, <i>x</i>为输入的图片样本数据, <i>y</i>是通过人工标注的标准分割图像, 生成器 (<i>G</i>) 尝试最小化目标函数, 判别器 (<i>D</i>) 尝试最大化目标函数:</p>
                </div>
                <div class="p1">
                    <p id="83" class="code-formula">
                        <mathml id="83"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>G</mi><msup><mrow></mrow><mo>*</mo></msup><mo>=</mo><mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mi>G</mi></munder></mrow><mtext> </mtext><mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mi>D</mi></munder></mrow><mtext> </mtext><mtext>L</mtext><msub><mrow></mrow><mrow><mtext>c</mtext><mtext>G</mtext><mtext>A</mtext><mtext>Ν</mtext></mrow></msub><mo stretchy="false"> (</mo><mi>G</mi><mo>, </mo><mi>D</mi><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="84">通过实验发现将cGAN的目标函数与传统损失函数 (像<i>L</i><sub>1</sub>距离) 相结合的分割效果会更好, <i>L</i><sub>1</sub>的距离函数如下:</p>
                </div>
                <div class="p1">
                    <p id="85">L<sub><i>L</i><sub>1</sub></sub> (<i>G</i>) =<image href="images/JSJC201904037_086.jpg" type="" display="inline" placement="inline"><alt></alt></image><sub><i>x</i>～<i>p</i><sub>data</sub> (<i>x</i>) , <i>y</i>, <i>z</i>～<i>p</i><sub><i>z</i></sub> (<i>z</i>) </sub>[‖<i>y</i>-<i>G</i> (<i>x</i>, <i>z</i>) ‖]      (4) </p>
                </div>
                <div class="p1">
                    <p id="87">其中, <i>y</i>是分割的目标图像, <i>G</i> (<i>x</i>, <i>z</i>) 是训练样本数据输入生成器中生成的分割图像。最终目标函数变换为:</p>
                </div>
                <div class="p1">
                    <p id="88" class="code-formula">
                        <mathml id="88"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>G</mi><msup><mrow></mrow><mo>*</mo></msup><mo>=</mo><mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mtext>a</mtext><mtext>r</mtext><mtext>g</mtext><mtext>m</mtext><mtext>i</mtext><mtext>n</mtext></mrow></mstyle><mi>G</mi></munder></mrow><mtext> </mtext><mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mi>D</mi></munder></mrow><mtext> </mtext><mtext>L</mtext><msub><mrow></mrow><mrow><mtext>c</mtext><mtext>G</mtext><mtext>A</mtext><mtext>Ν</mtext></mrow></msub><mo stretchy="false"> (</mo><mi>G</mi><mo>, </mo><mi>D</mi><mo stretchy="false">) </mo><mo>+</mo><mi>λ</mi><mtext>L</mtext><msub><mrow></mrow><mrow><mi>L</mi><msub><mrow></mrow><mn>1</mn></msub></mrow></msub><mo stretchy="false"> (</mo><mi>G</mi><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>5</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="89">由于<i>L</i><sub>1</sub>距离函数会产生模糊化效果, 因此通过加入超参<i>λ</i>进行控制。当生成器能够对输入的咬翼片X射线图像进行准确分割时, 对参数的改变较敏感, 超参<i>λ</i>的选择非常重要, 因此设置超参<i>λ</i>=100较合理。</p>
                </div>
                <h4 class="anchor-tag" id="90" name="90">2.2 模型结构</h4>
                <div class="p1">
                    <p id="91">生成器 (G) 的训练过程 (见图1) 分为2个部分:1) 输入一张咬翼片<i>X</i>射线图像到生成器 (G) , 生成器 (G) 输出一张分割后的图像, 比较生成器 (G) 输出的分割图像和人工标注的标准分割图像 (<i>Ground truth</i>) 之间的误差, 通过误差调整生成器 (G) 的权重。2) 输入一对假图像 (咬翼片<i>X</i>射线图像和生成器通过该咬翼片生成的分割图像) 到判别器 (D) 中, 由于生成器 (G) 是生成一张无限接近于目标的分割图像, 使判别器 (D) 误认为生成器 (G) 输出的分割图像是人工标注的标注图像, 期待判别器输出的结果为“1”。比较判别器 (D) 输出的结果与标准的正确结果“1”之间的误差, 从而优化生成器 (G) 的权重, 使其生成的分割图像更加接近目标分割图像。</p>
                </div>
                <div class="area_img" id="92">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201904037_092.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 生成器 (G) 的训练过程" src="Detail/GetImg?filename=images/JSJC201904037_092.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图1 生成器 (<i>G</i>) 的训练过程</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201904037_092.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="93">判别器 (<i>D</i>) 的训练过程 (见图2) 同样分为2个部分:1) 输入一对真实的图像对 (咬翼片X射线图像和人工标注的标准目标分割图像) 到判别器 (<i>D</i>) , 判别器 (<i>D</i>) 输出判别后的结果。由于已知输入的是真实的数据样本, 因此判别器 (<i>D</i>) 输出的结果应该为“1”, 通过比较判别器 (<i>D</i>) 输出的结果与标准答案“1”之间的差值, 然后优化判别器 (<i>D</i>) 的权重。2) 输入一对假图像对 (咬翼片X射线图像和生成器输出的分割图像) 到判别器 (<i>D</i>) 中, 已知输入的数据样本为假, 所以判别器 (<i>D</i>) 输出的结果应该为“0”, 比较判别器 (<i>D</i>) 输出的结果与标准答案“0”之间的差值, 然后调整判别器 (<i>D</i>) 的权重, 使判别器 (<i>D</i>) 能够正确区分图像是人工标注的标准分割图像还是由生成器 (<i>G</i>) 生成的分割图像。</p>
                </div>
                <div class="area_img" id="94">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201904037_094.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 判别器 (D) 的训练过程" src="Detail/GetImg?filename=images/JSJC201904037_094.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图2 判别器 (<i>D</i>) 的训练过程</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201904037_094.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="95" name="95">2.3 数据增强</h4>
                <div class="p1">
                    <p id="96">由于只能提供少量咬翼片X射线样本图像对网络结构进行训练, 但深度神经网络训练大量参数需要大量的训练样本, 会造成数据饥饿, 因此本文通过数据增强的方式对训练样本进行扩充, 从而解决该问题。数据增强对于提高网络分类准确率、鲁棒性以及防止过拟合至关重要。通过对图像旋转、水平翻转、垂直翻转、平移变换、图像灰度值的变化等方法对训练数据进行增强。</p>
                </div>
                <h4 class="anchor-tag" id="97" name="97">2.4 网络结构</h4>
                <div class="p1">
                    <p id="98">本文基于深度卷积对抗神经网络 (DCGAN) <citation id="137" type="reference"><link href="39" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>和条件生成对抗网络进行实现, 使用DCGAN中推荐的训练参数进行训练, 训练时使用Adam优化算法<citation id="138" type="reference"><link href="41" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation> (<i>β</i><sub>1</sub>=0.5、<i>β</i><sub>2</sub>=0.999、<i>ε</i>=10<sup>-8</sup>) , 学习率<i>lr</i>=0.000 2, <i>mini</i>-<i>batch</i>=2, 每层输出结果进行批量归一化, 从而减少每层之间的依赖性, 提高各网络层之间的独立性<citation id="139" type="reference"><link href="43" rel="bibliography" /><link href="45" rel="bibliography" /><sup>[<a class="sup">21</a>,<a class="sup">22</a>]</sup></citation>。训练20个周期, 输出的分割图像大小为256像素×256像素, 生成器 (<i>G</i>) 和判别器 (<i>D</i>) 具体的网络结构如表1、表2所示。</p>
                </div>
                <div class="area_img" id="99">
                    <p class="img_tit"><b>表1 生成器 (<i>G</i>) 的网络结构</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="99" border="1"><tr><td>操作</td><td>核大小</td><td>步幅</td><td>卷积核数</td><td>归一化</td><td>激活函数</td></tr><tr><td><br /><i>e</i><sub>1</sub>:卷积</td><td>5×5</td><td>2×2</td><td>64</td><td>是</td><td>Leaky ReLU</td></tr><tr><td><br /><i>e</i><sub>2</sub>:卷积</td><td>5×5</td><td>2×2</td><td>128</td><td>是</td><td>Leaky ReLU</td></tr><tr><td><br /><i>e</i><sub>3</sub>:卷积</td><td>5×5</td><td>2×2</td><td>256</td><td>是</td><td>Leaky ReLU</td></tr><tr><td><br /><i>e</i><sub>4</sub>:卷积</td><td>5×5</td><td>2×2</td><td>512</td><td>是</td><td>Leaky ReLU</td></tr><tr><td><br /><i>e</i><sub>5</sub>:卷积</td><td>5×5</td><td>2×2</td><td>512</td><td>是</td><td>Leaky ReLU</td></tr><tr><td><br /><i>e</i><sub>6</sub>:卷积</td><td>5×5</td><td>2×2</td><td>512</td><td>是</td><td>Leaky ReLU</td></tr><tr><td><br /><i>e</i><sub>7</sub>:卷积</td><td>5×5</td><td>2×2</td><td>512</td><td>是</td><td>Leaky ReLU</td></tr><tr><td><br /><i>e</i><sub>8</sub>:卷积</td><td>5×5</td><td>2×2</td><td>512</td><td>是</td><td>Leaky ReLU</td></tr><tr><td><br /><i>d</i><sub>1</sub>:反卷积</td><td>5×5</td><td>2×2</td><td>512+512</td><td>是</td><td>ReLU</td></tr><tr><td><br /><i>d</i><sub>2</sub>:反卷积</td><td>5×5</td><td>2×2</td><td>512+512</td><td>是</td><td>ReLU</td></tr><tr><td><br /><i>d</i><sub>3</sub>:反卷积</td><td>5×5</td><td>2×2</td><td>512+512</td><td>是</td><td>ReLU</td></tr><tr><td><br /><i>d</i><sub>4</sub>:反卷积</td><td>5×5</td><td>2×2</td><td>512+512</td><td>是</td><td>ReLU</td></tr><tr><td><br /><i>d</i><sub>5</sub>:反卷积</td><td>5×5</td><td>2×2</td><td>256+256</td><td>是</td><td>ReLU</td></tr><tr><td><br /><i>d</i><sub>6</sub>:反卷积</td><td>5×5</td><td>2×2</td><td>128+128</td><td>是</td><td>ReLU</td></tr><tr><td><br /><i>d</i><sub>7</sub>:反卷积</td><td>5×5</td><td>2×2</td><td>64+64</td><td>是</td><td>ReLU</td></tr><tr><td><br /><i>d</i><sub>8</sub>:反卷积</td><td>5×5</td><td>2×2</td><td>3</td><td>—</td><td>ReLU</td></tr><tr><td><br />全连接</td><td>—</td><td>—</td><td>3</td><td>—</td><td>tanh</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="100">
                    <p class="img_tit"><b>表2 判别器 (<i>D</i>) 的网络结构</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="100" border="1"><tr><td>操作</td><td>核大小</td><td>步幅</td><td>卷积核数</td><td>归一化</td><td>激活函数</td></tr><tr><td><br /><i>h</i><sub>0</sub>:卷积</td><td>5×5</td><td>2×2</td><td>64</td><td>是</td><td>Leaky ReLU</td></tr><tr><td><br /><i>h</i><sub>1</sub>:卷积</td><td>5×5</td><td>2×2</td><td>128</td><td>是</td><td>Leaky ReLU</td></tr><tr><td><br /><i>h</i><sub>2</sub>:卷积</td><td>5×5</td><td>2×2</td><td>256</td><td>是</td><td>Leaky ReLU</td></tr><tr><td><br /><i>h</i><sub>3</sub>:卷积</td><td>5×5</td><td>2×2</td><td>512</td><td>是</td><td>Leaky ReLU</td></tr><tr><td><br /><i>h</i><sub>4</sub>:全连接</td><td>—</td><td>—</td><td>16×16×512</td><td>否</td><td>—</td></tr><tr><td><br />全连接</td><td>—</td><td>—</td><td>1</td><td>否</td><td>Sigmoid</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="101">生成器中使用U-Net网络结构<citation id="140" type="reference"><link href="47" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>, 在<i>d</i><sub>1</sub>、<i>d</i><sub>2</sub>、<i>d</i><sub>3</sub> 3层使用<i>dropout</i>, 每层随机删除50%的节点以防止过拟合。</p>
                </div>
                <div class="p1">
                    <p id="102">现有分割方法的网络结构多数使用编码器-解码器<citation id="141" type="reference"><link href="49" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>, 该网络结构通过向下采样, 逐渐降低采样层, 直到达到一个瓶颈层, 将提取的信息变为一个一维向量, 此时过程被逆转, 逐渐向上采样, 最后还原成图像。该网络结构要求所有的信息流通过所有的网络层, 包括瓶颈层。在许多图像翻译应用中, 输入和输出之间共享大量低级信息, 但通常期望这些信息能直接通过网络。</p>
                </div>
                <div class="p1">
                    <p id="103">为使生成器 (<i>G</i>) 能够避免出现信息瓶颈, 本文使用跳远连接, 遵循U-Net网络结构, 具体操作是将第<i>i</i>层和第<i>n</i>-<i>i</i>层进行跳远连接, 每个跳远连接只是简单地将第<i>i</i>层网络输出的所有通道和第<i>n</i>-<i>i</i>层的所有输出进行连接 (<i>n</i>为网络结构的总层数) , 作为第<i>n</i>-<i>i</i>+1层节点的输入。生成器网络结构如图3所示。</p>
                </div>
                <div class="area_img" id="104">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201904037_104.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 生成器网络结构" src="Detail/GetImg?filename=images/JSJC201904037_104.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图3 生成器网络结构</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201904037_104.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h3 id="105" name="105" class="anchor-tag">3 实验结果与分析</h3>
                <div class="p1">
                    <p id="106">实验目标是研究一种用于龋齿检测的自动图像分割方法, 识别7个部位, 包括龋齿、牙釉质、牙本质、牙髓、牙冠、牙根管和修复体, 并进行标记。图4 (a) 为咬翼片X射线图像, 图4 (b) 为该图像由医生进行手工标记的7个部位。</p>
                </div>
                <div class="area_img" id="107">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201904037_107.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 训练样本与分割标签图像" src="Detail/GetImg?filename=images/JSJC201904037_107.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图4 训练样本与分割标签图像</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201904037_107.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="108">训练的数据样本来自80个患者的咬翼片X射线图像和医生手工标记的标准分割图像对, 由于图像的分辨率不统一, 因此将图像重新进行缩放至256像素×256像素, 并且归一化灰度值到[-1, 1]的范围内。通过数据增强的方法将训练数据集扩充到28 800对, 使用这些数据集训练本文模型, 训练出的模型对咬翼片X射线图像的分割效果如图5所示。可以看出, 本文方法对于常见类型牙釉质、牙本质、牙髓能够进行较好的分割。</p>
                </div>
                <div class="area_img" id="109">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201904037_109.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 基于cGAN与U-Net的咬翼片X射线图像分割结果" src="Detail/GetImg?filename=images/JSJC201904037_109.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图5 基于cGAN与U-Net的咬翼片X射线图像分割结果</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201904037_109.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="110">实验通过精确率、准确率、真阳率、假阳率、Dice系数这些主要指标来评估本文方法的性能。训练出的模型对测试集 (40个患者的咬翼片X射线图像) 进行分割后, 通过模型分割出的7个部位与医生手工标记的标准结果进行对比 (见表3) , 可以看出对常见部位 (牙釉质、牙本质、牙髓) 的分割相似度超过75%, 对龋齿、牙冠、牙根管等其他部位的分割效果还需进一步提高。</p>
                </div>
                <div class="area_img" id="111">
                    <p class="img_tit"><b>表3 测试集上不同部位分割的性能比较结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="111" border="1"><tr><td>部位</td><td>精确率</td><td>真阳率</td><td>假阳率</td><td>Dice系数</td></tr><tr><td><br />龋齿</td><td>0.418</td><td>0.768</td><td>0.973</td><td>0.584</td></tr><tr><td><br />牙釉质</td><td>0.588</td><td>0.886</td><td>0.923</td><td>0.759</td></tr><tr><td><br />牙本质</td><td>0.641</td><td>0.797</td><td>0.848</td><td>0.781</td></tr><tr><td><br />牙髓</td><td>0.542</td><td>0.878</td><td>0.941</td><td>0.751</td></tr><tr><td><br />牙冠</td><td>0.494</td><td>0.790</td><td>0.901</td><td>0.567</td></tr><tr><td><br />修复体</td><td>0.513</td><td>0.792</td><td>0.946</td><td>0.663</td></tr><tr><td><br />牙根管</td><td>0.433</td><td>0.721</td><td>0.958</td><td>0.597</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="112">本文方法通过结合U-Net和cGAN后, 图像分割的Dice系数达到0.697, 而文献<citation id="142" type="reference">[<a class="sup">25</a>]</citation>采用U-Net分割的Dice系数最高为0.564, 相对来说提高了0.133。表4为2个分割模型的评估性能比较。</p>
                </div>
                <div class="area_img" id="113">
                    <p class="img_tit"><b>表4 测试集上2个分割模型的性能比较结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="113" border="1"><tr><td><br />模型</td><td>精确率</td><td>真阳率</td><td>假阳率</td><td>Dice系数</td></tr><tr><td><br />U-Net</td><td>0.453</td><td>0.613</td><td>0.983</td><td>0.564</td></tr><tr><td><br />cGAN+U-Net</td><td>0.546</td><td>0.784</td><td>0.956</td><td>0.697</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="114">实验结果表明, 条件生成对抗网络对于图像分割具有较好的性能, 对牙齿的常见部位 (牙本质、牙釉质、牙髓) 分割较准确, 由于数据增强的原因, 因此有大量的训练样本可以对这些部位进行训练。对于龋齿、牙根管等这些部位的分割准确率较低, 如果有更多的数据样本对模型进行训练, 那么分割的Dice系数将会更高。标准目标分割图像中有时并未标记牙齿, 加大了模型训练难度, 若假设提供的训练数据标注更加严格, 则模型整体性能将更好。</p>
                </div>
                <h3 id="115" name="115" class="anchor-tag">4 结束语</h3>
                <div class="p1">
                    <p id="116">本文通过使用条件生成对抗网络结合U型网络对咬翼片X射线图像进行分割, 将其分割成龋齿、牙冠、牙釉质、牙本质、牙髓、牙根管和修复体7个部位。实验结果表明, 与基于U-Net的分割方法相比, 本文方法的咬翼片分割准确率更高, 特别是对于常见部位 (牙釉质、牙本质、牙髓) 的分割准确率超过75%。下一步将对包含龋齿、牙根管等部位的训练数据样本增多情况下的图像分割准确率和性能做进一步研究与测试。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image segmentation by using threshold techniques">

                                <b>[1]</b> AL-AMRI S S, KALYANKAR N V.Image segmentation by using threshold techniques[EB/OL].[2018-01-16].http://www.arXiv preprint arXiv:1005.
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Moving region segmentation using parse motion cue from a moving camera">

                                <b>[2]</b> KANG J K.Moving region segmentation using parse motion cue from a moving camera[C]//Proceedings of the 12th International Conference Intelligent Autonomous Systems.Berlin, Germany:Springer, 2012:257-264.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Recognition of 2D barcode images using edge detection and morphological operation">

                                <b>[3]</b> GAUR P.Recognition of 2D barcode images using edge detection and morphological operation[EB/OL].[2018-01-16].https://ijcsmc.com/docs/papers/April2 014/V3I4201499b59.pdf.
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Auto-encoding variational Bayes">

                                <b>[4]</b> KINGMA D P, MAX W.Auto-encoding variational Bayes[EB/OL].[2018-01-16].https://arxiv.org/abs/1312.6114.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Generative adversarial networks">

                                <b>[5]</b> GOODFELLOW I J, POUGET-ABADIE J, MIRZA M, et al.Generative adversarial nets[EB/OL].[2018-01-16].https://arxiv.org/abs/1406.2661.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Photo-realistic single image super-resolution using a generative adversarial network">

                                <b>[6]</b> LEDIG C, THEIS L, HUSZAR F, et al.Photo-realistic single image super-resolution using a generative adversarial network[EB/OL].[2018-01-16].https://arxiv.org/abs/1609.04802.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Intraoperative organ motion models with an ensemble of conditional generative adversarial networks">

                                <b>[7]</b> HU Y P, GIBSON E, VERCAUTEREN T, et al.Intraoperative organ motion models with an ensemble of conditional generative adversarial networks[C]//Proceedings of International Conference on Medical Image Computing and Computer-assisted Intervention.Berlin, Germany:Springer, 2017:368-376.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Freehand ultrasound image simulation with spatially conditioned generative adversarial networks">

                                <b>[8]</b> HU Y P, GIBSON E, LEE L L.Freehand ultrasound image simulation with spatially conditioned generative adversarial networks[C]//Proceedings of International Workshop on Reconstruction and Analysis of Moving Body Organs.Berlin, Germany:Springer, 2017:105-115.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Adversarial networks for the detection of aggressive prostate cancer">

                                <b>[9]</b> KOHL S, BONEKAMP D, SCHLEMMER H.Adversarial networks for the detection of aggressive prostate cancer[EB/OL].[2018-01-16].https://arxiv.org/abs/1702.08014.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_10" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJWD&amp;filename=SJWD00000227378&amp;v=MTU0NzFlYnVkdEZDN2xWTHJKSUZnPU5pZmNhck80SHRITXJZMUNaK3dIWTNrNXpCZGg0ajk5U1hxUnJ4b3hjTUg3Ujdx&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[10]</b> TAN P L, EVANS R W, MORGAN M V.Caries, bitewings, and treatment decisions[J].Australian Dental Journal, 2002, 47 (2) :138-141.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_11" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=SYDL200403017&amp;v=MjMxODM0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSb0Z5L2dVci9PTmpUUFlyRzRIdFhNckk5RVk0UUtESDg=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[11]</b> 臧晶, 宋凯.基于多分辨率分析理论的口腔CT图像的增强[J].沈阳电力高等专科学校学报, 2004, 6 (3) :48-50.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_12" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=SJSJ200921069&amp;v=MjEwMTFPcm85RGJZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSb0Z5L2dVci9PTmlmWVpMRzRIdGo=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[12]</b> 杨玲, 王中科, 王云鹏.牙缝约束下模糊连接法对单颗牙齿的分割[J].计算机工程与设计, 2009, 30 (21) :5031-5036.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_13" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD130122003664&amp;v=MjIyMjByWTlGWiswSkNCTTh6eFVTbURkOVNIN24zeEU5ZmJ2bktyaWZadTl1RkN2alU3ak5JVjRTTmo3QmFySzdIdERP&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[13]</b> LIN P L, HUANG P W, CHO Y S, et al.An automatic and effective tooth isolation method for dental radiographs[J].Opto-electronics Review, 2013, 21 (1) :126-136.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_14" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=KXJY201101078&amp;v=MDkyMTZxQnRHRnJDVVJMT2VaZVJvRnkvZ1VyL09MalhCZDdHNEg5RE1ybzlDYklRS0RIODR2UjRUNmo1NE8zenE=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[14]</b> 莫宏兵, 赵刚.基于细菌模糊聚类算法的牙齿图像分割技术研究[J].黑龙江医药科学, 2014, 34 (1) :108-109.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Dental X-ray image segmentation using a U-shaped deep convolutional network">

                                <b>[15]</b> RONNEBERGER O, FISCHER P, BROX T.Dental X-ray image segmentation using a U-shaped deep convolutional network[EB/OL].[2018-01-16].http://www-o.ntust.edu.tw/～cweiwang/ISBI2015/challen ge2/isbi2015_Ronneberger.pdf.
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Towards principled unsupervised learning">

                                <b>[16]</b> SUTSKEVER I, JOZEFOWICZ R, GREGOR K, et al.Towards principled unsupervised learning[EB/OL].[2018-01-16].https://arxiv.org/abs/1511.06440
                            </a>
                        </p>
                        <p id="35">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep learning">

                                <b>[17]</b> GOODFELLOW I, BENGIO Y, COURVILLE A.Deep learning[M].Cambridge, UK:MIT Press, 2016.
                            </a>
                        </p>
                        <p id="37">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Characterization and computation of local Nash equilibria in continuous games">

                                <b>[18]</b> RATLIff L J, BURDEN S A, SASTRY S S.Characterization and computation of local Nash equilibria in continuous games[C]//Proceedings of the 51st Annual Allerton Con-ference on Communication, Control, and Computing.Monticello, USA:IEEE Press, 2013:917-924.
                            </a>
                        </p>
                        <p id="39">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Conditional generative adversarial nets">

                                <b>[19]</b> MIRZA M, OSINDERO S.Conditional generative adversarial nets[EB/OL].[2018-01-16].https://arxiv.org/abs/1411.1784.
                            </a>
                        </p>
                        <p id="41">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Unsupervised representation learning with deep convolutional generative adversarial networks[Online]">

                                <b>[20]</b> RADFORD A, METZ L, CHINTALA S.Unsupervised representation learning with deep convolutional generative adversarial networks[EB/OL].[2018-01-16].https://arxiv.org/abs/1511.06434.
                            </a>
                        </p>
                        <p id="43">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Adam:amethodforstochasticoptimization[OL]">

                                <b>[21]</b> KINGMA D, BA J.Adam:a method for stochastic optimization[EB/OL].[2018-01-16].https://arxiv.org/abs/1412.6980v8.
                            </a>
                        </p>
                        <p id="45">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Improved techniques for training gans">

                                <b>[22]</b> SALIMANS T, GOODFELLOW I, ZAREMBA W, et al.Improved techniques for training gans[EB/OL].[2018-01-16].https://arxiv.org/abs/1606.03498.
                            </a>
                        </p>
                        <p id="47">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Improved training of wasserstein GANs">

                                <b>[23]</b> GULRAJANI I, AHMED F, ARJOVSKY M, et al.Improved training of wasserstein GANs[EB/OL].[2018-01-16].https://arxiv.org/abs/1704.00028v3.
                            </a>
                        </p>
                        <p id="49">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=U-Net:Convolutional Networks for Biomedical Image Segmentation">

                                <b>[24]</b> RONNEBERGER O, FISCHER P, BROX T.U-net:convolutional networks for biomedical image segmenta-tion[C]//Proceedings of MICCAI’15.Berlin, Germany:Springer, 2015:234-241.
                            </a>
                        </p>
                        <p id="51">
                            <a id="bibliography_25" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Context encoders:feature learning by inpainting">

                                <b>[25]</b> PATHAK D, KRAHENBUHL P, DONAHUE J, et al.Context encoders:feature learning by inpainting[EB/OL].[2018-01-16].https://arxiv.org/abs/1604.07379.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJC201904037" />
        <input id="dpi" type="hidden" value="600" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201904037&amp;v=MjU0ODFyQ1VSTE9lWmVSb0Z5L2dVci9PTHo3QmJiRzRIOWpNcTQ5R1k0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0Y=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
