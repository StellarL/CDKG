<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637128889621551250%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJC201908009%26RESULT%3d1%26SIGN%3dx91VIX6MN%252bikh5%252fhX3oA%252bEQ7%252bOY%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201908009&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201908009&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201908009&amp;v=MDE5NTVxcUJ0R0ZyQ1VSTE9lWmVScUZ5L2dVTHJMTHo3QmJiRzRIOWpNcDQ5RmJZUUtESDg0dlI0VDZqNTRPM3o=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9xck9nSjc1VXR3SDJsaTcyc24rQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#51" data-title="0 概述 ">0 概述</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#57" data-title="1 数值属性与名词属性的粗糙聚类算法 ">1 数值属性与名词属性的粗糙聚类算法</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#58" data-title="1.1 基于优势关系的粗糙聚类定性组合方案">1.1 基于优势关系的粗糙聚类定性组合方案</a></li>
                                                <li><a href="#90" data-title="1.2 最大总平均分布精度算法">1.2 最大总平均分布精度算法</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#125" data-title="2 粗糙聚类定性组合方案 ">2 粗糙聚类定性组合方案</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#127" data-title="2.1 模糊优势关系">2.1 模糊优势关系</a></li>
                                                <li><a href="#131" data-title="2.2 基于模糊优势关系的混合属性数据聚类方案">2.2 基于模糊优势关系的混合属性数据聚类方案</a></li>
                                                <li><a href="#139" data-title="2.3 算法步骤">2.3 算法步骤</a></li>
                                                <li><a href="#148" data-title="2.4 算法时间复杂度分析">2.4 算法时间复杂度分析</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#150" data-title="3 实验结果与分析 ">3 实验结果与分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#151" data-title="3.1 聚类结果评价标准">3.1 聚类结果评价标准</a></li>
                                                <li><a href="#155" data-title="3.2 UCI数据集测试分析">3.2 UCI数据集测试分析</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#169" data-title="4 结束语 ">4 结束语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#61" data-title="&lt;b&gt;图1 数据集按照属性&lt;i&gt;v&lt;/i&gt;得到的粗糙聚类结果&lt;/b&gt;"><b>图1 数据集按照属性<i>v</i>得到的粗糙聚类结果</b></a></li>
                                                <li><a href="#157" data-title="&lt;b&gt;表1 数据集信息&lt;/b&gt;"><b>表1 数据集信息</b></a></li>
                                                <li><a href="#159" data-title="&lt;b&gt;表2 混合属性数据集上的算法准确率对比&lt;/b&gt;"><b>表2 混合属性数据集上的算法准确率对比</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="200">


                                    <a id="bibliography_1" title=" LLOYD S P.Least squares quantization in PCM[J].IEEE Transactions on Information Theory, 1982, 28 (2) :129-137." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Least squares quantization in PCM">
                                        <b>[1]</b>
                                         LLOYD S P.Least squares quantization in PCM[J].IEEE Transactions on Information Theory, 1982, 28 (2) :129-137.
                                    </a>
                                </li>
                                <li id="202">


                                    <a id="bibliography_2" title=" RODRIGUEZ A, LAIO A.Clustering by fast search and find of density peaks[J].Science, 2014, 344 (6191) :1492-1496." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Clustering by fast search and find of densitypeaks">
                                        <b>[2]</b>
                                         RODRIGUEZ A, LAIO A.Clustering by fast search and find of density peaks[J].Science, 2014, 344 (6191) :1492-1496.
                                    </a>
                                </li>
                                <li id="204">


                                    <a id="bibliography_3" title=" ESTER M, KRIEGEL H P, SANDER J, et al.A density-based algorithm for discovering clusters in large spatial databases with noise[C]//Proceedings of the 2nd International Conference on Knowledge Discovery and Data Mining.[S.l.]:AAAI Press, 1996:226-231." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise">
                                        <b>[3]</b>
                                         ESTER M, KRIEGEL H P, SANDER J, et al.A density-based algorithm for discovering clusters in large spatial databases with noise[C]//Proceedings of the 2nd International Conference on Knowledge Discovery and Data Mining.[S.l.]:AAAI Press, 1996:226-231.
                                    </a>
                                </li>
                                <li id="206">


                                    <a id="bibliography_4" title=" 许合利, 牛丽君.基于层次与密度的任意形状聚类算法[J].计算机工程, 2016, 42 (7) :159-164." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201607028&amp;v=MDY5ODdUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJxRnkvZ1VMckxMejdCYmJHNEg5Zk1xSTlIYklRS0RIODR2UjQ=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9xck9nSjc1VXR3SDJsaTcyc24rQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                         许合利, 牛丽君.基于层次与密度的任意形状聚类算法[J].计算机工程, 2016, 42 (7) :159-164.
                                    </a>
                                </li>
                                <li id="208">


                                    <a id="bibliography_5" title=" LINGRAS P.Rough set clustering for Web mining[C]//Proceedings of 2002 IEEE International Conference on Fuzzy Systems.Washington D.C., USA:IEEE Press, 2002:1039-1044." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Rough set clustering for web mining">
                                        <b>[5]</b>
                                         LINGRAS P.Rough set clustering for Web mining[C]//Proceedings of 2002 IEEE International Conference on Fuzzy Systems.Washington D.C., USA:IEEE Press, 2002:1039-1044.
                                    </a>
                                </li>
                                <li id="210">


                                    <a id="bibliography_6" title=" LINGRAS P, WEST C.Interval set clustering of Web users with Rough K-means[J].Journal of Intelligent Information Systems, 2004, 23 (1) :5-16." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00001510032&amp;v=MDg5MjVqOTlTWHFScnhveGNNSDdSN3FlYnVkdEZDemxWTHJMSlYwPU5qN0Jhck80SHRITnFvNUZaT2dOWTNrNXpCZGg0&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9xck9nSjc1VXR3SDJsaTcyc24rQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                         LINGRAS P, WEST C.Interval set clustering of Web users with Rough K-means[J].Journal of Intelligent Information Systems, 2004, 23 (1) :5-16.
                                    </a>
                                </li>
                                <li id="212">


                                    <a id="bibliography_7" title=" PETERS G.Outliers in Rough K-means clustering[J].Lecture Notes in Computer Science, 2005, 3776 (11) :702-707." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Outliers in rough k-means clustering">
                                        <b>[7]</b>
                                         PETERS G.Outliers in Rough K-means clustering[J].Lecture Notes in Computer Science, 2005, 3776 (11) :702-707.
                                    </a>
                                </li>
                                <li id="214">


                                    <a id="bibliography_8" title=" PETERS G.Rough clustering utilizing the principle of indifference[J].Information Science, 2014, 277 (2) :358-374." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14032300029442&amp;v=MTU3Mzg0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVVMdklKMXNYYnhFPU5pZk9mYks4SHRMT3JJOUZaT2tHQ0hnN29CTVQ2VA==&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9xck9nSjc1VXR3SDJsaTcyc24rQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                         PETERS G.Rough clustering utilizing the principle of indifference[J].Information Science, 2014, 277 (2) :358-374.
                                    </a>
                                </li>
                                <li id="216">


                                    <a id="bibliography_9" title=" 马福民, 逯瑞强, 张腾飞.基于边界区域局部模糊增强的πRKM聚类算法[J].控制与决策, 2017, 32 (11) :1949-1956." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=KZYC201711004&amp;v=MTE5OTN5L2dVTHJMTGpmU2JiRzRIOWJOcm85RllJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVScUY=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9xck9nSjc1VXR3SDJsaTcyc24rQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                         马福民, 逯瑞强, 张腾飞.基于边界区域局部模糊增强的πRKM聚类算法[J].控制与决策, 2017, 32 (11) :1949-1956.
                                    </a>
                                </li>
                                <li id="218">


                                    <a id="bibliography_10" title=" ZHANG Tengfei, MA Fumin.Improved Rough K-means clustering algorithm based on weighted distance measure with Gaussian function[J].International Journal of Computer Mathematics, 2017, 94 (4) :663-675." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Improved rough k-means clustering algorithm based on weighted distance measure with Gaussian function">
                                        <b>[10]</b>
                                         ZHANG Tengfei, MA Fumin.Improved Rough K-means clustering algorithm based on weighted distance measure with Gaussian function[J].International Journal of Computer Mathematics, 2017, 94 (4) :663-675.
                                    </a>
                                </li>
                                <li id="220">


                                    <a id="bibliography_11" title=" HUANG Zhexue.Clustering large data sets with mixed numeric and categorical values[C]//Proceedings of the 1st Pacific-Asia Conference on Knowledge Discovery and Data Mining.Washington D.C., USA:IEEE Press, 1997:21-34." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Clustering large data sets with mixed numeric and categorical values">
                                        <b>[11]</b>
                                         HUANG Zhexue.Clustering large data sets with mixed numeric and categorical values[C]//Proceedings of the 1st Pacific-Asia Conference on Knowledge Discovery and Data Mining.Washington D.C., USA:IEEE Press, 1997:21-34.
                                    </a>
                                </li>
                                <li id="222">


                                    <a id="bibliography_12" title=" JI Jinchao, BAI Tian, ZHOU Chunguang, et al.An improved K-prototypes clustering algorithm for mixed numeric and categorical data[J].Neurocomputing, 2013, 120:590-596." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES959D989A1159D3805B9013BD9D3B44CF&amp;v=MjI0MTFXV3I3SklGcG1hQnVIWWZPR1FsZkNwYlEzNU54aHc3Mjh4YXM9TmlmT2ZicTlGNlhGcDRZMFplb0tCUWc2eHhZV21EWjlTWHlRMkJ0QmVzQ1FRY25wQ09OdkZTaQ==&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9xck9nSjc1VXR3SDJsaTcyc24rQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[12]</b>
                                         JI Jinchao, BAI Tian, ZHOU Chunguang, et al.An improved K-prototypes clustering algorithm for mixed numeric and categorical data[J].Neurocomputing, 2013, 120:590-596.
                                    </a>
                                </li>
                                <li id="224">


                                    <a id="bibliography_13" title=" LI Cen, BISWAS G.Unsupervised learning with mixed numeric and nominal data[J].IEEE Transactions on Knowledge and Data Engineering, 2002, 14 (4) :673-690." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Unsupervised learning with mixed numeric and nominal data">
                                        <b>[13]</b>
                                         LI Cen, BISWAS G.Unsupervised learning with mixed numeric and nominal data[J].IEEE Transactions on Knowledge and Data Engineering, 2002, 14 (4) :673-690.
                                    </a>
                                </li>
                                <li id="226">


                                    <a id="bibliography_14" title=" JIA Hong, CHEUNG Yiuming.Subspace clustering of categorical and numerical data with an unknown number of clusters[J].IEEE Transactions on Neural Networks and Learning Systems, 2018, 29 (8) :3308-3325." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Subspace clustering of categorical and numerical data with an unknown number of clusters">
                                        <b>[14]</b>
                                         JIA Hong, CHEUNG Yiuming.Subspace clustering of categorical and numerical data with an unknown number of clusters[J].IEEE Transactions on Neural Networks and Learning Systems, 2018, 29 (8) :3308-3325.
                                    </a>
                                </li>
                                <li id="228">


                                    <a id="bibliography_15" title=" SALGADO C M, VIEIRA S M, SOUSA J M C.Mixed fuzzy clustering for deriving predictive models in intensive care units[M]//KAHRAMAN C, TOPCU Y I.Operations research applications in health care management.Berlin, Germany:Springer, 2018." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Mixed fuzzy clustering for deriving predictive models in intensive care units">
                                        <b>[15]</b>
                                         SALGADO C M, VIEIRA S M, SOUSA J M C.Mixed fuzzy clustering for deriving predictive models in intensive care units[M]//KAHRAMAN C, TOPCU Y I.Operations research applications in health care management.Berlin, Germany:Springer, 2018.
                                    </a>
                                </li>
                                <li id="230">


                                    <a id="bibliography_16" title=" FISHBURN P C.Utility theory for decision making[J].Publications in Operations Research, 1970, 22:308-309." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Utility theory for decision making">
                                        <b>[16]</b>
                                         FISHBURN P C.Utility theory for decision making[J].Publications in Operations Research, 1970, 22:308-309.
                                    </a>
                                </li>
                                <li id="232">


                                    <a id="bibliography_17" title=" YAO Yiyu.Decision-theoretic rough set models[C]//Proceedings of International Conference on Rough Sets and Knowledge Technology.Berlin, Germany:Springer, 2007:1-12." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Decision-theoretic rough set models">
                                        <b>[17]</b>
                                         YAO Yiyu.Decision-theoretic rough set models[C]//Proceedings of International Conference on Rough Sets and Knowledge Technology.Berlin, Germany:Springer, 2007:1-12.
                                    </a>
                                </li>
                                <li id="234">


                                    <a id="bibliography_18" title=" YAO Y Y, WONG S K M.A decision theoretic framework for approximating concepts[J].International Journal of Man-Machine Studies, 1992, 37 (6) :793-809." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A decision theoretic framework for approximating concepts">
                                        <b>[18]</b>
                                         YAO Y Y, WONG S K M.A decision theoretic framework for approximating concepts[J].International Journal of Man-Machine Studies, 1992, 37 (6) :793-809.
                                    </a>
                                </li>
                                <li id="236">


                                    <a id="bibliography_19" title=" YAO Yiyu, LINGRAS P, WANG Ruichu, et al.Interval set cluster analysis:a re-formulation[C]//Proceedings of International Workshop on Rough Sets, Fuzzy Sets, Data Mining, and Granular-Soft Computing.Berlin, Germany:Springer, 2009:398-405." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Interval set cluster analysis:a re-formulation">
                                        <b>[19]</b>
                                         YAO Yiyu, LINGRAS P, WANG Ruichu, et al.Interval set cluster analysis:a re-formulation[C]//Proceedings of International Workshop on Rough Sets, Fuzzy Sets, Data Mining, and Granular-Soft Computing.Berlin, Germany:Springer, 2009:398-405.
                                    </a>
                                </li>
                                <li id="238">


                                    <a id="bibliography_20" title=" LINGRAS P, CHEN Min, MIAO Duoqian.Qualitative and quantitative combinations of crisp and rough clustering schemes using dominance relations[J].International Journal of Approximate Reasoning, 2014, 55 (1) :238-258." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESE08B5B02BE6BDDFDB8F8E4E1F09FC0C0&amp;v=MjYwMzdpZk9mY2E0RnFQSjNZOUhGcDRKZmdoTnVXSmg0a2wxUFh1WHJXUTFjTVRuUmNtZkNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkNwYlEzNU54aHc3Mjh4YXM9Tg==&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9xck9nSjc1VXR3SDJsaTcyc24rQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[20]</b>
                                         LINGRAS P, CHEN Min, MIAO Duoqian.Qualitative and quantitative combinations of crisp and rough clustering schemes using dominance relations[J].International Journal of Approximate Reasoning, 2014, 55 (1) :238-258.
                                    </a>
                                </li>
                                <li id="240">


                                    <a id="bibliography_21" title=" LI Min, DENG Shaobo, WANG Lei, et al.Hierarchical clustering algorithm for categorical data using a probabilistic rough set model[J].Knowledge-based Systems, 2014, 65:60-71." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14061700208648&amp;v=MjYyMTVpclJkR2VycVFUTW53WmVadUh5am1VTHZJSjFzWGJ4RT1OaWZPZmJLOEh0Zk5xSTlGWnVzSENuZ3hvQk1UNlQ0UFFILw==&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9xck9nSjc1VXR3SDJsaTcyc24rQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[21]</b>
                                         LI Min, DENG Shaobo, WANG Lei, et al.Hierarchical clustering algorithm for categorical data using a probabilistic rough set model[J].Knowledge-based Systems, 2014, 65:60-71.
                                    </a>
                                </li>
                                <li id="242">


                                    <a id="bibliography_22" title=" YAGER R R, KACPRZYK J, FEDRIZZI M.Advances in the dempster-shafer theory of evidence[M].[S.l.]:John Wiley and Sons, Inc., 1994." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Advances in the Dempster-Shafer Theory of Evidence">
                                        <b>[22]</b>
                                         YAGER R R, KACPRZYK J, FEDRIZZI M.Advances in the dempster-shafer theory of evidence[M].[S.l.]:John Wiley and Sons, Inc., 1994.
                                    </a>
                                </li>
                                <li id="244">


                                    <a id="bibliography_23" title=" 王佐.基于粗糙集的聚类算法研究[D].长春:吉林大学, 2013." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CDFD&amp;filename=1014130233.nh&amp;v=MjIyNTdMckxWRjI2R3JLN0h0UFBySkViUElRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJxRnkvZ1U=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9xck9nSjc1VXR3SDJsaTcyc24rQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[23]</b>
                                         王佐.基于粗糙集的聚类算法研究[D].长春:吉林大学, 2013.
                                    </a>
                                </li>
                                <li id="246">


                                    <a id="bibliography_24" title=" PATHAK A, PAL N R.Clustering of mixed data by integrating fuzzy, probabilistic, and collaborative clustering framework[J].International Journal of Fuzzy Systems, 2016, 18 (3) :339-348." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Clustering of mixed data by integrating fuzzy,probabilistic and collaborative clustering framework">
                                        <b>[24]</b>
                                         PATHAK A, PAL N R.Clustering of mixed data by integrating fuzzy, probabilistic, and collaborative clustering framework[J].International Journal of Fuzzy Systems, 2016, 18 (3) :339-348.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJC" target="_blank">计算机工程</a>
                2019,45(08),48-52+59 DOI:10.19678/j.issn.1000-3428.0051542            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于模糊优势关系的粗糙聚类定性组合算法</b></span>
                                    </h1>

                <div class="btn-downloads  btn-downloads-new">

                        <a class="read read-btn-special" target="_blank" href=http://x.cnki.net/search/common/testlunbo?dbcode=CJFD&amp;tablename=CJFDLAST2019&amp;filename=JSJC201908009&amp;filesourcetype=1><i class="i-btn i-care"></i>精读</a>
                    <div class="read-btn-l">
                        <a class="caj" target="_blank" href="http://kns.cnki.net/kns/download.aspx?filename=0pGeh5UcwIUd5A1YNllcvlGexVFTZZWeJNVOxcHcrsmYUBHOItUcJ1UYTpHR1lUeJJ3a5QFUE1Ed5VETxpER5IzVxE1R0lkQxIEW5NUW4AHMxV0TVVlYLtWbLF0Z3wmMLVUWFVGdn1mQ2xGT2ZDcMZldkNlbVdne&tablename=CJFDLAST2019"><i class="i-btn i-caj"></i> CAJ下载</a>
                        <a class="pdf" target="_blank" href="http://kns.cnki.net/kns/download.aspx?filename=0pGeh5UcwIUd5A1YNllcvlGexVFTZZWeJNVOxcHcrsmYUBHOItUcJ1UYTpHR1lUeJJ3a5QFUE1Ed5VETxpER5IzVxE1R0lkQxIEW5NUW4AHMxV0TVVlYLtWbLF0Z3wmMLVUWFVGdn1mQ2xGT2ZDcMZldkNlbVdne&tablename=CJFDLAST2019&dflag=pdfdown"><i class="i-btn i-pdf"></i>PDF下载</a>

                        <p>永久保存本文,请下载至本地</p>
                    </div>

                </div>
            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%B2%88%E6%80%A1%E7%A7%80&amp;code=42444616&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9xck9nSjc1VXR3SDJsaTcyc24rQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">沈怡秀</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%A9%AC%E7%A6%8F%E6%B0%91&amp;code=21995493&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9xck9nSjc1VXR3SDJsaTcyc24rQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">马福民</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9B%B9%E6%9D%B0&amp;code=08027658&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9xck9nSjc1VXR3SDJsaTcyc24rQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">曹杰</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%8D%97%E4%BA%AC%E8%B4%A2%E7%BB%8F%E5%A4%A7%E5%AD%A6%E4%BF%A1%E6%81%AF%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0251839&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9xck9nSjc1VXR3SDJsaTcyc24rQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">南京财经大学信息工程学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>为对包含数值和名词属性的混合数据集进行定性组合聚类分析, 提出一种基于模糊优势关系的粗糙聚类定性组合算法f-QRD。根据混合数据集的不同属性分别进行聚类并计算类簇之间的模糊优势关系, 为避免组合后的类簇碎片过多, 对模糊优势关系差值较小的类簇进行合并处理。实验结果表明, 与QRD算法相比, f-QRD算法能够有效减少类簇数目并提高聚类准确率。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%B2%97%E7%B3%99%E8%81%9A%E7%B1%BB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9xck9nSjc1VXR3SDJsaTcyc24rQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">粗糙聚类;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=K%E5%9D%87%E5%80%BC&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9xck9nSjc1VXR3SDJsaTcyc24rQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">K均值;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%AE%9A%E6%80%A7%E7%BB%84%E5%90%88&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9xck9nSjc1VXR3SDJsaTcyc24rQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">定性组合;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%A8%A1%E7%B3%8A%E4%BC%98%E5%8A%BF%E5%85%B3%E7%B3%BB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9xck9nSjc1VXR3SDJsaTcyc24rQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">模糊优势关系;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B7%E5%90%88%E6%95%B0%E6%8D%AE%E9%9B%86&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9xck9nSjc1VXR3SDJsaTcyc24rQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">混合数据集;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    沈怡秀 (1994—) , 女, 硕士研究生, 主研方向为智能信息处理;;
                                </span>
                                <span>
                                    *马福民 (通信作者) , 副教授、博士;E-mail: fmmatj@ 126. com;
                                </span>
                                <span>
                                    曹杰, 教授、博士。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-05-14</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家重点研发计划 (2017YFD0401001);</span>
                                <span>国家自然科学基金 (61403184);</span>
                                <span>国家农业科技成果转化资金项目 (2014GB2C100300);</span>
                                <span>江苏省高等学校自然科学研究重大项目 (17KJA120001, 14KJA520001);</span>
                                <span>江苏省研究生科研创新计划 (KYCX17_1210);</span>
                    </p>
            </div>
                    <h1><b>Qualitative Combination Algorithm for Rough Clustering Based on Fuzzy Dominance Relations</b></h1>
                    <h2>
                    <span>SHEN Yixiu</span>
                    <span>MA Fumin</span>
                    <span>CAO Jie</span>
            </h2>
                    <h2>
                    <span>College of Information Engineering, Nanjing University of Finance and Economics</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>In order to perform qualitative combination clustering analysis on mixed data sets containing numeric and noun attributes, a qualitative combination algorithm f-QRD for rough clustering based on fuzzy dominance relations is proposed.The mixed data sets are clustered according to different attributes and calculating the fuzzy dominance relations between clusters.In order to avoid the excessive fragments of clusters after combination, clusters with small difference of fuzzy dominance relations are merged.Experimental results show that compared with QRD algorithm, f-QRD algorithm can effectively reduce the number of clusters and improve the clustering accuracy.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=rough%20clustering&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9xck9nSjc1VXR3SDJsaTcyc24rQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">rough clustering;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=K-means&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9xck9nSjc1VXR3SDJsaTcyc24rQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">K-means;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=qualitative%20combination&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9xck9nSjc1VXR3SDJsaTcyc24rQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">qualitative combination;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=fuzzy%20dominance%20relations&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9xck9nSjc1VXR3SDJsaTcyc24rQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">fuzzy dominance relations;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=mixed%20data%20sets&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9xck9nSjc1VXR3SDJsaTcyc24rQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">mixed data sets;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-05-14</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="51" name="51" class="anchor-tag">0 概述</h3>
                <div class="p1">
                    <p id="52">随着信息技术的快速发展, 众多行业积累了大量数据, 对这些数据进行高效的聚类分析以获取有价值的信息, 对于促进行业发展具有重要意义。传统聚类算法包括基于划分的K-means算法<citation id="248" type="reference"><link href="200" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>、密度峰值算法 (DPC) <citation id="249" type="reference"><link href="202" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>、具有噪声的基础密度聚类方法 (DBSCAN) <citation id="250" type="reference"><link href="204" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>、基于层次的聚类算法<citation id="251" type="reference"><link href="206" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>等。为解决边界数据对象的不确定性归属问题, 文献<citation id="259" type="reference">[<a class="sup">5</a>,<a class="sup">6</a>]</citation>将粗糙集理论引入到K-means聚类算法中, 提出Rough K-means算法, 文献<citation id="252" type="reference">[<a class="sup">7</a>]</citation>验证了该算法的有效性。在这之后, 又有多种基于粗糙集的聚类算法被提出<citation id="260" type="reference"><link href="214" rel="bibliography" /><link href="216" rel="bibliography" /><link href="218" rel="bibliography" /><sup>[<a class="sup">8</a>,<a class="sup">9</a>,<a class="sup">10</a>]</sup></citation>, 但这些算法仅针对数值属性数据集进行处理。随着监测手段的发展, 各行业的数据呈现出数值属性和名词属性混合的特征, 在处理混合属性数据时, 传统聚类方法不再适用, 因此, 一些学者提出了改进的聚类分析方法。文献<citation id="253" type="reference">[<a class="sup">11</a>]</citation>提出K-prototypes算法。文献<citation id="254" type="reference">[<a class="sup">12</a>]</citation>提出一种IK-prototypeps算法。文献<citation id="255" type="reference">[<a class="sup">13</a>]</citation>提出层次聚类算法SBAC (Similarity-Based Agglomerative Clustering) <citation id="256" type="reference"><link href="224" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>。考虑到对象归属的不确定性问题, 文献<citation id="257" type="reference">[<a class="sup">14</a>]</citation>提出一种软子空间聚类方法, 文献<citation id="258" type="reference">[<a class="sup">15</a>]</citation>提出FCM-UM算法。上述算法使用距离来度量对象之间的相似度, 属于定量聚类的范畴, 其存在的不足是无法分析聚类的语义结构。</p>
                </div>
                <div class="p1">
                    <p id="53">在聚类应用中, 对象与对象、类簇与类簇之间往往存在一个排序, 这种排序可以用优势关系进行体现。文献<citation id="261" type="reference">[<a class="sup">16</a>]</citation>提出一种基于优势关系的定性决策理论。文献<citation id="263" type="reference">[<a class="sup">17</a>,<a class="sup">18</a>,<a class="sup">19</a>]</citation>将粗糙集和优势关系相结合, 建立决策粗糙集模型。文献<citation id="262" type="reference">[<a class="sup">20</a>]</citation>将优势关系理论引入到聚类方案中, 提出定性的聚类组合算法QRD, 该算法能够增强聚类效果, 提升精度, 确定聚类的语义结构, 且具有以下2个优势:</p>
                </div>
                <div class="p1">
                    <p id="54">首先, 在现实世界中, 客户在选择某项产品或服务时通常还需要单独考虑其中某个属性的情况。QRD算法按照不同属性分别进行聚类得到子类簇, 然后再对类簇进行组合, 这样既能给客户提供总体最优的类簇, 也能兼顾客户在某个属性上的特殊要求。</p>
                </div>
                <div class="p1">
                    <p id="55">其次, QRD算法对每个属性分别进行粗糙聚类, 这样可以为不同属性选择合适的算法并设定不同的权重, 从而提高算法精度。</p>
                </div>
                <div class="p1">
                    <p id="56">但是, QRD算法只考虑数值属性数据集, 无法处理混合属性数据集, 且其在实际应用中会出现组合后类簇碎片过多的问题。因此, 本文将QRD算法扩展到混合数据集, 提出一种基于模糊优势关系的混合属性粗糙聚类定性组合方案fuzzy-QRD (f-QRD) , 以在精准描述类簇间序结构的同时提高聚类的准确率。</p>
                </div>
                <h3 id="57" name="57" class="anchor-tag">1 数值属性与名词属性的粗糙聚类算法</h3>
                <h4 class="anchor-tag" id="58" name="58">1.1 基于优势关系的粗糙聚类定性组合方案</h4>
                <div class="p1">
                    <p id="59">QRD算法考虑到在无监督学习中聚类语义分析的不便性, 其将优势关系和组合思想引入到算法中, 以进行语义分析, 但QRD仅适用于数值属性数据集。</p>
                </div>
                <div class="p1">
                    <p id="60">以一个合成的二维数值属性数据集为例。该数据集有2种属性<i>v</i>和<i>s</i>, 按照属性分别进行粗糙聚类。设聚类方案<i>rs</i><sub><i>v</i></sub>将数据集分成<i>k</i><sub><i>v</i></sub>个聚类, <i>rs</i><sub><i>v</i></sub>={<b><i>c</i></b><sub><i>v</i>1</sub>, <b><i>c</i></b><sub><i>v</i>2</sub>, …, <b><i>c</i></b><sub><i>vk</i><sub><i>v</i></sub></sub>}, 其中, <b><i>c</i></b><sub><i>vi</i></sub>不仅表示粗糙聚类后每个类簇的中心向量, 同时也表示该类簇中的其他对象, 即该类簇中的任一对象都与簇中心等价。因此, 不同类簇中对象之间的优势关系等价于类簇中心之间的优势关系。上述数据集按照属性<i>v</i>进行粗糙聚类后得到的结果如图1所示。其中, <i>lower</i> (·) 表示下近似集, <i>upper</i> (·) 表示上近似集。</p>
                </div>
                <div class="area_img" id="61">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201908009_061.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9xck9nSjc1VXR3SDJsaTcyc24rQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 数据集按照属性v得到的粗糙聚类结果" src="Detail/GetImg?filename=images/JSJC201908009_061.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9xck9nSjc1VXR3SDJsaTcyc24rQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图1 数据集按照属性<i>v</i>得到的粗糙聚类结果</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201908009_061.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9xck9nSjc1VXR3SDJsaTcyc24rQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="62">根据优势理论, 可以发现2个类簇中心之间存在3种联系:</p>
                </div>
                <div class="p1">
                    <p id="63">1) <b><i>c</i></b><sub><i>vi</i></sub>≻<sub><i>v</i></sub><b><i>c</i></b><sub><i>vj</i></sub>。</p>
                </div>
                <div class="p1">
                    <p id="64">2) <b><i>c</i></b><sub><i>vi</i></sub>≺<sub><i>v</i></sub><b><i>c</i></b><sub><i>vj</i></sub>。</p>
                </div>
                <div class="p1">
                    <p id="65">3) <b><i>c</i></b><sub><i>vi</i></sub>～<b><i>c</i></b><sub><i>vj</i></sub>, 即2个类簇中心之间等价。</p>
                </div>
                <div class="p1">
                    <p id="66">将聚类中心按照从小到大排序并赋予其优势程度 (<i>degree</i>) , 最小类簇的优势程度赋值0, 其余类簇的优势程度依次递增1。任意2个类簇之间的优势程度可以按照如下公式进行计算:</p>
                </div>
                <div class="p1">
                    <p id="67"><i>degree</i> (<b><i>c</i></b><sub><i>vi</i></sub>≻<sub><i>v</i></sub><b><i>c</i></b><sub><i>vj</i></sub>) =<i>i</i>-<i>j</i></p>
                </div>
                <div class="p1">
                    <p id="68">假设<i>rs</i><sub><i>v</i></sub>和<i>rs</i><sub><i>s</i></sub>是2种粗糙聚类方案:</p>
                </div>
                <div class="p1">
                    <p id="69"><i>rs</i><sub><i>v</i></sub>={<b><i>c</i></b><sub><i>vo</i></sub>, <b><i>c</i></b><sub><i>v</i>2</sub>, …, <b><i>c</i></b><sub><i>vk</i><sub><i>v</i></sub></sub>}</p>
                </div>
                <div class="p1">
                    <p id="70"><i>rs</i><sub><i>s</i></sub>={<b><i>c</i></b><sub><i>s</i>1</sub>, <b><i>c</i></b><sub><i>s</i>2</sub>, …, <b><i>c</i></b><sub><i>sk</i><sub><i>s</i></sub></sub>}</p>
                </div>
                <div class="p1">
                    <p id="71">其中, <i>k</i><sub><i>v</i></sub>和<i>k</i><sub><b><i>s</i></b></sub>分别是<i>rs</i><sub><i>v</i></sub>和<i>rs</i><sub><i>s</i></sub>的类簇数目。则聚类方案<i>rs</i><sub><i>v</i></sub>和<i>rs</i><sub><i>s</i></sub>的合并结果如下:</p>
                </div>
                <div class="p1">
                    <p id="72" class="code-formula">
                        <mathml id="72"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>r</mi><mi>s</mi><msub><mrow></mrow><mrow><mi>v</mi><mo>⊕</mo><mi>s</mi></mrow></msub><mo>=</mo><mo stretchy="false">{</mo><mi>c</mi><msub><mrow></mrow><mi>l</mi></msub><mrow><mo>|</mo><mrow><mi>l</mi><mi>o</mi><mi>w</mi><mi>e</mi><mi>r</mi><mo stretchy="false"> (</mo><mi>c</mi><msub><mrow></mrow><mi>l</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mi>l</mi><mi>o</mi><mi>w</mi><mi>e</mi><mi>r</mi><mo stretchy="false"> (</mo><mi>c</mi><msub><mrow></mrow><mrow><mi>v</mi><mi>i</mi></mrow></msub><mo stretchy="false">) </mo><mstyle displaystyle="true"><mo>∩</mo><mi>l</mi></mstyle><mi>o</mi><mi>w</mi><mi>e</mi><mi>r</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">c</mi><msub><mrow></mrow><mrow><mi>s</mi><mi>j</mi></mrow></msub><mo stretchy="false">) </mo><mo>∧</mo></mrow></mrow></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mi>u</mi><mi>p</mi><mi>p</mi><mi>e</mi><mi>r</mi><mo stretchy="false"> (</mo><mi>c</mi><msub><mrow></mrow><mi>l</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mi>u</mi><mi>p</mi><mi>p</mi><mi>e</mi><mi>r</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">c</mi><msub><mrow></mrow><mrow><mi>v</mi><mi>i</mi></mrow></msub><mo stretchy="false">) </mo><mstyle displaystyle="true"><mo>∩</mo><mi>u</mi></mstyle><mi>p</mi><mi>p</mi><mi>e</mi><mi>r</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">c</mi><msub><mrow></mrow><mrow><mi>s</mi><mi>j</mi></mrow></msub><mo stretchy="false">) </mo><mo>≠</mo><mo>∅</mo><mo stretchy="false">}</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="73">∀<b><i>c</i></b><sub><i>vi</i></sub>∈<i>rs</i><sub><i>v</i></sub>∧∀<b><i>c</i></b><sub><i>sj</i></sub>∈<i>rs</i><sub><i>s</i></sub></p>
                </div>
                <div class="p1">
                    <p id="74">假设<i>k</i><sub><i>v</i>♁<i>s</i></sub>是总的类簇数目, 则能够得出:</p>
                </div>
                <div class="p1">
                    <p id="75">max (<i>k</i><sub><i>v</i></sub>, <i>k</i><sub><i>s</i></sub>) ≤<i>k</i><sub><i>v</i>♁<i>s</i></sub>≤<i>k</i><sub><i>v</i></sub>·<i>k</i><sub><i>s</i></sub></p>
                </div>
                <div class="p1">
                    <p id="76">为计算类簇间的优势关系, 同时给不同聚类方案赋予不同的重要性, 需要对权重进行定义。</p>
                </div>
                <div class="p1">
                    <p id="77">假如≻<sub><i>v</i></sub>和≻<sub><i>s</i></sub>是2个优势关系, 且≻<sub><i>v</i></sub>比≻<sub><i>s</i></sub>更重要, 则≻<sub><i>v</i></sub>和≻<sub><i>s</i></sub>之间权重的比值<mathml id="78"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mo>≻</mo><msub><mrow></mrow><mi>v</mi></msub></mrow><mrow><mo>≻</mo><msub><mrow></mrow><mi>s</mi></msub></mrow></mfrac></mrow></math></mathml>大于1, 所有关系的权重之和等于1。</p>
                </div>
                <div class="p1">
                    <p id="79">组合后类簇之间的优势程度定义如下:</p>
                </div>
                <div class="area_img" id="80">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJC201908009_08000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9xck9nSjc1VXR3SDJsaTcyc24rQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="82">其中, <i>c</i><sub><i>l</i></sub>为类簇<b><i>c</i></b><sub><i>vi</i></sub>和<b><i>c</i></b><sub><i>st</i></sub>的组合, <i>c</i><sub><i>m</i></sub>为类簇<b><i>c</i></b><sub><i>vj</i></sub>和<b><i>c</i></b><sub><i>sr</i></sub>的组合。如果<i>degree</i> (<i>c</i><sub><i>l</i></sub>≻<sub><i>v</i>♁<i>s</i></sub><i>c</i><sub><i>m</i></sub>) &gt;0, 则<i>c</i><sub><i>l</i></sub>≻<sub><i>v</i>♁<i>s</i></sub><i>c</i><sub><i>m</i></sub>, 如果<i>degree</i> (<i>c</i><sub><i>l</i></sub>≻<sub><i>v</i>♁<i>s</i></sub><i>c</i><sub><i>m</i></sub>) &lt;0, 则<i>c</i><sub><i>m</i></sub>≻<sub><i>v</i>♁<i>s</i></sub><i>c</i><sub><i>l</i></sub>, 如果<i>degree</i> (<i>c</i><sub><i>l</i></sub>≻<sub><i>v</i>♁<i>s</i></sub><i>c</i><sub><i>m</i></sub>) =0, 则<i>c</i><sub><i>l</i></sub>～<sub><i>v</i>♁<i>s</i></sub><i>c</i><sub><i>m</i></sub>。</p>
                </div>
                <div class="p1">
                    <p id="83">类簇之间进行组合时很可能会得到下近似集为空的簇, 必须将这些簇进行消除, 步骤如下:</p>
                </div>
                <div class="p1">
                    <p id="84">1) 计算任意2个类簇的上近似集之间、下近似集之间的交集, 得到组合后的新类簇。</p>
                </div>
                <div class="p1">
                    <p id="85">2) 保留所有下近似集不为空的新类簇, 去掉下近似集为空的类簇, 然后检查即将被删去类簇的上近似集, 如果上近似集不为空, 则检查其中的对象, 若某对象属于2个或以上被保留类簇的上近似集, 则无需进行处理。</p>
                </div>
                <div class="p1">
                    <p id="86">3) 若某对象仅属于一个被保留类簇的上近似集, 则将该对象分配到所在类簇的所有邻居 (即与该对象所在类簇的优势关系绝对值小于设定阈值的邻居) 的上近似集中。</p>
                </div>
                <div class="p1">
                    <p id="87">4) 若某对象不属于任何被保留类簇的上近似集, 则将其分配给所有被保留类簇的上近似集。</p>
                </div>
                <div class="p1">
                    <p id="88">5) 利用权重比值公式计算优势程度, 得到优势关系。</p>
                </div>
                <div class="p1">
                    <p id="89">QRD算法考虑类簇之间存在的优势关系, 更符合实际情况, 其按照每个属性分别进行聚类后再实现组合, 可以节省大量的沟通成本, 也有助于在无监督学习过程中分析语义结构。因此, QRD算法在某种程度上可以取代定量的聚类分析方式。</p>
                </div>
                <h4 class="anchor-tag" id="90" name="90">1.2 最大总平均分布精度算法</h4>
                <div class="p1">
                    <p id="91">QRD算法最初用于处理数值属性数据集, 未考虑数据集的名词属性。为能够对名词属性数据集进行处理, 文献<citation id="264" type="reference">[<a class="sup">21</a>]</citation>提出一种最大总平均分布精度算法MTMDP, 该算法通过粗糙集理论计算最大总平均分布精度, 从而得出分类优先属性及优先值。</p>
                </div>
                <div class="p1">
                    <p id="92">一个数据表通常被称为一个信息系统, 假设<i>IS</i>= (<i>U</i>, <i>A</i>, <i>V</i>, <i>f</i><sub><i>a</i></sub>) <sub><i>a</i>∈<i>A</i></sub>为一个名词属性信息系统, 其中, <i>U</i>={<i>x</i><sub>1</sub>, <i>x</i><sub>2</sub>, …, <i>x</i><sub><i>n</i></sub>}为对象的非空有限集合, 也称为全集, <i>A</i>为属性的非空有限集合, <i>V</i>:∪<sub><i>a</i>∈<i>A</i></sub><i>V</i><sub><i>a</i></sub>, <i>V</i><sub><i>a</i></sub>是属性<i>a</i>的值域, 有限且无序, <i>f</i>:<i>U</i>×<i>A</i>→<i>V</i>是一个信息函数, 其对<i>U</i>中每个对象的每个属性赋予一个信息值。</p>
                </div>
                <div class="p1">
                    <p id="93">对于一个任意属性集<i>B</i>⊆<i>A</i>, 都存在等价关系<i>Ind</i> (<i>B</i>) ={ (<i>x</i>, <i>y</i>) ∈<i>U</i>×<i>U</i>|∀<i>a</i>∈<i>B</i>, <i>f</i><sub><i>a</i></sub> (<i>x</i>) =<i>f</i><sub><i>a</i></sub> (<i>y</i>) }。</p>
                </div>
                <div class="p1">
                    <p id="94">用<i>U</i>/<i>Ind</i> (<i>B</i>) 表示<i>Ind</i> (<i>B</i>) 将<i>U</i>分成一系列不相交的集合。</p>
                </div>
                <div class="p1">
                    <p id="95"><b>定义1</b> 对于属性集<i>B</i>, <i>X</i>中的每个对象<i>x</i>对于<i>X</i>⊆<i>U</i> (<i>X</i>≠Ø) 的概率粗糙隶属度<i>μ</i><mathml id="96"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>X</mi><mi>B</mi></msubsup></mrow></math></mathml> (<i>x</i>) 计算公式如下<citation id="265" type="reference"><link href="242" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>:</p>
                </div>
                <div class="p1">
                    <p id="97" class="code-formula">
                        <mathml id="97"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>μ</mi><msubsup><mrow></mrow><mi>X</mi><mi>B</mi></msubsup><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mo>=</mo><mi>p</mi><mo stretchy="false"> (</mo><mi>X</mi><mo stretchy="false">|</mo><mo stretchy="false">[</mo><mi>x</mi><mo stretchy="false">]</mo><msub><mrow></mrow><mi>B</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mfrac><mrow><mrow><mo stretchy="false">[</mo><mi>x</mi><mo stretchy="false">]</mo></mrow><msub><mrow></mrow><mi>B</mi></msub><mstyle displaystyle="true"><mo>∩</mo><mi>X</mi></mstyle></mrow><mrow><mrow><mo stretchy="false">[</mo><mi>x</mi><mo stretchy="false">]</mo></mrow><msub><mrow></mrow><mi>B</mi></msub></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="98"><b>定义2</b> 有<i>IS</i>= (<i>U</i>, <i>A</i>, <i>V</i>, <i>f</i> ) , <i>B</i>⊆<i>A</i>且<i>X</i>⊆<i>U</i> (<i>X</i>≠Ø) , 则基于属性集<i>B</i>的集合<i>X</i>的近似概率分布定义如下<citation id="266" type="reference"><link href="240" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>:</p>
                </div>
                <div class="p1">
                    <p id="99" class="code-formula">
                        <mathml id="99"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi>B</mi></mstyle><mrow><mspace width="0.25em" /><mo>∼</mo></mrow></mover><msup><mrow></mrow><mi>d</mi></msup><mo stretchy="false"> (</mo><mi>X</mi><mo stretchy="false">) </mo><mo>=</mo><mrow><mo>{</mo><mrow><mrow><mrow><mfrac><mrow><mi>μ</mi><msubsup><mrow></mrow><mi>X</mi><mi>B</mi></msubsup><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo></mrow><mi>x</mi></mfrac></mrow><mo>|</mo></mrow><mi>x</mi><mo>∈</mo><mi>X</mi></mrow><mo>}</mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="100">其中, 上标<i>d</i>表示近似分布。</p>
                </div>
                <div class="p1">
                    <p id="101"><b>定义3</b> 有<i>IS</i>= (<i>U</i>, <i>A</i>, <i>V</i>, <i>f</i> ) , <i>X</i>⊆<i>U</i> (<i>X</i>≠Ø) , 则基于属性集<i>B</i>的集合<i>X</i>的分布近似精度如下<citation id="267" type="reference"><link href="240" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>:</p>
                </div>
                <div class="p1">
                    <p id="102" class="code-formula">
                        <mathml id="102"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>r</mi><msubsup><mrow></mrow><mi>B</mi><mi>d</mi></msubsup><mo stretchy="false"> (</mo><mi>X</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mrow><mrow><mo>|</mo><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi>B</mi></mstyle><mrow><mspace width="0.25em" /><mo>∼</mo></mrow></mover><msup><mrow></mrow><mi>d</mi></msup><mo stretchy="false"> (</mo><mi>X</mi><mo stretchy="false">) </mo></mrow><mo>|</mo></mrow></mrow><mrow><mrow><mo>|</mo><mi>X</mi><mo>|</mo></mrow></mrow></mfrac><mo>=</mo><mfrac><mrow><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>x</mi><mo>∈</mo><mi>X</mi></mrow></munder><mrow></mrow></mstyle></mrow><mtext> </mtext><mi>μ</mi><msubsup><mrow></mrow><mi>X</mi><mi>B</mi></msubsup><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo></mrow><mrow><mrow><mo>|</mo><mi>X</mi><mo>|</mo></mrow></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="103">其中, <mathml id="104"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>|</mo><mo>⋅</mo><mo>|</mo></mrow></mrow></math></mathml>表示集合中的元素个数。</p>
                </div>
                <div class="p1">
                    <p id="105">显然, 0≤<i>r</i><mathml id="106"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>B</mi><mi>d</mi></msubsup></mrow></math></mathml> (<i>X</i>) ≤1, 如果<i>r</i><mathml id="107"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>B</mi><mi>d</mi></msubsup></mrow></math></mathml> (<i>X</i>) =1, 则<i>X</i>与其他类没有重合的对象。</p>
                </div>
                <div class="p1">
                    <p id="108"><b>定义4</b> 有<i>IS</i>= (<i>U</i>, <i>A</i>, <i>V</i>, <i>f</i> ) , <i>V</i> (<i>a</i><sub><i>i</i></sub>) 表示属性<i>a</i><sub><i>i</i></sub>值的集合, 则关于属性<i>a</i><sub><i>j</i></sub>的属性<i>a</i><sub><i>i</i></sub>的平均分布精度 (MDP) 定义如下<citation id="268" type="reference"><link href="240" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>:</p>
                </div>
                <div class="p1">
                    <p id="109" class="code-formula">
                        <mathml id="109"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Μ</mi><mi>D</mi><mi>Ρ</mi><msub><mrow></mrow><mrow><mi>a</mi><msub><mrow></mrow><mi>j</mi></msub></mrow></msub><mo stretchy="false"> (</mo><mi>a</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mfrac><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>X</mi><mo>∈</mo><mi>U</mi><mo>/</mo><mo stretchy="false">{</mo><mi>a</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">}</mo></mrow></munder><mi>r</mi></mstyle><msubsup><mrow></mrow><mrow><mo stretchy="false">{</mo><mi>a</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">}</mo></mrow><mi>d</mi></msubsup><mo stretchy="false"> (</mo><mi>X</mi><mo stretchy="false">) </mo></mrow><mrow><mrow><mo>|</mo><mrow><mi>V</mi><mo stretchy="false"> (</mo><mi>a</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo></mrow><mo>|</mo></mrow></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="110"><b>定义5</b> 有<i>IS</i>= (<i>U</i>, <i>A</i>, <i>V</i>, <i>f</i> ) , 则属性<i>a</i><sub><i>i</i></sub>的总平均分布精度 (TMDP) 定义如下<citation id="269" type="reference"><link href="240" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>:</p>
                </div>
                <div class="p1">
                    <p id="111" class="code-formula">
                        <mathml id="111"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Τ</mi><mi>Μ</mi><mi>D</mi><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>a</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mfrac><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>a</mi><msub><mrow></mrow><mi>j</mi></msub><mo>∈</mo><mi>A</mi><mo>∧</mo><mi>a</mi><msub><mrow></mrow><mi>j</mi></msub><mo>≠</mo><mi>a</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></munder><mi>Μ</mi></mstyle><mi>D</mi><mi>Ρ</mi><msub><mrow></mrow><mrow><mi>a</mi><msub><mrow></mrow><mi>j</mi></msub></mrow></msub><mo stretchy="false"> (</mo><mi>a</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo></mrow><mrow><mrow><mo>|</mo><mi>A</mi><mo>|</mo></mrow><mo>-</mo><mn>1</mn></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>5</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="112"><b>定义6</b> 有<i>IS</i>= (<i>U</i>, <i>A</i>, <i>V</i>, <i>f</i> ) , <i>X</i>是<i>U</i>的任意子集, 且<i>X</i>/<i>Ind</i>{<i>a</i>}={<i>X</i><sub>1</sub>, <i>X</i><sub>2</sub>, …, <i>X</i><sub><i>s</i></sub>}为属性<i>a</i> (<i>a</i>∈<i>A</i>) 诱导的等价类, 则关于属性<i>a</i>的<i>X</i>的粒度定义如下<citation id="270" type="reference"><link href="240" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>:</p>
                </div>
                <div class="p1">
                    <p id="113" class="code-formula">
                        <mathml id="113"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>G</mi><msub><mrow></mrow><mi>X</mi></msub><mo stretchy="false"> (</mo><mi>a</mi><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>s</mi></munderover><mrow><mfrac><mrow><mrow><mo>|</mo><mrow><mi>X</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mo>|</mo></mrow><msup><mrow></mrow><mn>2</mn></msup></mrow><mrow><mrow><mo>|</mo><mi>X</mi><mo>|</mo></mrow><msup><mrow></mrow><mn>2</mn></msup></mrow></mfrac></mrow></mstyle><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>6</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="114">根据定义6可以看出, 0&lt;<i>G</i><sub><i>X</i></sub> (<i>a</i>) ≤1。<i>G</i><sub><i>X</i></sub> (<i>a</i>) 的值越大, 则<i>X</i>相对于属性<i>a</i>的稳定性越大。<i>X</i>的内聚程度定义为:</p>
                </div>
                <div class="p1">
                    <p id="115" class="code-formula">
                        <mathml id="115"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>C</mi><mi>o</mi><mi>h</mi><mi>e</mi><mi>s</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo stretchy="false"> (</mo><mi>X</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>a</mi><mo>∈</mo><mi>A</mi></mrow></munder><mi>G</mi></mstyle><msub><mrow></mrow><mi>X</mi></msub><mo stretchy="false"> (</mo><mi>a</mi><mo stretchy="false">) </mo></mrow><mrow><mrow><mo>|</mo><mi>A</mi><mo>|</mo></mrow></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>7</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="116">显然, 0&lt;<i>Cohesion</i> (<i>X</i>) ≤1, <i>Cohesion</i> (<i>X</i>) 的值越大, 则<i>X</i>的内聚程度越高。</p>
                </div>
                <div class="p1">
                    <p id="117">设有一个名词属性信息系统<i>IS</i>= (<i>U</i>, <i>A</i>, <i>V</i>, <i>f</i> ) , 预先确定好的聚类数目为<mathml id="118"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>k</mi><mo stretchy="false"> (</mo><mi>k</mi><mo>&lt;</mo><mrow><mo>|</mo><mi>U</mi><mo>|</mo></mrow><mo stretchy="false">) </mo></mrow></math></mathml>, 以及通过某个聚类算法得到的聚类结果{<i>Y</i><sub>1</sub>, <i>Y</i><sub>2</sub>, …, <i>Y</i><sub><i>k</i></sub>}, 则平均内聚程度定义为:</p>
                </div>
                <div class="p1">
                    <p id="119" class="code-formula">
                        <mathml id="119"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Μ</mi><mi>e</mi><mi>a</mi><mi>n</mi><mi>C</mi><mi>o</mi><mi>h</mi><mi>e</mi><mi>s</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo stretchy="false"> (</mo><mo stretchy="false">{</mo><mi>Y</mi><msub><mrow></mrow><mn>1</mn></msub><mo>, </mo><mi>Y</mi><msub><mrow></mrow><mn>2</mn></msub><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>Y</mi><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false">}</mo><mo stretchy="false">) </mo><mo>=</mo><mfrac><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><mi>C</mi></mstyle><mi>o</mi><mi>h</mi><mi>e</mi><mi>s</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo stretchy="false"> (</mo><mi>Y</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo></mrow><mi>k</mi></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>8</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="120">MTMDP算法具体步骤<citation id="271" type="reference"><link href="240" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>如下:</p>
                </div>
                <div class="p1">
                    <p id="121"><b>步骤1</b> 初始化类簇数目<i>k</i>, <i>k</i>小于等于属性总数, 初始化聚类节点<i>CNode</i>为<i>U</i>。</p>
                </div>
                <div class="p1">
                    <p id="122"><b>步骤2</b> 计算数据集<i>CNode</i>中任意2个不同属性的等价类间的<i>MDP</i>值, 根据<i>MDP</i>值计算每个属性的<i>TMDP</i>值。</p>
                </div>
                <div class="p1">
                    <p id="123"><b>步骤3</b> 找到具有最大<i>TMDP</i>值的属性<i>a</i>, 在属性<i>a</i>相对于其他属性的<i>MDP</i>值中找到值最大的等价类, 以此为基准, 将聚类节点<i>CNode</i>分为2片叶子。</p>
                </div>
                <div class="p1">
                    <p id="124"><b>步骤4</b> 计算叶子总数, 若叶子总数等于<i>k</i>, 则算法终止。否则, 计算新增的2片叶子的内聚程度, 将内聚程度较小的叶子节点去掉属性<i>a</i>所在列的集合, 然后代替<i>CNode</i>作为新的聚类节点, 记录下内聚程度较大的叶子节点。跳转到步骤2继续执行。</p>
                </div>
                <h3 id="125" name="125" class="anchor-tag">2 粗糙聚类定性组合方案</h3>
                <div class="p1">
                    <p id="126">文献<citation id="272" type="reference">[<a class="sup">20</a>]</citation>提出一种聚类组合方案以分析聚类的语义结构。在该方案中, 先针对性地对各属性使用不同的粗糙聚类方法, 然后将各类簇进行合并, 同时使用经典优势关系来刻画各类簇之间的偏序关系。但是, 经典优势关系只能反映类簇之间的占优关系, 不能精细刻画样本之间的序结构。为解决该问题, 本文引入模糊优势关系来描述样本之间的序结构, 同时给出一种组合方案以处理合并过程中出现的类簇数目过多的问题。</p>
                </div>
                <h4 class="anchor-tag" id="127" name="127">2.1 模糊优势关系</h4>
                <div class="p1">
                    <p id="128">模糊优势关系<i>R</i>为笛卡儿积<i>U</i>×<i>U</i>的模糊子集<i>u</i><sub><i>r</i></sub>:<i>U</i>×<i>U</i>→[0, 1]。如果<i>U</i>是有穷集, 模糊优势关系也可用一个<i>n</i>×<i>n</i>的关系矩阵 (<i>r</i><sub><i>ij</i></sub>) <sub><i>n</i>×<i>n</i></sub>来描述, 其中, <i>r</i><sub><i>ij</i></sub>为<i>x</i><sub><i>i</i></sub>比<i>x</i><sub><i>j</i></sub>的优势程度, <i>r</i><sub><i>ij</i></sub>&gt;1/2时表示<i>x</i><sub><i>j</i></sub>比<i>x</i><sub><i>i</i></sub>占优, <i>r</i><sub><i>ij</i></sub>=1/2时表示<i>x</i><sub><i>i</i></sub>不比<i>x</i><sub><i>j</i></sub>占优, <i>r</i><sub><i>ij</i></sub>&lt;1/2时表示<i>x</i><sub><i>i</i></sub>比<i>x</i><sub><i>j</i></sub>占优, <i>r</i><sub><i>ij</i></sub>=1时表示<i>x</i><sub><i>j</i></sub>绝对比<i>x</i><sub><i>i</i></sub>占优。</p>
                </div>
                <div class="p1">
                    <p id="129">用式 (9) 确定各对象之间的模糊优势关系。</p>
                </div>
                <div class="p1">
                    <p id="130" class="code-formula">
                        <mathml id="130"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>r</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mtext>e</mtext><msup><mrow></mrow><mrow><mo>-</mo><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>j</mi></msub><mo>-</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo></mrow></msup></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>9</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <h4 class="anchor-tag" id="131" name="131">2.2 基于模糊优势关系的混合属性数据聚类方案</h4>
                <div class="p1">
                    <p id="132">在QRD算法的基础上, 本文引入模糊优势关系来精确描述各类簇之间的占优程度, 同时利用MTMDP算法处理名词属性数据。当算法出现类簇过多的情况时, 对某些类簇进行合并以便于分析。</p>
                </div>
                <div class="p1">
                    <p id="133">假设有待聚类的数据集<i>U</i>, 其中, 名词属性有<i>m</i>个, 数值属性有<i>n</i>个。首先将数据分成2个部分, 分别对应名词属性与数值属性。将名词属性数据集按照MTMDP算法实现聚类, 然后再对数值属性数据集中的每个属性进行处理。<i>rs</i><sub><i>v</i></sub>={<b><i>c</i></b><sub><i>v</i>1</sub>, <b><i>c</i></b><sub><i>v</i>2</sub>, …, <b><i>c</i></b><sub><i>vk</i><sub><i>v</i></sub></sub>}和<i>rs</i><sub><i>s</i></sub>={<b><i>c</i></b><sub><i>s</i>1</sub>, <b><i>c</i></b><sub><i>s</i>2</sub>, …, <b><i>c</i></b><sub><i>sk</i><sub><i>s</i></sub></sub>}是其中2个数值属性的粗糙聚类方案。在每个方案中, 都将类簇中心按从小到大排序, 由于第1个类簇与其自身等价, 同时根据式 (1) 可以发现<i>r</i><sub>11</sub>等于0.5。因此, 将最小类簇的模糊优势程度记为0.5并作为基准, 其他类簇的模糊优势程度 (<i>f</i>_<i>degree</i>) 通过式 (9) 进行计算。组合后类簇间的模糊优势程度定义为:</p>
                </div>
                <div class="area_img" id="134">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJC201908009_13400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9xck9nSjc1VXR3SDJsaTcyc24rQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="136">如果<i>f</i>_<i>degree</i> (<i>c</i><sub><i>l</i></sub>≻<sub><i>v</i>♁<i>s</i></sub><i>c</i><sub><i>m</i></sub>) &gt;1/2, 则<i>c</i><sub><i>l</i></sub>≻<sub><i>v</i>♁<i>s</i></sub><i>c</i><sub><i>m</i></sub>;如果<i>f</i>_<i>degree</i> (<i>c</i><sub><i>l</i></sub>≻<sub><i>v</i>♁<i>s</i></sub><i>c</i><sub><i>m</i></sub>) &lt;1/2, 则<i>c</i><sub><i>m</i></sub>≻<sub><i>v</i>♁<i>s</i></sub><i>c</i><sub><i>l</i></sub>;如果<i>f</i>_<i>degree</i> (<i>c</i><sub><i>l</i></sub>≻<sub><i>v</i>♁<i>s</i></sub><i>c</i><sub><i>m</i></sub>) =1/2, 则<i>c</i><sub><i>l</i></sub>～<sub><i>v</i>♁<i>s</i></sub><i>c</i><sub><i>m</i></sub>。</p>
                </div>
                <div class="p1">
                    <p id="137">2个属性组合后得到的新类簇再与第3个属性进行组合, 依此类推, 将所有类簇组合完毕, 再与通过MTMDP算法得到的名词属性类簇求交集, 在此基础上, 采用与QRD算法相同的方法消除下近似集为空的类簇后得到最终聚类结果。</p>
                </div>
                <div class="p1">
                    <p id="138">从max (<i>k</i><sub>1</sub>, <i>k</i><sub>2</sub>, …, <i>k</i><sub><i>n</i></sub>) ≤<i>k</i><sub>all</sub>≤<i>k</i><sub>1</sub>·<i>k</i><sub>2</sub>·…·<i>k</i><sub><i>n</i></sub>可以看出, 随着属性的增多, 组合后新的类簇数目在某些情况下呈几何增长。因此, 必须将一部分类簇进行合并, 以便对聚类结果进行分析。在合并的过程中, 选择<i>f</i>_<i>degree</i> (<i>c</i><sub>all<i>i</i></sub>≻<sub>all</sub><i>c</i><sub>all<i>j</i></sub>) 最小的类簇, 即将类簇间模糊优势关系差值最小的类簇进行合并, 合并后类簇的模糊优势程度定义为合并前这2个值的均值。重复这一过程, 直到类簇数目缩减到预期值。</p>
                </div>
                <h4 class="anchor-tag" id="139" name="139">2.3 算法步骤</h4>
                <div class="p1">
                    <p id="140">本文主要处理混合型数据。将待处理的数据分成2个部分:数值属性数据使用Rough K-means算法进行处理, 名词属性数据使用MTMDP算法实现聚类。然后, 将类簇分别进行组合并计算出各类簇之间的模糊优势关系。本文基于模糊优势关系的粗糙聚类定性组合算法描述如下:</p>
                </div>
                <div class="p1">
                    <p id="141"><b>输入</b> 待聚类的数据集 (名词属性为<i>m</i><sub>1</sub>个, 数值属性为<i>m</i><sub>2</sub>个) , 类簇数目<i>k</i></p>
                </div>
                <div class="p1">
                    <p id="142"><b>输出</b><i>k</i>个类簇以及类簇之间的优势程度</p>
                </div>
                <div class="p1">
                    <p id="143"><b>步骤1</b> 将名词属性数据按照MTMDP算法进行聚类。</p>
                </div>
                <div class="p1">
                    <p id="144"><b>步骤2</b> 每个数值属性数据按照属性特征选择合适的粗糙聚类算法, 根据式 (9) 计算模糊优势程度。</p>
                </div>
                <div class="p1">
                    <p id="145"><b>步骤3</b> 将步骤1、步骤2中得到的类簇依次进行组合, 清除下近似集为空的类簇后, 计算新类簇间的模糊优势程度。</p>
                </div>
                <div class="p1">
                    <p id="146"><b>步骤4</b> 检测剩余类簇数目是否大于指定的类簇数目<i>k</i>, 若大于, 则进行合并, 重复步骤4直到剩余类簇数目等于<i>k</i>。</p>
                </div>
                <div class="p1">
                    <p id="147"><b>步骤5</b> 输出各类簇以及其之间的优势程度。</p>
                </div>
                <h4 class="anchor-tag" id="148" name="148">2.4 算法时间复杂度分析</h4>
                <div class="p1">
                    <p id="149">假设数据集有<i>n</i>个对象、<i>m</i><sub>1</sub>个名词属性和<i>m</i><sub>2</sub>个数值属性, 算法步骤1和步骤2按属性分别进行聚类的总时间复杂度为<i>O</i> ( (<i>m</i><sub>2</sub>+1) <i>nkt</i>) , 其中, <i>k</i>为聚类过程中的最大类簇数目, <i>t</i>为按属性进行聚类的最大迭代次数, 步骤3的时间复杂度为<i>O</i> (<i>k</i><sup><i>m</i><sub>2</sub></sup>) 。因此, 算法总的时间复杂度为<i>O</i> (<i>m</i><sub>2</sub><i>nkt</i>+<i>k</i><sup><i>m</i><sub>2</sub></sup>) 。</p>
                </div>
                <h3 id="150" name="150" class="anchor-tag">3 实验结果与分析</h3>
                <h4 class="anchor-tag" id="151" name="151">3.1 聚类结果评价标准</h4>
                <div class="p1">
                    <p id="152">本文采用聚类准确率作为评价标准, 其计算公式如下<citation id="273" type="reference"><link href="244" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>:</p>
                </div>
                <div class="p1">
                    <p id="153" class="code-formula">
                        <mathml id="153"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>c</mi><mo>=</mo><mfrac><mrow><mi>ω</mi><msub><mrow></mrow><mrow><mi>l</mi><mi>o</mi><mi>w</mi><mi>e</mi><mi>r</mi></mrow></msub><mo>⋅</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>l</mi><mi>o</mi><mi>w</mi><mi>e</mi><mi>r</mi></mrow></munderover><mi>a</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mo>+</mo><mi>ω</mi><msub><mrow></mrow><mrow><mi>u</mi><mi>p</mi><mi>p</mi><mi>e</mi><mi>r</mi></mrow></msub><mo>⋅</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>u</mi><mi>p</mi><mi>p</mi><mi>e</mi><mi>r</mi></mrow></munderover><mi>a</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub></mrow><mi>n</mi></mfrac></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="154">其中, <i>a</i><sub><i>i</i></sub>表示对象正确分类到第<i>i</i>个类簇的个数, <i>ω</i><sub><i>upper</i></sub>和<i>ω</i><sub><i>lower</i></sub>分别代表上、下近似集的权重。</p>
                </div>
                <h4 class="anchor-tag" id="155" name="155">3.2 UCI数据集测试分析</h4>
                <div class="p1">
                    <p id="156">为验证算法的有效性, 本文选取UCI数据库中的Heart、Zoo、Contraceptive Method Choice (CMC) 、Teaching、Wine和Iris共6个数据集进行测试, 其中, Wine和Iris为数值型数据, 其余4个数据集为混合型数据。在4个混合型数据集上将f-QRD算法与K-prototypes算法<citation id="274" type="reference"><link href="220" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>、IK-prototypes算法<citation id="275" type="reference"><link href="222" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>、基于模糊集的混合数据聚类算法FPCM<citation id="276" type="reference"><link href="246" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>进行对比分析, 在Iris和Wine 2个数值型数据集上, 将f-QRD算法与QRD算法进行比较。实验环境如下:操作系统为Windows 10, 主频为3.3 GHz的CPU i5 4590, 内存为8 GB, 仿真软件为Matlab 7.10.0。6个数据集的特征信息如表1所示。</p>
                </div>
                <div class="area_img" id="157">
                    <p class="img_tit"><b>表1 数据集信息</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="157" border="1"><tr><td><br />数据集</td><td>数据量</td><td>数值属性<br />个数</td><td>名词属性<br />个数</td><td>分类数</td></tr><tr><td><br />Heart</td><td>178</td><td>9</td><td>5</td><td>3</td></tr><tr><td><br />Zoo</td><td>101</td><td>15</td><td>1</td><td>7</td></tr><tr><td><br />CMC</td><td>1 473</td><td>2</td><td>7</td><td>3</td></tr><tr><td><br />Teaching</td><td>2 310</td><td>1</td><td>4</td><td>3</td></tr><tr><td><br />Iris</td><td>150</td><td>7</td><td>0</td><td>3</td></tr><tr><td><br />Wine</td><td>43 500</td><td>9</td><td>0</td><td>5</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="158">4种算法在4个混合属性数据集上的实验结果如表2所示。</p>
                </div>
                <div class="area_img" id="159">
                    <p class="img_tit"><b>表2 混合属性数据集上的算法准确率对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="159" border="1"><tr><td>数据集</td><td>K-prototypes</td><td>IK-prototypes</td><td>FPCM</td><td>f-QRD</td></tr><tr><td><br />Heart</td><td>0.593</td><td>0.429</td><td>0.687</td><td>0.654</td></tr><tr><td><br />Zoo</td><td>0.406</td><td>0.419</td><td>0.278</td><td>0.767</td></tr><tr><td><br />CMC</td><td>0.179</td><td>0.389</td><td>0.331</td><td>0.646</td></tr><tr><td><br />Teaching</td><td>0.258</td><td>0.337</td><td>0.378</td><td>0.414</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="160">通过f-QRD算法得到4个混合属性数据集上各类簇之间的模糊优势关系如下:</p>
                </div>
                <div class="p1">
                    <p id="161">Heart数据集:0.506 7, 0.509 3, 0.511 6。</p>
                </div>
                <div class="p1">
                    <p id="162">Zoo数据集:0.500 0, 0.526 8, 0.526 9, 0.587 4, 0.592 0, 0.598 9, 0.620 3。</p>
                </div>
                <div class="p1">
                    <p id="163">CMC数据集:0.507 8, 0.515 5, 0.523 3。</p>
                </div>
                <div class="p1">
                    <p id="164">Teaching数据集:0.500 0, 0.655 6, 0.664 5。</p>
                </div>
                <div class="p1">
                    <p id="165">在数值属性数据集上, f-QRD算法与QRD算法比较结果如下:</p>
                </div>
                <div class="p1">
                    <p id="166">1) Wine数据集按照QRD算法进行聚类后, 类簇数目达到84, 碎片过多, 正确率极低, 无分析意义。而f-QRD算法的类簇数目是预先设定的3个, 其准确率达到0.65, 各类簇之间的优势关系为0.500 1、0.527 9、0.700 5。</p>
                </div>
                <div class="p1">
                    <p id="167">2) Iris数据集按照QRD算法进行聚类后, 类簇数目达到17, 而f-QRD算法的类簇数目为预先设定的3个, 其准确率达到0.627, 各类簇之间的优势关系为0.500 0、0.504 7、0.507 7。</p>
                </div>
                <div class="p1">
                    <p id="168">由上述结果可以看出, f-QRD算法适用范围较广, 除在Heart数据集上的准确率略低于FPCM算法外, 在其他混合型数据集上准确率均最高, 且相较于QRD算法中的优势关系, f-QRD算法中的模糊优势关系更加精准。此外, 在针对数值型数据的聚类分析中, 虽然与其他定量算法的准确率仍存在一定差距, 但f-QRD算法有效解决了QRD算法在数值属性个数增多时碎片过多的问题。</p>
                </div>
                <h3 id="169" name="169" class="anchor-tag">4 结束语</h3>
                <div class="p1">
                    <p id="170">聚类定性组合方案可以分析聚类的语义结构并提高聚类精度。本文对QRD算法进行扩展, 提出一种基于模糊优势关系的粗糙聚类定性组合算法f-QRD, 以适用混合属性数据的处理。实验结果表明, 该算法准确率较高, 其模糊优势关系较QRD算法更精准。但本文算法未挖掘各个属性粒之间的关系, 因此, 下一步将引入阴影集对属性粒之间的连接关系进行分析, 以提高f-QRD算法的精度。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="200">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Least squares quantization in PCM">

                                <b>[1]</b> LLOYD S P.Least squares quantization in PCM[J].IEEE Transactions on Information Theory, 1982, 28 (2) :129-137.
                            </a>
                        </p>
                        <p id="202">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Clustering by fast search and find of densitypeaks">

                                <b>[2]</b> RODRIGUEZ A, LAIO A.Clustering by fast search and find of density peaks[J].Science, 2014, 344 (6191) :1492-1496.
                            </a>
                        </p>
                        <p id="204">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise">

                                <b>[3]</b> ESTER M, KRIEGEL H P, SANDER J, et al.A density-based algorithm for discovering clusters in large spatial databases with noise[C]//Proceedings of the 2nd International Conference on Knowledge Discovery and Data Mining.[S.l.]:AAAI Press, 1996:226-231.
                            </a>
                        </p>
                        <p id="206">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201607028&amp;v=MDQzMTdMckxMejdCYmJHNEg5Zk1xSTlIYklRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJxRnkvZ1U=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9xck9nSjc1VXR3SDJsaTcyc24rQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b> 许合利, 牛丽君.基于层次与密度的任意形状聚类算法[J].计算机工程, 2016, 42 (7) :159-164.
                            </a>
                        </p>
                        <p id="208">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Rough set clustering for web mining">

                                <b>[5]</b> LINGRAS P.Rough set clustering for Web mining[C]//Proceedings of 2002 IEEE International Conference on Fuzzy Systems.Washington D.C., USA:IEEE Press, 2002:1039-1044.
                            </a>
                        </p>
                        <p id="210">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00001510032&amp;v=MjIwNjJxZWJ1ZHRGQ3psVkxyTEpWMD1OajdCYXJPNEh0SE5xbzVGWk9nTlkzazV6QmRoNGo5OVNYcVJyeG94Y01IN1I3&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9xck9nSjc1VXR3SDJsaTcyc24rQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b> LINGRAS P, WEST C.Interval set clustering of Web users with Rough K-means[J].Journal of Intelligent Information Systems, 2004, 23 (1) :5-16.
                            </a>
                        </p>
                        <p id="212">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Outliers in rough k-means clustering">

                                <b>[7]</b> PETERS G.Outliers in Rough K-means clustering[J].Lecture Notes in Computer Science, 2005, 3776 (11) :702-707.
                            </a>
                        </p>
                        <p id="214">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14032300029442&amp;v=MTEyNTA0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVVMdklKMXNYYnhFPU5pZk9mYks4SHRMT3JJOUZaT2tHQ0hnN29CTVQ2VA==&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9xck9nSjc1VXR3SDJsaTcyc24rQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b> PETERS G.Rough clustering utilizing the principle of indifference[J].Information Science, 2014, 277 (2) :358-374.
                            </a>
                        </p>
                        <p id="216">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=KZYC201711004&amp;v=MzE5MjFqZlNiYkc0SDliTnJvOUZZSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUnFGeS9nVUxyTEw=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9xck9nSjc1VXR3SDJsaTcyc24rQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b> 马福民, 逯瑞强, 张腾飞.基于边界区域局部模糊增强的πRKM聚类算法[J].控制与决策, 2017, 32 (11) :1949-1956.
                            </a>
                        </p>
                        <p id="218">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Improved rough k-means clustering algorithm based on weighted distance measure with Gaussian function">

                                <b>[10]</b> ZHANG Tengfei, MA Fumin.Improved Rough K-means clustering algorithm based on weighted distance measure with Gaussian function[J].International Journal of Computer Mathematics, 2017, 94 (4) :663-675.
                            </a>
                        </p>
                        <p id="220">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Clustering large data sets with mixed numeric and categorical values">

                                <b>[11]</b> HUANG Zhexue.Clustering large data sets with mixed numeric and categorical values[C]//Proceedings of the 1st Pacific-Asia Conference on Knowledge Discovery and Data Mining.Washington D.C., USA:IEEE Press, 1997:21-34.
                            </a>
                        </p>
                        <p id="222">
                            <a id="bibliography_12" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES959D989A1159D3805B9013BD9D3B44CF&amp;v=MDI3NTU4eGFzPU5pZk9mYnE5RjZYRnA0WTBaZW9LQlFnNnh4WVdtRFo5U1h5UTJCdEJlc0NRUWNucENPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkNwYlEzNU54aHc3Mg==&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9xck9nSjc1VXR3SDJsaTcyc24rQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[12]</b> JI Jinchao, BAI Tian, ZHOU Chunguang, et al.An improved K-prototypes clustering algorithm for mixed numeric and categorical data[J].Neurocomputing, 2013, 120:590-596.
                            </a>
                        </p>
                        <p id="224">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Unsupervised learning with mixed numeric and nominal data">

                                <b>[13]</b> LI Cen, BISWAS G.Unsupervised learning with mixed numeric and nominal data[J].IEEE Transactions on Knowledge and Data Engineering, 2002, 14 (4) :673-690.
                            </a>
                        </p>
                        <p id="226">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Subspace clustering of categorical and numerical data with an unknown number of clusters">

                                <b>[14]</b> JIA Hong, CHEUNG Yiuming.Subspace clustering of categorical and numerical data with an unknown number of clusters[J].IEEE Transactions on Neural Networks and Learning Systems, 2018, 29 (8) :3308-3325.
                            </a>
                        </p>
                        <p id="228">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Mixed fuzzy clustering for deriving predictive models in intensive care units">

                                <b>[15]</b> SALGADO C M, VIEIRA S M, SOUSA J M C.Mixed fuzzy clustering for deriving predictive models in intensive care units[M]//KAHRAMAN C, TOPCU Y I.Operations research applications in health care management.Berlin, Germany:Springer, 2018.
                            </a>
                        </p>
                        <p id="230">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Utility theory for decision making">

                                <b>[16]</b> FISHBURN P C.Utility theory for decision making[J].Publications in Operations Research, 1970, 22:308-309.
                            </a>
                        </p>
                        <p id="232">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Decision-theoretic rough set models">

                                <b>[17]</b> YAO Yiyu.Decision-theoretic rough set models[C]//Proceedings of International Conference on Rough Sets and Knowledge Technology.Berlin, Germany:Springer, 2007:1-12.
                            </a>
                        </p>
                        <p id="234">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A decision theoretic framework for approximating concepts">

                                <b>[18]</b> YAO Y Y, WONG S K M.A decision theoretic framework for approximating concepts[J].International Journal of Man-Machine Studies, 1992, 37 (6) :793-809.
                            </a>
                        </p>
                        <p id="236">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Interval set cluster analysis:a re-formulation">

                                <b>[19]</b> YAO Yiyu, LINGRAS P, WANG Ruichu, et al.Interval set cluster analysis:a re-formulation[C]//Proceedings of International Workshop on Rough Sets, Fuzzy Sets, Data Mining, and Granular-Soft Computing.Berlin, Germany:Springer, 2009:398-405.
                            </a>
                        </p>
                        <p id="238">
                            <a id="bibliography_20" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESE08B5B02BE6BDDFDB8F8E4E1F09FC0C0&amp;v=MTU0MzhmQ3BiUTM1TnhodzcyOHhhcz1OaWZPZmNhNEZxUEozWTlIRnA0SmZnaE51V0poNGtsMVBYdVhyV1ExY01UblJjbWZDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbA==&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9xck9nSjc1VXR3SDJsaTcyc24rQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[20]</b> LINGRAS P, CHEN Min, MIAO Duoqian.Qualitative and quantitative combinations of crisp and rough clustering schemes using dominance relations[J].International Journal of Approximate Reasoning, 2014, 55 (1) :238-258.
                            </a>
                        </p>
                        <p id="240">
                            <a id="bibliography_21" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14061700208648&amp;v=MTU5NTl3WmVadUh5am1VTHZJSjFzWGJ4RT1OaWZPZmJLOEh0Zk5xSTlGWnVzSENuZ3hvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbg==&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9xck9nSjc1VXR3SDJsaTcyc24rQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[21]</b> LI Min, DENG Shaobo, WANG Lei, et al.Hierarchical clustering algorithm for categorical data using a probabilistic rough set model[J].Knowledge-based Systems, 2014, 65:60-71.
                            </a>
                        </p>
                        <p id="242">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Advances in the Dempster-Shafer Theory of Evidence">

                                <b>[22]</b> YAGER R R, KACPRZYK J, FEDRIZZI M.Advances in the dempster-shafer theory of evidence[M].[S.l.]:John Wiley and Sons, Inc., 1994.
                            </a>
                        </p>
                        <p id="244">
                            <a id="bibliography_23" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CDFD&amp;filename=1014130233.nh&amp;v=MTM5MzFQckpFYlBJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVScUZ5L2dVTHJMVkYyNkdySzdIdFA=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9xck9nSjc1VXR3SDJsaTcyc24rQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[23]</b> 王佐.基于粗糙集的聚类算法研究[D].长春:吉林大学, 2013.
                            </a>
                        </p>
                        <p id="246">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Clustering of mixed data by integrating fuzzy,probabilistic and collaborative clustering framework">

                                <b>[24]</b> PATHAK A, PAL N R.Clustering of mixed data by integrating fuzzy, probabilistic, and collaborative clustering framework[J].International Journal of Fuzzy Systems, 2016, 18 (3) :339-348.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->

            <div class="btn-downloads">
                <span>
                    <a class="caj" target="_blank" href="http://kns.cnki.net/kns/download.aspx?filename=0pGeh5UcwIUd5A1YNllcvlGexVFTZZWeJNVOxcHcrsmYUBHOItUcJ1UYTpHR1lUeJJ3a5QFUE1Ed5VETxpER5IzVxE1R0lkQxIEW5NUW4AHMxV0TVVlYLtWbLF0Z3wmMLVUWFVGdn1mQ2xGT2ZDcMZldkNlbVdne&tablename=CJFDLAST2019">CAJ下载</a>
                    <a class="pdf" target="_blank" href="http://kns.cnki.net/kns/download.aspx?filename=0pGeh5UcwIUd5A1YNllcvlGexVFTZZWeJNVOxcHcrsmYUBHOItUcJ1UYTpHR1lUeJJ3a5QFUE1Ed5VETxpER5IzVxE1R0lkQxIEW5NUW4AHMxV0TVVlYLtWbLF0Z3wmMLVUWFVGdn1mQ2xGT2ZDcMZldkNlbVdne&tablename=CJFDLAST2019&dflag=pdfdown">PDF下载</a>
                </span>
                <p>永久保存本文,请下载至本地</p>
            </div>


    </div>

        <input id="fileid" type="hidden" value="JSJC201908009" />
        <input id="dpi" type="hidden" value="600" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcTdWa2FjcW9xck9nSjc1VXR3SDJsaTcyc24rQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201908009&amp;v=MDE5NTVxcUJ0R0ZyQ1VSTE9lWmVScUZ5L2dVTHJMTHo3QmJiRzRIOWpNcDQ5RmJZUUtESDg0dlI0VDZqNTRPM3o=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9xck9nSjc1VXR3SDJsaTcyc24rQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
            <a class="icon" id="copytext" target="_blank" href="http://x.cnki.net/search/common/testlunbo?dbcode=CJFD&amp;tablename=CJFDLAST2019&amp;filename=JSJC201908009&amp;filesourcetype=1">划线</a>
            <a class="icon" id="copytext" target="_blank" href="http://x.cnki.net/search/common/testlunbo?dbcode=CJFD&amp;tablename=CJFDLAST2019&amp;filename=JSJC201908009&amp;filesourcetype=1">笔记</a>
            <a class="icon" id="copytext" target="_blank" href="http://x.cnki.net/search/common/testlunbo?dbcode=CJFD&amp;tablename=CJFDLAST2019&amp;filename=JSJC201908009&amp;filesourcetype=1">文摘</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
