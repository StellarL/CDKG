<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637130386466238750%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJC201906038%26RESULT%3d1%26SIGN%3dC%252fRiXFYhnO%252f8976IkQZQUAtpza4%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201906038&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201906038&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201906038&amp;v=MDI0NzdJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSb0Z5bmxWYnpQTHo3QmJiRzRIOWpNcVk5R2I=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#35" data-title="0 概述 ">0 概述</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#40" data-title="1 相关工作 ">1 相关工作</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#41" data-title="1.1 评分与评论挖掘">1.1 评分与评论挖掘</a></li>
                                                <li><a href="#44" data-title="1.2 终身机器学习">1.2 终身机器学习</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#47" data-title="2 评论主题挖掘与评分预测联合模型 ">2 评论主题挖掘与评分预测联合模型</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#58" data-title="3 终身机器学习的联合模型 ">3 终身机器学习的联合模型</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#61" data-title="3.1 历史知识获取">3.1 历史知识获取</a></li>
                                                <li><a href="#66" data-title="3.2 历史知识使用">3.2 历史知识使用</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#104" data-title="4 实验结果与分析 ">4 实验结果与分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#108" data-title="4.1 数据集">4.1 数据集</a></li>
                                                <li><a href="#111" data-title="4.2 参数与实验设置">4.2 参数与实验设置</a></li>
                                                <li><a href="#118" data-title="4.3 结果分析">4.3 结果分析</a></li>
                                                <li><a href="#122" data-title="4.4 主题词汇发现">4.4 主题词汇发现</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#127" data-title="5 结束语 ">5 结束语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#60" data-title="&lt;b&gt;图1 基于终身学习的评分预测模型架构&lt;/b&gt;"><b>图1 基于终身学习的评分预测模型架构</b></a></li>
                                                <li><a href="#110" data-title="&lt;b&gt;表1 用于知识积累的历史任务的数据集参数&lt;/b&gt;"><b>表1 用于知识积累的历史任务的数据集参数</b></a></li>
                                                <li><a href="#121" data-title="&lt;b&gt;图2 历史知识的增加对任务的影响&lt;/b&gt;"><b>图2 历史知识的增加对任务的影响</b></a></li>
                                                <li><a href="#124" data-title="&lt;b&gt;表2 HFT模型的主题词汇训练结果&lt;/b&gt;"><b>表2 HFT模型的主题词汇训练结果</b></a></li>
                                                <li><a href="#125" data-title="&lt;b&gt;表3 终身学习模型主题词汇训练结果&lt;/b&gt;"><b>表3 终身学习模型主题词汇训练结果</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" title=" 金紫嫣, 张娟, 李向军, 等.一种带标签的协同过滤广告推荐算法[J].计算机工程, 2018, 44 (4) :236-242, 247." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201804038&amp;v=MTE5NDk5bk1xNDlHYklRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJvRnlubFZielBMejdCYmJHNEg=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         金紫嫣, 张娟, 李向军, 等.一种带标签的协同过滤广告推荐算法[J].计算机工程, 2018, 44 (4) :236-242, 247.
                                    </a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" title=" MCAULEY J, LESKOVEC J.From amateurs to connoisseurs:modeling the evolution of user expertise through online reviews [C]//Proceedings of the 22nd International Conference on World Wide Web.York, USA:ACM Press, 2015:897-908." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=From amateurs to connoisseurs:modeling the evolution of user expertise through online reviews">
                                        <b>[2]</b>
                                         MCAULEY J, LESKOVEC J.From amateurs to connoisseurs:modeling the evolution of user expertise through online reviews [C]//Proceedings of the 22nd International Conference on World Wide Web.York, USA:ACM Press, 2015:897-908.
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title=" MNIH A, SALAKHUTDINOV R.Probabilistic matrix factorization [C]//Proceedings of the 20th International Conference on Neural Information Processing Systems.New York, USA:Curran Associates Inc., 2008:1257-1264." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Probabilistic matrix factorization">
                                        <b>[3]</b>
                                         MNIH A, SALAKHUTDINOV R.Probabilistic matrix factorization [C]//Proceedings of the 20th International Conference on Neural Information Processing Systems.New York, USA:Curran Associates Inc., 2008:1257-1264.
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" title=" WANG Chong, BLEI D M.Collaborative topic modeling for recommending scientific articles [C]//Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.New York, USA:ACM Press, 2011:448-456." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Collaborative Topic Modeling for Recommending Scientific Articles">
                                        <b>[4]</b>
                                         WANG Chong, BLEI D M.Collaborative topic modeling for recommending scientific articles [C]//Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.New York, USA:ACM Press, 2011:448-456.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" title=" MCAULEY J, JURE L.Hidden factors and hidden topics:understanding rating dimensions with review text [C]//Proceedings of the 7th ACM Conference on Recommender Systems.New York, USA:ACM Press, 2013:165-172." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Hidden factors and hidden topics:understanding rating dimensions with review text">
                                        <b>[5]</b>
                                         MCAULEY J, JURE L.Hidden factors and hidden topics:understanding rating dimensions with review text [C]//Proceedings of the 7th ACM Conference on Recommender Systems.New York, USA:ACM Press, 2013:165-172.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" title=" DIAO Qiming, QIU Minghui, WU Chaoyuan, et al.Jointly modeling aspects, ratings and sentiments for movie recommendation[C]//Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.New York, USA:ACM Press, 2014:193-202." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Jointly modeling aspects ratings and sentiments for movie recom-mendation (JMARS)">
                                        <b>[6]</b>
                                         DIAO Qiming, QIU Minghui, WU Chaoyuan, et al.Jointly modeling aspects, ratings and sentiments for movie recommendation[C]//Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.New York, USA:ACM Press, 2014:193-202.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" title=" LING Guang, LYU M R, KING R.Ratings meet reviews, a combined approach to recommend [C]//Proceedings of the 8th ACM Conference on Recom-mender Systems.New York, USA:ACM Press, 2014:105-112." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Ratings meet reviews, a combined approach to recommend">
                                        <b>[7]</b>
                                         LING Guang, LYU M R, KING R.Ratings meet reviews, a combined approach to recommend [C]//Proceedings of the 8th ACM Conference on Recom-mender Systems.New York, USA:ACM Press, 2014:105-112.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" title=" CHEN Zhiyuan, LIU Bing.Lifelong machine learning[M].San Rafael, USA:Morgan and Claypool Publishers, 2016." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Lifelong machine learning">
                                        <b>[8]</b>
                                         CHEN Zhiyuan, LIU Bing.Lifelong machine learning[M].San Rafael, USA:Morgan and Claypool Publishers, 2016.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" title=" HU Guangneng, DAI Xinyu, SONG Yunya, et al.A synthetic approach for recommendation:combining ratings, social relations, and reviews[C]//Proceedings of the 24th International Conference on Artificial Intelligence.New York, USA:ACM Press, 2015:1756-1762." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A synthetic approach for recommendation:combining ratings,social relations,and reviews">
                                        <b>[9]</b>
                                         HU Guangneng, DAI Xinyu, SONG Yunya, et al.A synthetic approach for recommendation:combining ratings, social relations, and reviews[C]//Proceedings of the 24th International Conference on Artificial Intelligence.New York, USA:ACM Press, 2015:1756-1762.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_10" title=" THRUN S.Is learning the n-th thing any easier than learning the first[C]//Proceedings of the 8th International Con-ference on Neural Information Processing Systems.Cambridge, USA:MIT Press, 1996:640-646." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Is learning the n-th thing any easier than learning the first">
                                        <b>[10]</b>
                                         THRUN S.Is learning the n-th thing any easier than learning the first[C]//Proceedings of the 8th International Con-ference on Neural Information Processing Systems.Cambridge, USA:MIT Press, 1996:640-646.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_11" title=" FEI Geli, WANG Shuai, LIU Bing.Learning cumulatively to become more knowledgeable[C]//Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.New York, USA:ACM Press, 2016:1565-1574." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning Cumulatively to Become More Knowledgeable">
                                        <b>[11]</b>
                                         FEI Geli, WANG Shuai, LIU Bing.Learning cumulatively to become more knowledgeable[C]//Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.New York, USA:ACM Press, 2016:1565-1574.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_12" title=" SHU Lei, LIU Bin, XU Bin, et al.Lifelong-rl:lifelong relaxation labeling for separating entities and aspects in opinion targets [EB/OL].[2018-03-24].https://www.aclweb.org/anthology/D16-1022." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Lifelong-rl:lifelong relaxation labeling for separating entities and aspects in opinion targets">
                                        <b>[12]</b>
                                         SHU Lei, LIU Bin, XU Bin, et al.Lifelong-rl:lifelong relaxation labeling for separating entities and aspects in opinion targets [EB/OL].[2018-03-24].https://www.aclweb.org/anthology/D16-1022.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_13" title=" CHEN Zhiyuan, LIU Bing.Topic modeling using topics from many domains, lifelong learning and big data [C]//Proceedings of International Conference on Machine Learning.New York, USA:ACM Press, 2014:703-711." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Topic Modeling using Topics from Many Domains,Lifelong Learning and Big Data">
                                        <b>[13]</b>
                                         CHEN Zhiyuan, LIU Bing.Topic modeling using topics from many domains, lifelong learning and big data [C]//Proceedings of International Conference on Machine Learning.New York, USA:ACM Press, 2014:703-711.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_14" title=" KAPOOR A, HORVITZ E.Principles of lifelong learning for predictive user modeling [C]//Proceedings of International Conference on User Modeling.Berlin, Germany:Springer, 2007:37-46." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Principles of lifelong learning for predictive user modeling">
                                        <b>[14]</b>
                                         KAPOOR A, HORVITZ E.Principles of lifelong learning for predictive user modeling [C]//Proceedings of International Conference on User Modeling.Berlin, Germany:Springer, 2007:37-46.
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_15" title=" LIU Qian, LIU Bing, ZHANG Yuanlin, et al.Improving opinion aspect extraction using semantic similarity and aspect associations [EB/OL].[2018-03-24].https://aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/1 1973/12051." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Improving opinion aspect extraction using semantic similarity and aspect associations">
                                        <b>[15]</b>
                                         LIU Qian, LIU Bing, ZHANG Yuanlin, et al.Improving opinion aspect extraction using semantic similarity and aspect associations [EB/OL].[2018-03-24].https://aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/1 1973/12051.
                                    </a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_16" >
                                        <b>[16]</b>
                                     BLEI D M, NG A Y, JORDAN M I.Latent dirichlet allocation[J].Journal of Machine Learning Research, 2003, 3:993-1022.</a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJC" target="_blank">计算机工程</a>
                2019,45(06),237-241+248 DOI:10.19678/j.issn.1000-3428.0051131            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于终身机器学习的主题挖掘与评分预测联合模型</b></span>
 <span class="shoufa"></span>                                     </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%88%98%E4%B8%80%E5%AE%81&amp;code=25495205&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">刘一宁</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%94%B3%E5%BD%A6%E6%98%8E&amp;code=17503454&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">申彦明</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%A4%A7%E8%BF%9E%E7%90%86%E5%B7%A5%E5%A4%A7%E5%AD%A6%E7%94%B5%E5%AD%90%E4%BF%A1%E6%81%AF%E4%B8%8E%E7%94%B5%E6%B0%94%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%83%A8&amp;code=0222286&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">大连理工大学电子信息与电气工程学部</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>为充分利用历史知识, 提高评分预测精度, 基于终身机器学习 (LML) 机制提出一种同时挖掘用户评分和评论的推荐模型。在执行任务时积累知识并用于后续任务的训练, 提高评分预测精度。在真实数据集上的实验结果表明, 与无LML能力的模型相比, 该模型预测评分的均方误差降低5.4‰, 且随着知识的积累, 误差不断降低, 提高了主题词语分类的精度。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%96%87%E6%9C%AC%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">文本主题模型;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">推荐算法;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%BB%88%E8%BA%AB%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">终身机器学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%AF%84%E5%88%86%E9%A2%84%E6%B5%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">评分预测;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">协同过滤;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    刘一宁 (1984—) , 男, 博士研究生, 主研方向为推荐算法、自然语言处理;;
                                </span>
                                <span>
                                    申彦明, 教授、博士。E-mail: shen@dlut.edu.cn;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-04-09</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金重点项目 (61432002);</span>
                                <span>中央高校基本科研业务费专项资金 (DUT17ZD303);</span>
                    </p>
            </div>
                    <h1><b>Topic Mining and Ratings Prediction Joint Model Based on Lifelong Machine Learning</b></h1>
                    <h2>
                    <span>LIU Yining</span>
                    <span>SHEN Yanming</span>
            </h2>
                    <h2>
                    <span>Faculty of Electronic Information and Electrical Engineering, Dalian University of Technology</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>In order to make full use of the historical knowledge and improve the accuracy of rating prediction, a recommendation model based on Lifelong Machine Learning (LML) is proposed to mine both user ratings and comments.The model accumulates knowledge from previous tasks and utilizes it in future tasks to help improve the rating prediction accuracy.Experimental results on real datasets show that compared with models without LML ability, the mean square error of the predicted ratings of this model is reduced by 5.4‰, and with the accumulation of knowledge, its error is continuely dropped.The accuracy of topic word classification results is improved.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=text%20topic%20model&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">text topic model;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=recommendation%20algorithm&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">recommendation algorithm;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Lifelong%20Machine%20Learning%20(LML)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Lifelong Machine Learning (LML) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=ratings%20prediction&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">ratings prediction;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=collaborative%20filtering&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">collaborative filtering;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-04-09</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="35" name="35" class="anchor-tag">0 概述</h3>
                <div class="p1">
                    <p id="36">协同过滤作为最有效的推荐算法之一, 能够对用户的评分进行预测<citation id="131" type="reference"><link href="3" rel="bibliography" /><link href="5" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>]</sup></citation>。多数传统的协同过滤算法只能处理用户的评分信息, 通过挖掘用户的历史评分记录分析用户的兴趣。其中具代表性的是矩阵分解 (Matrix Factorization, MF) 算法<citation id="129" type="reference"><link href="7" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>, 该算法已被证明能够有效解决现实世界中的推荐问题<citation id="130" type="reference"><link href="9" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>。然而, 这种方法虽然有效, 但其忽略了用户在评论区的评论内容, 而多数情况下这些内容直接解释了用户给出评分的原因。如果推荐系统能够读懂这些评论内容, 就能够了解不同用户关注点的差异, 从而得到更加准确的用户潜在特征和评分预测。</p>
                </div>
                <div class="p1">
                    <p id="37">近年来, 研究者们提出多种用于挖掘用户评分和评论主题的联合模型。HFT (Hidden Factors as Topics) <citation id="132" type="reference"><link href="11" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>是最早被提出的模型, 它通过文本主题模型隐含狄利克雷分布 (Latent Dirichlet Allocation, LDA) 来挖掘用户评论, 并将用户或项目的评论主题与MF算法中的潜在特征关联, 通过LDA对文档中的词汇进行采样, 再基于上述关联关系修正用户的潜在特征, 最终获得准确的用户和项目潜在特征与评分预测。其他模型也采用了类似的方法, 针对不同数据集的特点, 对LDA模型的原有架构进行修改, 再与MF模型关联<citation id="133" type="reference"><link href="13" rel="bibliography" /><link href="15" rel="bibliography" /><sup>[<a class="sup">6</a>,<a class="sup">7</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="38">尽管同时挖掘用户评分和评论主题的联合模型有效地提高了推荐系统的评分预测精度, 但其性能仍有提高的空间。目前已经提出的模型大多只能处理单个数据集, 而文本中词语的含义在数据集之间具有一定的通用性, 仅依靠单个数据集中的文本内容进行主题分析, 可能会由于文本信息不足而使结果出现偏差。如果合并多个数据集同时进行运算, 则会因为数据集之间项目的独立性和类别差异较大而影响评分预测精度。</p>
                </div>
                <div class="p1">
                    <p id="39">为解决上述问题, 本文提出一种基于终身机器学习 (Lifelong Machine Learning, LML) , 同时挖掘用户评分和评论的推荐系统。LML是一种模仿人类学习过程的机器学习算法<citation id="134" type="reference"><link href="17" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>, 它能够从训练过的任务中积累知识, 并将这些知识用于训练新的任务。本文以现有的联合模型为基础, 构建适应推荐系统的LML架构, 并改进模型在训练评论内容时的算法。根据不同词语在各个已知任务中得到的主题分布的相似性, 将参数化的词语属性转化为词与词之间的关系, 并通过设计具有容错能力的LML模型, 将这些关系 (知识) 用于新的推荐任务。</p>
                </div>
                <h3 id="40" name="40" class="anchor-tag">1 相关工作</h3>
                <h4 class="anchor-tag" id="41" name="41">1.1 评分与评论挖掘</h4>
                <div class="p1">
                    <p id="42">在推荐系统领域, 自然语言处理技术已经被广泛应用于辅助预测用户对商品的评分。其中, HFT是较早将LDA技术与MF相结合的模型, 作者将MF学习到的潜在特征转化为用户或产品的评论主题分布, 基于该主题分布对评论中的词语进行采样, 根据采样得到的结果修正主题分布以及用户或产品的潜在特征。JMARS<citation id="135" type="reference"><link href="13" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>同样采用自然语言处理模型预测用户的评分, 该模型将用户和项目的潜在属性用一个矩阵进行分解, 并将结果通过3种不同的转化公式转化为相应的文本主题分布, 每个评论中的词语会根据所属分类采用相应的主题分布。文献<citation id="136" type="reference">[<a class="sup">7</a>]</citation>提出一种高斯混合的方法, 利用LDA模型对评论进行学习。文献<citation id="137" type="reference">[<a class="sup">9</a>]</citation>提出一种推荐模型MR3, 该模型能够同时挖掘用户的评分行为、社交关系和评论内容。</p>
                </div>
                <div class="p1">
                    <p id="43">尽管已有的推荐系统能够同时挖掘评分和评论内容, 但它们不能通过学习任务积累知识, 也无法在学习新任务的时候利用知识提高推荐精度。</p>
                </div>
                <h4 class="anchor-tag" id="44" name="44">1.2 终身机器学习</h4>
                <div class="p1">
                    <p id="45">终身机器学习, 也称作终身学习机, 是一种能够在训练任务的过程中积累知识并在训练新任务时使用这些知识的机器学习模型。这种机制类似于人类的学习过程, 在学习和理解新事物时不需要大量的训练数据和冗长的训练过程, 能够根据已有的经验和方法迅速掌握和理解新事物。</p>
                </div>
                <div class="p1">
                    <p id="46">目前, LML主要应用于自然语言处理领域。文献<citation id="138" type="reference">[<a class="sup">10</a>]</citation>研究了终身概念学习的方法, 其采用二元分类法识别每个已知任务或新任务的特殊概念或类别。文献<citation id="139" type="reference">[<a class="sup">11</a>]</citation>提出一种积累学习的方法, 该方法在处理新任务的同时更新已有的分类器, 使之能够识别已知的所有任务的类别。文献<citation id="140" type="reference">[<a class="sup">12</a>]</citation>利用终身松弛标记算法以解决非监督的分类问题。文献<citation id="141" type="reference">[<a class="sup">13</a>]</citation>构建了一种基于终身学习的主题模型, 从历史任务的主题词语分布中关联共同出现在多个主题中的词语, 并将这些关联词语应用于当前任务, 以提高文本主题模型的学习效果。LML除了应用于自然语言处理领域之外, 文献<citation id="142" type="reference">[<a class="sup">14</a>]</citation>构建了一种处理用户反馈的LML模型。尽管LML的概念早已被提出, 但在推荐系统方面的应用却不多见。文献<citation id="143" type="reference">[<a class="sup">15</a>]</citation>基于大量的产品评论信息来帮助识别当前评论。尽管该方法通过LML技术进行产品评论的分析, 但其并没有利用LML来改进产品的评分预测。</p>
                </div>
                <h3 id="47" name="47" class="anchor-tag">2 评论主题挖掘与评分预测联合模型</h3>
                <div class="p1">
                    <p id="48">HFT模型<citation id="144" type="reference"><link href="11" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>是较早结合评分模型与主题模型的评分预测联合模型, 其评分预测过程如下:</p>
                </div>
                <div class="p1">
                    <p id="49" class="code-formula">
                        <mathml id="49"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi>r</mi></mstyle><mrow><mspace width="0.25em" /><mo>^</mo></mrow></mover><msub><mrow></mrow><mrow><mi>u</mi><mo>, </mo><mi>v</mi></mrow></msub><mo>=</mo><mi>μ</mi><mo>+</mo><mi>β</mi><msub><mrow></mrow><mi>u</mi></msub><mo>+</mo><mi>β</mi><msub><mrow></mrow><mi>v</mi></msub><mo>+</mo><mi mathvariant="bold-italic">p</mi><msubsup><mrow></mrow><mi>u</mi><mtext>Τ</mtext></msubsup><mo>⋅</mo><mi mathvariant="bold-italic">q</mi><msub><mrow></mrow><mi>v</mi></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="50">其中, <i>μ</i>是整体偏差, <i>β</i><sub><i>u</i></sub>和<i>β</i><sub><i>v</i></sub>分别是用户和项目偏差, <b><i>p</i></b><sub><i>u</i></sub>和<b><i>q</i></b><sub><i>v</i></sub>分别是用户和项目的<i>K</i>维潜在特征向量。HFT的主题模型部分采用LDA模型<citation id="145" type="reference"><link href="33" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>, 该模型用于挖掘用户评论中的主题分布, 对于一组文本集合<i>D</i>, 其似然函数为:</p>
                </div>
                <div class="p1">
                    <p id="51" class="code-formula">
                        <mathml id="51"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>p</mi><mo stretchy="false"> (</mo><mi>D</mi><mo stretchy="false">|</mo><mi mathvariant="bold-italic">θ</mi><mo>, </mo><mtext>ϕ</mtext><mo>, </mo><mi mathvariant="bold-italic">z</mi><mo stretchy="false">) </mo><mo>=</mo><mrow><mstyle displaystyle="true"><munder><mo>∏</mo><mrow><mi>d</mi><mo>∈</mo><mi>D</mi></mrow></munder><mrow></mrow></mstyle><mspace width="0.25em" /></mrow><mtext> </mtext><mrow><mstyle displaystyle="true"><munderover><mo>∏</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>Ν</mi><msub><mrow></mrow><mi>d</mi></msub></mrow></munderover><mrow></mrow></mstyle><mspace width="0.25em" /></mrow><mi>θ</mi><msub><mrow></mrow><mrow><mi>z</mi><msub><mrow></mrow><mrow><mi>d</mi><mo>, </mo><mi>i</mi></mrow></msub></mrow></msub><mtext>ϕ</mtext><msub><mrow></mrow><mrow><mi>z</mi><msub><mrow></mrow><mrow><mi>d</mi><mo>, </mo><mi>i</mi></mrow></msub><mo>, </mo><mi>w</mi><msub><mrow></mrow><mrow><mi>d</mi><mo>, </mo><mi>i</mi></mrow></msub></mrow></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="52">其中, 对于文本<i>d</i>∈<i>D</i>中的每个词<i>w</i><sub><i>d</i>, <i>i</i></sub>, 依据<i>d</i>的主题分布<i>θ</i><sub><i>d</i></sub>和<i>w</i><sub><i>d</i>, <i>i</i></sub>的主题词语分布ϕ<sub><i>w</i></sub>而为其分配一个主题<i>z</i><sub><i>d</i>, <i>i</i></sub>。模型整体概率就是文本集中所有文本的所有单词的文本主题分布与主题词语分布的乘积。在HFT模型中, 每个文本对应一个项目的所有评论的总和。由此, 模型的损失函数定义为评分预测的均方误差与似然估计的对数的差, 如下:</p>
                </div>
                <div class="p1">
                    <p id="53" class="code-formula">
                        <mathml id="53"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>L</mi><mo stretchy="false"> (</mo><mi>R</mi><mo>, </mo><mi>D</mi><mo stretchy="false">|</mo><mi>Y</mi><mo>, </mo><mi mathvariant="bold-italic">θ</mi><mo>, </mo><mtext>ϕ</mtext><mo>, </mo><mi mathvariant="bold-italic">z</mi><mo stretchy="false">) </mo><mo>=</mo><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>r</mi><mo>∈</mo><mi>R</mi></mrow></munder><mo stretchy="false"> (</mo></mstyle><mi>r</mi><msub><mrow></mrow><mrow><mi>u</mi><mo>, </mo><mi>v</mi></mrow></msub><mo>-</mo><mover><mstyle mathsize="140%" displaystyle="true"><mi>r</mi></mstyle><mrow><mspace width="0.25em" /><mo>^</mo></mrow></mover><msub><mrow></mrow><mrow><mi>u</mi><mo>, </mo><mi>v</mi></mrow></msub><mo stretchy="false">) </mo></mrow><msup><mrow></mrow><mn>2</mn></msup><mo>-</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mi>η</mi><mtext> </mtext><mrow><mi>ln</mi></mrow><mtext> </mtext><mi>p</mi><mo stretchy="false"> (</mo><mi>D</mi><mo stretchy="false">|</mo><mi mathvariant="bold-italic">θ</mi><mo>, </mo><mtext>ϕ</mtext><mo>, </mo><mi mathvariant="bold-italic">z</mi><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="54">其中, <i>R</i>是数据集中所有评分的集合, <i>Y</i>={<i>μ</i>, <i>β</i><sub><i>u</i></sub>, <i>β</i><sub><i>v</i></sub>, <b><i>p</i></b><sub><i>u</i></sub>, <b><i>q</i></b><sub><i>v</i></sub>}表示评分预测部分的优化参数, <i>η</i>表示用来平衡公式2个部分的预设超参数。HFT模型通过式 (4) 将项目的潜在特征与评论主题进行关联。</p>
                </div>
                <div class="p1">
                    <p id="55" class="code-formula">
                        <mathml id="55"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>θ</mi><msub><mrow></mrow><mrow><mi>v</mi><mo>, </mo><mi>k</mi></mrow></msub><mo>=</mo><mfrac><mrow><mtext>e</mtext><msup><mrow></mrow><mrow><mi>δ</mi><mo>⋅</mo><mi>q</mi><msub><mrow></mrow><mrow><mi>v</mi><mo>, </mo><mi>k</mi></mrow></msub></mrow></msup></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><msup><mi>k</mi><mo>′</mo></msup><mo>=</mo><mn>1</mn></mrow><mi>Κ</mi></munderover><mtext>e</mtext></mstyle><msup><mrow></mrow><mrow><mi>δ</mi><mo>⋅</mo><mi>q</mi><msub><mrow></mrow><mrow><mi>v</mi><mo>, </mo><msup><mi>k</mi><mo>′</mo></msup></mrow></msub></mrow></msup></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="56">其中, <i>δ</i>是用来控制转换强度的参数, 其在训练过程中学习得到。</p>
                </div>
                <div class="p1">
                    <p id="57">在训练时, HFT采用最大期望 (Expectation-Maximization, EM) 算法, 先用梯度下降法对整体损失函数中的参数进行更新, 再固定项目潜在特征和评论主题分布, 对评论集中的每个单词进行采样, 如此往复直到收敛为止。</p>
                </div>
                <h3 id="58" name="58" class="anchor-tag">3 终身机器学习的联合模型</h3>
                <div class="p1">
                    <p id="59">因为评论主题挖掘与评分预测联合模型在模型训练和取得主题分布的方法上与单独的文本主题模型有很大不同, 所以不能简单地将文本主题模型的终身学习机制直接套用于评分预测模型, 必须开发适用于以评分预测为目的的基于终身学习的联合模型, 其整体架构如图1所示。当新任务到来时, 任务数据 (评论与评分) 首先经过带历史知识提取功能的联合模型进行学习, 得到词语主题分布并保存在历史知识库中。同时, 从历史知识库中提取历史任务存储的主题词语分布, 使用历史任务和当前任务的主题词语分布共同得出满足当前任务条件的词语近似关系。采用联合模型, 结合上述步骤得到的历史知识和当前任务数据, 对新任务中的评分进行预测。</p>
                </div>
                <div class="area_img" id="60">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201906038_060.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 基于终身学习的评分预测模型架构" src="Detail/GetImg?filename=images/JSJC201906038_060.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图1 基于终身学习的评分预测模型架构</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201906038_060.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="61" name="61">3.1 历史知识获取</h4>
                <div class="p1">
                    <p id="62">为了在后面的任务中利用知识来帮助训练, 系统必须具备从当前任务中挖掘知识的能力。本文采用联合模型训练得到的主题词语分布作为原始知识, 而不对知识挖掘词语过程进行迭代。这是因为在联合模型中, 评论的主题分布不仅取决于评论中的词语, 同时取决于训练集中的已知评分, 这些评分被解析为用户和项目的潜在特征向量, 直接影响评论的主题分布。不同任务的项目和用户是相对独立的, 在前一个任务中得到的潜在特征值在后续任务中几乎没有借鉴意义, 这就要求模型在使用任务知识时具有较大的容错能力。只采用原始知识可以有效避免知识中的错误在任务间的传递和不断放大, 有助于保证模型的容错能力和可靠性。</p>
                </div>
                <div class="p1">
                    <p id="63">对于当前任务<i>t</i>的评分数据<i>R</i><sub><i>t</i></sub>和文本数据<i>D</i><sub><i>t</i></sub>, 首先执行联合模型得到的、在验证集上取得最优评分预测的参数组合。然后, 将该参数组合中的主题词语分布作为从该任务获取的知识保存到知识库中。在任务<i>t</i>中, 主题<i>k</i>对应的词语<i>w</i>的实际概率为<i>φ</i><sub><i>t</i>, <i>k</i>, <i>w</i></sub>, 为计算方便, 在联合模型中采用梯度下降法迭代时, 用指数函数对<i>φ</i>进行变换, 具体如下:</p>
                </div>
                <div class="p1">
                    <p id="64" class="code-formula">
                        <mathml id="64"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtext>ϕ</mtext><msub><mrow></mrow><mrow><mi>t</mi><mo>, </mo><mi>k</mi><mo>, </mo><mi>w</mi></mrow></msub><mo>=</mo><mfrac><mrow><mtext>e</mtext><msup><mrow></mrow><mrow><mi>φ</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>, </mo><mi>k</mi><mo>, </mo><mi>w</mi></mrow></msub></mrow></msup></mrow><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><msup><mi>w</mi><mo>′</mo></msup><mo>∈</mo><mi>W</mi></mrow></munder><mtext>e</mtext></mstyle><msup><mrow></mrow><mrow><mi>φ</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>, </mo><mi>k</mi><mo>, </mo><msup><mi>w</mi><mo>′</mo></msup></mrow></msub></mrow></msup></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>5</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="65">其中, <i>W</i>是所有词语的集合。在联合模型完成对任务<i>t</i>的训练之后, 保存每个主题下所有词语的分布<i>φ</i><sub><i>t</i>, <i>k</i></sub>, 供后续任务使用。</p>
                </div>
                <h4 class="anchor-tag" id="66" name="66">3.2 历史知识使用</h4>
                <h4 class="anchor-tag" id="67" name="67">3.2.1 相同主题词语对获取</h4>
                <div class="p1">
                    <p id="68">由于不同任务之间存在差异, 联合模型训练得到的项目潜在特征和评论主题分布无法直接与其他任务相对比。为了能够利用其他任务收集到的知识, 必须将2个任务中得到的主题相关联。</p>
                </div>
                <div class="p1">
                    <p id="69">本文用2个任务中主题词语分布的相似度来表示2个主题之间的距离。设<i>k</i>和<i>k</i>′分别是任务<i>t</i>和<i>t</i> ′的2个主题, 则其相似度为2个主题词语分布的对称相对熵, 具体如下:</p>
                </div>
                <div class="p1">
                    <p id="70" class="code-formula">
                        <mathml id="70"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>S</mi><msub><mrow></mrow><mrow><mi>k</mi><mo>, </mo><msup><mi>k</mi><mo>′</mo></msup></mrow></msub><mo>=</mo><mfrac><mrow><mi>D</mi><msub><mrow></mrow><mrow><mi>Κ</mi><mi>L</mi></mrow></msub><mo stretchy="false"> (</mo><mtext>ϕ</mtext><msub><mrow></mrow><mrow><mi>t</mi><mo>, </mo><mi>k</mi></mrow></msub><mo stretchy="false">∥</mo><mtext>ϕ</mtext><msub><mrow></mrow><mrow><mi>t</mi><msup><mtext> </mtext><mo>′</mo></msup><mo>, </mo><msup><mi>k</mi><mo>′</mo></msup></mrow></msub><mo stretchy="false">) </mo><mo>+</mo><mi>D</mi><msub><mrow></mrow><mrow><mi>Κ</mi><mi>L</mi></mrow></msub><mo stretchy="false"> (</mo><mtext>ϕ</mtext><msub><mrow></mrow><mrow><mi>t</mi><msup><mtext> </mtext><mo>′</mo></msup><mo>, </mo><msup><mi>k</mi><mo>′</mo></msup></mrow></msub><mo stretchy="false">∥</mo><mtext>ϕ</mtext><msub><mrow></mrow><mrow><mi>t</mi><mo>, </mo><mi>k</mi></mrow></msub><mo stretchy="false">) </mo></mrow><mn>2</mn></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>6</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi>D</mi><msub><mrow></mrow><mrow><mi>Κ</mi><mi>L</mi></mrow></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">A</mi><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">B</mi><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>i</mi></munder><mrow></mrow></mstyle><mi>A</mi><msub><mrow></mrow><mi>i</mi></msub><mrow><mi>ln</mi></mrow><mfrac><mrow><mi>A</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mrow><mi>B</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>7</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="71">其中, <i>D</i><sub><i>KL</i></sub> (<b><i>A</i></b>‖<b><i>B</i></b>) 为向量<b><i>A</i></b>和<b><i>B</i></b>的相对熵。</p>
                </div>
                <div class="p1">
                    <p id="72">在训练过程中, 首先用联合模型对当前任务进行训练, 得到不依赖历史知识的主题词语分布。通过式 (6) 计算知识库中保存的每个任务中的历史主题与当前任务<i>t</i>中所有主题词语分布之间的距离。若得到的最小距离小于等于阈值<i>y</i>, 则将该历史主题词语分布保存至当前任务的采纳知识库<i>C</i><mathml id="73"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>k</mi><msup><mrow></mrow><mo>*</mo></msup></mrow><mi>t</mi></msubsup></mrow></math></mathml>中, 其中, k<sup>*</sup>表示与该历史主题距离最小的当前主题。由此, 每个当前任务中的主题都有一组历史主题与之关联, 这些历史主题之间也建立了间接关联性。对任务t中的主题k, 找到C<mathml id="74"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>k</mi><mi>t</mi></msubsup></mrow></math></mathml>中每个历史主题词语分布概率最高的f个词, 如果词语w<sub>1</sub>和w<sub>2</sub>同时出现2次以上, 则将该词语对{w<sub>1</sub>, w<sub>2</sub>}视为一组属于相同主题的词语。当前任务所有的C<mathml id="75"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>k</mi><mi>t</mi></msubsup></mrow></math></mathml>, k∈{1, 2, …, K}经上述过程后, 可以得到每个当前主题的词语对集合, 用Q<sub>k</sub>表示。</p>
                </div>
                <h4 class="anchor-tag" id="76" name="76">3.2.2 相同主题词语的主题分布更新</h4>
                <div class="p1">
                    <p id="77">Q<sub>k</sub>中的词语对{w<sub>i</sub>, w<sub>j</sub>}表示w<sub>i</sub>和w<sub>j</sub>均属于主题k的概率较高。当词语w<sub>i</sub>被采样为主题k时, 可视为w<sub>j</sub>也被采样为主题k。然而, 在实践中, 从历史任务中获取的知识并不一定符合当前任务的环境, 因此需要计算w<sub>j</sub>被采样为主题k的权重。当词语w<sub>i</sub>被采样为主题k时, 可以视作有b<sub>w<sub>i</sub>, w<sub>j</sub></sub>个w<sub>j</sub>被采样为主题k, 具体如下:</p>
                </div>
                <div class="p1">
                    <p id="78" class="code-formula">
                        <mathml id="78"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>b</mi><msub><mrow></mrow><mrow><mi>w</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>w</mi><msub><mrow></mrow><mi>j</mi></msub></mrow></msub><mo>=</mo><mi>λ</mi><mtext> </mtext><mrow><mi>ln</mi></mrow><mrow><mo> (</mo><mrow><mfrac><mrow><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>w</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>w</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo></mrow><mrow><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>w</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>w</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo></mrow></mfrac></mrow><mo>) </mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>8</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="79">其中, P (w<sub>i</sub>, w<sub>j</sub>) 为2个单词同时出现在当前任务文本中的概率, P (w<sub>i</sub>) 和P (w<sub>j</sub>) 分别为其单独出现在当前任务文本中的概率, 上述概率可通过对当前任务的文本集进行统计得到。λ为调节历史知识影响权重的超参数。</p>
                </div>
                <div class="p1">
                    <p id="80">经过对当前任务的学习, 每个主题取得对应的Q后, 联合模型的各个参数被重置, 仅保留取得最优评分预测时的主题词语分布φ。这是因为在采用梯度下降法求解模型参数时, 不能采用2种模型进行计算, 否则无法取得局部最优解。因此, 只有与Q有关的主题词语分布被保留下来。历史知识使用的具体算法描述如下:</p>
                </div>
                <div class="p1">
                    <p id="81"><b>算法</b> 基于历史知识的主题挖掘与评分预测算法</p>
                </div>
                <div class="p1">
                    <p id="82"><b>输入</b> 当前任务<i>t</i>的评分数据<i>R</i><sub><i>t</i></sub>和文本数据<i>D</i><sub><i>t</i></sub>, 历史知识</p>
                </div>
                <div class="p1">
                    <p id="83"><b>输出</b> 评分预测结果和参数配置</p>
                </div>
                <div class="p1">
                    <p id="84">1.利用联合模型对任务t进行处理</p>
                </div>
                <div class="p1">
                    <p id="85">2.for t′ in 所有历史任务:</p>
                </div>
                <div class="p1">
                    <p id="86">3.for k′ in 任务t′中的主题:</p>
                </div>
                <div class="p1">
                    <p id="87">4.for k in 任务t中的主题:</p>
                </div>
                <div class="p1">
                    <p id="88">5.根据式 (6) 计算S<sub>k, k′</sub></p>
                </div>
                <div class="p1">
                    <p id="89">6.if Minimum S &gt;S<sub>k, k′</sub>:</p>
                </div>
                <div class="p1">
                    <p id="90">7.Minimum S = S<sub>k, k′</sub></p>
                </div>
                <div class="p1">
                    <p id="91">8.k* = k</p>
                </div>
                <div class="p1">
                    <p id="92">9.end if</p>
                </div>
                <div class="p1">
                    <p id="93">10.end for</p>
                </div>
                <div class="p1">
                    <p id="94">11.if Minimum S≤y:</p>
                </div>
                <div class="p1">
                    <p id="95">12.φ<sub>t ′k′</sub>→C<mathml id="96"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mtext>k</mtext><msup><mrow></mrow><mo>*</mo></msup></mrow><mtext>t</mtext></msubsup></mrow></math></mathml></p>
                </div>
                <div class="p1">
                    <p id="97">13.end if</p>
                </div>
                <div class="p1">
                    <p id="98">14.重置 Minimum S</p>
                </div>
                <div class="p1">
                    <p id="99">15.end for</p>
                </div>
                <div class="p1">
                    <p id="100">16.end for</p>
                </div>
                <div class="p1">
                    <p id="101">17.重置任务t除φ<sub>t</sub>以外的所有参数和词语计数器, 保留主题词语分布φ</p>
                </div>
                <div class="p1">
                    <p id="102">18.利用式 (8) 执行带知识处理功能的联合模型</p>
                </div>
                <div class="p1">
                    <p id="103">19.结束处理</p>
                </div>
                <h3 id="104" name="104" class="anchor-tag">4 实验结果与分析</h3>
                <div class="p1">
                    <p id="105">本文通过实验验证终身机器学习在评论挖掘与评分预测方面的作用, 主要关注2个方面的问题:</p>
                </div>
                <div class="p1">
                    <p id="106">1) 与已有的评论挖掘与评分预测模型相比, 本文的模型是否能够获得更加准确的评分预测。</p>
                </div>
                <div class="p1">
                    <p id="107">2) 通过终身机器学习找到的主题词语是否合理并具有代表性。</p>
                </div>
                <h4 class="anchor-tag" id="108" name="108">4.1 数据集</h4>
                <div class="p1">
                    <p id="109">实验采用从Amazon.com网站收集的真实数据集, 其中包括“Amazon Instant Video”类别中1999年—2014年的583 933个评分和评论文本。本文使用同时期Amazon网站另外4个类别的数据集作为历史任务, 具体统计数据见表1。</p>
                </div>
                <div class="area_img" id="110">
                    <p class="img_tit"><b>表1 用于知识积累的历史任务的数据集参数</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="110" border="1"><tr><td>序号</td><td>数据集名称</td><td>用户数</td><td>项目数</td><td>评论数</td></tr><tr><td>1</td><td>Electronics</td><td>4 201 696</td><td>476 002</td><td>7 824 482</td></tr><tr><td><br />2</td><td>Home &amp; Kitchen</td><td>2 511 610</td><td>410 243</td><td>4 253 926</td></tr><tr><td><br />3</td><td>Movies &amp; TV</td><td>2 088 620</td><td>200 941</td><td>4 607 047</td></tr><tr><td><br />4</td><td>Health &amp; Personal Care</td><td>1 758 807</td><td>238 983</td><td>2 783 886</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="111" name="111">4.2 参数与实验设置</h4>
                <h4 class="anchor-tag" id="112" name="112">4.2.1 联合模型的参数设置</h4>
                <div class="p1">
                    <p id="113">本文采用HFT联合模型。实验中超参数<i>η</i>取原文中推荐的设置0.1, <i>δ</i>取1。由于评论中会出现一些不常见的缩写和拼写错误, 对每个历史任务运行联合模型时只保留该任务中出现频率最多的5 000个单词。在所有实验中, 用户和项目潜在特征向量的维度<i>K</i>=15。</p>
                </div>
                <h4 class="anchor-tag" id="114" name="114">4.2.2 实验过程设置</h4>
                <div class="p1">
                    <p id="115">在所有任务中, 按8∶1∶1的比例将数据集随机分为3份, 分别为训练集、验证集和测试集。对每个任务执行1 000次梯度下降迭代, 每隔50次迭代进行一次评论文本词语的Gibbs Sampling采样, 并在验证集和测试集上计算评分的均方误差。将在验证集上取得最优结果的参数在测试集上得到的均方误差作为实验最终结果。在进行历史知识提取时, 将验证集上取得最优结果的<i>φ</i>保存为系统积累的知识。</p>
                </div>
                <h4 class="anchor-tag" id="116" name="116">4.2.3 有关历史知识的参数</h4>
                <div class="p1">
                    <p id="117">在使用历史知识进行评分预测时, 将<i>f</i>设置为30, <i>λ</i>为0.3。先执行1 000次梯度下降, 然后采用第3.2.2节介绍的算法提取历史知识, 并再执行1 000次带知识处理过程的联合模型。</p>
                </div>
                <h4 class="anchor-tag" id="118" name="118">4.3 结果分析</h4>
                <div class="p1">
                    <p id="119">在实验中, 首先单独运行每个历史任务提取历史知识。然后, 对当前任务“Amazon Instant Video”逐渐增加所使用的历史知识的数量并取得模型在测试集上的均方误差, 增加历史知识的顺序与表1中的序号一致。</p>
                </div>
                <div class="p1">
                    <p id="120">图2给出任务“Amazon Instant Video”在采用不同数量历史知识的情况下的均方误差。从图2可以看出, 相比不使用知识的HFT模型, 基于终身学习的模型在只使用“Electronics”的知识时, 均方误差有较明显的下降。随着使用的历史知识量不断增加, 均方误差的下降幅度逐渐减小。在历史任务数达到3时, 均方误差下降到最低点, 总的下降幅度达到5.4‰;在历史任务数达到4时, 均方误差停止下降, 且有不到0.001的上浮。这表明对同一任务而言, 增加历史知识对提高评分预测精度的帮助会逐渐减小, 并且当其使用的有效历史知识趋于饱和时, 再增加额外的知识可能会对模型造成干扰。这是由于历史知识中可能存在与当前任务不符合的情况, 或者历史知识中各词语对与当前任务的实际情况的契合度有所不同, 而系统对它们采用同样的参数, 造成模型部分历史知识被过度强调, 而另一些则被相对弱化, 继而影响模型的预测精度。</p>
                </div>
                <div class="area_img" id="121">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201906038_121.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 历史知识的增加对任务的影响" src="Detail/GetImg?filename=images/JSJC201906038_121.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图2 历史知识的增加对任务的影响</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201906038_121.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="122" name="122">4.4 主题词汇发现</h4>
                <div class="p1">
                    <p id="123">除了评分预测精度, 终身学习模型对评论中各主题的词汇能否准确分类同样具有重要意义。如果各主题词汇有明显的含义, 说明模型有能力将数据集中的项目准确地区分。表2、表3分别给出HFT模型和基于终身学习的模型在历史任务数为4的情况下取得最优预测精度时, 各主题中<i>φ</i>值最高的5个词语。其中, 每个主题下的第一个词语为<i>φ</i>值最高的词语。</p>
                </div>
                <div class="area_img" id="124">
                    <p class="img_tit"><b>表2 HFT模型的主题词汇训练结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="124" border="1"><tr><td>主题1</td><td>主题2</td><td>主题3</td><td>主题4</td><td>主题5</td><td>主题6</td><td>主题7</td><td>主题8</td><td>主题9</td><td>主题10</td><td>主题11</td><td>主题12</td><td>主题13</td><td>主题14</td><td>主题15</td></tr><tr><td><br />minutes</td><td>third</td><td>hooked</td><td>knew</td><td>creative</td><td>team</td><td>dome</td><td>making</td><td>actor</td><td>matter</td><td>everything</td><td>living</td><td>king</td><td>bosch</td><td>bosch</td></tr><tr><td><br />books</td><td>smart</td><td>missed</td><td>using</td><td>casting</td><td>entertainment</td><td>stephen</td><td>fans</td><td>harry</td><td>buy</td><td>problems</td><td>able</td><td>turn</td><td>bring</td><td>surprise</td></tr><tr><td><br />usually</td><td>beginning</td><td>information</td><td>relationship</td><td>guess</td><td>star</td><td>king</td><td>exciting</td><td>four</td><td>wife</td><td>lots</td><td>bosch</td><td>view</td><td>experience</td><td>young</td></tr><tr><td><br />least</td><td>annoying</td><td>reason</td><td>period</td><td>content</td><td>fantastic</td><td>book</td><td>move</td><td>adult</td><td>heart</td><td>yet</td><td>intriguing</td><td>killer</td><td>harry</td><td>day</td></tr><tr><td><br />parts</td><td>clearly</td><td>probably</td><td>entertainment</td><td>background</td><td>except</td><td>town</td><td>hard</td><td>classic</td><td>strong</td><td>sense</td><td>become</td><td>side</td><td>remember</td><td>harry</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="125">
                    <p class="img_tit"><b>表3 终身学习模型主题词汇训练结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="125" border="1"><tr><td><br />主题1</td><td>主题2</td><td>主题3</td><td>主题4</td><td>主题5</td><td>主题6</td><td>主题7</td><td>主题8</td><td>主题9</td><td>主题10</td><td>主题11</td><td>主题12</td><td>主题13</td><td>主题14</td><td>主题15</td></tr><tr><td><br />action</td><td>love</td><td>history</td><td>tv</td><td>look</td><td>star</td><td>dome</td><td>hooked</td><td>comedy</td><td>music</td><td>loves</td><td>series</td><td>really</td><td>bosch</td><td>entertaining</td></tr><tr><td><br />ever</td><td>really</td><td>doctor</td><td>characters</td><td>stories</td><td>family</td><td>stephen</td><td>pilot</td><td>laugh</td><td>video</td><td>documentary</td><td>season</td><td>one</td><td>harry</td><td>three</td></tr><tr><td><br />dark</td><td>great</td><td>line</td><td>episodes</td><td>gives</td><td>dvd</td><td>king</td><td>next</td><td>cast</td><td>buy</td><td>son</td><td>really</td><td>well</td><td>books</td><td>guy</td></tr><tr><td><br />family</td><td>time</td><td>hooked</td><td>episode</td><td>favorite</td><td>history</td><td>book</td><td>wait</td><td>real</td><td>ever</td><td>end</td><td>movie</td><td>time</td><td>titus</td><td>young</td></tr><tr><td><br />quality</td><td>one</td><td>action</td><td>shows</td><td>quite</td><td>kids</td><td>read</td><td>plot</td><td>humor</td><td>hilarious</td><td>kids</td><td>best</td><td>could</td><td>connelly</td><td>forward</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="126">从表2、表3可以看到, HFT模型的各个主题词汇意义比较模糊, 特别是从第一个词几乎无法读懂该主题代表哪个方面的内容。例如, “minutes”“knew”和“third”等词语不能表明影视作品的类型, 而最后2个主题的第一个词语相同, 体现出HFT模型在区分不同种类项目时的局限性。终身学习模型每个主题的第一个词汇大多具有明确的含义, 如“action”“love”“history”“tv”等都各自代表了一类影视作品, 并且对应的后面几个词汇也都表达了类似的含义。由此可以看出, 在历史知识充足的情况下, 终身学习模型能够更好地理解评论内容, 并根据该内容修正项目的潜在特征, 从而得到更好的主题词语分类和评分预测结果。</p>
                </div>
                <h3 id="127" name="127" class="anchor-tag">5 结束语</h3>
                <div class="p1">
                    <p id="128">本文构建一种可同时挖掘用户评分和评论的推荐模型。在现有的评论主题挖掘和评分预测模型的基础上, 采用LML机制, 在完成任务的同时, 积累知识用于后续任务的训练。在真实数据集上的实验结果表明, 该模型能够有效提高评分预测精度, 并且主题词语分类结果明显优于普通的联合模型。下一步将细化模型中相关参数的计算方法, 提高模型甄别错误知识的能力。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201804038&amp;v=MTcyNjllUm9GeW5sVmJ6UEx6N0JiYkc0SDluTXE0OUdiSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVo=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> 金紫嫣, 张娟, 李向军, 等.一种带标签的协同过滤广告推荐算法[J].计算机工程, 2018, 44 (4) :236-242, 247.
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=From amateurs to connoisseurs:modeling the evolution of user expertise through online reviews">

                                <b>[2]</b> MCAULEY J, LESKOVEC J.From amateurs to connoisseurs:modeling the evolution of user expertise through online reviews [C]//Proceedings of the 22nd International Conference on World Wide Web.York, USA:ACM Press, 2015:897-908.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Probabilistic matrix factorization">

                                <b>[3]</b> MNIH A, SALAKHUTDINOV R.Probabilistic matrix factorization [C]//Proceedings of the 20th International Conference on Neural Information Processing Systems.New York, USA:Curran Associates Inc., 2008:1257-1264.
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Collaborative Topic Modeling for Recommending Scientific Articles">

                                <b>[4]</b> WANG Chong, BLEI D M.Collaborative topic modeling for recommending scientific articles [C]//Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.New York, USA:ACM Press, 2011:448-456.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Hidden factors and hidden topics:understanding rating dimensions with review text">

                                <b>[5]</b> MCAULEY J, JURE L.Hidden factors and hidden topics:understanding rating dimensions with review text [C]//Proceedings of the 7th ACM Conference on Recommender Systems.New York, USA:ACM Press, 2013:165-172.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Jointly modeling aspects ratings and sentiments for movie recom-mendation (JMARS)">

                                <b>[6]</b> DIAO Qiming, QIU Minghui, WU Chaoyuan, et al.Jointly modeling aspects, ratings and sentiments for movie recommendation[C]//Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.New York, USA:ACM Press, 2014:193-202.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Ratings meet reviews, a combined approach to recommend">

                                <b>[7]</b> LING Guang, LYU M R, KING R.Ratings meet reviews, a combined approach to recommend [C]//Proceedings of the 8th ACM Conference on Recom-mender Systems.New York, USA:ACM Press, 2014:105-112.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Lifelong machine learning">

                                <b>[8]</b> CHEN Zhiyuan, LIU Bing.Lifelong machine learning[M].San Rafael, USA:Morgan and Claypool Publishers, 2016.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A synthetic approach for recommendation:combining ratings,social relations,and reviews">

                                <b>[9]</b> HU Guangneng, DAI Xinyu, SONG Yunya, et al.A synthetic approach for recommendation:combining ratings, social relations, and reviews[C]//Proceedings of the 24th International Conference on Artificial Intelligence.New York, USA:ACM Press, 2015:1756-1762.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Is learning the n-th thing any easier than learning the first">

                                <b>[10]</b> THRUN S.Is learning the n-th thing any easier than learning the first[C]//Proceedings of the 8th International Con-ference on Neural Information Processing Systems.Cambridge, USA:MIT Press, 1996:640-646.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning Cumulatively to Become More Knowledgeable">

                                <b>[11]</b> FEI Geli, WANG Shuai, LIU Bing.Learning cumulatively to become more knowledgeable[C]//Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.New York, USA:ACM Press, 2016:1565-1574.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Lifelong-rl:lifelong relaxation labeling for separating entities and aspects in opinion targets">

                                <b>[12]</b> SHU Lei, LIU Bin, XU Bin, et al.Lifelong-rl:lifelong relaxation labeling for separating entities and aspects in opinion targets [EB/OL].[2018-03-24].https://www.aclweb.org/anthology/D16-1022.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Topic Modeling using Topics from Many Domains,Lifelong Learning and Big Data">

                                <b>[13]</b> CHEN Zhiyuan, LIU Bing.Topic modeling using topics from many domains, lifelong learning and big data [C]//Proceedings of International Conference on Machine Learning.New York, USA:ACM Press, 2014:703-711.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Principles of lifelong learning for predictive user modeling">

                                <b>[14]</b> KAPOOR A, HORVITZ E.Principles of lifelong learning for predictive user modeling [C]//Proceedings of International Conference on User Modeling.Berlin, Germany:Springer, 2007:37-46.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Improving opinion aspect extraction using semantic similarity and aspect associations">

                                <b>[15]</b> LIU Qian, LIU Bing, ZHANG Yuanlin, et al.Improving opinion aspect extraction using semantic similarity and aspect associations [EB/OL].[2018-03-24].https://aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/1 1973/12051.
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_16" >
                                    <b>[16]</b>
                                 BLEI D M, NG A Y, JORDAN M I.Latent dirichlet allocation[J].Journal of Machine Learning Research, 2003, 3:993-1022.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJC201906038" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201906038&amp;v=MDI0NzdJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSb0Z5bmxWYnpQTHo3QmJiRzRIOWpNcVk5R2I=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
