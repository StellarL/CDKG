<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637128045631123750%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJC201909038%26RESULT%3d1%26SIGN%3dJYpdp9rGOfln7cSgFZzV%252fIcJp4E%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201909038&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201909038&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201909038&amp;v=MDMxNzBSckZ5N2xWTDdOTHo3QmJiRzRIOWpNcG85R2JJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmU=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtOc0tUWDRGRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#41" data-title="0 概述 ">0 概述</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#43" data-title="1 相关工作 ">1 相关工作</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#50" data-title="2 能见度检测模型 ">2 能见度检测模型</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#59" data-title="3 VGG-16网络迁移 ">3 VGG-16网络迁移</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#60" data-title="3.1 网络编码">3.1 网络编码</a></li>
                                                <li><a href="#64" data-title="3.2 支持向量回归机训练">3.2 支持向量回归机训练</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#75" data-title="4 子区域估计值融合 ">4 子区域估计值融合</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#89" data-title="5 实验结果与分析 ">5 实验结果与分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#90" data-title="5.1 实验数据集与设备">5.1 实验数据集与设备</a></li>
                                                <li><a href="#94" data-title="5.2 结果分析">5.2 结果分析</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#105" data-title="6 结束语 ">6 结束语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#58" data-title="&lt;b&gt;图1 本文方法流程&lt;/b&gt;"><b>图1 本文方法流程</b></a></li>
                                                <li><a href="#63" data-title="&lt;b&gt;图2 VGG-16网络编码过程&lt;/b&gt;"><b>图2 VGG-16网络编码过程</b></a></li>
                                                <li><a href="#92" data-title="&lt;b&gt;表1 不同能见度区间内样本图像的数量分布&lt;/b&gt;"><b>表1 不同能见度区间内样本图像的数量分布</b></a></li>
                                                <li><a href="#93" data-title="&lt;b&gt;图3 不同视角样本图像示例&lt;/b&gt;"><b>图3 不同视角样本图像示例</b></a></li>
                                                <li><a href="#98" data-title="&lt;b&gt;图4 测试样本能见度检测值分布&lt;/b&gt;"><b>图4 测试样本能见度检测值分布</b></a></li>
                                                <li><a href="#100" data-title="&lt;b&gt;表2 本文方法的结果统计&lt;/b&gt;"><b>表2 本文方法的结果统计</b></a></li>
                                                <li><a href="#102" data-title="&lt;b&gt;表3 4种融合策略的检测正确率&lt;/b&gt;"><b>表3 4种融合策略的检测正确率</b></a></li>
                                                <li><a href="#104" data-title="&lt;b&gt;表4 5种方法检测正确率的对比&lt;/b&gt;"><b>表4 5种方法检测正确率的对比</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="166">


                                    <a id="bibliography_1" >
                                        <b>[1]</b>
                                     中国气象局.中华人民共和国气象行业标准:能见度等级和预报:QX/T 114-2010[S].北京:气象出版社,2010:1-3.</a>
                                </li>
                                <li id="168">


                                    <a id="bibliography_2" title=" 苗苗.视频能见度检测算法综述[J].现代电子技术,2012,35(15):72-75." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XDDJ201215025&amp;v=MDg4OTlLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUnJGeTdsVkw3TlBTblBaTEc0SDlQTnFvOUhZWVE=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtOc0tUWDRGRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         苗苗.视频能见度检测算法综述[J].现代电子技术,2012,35(15):72-75.
                                    </a>
                                </li>
                                <li id="170">


                                    <a id="bibliography_3" title=" 孙学金.大气探测学[M].北京:气象出版社,2009." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CBBD&amp;filename=9787502948009001&amp;v=MDEwMjRYRnF6R2JhNEhOaklwNDlGYmVzUERSTTh6eFVTbURkOVNIN24zeEU5ZmJ2bktyaWZadTl1RkN2Z1U3bklKMThS&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtOc0tUWDRGRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         孙学金.大气探测学[M].北京:气象出版社,2009.
                                    </a>
                                </li>
                                <li id="172">


                                    <a id="bibliography_4" title=" 李志乾,张志伟,成文,等.海上能见度观测研究进展[J].自动化仪表,2015,36(10):33-36." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZDYB201510010&amp;v=MTg3NzlOUHluU2JMRzRIOVROcjQ5RVpJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSckZ5N2xWTDc=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtOc0tUWDRGRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                         李志乾,张志伟,成文,等.海上能见度观测研究进展[J].自动化仪表,2015,36(10):33-36.
                                    </a>
                                </li>
                                <li id="174">


                                    <a id="bibliography_5" title=" HAUTI&#201;RE N,BABARI R,DUMONT &#201;,et al.Estimating meteorological visibility using cameras:a probabilistic model-driven approach[C]//Proceedings of Asian Conference on Computer Vision.Berlin,Germany:Springer,2010:243-254." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Estimating meteorological visibility using cameras:a probabilistic model-driven approach">
                                        <b>[5]</b>
                                         HAUTI&#201;RE N,BABARI R,DUMONT &#201;,et al.Estimating meteorological visibility using cameras:a probabilistic model-driven approach[C]//Proceedings of Asian Conference on Computer Vision.Berlin,Germany:Springer,2010:243-254.
                                    </a>
                                </li>
                                <li id="176">


                                    <a id="bibliography_6" title=" 宋洪军,郜园园,陈阳舟.基于摄像机动态标定的交通能见度估计[J].计算机学报,2015,38(6):1172-1187." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJX201506007&amp;v=MzE2MjNCZHJHNEg5VE1xWTlGWTRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJyRnk3bFZMN05Mejc=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtOc0tUWDRGRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                         宋洪军,郜园园,陈阳舟.基于摄像机动态标定的交通能见度估计[J].计算机学报,2015,38(6):1172-1187.
                                    </a>
                                </li>
                                <li id="178">


                                    <a id="bibliography_7" title=" 胡平,杨旭东.高速公路能见度快速检测算法[J].公路交通科技,2017,34(4):115-122." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GLJK201704018&amp;v=MjYxMDRaYkc0SDliTXE0OUViSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUnJGeTdsVkw3TklpSEI=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtOc0tUWDRGRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                         胡平,杨旭东.高速公路能见度快速检测算法[J].公路交通科技,2017,34(4):115-122.
                                    </a>
                                </li>
                                <li id="180">


                                    <a id="bibliography_8" title=" XIANG Wenshu,XIAO Jianli,WANG Chongjing,et al.A new model for daytime visibility index estimation fused average sobel gradient and dark channel ratio[C]//Proceedings of International Conference on Computer Science and Network Technology.Washington D.C.,USA:IEEE Press,2014:109-112." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A new model for daytime visibility index estimation fused average sobel gradient and dark channel ratio">
                                        <b>[8]</b>
                                         XIANG Wenshu,XIAO Jianli,WANG Chongjing,et al.A new model for daytime visibility index estimation fused average sobel gradient and dark channel ratio[C]//Proceedings of International Conference on Computer Science and Network Technology.Washington D.C.,USA:IEEE Press,2014:109-112.
                                    </a>
                                </li>
                                <li id="182">


                                    <a id="bibliography_9" title=" 安明伟,陈启美,郭宗良.基于路况视频的气象能见度检测方法与系统设计[J].仪器仪表学报,2010,31(5):1148-1153." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=YQXB201005034&amp;v=MTk2MzhRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJyRnk3bFZMN05QRHpUYkxHNEg5SE1xbzlHWUk=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtOc0tUWDRGRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                         安明伟,陈启美,郭宗良.基于路况视频的气象能见度检测方法与系统设计[J].仪器仪表学报,2010,31(5):1148-1153.
                                    </a>
                                </li>
                                <li id="184">


                                    <a id="bibliography_10" title=" 许茜,殷绪成,李岩,等.基于图像理解的能见度测量方法[J].模式识别与人工智能,2013,26(6):543-551." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201306006&amp;v=Mjc1MTZxQnRHRnJDVVJMT2VaZVJyRnk3bFZMN05LRDdZYkxHNEg5TE1xWTlGWW9RS0RIODR2UjRUNmo1NE8zenE=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtOc0tUWDRGRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[10]</b>
                                         许茜,殷绪成,李岩,等.基于图像理解的能见度测量方法[J].模式识别与人工智能,2013,26(6):543-551.
                                    </a>
                                </li>
                                <li id="186">


                                    <a id="bibliography_11" title=" 钟丽,吴关胜,谢斌,等.基于图像分析的航道能见度评估算法研究[J].交通科技,2017(2):151-154." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=SKQB201702046&amp;v=MDE2ODhIOWJNclk5QllvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSckZ5N2xWTDdOTmliYWJMRzQ=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtOc0tUWDRGRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[11]</b>
                                         钟丽,吴关胜,谢斌,等.基于图像分析的航道能见度评估算法研究[J].交通科技,2017(2):151-154.
                                    </a>
                                </li>
                                <li id="188">


                                    <a id="bibliography_12" title=" 花毓幸,曾燕,邱新法.基于图像兴趣窗格测算大气能见度的方法研究[J].科技通报,2017,33(8):39-42." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=KJTB201708010&amp;v=MTU3OTFNcDQ5RVpJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSckZ5N2xWTDdOTGlmZmJMRzRIOWI=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtOc0tUWDRGRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[12]</b>
                                         花毓幸,曾燕,邱新法.基于图像兴趣窗格测算大气能见度的方法研究[J].科技通报,2017,33(8):39-42.
                                    </a>
                                </li>
                                <li id="190">


                                    <a id="bibliography_13" title=" SIMONYAN K,ZISSERMAN A.Very deep convolutional networks for large-scale image recognition[EB/OL].[2018-06-01].https://www.arxiv-vanity.com/papers/1409.1556/." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Very deep convolutional networks for large-scale image recognition">
                                        <b>[13]</b>
                                         SIMONYAN K,ZISSERMAN A.Very deep convolutional networks for large-scale image recognition[EB/OL].[2018-06-01].https://www.arxiv-vanity.com/papers/1409.1556/.
                                    </a>
                                </li>
                                <li id="192">


                                    <a id="bibliography_14" title=" BOSSE S,MANIRY D,MULLER K R,et al.Deep neural networks for no-reference and full-reference image quality assessment[J].IEEE Transactions on Image Processing,2018,27(1):206-219." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep Neural Networks for No-Reference and Full-Reference Image Quality Assessment">
                                        <b>[14]</b>
                                         BOSSE S,MANIRY D,MULLER K R,et al.Deep neural networks for no-reference and full-reference image quality assessment[J].IEEE Transactions on Image Processing,2018,27(1):206-219.
                                    </a>
                                </li>
                                <li id="194">


                                    <a id="bibliography_15" title=" LIN Kwan Yee,WANG Guangxiang.Hallucinated-IQA:no-reference image quality assessment via adversarial learning[EB/OL].[2018-06-01].https://arxiv.org/pdf/1804.01681.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Hallucinated-IQA:no-reference image quality assessment via adversarial learning">
                                        <b>[15]</b>
                                         LIN Kwan Yee,WANG Guangxiang.Hallucinated-IQA:no-reference image quality assessment via adversarial learning[EB/OL].[2018-06-01].https://arxiv.org/pdf/1804.01681.pdf.
                                    </a>
                                </li>
                                <li id="196">


                                    <a id="bibliography_16" title=" DRUCKER H,BURGES C J C,KAUFMAN L,et al.Support vector regression machines[EB/OL].[2018-06-01].http://papers.nips.cc/paper/1238-support-vector-regression-machines.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Support vector regression machines">
                                        <b>[16]</b>
                                         DRUCKER H,BURGES C J C,KAUFMAN L,et al.Support vector regression machines[EB/OL].[2018-06-01].http://papers.nips.cc/paper/1238-support-vector-regression-machines.pdf.
                                    </a>
                                </li>
                                <li id="198">


                                    <a id="bibliography_17" title=" SMOLA A J.A tutorial on support vector regression[M].[S.l.]:Kluwer Academic Publishers,2004." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A tutorial on support vector regression">
                                        <b>[17]</b>
                                         SMOLA A J.A tutorial on support vector regression[M].[S.l.]:Kluwer Academic Publishers,2004.
                                    </a>
                                </li>
                                <li id="200">


                                    <a id="bibliography_18" title=" EIGENSATZ M,PAULY M.Insights into the geometry of the gaussian kernel and an application in geometric modeling[EB/OL].[2018-06-01].https://lgg.epfl.ch/publications/2006/eigensatz_2006_IGG.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Insights into the geometry of the gaussian kernel and an application in geometric modeling">
                                        <b>[18]</b>
                                         EIGENSATZ M,PAULY M.Insights into the geometry of the gaussian kernel and an application in geometric modeling[EB/OL].[2018-06-01].https://lgg.epfl.ch/publications/2006/eigensatz_2006_IGG.pdf.
                                    </a>
                                </li>
                                <li id="202">


                                    <a id="bibliography_19" title=" GAO Junbin,GUNN S R,HARRIS C J,et al.A probabilistic framework for SVM regression and error bar estimation[J].Machine Learning,2002,46(1/2/3):71-89." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00001340326&amp;v=Mjk0NjEzazV6QmRoNGo5OVNYcVJyeG94Y01IN1I3cWVidWR0RkMzbFZiL1BJVnM9Tmo3QmFyTzRIdEhOckl0Rlora0pZ&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtOc0tUWDRGRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[19]</b>
                                         GAO Junbin,GUNN S R,HARRIS C J,et al.A probabilistic framework for SVM regression and error bar estimation[J].Machine Learning,2002,46(1/2/3):71-89.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJC" target="_blank">计算机工程</a>
                2019,45(09),242-247 DOI:10.19678/j.issn.1000-3428.0051855            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>一种基于迁移学习的能见度检测方法</b></span>
 <span class="shoufa"></span>                                     </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%94%90%E7%BB%8D%E6%81%A9&amp;code=39014182&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtOc0tUWDRGRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">唐绍恩</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%8E%E9%AA%9E&amp;code=38845000&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtOc0tUWDRGRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">李骞</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%83%A1%E7%A3%8A&amp;code=29342924&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtOc0tUWDRGRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">胡磊</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%A9%AC%E5%BC%BA&amp;code=40084334&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtOc0tUWDRGRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">马强</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%A1%BE%E5%A4%A7%E6%9D%83&amp;code=38845001&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtOc0tUWDRGRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">顾大权</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%9B%BD%E9%98%B2%E7%A7%91%E6%8A%80%E5%A4%A7%E5%AD%A6%E6%B0%94%E8%B1%A1%E6%B5%B7%E6%B4%8B%E5%AD%A6%E9%99%A2&amp;code=0269230&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtOc0tUWDRGRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">国防科技大学气象海洋学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=61741%E9%83%A8%E9%98%9F&amp;code=1515824&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtOc0tUWDRGRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">61741部队</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>为学习可有效反映能见度的视觉特征,解决大规模训练数据集构建困难的问题,提出一种将深度卷积神经网络应用于能见度检测的方法。将样本图像划分为多个子区域,利用预训练的VGG-16网络对其进行编码。通过编码特征集训练支持向量回归模型,并根据支持向量误差计算各子区域的融合权重,按权重融合子区域能见度估计值。实验结果表明,该方法检测正确率超过90%,可满足实际应用的需求。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%83%BD%E8%A7%81%E5%BA%A6%E6%A3%80%E6%B5%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtOc0tUWDRGRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">能见度检测;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtOc0tUWDRGRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtOc0tUWDRGRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">迁移学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E5%9B%9E%E5%BD%92&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtOc0tUWDRGRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">支持向量回归;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%9D%83%E9%87%8D%E8%9E%8D%E5%90%88&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtOc0tUWDRGRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">权重融合;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    唐绍恩(1993—),男,硕士研究生,主研方向为图像检测;;
                                </span>
                                <span>
                                    *李骞(通信作者),副教授、博士;E-mail:public_liqian@ 163. com;
                                </span>
                                <span>
                                    胡磊,工程师、硕士;;
                                </span>
                                <span>
                                    马强,讲师、硕士;;
                                </span>
                                <span>
                                    顾大权,教授。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-06-19</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金(41305138);</span>
                                <span>中国博士后科学基金(2017M621700);</span>
                    </p>
            </div>
                    <h1><b>A Visibility Detection Method Based on Transfer Learning</b></h1>
                    <h2>
                    <span>TANG Shaoen</span>
                    <span>LI Qian</span>
                    <span>HU Lei</span>
                    <span>MA Qiang</span>
                    <span>GU Daquan</span>
            </h2>
                    <h2>
                    <span>College of Meteorology and Oceanography,National University of Defense Technology</span>
                    <span>61741 Troops</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>In order to study the image features that can effectively reflect visibility,and solve the difficulties to structure a large-scale training data set,this paper proposes a method for applying deep convolution neural networks to visibility detection.The sample image is divided into several subdomains and encoded by using the pre-trained VGG-16 network.Support Vector Regression(SVR) models are trained with coded feature sets,and each subdomains' fusion weight is calculated according to the error analysis of the support vector,and then fuse the visibility estimates of subdomains by weight.Experimental results show that the detection accuracy of the proposed method exceeds 90%,which can meet the requirements of application.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=visibility%20detection&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtOc0tUWDRGRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">visibility detection;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=deep%20neural%20networks&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtOc0tUWDRGRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">deep neural networks;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=transfer%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtOc0tUWDRGRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">transfer learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Support%20Vector%20Regression%20(SVR)&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtOc0tUWDRGRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Support Vector Regression (SVR);</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=weights%20fusion&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtOc0tUWDRGRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">weights fusion;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-06-19</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="41" name="41" class="anchor-tag">0 概述</h3>
                <div class="p1">
                    <p id="42">能见度是指视力正常的人能从背景中识别出目标物的最大距离<citation id="204" type="reference"><link href="166" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>,是反映大气透明程度的一个重要指标。低能见度天气对交通安全和军事行动等影响较大<citation id="205" type="reference"><link href="168" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>,因此,准确、高效的能见度检测方法是地面气象观测重要的研究内容之一。近年来,能见度自动检测方法以及相关仪器的研究取得了一定进展,但由于能见度的定义具有较强的主观性,其本身受诸多因素的影响。与人工观测相比,各类方法的精确性与稳定性均存在较大差距,因此,对能见度的自动检测与估计已成为当前大气科学领域研究的热点之一。为学习可有效反映能见度的视觉特征,解决能见度数据集样本量少且分布不均匀的问题,本文提出一种基于迁移学习的能见度检测方法,迁移已有的深度神经网络来进行图像抽象特征的提取,并通过回归模型和子区域图像估计值融合实现能见度检测。</p>
                </div>
                <h3 id="43" name="43" class="anchor-tag">1 相关工作</h3>
                <div class="p1">
                    <p id="44">现有能见度检测方法主要包括目测法、器测法和基于图像视频的检测方法等。目测法通过人工观测来估计所在场景的能见度<citation id="206" type="reference"><link href="170" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>,其准确程度受观测人员主观经验、视力情况和目标物选取的影响,且检测频次受限。器测法主要利用透视式或散射式能见度仪进行检测<citation id="207" type="reference"><link href="172" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>,两者均以采样空间数据代表大气全程范围能见度,检测精度易受局部空间大气状况的影响,且检测仪器较昂贵,难以满足能见度检测的实际应用需求。</p>
                </div>
                <div class="p1">
                    <p id="45">近年来,摄像机因其覆盖范围广、信息内容丰富等优点,被广泛用于交通、安全等领域<citation id="210" type="reference"><link href="174" rel="bibliography" /><link href="176" rel="bibliography" /><sup>[<a class="sup">5</a>,<a class="sup">6</a>]</sup></citation>。其中,模型驱动法根据大气对图像的衰减效应建立光传播物理模型,估计模型参数,以此反推能见度。文献<citation id="208" type="reference">[<a class="sup">7</a>]</citation>利用暗原色先验原理计算大气光传播模型的透射率,根据CIE能见度公式计算能见度。文献<citation id="209" type="reference">[<a class="sup">8</a>]</citation>提取图像的平均Sobel梯度,构建能见度估计模型。该方法的估计精度与物理模型定义、参数设置紧密相关,而大气中影响光传播的悬浮粒子种类较多,粒子分布不均匀,难以准确定义光传播的物理模型。</p>
                </div>
                <div class="p1">
                    <p id="46">数据驱动法主要针对低能见度天气造成的图像模糊和退化,通过历史数据来训练图像特征与能见度的关系模型<citation id="213" type="reference"><link href="182" rel="bibliography" /><link href="184" rel="bibliography" /><sup>[<a class="sup">9</a>,<a class="sup">10</a>]</sup></citation>。该方法无须建立精确的光传播物理模型,可充分利用累积的先验数据获得相对较高的检测精度。例如:文献<citation id="211" type="reference">[<a class="sup">11</a>]</citation>拟合能见度与Lip对比度的多元线性函数;文献<citation id="212" type="reference">[<a class="sup">12</a>]</citation>训练能见度和兴趣域窗格灰度均方差的线性回归模型。这类方法需要提取一种或多种特征,然而,特定的视觉特征通常不能完全反映能见度衰减对图像的各类影响,因此该类方法的能见度检测精度较低。随着数据规模的扩大和计算机的快速发展,深度学习在机器视觉领域取得重大突破,其可提取到更具表征能力的图像特征。深度神经网络需要海量数据进行训练,但大规模能见度标注样本集通常难以构建,其主要原因是:</p>
                </div>
                <div class="p1">
                    <p id="47">1)能见度标注易受观测员主观因素的影响,标注精度难以保证且标注工作量较大。</p>
                </div>
                <div class="p1">
                    <p id="48">2)能见度受天气情况影响较大,恶劣天气出现频次较低,低能见度样本数量较少。</p>
                </div>
                <div class="p1">
                    <p id="49">为解决上述问题,本文提出一种基于迁移学习的能见度检测方法,将已有的深度神经网络迁移至能见度检测领域,通过深度神经网络提取能见度图像的抽象特征,修改网络分类层并构建能见度检测模型。由于图像的部分区域易受内容单一、遮挡物多、镜面反射等因素影响,能见度估计结果会产生显著误差。本文将样本图像划分为多个子区域,利用预训练的VGG-16网络对其进行编码。用支持向量回归(Support Vector Regression,SVR)替换网络分类层构建各子区域图像特征编码结果与能见度的回归模型,并通过训练得到回归模型参数。最后,将各回归模型的预测分布方差和预测拟合方差作为融合权重,按权重融合各子区域估计值,得到该图像场景的最终能见度检测结果。</p>
                </div>
                <h3 id="50" name="50" class="anchor-tag">2 能见度检测模型</h3>
                <div class="p1">
                    <p id="51">利用历史图像或视频数据训练能见度检测模型时,定义:已知训练样本图像集<i>X</i>={<i>x</i><sub>1</sub>,<i>x</i><sub>2</sub>,…,<i>x</i><sub><i>N</i></sub>},其对应的能见度标注集为<i>Y</i>={<i>y</i><sub>1</sub>,<i>y</i><sub>2</sub>,…,<i>y</i><sub><i>N</i></sub>},建立训练图像与能见度检测值的回归函数<i>f</i>(<i>x</i>),得到检测样本图像<i>x</i>与能见度检测值的映射关系。为降低部分区域估计误差对整体能见度检测结果的影响,本文将样本图像划分为<i>n</i>个大小相同的子区域,则训练样本集可表示为:</p>
                </div>
                <div class="p1">
                    <p id="52"><i>X</i>′={<i>X</i>′<sub>1</sub>,<i>X</i>′<sub>2</sub>,…,<i>X</i>′<sub><i>n</i></sub>}={{<i>x</i><mathml id="107"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>1</mn><mn>1</mn></msubsup></mrow></math></mathml>,<i>x</i><mathml id="108"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>2</mn><mn>1</mn></msubsup></mrow></math></mathml>,…,<i>x</i><mathml id="109"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>Ν</mi><mn>1</mn></msubsup></mrow></math></mathml>},</p>
                </div>
                <div class="p1">
                    <p id="53">{<i>x</i><mathml id="110"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>1</mn><mn>2</mn></msubsup></mrow></math></mathml>,<i>x</i><mathml id="111"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></math></mathml>,…,<i>x</i><mathml id="112"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>Ν</mi><mn>2</mn></msubsup></mrow></math></mathml>},…,{<i>x</i><mathml id="113"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>1</mn><mi>n</mi></msubsup></mrow></math></mathml>,<i>x</i><mathml id="114"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>2</mn><mi>n</mi></msubsup></mrow></math></mathml>,…,<i>x</i><mathml id="115"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>Ν</mi><mi>n</mi></msubsup></mrow></math></mathml>}}</p>
                </div>
                <div class="p1">
                    <p id="54">对应的能见度标注集为:</p>
                </div>
                <div class="p1">
                    <p id="55"><i>Y</i>′={<i>Y</i>′<sub>1</sub>,<i>Y</i>′<sub>2</sub>,…,<i>Y</i>′<sub><i>n</i></sub>}={{<i>y</i><mathml id="116"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>1</mn><mn>1</mn></msubsup></mrow></math></mathml>,<i>y</i><mathml id="117"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>2</mn><mn>1</mn></msubsup></mrow></math></mathml>,…,<i>y</i><mathml id="118"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>Ν</mi><mn>1</mn></msubsup></mrow></math></mathml>},</p>
                </div>
                <div class="p1">
                    <p id="56">{<i>y</i><mathml id="119"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>1</mn><mn>2</mn></msubsup></mrow></math></mathml>,<i>y</i><mathml id="120"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></math></mathml>,…,<i>y</i><mathml id="121"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>Ν</mi><mn>2</mn></msubsup></mrow></math></mathml>},…,{<i>y</i><mathml id="122"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>1</mn><mi>n</mi></msubsup></mrow></math></mathml>,<i>y</i><mathml id="123"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>2</mn><mi>n</mi></msubsup></mrow></math></mathml>,…,<i>y</i><mathml id="124"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>Ν</mi><mi>n</mi></msubsup></mrow></math></mathml>}}</p>
                </div>
                <div class="p1">
                    <p id="57">本文方法流程如图1所示。首先,将输入图像划分成<i>n</i>个子区域,本文将<i>n</i>值设为9,并利用预训练VGG-16网络对各子区域图像进行编码,得到编码特征集。然后,以编码特征集作为输入,训练相应支持向量回归模型并估计子区域能见度值。最后,根据支持向量误差计算各子区域融合权重,并按权重融合子区域能见度估计值,得到最终检测结果。</p>
                </div>
                <div class="area_img" id="58">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201909038_058.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtOc0tUWDRGRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 本文方法流程" src="Detail/GetImg?filename=images/JSJC201909038_058.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtOc0tUWDRGRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图1 本文方法流程</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201909038_058.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtOc0tUWDRGRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h3 id="59" name="59" class="anchor-tag">3 VGG-16网络迁移</h3>
                <h4 class="anchor-tag" id="60" name="60">3.1 网络编码</h4>
                <div class="p1">
                    <p id="61">大气能见度变化对成像的影响主要体现在其对图像亮度、对比度、颜色和景深等视觉特征的退化效果上。然而,只利用特定几种视觉特征难以准确、全面地反映大气对成像衰减的影响。为此,本文将经过预训练的深度神经网络迁移至能见度检测领域,通过网络提取特征对图像进行编码。作为一种典型的深度神经网络,VGG-16网络已广泛应用于图像分类、目标识别等计算机视觉任务中<citation id="214" type="reference"><link href="190" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>。同时,由于VGG-16网络可有效提取不同层次的抽象特征,其逐渐被迁移并应用到无参考图像的质量评估领域<citation id="215" type="reference"><link href="192" rel="bibliography" /><link href="194" rel="bibliography" /><sup>[<a class="sup">14</a>,<a class="sup">15</a>]</sup></citation>。与图像质量评估问题相似,本文也侧重考察外部环境对图像衰减的影响,因此,将预训练的VGG-16网络迁移至能见度检测模型中,以有效提取图像抽象特征,即网络编码。</p>
                </div>
                <div class="p1">
                    <p id="62">VGG-16网络共有13个卷积层、5个最大池化层、3个全连接层和1个分类层,卷积滤波器大小均为3×3,输入为224×224的RGB图像,其网络结构如图2所示。为训练能见度检测模型,本文将子区域样本图像集<i>X</i>′作为输入,将网络第3个全连接层1 000维向量作为编码特征集<i>M</i>′={<i>M</i>′<sub>1</sub>,<i>M</i>′<sub>2</sub>,…,<i>M</i>′<sub><i>n</i></sub>}={{<i>m</i><mathml id="125"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>1</mn><mn>1</mn></msubsup></mrow></math></mathml>,<i>m</i><mathml id="126"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>2</mn><mn>1</mn></msubsup></mrow></math></mathml>,…,<i>m</i><mathml id="127"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>Ν</mi><mn>1</mn></msubsup></mrow></math></mathml>},{<i>m</i><mathml id="128"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>1</mn><mn>2</mn></msubsup></mrow></math></mathml>,<i>m</i><mathml id="129"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></math></mathml>,…,<i>m</i><mathml id="130"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>Ν</mi><mn>2</mn></msubsup></mrow></math></mathml>},…,{<i>m</i><mathml id="131"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>1</mn><mi>n</mi></msubsup></mrow></math></mathml>,<i>m</i><mathml id="132"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>2</mn><mi>n</mi></msubsup></mrow></math></mathml>,…,<i>m</i><mathml id="133"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>Ν</mi><mi>n</mi></msubsup></mrow></math></mathml>}},即为子区域图像编码结果。</p>
                </div>
                <div class="area_img" id="63">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201909038_063.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtOc0tUWDRGRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 VGG-16网络编码过程" src="Detail/GetImg?filename=images/JSJC201909038_063.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtOc0tUWDRGRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图2 VGG-16网络编码过程</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201909038_063.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtOc0tUWDRGRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="64" name="64">3.2 支持向量回归机训练</h4>
                <div class="p1">
                    <p id="65">为建立图像特征与能见度值间的关系,本文采用SVR建立回归模型<citation id="216" type="reference"><link href="196" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>。SVR可解决小样本条件下的非线性回归问题<citation id="217" type="reference"><link href="198" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>,能有效改善“维数灾难”和“过拟合”等缺点,提高泛化能力。本文将训练样本集中第<i>i</i>幅图像的第<i>q</i>个子区域编码特征<i>m</i><mathml id="134"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>q</mi></msubsup></mrow></math></mathml>映射到高维特征空间<i>φ</i>(<i>m</i><mathml id="135"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>q</mi></msubsup></mrow></math></mathml>)中,并将其作为自变量,对应的能见度标注值<i>y</i><sub><i>i</i></sub>作为因变量,建立高维特征空间下的线性回归模型:</p>
                </div>
                <div class="p1">
                    <p id="66"><i>y</i><sub><i>i</i></sub>=<i>f</i>(<i>m</i><mathml id="136"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>q</mi></msubsup></mrow></math></mathml>)=<i>ωφ</i>(<i>m</i><mathml id="137"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>q</mi></msubsup></mrow></math></mathml>)+<i>b</i>      (1)</p>
                </div>
                <div class="p1">
                    <p id="67">其中,<i>ω</i>为权向量,<i>b</i>为偏置项。依据结构风险最小化原则,回归模型权重<i>ω</i>与偏差<i>b</i>可通过最小化目标函数得到,具体如下:</p>
                </div>
                <div class="p1">
                    <p id="68" class="code-formula">
                        <mathml id="68"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>R</mi><mo stretchy="false">(</mo><mi>m</mi><msubsup><mrow></mrow><mi>i</mi><mi>q</mi></msubsup><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mrow><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">ω</mi><mo stretchy="false">∥</mo></mrow><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mfrac><mn>1</mn><mi>Ν</mi></mfrac><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mrow><mrow><mo>|</mo><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>m</mi><msubsup><mrow></mrow><mi>i</mi><mi>q</mi></msubsup><mo stretchy="false">)</mo><mo>-</mo><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mo>|</mo></mrow></mrow></mstyle></mrow><msub><mrow></mrow><mi>ε</mi></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="69">其中,<i>N</i>为训练样本集中的图像数量,损失函数设为<mathml id="138"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>|</mo><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>m</mi><msubsup><mrow></mrow><mi>i</mi><mi>q</mi></msubsup><mo stretchy="false">)</mo><mo>-</mo><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mo>|</mo></mrow><mo>,</mo><mi>ε</mi></mrow></math></mathml>为回归函数误差限值。为使‖<i>ω</i>‖<sup>2</sup>欧拉范数最小并控制拟合误差,引入松弛变量<mathml id="139"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mrow><mo>{</mo><mrow><mi>ξ</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mo>}</mo></mrow></mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></msubsup></mrow></math></mathml>、<mathml id="140"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mrow><mo>{</mo><mrow><mi>ξ</mi><msubsup><mrow></mrow><mi>i</mi><mo>*</mo></msubsup></mrow><mo>}</mo></mrow></mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></msubsup></mrow></math></mathml>和惩罚因子C,将式(2)中的最优化问题转化为约束最小化问题,即:</p>
                </div>
                <div class="p1">
                    <p id="70" class="code-formula">
                        <mathml id="70"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mi>min</mi><mfrac><mn>1</mn><mn>2</mn></mfrac><mi mathvariant="bold-italic">ω</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">ω</mi><mo>+</mo><mi>C</mi><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mi>ξ</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mo>+</mo><mi>C</mi><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mi>ξ</mi></mstyle><msubsup><mrow></mrow><mi>i</mi><mo>*</mo></msubsup></mtd></mtr><mtr><mtd><mtext>s</mtext><mo>.</mo><mtext>t</mtext><mo>.</mo><mi mathvariant="bold-italic">ω</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">φ</mi><mo stretchy="false">(</mo><mi>m</mi><msubsup><mrow></mrow><mi>i</mi><mi>q</mi></msubsup><mo stretchy="false">)</mo><mo>+</mo><mi>b</mi><mo>-</mo><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>≤</mo><mi>ε</mi><mo>+</mo><mi>ξ</mi><msubsup><mrow></mrow><mi>i</mi><mo>*</mo></msubsup></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mi mathvariant="bold-italic">ω</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">φ</mi><mo stretchy="false">(</mo><mi>m</mi><msubsup><mrow></mrow><mi>i</mi><mi>q</mi></msubsup><mo stretchy="false">)</mo><mo>-</mo><mi>b</mi><mo>≤</mo><mi>ε</mi><mo>+</mo><mi>ξ</mi><msub><mrow></mrow><mi>i</mi></msub></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mi>ξ</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mi>ξ</mi><msubsup><mrow></mrow><mi>i</mi><mo>*</mo></msubsup><mo>≥</mo><mn>0</mn><mo>,</mo><mi>i</mi><mo>=</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mo>⋯</mo><mo>,</mo><mi>Ν</mi></mtd></mtr></mtable></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>3</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="71">为便于求解,分别引入拉格朗日乘子α<sub>i</sub>、α<sup>*</sup><sub>i</sub>、η<sub>i</sub>、η<sup>*</sup><sub>i</sub>,将式(3)转化为对偶问题,通过求解约束化问题的对偶凸二次规划,可得<i>SVR</i>模型:</p>
                </div>
                <div class="p1">
                    <p id="72" class="code-formula">
                        <mathml id="72"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>m</mi><mo stretchy="false">)</mo><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mo stretchy="false">(</mo></mstyle><mi>α</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mi>α</mi><msubsup><mrow></mrow><mi>i</mi><mo>*</mo></msubsup><mo stretchy="false">)</mo><mi>Q</mi><mo stretchy="false">(</mo><mi>m</mi><msubsup><mrow></mrow><mi>i</mi><mi>q</mi></msubsup><mo>,</mo><mi>m</mi><mo stretchy="false">)</mo><mo>+</mo><mi>b</mi><msup><mrow></mrow><mo>*</mo></msup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>4</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="73">其中,Q(m<mathml id="141"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>q</mi></msubsup></mrow></math></mathml>,m)为核函数,m为输入的子区域图像编码特征,f(m)为其能见度估计值。式(4)即为图像编码特征与能见度的回归模型。常用的核函数有线性核函数、多项式核函数和高斯径向基核函数<citation id="218" type="reference"><link href="200" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>等,本文采用高斯径向基核函数建模。</p>
                </div>
                <div class="p1">
                    <p id="74">在上述<i>SVR</i>模型中,ε控制<i>SVR</i>模型对样本数据的不敏感程度,其值与样本噪声密切相关。ε过大则模型精度低,ε过小则模型泛化能力差。C为惩罚因子,反映模型对样本数据的惩罚程度,C过大则模型泛化能力差,C过小则模型误差大。基于上述分析,本文选取回归函数误差限值ε和惩罚因子C作为子区域融合权重的一部分。为保证模型精度与泛化能力,将<i>k</i>折-交叉验证和网络搜索相结合来训练模型参数。</p>
                </div>
                <h3 id="75" name="75" class="anchor-tag">4 子区域估计值融合</h3>
                <div class="p1">
                    <p id="76">当图像部分区域受内容单一、遮挡物多、镜面反射等因素影响时,其能见度估计结果易产生较大误差。为降低单一图像对能见度检测的影响,本文按权重融合图像中所有子区域的能见度估计值,从而获得更稳定的检测结果,融合公式如下:</p>
                </div>
                <div class="p1">
                    <p id="77" class="code-formula">
                        <mathml id="77"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>V</mi><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>q</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>v</mi></mstyle><msup><mrow></mrow><mi>q</mi></msup><mo>×</mo><mi>ω</mi><msup><mrow></mrow><mi>q</mi></msup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>5</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="78">其中,<i>V</i>为单幅图像融合后的能见度检测值,<i>n</i>为子区域数量,<i>v</i><sup><i>q</i></sup>为图像第<i>q</i>个子区域的能见度估计值,<i>ω</i><sup><i>q</i></sup>为其融合权重,其数值反映该子区域能见度估计值的可信度。考虑到回归模型预测方差<i>σ</i><sup><i>q</i></sup>是刻画数据扰动对模型拟合能力的影响的重要因素,本文子区域融合权重<i>ω</i><sup><i>q</i></sup>表示为归一化的预测方差倒数,即:</p>
                </div>
                <div class="p1">
                    <p id="79" class="code-formula">
                        <mathml id="79"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>ω</mi><msup><mrow></mrow><mi>q</mi></msup><mo>=</mo><mfrac><mrow><mfrac><mn>1</mn><mrow><mi>σ</mi><msup><mrow></mrow><mi>q</mi></msup></mrow></mfrac></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>p</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><mfrac><mn>1</mn><mrow><mi>σ</mi><msup><mrow></mrow><mi>q</mi></msup></mrow></mfrac></mrow></mstyle></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>6</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="80">根据文献<citation id="219" type="reference">[<a class="sup">19</a>]</citation>提出的SVR回归误差分析方法,本文将第<i>q</i>个子区域图像的预测方差<i>σ</i><sup><i>q</i></sup>定义为预测分布方差<i>δ</i><sup><i>q</i></sup>和预测拟合方差<i>β</i><sup><i>q</i></sup> 2个部分。其中,预测分布方差由数据联合分布的不确定性产生,可通过训练集数据和测试数据的协方差矩阵<i><b>K</b></i>计算得到,即:</p>
                </div>
                <div class="p1">
                    <p id="81"><i>δ</i><sup><i>q</i></sup>=<i>K</i><sub>(</sub><i>l</i>,<i>l</i>)-<i><b>K</b></i><sup>T</sup><sub>(</sub><i>Z</i>,<i>l</i>)<i><b>K</b></i><mathml id="142"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>Ζ</mi><mo>,</mo><mi>Ζ</mi><mo stretchy="false">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msubsup></mrow></math></mathml><i><b>K</b></i><sub>(</sub><i>Z</i>,<i>l</i>)      (7)</p>
                </div>
                <div class="p1">
                    <p id="82">其中,<mathml id="143"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ζ</mi><mo>=</mo><mrow><mo>{</mo><mrow><mo stretchy="false">{</mo><mi>m</mi><msubsup><mrow></mrow><mn>1</mn><mi>q</mi></msubsup><mo>,</mo><mi>y</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">}</mo><mo>,</mo><mo stretchy="false">{</mo><mi>m</mi><msubsup><mrow></mrow><mn>2</mn><mi>q</mi></msubsup><mo>,</mo><mi>y</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">}</mo><mo>,</mo><mo>⋯</mo><mrow><mrow><mo>,</mo><mo stretchy="false">{</mo><mi>m</mi><msubsup><mrow></mrow><mi>Ν</mi><mi>q</mi></msubsup><mo>,</mo><mi>y</mi><msub><mrow></mrow><mi>Ν</mi></msub><mo stretchy="false">}</mo></mrow><mo>}</mo></mrow></mrow></mrow></mrow></math></mathml>为训练样本集第q个子区域编码特征与其能见度标注值构成的集合,l为测试图像第q个子区域编码特征与其能见度预测值构成的样本点,K<sub>(l,l)</sub>为l点的自协方差,<i><b>K</b></i><sub>(</sub><i>Z</i>,<i>l</i>)为集合<i>Z</i>与<i>l</i>点的协方差矩阵,<i><b>K</b></i><mathml id="144"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>Ζ</mi><mo>,</mo><mi>Ζ</mi><mo stretchy="false">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msubsup></mrow></math></mathml>为集合Z的协方差矩阵逆矩阵,其中,协方差矩阵<i><b>K</b></i><sub>(</sub><i>A</i>,<i>B</i>)如下:</p>
                </div>
                <div class="p1">
                    <p id="83" class="code-formula">
                        <mathml id="83"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi mathvariant="bold-italic">Κ</mi><msub><mrow></mrow><mrow><mrow><mo stretchy="false">(</mo><mi>A</mi><mo>,</mo><mi>B</mi><mo stretchy="false">)</mo></mrow><msub><mrow></mrow><mi>u</mi></msub></mrow></msub><mo>=</mo><mrow><mo>[</mo><mrow><mtable><mtr><mtd><mi>k</mi><msub><mrow></mrow><mrow><mn>1</mn><mn>1</mn></mrow></msub></mtd><mtd><mi>k</mi><msub><mrow></mrow><mrow><mn>1</mn><mn>2</mn></mrow></msub></mtd><mtd><mo>⋯</mo></mtd><mtd><mi>k</mi><msub><mrow></mrow><mrow><mn>1</mn><mi>u</mi></mrow></msub></mtd></mtr><mtr><mtd><mi>k</mi><msub><mrow></mrow><mrow><mn>2</mn><mn>1</mn></mrow></msub></mtd><mtd><mi>k</mi><msub><mrow></mrow><mrow><mn>2</mn><mn>2</mn></mrow></msub></mtd><mtd><mo>⋯</mo></mtd><mtd><mi>k</mi><msub><mrow></mrow><mrow><mn>2</mn><mi>u</mi></mrow></msub></mtd></mtr><mtr><mtd><mo>⋮</mo></mtd><mtd><mo>⋮</mo></mtd><mtd></mtd><mtd><mo>⋮</mo></mtd></mtr><mtr><mtd><mi>k</mi><msub><mrow></mrow><mrow><mi>u</mi><mn>1</mn></mrow></msub></mtd><mtd><mi>k</mi><msub><mrow></mrow><mrow><mi>u</mi><mn>2</mn></mrow></msub></mtd><mtd><mo>⋯</mo></mtd><mtd><mi>k</mi><msub><mrow></mrow><mrow><mi>u</mi><mi>u</mi></mrow></msub></mtd></mtr></mtable></mrow><mo>]</mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>8</mn><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mi>k</mi><msub><mrow></mrow><mrow><mi>o</mi><mi>r</mi></mrow></msub><mo>=</mo><mi>C</mi><mi>o</mi><mi>v</mi><mo stretchy="false">(</mo><mi>h</mi><msub><mrow></mrow><mi>o</mi></msub><mo>,</mo><mi>h</mi><msub><mrow></mrow><mi>r</mi></msub><mo stretchy="false">)</mo><mo>,</mo><mi>o</mi><mo>,</mo><mi>r</mi><mo>=</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mo>⋯</mo><mo>,</mo><mi>u</mi><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>9</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="84">其中,<i>u</i>表示集合<i>A</i>与集合<i>B</i>构成新集合<i>H</i>的样本点数量,<i>h</i><sub><i>o</i></sub>和<i>h</i><sub><i>r</i></sub>分别表示集合<i>H</i>中第<i>o</i>个和第<i>r</i>个样本点,<i>Cov</i>(<i>h</i><sub><i>o</i></sub>,<i>h</i><sub><i>r</i></sub>)为样本点<i>o</i>与样本点<i>r</i>的协方差。</p>
                </div>
                <div class="p1">
                    <p id="85">预测拟合方差<i>β</i><sup><i>q</i></sup>表示由训练集数据中的固有噪声导致的拟合误差,本文采用训练SVR模型时得到的惩罚因子和误差限值计算预测拟合方差,即:</p>
                </div>
                <div class="p1">
                    <p id="86" class="code-formula">
                        <mathml id="86"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>β</mi><msup><mrow></mrow><mi>q</mi></msup><mo>=</mo><mfrac><mn>2</mn><mrow><mrow><mo stretchy="false">(</mo><mi>C</mi><msup><mrow></mrow><mi>q</mi></msup><mo stretchy="false">)</mo></mrow><msup><mrow></mrow><mn>2</mn></msup></mrow></mfrac><mo>+</mo><mfrac><mrow><mrow><mo stretchy="false">(</mo><mi>ε</mi><msup><mrow></mrow><mi>q</mi></msup><mo stretchy="false">)</mo></mrow><msup><mrow></mrow><mn>2</mn></msup><mo stretchy="false">(</mo><mi>ε</mi><msup><mrow></mrow><mi>q</mi></msup><mi>C</mi><msup><mrow></mrow><mi>q</mi></msup><mo>+</mo><mn>3</mn><mo stretchy="false">)</mo></mrow><mrow><mn>3</mn><mo stretchy="false">(</mo><mi>ε</mi><msup><mrow></mrow><mi>q</mi></msup><mi>C</mi><msup><mrow></mrow><mi>q</mi></msup><mo>+</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>0</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="87">其中,惩罚因子<i>C</i><sup><i>q</i></sup>和误差限值<i>ε</i><sup><i>q</i></sup>通过第3.2节SVR模型训练得到。综合式(7)和式(10),测试图像第<i>q</i>个子区域预测方差<i>σ</i><sup><i>q</i></sup>可表示为:</p>
                </div>
                <div class="p1">
                    <p id="88" class="code-formula">
                        <mathml id="88"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>σ</mi><msup><mrow></mrow><mi>q</mi></msup><mo>=</mo><mi>δ</mi><msup><mrow></mrow><mi>q</mi></msup><mo>+</mo><mi>β</mi><msup><mrow></mrow><mi>q</mi></msup><mo>=</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mi>Κ</mi><msub><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>l</mi><mo>,</mo><mi>l</mi><mo stretchy="false">)</mo></mrow></msub><mo>-</mo><mi mathvariant="bold-italic">Κ</mi><msubsup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>Ζ</mi><mo>,</mo><mi>l</mi><mo stretchy="false">)</mo></mrow><mtext>Τ</mtext></msubsup><mi mathvariant="bold-italic">Κ</mi><msubsup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>Ζ</mi><mo>,</mo><mi>Ζ</mi><mo stretchy="false">)</mo></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msubsup><mi mathvariant="bold-italic">Κ</mi><msub><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>Ζ</mi><mo>,</mo><mi>l</mi><mo stretchy="false">)</mo></mrow></msub><mo>+</mo><mfrac><mn>2</mn><mrow><mo stretchy="false">(</mo><mi>C</mi><msup><mrow></mrow><mi>q</mi></msup><mo stretchy="false">)</mo><msup><mrow></mrow><mn>2</mn></msup></mrow></mfrac><mo>+</mo><mfrac><mrow><mo stretchy="false">(</mo><mi>ε</mi><msup><mrow></mrow><mi>q</mi></msup><mo stretchy="false">)</mo><msup><mrow></mrow><mn>2</mn></msup><mo stretchy="false">(</mo><mi>ε</mi><msup><mrow></mrow><mi>q</mi></msup><mi>C</mi><msup><mrow></mrow><mi>q</mi></msup><mo>+</mo><mn>3</mn><mo stretchy="false">)</mo></mrow><mrow><mn>3</mn><mo stretchy="false">(</mo><mi>ε</mi><msup><mrow></mrow><mi>q</mi></msup><mi>C</mi><msup><mrow></mrow><mi>q</mi></msup><mo>+</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>1</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <h3 id="89" name="89" class="anchor-tag">5 实验结果与分析</h3>
                <h4 class="anchor-tag" id="90" name="90">5.1 实验数据集与设备</h4>
                <div class="p1">
                    <p id="91">为验证本文方法的有效性,在CPU为Intel i7-6700k 4.00 GHz、内存为16 GB的PC机上基于Matlab进行实验。实验数据为固定地点主动摄像机采集的图像集,采集时间为2014年1月—12月,从5时—18时每小时采集一次数据,每次采集均通过摄像机获取3个无重叠视角的图像,图像分辨率为640像素×480像素,样本图像标注值为能见度的人工观测值,选取5 511帧图像作为样本数据集,其中,4 078帧作为模型训练样本集,其余1 433帧作为测试样本集,并将9个子区域的图像分辨率缩放至224像素×224像素。为分析本文方法在不同能见度范围下的有效性,根据能见度等级划分规范<citation id="220" type="reference"><link href="166" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>,将所有能见度样本分为4个区间,区间样本数量分布如表1所示,其中,不同能见度下视角1和视角2的示例图像分别如图3(a)、图3(c)和图3(b)、图3(d)所示。表1中选取的数据样本覆盖了不同能见度的值域范围,但低能见度的样本数量相对较少。</p>
                </div>
                <div class="area_img" id="92">
                    <p class="img_tit"><b>表1 不同能见度区间内样本图像的数量分布</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="92" border="1"><tr><td colspan="2"><br />能见度<br />范围/km</td><td>训练集样本<br />图像数/帧</td><td>测试集样本<br />图像数/帧</td></tr><tr><td>0.0</td><td>～0.5</td><td>111</td><td>39</td></tr><tr><td><br />0.5</td><td>～2.0</td><td>800</td><td>281</td></tr><tr><td><br />2.0</td><td>～10.0</td><td>2 866</td><td>1 007</td></tr><tr><td><br /></td><td>≥10.0</td><td>301</td><td>106</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="93">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201909038_093.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtOc0tUWDRGRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 不同视角样本图像示例" src="Detail/GetImg?filename=images/JSJC201909038_093.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtOc0tUWDRGRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图3 不同视角样本图像示例</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201909038_093.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtOc0tUWDRGRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="94" name="94">5.2 结果分析</h4>
                <div class="p1">
                    <p id="95">本文首先对图3中的示例图像检测结果进行分析,然后对测试集能见度检测值的分布、标注值与检测值的均值和方差、检测结果的平均误差和正确率进行分析。通过对比不同融合策略来验证本文权重融合方法的有效性。最后,将本文方法与光传播模型(Light Propagation Model,LPM)<citation id="221" type="reference"><link href="178" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>、平均Sobel梯度 (Average Sobel gradient,AS)方法<citation id="222" type="reference"><link href="180" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>、对比度学习(Contrast Learning,CL)方法<citation id="223" type="reference"><link href="186" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>和兴趣窗格拟合(Interest pane Fitting,IF)方法<citation id="224" type="reference"><link href="188" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>进行对比分析。</p>
                </div>
                <div class="p1">
                    <p id="96">应用本文方法检测图3图像的能见度。图3(a)、图3(c)的检测结果分别为14.119 km和1.128 km,与标注值的误差分别为0.881 km和0.072 km;图3(b)、图3(d)的检测结果分别为7.328 km和0.466 km,与标注值的误差分别为0.328 km和0.016 km,检测结果均接近标注值。</p>
                </div>
                <div class="p1">
                    <p id="97">为对检测结果的正确性进行量化,本文设定当检测误差小于标注值的10%时,判定检测结果正确,否则为错误。测试样本集能见度检测结果分布如图4所示,其中3条直线由上至下分别表示检测值为标注值的110%、100%和90%,即当检测值位于上下2条直线之间时,检测结果正确。从图4可以看出,测试结果基本正确。</p>
                </div>
                <div class="area_img" id="98">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201909038_098.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtOc0tUWDRGRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 测试样本能见度检测值分布" src="Detail/GetImg?filename=images/JSJC201909038_098.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtOc0tUWDRGRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图4 测试样本能见度检测值分布</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201909038_098.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtOc0tUWDRGRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="99">在量化检测分布的基础上,分别对标注值与检测值的均值、标准差等统计量进行分析,并对检测正确率进行统计,结果如表2所示。从表2可以看出,检测结果与标注值的均值、标准差相差较小,表明检测结果的集中趋势和离散程度与实际能见度基本一致。检测结果的平均误差均在标注值平均值的10%以内,检测结果总体较好。在正确率方面,除0.0 km～0.5 km区间的正确率较低外,其他区间的正确率均接近或超过90%,效果较好,其主要原因是当能见度过低时,图像衰退较严重,所提取的抽象特征的表征能力较弱,但总体仍能达到78.36%。</p>
                </div>
                <div class="area_img" id="100">
                    <p class="img_tit"><b>表2 本文方法的结果统计</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="100" border="1"><tr><td rowspan="3" colspan="2">能见度范围/km</td><td colspan="2"><br />均值/km</td><td colspan="2">标准差/km</td><td rowspan="3">平均误差/km</td><td rowspan="3">正确率/%</td></tr><tr><td colspan="2"><br /></td><td colspan="2"></td></tr><tr></tr><tr><td>0.0</td><td>～0.5</td><td>208.01</td><td>200.46</td><td>136.00</td><td>134.85</td><td>14.75</td><td>78.36</td></tr><tr><td><br />0.5</td><td>～2.0</td><td>1 338.16</td><td>1 419.57</td><td>312.03</td><td>349.82</td><td>59.35</td><td>94.31</td></tr><tr><td><br />2.0</td><td>～10.0</td><td>6 561.18</td><td>6 728.04</td><td>2 719.40</td><td>2 600.88</td><td>250.40</td><td>92.06</td></tr><tr><td><br /></td><td>≥10.0</td><td>13 442.14</td><td>12 709.43</td><td>2 594.73</td><td>2 507.75</td><td>622.93</td><td>89.62</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="101">为验证本文权重融合策略的有效性,将随机选取子区域、图像整体缩放、估计值平均等方法与本文权重融合方法进行对比。其中,图像整体缩放是将测试图像按模型输入分辨率进行整体缩放后输入,估计值平均是将所有子区域估计值取平均作为检测值,实验结果对比如表3所示。从表3可以看出,随机选取子区域的正确率最差。图像整体缩放的正确率较低,主要是由于该方法图像部分区域内容单一,对检测结果有影响。各子区域估计值平均方法的效果较好,其在一定程度上避免了部分区域估计误差过大而影响整体检测效果,但当能见度较低时,检测误差较大,且稳定性低。本文权重融合策略能够有效避免图像局部对整体检测效果的影响,在各能见度范围内的正确率均达到较高水平,且稳定性较好。</p>
                </div>
                <div class="area_img" id="102">
                    <p class="img_tit"><b>表3 4种融合策略的检测正确率</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="102" border="1"><tr><td rowspan="2" colspan="2"><br />能见度<br />范围/km</td><td colspan="4"><br />正确率/%</td></tr><tr><td><br />随机选取<br />子区域方法</td><td>整体缩放<br />方法</td><td>估计值平均<br />方法</td><td>权重融合<br />方法</td></tr><tr><td>0.0</td><td>～0.5</td><td>20.26</td><td>45.58</td><td>52.64</td><td>78.36</td></tr><tr><td><br />0.5</td><td>～2.0</td><td>41.64</td><td>53.10</td><td>74.41</td><td>94.31</td></tr><tr><td><br />2.0</td><td>～10.0</td><td>61.17</td><td>64.03</td><td>78.35</td><td>92.06</td></tr><tr><td><br /></td><td>≥10.0</td><td>68.30</td><td>63.02</td><td>87.74</td><td>89.62</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="103">将本文方法与LPM、AS、CL和IF等方法进行对比,其中LPM和AS为模型驱动方法,CL和IF为数据驱动方法,对比结果如表4所示。由表4可知,数据驱动方法的实验结果总体上优于模型驱动法,但在能见度较低时,LPM方法表现较好。这主要是因为大气中不同粒子对光的折射和散射不同,LPM方法和AS方法计算的大气透射率存在误差,根据单一物理模型估计能见度,难以准确反映不同粒子对能见度的影响,同时CL方法和IF方法因训练样本数量多且图像特征明显,检测效果较好。当能见度较低时,CL方法和IF方法检测效果较差,这是由于训练样本数量少,模型训练不充分而产生过拟合现象。此外,LPM利用暗原色先验理论计算透射率,由于该理论对于大雾图像的去雾效果较好,因此在低能见度区间,LPM的检测正确率较高。本文方法在各能见度范围内的检测正确率均优于其他方法,其原因是该方法利用同构空间下的特征迁移,虽然能见度样本较少,但其仍可通过预训练的深度神经网络提取到有效的抽象特征,以解决低能见度样本检测效果差的问题。同时,本文提出的区域融合策略使整体正确率有了较大提升,达到91.67%。综上,通过对比5种检测方法可以发现,本文方法优于传统的模型驱动方法和数据驱动方法,更适用于能见度检测的应用。</p>
                </div>
                <div class="area_img" id="104">
                    <p class="img_tit"><b>表4 5种方法检测正确率的对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="104" border="1"><tr><td rowspan="2" colspan="2"><br />能见度范围<br />/km</td><td colspan="5"><br />正确率/%</td></tr><tr><td>LPM<br />方法</td><td>AS<br />方法</td><td>CL<br />方法</td><td>IF<br />方法</td><td>本文<br />方法</td></tr><tr><td>0.0</td><td>～0.5</td><td>76.12</td><td>70.15</td><td>68.75</td><td>73.13</td><td>78.36</td></tr><tr><td><br />0.5</td><td>～2.0</td><td>80.47</td><td>78.79</td><td>74.26</td><td>85.39</td><td>94.31</td></tr><tr><td><br />2.0</td><td>～10.0</td><td>81.46</td><td>78.70</td><td>78.21</td><td>82.92</td><td>92.06</td></tr><tr><td><br /></td><td>≥10.0</td><td>77.36</td><td>76.41</td><td>83.50</td><td>82.08</td><td>89.62</td></tr><tr><td colspan="2">总体正确率</td><td>80.51</td><td>78.20</td><td>79.13</td><td>83.60</td><td>91.67</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h3 id="105" name="105" class="anchor-tag">6 结束语</h3>
                <div class="p1">
                    <p id="106">本文提出一种基于迁移学习的能见度检测方法,利用迁移的VGG-16网络提取子区域图像特征,训练能见度与特征向量的支持向量回归机并计算融合权重,按权重融合各子区域的能见度估计值。实验结果表明,该方法具有较高的检测正确率和稳定性,可有效避免图像局部对整体检测效果的影响。该方法无须定义精确的影响因子和物理模型,同时解决了难以构建大规模能见度标记数据集的问题,可降低计算复杂度和模型训练时间。下一步将利用检测结果对深度神经网络进行微调,以训练更适合能见度检测的图像特征编码模型。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="166">
                            <a id="bibliography_1" >
                                    <b>[1]</b>
                                 中国气象局.中华人民共和国气象行业标准:能见度等级和预报:QX/T 114-2010[S].北京:气象出版社,2010:1-3.
                            </a>
                        </p>
                        <p id="168">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XDDJ201215025&amp;v=MTAwMDJDVVJMT2VaZVJyRnk3bFZMN05QU25QWkxHNEg5UE5xbzlIWVlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnI=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtOc0tUWDRGRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> 苗苗.视频能见度检测算法综述[J].现代电子技术,2012,35(15):72-75.
                            </a>
                        </p>
                        <p id="170">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CBBD&amp;filename=9787502948009001&amp;v=MDg3MDRYRnF6R2JhNEhOaklwNDlGYmVzUERSTTh6eFVTbURkOVNIN24zeEU5ZmJ2bktyaWZadTl1RkN2Z1U3bklKMThS&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtOc0tUWDRGRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> 孙学金.大气探测学[M].北京:气象出版社,2009.
                            </a>
                        </p>
                        <p id="172">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZDYB201510010&amp;v=MjkyMzdJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSckZ5N2xWTDdOUHluU2JMRzRIOVROcjQ5RVo=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtOc0tUWDRGRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b> 李志乾,张志伟,成文,等.海上能见度观测研究进展[J].自动化仪表,2015,36(10):33-36.
                            </a>
                        </p>
                        <p id="174">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Estimating meteorological visibility using cameras:a probabilistic model-driven approach">

                                <b>[5]</b> HAUTIÉRE N,BABARI R,DUMONT É,et al.Estimating meteorological visibility using cameras:a probabilistic model-driven approach[C]//Proceedings of Asian Conference on Computer Vision.Berlin,Germany:Springer,2010:243-254.
                            </a>
                        </p>
                        <p id="176">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJX201506007&amp;v=MjgwNDBESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSckZ5N2xWTDdOTHo3QmRyRzRIOVRNcVk5Rlk0UUs=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtOc0tUWDRGRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b> 宋洪军,郜园园,陈阳舟.基于摄像机动态标定的交通能见度估计[J].计算机学报,2015,38(6):1172-1187.
                            </a>
                        </p>
                        <p id="178">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GLJK201704018&amp;v=MjczNjc0SDliTXE0OUViSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUnJGeTdsVkw3TklpSEJaYkc=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtOc0tUWDRGRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b> 胡平,杨旭东.高速公路能见度快速检测算法[J].公路交通科技,2017,34(4):115-122.
                            </a>
                        </p>
                        <p id="180">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A new model for daytime visibility index estimation fused average sobel gradient and dark channel ratio">

                                <b>[8]</b> XIANG Wenshu,XIAO Jianli,WANG Chongjing,et al.A new model for daytime visibility index estimation fused average sobel gradient and dark channel ratio[C]//Proceedings of International Conference on Computer Science and Network Technology.Washington D.C.,USA:IEEE Press,2014:109-112.
                            </a>
                        </p>
                        <p id="182">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=YQXB201005034&amp;v=MTkyNTVsVkw3TlBEelRiTEc0SDlITXFvOUdZSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUnJGeTc=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtOc0tUWDRGRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b> 安明伟,陈启美,郭宗良.基于路况视频的气象能见度检测方法与系统设计[J].仪器仪表学报,2010,31(5):1148-1153.
                            </a>
                        </p>
                        <p id="184">
                            <a id="bibliography_10" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201306006&amp;v=MjgwODRiTEc0SDlMTXFZOUZZb1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUnJGeTdsVkw3TktEN1k=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtOc0tUWDRGRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[10]</b> 许茜,殷绪成,李岩,等.基于图像理解的能见度测量方法[J].模式识别与人工智能,2013,26(6):543-551.
                            </a>
                        </p>
                        <p id="186">
                            <a id="bibliography_11" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=SKQB201702046&amp;v=MDU2MDM0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSckZ5N2xWTDdOTmliYWJMRzRIOWJNclk5QllvUUtESDg=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtOc0tUWDRGRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[11]</b> 钟丽,吴关胜,谢斌,等.基于图像分析的航道能见度评估算法研究[J].交通科技,2017(2):151-154.
                            </a>
                        </p>
                        <p id="188">
                            <a id="bibliography_12" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=KJTB201708010&amp;v=MDgzMzg3TkxpZmZiTEc0SDliTXA0OUVaSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUnJGeTdsVkw=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtOc0tUWDRGRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[12]</b> 花毓幸,曾燕,邱新法.基于图像兴趣窗格测算大气能见度的方法研究[J].科技通报,2017,33(8):39-42.
                            </a>
                        </p>
                        <p id="190">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Very deep convolutional networks for large-scale image recognition">

                                <b>[13]</b> SIMONYAN K,ZISSERMAN A.Very deep convolutional networks for large-scale image recognition[EB/OL].[2018-06-01].https://www.arxiv-vanity.com/papers/1409.1556/.
                            </a>
                        </p>
                        <p id="192">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep Neural Networks for No-Reference and Full-Reference Image Quality Assessment">

                                <b>[14]</b> BOSSE S,MANIRY D,MULLER K R,et al.Deep neural networks for no-reference and full-reference image quality assessment[J].IEEE Transactions on Image Processing,2018,27(1):206-219.
                            </a>
                        </p>
                        <p id="194">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Hallucinated-IQA:no-reference image quality assessment via adversarial learning">

                                <b>[15]</b> LIN Kwan Yee,WANG Guangxiang.Hallucinated-IQA:no-reference image quality assessment via adversarial learning[EB/OL].[2018-06-01].https://arxiv.org/pdf/1804.01681.pdf.
                            </a>
                        </p>
                        <p id="196">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Support vector regression machines">

                                <b>[16]</b> DRUCKER H,BURGES C J C,KAUFMAN L,et al.Support vector regression machines[EB/OL].[2018-06-01].http://papers.nips.cc/paper/1238-support-vector-regression-machines.pdf.
                            </a>
                        </p>
                        <p id="198">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A tutorial on support vector regression">

                                <b>[17]</b> SMOLA A J.A tutorial on support vector regression[M].[S.l.]:Kluwer Academic Publishers,2004.
                            </a>
                        </p>
                        <p id="200">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Insights into the geometry of the gaussian kernel and an application in geometric modeling">

                                <b>[18]</b> EIGENSATZ M,PAULY M.Insights into the geometry of the gaussian kernel and an application in geometric modeling[EB/OL].[2018-06-01].https://lgg.epfl.ch/publications/2006/eigensatz_2006_IGG.pdf.
                            </a>
                        </p>
                        <p id="202">
                            <a id="bibliography_19" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00001340326&amp;v=MDgyNTlCYXJPNEh0SE5ySXRGWitrSlkzazV6QmRoNGo5OVNYcVJyeG94Y01IN1I3cWVidWR0RkMzbFZiL1BJVnM9Tmo3&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtOc0tUWDRGRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[19]</b> GAO Junbin,GUNN S R,HARRIS C J,et al.A probabilistic framework for SVM regression and error bar estimation[J].Machine Learning,2002,46(1/2/3):71-89.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJC201909038" />
        <input id="dpi" type="hidden" value="600" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtOc0tUWDRGRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201909038&amp;v=MDMxNzBSckZ5N2xWTDdOTHo3QmJiRzRIOWpNcG85R2JJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmU=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2trN0Q3YmtOc0tUWDRGRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
