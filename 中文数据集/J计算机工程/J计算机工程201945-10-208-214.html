<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637126203033208750%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJC201910035%26RESULT%3d1%26SIGN%3d%252bmlUNXu7P8O6PB6vMdnDTEqxQ5g%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201910035&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201910035&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201910035&amp;v=MjM3OTVxcUJ0R0ZyQ1VSTE9lWmVSdEZ5M2hVTDdOTHo3QmJiRzRIOWpOcjQ5R1lZUUtESDg0dlI0VDZqNTRPM3o=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZyTnZtdlVYc3IzaEdiaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#43" data-title="0 概述 ">0 概述</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#47" data-title="1 神经网络与文本分类 ">1 神经网络与文本分类</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#48" data-title="1.1 神经网络">1.1 神经网络</a></li>
                                                <li><a href="#55" data-title="1.2 基于LDA主题模型的文本分类">1.2 基于LDA主题模型的文本分类</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#76" data-title="2 &lt;i&gt;NLDA&lt;/i&gt;算法 ">2 <i>NLDA</i>算法</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#77" data-title="2.1 文本分类">2.1 文本分类</a></li>
                                                <li><a href="#80" data-title="2.2 NLDA算法模型">2.2 NLDA算法模型</a></li>
                                                <li><a href="#90" data-title="2.3 NLDA算法的实现">2.3 NLDA算法的实现</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#134" data-title="3 实验结果与分析 ">3 实验结果与分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#135" data-title="3.1 实验数据与环境">3.1 实验数据与环境</a></li>
                                                <li><a href="#143" data-title="3.2 结果分析">3.2 结果分析</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#160" data-title="4 结束语 ">4 结束语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#50" data-title="&lt;b&gt;图1 神经网络结构&lt;/b&gt;"><b>图1 神经网络结构</b></a></li>
                                                <li><a href="#57" data-title="&lt;b&gt;图2 LDA模型&lt;/b&gt;"><b>图2 LDA模型</b></a></li>
                                                <li><a href="#58" data-title="&lt;b&gt;表1 LDA模型中各符号含义&lt;/b&gt;"><b>表1 LDA模型中各符号含义</b></a></li>
                                                <li><a href="#82" data-title="&lt;b&gt;图3 NLDA分类模型的结构&lt;/b&gt;"><b>图3 NLDA分类模型的结构</b></a></li>
                                                <li><a href="#85" data-title="&lt;b&gt;图4 NLDA拟合参数部分的神经网络结构&lt;/b&gt;"><b>图4 NLDA拟合参数部分的神经网络结构</b></a></li>
                                                <li><a href="#88" data-title="&lt;b&gt;图5 输入层与隐藏层神经元间的权重矩阵训练模型&lt;/b&gt;"><b>图5 输入层与隐藏层神经元间的权重矩阵训练模型</b></a></li>
                                                <li><a href="#89" data-title="&lt;b&gt;图6 隐藏层与输出层神经元间的矩阵&lt;/b&gt;&lt;i&gt;&lt;b&gt;V&lt;/b&gt;&lt;/i&gt;&lt;sub&gt;&lt;i&gt;t&lt;/i&gt;&lt;/sub&gt;(&lt;i&gt;p&lt;/i&gt;)&lt;b&gt;的训练模型&lt;/b&gt;"><b>图6 隐藏层与输出层神经元间的矩阵</b><i><b>V</b></i><sub><i>t</i></sub>(<i>p</i>)<b>的训练模型</b></a></li>
                                                <li><a href="#138" data-title="&lt;b&gt;表2 THUCNews语料库文本类别与数量分布&lt;/b&gt;"><b>表2 THUCNews语料库文本类别与数量分布</b></a></li>
                                                <li><a href="#140" data-title="&lt;b&gt;表3 复旦大学语料库类别与数量分布&lt;/b&gt;"><b>表3 复旦大学语料库类别与数量分布</b></a></li>
                                                <li><a href="#142" data-title="&lt;b&gt;表4 实验环境与配置&lt;/b&gt;"><b>表4 实验环境与配置</b></a></li>
                                                <li><a href="#145" data-title="&lt;b&gt;表5 混合矩阵&lt;/b&gt;"><b>表5 混合矩阵</b></a></li>
                                                <li><a href="#149" data-title="&lt;b&gt;表6 THUCNews语料库上4种算法的准确率对比&lt;/b&gt; %"><b>表6 THUCNews语料库上4种算法的准确率对比</b> %</a></li>
                                                <li><a href="#150" data-title="&lt;b&gt;表7 复旦大学语料库上4种算法的准确率对比&lt;/b&gt; %"><b>表7 复旦大学语料库上4种算法的准确率对比</b> %</a></li>
                                                <li><a href="#153" data-title="&lt;b&gt;图7 THUCNews语料库上4种算法的准确率对比&lt;/b&gt;"><b>图7 THUCNews语料库上4种算法的准确率对比</b></a></li>
                                                <li><a href="#154" data-title="&lt;b&gt;图8 复旦大学语料库上4种算法的准确率对比&lt;/b&gt;"><b>图8 复旦大学语料库上4种算法的准确率对比</b></a></li>
                                                <li><a href="#156" data-title="&lt;b&gt;表8 2种算法在不同数据集上的时间开销对比&lt;/b&gt; min"><b>表8 2种算法在不同数据集上的时间开销对比</b> min</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" title=" HU Minqing,LIU Bing.Mining and summarizing customer reviews[C]//Proceedings of the 10th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.New York,USA:ACM Press,2004:168-177." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Mining and summarizing customer reviews">
                                        <b>[1]</b>
                                         HU Minqing,LIU Bing.Mining and summarizing customer reviews[C]//Proceedings of the 10th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.New York,USA:ACM Press,2004:168-177.
                                    </a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" title=" PARK E K,RA D Y,JANG M G.Techniques for improving Web retrieval effectiveness[J].Information Processing Management,2005,41(5):1207-1223." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012100870114&amp;v=MDU3NjN1SHlqbVVMeklKVm9YYXhjPU5pZk9mYks3SHRET3JvOUZiT3dQRFgwOW9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWg==&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZyTnZtdlVYc3IzaEdiaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         PARK E K,RA D Y,JANG M G.Techniques for improving Web retrieval effectiveness[J].Information Processing Management,2005,41(5):1207-1223.
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title=" BLEI D M,NG A Y,JORDAN M I.Latent Dirichlet allocation[J].The Journal of Machine Learning Research,2003,3(4/5):993-1022." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Latent Dirichlet allocation">
                                        <b>[3]</b>
                                         BLEI D M,NG A Y,JORDAN M I.Latent Dirichlet allocation[J].The Journal of Machine Learning Research,2003,3(4/5):993-1022.
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" title=" GOUDJIL M,KOUDIL M,BEDDA M,et al.Anovel active learning method using SVM for text classification[J].International Journal of Automation and Computing,2018,15(3):290-298." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JDYS201803004&amp;v=Mjk4NzlOTHluU2ZiRzRIOW5Nckk5RllJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSdEZ5M2hVTDc=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZyTnZtdlVYc3IzaEdiaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                         GOUDJIL M,KOUDIL M,BEDDA M,et al.Anovel active learning method using SVM for text classification[J].International Journal of Automation and Computing,2018,15(3):290-298.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" >
                                        <b>[5]</b>
                                     LECUN Y,BENGIO Y,HINTON G.Deep learning[J].Nature,2015,521(7553):436-444.</a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" title=" KALCHBRENNER N,GREFENSTETTE E,BLUNSOM P.A convolutional neural network formodelling sentences[EB/OL].[2019-02-20].http://de.arxiv.org/pdf/1404.2188." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A convolutional neural network formodelling sentences">
                                        <b>[6]</b>
                                         KALCHBRENNER N,GREFENSTETTE E,BLUNSOM P.A convolutional neural network formodelling sentences[EB/OL].[2019-02-20].http://de.arxiv.org/pdf/1404.2188.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" title=" SALTON G,WONG A,YANG C S.A vector space model for automatic indexing[J].Communications of the ACM,1975,18(11):613-620." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000024403&amp;v=MjQxNTdPa0xDSHc2b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VTHpJSlZvWGF4Yz1OaWZJWTdLN0h0ak5yNDlGWg==&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZyTnZtdlVYc3IzaEdiaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                         SALTON G,WONG A,YANG C S.A vector space model for automatic indexing[J].Communications of the ACM,1975,18(11):613-620.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" title=" PHAN X H,NGUYEN M L,HORIGUCHI S.Learning to classify short and sparse text &amp;amp; Web with hidden topics from largescale data collections[C]//Proceedings of the 17th Conference on World Wide Web.New York,USA:ACM Press,2008:91-100." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning to classify short and sparse text&amp;amp;web with hidden topics from large-scale data collections">
                                        <b>[8]</b>
                                         PHAN X H,NGUYEN M L,HORIGUCHI S.Learning to classify short and sparse text &amp;amp; Web with hidden topics from largescale data collections[C]//Proceedings of the 17th Conference on World Wide Web.New York,USA:ACM Press,2008:91-100.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" title=" WANG Le,JIA Yan,HAN Weihong.Instant message clustering based on extended vector space model[C]// Proceedings of the 2nd International Conference on Advances in Computation and Intelligence.Berlin,Germany:Springer,2007:435-443." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Instant Message Clustering Based on Extended Vector Space Model">
                                        <b>[9]</b>
                                         WANG Le,JIA Yan,HAN Weihong.Instant message clustering based on extended vector space model[C]// Proceedings of the 2nd International Conference on Advances in Computation and Intelligence.Berlin,Germany:Springer,2007:435-443.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_10" title=" BOUAZIZ A,DARTIGUES-PALLEZ C,PEREIRA C D C,et al.Short text classification using semantic random forest[C]//Proceedings of International Conference on Data Warehousing and Knowledge Discovery.Berlin,Germany:Springer,2014:288-299." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Short Text Classification Using Semantic Random Forest">
                                        <b>[10]</b>
                                         BOUAZIZ A,DARTIGUES-PALLEZ C,PEREIRA C D C,et al.Short text classification using semantic random forest[C]//Proceedings of International Conference on Data Warehousing and Knowledge Discovery.Berlin,Germany:Springer,2014:288-299.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_11" title=" GUO Hongchen,LIANG Qiliang,LI Zhiqiang.An improved AD-LDA topic model based on weighted Gibbs sampling[C]//Proceedings of IEEE Advanced Information Management,Communicates,Electronic and Automation Control Conference.Washington D.C.,USA:IEEE Press,2016:1978-1982." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=An improved AD-LDA topic model based on weighted Gibbs sampling">
                                        <b>[11]</b>
                                         GUO Hongchen,LIANG Qiliang,LI Zhiqiang.An improved AD-LDA topic model based on weighted Gibbs sampling[C]//Proceedings of IEEE Advanced Information Management,Communicates,Electronic and Automation Control Conference.Washington D.C.,USA:IEEE Press,2016:1978-1982.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_12" title=" 张志飞,苗夺谦,高灿.基于LDA主题模型的短文本分类方法[J].计算机应用,2013,33(6):1587-1590." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201306027&amp;v=MjYwNjg2ajU0TzN6cXFCdEdGckNVUkxPZVplUnRGeTNoVUw3Tkx6N0JkN0c0SDlMTXFZOUhZNFFLREg4NHZSNFQ=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZyTnZtdlVYc3IzaEdiaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[12]</b>
                                         张志飞,苗夺谦,高灿.基于LDA主题模型的短文本分类方法[J].计算机应用,2013,33(6):1587-1590.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_13" title=" MAO Qirong,DONG Ming,HUANG Zhengwei,et al.Learning salient features for speech emotion recognition using convolutional neural networks[J].IEEE Transactions on Multimedia,2014,16(8):2203-2213." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning Salient Features for Speech Emotion Recognition Using Convolutional Neural Networks">
                                        <b>[13]</b>
                                         MAO Qirong,DONG Ming,HUANG Zhengwei,et al.Learning salient features for speech emotion recognition using convolutional neural networks[J].IEEE Transactions on Multimedia,2014,16(8):2203-2213.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_14" title=" IOFFE S,SZEGEDY C.Batch normalization:accelerating deep network training by reducing internal covariate shift[C]//Proceedings of the 32nd International Conference on International Conference on Machine Learning.[S.l.]:JMLR.org,2015:448-456." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Batch normalization:Accelerating deep network training by reducing internal covariate shift">
                                        <b>[14]</b>
                                         IOFFE S,SZEGEDY C.Batch normalization:accelerating deep network training by reducing internal covariate shift[C]//Proceedings of the 32nd International Conference on International Conference on Machine Learning.[S.l.]:JMLR.org,2015:448-456.
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_15" title=" 奚雪峰,周国栋.面向自然语言处理的深度学习研究[J].自动化学报,2016,42(10):1445-1465." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201610001&amp;v=MjAxMzVxcUJ0R0ZyQ1VSTE9lWmVSdEZ5M2hVTDdOS0NMZlliRzRIOWZOcjQ5RlpZUUtESDg0dlI0VDZqNTRPM3o=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZyTnZtdlVYc3IzaEdiaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[15]</b>
                                         奚雪峰,周国栋.面向自然语言处理的深度学习研究[J].自动化学报,2016,42(10):1445-1465.
                                    </a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_16" title=" HE Kaiming,ZHANG Xiangyu,REN Shaoqing,et al.Deep residual learning for image recognition[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Press,2016:770-778." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep Residual Learning for Image Recognition">
                                        <b>[16]</b>
                                         HE Kaiming,ZHANG Xiangyu,REN Shaoqing,et al.Deep residual learning for image recognition[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Press,2016:770-778.
                                    </a>
                                </li>
                                <li id="35">


                                    <a id="bibliography_17" title=" 周志华.机器学习[M].北京:清华大学出版社,2016." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CBBD&amp;filename=9787302423287000&amp;v=Mjg5ODU3ck1JMThSWEZxekdiQzRITlhPckkxTlkrc1BEQk04enhVU21EZDlTSDduM3hFOWZidm5LcmlmWnU5dUZDdm1V&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZyTnZtdlVYc3IzaEdiaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[17]</b>
                                         周志华.机器学习[M].北京:清华大学出版社,2016.
                                    </a>
                                </li>
                                <li id="37">


                                    <a id="bibliography_18" title=" 刘泽锦.基于主题模型和卷积神经网络的短文本分类算法研究[D].北京:北京工业大学,2017." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=1018705086.nh&amp;v=MDg3ODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJ0RnkzaFVMN05WRjI2RnJTNEc5SEVxWkViUElRS0RIODQ=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZyTnZtdlVYc3IzaEdiaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[18]</b>
                                         刘泽锦.基于主题模型和卷积神经网络的短文本分类算法研究[D].北京:北京工业大学,2017.
                                    </a>
                                </li>
                                <li id="39">


                                    <a id="bibliography_19" title=" 王懿.基于自然语言处理和机器学习的文本分类及其应用研究[D].成都:中国科学院成都计算机应用研究所,2006." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=2006103241.nh&amp;v=Mjk5MDR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJ0RnkzaFVMN05WMTI3R0xLNEhkUElycEViUElRS0RIODQ=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZyTnZtdlVYc3IzaEdiaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[19]</b>
                                         王懿.基于自然语言处理和机器学习的文本分类及其应用研究[D].成都:中国科学院成都计算机应用研究所,2006.
                                    </a>
                                </li>
                                <li id="41">


                                    <a id="bibliography_20" title=" GRIFFITHS T L,STEYVERS M.Finding scientific topics[J].National Academy of Sciences,2004,101(S1):5228-5235." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Finding scientific topics">
                                        <b>[20]</b>
                                         GRIFFITHS T L,STEYVERS M.Finding scientific topics[J].National Academy of Sciences,2004,101(S1):5228-5235.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJC" target="_blank">计算机工程</a>
                2019,45(10),208-214 DOI:10.19678/j.issn.1000-3428.0054297            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>一种基于神经网络与LDA的文本分类算法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%89%9B%E7%A1%95%E7%A1%95&amp;code=42929494&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZyTnZtdlVYc3IzaEdiaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">牛硕硕</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9F%B4%E5%B0%8F%E4%B8%BD&amp;code=26546184&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZyTnZtdlVYc3IzaEdiaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">柴小丽</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%8E%E5%BE%B7%E5%90%AF&amp;code=42929495&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZyTnZtdlVYc3IzaEdiaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">李德启</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%B0%A2%E5%BD%AC&amp;code=33596265&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZyTnZtdlVYc3IzaEdiaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">谢彬</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E7%94%B5%E5%AD%90%E7%A7%91%E6%8A%80%E9%9B%86%E5%9B%A2%E5%85%AC%E5%8F%B8%E7%AC%AC%E4%B8%89%E5%8D%81%E4%BA%8C%E7%A0%94%E7%A9%B6%E6%89%80&amp;code=0131038&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZyTnZtdlVYc3IzaEdiaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国电子科技集团公司第三十二研究所</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>传统隐含狄利克雷分配(LDA)主题模型在文本分类计算时利用Gibbs Sampling拟合已知条件分布下的未知参数,较难权衡分类准确率与计算复杂度间的关系。为此,在LDA主题模型的基础上,利用神经网络拟合单词-主题概率分布,提出一种文本分类算法NLDA。在THUCNews语料库和复旦大学语料库上进行实验,结果表明,与传统LDA模型相比,该算法的平均分类准确率分别提升5.53%和4.67%,平均训练时间分别减少8%和10%。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZyTnZtdlVYc3IzaEdiaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">文本分类;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZyTnZtdlVYc3IzaEdiaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZyTnZtdlVYc3IzaEdiaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%9A%90%E5%90%AB%E7%8B%84%E5%88%A9%E5%85%8B%E9%9B%B7%E5%88%86%E9%85%8D&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZyTnZtdlVYc3IzaEdiaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">隐含狄利克雷分配;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZyTnZtdlVYc3IzaEdiaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">主题模型;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    牛硕硕(1993—),男,硕士研究生,主研方向为机器学习、自然语言处理;E-mail:ns930327@163.com;
                                </span>
                                <span>
                                    柴小丽,研究员;;
                                </span>
                                <span>
                                    李德启,工程师;;
                                </span>
                                <span>
                                    谢彬,研究员、博士。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-03-20</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家部委基金;</span>
                    </p>
            </div>
                    <h1><b>A Text Classification Algorithm Based on Neural Network and LDA</b></h1>
                    <h2>
                    <span>NIU Shuoshuo</span>
                    <span>CHAI Xiaoli</span>
                    <span>LI Deqi</span>
                    <span>XIE Bin</span>
            </h2>
                    <h2>
                    <span>The 32nd Research Institute of China Electronics Technology Group Corporation</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>The traditional Latent Dirichlet Allocation(LDA) topic model uses Gibbs Sampling to fit unknown parameters under known conditional distributions in text classification calculations,making it difficult to weigh classification accuracy and computation complexity.Therefore,based on the LDA topic model,a neural network is used to fit the word-topic probability distribution,and a text classification algorithm NLDA is proposed.Experiments on the THUCNews corpus and Fudan University corpus show that compared with the traditional LDA model,the average classification accuracy of the algorithm is increased by 5.53% and 4.67% respectively,and the average training time is reduced by 8% and 10%.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=text%20classification&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZyTnZtdlVYc3IzaEdiaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">text classification;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=deep%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZyTnZtdlVYc3IzaEdiaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">deep learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=neural%20network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZyTnZtdlVYc3IzaEdiaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">neural network;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Latent%20Dirichlet%20Allocation(LDA)&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZyTnZtdlVYc3IzaEdiaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Latent Dirichlet Allocation(LDA);</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=topic%20model&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZyTnZtdlVYc3IzaEdiaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">topic model;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2019-03-20</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="43" name="43" class="anchor-tag">0 概述</h3>
                <div class="p1">
                    <p id="44">近年来,文本分类算法成为处理海量数据的关键技术之一<citation id="169" type="reference"><link href="3" rel="bibliography" /><link href="5" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>]</sup></citation>。当前,在军事领域,传统的数据分析工具处理海量数据的效率较低,如何高效实时地提取军事数据是亟待解决的难题。由于军事数据具有体量大、类型复杂、来源多样等特征,因此使用文本分类算法研究军事数据,对军事信息存储、管理及情报搜集等具有重要意义。</p>
                </div>
                <div class="p1">
                    <p id="45">在中文文本分类中,国内外学者提出多种方法,如隐含狄利克雷分配(Latent Dirichlet Allocation,LDA)主题模型<citation id="170" type="reference"><link href="7" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>、支持向量机(Support Vector Machine,SVM)算法<citation id="171" type="reference"><link href="9" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>、深度学习<citation id="172" type="reference"><link href="11" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>、神经网络<citation id="173" type="reference"><link href="13" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>等。如何利用现有的文本数据集构建一个泛化能力较强的文本分类器,并利用此分类器将待预测的文本数据进行类别判断是研究的热点问题。文献<citation id="174" type="reference">[<a class="sup">7</a>]</citation>利用向量空间模型(Vector Space Model,VSM)进行文本分类,实现文本自动索引。由于文本的特征极度稀疏,且输入的文本向量维度较高,在一定程度上导致VSM分类效果较差<citation id="178" type="reference"><link href="17" rel="bibliography" /><link href="19" rel="bibliography" /><sup>[<a class="sup">8</a>,<a class="sup">9</a>]</sup></citation>。文献<citation id="175" type="reference">[<a class="sup">10</a>]</citation>利用LDA主题模型学习维基百科数据中文本的主题分布和主题下词语的分布,用同主题下的高频词语对短文本进行特征扩展。文献<citation id="176" type="reference">[<a class="sup">11</a>]</citation>提出一种基于Gibbs Sampling的改进LDA主题模型,其利用Gibbs Sampling拟合已知条件分布下的未知参数,但该模型的准确率和复杂度难以权衡。文献<citation id="177" type="reference">[<a class="sup">12</a>]</citation>为解决短文本的特征稀疏性和上下文依赖性问题,提出一种基于LDA模型的短文本分类方法,但是由于中文表述口语化和不规范的问题,该方法分类准确率较差。</p>
                </div>
                <div class="p1">
                    <p id="46">基于上述研究,本文提出一种基于神经网络的隐含狄利克雷分配文本分类(Neural network-based Latent Dirichlet Allocation,NLDA)算法。该算法将神经网络应用在LDA拟合参数的计算过程中,通过拟合单词-主题概率分布,得到文本的分类结果。</p>
                </div>
                <h3 id="47" name="47" class="anchor-tag">1 神经网络与文本分类</h3>
                <h4 class="anchor-tag" id="48" name="48">1.1 神经网络</h4>
                <div class="p1">
                    <p id="49">神经网络<citation id="180" type="reference"><link href="27" rel="bibliography" /><link href="29" rel="bibliography" /><sup>[<a class="sup">13</a>,<a class="sup">14</a>]</sup></citation>被广泛应用于计算机视觉、语音识别和文本分类等领域<citation id="181" type="reference"><link href="31" rel="bibliography" /><link href="33" rel="bibliography" /><sup>[<a class="sup">15</a>,<a class="sup">16</a>]</sup></citation>。以BP神经网络为例,该网络是一种多层的前馈神经网络,其基本单元是神经元。一个神经网络包括输入层、隐藏层和输出层<citation id="179" type="reference"><link href="35" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>。输入层神经元个数与输入数据的维数相同,输出层神经元个数与需要拟合的数据个数相同,隐藏层神经元个数与层数根据规则和目标来设定。图1所示为神经网络的结构。</p>
                </div>
                <div class="area_img" id="50">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201910035_050.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZyTnZtdlVYc3IzaEdiaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 神经网络结构" src="Detail/GetImg?filename=images/JSJC201910035_050.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZyTnZtdlVYc3IzaEdiaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图1 神经网络结构</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201910035_050.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZyTnZtdlVYc3IzaEdiaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="51">神经网络的输入是<i>x</i><sub>1</sub>,<i>x</i><sub>2</sub>,…,<i>x</i><sub><i>n</i></sub>,各层间连接权重为<i>w</i><sub><i>i</i></sub>,经过神经网络的传播,其输出可表示为:</p>
                </div>
                <div class="p1">
                    <p id="52" class="code-formula">
                        <mathml id="52"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>y</mi><mo>=</mo><mi>f</mi><mo stretchy="false">(</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mi>w</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mo>×</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="53">当神经网络正向传播时,信号从输入层到隐藏层再到输出层。当逆向传播时,误差从输出层到隐藏层最后到输入层,依次调节隐藏层到输出层的权重和偏置、输入层到隐藏层的权重和偏置,直至收敛,训练出最优的模型。</p>
                </div>
                <div class="p1">
                    <p id="54">神经网络具有较强的非线性拟合能力,可映射任意复杂的非线性关系,同时学习规则简单,学习能力强,适合求解内部机制复杂的问题<citation id="182" type="reference"><link href="37" rel="bibliography" /><link href="39" rel="bibliography" /><sup>[<a class="sup">18</a>,<a class="sup">19</a>]</sup></citation>,因此,常被用于复杂的参数拟合。</p>
                </div>
                <h4 class="anchor-tag" id="55" name="55">1.2 基于LDA主题模型的文本分类</h4>
                <div class="p1">
                    <p id="56">LDA主题模型是一个“文本-主题-单词”的3层贝叶斯产生式模型<citation id="183" type="reference"><link href="7" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>,每篇文本表示主题的混合分布,每个主题则是词上的概率分布。LDA模型假设一篇文章如果属于某个主题,那么这篇文章所有的单词都与该主题有关。在该模型中,文档以主题的形式表示,通过主题之间的内在语义关系展现文章之间的关联。LDA模型如图2所示,其中,各符号的含义如表1所示。</p>
                </div>
                <div class="area_img" id="57">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201910035_057.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZyTnZtdlVYc3IzaEdiaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 LDA模型" src="Detail/GetImg?filename=images/JSJC201910035_057.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZyTnZtdlVYc3IzaEdiaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图2 LDA模型</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201910035_057.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZyTnZtdlVYc3IzaEdiaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="58">
                    <p class="img_tit"><b>表1 LDA模型中各符号含义</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="58" border="1"><tr><td>符号</td><td>含义</td><td>符号</td><td>含义</td></tr><tr><td colspan="2"><br /></td><td colspan="2"></td></tr><tr><td rowspan="2"><i>α</i></td><td rowspan="2"><i>θ</i>的超参数</td><td rowspan="2"><i>w</i></td><td rowspan="2">词</td></tr><tr></tr><tr><td><br /><i>β</i></td><td><i>φ</i>的超参数</td><td><i>M</i></td><td>文章数</td></tr><tr><td><br /><i>θ</i></td><td>文章-主题概率分布</td><td><i>v</i></td><td>词数</td></tr><tr><td><br /><i>φ</i></td><td>单词-主题概率分布</td><td><i>T</i></td><td>主题数</td></tr><tr><td><br /><i>z</i></td><td>词的主题分配</td><td></td><td></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="59">单词是中文文本的基本元素,用{1,2,…,<i>V</i>}索引词表中的分项。词表中的第<i>v</i>个单词用一个<i>V</i>维向量<i><b>w</b></i>表示,其中,对于任意<i>u</i>≠<i>v</i>,有<i>w</i><sub><i>v</i></sub>=1、<i>w</i><sub><i>u</i></sub>=1。文章是由<i>N</i>个单词构成的序列,用<i>d</i>={<i>w</i><sub>1</sub>,<i>w</i><sub>2</sub>,…,<i>w</i><sub><i>n</i></sub>}表示,<i>w</i><sub><i>n</i></sub>是该序列中的第<i>n</i>单词。所有<i>M</i>篇文章组成集合<i>D</i>,<i>D</i>={<i>d</i><sub>1</sub>,<i>d</i><sub>2</sub>,…,<i>d</i><sub><i>M</i></sub>}。</p>
                </div>
                <div class="p1">
                    <p id="60">假设有<i>T</i>个主题,那么文章<i>d</i>中第<i>i</i>个单词<i>w</i><sub><i>i</i></sub>的概率可表示为:</p>
                </div>
                <div class="p1">
                    <p id="61" class="code-formula">
                        <mathml id="61"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><mo stretchy="false">(</mo><mi>w</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>Τ</mi></munderover><mi>Ρ</mi></mstyle><mo stretchy="false">(</mo><mi>w</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">|</mo><mi>z</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mi>j</mi><mo stretchy="false">)</mo><mi>Ρ</mi><mo stretchy="false">(</mo><mi>z</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mi>j</mi><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="62">其中,<i>z</i><sub><i>i</i></sub>是隐藏的变量,表示第<i>i</i>个单词<i>w</i><sub><i>i</i></sub>取自该主题,<i>P</i>(<i>w</i><sub><i>i</i></sub>|<i>z</i><sub><i>i</i></sub>=<i>j</i>)是单词<i>w</i><sub><i>i</i></sub>属于主题<i>j</i>的概率,<i>P</i>(<i>z</i><sub><i>i</i></sub>=<i>j</i>)是文章<i>d</i>属于主题<i>j</i>的概率。文章<i>d</i>中出现单词<i><b>w</b></i>的概率可表示为:</p>
                </div>
                <div class="p1">
                    <p id="63" class="code-formula">
                        <mathml id="63"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">w</mi><mo stretchy="false">|</mo><mi>d</mi><mo stretchy="false">)</mo><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Μ</mi></munderover><mi>φ</mi></mstyle><msubsup><mrow></mrow><mi>w</mi><mi>j</mi></msubsup><mo>×</mo><mi>θ</mi><msubsup><mrow></mrow><mi>j</mi><mi>d</mi></msubsup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>3</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="64">其中,<i>φ</i><mathml id="162"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>w</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mi>j</mi></msubsup></mrow></math></mathml>=<i>P</i>(<i>w</i><sub><i>i</i></sub>|<i>z</i><sub><i>i</i></sub>=<i>j</i>)是将第<i>j</i>个主题表示为词汇表中<i>V</i>个单词的多项式分布,<i>θ</i><mathml id="163"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mi>d</mi></msubsup></mrow></math></mathml>=<i>P</i>(<i>z</i><sub><i>i</i></sub>=<i>j</i>)是将文本表示成<i>T</i>个隐藏主题上的随机混合。</p>
                </div>
                <div class="p1">
                    <p id="65">LDA主题模型可表示为:</p>
                </div>
                <div class="p1">
                    <p id="66"><i>P</i>(<i><b>w</b></i>|<i>d</i>)=<i>P</i>(<i><b>w</b></i>|<i>T</i>)<i>P</i>(<i>T</i>|<i>d</i>)      (4)</p>
                </div>
                <div class="p1">
                    <p id="67">初始模型对文本-主题概率分布<i>θ</i>引入一个超参数<i>α</i>,使其服从狄利克雷分布。文献<citation id="184" type="reference">[<a class="sup">20</a>]</citation>对单词-主题概率分布<i>φ</i>引入一个超参数<i>β</i>,使其服从狄利克雷分布。2个超参数一般设置为<i>α</i> = 50/<i>T</i>,<i>β</i> = 0.01。通过对变量<i>z</i>进行Gibbs Sampling估算参数<i>θ</i>、<i>φ</i>。</p>
                </div>
                <div class="p1">
                    <p id="68">Gibbs Sampling拟合<i>θ</i>、<i>φ</i>进行文本分类的具体过程如下:</p>
                </div>
                <div class="p1">
                    <p id="69">1)扫描文章,对每个词<i>w</i><sub><i>n</i></sub>随机赋予一个主题。</p>
                </div>
                <div class="p1">
                    <p id="70">2)初始化<i>z</i><sub><i>i</i></sub>,使其为1～<i>T</i>之间的某个整数。</p>
                </div>
                <div class="p1">
                    <p id="71">3)重新扫描每篇文章,采用LDA模型对语料库进行主题建模,参数推理利用Gibbs Sampling不断迭代,同时记录<i>z</i><sub><i>i</i></sub>的值。参数<i>θ</i>、<i>φ</i>的计算公式如下:</p>
                </div>
                <div class="p1">
                    <p id="72" class="code-formula">
                        <mathml id="72"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>θ</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>d</mi><mo stretchy="false">)</mo></mrow></msup><mo>=</mo><mfrac><mrow><mi>n</mi><msubsup><mrow></mrow><mi>j</mi><mi>d</mi></msubsup><mo>+</mo><mi>α</mi></mrow><mrow><mi>n</mi><msubsup><mrow></mrow><mo>*</mo><mrow><mo stretchy="false">(</mo><mi>d</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>+</mo><mi>Τ</mi><mi>α</mi></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>5</mn><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mi>φ</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>w</mi><mo stretchy="false">)</mo></mrow></msup><mo>=</mo><mfrac><mrow><mi>n</mi><msubsup><mrow></mrow><mi>j</mi><mrow><mo stretchy="false">(</mo><mi>w</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>+</mo><mi>β</mi></mrow><mrow><mi>n</mi><msubsup><mrow></mrow><mi>j</mi><mrow><mo stretchy="false">(</mo><mo>*</mo><mo stretchy="false">)</mo></mrow></msubsup><mo>+</mo><mi>v</mi><mi>β</mi></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>6</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="73">其中,<i>n</i><mathml id="164"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mi>d</mi></msubsup></mrow></math></mathml>是文章d中主题j的单词数,n<sup>(d)</sup><sub>*</sub>是文章d中所有主题的单词数,n<mathml id="165"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mrow><mo stretchy="false">(</mo><mi>w</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow></math></mathml>是单词w在主题j下出现的次数,n<mathml id="166"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mrow><mo stretchy="false">(</mo><mo>*</mo><mo stretchy="false">)</mo></mrow></msubsup></mrow></math></mathml>是文章d中主题j的单词总数。</p>
                </div>
                <div class="p1">
                    <p id="74">4)采用<i>Gibbs Sampling</i>,迭代足够多的次数确定最优主题T,使模型对于语料有最优的拟合结果。</p>
                </div>
                <div class="p1">
                    <p id="75"><i>LDA</i>主题模型拟合参数θ、φ时采用<i>Gibbs Sampling</i>方法。使用这种方法在进行参数拟合时,其数学过程非常复杂。由于θ、φ存在配对现象,无法计算出解析式,可采用<i>Gibbs Sampling</i>这种近似推理的方法间接获取待估参数,该方法是一个复杂的概率分布近似抽取过程<citation id="185" type="reference"><link href="7" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>,为达到收敛状态,需进行多轮采样。随着数据量的增加,为更加准确地拟合参数,得到较好的分类结果,该方法通过增加采样轮数使复杂度变高。复杂度提高造成训练时间增加,使得<i>Gibbs Sampling</i>在<i>LDA</i>模型参数计算时的优势逐渐减小,导致模型在文本分类时的性能降低。</p>
                </div>
                <h3 id="76" name="76" class="anchor-tag">2 <i>NLDA</i>算法</h3>
                <h4 class="anchor-tag" id="77" name="77">2.1 文本分类</h4>
                <div class="p1">
                    <p id="78">基于LDA主题模型的不足,本文提出一种应用神经网络拟合参数的算法,该算法将神经网络应用在LDA拟合参数的计算中,采用LDA模型假设,即一篇文章如果属于某个主题,那么这篇文章所有的单词都与该主题有关,单词与主题之间的权重正相关于词频的大小。</p>
                </div>
                <div class="p1">
                    <p id="79">在LDA主题模型的基础上,单词-主题的频率分布计算不再利用Gibbs Sampling方法,而是使用神经网络来训练得到单词-主题的频率分布。文本词频作为神经网络的输入,其对应于分类的主题作为隐藏层的神经元,使用神经网络来拟合单词-主题概率分布<i>φ</i>。</p>
                </div>
                <h4 class="anchor-tag" id="80" name="80">2.2 NLDA算法模型</h4>
                <div class="p1">
                    <p id="81">NLDA算法假定有<i>t</i><sub>1</sub>,<i>t</i><sub>2</sub>,…,<i>t</i><sub><i>i</i></sub>共<i>i</i>个待分类的文本,训练数据集中有<i>d</i><sub>1</sub>,<i>d</i><sub>2</sub>,…,<i>d</i><sub><i>j</i></sub>共<i>j</i>篇文章,每篇文章有<i>w</i><sub>1</sub>,<i>w</i><sub>2</sub>,…,<i>w</i><sub><i>m</i></sub>个单词。对所有文本进行分词、去停用词等预处理后,形成一个序列&lt;<i>w</i><sub>1</sub>,<i>w</i><sub>2</sub>,…,<i>w</i><sub><i>n</i></sub>&gt;,即关键词的词典,用<i>D</i>表示。其中,<i>n</i>表示<i>D</i>中的单词数。图3给出NLDA分类模型的结构。</p>
                </div>
                <div class="area_img" id="82">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201910035_082.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZyTnZtdlVYc3IzaEdiaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 NLDA分类模型的结构" src="Detail/GetImg?filename=images/JSJC201910035_082.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZyTnZtdlVYc3IzaEdiaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图3 NLDA分类模型的结构</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201910035_082.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZyTnZtdlVYc3IzaEdiaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="83">在对该模型进行文本分类时,数据预处理阶段采用LDA主题模型。通过LDA处理得到<i>n</i><mathml id="167"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mi>d</mi></msubsup></mrow></math></mathml>,即d贡献给主题j的单词数。在参数拟合阶段,不再通过引入超参数去得到单词-主题的概率分布,而是将提取到的词频对应到神经网络的输入层神经元,进行训练来拟合单词-主题概率分布φ。</p>
                </div>
                <div class="p1">
                    <p id="84">图4所示为<i>NLDA</i>拟合参数部分神经网络的结构。</p>
                </div>
                <div class="area_img" id="85">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201910035_085.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZyTnZtdlVYc3IzaEdiaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 NLDA拟合参数部分的神经网络结构" src="Detail/GetImg?filename=images/JSJC201910035_085.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZyTnZtdlVYc3IzaEdiaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图4 NLDA拟合参数部分的神经网络结构</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201910035_085.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZyTnZtdlVYc3IzaEdiaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="86">在输入层中,每个神经元对应词典<i>D</i>中唯一的一个单词,因此输入层神经元有<i>n</i>个。<i>t</i><sub>1</sub>,<i>t</i><sub>2</sub>,…,<i>t</i><sub><i>j</i></sub>对应隐藏层神经元,每一个神经元代表一个分类主题,这样隐藏层神经元就有<i>j</i>个。<i><b>W</b></i><b>_</b><i><b>T</b></i><sub><i>nj</i></sub>(<i>p</i>)矩阵是输入层神经元与隐藏层神经元之间的连接权重,表示词典<i>D</i>中的每个单词分别在主题<i>t</i><sub><i>j</i></sub>下出现的概率。<i><b>V</b></i><sub><i>t</i></sub>矩阵是隐藏层与输出层之间的连接,表示一篇文章<i>d</i>在神经网络各个主题中出现的概率。这种网络结构是针对单词-主题概率分布参数<i>φ</i>的特点而设计,由于需要拟合的参数是关于单词与主题的概率分布,因此在保证参数拟合准确的前提下达到快速收敛的效果。</p>
                </div>
                <div class="p1">
                    <p id="87">在第<i>p</i>轮训练中,输入层神经元与隐藏层神经元之间的权值矩阵的训练模型<i><b>W</b></i><b>_</b><i><b>T</b></i><sub><i>nj</i></sub>(<i>p</i>)如图5所示,隐藏层神经元与输出层神经元之间的矩阵<i><b>V</b></i><sub><i>t</i></sub>(<i>p</i>)的训练模型如图6所示。其中,<i>Min</i><sub><i>j</i></sub>(<i>p</i>)表示该神经元当前的总输入,<i>Mout</i><sub><i>j</i></sub>(<i>p</i>)是当前神经元<i>t</i><sub><i>j</i></sub>的输出,是神经元激活函数对输入总量作用的结果。在进行文本训练时,不仅要考虑学习因子,还应该在隐藏层和输出层之间引入权值的调节因子<i>γ</i><sub><i>j</i></sub>。此时,<i>Tin</i>(<i>p</i>)表示输出神经元的总输入,<i>Tout</i>(<i>p</i>)表示输出神经元经过激活函数作用以后的输出。</p>
                </div>
                <div class="area_img" id="88">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201910035_088.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZyTnZtdlVYc3IzaEdiaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 输入层与隐藏层神经元间的权重矩阵训练模型" src="Detail/GetImg?filename=images/JSJC201910035_088.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZyTnZtdlVYc3IzaEdiaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图5 输入层与隐藏层神经元间的权重矩阵训练模型</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201910035_088.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZyTnZtdlVYc3IzaEdiaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="89">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201910035_089.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZyTnZtdlVYc3IzaEdiaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 隐藏层与输出层神经元间的矩阵Vt(p)的训练模型" src="Detail/GetImg?filename=images/JSJC201910035_089.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZyTnZtdlVYc3IzaEdiaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图6 隐藏层与输出层神经元间的矩阵</b><i><b>V</b></i><sub><i>t</i></sub>(<i>p</i>)<b>的训练模型</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201910035_089.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZyTnZtdlVYc3IzaEdiaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="90" name="90">2.3 NLDA算法的实现</h4>
                <h4 class="anchor-tag" id="91" name="91">2.3.1 算法训练</h4>
                <div class="p1">
                    <p id="92">NLDA算法的训练步骤具体如下:</p>
                </div>
                <div class="p1">
                    <p id="93"><b>步骤1</b> 对文本进行分词、去停用词等预处理,初始化<i><b>W</b></i><b>_</b><i><b>T</b></i><sub><i>nj</i></sub>(0)矩阵、<i><b>V</b></i><sub><i>t</i></sub>(0)矩阵以及输入层与隐藏层的学习因子<i>η</i><sub>1</sub><sub><i>j</i></sub>(0)、隐藏层与输出层的学习因子<i>η</i><sub>2</sub><sub><i>j</i></sub>(0)。</p>
                </div>
                <div class="p1">
                    <p id="94"><b>步骤2</b> NLDA算法继续执行如下过程:</p>
                </div>
                <div class="p1">
                    <p id="95">1)训练时,先抽取<i>d</i><sub>1</sub>,设<i>d</i><sub>1</sub>∈<i>t</i><sub><i>i</i></sub>,在<i><b>W</b></i><b>_</b><i><b>T</b></i><sub><i>nj</i></sub>(0)矩阵中,若<i>w</i><sub><i>n</i></sub>∈<i>d</i><sub>1</sub>,则该单词<i>w</i><sub><i>n</i></sub>在此主题下的词频增加1,即:</p>
                </div>
                <div class="p1">
                    <p id="96">(<i>w</i><sub><i>n</i></sub>,<i>t</i><sub><i>i</i></sub><sub>+1</sub>)=(<i>w</i><sub><i>n</i></sub>,<i>t</i><sub><i>i</i></sub>)+1      (7)</p>
                </div>
                <div class="p1">
                    <p id="97">否则,将该单词<i>w</i><sub><i>n</i></sub>在此主题下的词频初始化为0,即:</p>
                </div>
                <div class="p1">
                    <p id="98">(<i>w</i><sub><i>n</i></sub>,<i>t</i><sub><i>i</i></sub>)=0      (8)</p>
                </div>
                <div class="p1">
                    <p id="99">2)执行正向传播算法,计算出文章<i>d</i><sub>1</sub>在神经网络中的输出,即该篇文章在各个主题下的输出概率<i>P</i><sub><i>j</i></sub>(1)。然后与输出层目标概率<i>P</i><sub><i>j</i></sub>对比,得到误差。</p>
                </div>
                <div class="p1">
                    <p id="100">3)执行逆向传播算法,将得到的误差反向传播,依次修正更新<i><b>W</b></i><b>_</b><i><b>T</b></i><sub><i>nj</i></sub>(1)矩阵、<i><b>V</b></i><sub><i>t</i></sub>(1)矩阵、学习因子<i>η</i><sub>1</sub><sub><i>j</i></sub>(1)、<i>η</i><sub>2</sub><sub><i>j</i></sub>(1)。</p>
                </div>
                <div class="p1">
                    <p id="101">4)对权重进行归一化。权重乘以一个统一的调节因子<i>δ</i>,再除以该主题下的总词数,其计算公式如下:</p>
                </div>
                <div class="p1">
                    <p id="102" class="code-formula">
                        <mathml id="102"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">W</mi><mo>_</mo><mi mathvariant="bold-italic">Τ</mi><msub><mrow></mrow><mrow><mi>n</mi><mi>j</mi></mrow></msub><mo stretchy="false">(</mo><mi>p</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><mi mathvariant="bold-italic">V</mi><msub><mrow></mrow><mrow><mi>w</mi><mi>t</mi></mrow></msub><mo stretchy="false">(</mo><mi>p</mi><mo stretchy="false">)</mo><mi>δ</mi></mrow><mi>k</mi></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>9</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="103">其中,<i><b>V</b></i><sub><i>wt</i></sub>(<i>p</i>)是归一化前的权重,<i>δ</i>是调节因子,<i>k</i>是该主题下所有单词的数目。</p>
                </div>
                <div class="p1">
                    <p id="104"><b>步骤3</b> 依次抽取<i>d</i><sub>1</sub>,<i>d</i><sub>2</sub>,…,<i>d</i><sub><i>j</i></sub>,重复步骤2过程,直到所有文章抽取完毕。</p>
                </div>
                <div class="p1">
                    <p id="105"><b>步骤4</b> 当所有文章都遍历以后,经过<i>p</i>轮训练,得到一个单词与主题<i><b>W</b></i><b>_</b><i><b>T</b></i><sub><i>nj</i></sub>(<i>p</i>)的矩阵,即为训练模型。</p>
                </div>
                <h4 class="anchor-tag" id="106" name="106">2.3.2 模型参数更新</h4>
                <div class="p1">
                    <p id="107">在神经网络中用逆向传播算法更新<i><b>W</b></i><b>_</b><i><b>T</b></i><sub><i>nj</i></sub>(<i>p</i>)矩阵,得到单词与主题的概率分布。假定神经网络的输出为<i>T</i><sub>1</sub>,<i>T</i><sub>2</sub>,…,<i>T</i><sub><i>p</i></sub>,其中,<i>p</i>代表神经网络训练的轮数。神经网络的当前输出<i>T</i><sub><i>p</i></sub>与理想输出<i>T</i><sub><i>s</i></sub>存在一定的误差<i>E</i><sub><i>p</i></sub>,训练过程的当前误差如式(10)所示。</p>
                </div>
                <div class="p1">
                    <p id="108" class="code-formula">
                        <mathml id="108"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>E</mi><msub><mrow></mrow><mi>p</mi></msub><mo>=</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mrow><mo stretchy="false">(</mo><mi>Τ</mi><msub><mrow></mrow><mi>s</mi></msub><mo>-</mo><mi>Τ</mi><msub><mrow></mrow><mi>p</mi></msub><mo stretchy="false">)</mo></mrow><msup><mrow></mrow><mn>2</mn></msup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>0</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="109">在当前误差下,神经网络进入逆向修正更新状态,更新<i><b>W</b></i><b>_</b><i><b>T</b></i><sub><i>nj</i></sub>(<i>p</i>)矩阵、<i><b>V</b></i><sub><i>t</i></sub>(<i>p</i>)矩阵、学习因子<i>η</i><sub>1</sub><sub><i>j</i></sub>(<i>p</i>)、<i>η</i><sub>2</sub><sub><i>j</i></sub>(<i>p</i>)。神经网络采用梯度下降的策略,在<i><b>V</b></i><sub><i>t</i></sub>(<i>p</i>)上学习的梯度方向为:</p>
                </div>
                <div class="p1">
                    <p id="110" class="code-formula">
                        <mathml id="110"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mo>∂</mo><mi>E</mi><msub><mrow></mrow><mi>p</mi></msub></mrow><mrow><mo>∂</mo><mi mathvariant="bold-italic">V</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">(</mo><mi>p</mi><mo stretchy="false">)</mo></mrow></mfrac><mo>=</mo><mfrac><mrow><mo>∂</mo><mi>E</mi><msub><mrow></mrow><mi>p</mi></msub></mrow><mrow><mo>∂</mo><mi>Τ</mi><mi>o</mi><mi>u</mi><mi>t</mi><mo stretchy="false">(</mo><mi>p</mi><mo stretchy="false">)</mo></mrow></mfrac><mfrac><mrow><mo>∂</mo><mi>Τ</mi><mi>o</mi><mi>u</mi><mi>t</mi><mo stretchy="false">(</mo><mi>p</mi><mo stretchy="false">)</mo></mrow><mrow><mo>∂</mo><mi>Τ</mi><mi>i</mi><mi>n</mi><mo stretchy="false">(</mo><mi>p</mi><mo stretchy="false">)</mo></mrow></mfrac><mfrac><mrow><mo>∂</mo><mi>Τ</mi><mi>i</mi><mi>n</mi><mo stretchy="false">(</mo><mi>p</mi><mo stretchy="false">)</mo></mrow><mrow><mo>∂</mo><mi>V</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">(</mo><mi>p</mi><mo stretchy="false">)</mo></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>1</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="111">激活函数<mathml id="168"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mtext>e</mtext><msup><mrow></mrow><mrow><mo>-</mo><mi>x</mi></mrow></msup></mrow></mfrac></mrow></math></mathml>具有如下性质:</p>
                </div>
                <div class="p1">
                    <p id="112" class="code-formula">
                        <mathml id="112"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mfrac><mrow><mo>∂</mo><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><mrow><mo>∂</mo><mi>x</mi></mrow></mfrac><mo>=</mo><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo stretchy="false">(</mo><mn>1</mn><mo>-</mo><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>2</mn><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mfrac><mrow><mo>∂</mo><mi>E</mi><msub><mrow></mrow><mi>p</mi></msub></mrow><mrow><mo>∂</mo><mi>Τ</mi><mi>o</mi><mi>u</mi><mi>t</mi><mo stretchy="false">(</mo><mi>p</mi><mo stretchy="false">)</mo></mrow></mfrac><mo>=</mo><mo>-</mo><mo stretchy="false">(</mo><mi>Τ</mi><msub><mrow></mrow><mi>s</mi></msub><mo>-</mo><mi>Τ</mi><mi>o</mi><mi>u</mi><mi>t</mi><mo stretchy="false">(</mo><mi>p</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>3</mn><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mfrac><mrow><mo>∂</mo><mi>Τ</mi><mi>i</mi><mi>n</mi><mo stretchy="false">(</mo><mi>p</mi><mo stretchy="false">)</mo></mrow><mrow><mo>∂</mo><mi>V</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">(</mo><mi>p</mi><mo stretchy="false">)</mo></mrow></mfrac><mo>=</mo><mi>Μ</mi><mi>o</mi><mi>u</mi><mi>t</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">(</mo><mi>p</mi><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>4</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="113">根据激活函数的性质,得到:</p>
                </div>
                <div class="p1">
                    <p id="114" class="code-formula">
                        <mathml id="114"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mo>-</mo><mfrac><mrow><mo>∂</mo><mi>E</mi><msub><mrow></mrow><mi>p</mi></msub></mrow><mrow><mo>∂</mo><mi>V</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">(</mo><mi>p</mi><mo stretchy="false">)</mo></mrow></mfrac><mo>=</mo><mi>Μ</mi><mi>o</mi><mi>u</mi><mi>t</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">(</mo><mi>p</mi><mo stretchy="false">)</mo><mo stretchy="false">(</mo><mi>Τ</mi><msub><mrow></mrow><mi>s</mi></msub><mo>-</mo><mi>Τ</mi><mi>o</mi><mi>u</mi><mi>t</mi><mo stretchy="false">(</mo><mi>p</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>×</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mi>Τ</mi><mi>o</mi><mi>u</mi><mi>t</mi><mo stretchy="false">(</mo><mi>p</mi><mo stretchy="false">)</mo><mo stretchy="false">(</mo><mn>1</mn><mo>-</mo><mi>Τ</mi><mi>o</mi><mi>u</mi><mi>t</mi><mo stretchy="false">(</mo><mi>p</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>5</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="115">引入记号:</p>
                </div>
                <div class="p1">
                    <p id="116">λ<sub>T(p)</sub>=(T<sub>s</sub>-Tout(p))Tout(p)(1-Tout(p))      (16)</p>
                </div>
                <div class="p1">
                    <p id="117">最终得到:</p>
                </div>
                <div class="p1">
                    <p id="118" class="code-formula">
                        <mathml id="118"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mo>∂</mo><mi>E</mi><msub><mrow></mrow><mi>p</mi></msub></mrow><mrow><mo>∂</mo><mi>V</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">(</mo><mi>p</mi><mo stretchy="false">)</mo></mrow></mfrac><mo>=</mo><mo>-</mo><mi>λ</mi><msub><mrow></mrow><mrow><mi>Τ</mi><mo stretchy="false">(</mo><mi>p</mi><mo stretchy="false">)</mo></mrow></msub><mi>Μ</mi><mi>o</mi><mi>u</mi><mi>t</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">(</mo><mi>p</mi><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>7</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="119">由以上公式修正更新学习因子η<sub>2j</sub>(p),可以得到修正关系,有:</p>
                </div>
                <div class="p1">
                    <p id="120">η<sub>2j</sub>(p)=η<sub>2j</sub>(p-1)(1-λ<sub>T(p)</sub>)      (18)</p>
                </div>
                <div class="p1">
                    <p id="121">隐藏层与输出层之间的权值修正值<i>Δ</i>V<sub>t</sub>(p)可以通过式(19)得到:</p>
                </div>
                <div class="p1">
                    <p id="122"><i>Δ</i>V<sub>t</sub>(p)=η<sub>2j</sub>γ<sub>j</sub>λ<sub>T(p)</sub>Mout<sub>j</sub>(p)      (19)</p>
                </div>
                <div class="p1">
                    <p id="123">则修正更新以后的权值为:</p>
                </div>
                <div class="p1">
                    <p id="124">V<sub>t</sub>(p)=V<sub>t</sub>(p-1)+<i>Δ</i>V<sub>t</sub>(p)      (20)</p>
                </div>
                <div class="p1">
                    <p id="125">同理,可以用相似计算方法继续反向修正更新,得到η<sub>1j</sub>(p)和W_T<sub>nj</sub>(p),有:</p>
                </div>
                <div class="p1">
                    <p id="126">η<sub>1j</sub>(p)=η<sub>1j</sub>(p-1)(1-λ<sub>T(p)</sub>)      (21)</p>
                </div>
                <div class="p1">
                    <p id="127">W_T<sub>nj</sub>(p)=W_T<sub>nj</sub>(p-1)+<i>Δ</i>W_T<sub>nj</sub>(p)      (22)</p>
                </div>
                <div class="p1">
                    <p id="128">最终得到的<i><b>W</b></i><b>_</b><i><b>T</b></i><sub><i>nj</i></sub>(<i>p</i>)矩阵就是拟合出的单词-主题概率分布<i>φ</i>,从而应用神经网络代替Gibbs Sampling完成参数拟合的过程,实现改进LDA主题模型对文本的分类。</p>
                </div>
                <h4 class="anchor-tag" id="129" name="129">2.3.3 NLDA算法特性分析</h4>
                <div class="p1">
                    <p id="130">Gibbs Sampling在数据量较大时,近似效率不高,而且需要反复进行采样才能取得较好的效果。本文提出NLDA算法,应用神经网络进行参数拟合,简化Gibbs Sampling在复杂数学理论的基础上采用近似推理的方法间接获取待估参数的过程。</p>
                </div>
                <div class="p1">
                    <p id="131">NLDA中的神经网络实现一个从输入到输出的映射功能。在训练时,能够通过自动学习提取输入、输出数据间的“合理规则”,并将学习到的参数值在网络的权值中进行存储。相关数学理论也证明3层神经网络能够以任意精度逼近任何非线性连续函数,使其适合于求解内部机制复杂的问题,通过神经网络拟合得到的参数比通过Gibbs Sampling方法间接获取的参数更加精确,能够使NLDA算法的文本分类比传统LDA主题模型分类准确率高。</p>
                </div>
                <div class="p1">
                    <p id="132">由于在NLDA中需要拟合的参数是单词-主题概率分布<i>φ</i>,对于神经网络而言,输入是一个较低维度的数据,在学习步长设定合理的前提下,网络具有较快的收敛速度。随着训练数据量的增加,应用Gibbs Sampling拟合参数时,为达到精确的收敛状态,需不断进行采样。LDA主题模型对文章进行反复迭代采样时较耗时。因此,引入神经网络来拟合参数,替换采样过程,使其收敛速度得到一定提升。</p>
                </div>
                <div class="p1">
                    <p id="133">NLDA算法从整体上简化了传统LDA的计算过程,并利用神经网络的容错性拟合文本,从而保证了NLDA算法的鲁棒性。针对参数<i>φ</i>所设计的3层神经网络,使得NLDA算法拥有了更高参数精度和更快收敛速度的特性,确保改进后的算法在理论上比传统的LDA在文本分类时有更好的性能。</p>
                </div>
                <h3 id="134" name="134" class="anchor-tag">3 实验结果与分析</h3>
                <h4 class="anchor-tag" id="135" name="135">3.1 实验数据与环境</h4>
                <div class="p1">
                    <p id="136">本文使用公开的THUCNews语料库和复旦大学语料库进行文本分类,以测试NLDA算法的性能。</p>
                </div>
                <div class="p1">
                    <p id="137">THUCNews语料库是新浪新闻RSS订阅频道2005年—2011年的数据,共有74万篇新闻文档。本文选取其中6 000篇文本进行实验,如表2所示。</p>
                </div>
                <div class="area_img" id="138">
                    <p class="img_tit"><b>表2 THUCNews语料库文本类别与数量分布</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="138" border="1"><tr><td>主题</td><td>数量</td><td>主题</td><td>数量</td></tr><tr><td colspan="2"><br /></td><td colspan="2"></td></tr><tr><td rowspan="2">财经</td><td rowspan="2">600</td><td rowspan="2">时尚</td><td rowspan="2">600</td></tr><tr></tr><tr><td><br />房产</td><td>600</td><td>时政</td><td>600</td></tr><tr><td><br />家居</td><td>600</td><td>体育</td><td>600</td></tr><tr><td><br />教育</td><td>600</td><td>游戏</td><td>600</td></tr><tr><td><br />科技</td><td>600</td><td>军事</td><td>600</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="139">复旦大学语料库也是文本分类领域广泛使用的数据库,其中包含20个主题,共计9 833篇文章。由于电子、通信、能源、矿产这4类文章数量较少,且电子与通信相似度较高,能源与矿产相似度较高,因此将这4类数据合并为2类,经过处理后的复旦大学语料库共18个类别,其类别与数量的分布如表3所示。</p>
                </div>
                <div class="area_img" id="140">
                    <p class="img_tit"><b>表3 复旦大学语料库类别与数量分布</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="140" border="1"><tr><td>主题</td><td>数量</td><td>主题</td><td>数量</td></tr><tr><td colspan="2"><br /></td><td colspan="2"></td></tr><tr><td rowspan="2">艺术</td><td rowspan="2">742</td><td rowspan="2">空间</td><td rowspan="2">642</td></tr><tr></tr><tr><td><br />文学</td><td>34</td><td>法律</td><td>52</td></tr><tr><td><br />运输</td><td>59</td><td>医学</td><td>53</td></tr><tr><td><br />教育</td><td>61</td><td>军事</td><td>76</td></tr><tr><td><br />环境</td><td>1 218</td><td>政治</td><td>1 026</td></tr><tr><td><br />哲学</td><td>45</td><td>计算机</td><td>1 358</td></tr><tr><td><br />农业</td><td>1 022</td><td>体育</td><td>1 254</td></tr><tr><td><br />历史</td><td>468</td><td>电子与通信</td><td>55</td></tr><tr><td><br />经济</td><td>1 601</td><td>能源与矿产</td><td>67</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="141">本文将上述2个文本语料库随机分成两部分,一部分作为训练集,另一部分作为测试集,且训练集和测试集的文章数量比例约5∶1。当迭代轮数<i>p</i>达到3 000或者训练准确率收敛于99%时,结束训练,并保存模型。本文实验环境如表4所示。</p>
                </div>
                <div class="area_img" id="142">
                    <p class="img_tit"><b>表4 实验环境与配置</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="142" border="1"><tr><td><br />硬件与软件</td><td>配置</td></tr><tr><td><br />CPU</td><td>Intel(R) Core(TM) i7-7700K</td></tr><tr><td><br />内存</td><td>DDR4,16 GB×2</td></tr><tr><td><br />操作系统</td><td>Windows 10</td></tr><tr><td><br />开发环境</td><td>Eclipse 4.6.1JDK 1.8.0</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="143" name="143">3.2 结果分析</h4>
                <div class="p1">
                    <p id="144">本文采用准确率和训练时间这2个文本分类领域常用的指标来评价实验结果。其中,准确率指正确分类的文本数与总文本数的比值。建立混合矩阵,如表5所示。</p>
                </div>
                <div class="area_img" id="145">
                    <p class="img_tit"><b>表5 混合矩阵</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="145" border="1"><tr><td><br />分类后是否属于该类</td><td>原文本属于该类</td><td>原文本不属于该类</td></tr><tr><td><br />是</td><td><i>a</i></td><td><i>b</i></td></tr><tr><td><br />否</td><td><i>c</i></td><td><i>d</i></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="146">准确率的计算公式如下:</p>
                </div>
                <div class="p1">
                    <p id="147" class="code-formula">
                        <mathml id="147"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>A</mi><mo>=</mo><mfrac><mrow><mi>a</mi><mo>+</mo><mi>d</mi></mrow><mrow><mi>a</mi><mo>+</mo><mi>b</mi><mo>+</mo><mi>c</mi><mo>+</mo><mi>d</mi></mrow></mfrac><mo>×</mo><mn>1</mn><mn>0</mn><mn>0</mn><mi>%</mi><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>2</mn><mn>3</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="148">为验证本文NLDA算法的分类效果,使用SVM、LDA主题模型、BP神经网络作为对比算法。在THUCNews语料库上选用6 000篇文本,4种算法的准确率对比结果如表6所示。在复旦大学语料库中选用9 833篇文章进行验证,4种算法的准确率对比结果如表7所示。</p>
                </div>
                <div class="area_img" id="149">
                    <p class="img_tit"><b>表6 THUCNews语料库上4种算法的准确率对比</b> % <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="149" border="1"><tr><td><br />算法</td><td>准确率</td></tr><tr><td><br />SVM算法</td><td>86.7</td></tr><tr><td><br />LDA主题模型算法</td><td>85.2</td></tr><tr><td><br />BP神经网络算法</td><td>88.1</td></tr><tr><td><br />NLDA算法</td><td>92.2</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="150">
                    <p class="img_tit"><b>表7 复旦大学语料库上4种算法的准确率对比</b> % <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="150" border="1"><tr><td><br />算法</td><td>准确率</td></tr><tr><td><br />SVM算法</td><td>85.3</td></tr><tr><td><br />LDA主题模型算法</td><td>83.3</td></tr><tr><td><br />BP神经网络算法</td><td>89.0</td></tr><tr><td><br />NLDA算法</td><td>90.5</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="151">从表6、表7可以看出,在THUCNews语料库上,本文NLDA算法的分类准确率为92.2%,相对于SVM算法、LDA主题模型算法和BP神经网络算法,分类准确率分别提升了5.5%、7.0%和4.1%,准确率平均提升5.53%。在复旦大学语料库上,相对SVM算法、LDA主题模型算法和BP神经网络算法,本文NLDA算法准确率分别提高了5.2%、7.3%、1.5%,准确率平均提升4.67%。</p>
                </div>
                <div class="p1">
                    <p id="152">此外,本文在2个不同数据集上选取不同数量的文本数据来验证NLDA算法的性能。在THUCNews数据集上,分别选择1 000篇、2 000篇、3 000篇、4 000篇、5 000篇、6 000篇,在复旦语料库中,分别选取1 500篇、3 000篇、4 500篇、6 000篇、7 500篇、9 000篇。THUCNews语料库上不同数量文章情况下4种算法的准确率对比结果如图7所示,复旦大学语料库上不同数量文章的情况下4种算法的准确率对比结果如图8所示。可以看出,在 2个数量不同的数据集上,与其他3种算法相比,本文NLDA算法分类准确率较高。</p>
                </div>
                <div class="area_img" id="153">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201910035_153.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZyTnZtdlVYc3IzaEdiaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 THUCNews语料库上4种算法的准确率对比" src="Detail/GetImg?filename=images/JSJC201910035_153.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZyTnZtdlVYc3IzaEdiaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图7 THUCNews语料库上4种算法的准确率对比</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201910035_153.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZyTnZtdlVYc3IzaEdiaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="154">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201910035_154.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZyTnZtdlVYc3IzaEdiaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 复旦大学语料库上4种算法的准确率对比" src="Detail/GetImg?filename=images/JSJC201910035_154.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZyTnZtdlVYc3IzaEdiaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图8 复旦大学语料库上4种算法的准确率对比</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201910035_154.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZyTnZtdlVYc3IzaEdiaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="155">为对比本文算法和LDA算法的训练时间开销,分别在THUCNews语料库和复旦大学语料库上选取不同数量的数据进行实验。LDA算法和NLDA算法在不同数据集上训练时间对比结果如表8所示。</p>
                </div>
                <div class="area_img" id="156">
                    <p class="img_tit"><b>表8 2种算法在不同数据集上的时间开销对比</b> min <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="156" border="1"><tr><td rowspan="2"><br />语料库</td><td colspan="2"><br />时间</td></tr><tr><td><br />LDA算法</td><td>NLDA算法</td></tr><tr><td><br />THUCNews(1 000篇)</td><td>8.1</td><td>11.6</td></tr><tr><td><br />THUCNews(3 000篇)</td><td>21.3</td><td>20.1</td></tr><tr><td><br />THUCNews(6 000篇)</td><td>40.5</td><td>32.5</td></tr><tr><td><br />复旦大学语料库(1 500篇)</td><td>10.6</td><td>15.9</td></tr><tr><td><br />复旦大学语料库(4 500篇)</td><td>30.3</td><td>28.6</td></tr><tr><td><br />复旦大学语料库(9 000篇)</td><td>53.0</td><td>40.3</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="157">从表8可以看出,NLDA算法在数据量较小时,训练时间比LDA算法多。但随着数据量增大,使用神经网络去拟合参数的方法相对于传统的Gibbs Sampling拟合,在分类准确率相差不大时,训练开销降低。NLDA算法在THUCNews数据集和复旦大学语料库上的平均训练时间减少8%和10%</p>
                </div>
                <div class="p1">
                    <p id="158">相对LDA算法,本文算法利用神经网络来拟合单词-主题之间的概率分布,由于神经网络强大的非线性拟合能力,使得训练的模型参数在准确率上较高,模型的泛化能力也进一步得到加强。随着数据量的增大,NLDA算法的时间开销小于LDA算法,是因为Gibbs Sampling为达到更高的分类准确率,迭代轮数增加,使训练时间增加,而对于神经网络,可以保证在参数拟合时,达到较快收敛的目的。</p>
                </div>
                <div class="p1">
                    <p id="159">综上,本文算法分类的准确率没有达到最优,是因为该神经网络拟合参数是一个解决局部搜索问题的过程,其需要解决一个复杂非线性化问题,网络的权值沿局部改善方向逐渐进行调整更新,这会导致模型在训练时容易陷入局部极值的状态,使训练出的模型存在一定误差。此外,该网络对初始值的设定存在一定的敏感性,会造成训练模型出现偏差。这些问题导致的误差会影响参数拟合的精确度,使算法的分类准确率在一定程度上受到影响。</p>
                </div>
                <h3 id="160" name="160" class="anchor-tag">4 结束语</h3>
                <div class="p1">
                    <p id="161">本文提出一种基于神经网络拟合参数的LDA文本分类算法NLDA。基于LDA主题模型,利用神经网络替换Gibbs Sampling方法,拟合单词-主题概率分布,进而得到文本的分类结果。实验结果表明,与LDA主题模型算法相比,该算法的分类准确率较高。由于NLDA算法是将词典<i>D</i>中的所有单词与输入层神经元建立映射关系,使文本的输入数据存在稀疏性问题,因此下一步将建立动态的词频-神经元映射关系以解决该问题。在此基础上,对神经网络结构进行改进,解决文本分类模型易陷入局部极值和初始参数敏感等问题,从而满足LDA模型的参数拟合要求。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Mining and summarizing customer reviews">

                                <b>[1]</b> HU Minqing,LIU Bing.Mining and summarizing customer reviews[C]//Proceedings of the 10th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.New York,USA:ACM Press,2004:168-177.
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012100870114&amp;v=MzE2MDFlWnVIeWptVUx6SUpWb1hheGM9TmlmT2ZiSzdIdERPcm85RmJPd1BEWDA5b0JNVDZUNFBRSC9pclJkR2VycVFUTW53Wg==&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZyTnZtdlVYc3IzaEdiaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> PARK E K,RA D Y,JANG M G.Techniques for improving Web retrieval effectiveness[J].Information Processing Management,2005,41(5):1207-1223.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Latent Dirichlet allocation">

                                <b>[3]</b> BLEI D M,NG A Y,JORDAN M I.Latent Dirichlet allocation[J].The Journal of Machine Learning Research,2003,3(4/5):993-1022.
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JDYS201803004&amp;v=MDc5MDBGckNVUkxPZVplUnRGeTNoVUw3Tkx5blNmYkc0SDluTXJJOUZZSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEc=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZyTnZtdlVYc3IzaEdiaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b> GOUDJIL M,KOUDIL M,BEDDA M,et al.Anovel active learning method using SVM for text classification[J].International Journal of Automation and Computing,2018,15(3):290-298.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" >
                                    <b>[5]</b>
                                 LECUN Y,BENGIO Y,HINTON G.Deep learning[J].Nature,2015,521(7553):436-444.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A convolutional neural network formodelling sentences">

                                <b>[6]</b> KALCHBRENNER N,GREFENSTETTE E,BLUNSOM P.A convolutional neural network formodelling sentences[EB/OL].[2019-02-20].http://de.arxiv.org/pdf/1404.2188.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000024403&amp;v=MTQ3NTRjPU5pZklZN0s3SHRqTnI0OUZaT2tMQ0h3Nm9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVUx6SUpWb1hheA==&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZyTnZtdlVYc3IzaEdiaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b> SALTON G,WONG A,YANG C S.A vector space model for automatic indexing[J].Communications of the ACM,1975,18(11):613-620.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning to classify short and sparse text&amp;amp;web with hidden topics from large-scale data collections">

                                <b>[8]</b> PHAN X H,NGUYEN M L,HORIGUCHI S.Learning to classify short and sparse text &amp; Web with hidden topics from largescale data collections[C]//Proceedings of the 17th Conference on World Wide Web.New York,USA:ACM Press,2008:91-100.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Instant Message Clustering Based on Extended Vector Space Model">

                                <b>[9]</b> WANG Le,JIA Yan,HAN Weihong.Instant message clustering based on extended vector space model[C]// Proceedings of the 2nd International Conference on Advances in Computation and Intelligence.Berlin,Germany:Springer,2007:435-443.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Short Text Classification Using Semantic Random Forest">

                                <b>[10]</b> BOUAZIZ A,DARTIGUES-PALLEZ C,PEREIRA C D C,et al.Short text classification using semantic random forest[C]//Proceedings of International Conference on Data Warehousing and Knowledge Discovery.Berlin,Germany:Springer,2014:288-299.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=An improved AD-LDA topic model based on weighted Gibbs sampling">

                                <b>[11]</b> GUO Hongchen,LIANG Qiliang,LI Zhiqiang.An improved AD-LDA topic model based on weighted Gibbs sampling[C]//Proceedings of IEEE Advanced Information Management,Communicates,Electronic and Automation Control Conference.Washington D.C.,USA:IEEE Press,2016:1978-1982.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_12" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201306027&amp;v=MDY1MDlqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSdEZ5M2hVTDdOTHo3QmQ3RzRIOUxNcVk5SFk0UUtESDg0dlI0VDY=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZyTnZtdlVYc3IzaEdiaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[12]</b> 张志飞,苗夺谦,高灿.基于LDA主题模型的短文本分类方法[J].计算机应用,2013,33(6):1587-1590.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning Salient Features for Speech Emotion Recognition Using Convolutional Neural Networks">

                                <b>[13]</b> MAO Qirong,DONG Ming,HUANG Zhengwei,et al.Learning salient features for speech emotion recognition using convolutional neural networks[J].IEEE Transactions on Multimedia,2014,16(8):2203-2213.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Batch normalization:Accelerating deep network training by reducing internal covariate shift">

                                <b>[14]</b> IOFFE S,SZEGEDY C.Batch normalization:accelerating deep network training by reducing internal covariate shift[C]//Proceedings of the 32nd International Conference on International Conference on Machine Learning.[S.l.]:JMLR.org,2015:448-456.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_15" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201610001&amp;v=Mjc2ODM0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSdEZ5M2hVTDdOS0NMZlliRzRIOWZOcjQ5RlpZUUtESDg=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZyTnZtdlVYc3IzaEdiaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[15]</b> 奚雪峰,周国栋.面向自然语言处理的深度学习研究[J].自动化学报,2016,42(10):1445-1465.
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep Residual Learning for Image Recognition">

                                <b>[16]</b> HE Kaiming,ZHANG Xiangyu,REN Shaoqing,et al.Deep residual learning for image recognition[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Press,2016:770-778.
                            </a>
                        </p>
                        <p id="35">
                            <a id="bibliography_17" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CBBD&amp;filename=9787302423287000&amp;v=MjIxMjZpZlp1OXVGQ3ZtVTdyTUkxOFJYRnF6R2JDNEhOWE9ySTFOWStzUERCTTh6eFVTbURkOVNIN24zeEU5ZmJ2bkty&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZyTnZtdlVYc3IzaEdiaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[17]</b> 周志华.机器学习[M].北京:清华大学出版社,2016.
                            </a>
                        </p>
                        <p id="37">
                            <a id="bibliography_18" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=1018705086.nh&amp;v=MDE1OTE0TzN6cXFCdEdGckNVUkxPZVplUnRGeTNoVUw3TlZGMjZGclM0RzlIRXFaRWJQSVFLREg4NHZSNFQ2ajU=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZyTnZtdlVYc3IzaEdiaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[18]</b> 刘泽锦.基于主题模型和卷积神经网络的短文本分类算法研究[D].北京:北京工业大学,2017.
                            </a>
                        </p>
                        <p id="39">
                            <a id="bibliography_19" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=2006103241.nh&amp;v=MTA1MTJycEViUElRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJ0RnkzaFVMN05WMTI3R0xLNEhkUEk=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZyTnZtdlVYc3IzaEdiaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[19]</b> 王懿.基于自然语言处理和机器学习的文本分类及其应用研究[D].成都:中国科学院成都计算机应用研究所,2006.
                            </a>
                        </p>
                        <p id="41">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Finding scientific topics">

                                <b>[20]</b> GRIFFITHS T L,STEYVERS M.Finding scientific topics[J].National Academy of Sciences,2004,101(S1):5228-5235.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJC201910035" />
        <input id="dpi" type="hidden" value="600" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZyTnZtdlVYc3IzaEdiaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201910035&amp;v=MjM3OTVxcUJ0R0ZyQ1VSTE9lWmVSdEZ5M2hVTDdOTHo3QmJiRzRIOWpOcjQ5R1lZUUtESDg0dlI0VDZqNTRPM3o=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZyTnZtdlVYc3IzaEdiaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
