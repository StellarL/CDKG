<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637130386644520000%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJC201906040%26RESULT%3d1%26SIGN%3dkCDJMWkvD12ihrNo61E4jB3RH9s%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201906040&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201906040&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201906040&amp;v=MDU2NTlLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm9GeW5sVmJyTUx6N0JiYkc0SDlqTXFZOUJaSVE=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#39" data-title="0 概述 ">0 概述</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#44" data-title="1 长短时记忆网络 ">1 长短时记忆网络</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#75" data-title="2 基于连接时序分类的语音识别系统 ">2 基于连接时序分类的语音识别系统</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#78" data-title="2.1 连接时序分类">2.1 连接时序分类</a></li>
                                                <li><a href="#92" data-title="2.2 基于WFST的解码方法">2.2 基于WFST的解码方法</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#105" data-title="3 实验结果与分析 ">3 实验结果与分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#107" data-title="3.1 基线系统">3.1 基线系统</a></li>
                                                <li><a href="#115" data-title="3.2 端到端语音识别系统">3.2 端到端语音识别系统</a></li>
                                                <li><a href="#120" data-title="3.3 实验结果">3.3 实验结果</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#137" data-title="4 结束语 ">4 结束语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#53" data-title="&lt;b&gt;图1 RNN在时间上展开的示意图&lt;/b&gt;"><b>图1 RNN在时间上展开的示意图</b></a></li>
                                                <li><a href="#77" data-title="&lt;b&gt;图2 DBLSTM-CTC语音识别系统示意图&lt;/b&gt;"><b>图2 DBLSTM-CTC语音识别系统示意图</b></a></li>
                                                <li><a href="#118" data-title="&lt;b&gt;图3 输入帧的堆叠&lt;/b&gt;"><b>图3 输入帧的堆叠</b></a></li>
                                                <li><a href="#124" data-title="&lt;b&gt;表1 基线系统与端到端系统的性能对比&lt;/b&gt;"><b>表1 基线系统与端到端系统的性能对比</b></a></li>
                                                <li><a href="#126" data-title="&lt;b&gt;图4 4种声学模型的解码时间对比&lt;/b&gt;"><b>图4 4种声学模型的解码时间对比</b></a></li>
                                                <li><a href="#129" data-title="&lt;b&gt;表2 不同网络层数下BLSTM-CTC模型的性能对比&lt;/b&gt;"><b>表2 不同网络层数下BLSTM-CTC模型的性能对比</b></a></li>
                                                <li><a href="#132" data-title="&lt;b&gt;表3 添加音调特征前后的性能对比&lt;/b&gt; %"><b>表3 添加音调特征前后的性能对比</b> %</a></li>
                                                <li><a href="#136" data-title="&lt;b&gt;表4 序列区分度训练前后系统的性能对比&lt;/b&gt;"><b>表4 序列区分度训练前后系统的性能对比</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" title=" HINTON G, DENG Li, YU Dong, et al.Deep neural networks for acoustic modeling in speech recognition:the shared views of four research groups[J].IEEE Signal Processing Magazine, 2012, 29 (6) :82-97." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups">
                                        <b>[1]</b>
                                         HINTON G, DENG Li, YU Dong, et al.Deep neural networks for acoustic modeling in speech recognition:the shared views of four research groups[J].IEEE Signal Processing Magazine, 2012, 29 (6) :82-97.
                                    </a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" title=" 李伟林, 文剑, 马文凯.基于深度神经网络的语音识别系统研究[J].计算机科学, 2016, 43 (11A) :45-49." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJA2016S2010&amp;v=MDUwMjVMT2VaZVJvRnlubFZick1MejdCYjdHNEg5ZXZyWTlFWklRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         李伟林, 文剑, 马文凯.基于深度神经网络的语音识别系统研究[J].计算机科学, 2016, 43 (11A) :45-49.
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title=" GRAVES A, JAITLY N.Towards end-to-end speech recognition with recurrent neural networks[EB/OL].[2018-03-09].http://proceedings.mlr.press/v32/graves14.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Towards end-to-end speech recognition with recurrent neural networks">
                                        <b>[3]</b>
                                         GRAVES A, JAITLY N.Towards end-to-end speech recognition with recurrent neural networks[EB/OL].[2018-03-09].http://proceedings.mlr.press/v32/graves14.pdf.
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" title=" HOCHREITER S, SCHMIDHUBER J.Long short-term memory[J].Neural Computation, 1997, 9 (8) :1735-1780." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJBK&amp;filename=SJBK15090500014198&amp;v=MDYwNjF0ak1xbzlGWk9vTERYVXhvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVVMbklJVjRTYnhZPU5pZkpaYks5SA==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                         HOCHREITER S, SCHMIDHUBER J.Long short-term memory[J].Neural Computation, 1997, 9 (8) :1735-1780.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" title=" GRAVES A, FERN&#193;NDEZ S, GOMEZ F, et al.Connectionist temporal classification:labelling unsegmented sequence data with recurrent neural networks[C]//Proceedings of the 23rd International Conference on Machine Learning.New York, USA:ACM Press, 2006:369-376." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Connectionist temporal classification:Labelling unsegmented sequence data with recurrent neural networks">
                                        <b>[5]</b>
                                         GRAVES A, FERN&#193;NDEZ S, GOMEZ F, et al.Connectionist temporal classification:labelling unsegmented sequence data with recurrent neural networks[C]//Proceedings of the 23rd International Conference on Machine Learning.New York, USA:ACM Press, 2006:369-376.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" title=" LI Jie, ZHANG Heng, CAI Xinyuan, et al.Towards end-to-end speech recognition for Chinese mandarin using long short-term memory recurrent neural networks[C]//Proceedings of the 16th Annual Conference of International Speech Com-munication Association.Baixas, France:International Speech Communication Association, 2015:1-5." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Towards end-to-end speech recognition for Chinese mandarin using long short-term memory recurrent neural networks">
                                        <b>[6]</b>
                                         LI Jie, ZHANG Heng, CAI Xinyuan, et al.Towards end-to-end speech recognition for Chinese mandarin using long short-term memory recurrent neural networks[C]//Proceedings of the 16th Annual Conference of International Speech Com-munication Association.Baixas, France:International Speech Communication Association, 2015:1-5.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" title=" SAK H, SENIOR A, BEAUFAYS F.Long short-term memory based recurrent neural network architectures for large vocabulary speech recognition[EB/OL].[2018-03-09].https://arxiv.org/pdf/1402.1128v1.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Long short-term memory based recurrent neural network architectures for large vocabulary speech recognition">
                                        <b>[7]</b>
                                         SAK H, SENIOR A, BEAUFAYS F.Long short-term memory based recurrent neural network architectures for large vocabulary speech recognition[EB/OL].[2018-03-09].https://arxiv.org/pdf/1402.1128v1.pdf.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" title=" YU Dong, LI Jinyu.Recent progresses in deep learning based acoustic models[J].IEEE/CAA Journal of Automatica Sinica, 2017, 4 (3) :396-409." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZDHB201703002&amp;v=MTA4NDllUm9GeW5sVmJyTVB5bkRiTEc0SDliTXJJOUZab1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVo=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                         YU Dong, LI Jinyu.Recent progresses in deep learning based acoustic models[J].IEEE/CAA Journal of Automatica Sinica, 2017, 4 (3) :396-409.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" title=" MIAO Yajie, GOWAYYED M, METZE F.EESEN:end-to-end speech recognition using deep RNN models and WFS T-based decoding[C]//Proceedings of 2015 IEEE Workshop on Automatic Speech Recognition and Understanding.Washington D.C., USA:IEEE Press, 2016:167-174." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=EESEN:End-to-end speech recognition using deep RNN models and WFST-based decoding">
                                        <b>[9]</b>
                                         MIAO Yajie, GOWAYYED M, METZE F.EESEN:end-to-end speech recognition using deep RNN models and WFS T-based decoding[C]//Proceedings of 2015 IEEE Workshop on Automatic Speech Recognition and Understanding.Washington D.C., USA:IEEE Press, 2016:167-174.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_10" title=" 黎长江, 胡燕.基于循环神经网络的音素识别研究[J].微电子学与计算机, 2017, 34 (8) :47-51." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=WXYJ201708010&amp;v=MDYwNjJYU1pMRzRIOWJNcDQ5RVpJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSb0Z5bmxWYnJNTWo=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[10]</b>
                                         黎长江, 胡燕.基于循环神经网络的音素识别研究[J].微电子学与计算机, 2017, 34 (8) :47-51.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_11" title=" ZENKEL T, SANABRIA R, METZE F, et al.Comparison of decoding strategies for CTC acoustic models[EB/OL].[2018-03-09].https://arxiv.org/pdf/1708.04469.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Comparison of decoding strategies for CTC acoustic models">
                                        <b>[11]</b>
                                         ZENKEL T, SANABRIA R, METZE F, et al.Comparison of decoding strategies for CTC acoustic models[EB/OL].[2018-03-09].https://arxiv.org/pdf/1708.04469.pdf.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_12" title=" WANG Dong, ZHANG Xuewei.Thchs-30:A free chinese speech corpus[EB/OL].[2018-03-09].https://arxiv.org/pdf/1512.01882.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Thchs-30:A free chinese speech corpus">
                                        <b>[12]</b>
                                         WANG Dong, ZHANG Xuewei.Thchs-30:A free chinese speech corpus[EB/OL].[2018-03-09].https://arxiv.org/pdf/1512.01882.pdf.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_13" title=" PUNDAK G, SAINATH T N.Lower frame rate neural network acoustic models[EB/OL].[2018-03-09].https://storage.googleapis.com/pub-tools-public-publication-data/pdf/45555.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Lower frame rate neural network acoustic models">
                                        <b>[13]</b>
                                         PUNDAK G, SAINATH T N.Lower frame rate neural network acoustic models[EB/OL].[2018-03-09].https://storage.googleapis.com/pub-tools-public-publication-data/pdf/45555.pdf.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_14" title=" MIAO Yajie, GOWAYYED M, NA Xingyu, et al.An empirical exploration of CTC acoustic models[C]//Proceedings of International Conference on Acoustics, Speech and Signal Processing.Washington D.C., USA:IEEE Press, 2016:2623-2627." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=An empirical explo-ration of CTC acoustic models">
                                        <b>[14]</b>
                                         MIAO Yajie, GOWAYYED M, NA Xingyu, et al.An empirical exploration of CTC acoustic models[C]//Proceedings of International Conference on Acoustics, Speech and Signal Processing.Washington D.C., USA:IEEE Press, 2016:2623-2627.
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_15" title=" GHAHREMANI P, BABAALI B, POVEY D, et al.A pitch extraction algorithm tuned for automatic speech recognition[C]//Proceedings of International Conference on Acoustics, Speech and Signal Processing.Washington D.C., USA:IEEE Press, 2014:2494-2498." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A pitch extraction algorithm tuned for automatic speech recognition">
                                        <b>[15]</b>
                                         GHAHREMANI P, BABAALI B, POVEY D, et al.A pitch extraction algorithm tuned for automatic speech recognition[C]//Proceedings of International Conference on Acoustics, Speech and Signal Processing.Washington D.C., USA:IEEE Press, 2014:2494-2498.
                                    </a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_16" title=" 秦楚雄, 张连海.低资源语音识别中融合多流特征的卷积神经网络声学建模方法[J].计算机应用, 2016, 36 (9) :2609-2615." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201609049&amp;v=MDY1ODhaZVJvRnlubFZick1MejdCZDdHNEg5Zk1wbzlCYllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2U=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[16]</b>
                                         秦楚雄, 张连海.低资源语音识别中融合多流特征的卷积神经网络声学建模方法[J].计算机应用, 2016, 36 (9) :2609-2615.
                                    </a>
                                </li>
                                <li id="35">


                                    <a id="bibliography_17" title=" BILLA J.Improving LSTM-CTC based ASR performance in domains with limited training data[EB/OL].[2018-03-09].https://arxiv.org/pdf/1707.00722.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Improving LSTM-CTC based ASR performance in domains with limited training data">
                                        <b>[17]</b>
                                         BILLA J.Improving LSTM-CTC based ASR performance in domains with limited training data[EB/OL].[2018-03-09].https://arxiv.org/pdf/1707.00722.pdf.
                                    </a>
                                </li>
                                <li id="37">


                                    <a id="bibliography_18" title=" KINGSBURY B.Lattice-based optimization of sequence classi-fication criteria for neural-network acoustic modeling[C]//Proceedings of International Conference on Acoustics, Speech and Signal Processing.Washington D.C., USA:IEEE Press, 2009:3761-3764." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Lattice-based optimization of sequence classification criteria for neural-network acoustic modeling">
                                        <b>[18]</b>
                                         KINGSBURY B.Lattice-based optimization of sequence classi-fication criteria for neural-network acoustic modeling[C]//Proceedings of International Conference on Acoustics, Speech and Signal Processing.Washington D.C., USA:IEEE Press, 2009:3761-3764.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJC" target="_blank">计算机工程</a>
                2019,45(06),249-253+266 DOI:10.19678/j.issn.1000-3428.0051065            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于CTC准则的普通话识别及改进</b></span>
 <span class="shoufa"></span>                                     </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BC%A0%E7%AB%8B%E6%B0%91&amp;code=38019852&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">张立民</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E5%BD%A6%E5%93%B2&amp;code=39428916&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王彦哲</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BC%A0%E5%85%B5%E5%BC%BA&amp;code=39428917&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">张兵强</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9C%B1%E5%BF%B5%E6%96%8C&amp;code=42301512&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">朱念斌</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%B5%B7%E5%86%9B%E8%88%AA%E7%A9%BA%E5%A4%A7%E5%AD%A6%E4%BF%A1%E6%81%AF%E8%9E%8D%E5%90%88%E7%A0%94%E7%A9%B6%E6%89%80&amp;code=1701745&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">海军航空大学信息融合研究所</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E4%BA%BA%E6%B0%91%E8%A7%A3%E6%94%BE%E5%86%9B61923%E9%83%A8%E9%98%9F&amp;code=1747048&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国人民解放军61923部队</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>主流神经网络训练的交叉熵准则针对声学数据的每个帧进行分类优化, 而连续语音识别需以序列级的转录准确性为性能度量指标。针对这一差异, 构建一种基于序列级转录的端到端语音识别系统。以音素为基本单元建模, 并采用连接时序分类 (CTC) 的目标函数改进长短时记忆网络的结构。在解码过程中引入词典和语言模型, 并在前端增加音调特征以丰富声学特征。利用序列区分度训练技术提升CTC模型的建模效果。实验结果表明, 该系统的识别效率和识别准确率得到提高, 词错误率最低可降至19.09%±0.16%。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%BA%8F%E5%88%97%E7%BA%A7&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">序列级;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%AB%AF%E5%88%B0%E7%AB%AF&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">端到端;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%A7%A3%E7%A0%81&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">解码;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%A3%B0%E5%AD%A6%E7%89%B9%E5%BE%81&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">声学特征;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8C%BA%E5%88%86%E5%BA%A6%E8%AE%AD%E7%BB%83&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">区分度训练;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    张立民 (1966—) , 男, 教授、博士生导师, 主研方向为人工智能、电子系统仿真;;
                                </span>
                                <span>
                                    *王彦哲 (通信作者) , 硕士研究生;E-mail: iamwyz@foxmail.com;
                                </span>
                                <span>
                                    张兵强, 副教授、博士;;
                                </span>
                                <span>
                                    朱念斌, 高级工程师、硕士。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-04-03</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金重大研究计划 (91538201);</span>
                                <span>泰山学者工程专项经费 (ts201511020);</span>
                    </p>
            </div>
                    <h1><b>Mandarin Recognition and Improvement Based on CTC Criterion</b></h1>
                    <h2>
                    <span>ZHANG Limin</span>
                    <span>WANG Yanzhe</span>
                    <span>ZHANG Bingqiang</span>
                    <span>ZHU Nianbin</span>
            </h2>
                    <h2>
                    <span>Institute of Information Fusion, Naval Aeronautical University</span>
                    <span>Troops 61923 of PLA</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>The cross-entropy criterion of mainstream neural network training classifies and optimizes each frame of acoustic data, while the continuous speech recognition uses the sequence-level transcription accuracy as the performance measurement.For this difference, an end-to-end speech recognition system based on sequence-level transcription is constructed.The phoneme is used as the basic unit to build the model, and the target function of Connectionist Temporal Classification (CTC) is used to improve the structure of Long Short-Term Memory (LSTM) network.The dictionary and language model are introduced in the decoding process, and the tone feature is added to the front end to enrich the acoustic feature.The modeling effect of CTC is improved by using the sequence discrimination training technique.Experimental results show that the recognition efficiency and accuracy of the proposed system is improved, and the Word Error Rate (WER) can be as low as 19.09%±0.16%.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=sequence%20level&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">sequence level;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=end-to-end&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">end-to-end;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=decode&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">decode;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=acoustic%20feature&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">acoustic feature;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=discrimination%20training&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">discrimination training;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-04-03</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="39" name="39" class="anchor-tag">0 概述</h3>
                <div class="p1">
                    <p id="40">构建现代自动语音识别 (Automatic Speech Recognition, ASR) 系统是一项复杂任务, 系统设计基于严格的处理流程, 包括输入特征、声学模型、语言模型和隐马尔可夫模型 (Hidden Markov Model, HMM) 。深度学习算法的引入使得传统的混合高斯模型 (Gaussian Mixture Model, GMM) 开始被深度神经网络 (Deep Neural Network, DNN) 取代。利用DNN对状态输出进行建模, 语音识别的准确率得到大幅提高<citation id="139" type="reference"><link href="3" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>。在DNN的基础上, 卷积神经网络 (Convolutional Neural Network, CNN) 和循环神经网络 (Recurrent Neural Networks, RNN) 的应用进一步提高了语音建模的能力。</p>
                </div>
                <div class="p1">
                    <p id="41">上述算法通常侧重于声学模型的改进, 而声学模型只是系统中的一个部件。在语音识别中, 神经网络使用交叉熵 (Cross-Entropy, CE) <citation id="140" type="reference"><link href="5" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>作为目标函数来对声学数据的各个帧进行分类, 这与以序列级的转录准确性为实际性能指标有很大的不同。帧级别的训练标注必须事先获得, 其通过训练好的GMM-HMMs对训练集进行强制对齐得到, 同时, 这种训练标注方式还需要相应的专业语音学、语言学知识。因此, 构建现代语音识别系统的难度较大, 门槛较高。</p>
                </div>
                <div class="p1">
                    <p id="42">为尽可能地减少系统所需流程, 文献<citation id="141" type="reference">[<a class="sup">3</a>]</citation>对深度长短时记忆 (Long Short-Term Memory, LSTM) 网络和连接时序分类 (Connectionist Temporal Classification, CTC) <citation id="143" type="reference"><link href="9" rel="bibliography" /><link href="11" rel="bibliography" /><sup>[<a class="sup">4</a>,<a class="sup">5</a>]</sup></citation>目标函数进行结合。该方法无需中间的语音表示, 直接把音频数据转录成文本, 且不需要在输入序列和目标序列之间进行任何预先对齐。虽然端到端语音识别是未来发展的趋势, 但是CTC的鲁棒性需要足够的语料进行训练。同时, 由于解码时语言模型的缺失, 与传统深度学习的建模方法相比, 端到端语音识别的识别性能和准确率有待提高<citation id="142" type="reference"><link href="13" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="43">由于现有研究主要集中在英语语音识别领域, 因此本文结合深度双向长短时记忆 (Deep Bidirectional Long Short-Term Memory, DBLSTM) 网络和CTC技术, 建立一种中文普通话的端到端语音识别系统。根据中文普通话的语音特点, 以声韵母作为基本单元进行建模。解码时结合词典和语音模型, 通过序列区分性训练提升声学模型的区分能力。</p>
                </div>
                <h3 id="44" name="44" class="anchor-tag">1 长短时记忆网络</h3>
                <div class="p1">
                    <p id="45">标准的前馈网络通常仅考虑帧的固定长度滑动窗口中的信息, 因此无法利用语音信号的长范围相关性。RNN可以将序列历史编码为内部状态, 基于当前帧之前得到的所有语音特征来预测音素, 其在学习序列的复杂动态时间方面具有一定的优势。</p>
                </div>
                <div class="p1">
                    <p id="46">然而, 标准的RNN在时序上对序列进行处理时通常忽视了上下文信息。在某些情况下, 当前时刻的输出同时需要之前和之后的状态, 语音的前后相关性十分明显, 因此, 研究人员提出双向RNN网络, 如图1所示, 以充分利用语音的未来信息。假设给定一个输入序列<b><i>X</i></b>= (<b><i>x</i></b><sub>1</sub>, <b><i>x</i></b><sub>2</sub>, …, <b><i>x</i></b><sub><i>t</i></sub>) , 一个循环层从<i>t</i>=1到<i>t</i>=<i>T</i>计算隐藏状态的前向序列:</p>
                </div>
                <div class="p1">
                    <p id="47" class="code-formula">
                        <mathml id="47"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mrow><mi mathvariant="bold-italic">h</mi><msub><mrow></mrow><mi>t</mi></msub></mrow><mo stretchy="true">→</mo></mover><mo>=</mo><mi>σ</mi><mo stretchy="false"> (</mo><mover accent="true"><mrow><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mrow><mi>h</mi><mi>x</mi></mrow></msub></mrow><mo stretchy="true">→</mo></mover><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>t</mi></msub><mo>+</mo><mrow><mover accent="true"><mrow><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mrow><mi>h</mi><mi>h</mi></mrow></msub></mrow><mo stretchy="true">→</mo></mover></mrow><mtext> </mtext><mrow><mover accent="true"><mrow><mi mathvariant="bold-italic">h</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow><mo stretchy="true">→</mo></mover></mrow><mo>+</mo><mover accent="true"><mrow><mi mathvariant="bold-italic">b</mi><msub><mrow></mrow><mi>h</mi></msub></mrow><mo stretchy="true">→</mo></mover><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="48">其中, <mathml id="49"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mrow><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mrow><mi>h</mi><mi>x</mi></mrow></msub></mrow><mo stretchy="true">→</mo></mover></mrow></math></mathml>是输入层到隐藏层的权重矩阵, <mathml id="50"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mrow><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mrow><mi>h</mi><mi>h</mi></mrow></msub></mrow><mo stretchy="true">→</mo></mover></mrow></math></mathml>是隐藏层之间的权重矩阵, <i>σ</i>是sigmoid函数。除了输入<b><i>x</i></b><sub><i>t</i></sub>之外, 传递前一时刻的隐藏激活<mathml id="51"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mrow><mi mathvariant="bold-italic">h</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow><mo stretchy="true">→</mo></mover></mrow></math></mathml>来影响当前时刻的隐藏输出。附加的循环层通过从t=T到t=1来计算隐藏状态的反向序列:</p>
                </div>
                <div class="p1">
                    <p id="52" class="code-formula">
                        <mathml id="52"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi mathvariant="bold-italic">h</mi><mo>←</mo></mover><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mi>σ</mi><mo stretchy="false"> (</mo><mover accent="true"><mrow><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mrow><mi>h</mi><mi>x</mi></mrow></msub></mrow><mo stretchy="true">←</mo></mover><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>t</mi></msub><mo>+</mo><mrow><mover accent="true"><mrow><mi mathvariant="bold-italic">W</mi><msub><mrow></mrow><mrow><mi>h</mi><mi>h</mi></mrow></msub></mrow><mo stretchy="true">←</mo></mover></mrow><mtext> </mtext><mrow><mover accent="true"><mrow><mi mathvariant="bold-italic">h</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow><mo stretchy="true">←</mo></mover></mrow><mo>+</mo><mover accent="true"><mrow><mi mathvariant="bold-italic">b</mi><msub><mrow></mrow><mi>h</mi></msub></mrow><mo stretchy="true">←</mo></mover><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="area_img" id="53">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201906040_053.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 RNN在时间上展开的示意图" src="Detail/GetImg?filename=images/JSJC201906040_053.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图1 RNN在时间上展开的示意图</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201906040_053.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="54">本文用的是一个深层结构, 其中堆叠着多个双向RNN, 每个帧<i>t</i>的前向和后向输出<mathml id="55"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false">[</mo><mover accent="true"><mrow><mi mathvariant="bold-italic">h</mi><msub><mrow></mrow><mi>t</mi></msub></mrow><mo stretchy="true">→</mo></mover><mo>, </mo><mover accent="true"><mi mathvariant="bold-italic">h</mi><mo>←</mo></mover><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">]</mo></mrow></math></mathml>是下一个双向层的输入。假设所有N层使用同一种激活函数, 隐藏矢量序列<b><i>h</i></b><sup><i>n</i></sup>的迭代计算如下:</p>
                </div>
                <div class="p1">
                    <p id="56"><b><i>h</i></b><mathml id="57"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>t</mi><mi>n</mi></msubsup></mrow></math></mathml>=<i>σ</i> (<b><i>W</i></b><sub><i>h</i><sup><i>n</i>-1</sup><i>h</i><sup><i>n</i></sup></sub><b><i>h</i></b><mathml id="58"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>t</mi><mrow><mi>n</mi><mo>-</mo><mn>1</mn></mrow></msubsup></mrow></math></mathml>+<b><i>W</i></b><sub><i>h</i><sup><i>n</i></sup><i>h</i><sup><i>n</i></sup></sub><b><i>h</i></b><mathml id="59"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow><mi>n</mi></msubsup></mrow></math></mathml>+<b><i>b</i></b><mathml id="60"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>h</mi><mi>n</mi></msubsup></mrow></math></mathml>)      (3) </p>
                </div>
                <div class="p1">
                    <p id="61">其中, <i>n</i>=1, 2, …, <i>N</i>, <i>t</i>=1, 2, …, <i>T</i>, 定义<b><i>h</i></b><sup>0</sup>=<b><i>x</i></b>, 网络的输出<b><i>y</i></b><sub><i>t</i></sub>为:</p>
                </div>
                <div class="p1">
                    <p id="62"><b><i>y</i></b><sub><i>t</i></sub>=<b><i>W</i></b><sub><i>h</i><sup><i>N</i></sup><i>y</i></sub><b><i>h</i></b><mathml id="63"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>t</mi><mi>Ν</mi></msubsup></mrow></math></mathml>+<b><i>b</i></b><sub><i>y</i></sub>      (4) </p>
                </div>
                <div class="p1">
                    <p id="64">RNN适用于序列建模任务, 但其存在随时间呈指数增加或减小的梯度问题, 难以在长时间序列任务上训练, 实际上只能模拟短程效应。LSTM很好地解决了这个问题, 其已经在多种ASR任务上取得了优于DNN的结果<citation id="144" type="reference"><link href="15" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="65">虽然LSTM的建模能力很强大, 但其需要对大量的网络参数进行优化, 因此LSTM网络的训练较缓慢, 尤其是BLSTM, 需要较高的计算能力。文献<citation id="145" type="reference">[<a class="sup">7</a>]</citation>提出带投影层的长短时记忆 (Long Short-Term Memory Projection, LSTMP) 网络结构, 即在LSTM层的顶部都具有单独的线性投影层, 这个投影矢量比LSTM的输出具有更小的尺寸, 结果表明LSTMP可以更好地利用模型参数降低错误率, 并加快训练速度<citation id="146" type="reference"><link href="17" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="66">给定一个输入序列<b><i>X</i></b>= (<b><i>x</i></b><sub>1</sub>, <b><i>x</i></b><sub>2</sub>, …, <b><i>x</i></b><sub><i>t</i></sub>) , LSTMP网络从<i>t</i>=1到<i>t</i>=<i>T</i>进行迭代计算, 具体如下:</p>
                </div>
                <div class="p1">
                    <p id="67"><b><i>i</i></b><sub><i>t</i></sub>=<i>σ</i> (<b><i>W</i></b><sub><i>ix</i></sub><b><i>x</i></b><sub><i>t</i></sub>+<b><i>W</i></b><sub><i>ip</i></sub><b><i>p</i></b><sub><i>t</i>-1</sub>+<b><i>W</i></b><sub><i>ic</i></sub><b><i>c</i></b><sub><i>t</i>-1</sub>+<b><i>b</i></b><sub><i>i</i></sub>)      (5) </p>
                </div>
                <div class="p1">
                    <p id="68"><b><i>f</i></b><sub><i>t</i></sub>=<i>σ</i> (<b><i>W</i></b><sub><i>fx</i></sub><b><i>x</i></b><sub><i>t</i></sub>+<b><i>W</i></b><sub><i>fp</i></sub><b><i>p</i></b><sub><i>t</i>-1</sub>+<b><i>W</i></b><sub><i>fc</i></sub><b><i>c</i></b><sub><i>t</i>-1</sub>+<b><i>b</i></b><sub><i>f</i></sub>)      (6) </p>
                </div>
                <div class="p1">
                    <p id="69"><b><i>a</i></b><sub><i>t</i></sub>=<i>g</i> (<b><i>W</i></b><sub><i>cx</i></sub><b><i>x</i></b><sub><i>t</i></sub>+<b><i>W</i></b><sub><i>cp</i></sub><b><i>p</i></b><sub><i>t</i>-1</sub>+<b><i>b</i></b><sub><i>c</i></sub>)      (7) </p>
                </div>
                <div class="p1">
                    <p id="70"><b><i>c</i></b><sub><i>t</i></sub>=<b><i>f</i></b><sub><i>t</i></sub><b><i>c</i></b><sub><i>t</i>-1</sub>+<b><i>i</i></b><sub><i>t</i></sub><b><i>a</i></b><sub><i>t</i></sub>      (8) </p>
                </div>
                <div class="p1">
                    <p id="71"><b><i>o</i></b><sub><i>t</i></sub>=<i>σ</i> (<b><i>W</i></b><sub><i>ox</i></sub><b><i>x</i></b><sub><i>t</i></sub>+<b><i>W</i></b><sub><i>op</i></sub><b><i>p</i></b><sub><i>t</i>-1</sub>+<b><i>W</i></b><sub><i>oc</i></sub><b><i>c</i></b><sub><i>t</i></sub>+<b><i>b</i></b><sub>0</sub>)      (9) </p>
                </div>
                <div class="p1">
                    <p id="72"><b><i>p</i></b><sub><i>t</i></sub>=<b><i>W</i></b><sub><i>pm</i></sub> (<b><i>o</i></b><sub><i>t</i></sub><i>h</i> (<b><i>c</i></b><sub><i>t</i></sub>) )      (10) </p>
                </div>
                <div class="p1">
                    <p id="73"><b><i>y</i></b><sub><i>t</i></sub>=softmax (<b><i>W</i></b><sub><i>yp</i></sub><b><i>p</i></b><sub><i>t</i></sub>+<b><i>b</i></b><sub><i>y</i></sub>)      (11) </p>
                </div>
                <div class="p1">
                    <p id="74">其中, <b><i>i</i>、<i>f</i>、<i>o</i>、<i>a</i></b>和<b><i>c</i></b>分别代表输入门、忘记门、输出门、单元输入激活和单元状态向量, <b><i>W</i></b>和<b><i>b</i></b>分别表示权重矩阵和偏置向量, <b><i>W</i></b><sub><i>ic</i></sub>、<b><i>W</i></b><sub><i>fc</i></sub>和<b><i>W</i></b><sub><i>oc</i></sub>是各个门和窥孔 (Peephole) 连接的对角线权重矩阵, <i>g</i>和<i>h</i>是输入和输出激活函数, 本文采用tanh函数。</p>
                </div>
                <h3 id="75" name="75" class="anchor-tag">2 基于连接时序分类的语音识别系统</h3>
                <div class="p1">
                    <p id="76"><i>ASR</i>利用神经网络进行声学建模, 首先必须选择声学建模的基本单元, 例如<i>HMM</i>状态、音素、字符等, 训练时通常需要预先和标签进行对齐, 然后以目标函数作为训练的标准。如图2所示, 与混合深度学习语音识别系统不同, 本文<i>DBLSTM</i>-<i>CTC</i>系统未使用<i>CE</i>准则来训练帧级标签, 而是专注于端到端的训练, 采用<i>CTC</i>的目标函数, 并使用基于加权有限状态机 (<i>Weighted Finite State Transducers</i>, <i>WFST</i>) 的方法进行解码<citation id="147" type="reference"><link href="19" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>。</p>
                </div>
                <div class="area_img" id="77">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201906040_077.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 DBLSTM-CTC语音识别系统示意图" src="Detail/GetImg?filename=images/JSJC201906040_077.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图2 DBLSTM-CTC语音识别系统示意图</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201906040_077.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="78" name="78">2.1 连接时序分类</h4>
                <div class="p1">
                    <p id="79">CTC使用RNN来学习序列标记, 可直接对语音特征和标签之间的映射进行建模, 而不依赖于音频序列和标签序列之间的对齐。CTC的输入来自RNN的softmax层的输出, 假设训练数据中的标签序列包含<i>k</i>个标签, softmax层中的节点与标签序列相对应, 则需额外添加一个空白标签用于估计特定时刻不输出标签的概率。因此, 来自网络的输出标签概率定义了包括空白标签在内所有可能的输入序列标签的概率分布。</p>
                </div>
                <div class="p1">
                    <p id="80">给定一个长度为<i>T</i>的系统输入序列<b><i>X</i></b>, 在<i>t</i>时刻softmax层输出标签或者空白的索引<i>k</i>的概率为:</p>
                </div>
                <div class="p1">
                    <p id="81" class="code-formula">
                        <mathml id="81"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>k</mi><mo stretchy="false">|</mo><mi>t</mi><mo>, </mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mrow><mi>exp</mi><mo stretchy="false"> (</mo><mi>y</mi><msubsup><mrow></mrow><mi>t</mi><mi>k</mi></msubsup><mo stretchy="false">) </mo></mrow><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><msup><mi>k</mi><mo>′</mo></msup></munder><mtext>e</mtext></mstyle><mtext>x</mtext><mtext>p</mtext><mo stretchy="false"> (</mo><mi>y</mi><msubsup><mrow></mrow><mi>t</mi><msup><mi>k</mi><mo>′</mo></msup></msubsup><mo stretchy="false">) </mo></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="82">概率<i>p</i> (<b><i>z</i></b><sup><i>l</i></sup>|<b><i>x</i></b>) 是每个时间步的输出概率的乘积, 计算过程如下:</p>
                </div>
                <div class="p1">
                    <p id="83" class="code-formula">
                        <mathml id="83"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>p</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">z</mi><msup><mrow></mrow><mi>l</mi></msup><mo stretchy="false">|</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∏</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>Τ</mi></munderover><mi>Ρ</mi></mstyle><mo stretchy="false"> (</mo><mi>k</mi><mo stretchy="false">|</mo><mi>t</mi><mo>, </mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="84">其中, <b><i>z</i></b><sup><i>l</i></sup>是网格编码的<b><i>x</i></b>和<b><i>l</i></b>所有可能的对齐, 其允许存在重复标签和空白标签。因此, 一个音频序列存在许多可能的路径与其对应, 将这些路径映射到转录, 去掉重复的标签和空白的路径。标签序列<b><i>l</i></b>是映射到<b><i>l</i></b>的所有可能CTC路径的集合, 其概率是所有路径概率的总和:</p>
                </div>
                <div class="p1">
                    <p id="85" class="code-formula">
                        <mathml id="85"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>p</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">l</mi><mo stretchy="false">|</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>p</mi><mo>∈</mo><mtext>ϕ</mtext><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">l</mi><mo stretchy="false">) </mo></mrow></munder><mi>p</mi></mstyle><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">z</mi><mo stretchy="false">|</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>4</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="86">其中, ϕ (<b><i>l</i></b>) 是与<b><i>l</i></b>对应的CTC路径的集合, 这是一个多对一的映射, 因为多个CTC路径可以对应相同的标签序列。<i>p</i> (<b><i>l</i></b>|<b><i>x</i></b>) 可以通过前向-后向算法得到<citation id="148" type="reference"><link href="21" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="87">CTC的损失函数定义为每个训练样本正确标记的负对数概率之和, 而且函数可微, 因此可通过反向传播算法训练CTC网络。损失函数的具体计算如下:</p>
                </div>
                <div class="p1">
                    <p id="88" class="code-formula">
                        <mathml id="88"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><msub><mrow></mrow><mrow><mtext>C</mtext><mtext>Τ</mtext><mtext>C</mtext></mrow></msub><mo>=</mo><mo>-</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><mo>, </mo><mi mathvariant="bold-italic">l</mi><mo stretchy="false">) </mo></mrow></munder><mrow><mi>ln</mi></mrow></mstyle><mtext> </mtext><mi>p</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">z</mi><msup><mrow></mrow><mi>l</mi></msup><mo stretchy="false">|</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">) </mo><mo>=</mo><mo>-</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><mo>, </mo><mi mathvariant="bold-italic">l</mi><mo stretchy="false">) </mo></mrow></munder><mi>L</mi></mstyle><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><mo>, </mo><mi mathvariant="bold-italic">z</mi><msup><mrow></mrow><mi>l</mi></msup><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>5</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="89">CTC和传统标签框架的主要区别有2点:</p>
                </div>
                <div class="p1">
                    <p id="90">1) 当输出不确定时, 额外添加的空白标签可以减少网络在一帧进行标签预测的概率。</p>
                </div>
                <div class="p1">
                    <p id="91">2) CTC的训练标准是对状态序列的对数概率进行优化, 而不是输入的对数似然。</p>
                </div>
                <h4 class="anchor-tag" id="92" name="92">2.2 基于WFST的解码方法</h4>
                <div class="p1">
                    <p id="93">网络经过<i>CTC</i>的训练后, 需要有算法对其进行解码。贪婪搜索方法无须添加任何语言信息, 选取每一个时间节点最大概率输出, 搜索最佳的路径p∈L<sup>′T</sup>, 具体如下:</p>
                </div>
                <div class="p1">
                    <p id="94" class="code-formula">
                        <mathml id="94"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mtext>a</mtext><mtext>r</mtext><mtext>g</mtext><mtext>m</mtext><mtext>a</mtext><mtext>x</mtext></mrow></mstyle><mi>p</mi></munder><mstyle displaystyle="true"><munderover><mo>∏</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>Τ</mi></munderover><mi>Ρ</mi></mstyle><msubsup><mrow></mrow><mrow><mi>A</mi><mi>Μ</mi></mrow><mi>t</mi></msubsup><mo stretchy="false"> (</mo><mi>p</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">|</mo><mi mathvariant="bold-italic">X</mi><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>6</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="95">在<i>CTC</i>训练过程中, 声学模型具有语言模型的属性, 其在不添加外部语言信息的情况下解码效果良好。然而, 用大型文本语料库训练外部语言模型是获得最佳结果的必要条件。2种模型都包含语言模型的特性, 可能会出现性能相互影响的情况。以往的工作引入多种方法进行解码, 但这些方法有的不能整合单词级语言模型, 有的需在约束条件下实现整合<citation id="149" type="reference"><link href="7" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>。此外, 由于空白标签的存在, <i>CTC</i>声学模型的高效解码一直是一个难题。</p>
                </div>
                <div class="p1">
                    <p id="96">本文采用基于<i>WFST</i>的解码方法, <i>CTC</i>标签、词典和语言模型都被编码成<i>WFST</i>, 从而组成一个全面的搜索图。用扩展标签集L′、每个单元的先验概率来对概率序列进行预处理<citation id="150" type="reference"><link href="23" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>, 具体如下:</p>
                </div>
                <div class="p1">
                    <p id="97">p (<b><i>x</i></b>|<i>k</i>) ∝<i>P</i> (<i>k</i>|<b><i>x</i></b>) /<i>P</i> (<i>k</i>)      (17) </p>
                </div>
                <div class="p1">
                    <p id="98">WFST的搜索图由3个单独的部分组成:</p>
                </div>
                <div class="p1">
                    <p id="99">1) 语法 (Grammar) 。WFST基于n-gram形式的语言模型对符合的单词序列进行编码。</p>
                </div>
                <div class="p1">
                    <p id="100">2) 标注符号 (Token) 。WFST通过多对一的映射函数ϕ (<b><i>l</i></b>) , 将扩展标签集<i>L</i>′中的单元映射到标签序列<i>L</i>中的每个单元。</p>
                </div>
                <div class="p1">
                    <p id="101">3) 词典 (Lexicon) 。WFST将标签序列<i>L</i>的单元序列映射到单词。</p>
                </div>
                <div class="p1">
                    <p id="102">编译完上述3个独立的WFST, 再组合成一个全面的搜索图, 具体如下:</p>
                </div>
                <div class="p1">
                    <p id="103"><i>S</i>=<i>T</i>。 min (det (<i>L</i>。 <i>G</i>) )      (18) </p>
                </div>
                <div class="p1">
                    <p id="104">其中, 。 、min、det分别代表构图、最小化、确定化算法, 搜索图<i>S</i>将由语音帧得到的CTC标签序列映射为字序列。该方法提供了一种处理空白标签的便捷方式, 可以高效地将词语模型并入CTC解码中, 提升解码性能, 解码效率优于传统的HMM模型。</p>
                </div>
                <h3 id="105" name="105" class="anchor-tag">3 实验结果与分析</h3>
                <div class="p1">
                    <p id="106">实验采用清华大学开源的中文普通话语料库<i>THCHS</i>-30<citation id="151" type="reference"><link href="25" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>, 该语料库使用单个麦克风, 在室内安静环境下以16 <i>kHz</i>的采样频率和16 <i>bit</i>的采样大小进行录制。声学模型的训练集选取时长25 <i>h</i>的10 000句发音, 测试集时长6.24 <i>h</i>的2 495句发音, 而验证集选取时长1.9 <i>h</i>的893句发音来交叉验证训练效果。语言模型为三元语音模型 (3-<i>gram</i>) , 由一个从中文<i>Gigaword</i>语料库中随机选取的文本集合训练而成。</p>
                </div>
                <h4 class="anchor-tag" id="107" name="107">3.1 基线系统</h4>
                <div class="p1">
                    <p id="108">实验总共设置<i>GMM</i>-<i>HMMs</i>、<i>DNN</i>-<i>HMMs</i>以及<i>BLSTM</i>-<i>HMMs</i> 3种语音识别系统。</p>
                </div>
                <h4 class="anchor-tag" id="109" name="109">3.1.1 <i>GMM</i>-<i>HMMs</i>系统</h4>
                <div class="p1">
                    <p id="110">基于<i>GMM</i>的隐马尔科夫语音识别系统首先用标准的13维梅尔频率倒谱系数和一阶、二阶导数来训练一个单音素的模型, 采用倒谱均值归一化来减轻信道噪声的影响。再基于单音素模型, 通过线性判别式分析 (<i>Linear Discriminant Analysis</i>, <i>LDA</i>) 和最大似然线性变换进行特征转换, 构建三音素的系统。</p>
                </div>
                <h4 class="anchor-tag" id="111" name="111">3.1.2 <i>DNN</i>-<i>HMMs</i>系统</h4>
                <div class="p1">
                    <p id="112">基于<i>DNN</i>的语音识别系统通过<i>GMM</i>-<i>HMMs</i>获得强制对齐训练语料后的帧级标注, 在<i>CE</i>准则下训练。输入特征是40维的<i>filterbank</i>加一阶、二阶导数, 每帧左右各拼接3帧, 共输入7帧来提高<i>DNN</i>模型的区分度, 拼接特征经过<i>LDA</i>处理后减少为200维。<i>DNN</i>由4个隐层构成, 每层有1 024个节点, 输出层有3 386个节点。初始学习率设定为8<i>e</i>-3, 最小批处理数为256帧。</p>
                </div>
                <h4 class="anchor-tag" id="113" name="113">3.1.3 <i>BLSTM</i>-<i>HMMs</i>系统</h4>
                <div class="p1">
                    <p id="114">由于语音场景的特殊性, 利用未来帧的信息通常能提高当前帧的识别准确率。本次实验的实时性要求不高, 双向神经网络比单向神经网络的性能更好<citation id="152" type="reference"><link href="27" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>, 因此采用<i>BLSTM</i>网络, 其中的神经网络单元为<i>LSTMP</i>。系统输入特征是40维的<i>filterbank</i>加一阶和二阶导数, 网络正向、反向隐层各有2层, 每层有1 024个节点, 输出层有3 386个节点。网络初始学习率为5<i>e</i>-4, 动量参数设定为0.9, 并采用多句并行来加快训练速度。</p>
                </div>
                <h4 class="anchor-tag" id="115" name="115">3.2 端到端语音识别系统</h4>
                <div class="p1">
                    <p id="116">端到端的语音识别系统中所用的网络结构和<i>BLSTM</i>-<i>HMMs</i>相同, 输入特征是40维的<i>filterbank</i>加一阶和二阶导数, 而<i>CTC</i>的输出为217个节点, 其中包括216个声韵母标签和1个空白标签。因为<i>CTC</i>不需要上下文决策树来获得良好的性能, 所以将上下文无关的音素作为目标。</p>
                </div>
                <div class="p1">
                    <p id="117">以音素为标签的<i>CTC</i>允许模型每30 <i>ms</i>输出一次, 而不是传统的10 <i>ms</i>, 较低的帧率可以减少输入和输出序列的长度, 降低解码过程的计算成本, 并可提供延迟的改进方法<citation id="153" type="reference"><link href="27" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>。因此, 系统的输入将3个10 <i>ms</i>的帧堆叠在一起, 处理完一次堆叠帧可跳过3帧, 如图3所示。解码过程中的声学得分评估每30 <i>ms</i>发生一次, 其速度大幅提高。</p>
                </div>
                <div class="area_img" id="118">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201906040_118.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 输入帧的堆叠" src="Detail/GetImg?filename=images/JSJC201906040_118.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图3 输入帧的堆叠</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201906040_118.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="119">神经网络的模型参数初始值从[-0.1, 0.1]的均匀分布范围中随机抽取, 初始学习率设定为4e-5, 网络中的错误由CTC反向传播。通常, LSTM参数在一个区间内被初始化为较小的随机值, 现有的大多数工作将遗忘门的偏置矢量初始化为0或较小的随机权重, 然而这是次优选择, 其以小权重有效地关闭门, 防止单元记忆及其梯度及时流入<citation id="154" type="reference"><link href="29" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>。本文将遗忘门的偏置初始化为1, 使信息更容易流动传递。</p>
                </div>
                <h4 class="anchor-tag" id="120" name="120">3.3 实验结果</h4>
                <div class="p1">
                    <p id="121">本文实验使用的机器配置为Intel Xeon E5-2640 v4 CPU, Nvidia Tesla M40 GPU, 内存128 GB。每组实验分别进行5次, 取平均值作为最后的结果。</p>
                </div>
                <h4 class="anchor-tag" id="122" name="122">3.3.1 端到端系统与基线系统的比较</h4>
                <div class="p1">
                    <p id="123">表1给出端到端系统与基线系统的性能对比, 以词错误率 (Word Error Rate, WER) 为评价指标。由表1可知, 基线系统中BLSTM-HMMs模型相对于GMM-HMMs、DNN-HMMs模型的表现更好, <i>WER</i>值分别降低25%、11%, 说明LSTM的建模能力更强。此外, 端到端系统性能稍差于基线系统, 在端到端系统中, LSTM的遗忘门偏置初始化为1时比随机初始化时的<i>WER</i>值低3.1%。</p>
                </div>
                <div class="area_img" id="124">
                    <p class="img_tit"><b>表1 基线系统与端到端系统的性能对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="124" border="1"><tr><td>系统</td><td>模型</td><td>建模单元</td><td>遗忘门偏置</td><td><i>WER</i>值/%</td></tr><tr><td rowspan="3"><br />基线<br />系统</td><td>GMM-HMMs</td><td>状态</td><td>—</td><td>28.07±0.12</td></tr><tr><td><br />DNN-HMMs</td><td>状态</td><td>—</td><td>23.65±0.08</td></tr><tr><td><br />BLSTM-HMMs</td><td>状态</td><td>—</td><td>21.12±0.06</td></tr><tr><td rowspan="2"><br />端到端<br />系统</td><td>BLSTM-CTC</td><td>音素</td><td>小的随机值</td><td>26.07±0.13</td></tr><tr><td><br />BLSTM-CTC</td><td>音素</td><td>1</td><td>25.35±0.11</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="125">图4给出相同解码搜索空间裁剪下, 4种声学模型解码时间的对比。其中, BLSTM-CTC2表示遗忘门偏置为1。在图4中, BLSTM-CTC模型的解码速度比DNN-HMMs、BLSTM-HMMs模型快, 说明在相同硬件配置条件下, 端到端系统的解码速度快于基线系统。端到端系统使用的基于WFST的解码方法不再需要HMM模型, 构建的解码搜索空间要比传统解码方法小, 使得解码效率大幅提升。而CTC的解码在空白段时, 搜索空间可大幅缩小, 速度提高。</p>
                </div>
                <div class="area_img" id="126">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201906040_126.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 4种声学模型的解码时间对比" src="Detail/GetImg?filename=images/JSJC201906040_126.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图4 4种声学模型的解码时间对比</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201906040_126.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="127" name="127">3.3.2 不同LSTM隐藏层的比较</h4>
                <div class="p1">
                    <p id="128">表1结果显示2层BLSTM的端到端系统比基线系统的性能要差, 提高神经网络建模能力的关键是让网络层数加深。表2给出网络层数不同的条件下, 端到端系统性能的对比, 结果表明, 随着网络层数从2层增加到3层, 系统的<i>WER</i>值降低10.6%左右。当网络层数增加到4层时, 系统性能减弱, 这是由于训练语料不足, 参数训练不充分导致网络欠拟合, 因此其鲁棒性下降<citation id="155" type="reference"><link href="33" rel="bibliography" /><link href="35" rel="bibliography" /><sup>[<a class="sup">16</a>,<a class="sup">17</a>]</sup></citation>。</p>
                </div>
                <div class="area_img" id="129">
                    <p class="img_tit"><b>表2 不同网络层数下BLSTM-CTC模型的性能对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="129" border="1"><tr><td><br />LSTM层数</td><td>遗忘门偏置</td><td><i>WER</i>值/%</td></tr><tr><td rowspan="2"><br />2</td><td><br />小的随机值</td><td>26.07±0.13</td></tr><tr><td><br />1</td><td>25.35±0.11</td></tr><tr><td rowspan="2"><br />3</td><td><br />小的随机值</td><td>23.29±0.17</td></tr><tr><td><br />1</td><td>22.68±0.08</td></tr><tr><td rowspan="2"><br />4</td><td><br />小的随机值</td><td>28.35±0.06</td></tr><tr><td><br />1</td><td>27.53±0.11</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="130" name="130">3.3.3 普通话的音调特征</h4>
                <div class="p1">
                    <p id="131">在基线系统中, 输入通常可以用GMM模型学习特征或者额外附加特征来增强模型性能。声音中的音调 (pitch) 特征对带有声调的语音, 例如普通话和粤语等的识别是有利的<citation id="156" type="reference"><link href="31" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>。因此, 将音调特征引入具有3个隐藏层的CTC模型中, 在每一帧上, 将三维音调添加到40维滤波器组特征中, 由此得到43维特征向量。在BLSTM-CTC模型上进行实验, 遗忘门偏置设为1, 结果如表3所示。由表3可知, 音调特征的添加使系统性能有所提升。</p>
                </div>
                <div class="area_img" id="132">
                    <p class="img_tit"><b>表3 添加音调特征前后的性能对比</b> % <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="132" border="1"><tr><td><br />特征</td><td><i>WER</i>值</td></tr><tr><td><br />filterbank</td><td>22.68±0.08</td></tr><tr><td><br />filterbank+pitch</td><td>21.87±0.12</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="133" name="133">3.3.4 序列区分度训练</h4>
                <div class="p1">
                    <p id="134">CE和CTC准则对语音识别中词错误率降低的作用有限, 区分度训练准则在解码过程中可以引入词汇和语音模型约束, 进一步提高由CE准则训练的声学模型的性能<citation id="157" type="reference"><link href="27" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>。本文应用状态级最小风险贝叶斯 (state level Minimum Bayes Risk, sMBR) 准则<citation id="158" type="reference"><link href="37" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>对使用CE和CTC准则初始化的声学模型进行序列区分度训练, sMBR准则的梯度可以通过最短路径算法进行计算。</p>
                </div>
                <div class="p1">
                    <p id="135">表4给出3种声学模型进行sMBR训练后系统性能的对比。由表4可知, 相对于初始模型, sMBR训练后的系统性能提升了大约11%。结合声学模型和语言模型来优化, CTC系统的效果明显提升。</p>
                </div>
                <div class="area_img" id="136">
                    <p class="img_tit"><b>表4 序列区分度训练前后系统的性能对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="136" border="1"><tr><td>模型</td><td>建模单元</td><td>是否进行<br />sMBR训练</td><td><i>WER</i>值/%</td></tr><tr><td rowspan="2"><br />DNN-HMMs</td><td rowspan="2">状态</td><td><br />否</td><td>23.65±0.08</td></tr><tr><td><br />是</td><td>21.50±0.13</td></tr><tr><td rowspan="2"><br />BLSTM-HMMs</td><td rowspan="2">状态</td><td><br />否</td><td>21.12±0.06</td></tr><tr><td><br />是</td><td>19.53±0.06</td></tr><tr><td rowspan="2"><br />BLSTM-CTC2</td><td rowspan="2">音素</td><td><br />否</td><td>21.87±0.12</td></tr><tr><td><br />是</td><td>19.09±0.16</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h3 id="137" name="137" class="anchor-tag">4 结束语</h3>
                <div class="p1">
                    <p id="138">本文搭建了基于BLSTM的CTC端到端语音识别系统, 并将其应用于中文普通话的识别。采用WFST的解码方法, 将语言模型引入解码过程中。在前端声学特征处理时加入音调特征, 使其更加符合普通话的语音特点。实验结果表明, 与基线系统相比, 该系统的识别效率较高, 识别准确率相当。下一步将改善由于训练语料不足带来的模型欠拟合问题, 同时融合多流特征输入和dropout策略来提升系统鲁棒性。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups">

                                <b>[1]</b> HINTON G, DENG Li, YU Dong, et al.Deep neural networks for acoustic modeling in speech recognition:the shared views of four research groups[J].IEEE Signal Processing Magazine, 2012, 29 (6) :82-97.
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJA2016S2010&amp;v=MTgyMTE0TzN6cXFCdEdGckNVUkxPZVplUm9GeW5sVmJyTUx6N0JiN0c0SDlldnJZOUVaSVFLREg4NHZSNFQ2ajU=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> 李伟林, 文剑, 马文凯.基于深度神经网络的语音识别系统研究[J].计算机科学, 2016, 43 (11A) :45-49.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Towards end-to-end speech recognition with recurrent neural networks">

                                <b>[3]</b> GRAVES A, JAITLY N.Towards end-to-end speech recognition with recurrent neural networks[EB/OL].[2018-03-09].http://proceedings.mlr.press/v32/graves14.pdf.
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJBK&amp;filename=SJBK15090500014198&amp;v=MTIyNzJRSC9pclJkR2VycVFUTW53WmVadUh5am1VTG5JSVY0U2J4WT1OaWZKWmJLOUh0ak1xbzlGWk9vTERYVXhvQk1UNlQ0UA==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b> HOCHREITER S, SCHMIDHUBER J.Long short-term memory[J].Neural Computation, 1997, 9 (8) :1735-1780.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Connectionist temporal classification:Labelling unsegmented sequence data with recurrent neural networks">

                                <b>[5]</b> GRAVES A, FERNÁNDEZ S, GOMEZ F, et al.Connectionist temporal classification:labelling unsegmented sequence data with recurrent neural networks[C]//Proceedings of the 23rd International Conference on Machine Learning.New York, USA:ACM Press, 2006:369-376.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Towards end-to-end speech recognition for Chinese mandarin using long short-term memory recurrent neural networks">

                                <b>[6]</b> LI Jie, ZHANG Heng, CAI Xinyuan, et al.Towards end-to-end speech recognition for Chinese mandarin using long short-term memory recurrent neural networks[C]//Proceedings of the 16th Annual Conference of International Speech Com-munication Association.Baixas, France:International Speech Communication Association, 2015:1-5.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Long short-term memory based recurrent neural network architectures for large vocabulary speech recognition">

                                <b>[7]</b> SAK H, SENIOR A, BEAUFAYS F.Long short-term memory based recurrent neural network architectures for large vocabulary speech recognition[EB/OL].[2018-03-09].https://arxiv.org/pdf/1402.1128v1.pdf.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZDHB201703002&amp;v=MTYyNDBGckNVUkxPZVplUm9GeW5sVmJyTVB5bkRiTEc0SDliTXJJOUZab1FLREg4NHZSNFQ2ajU0TzN6cXFCdEc=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b> YU Dong, LI Jinyu.Recent progresses in deep learning based acoustic models[J].IEEE/CAA Journal of Automatica Sinica, 2017, 4 (3) :396-409.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=EESEN:End-to-end speech recognition using deep RNN models and WFST-based decoding">

                                <b>[9]</b> MIAO Yajie, GOWAYYED M, METZE F.EESEN:end-to-end speech recognition using deep RNN models and WFS T-based decoding[C]//Proceedings of 2015 IEEE Workshop on Automatic Speech Recognition and Understanding.Washington D.C., USA:IEEE Press, 2016:167-174.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_10" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=WXYJ201708010&amp;v=MDcyNzBiTXA0OUVaSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm9GeW5sVmJyTU1qWFNaTEc0SDk=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[10]</b> 黎长江, 胡燕.基于循环神经网络的音素识别研究[J].微电子学与计算机, 2017, 34 (8) :47-51.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Comparison of decoding strategies for CTC acoustic models">

                                <b>[11]</b> ZENKEL T, SANABRIA R, METZE F, et al.Comparison of decoding strategies for CTC acoustic models[EB/OL].[2018-03-09].https://arxiv.org/pdf/1708.04469.pdf.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Thchs-30:A free chinese speech corpus">

                                <b>[12]</b> WANG Dong, ZHANG Xuewei.Thchs-30:A free chinese speech corpus[EB/OL].[2018-03-09].https://arxiv.org/pdf/1512.01882.pdf.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Lower frame rate neural network acoustic models">

                                <b>[13]</b> PUNDAK G, SAINATH T N.Lower frame rate neural network acoustic models[EB/OL].[2018-03-09].https://storage.googleapis.com/pub-tools-public-publication-data/pdf/45555.pdf.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=An empirical explo-ration of CTC acoustic models">

                                <b>[14]</b> MIAO Yajie, GOWAYYED M, NA Xingyu, et al.An empirical exploration of CTC acoustic models[C]//Proceedings of International Conference on Acoustics, Speech and Signal Processing.Washington D.C., USA:IEEE Press, 2016:2623-2627.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A pitch extraction algorithm tuned for automatic speech recognition">

                                <b>[15]</b> GHAHREMANI P, BABAALI B, POVEY D, et al.A pitch extraction algorithm tuned for automatic speech recognition[C]//Proceedings of International Conference on Acoustics, Speech and Signal Processing.Washington D.C., USA:IEEE Press, 2014:2494-2498.
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_16" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201609049&amp;v=MjY1MTRubFZick1MejdCZDdHNEg5Zk1wbzlCYllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJvRnk=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[16]</b> 秦楚雄, 张连海.低资源语音识别中融合多流特征的卷积神经网络声学建模方法[J].计算机应用, 2016, 36 (9) :2609-2615.
                            </a>
                        </p>
                        <p id="35">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Improving LSTM-CTC based ASR performance in domains with limited training data">

                                <b>[17]</b> BILLA J.Improving LSTM-CTC based ASR performance in domains with limited training data[EB/OL].[2018-03-09].https://arxiv.org/pdf/1707.00722.pdf.
                            </a>
                        </p>
                        <p id="37">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Lattice-based optimization of sequence classification criteria for neural-network acoustic modeling">

                                <b>[18]</b> KINGSBURY B.Lattice-based optimization of sequence classi-fication criteria for neural-network acoustic modeling[C]//Proceedings of International Conference on Acoustics, Speech and Signal Processing.Washington D.C., USA:IEEE Press, 2009:3761-3764.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJC201906040" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201906040&amp;v=MDU2NTlLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm9GeW5sVmJyTUx6N0JiYkc0SDlqTXFZOUJaSVE=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="1" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
