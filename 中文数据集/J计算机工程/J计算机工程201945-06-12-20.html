<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637130373472020000%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJC201906003%26RESULT%3d1%26SIGN%3dcnbLkPFHZVBRYzZ9FYBmJEH1J3k%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201906003&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201906003&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201906003&amp;v=MzE5NjZPZVplUm9GeW5sVUw3Tkx6N0JiYkc0SDlqTXFZOUZaNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkw=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#33" data-title="0 概述 ">0 概述</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#36" data-title="1 相关研究 ">1 相关研究</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#41" data-title="2 算法流程 ">2 算法流程</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#43" data-title="2.1 部件检测定位模型">2.1 部件检测定位模型</a></li>
                                                <li><a href="#73" data-title="2.2 特征融合算法">2.2 特征融合算法</a></li>
                                                <li><a href="#98" data-title="2.3 重识别距离度量">2.3 重识别距离度量</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#101" data-title="3 实验结果与分析 ">3 实验结果与分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#102" data-title="3.1 实验设置">3.1 实验设置</a></li>
                                                <li><a href="#123" data-title="3.2 结果分析">3.2 结果分析</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#153" data-title="4 结束语 ">4 结束语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#40" data-title="&lt;b&gt;图1 车辆重识别算法整体框架&lt;/b&gt;"><b>图1 车辆重识别算法整体框架</b></a></li>
                                                <li><a href="#45" data-title="&lt;b&gt;图2 部件检测算法流程&lt;/b&gt;"><b>图2 部件检测算法流程</b></a></li>
                                                <li><a href="#52" data-title="&lt;b&gt;图3 区域网络示意图&lt;/b&gt;"><b>图3 区域网络示意图</b></a></li>
                                                <li><a href="#91" data-title="&lt;b&gt;图4 特征训练示意图&lt;/b&gt;"><b>图4 特征训练示意图</b></a></li>
                                                <li><a href="#92" data-title="&lt;b&gt;图5 特征提取示意图&lt;/b&gt;"><b>图5 特征提取示意图</b></a></li>
                                                <li><a href="#97" data-title="&lt;b&gt;图6 特征融合示意图&lt;/b&gt;"><b>图6 特征融合示意图</b></a></li>
                                                <li><a href="#104" data-title="&lt;b&gt;图7 数据集部分车辆图像&lt;/b&gt;"><b>图7 数据集部分车辆图像</b></a></li>
                                                <li><a href="#105" data-title="&lt;b&gt;图8 数据集分布示意图&lt;/b&gt;"><b>图8 数据集分布示意图</b></a></li>
                                                <li><a href="#128" data-title="&lt;b&gt;图9 车脸、车窗检测准确率示意图&lt;/b&gt;"><b>图9 车脸、车窗检测准确率示意图</b></a></li>
                                                <li><a href="#130" data-title="&lt;b&gt;图10 部件检测效果&lt;/b&gt;"><b>图10 部件检测效果</b></a></li>
                                                <li><a href="#134" data-title="&lt;b&gt;图11 不同特征匹配率对比&lt;/b&gt;"><b>图11 不同特征匹配率对比</b></a></li>
                                                <li><a href="#135" data-title="&lt;b&gt;图12 不同特征召回率对比&lt;/b&gt;"><b>图12 不同特征召回率对比</b></a></li>
                                                <li><a href="#139" data-title="&lt;b&gt;图13 不同特征融合方法匹配率对比&lt;/b&gt;"><b>图13 不同特征融合方法匹配率对比</b></a></li>
                                                <li><a href="#140" data-title="&lt;b&gt;图14 不同特征融合方法召回率对比&lt;/b&gt;"><b>图14 不同特征融合方法召回率对比</b></a></li>
                                                <li><a href="#147" data-title="&lt;b&gt;表1 不同车辆重识别方法匹配率&lt;/b&gt;"><b>表1 不同车辆重识别方法匹配率</b></a></li>
                                                <li><a href="#148" data-title="&lt;b&gt;表2 不同车辆重识别方法召回率&lt;/b&gt;"><b>表2 不同车辆重识别方法召回率</b></a></li>
                                                <li><a href="#149" data-title="&lt;b&gt;图15 重识别性能曲线&lt;/b&gt;"><b>图15 重识别性能曲线</b></a></li>
                                                <li><a href="#152" data-title="&lt;b&gt;图16 车辆重识别效果示意图&lt;/b&gt;"><b>图16 车辆重识别效果示意图</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" title=" MEI L T, LAM W H K.Application of automatic vehicle identification technology for real-time journey time estimation[J].Information Fusion, 2011, 12 (1) :11-19." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011300328999&amp;v=MTI0NTJIdEROckk5Rlora0hCWFV3b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VTG5JSVY0WGF4Yz1OaWZPZmJLNw==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         MEI L T, LAM W H K.Application of automatic vehicle identification technology for real-time journey time estimation[J].Information Fusion, 2011, 12 (1) :11-19.
                                    </a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" title=" 张耿宁, 王家宝, 张亚非, 等.基于特征融合的行人重识别方法[J].计算机工程与应用, 2017, 53 (12) :185-189." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSGG201712031&amp;v=MjE5MDNNYWJHNEg5Yk5yWTlHWllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJvRnlubFVMN05Mejc=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         张耿宁, 王家宝, 张亚非, 等.基于特征融合的行人重识别方法[J].计算机工程与应用, 2017, 53 (12) :185-189.
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title=" 刘娜.基于卷积神经网络的行人重识别算法[D].上海:华东师范大学, 2017." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=1017074510.nh&amp;v=MjI0MDY0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSb0Z5bmxVTDdOVkYyNkdiTy9HdFROcjVFYlBJUUtESDg0dlI=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         刘娜.基于卷积神经网络的行人重识别算法[D].上海:华东师范大学, 2017.
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" title=" CHENG Deng, GONG Yihong, ZHOU Sanping, et al.Person re-identification by multi-channel parts-based CNN with improved triplet loss function[C]//Proceedings of 2016 IEEE Conference on Computer Vision and Pattern Recognition.Las Vegas, USA:IEEE Press, 2016:1335-1344." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Person re-identification by multi-channel parts-based cnn with improved triplet loss function">
                                        <b>[4]</b>
                                         CHENG Deng, GONG Yihong, ZHOU Sanping, et al.Person re-identification by multi-channel parts-based CNN with improved triplet loss function[C]//Proceedings of 2016 IEEE Conference on Computer Vision and Pattern Recognition.Las Vegas, USA:IEEE Press, 2016:1335-1344.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" title=" TIAN Yin, DONG Honghui, JIA Limin, et al.A vehicle re-identification algorithm based on multi-sensor correlation[J].Journal of Zhejiang University-SCIENCE C (Computers and Electronics) , 2014, 15 (5) :372-382." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JZUS201405005&amp;v=MTEwNTBYTXFvOUZZWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm9GeW5sVUw3Tkx6ZmVmYkc0SDk=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                         TIAN Yin, DONG Honghui, JIA Limin, et al.A vehicle re-identification algorithm based on multi-sensor correlation[J].Journal of Zhejiang University-SCIENCE C (Computers and Electronics) , 2014, 15 (5) :372-382.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" title=" COIFMAN B.Vehicle reidentification and travel time measurement, part II:uncongested freeways and the onset of congestion[C]//Proceedings of 2001 IEEE Intelligent Transportation Systems.Oakland, USA:IEEE Press, 1999:899-917." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Vehicle reidentification and travel time measurement. I. Congested freeways">
                                        <b>[6]</b>
                                         COIFMAN B.Vehicle reidentification and travel time measurement, part II:uncongested freeways and the onset of congestion[C]//Proceedings of 2001 IEEE Intelligent Transportation Systems.Oakland, USA:IEEE Press, 1999:899-917.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" title=" 王盼盼, 李玉惠.基于特征融合和L-M算法的车辆重识别方法[J].电子科技, 2018, 4 (1) :12-15." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZKK201804005&amp;v=MjQ3MTlOSVRmQVpiRzRIOW5NcTQ5RllZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSb0Z5bmxVTDc=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                         王盼盼, 李玉惠.基于特征融合和L-M算法的车辆重识别方法[J].电子科技, 2018, 4 (1) :12-15.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" title=" ZHANG Yiheng, LIU Dong, ZHA Zhengjun.Improving triplet-wise training of convolutional neural network for vehicle re-identification[C]//Proceedings of IEEE International Conference on Multimedia and Expo.Washington D.C., USA:IEEE Press, 2017:1386-1391." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Improving triplet-wise training of convolutional neural network for ve-hicle re-identification">
                                        <b>[8]</b>
                                         ZHANG Yiheng, LIU Dong, ZHA Zhengjun.Improving triplet-wise training of convolutional neural network for vehicle re-identification[C]//Proceedings of IEEE International Conference on Multimedia and Expo.Washington D.C., USA:IEEE Press, 2017:1386-1391.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" title=" XU Qingtong, YAN Ke, TIAN Yonghong.Learning a repression network for precise vehicle search[C]//Proceedings of IEEE CVPR’17.Washington D.C., USA:IEEE Press, 2017:125-136." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning a repression network for precise vehicle search">
                                        <b>[9]</b>
                                         XU Qingtong, YAN Ke, TIAN Yonghong.Learning a repression network for precise vehicle search[C]//Proceedings of IEEE CVPR’17.Washington D.C., USA:IEEE Press, 2017:125-136.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_10" title=" ZAPLETAL D, HEROUT A.Vehicle re-identification for automatic video traffic surveillance[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition Workshops.[S.1.]:IEEE Computer Society, 2016:1568-1574." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Vehicle Re-Identification for Automatic Video Traffic Surveillance">
                                        <b>[10]</b>
                                         ZAPLETAL D, HEROUT A.Vehicle re-identification for automatic video traffic surveillance[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition Workshops.[S.1.]:IEEE Computer Society, 2016:1568-1574.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_11" title=" ZHOU Yi, LIU Li, SHAO Ling.Vehicle re-identification by deep hidden multi-view inference[J].IEEE Transactions on Image Processing, 2018, 27 (7) :3275-3287." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Vehicle re-identification by deep hidden multi-view inference">
                                        <b>[11]</b>
                                         ZHOU Yi, LIU Li, SHAO Ling.Vehicle re-identification by deep hidden multi-view inference[J].IEEE Transactions on Image Processing, 2018, 27 (7) :3275-3287.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_12" title=" REN Shaoqing, GIRSHICK R.Faster R-CNN:towards real-time object detection with region proposal networks[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017, 39 (6) :1137-1149." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Faster R-CNN:Towards Real-Time Object Detection with Region Proposal Networks">
                                        <b>[12]</b>
                                         REN Shaoqing, GIRSHICK R.Faster R-CNN:towards real-time object detection with region proposal networks[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017, 39 (6) :1137-1149.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_13" title=" SZEGEDY C, LIU Wei, JIA Yangqing, et al.Going deeper with convolutions[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2015:1-9." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Going deeper with convolutions">
                                        <b>[13]</b>
                                         SZEGEDY C, LIU Wei, JIA Yangqing, et al.Going deeper with convolutions[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2015:1-9.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_14" >
                                        <b>[14]</b>
                                     MACQUEEN J.Some methods for classification and analysis of multi-variate observations[C]//Proceedings of Berkeley Symposium on Mathematical Statistics and Probability.Berkeley, USA:California University Press, 1967:281-297.</a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_15" title=" LI Xiying, YUAN Minxian, JIANG Qianyin, et al.VRID-1:a basic vehicle re-identification dataset for similar vehicles[C]//Proceedings of IEEE International Conference on Intelligent Transportation Systems.Washington D.C., USA:IEEE Press, 2017:1-8." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=VRID-1:a basic vehicle re-identification dataset for similar vehicles">
                                        <b>[15]</b>
                                         LI Xiying, YUAN Minxian, JIANG Qianyin, et al.VRID-1:a basic vehicle re-identification dataset for similar vehicles[C]//Proceedings of IEEE International Conference on Intelligent Transportation Systems.Washington D.C., USA:IEEE Press, 2017:1-8.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJC" target="_blank">计算机工程</a>
                2019,45(06),12-20 DOI:10.19678/j.issn.1000-3428.0052284            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于部件融合特征的车辆重识别算法</b></span>
 <span class="shoufa"></span>                                     </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%8E%E7%86%99%E8%8E%B9&amp;code=10170841&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">李熙莹</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%91%A8%E6%99%BA%E8%B1%AA&amp;code=40422060&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">周智豪</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%82%B1%E9%93%AD%E5%87%AF&amp;code=40422063&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">邱铭凯</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%B1%B1%E5%A4%A7%E5%AD%A6%E6%99%BA%E8%83%BD%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0140250&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中山大学智能工程学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%B9%BF%E4%B8%9C%E7%9C%81%E6%99%BA%E8%83%BD%E4%BA%A4%E9%80%9A%E7%B3%BB%E7%BB%9F%E9%87%8D%E7%82%B9%E5%AE%9E%E9%AA%8C%E5%AE%A4&amp;code=1697914&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">广东省智能交通系统重点实验室</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E8%A7%86%E9%A2%91%E5%9B%BE%E5%83%8F%E6%99%BA%E8%83%BD%E5%88%86%E6%9E%90%E4%B8%8E%E5%BA%94%E7%94%A8%E6%8A%80%E6%9C%AF%E5%85%AC%E5%AE%89%E9%83%A8%E9%87%8D%E7%82%B9%E5%AE%9E%E9%AA%8C%E5%AE%A4&amp;code=0140250&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">视频图像智能分析与应用技术公安部重点实验室</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E8%A7%86%E9%A2%91%E5%9B%BE%E5%83%8F%E4%BF%A1%E6%81%AF%E6%99%BA%E8%83%BD%E5%88%86%E6%9E%90%E4%B8%8E%E5%85%B1%E4%BA%AB%E5%BA%94%E7%94%A8%E6%8A%80%E6%9C%AF%E5%9B%BD%E5%AE%B6%E5%B7%A5%E7%A8%8B%E5%AE%9E%E9%AA%8C%E5%AE%A4&amp;code=1514538&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">视频图像信息智能分析与共享应用技术国家工程实验室</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>针对车辆型号相同但车辆个体不同的重识别问题, 提出一种新的车辆重识别算法。运用部件检测算法获取不同车辆之间差异较大的车窗和车脸区域, 对检测到的车窗和车脸区域进行特征提取并进行融合, 生成新的融合特征, 计算图像特征之间距离度量进行分类识别。在中山大学公开数据集VRID-1上进行测试, 结果表明, 该算法的Rank1匹配率达到66.67%, 明显优于经典的传统特征表征算法, 从而验证该算法是可行且有效的。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%BD%A6%E8%BE%86%E9%87%8D%E8%AF%86%E5%88%AB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">车辆重识别;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%83%A8%E4%BB%B6%E6%A3%80%E6%B5%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">部件检测;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">特征提取;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%89%B9%E5%BE%81%E8%9E%8D%E5%90%88&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">特征融合;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%B7%9D%E7%A6%BB%E5%BA%A6%E9%87%8F&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">距离度量;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    李熙莹 (1972—) , 女, 副教授、博士, 主研方向为交通信息技术、视频图像、视频大数据技术;E-mail: stslxy@mail.sysu.edu.cn;
                                </span>
                                <span>
                                    周智豪, 硕士研究生。;
                                </span>
                                <span>
                                    邱铭凯, 硕士研究生。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-08-03</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金“视频大数据高效表达、深度分析与综合利用” (U1611461);</span>
                    </p>
            </div>
                    <h1><b>Vehicle Re-identification Algorithm Based on Component Fusion Feature</b></h1>
                    <h2>
                    <span>LI Xiying</span>
                    <span>ZHOU Zhihao</span>
                    <span>QIU Mingkai</span>
            </h2>
                    <h2>
                    <span>School of Intelligent Systems Engineering, Sun Yat-sen University</span>
                    <span>Guangdong Province Key Laboratory of Intelligent Transportation System</span>
                    <span>Key Laboratory of Video and Image Intelligent Analysis and Application Technology, Ministry of Public Security of PRC</span>
                    <span>National Engineering Laboratory of Video and Image Information Intelligent Analysis and Sharing Application Technology</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>To address the re-identification problem of different individual vehicles with identical types, a new vehicle re-identification algorithm is proposed.According to the component detection algorithm, the window and the vehicle face region with large differences between different vehicles are obtained, and the vehicle features of the detected vehicle window and the vehicle face region are extracted and merged to generate new fusion features.The distance measurement between image features is calculated for classification and recognition.The test is carried out on the public dataset VRID-1 of Sun Yat-sen university and results show that the Rank1 matching rate of the algorithm reaches 66.67%, which is obviously better than the classical traditional feature representation algorithm, thus verifies the feasibility and validity of the algorithm.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=vehicle%20re-identification&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">vehicle re-identification;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=component%20detection&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">component detection;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=feature%20extraction&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">feature extraction;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=feature%20fusion&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">feature fusion;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=distance%20measurement&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">distance measurement;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-08-03</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="33" name="33" class="anchor-tag">0 概述</h3>
                <div class="p1">
                    <p id="34">车辆重识别广泛应用于车辆旅行时间估计<citation id="155" type="reference"><link href="3" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>、起讫点流量统计及车辆行驶轨迹等交通研究。最为常用的车辆重识别是在不同位置的视频图像中识别到相同车牌号码的车辆, 但在一些如变更车牌、无车牌、遮挡车牌或车牌分辨率较低等特殊情况下, 利用车牌识别实现车辆重识别的方法则较为困难, 需要通过其他车辆识别匹配方法进行车辆重识别。</p>
                </div>
                <div class="p1">
                    <p id="35">不同车辆情形下的车辆重识别主要挑战在于同一款式的车辆外观基本一致。同时重识别问题是对不同位置的视频图像进行匹配, 同一车辆的空间位置、拍摄角度、光照变化等都会产生极大的差异, 从而增大了车辆匹配的难度。针对上述问题, 本文提出基于部件融合特征的车辆重识别算法, 利用物体检测定位车辆间差异较大区域, 运用深度卷积网络提取区域特征, 并将不同区域特征融合, 最终运用测量距离来实现车辆重识别。</p>
                </div>
                <h3 id="36" name="36" class="anchor-tag">1 相关研究</h3>
                <div class="p1">
                    <p id="37">车辆重识别属于目标识别的一种, 目标重识别是计算机视觉中的一个热点问题, 最早的目标重识别主要着眼于行人重识别。针对已有行人重识别方法难以解决行人图像光照、视角变化大的问题, 文献<citation id="156" type="reference">[<a class="sup">2</a>]</citation>提出一种基于特征融合的行人重识别方法。首先利用Retinex变换对图像进行预处理, 然后将CN特征与原有的颜色和纹理特征融合, 并通过区域和块划分的方式提取直方图获得图像特征, 最终采用距离学习方法实现行人重识别。针对传统特征描述子描述能力较弱的问题, 文献<citation id="157" type="reference">[<a class="sup">3</a>]</citation>运用行人数据库对AlexNet网络进行微调, 提取行人深度特征, 并采用度量学习方法进行识别, 验证了深度卷积特征的使用能够大幅提升重识别的性能。文献<citation id="158" type="reference">[<a class="sup">4</a>]</citation>基于多通道部分的卷积神经网络模型, 利用CNN模型学习行人的整体特征和局部身体部位特征, 重识别效果表明了局部特征同样具有较强的表征能力。随着道路监控系统的完善及公安刑侦的需求, 车辆的重识别逐渐兴起。文献<citation id="159" type="reference">[<a class="sup">5</a>]</citation>提出一种利用多个传感器节点实现车辆重识别的算法。根据不同节点获得的同一车辆标签匹配结果, 确定车辆状态, 并对标签分割进行修正, 同时根据获取的不同标签之间的相互关系修正车辆间的时间差。文献<citation id="160" type="reference">[<a class="sup">6</a>]</citation>提出一种用于在高速公路探测器上测量的个体车辆匹配算法, 并在上游的另一个探测器上进行了相应的测量。</p>
                </div>
                <div class="p1">
                    <p id="38">计算机视觉技术及深度学习的发展使得车辆重识别不再局限于检测器。文献<citation id="161" type="reference">[<a class="sup">7</a>]</citation>提出一种基于特征融合和L-M的算法, 该算法将车辆图片的 HSV 特征和LBP特征进行融合, 并对融合特征矩阵进行奇异值分解, 提取特征值, 该方法在性能上明显优于单一特征。文献<citation id="162" type="reference">[<a class="sup">8</a>]</citation>提出一种引导型Triplet network, 其将分类损失加入到原始的Triplet损失函数中, 对原始的训练网络进行限制, 从而提升识别效率。文献<citation id="163" type="reference">[<a class="sup">9</a>]</citation>提出一个抑制网络, 其采用了新颖的多任务学习框架, 可以同时学习车辆图像之间的细粒度和粗粒度2种层级的差异性特征, 同时提出一种至上而下的搜索方法, 在保持查询精度的情形下减少查询时间。文献<citation id="164" type="reference">[<a class="sup">10</a>]</citation>提出在获取完整的3D 包围框的前提下, 利用对颜色直方图和定向梯度的直方图进行线性回归来解决车辆重识别问题。文献<citation id="165" type="reference">[<a class="sup">11</a>]</citation>主要针对车辆视角的不确定性, 提出2种端到端深层结构:空间串联卷积网络和CNN-LSTM双向环路, 利用CNN和LSTM的优势来学习不同车辆视角的转换, 可以从一个输入视角中推断出一个包含所有视角信息的车辆特征表示, 用于学习测量距离。同时为了验证其算法, 建立了一个包含200个玩具车辆的多视角数据集。</p>
                </div>
                <div class="p1">
                    <p id="39">现有的大部分车辆重识别工作主要是优化类间差异, 较少有研究人员研究类内个体间差异约束。不论是在行人重识别还是车辆的细粒度识别中, 都已证明在一些特定的情形下, 相对于整体图像特征, 利用局部区域的特征去实现分类与识别的效果会更好。针对同一款式车辆下的不同车辆差异性较小这一难点, 本文提出基于部件融合特征的车辆重识别算法。算法整体框架如图1所示。</p>
                </div>
                <div class="area_img" id="40">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201906003_040.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 车辆重识别算法整体框架" src="Detail/GetImg?filename=images/JSJC201906003_040.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图1 车辆重识别算法整体框架</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201906003_040.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h3 id="41" name="41" class="anchor-tag">2 算法流程</h3>
                <div class="p1">
                    <p id="42">本文提出一种基于特征融合的车辆重识别算法, 该算法采用局部区域替代整体图像的策略, 放大关键局部区域对车辆重识别的作用力, 以减少弱差异性部件区域对重识别的干扰。首先对Faster R-CNN (Faster Region-based Convolutional Neural Network) <citation id="166" type="reference"><link href="25" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>算法进行优化, 构建车辆部件 (车窗、车脸) 检测定位模型;随后以VGG16<citation id="167" type="reference"><link href="27" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>模型为基础模型, 针对融合目标调整网络结构, 提取相应区域的深度卷积特征并对提取的特征进行融合;最后计算查询图片与检索库中图片的欧氏距离, 实现车辆的重识别。</p>
                </div>
                <h4 class="anchor-tag" id="43" name="43">2.1 部件检测定位模型</h4>
                <div class="p1">
                    <p id="44">本文对Faster R-CNN算法进行优化。在原始Faster R-CNN算法中, 首先对整张图像利用卷积层进行特征提取, 本文特征提取以VGG16的5组卷积层为特征提取算子;随后将第5组的特征图分别送入RPN网络以及ROI池化层, RPN网络获取的Proposal同时输入ROI池化层, 得到ROI区域的特征图;再经过2个全连接层, 最后送入分类回归层, 计算损失函数。部件检测流程如图2所示。</p>
                </div>
                <div class="area_img" id="45">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201906003_045.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 部件检测算法流程" src="Detail/GetImg?filename=images/JSJC201906003_045.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图2 部件检测算法流程</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201906003_045.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="46">RPN网络用来获取初始的候选框 (Proposal) 。为获取更为准确的定位框, 在获取到Proposal之后会对Proposal进行边框回归操作。但对于边框回归操作, 只有当Proposal和Ground Truth比较接近时, 才能将其作为训练样本训练本文的线性回归模型, 否则会导致训练的回归模型无效。为达到精准的定位效果, 本文研究对Faster R-CNN进行改进, 在Proposal产生阶段加入了K-means聚类<citation id="168" type="reference"><link href="29" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>优化, 使得RPN网络得到更为精准的Proposal, 进一步促进边框回归操作的回归效果, 从而精确定位车脸、车窗区域。</p>
                </div>
                <div class="p1">
                    <p id="47">首先对训练集数据进行聚类, 获取图片目标数据, 以目标物体的宽高值作为坐标轴进行二维数据聚类, 假设数据对象目标数量为<i>m</i>, 则在<i>m</i>个数据对象任意选择<i>k</i>个对象<i>μ</i><sub>1</sub>, <i>μ</i><sub>2</sub>, …, <i>μ</i><sub><i>k</i></sub>作为初始聚类中心, 对于所剩下其他对象, 则根据它们与这些聚类中心的相似度, 分别将它们分配给与其最相似的聚类。第<i>i</i>个数据对象所属类别<i>c</i><sup> (<i>i</i>) </sup>计算如式 (1) 所示。计算聚类中所有对象的均值, 更新每个聚类的聚类中心, 聚类中心计算如式 (2) 所示。重复更新聚类中心以及对象类别, 直至所有数据对象与其所属的聚类中心点的距离之和满足式 (3) , 最终获取数据的聚类中心点。</p>
                </div>
                <div class="p1">
                    <p id="48" class="code-formula">
                        <mathml id="48"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>c</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msup><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mtext>a</mtext><mtext>r</mtext><mtext>g</mtext><mtext>m</mtext><mtext>i</mtext><mtext>n</mtext></mrow></mstyle><mi>j</mi></munder><mo stretchy="false">∥</mo><mi>x</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msup><mo>-</mo><mi>μ</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">∥</mo><mo>, </mo><mi>j</mi><mo>=</mo><mn>1</mn><mo>, </mo><mn>2</mn><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>k</mi><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi>μ</mi><msub><mrow></mrow><mi>j</mi></msub><mo>=</mo><mfrac><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mi>l</mi></mstyle><mo stretchy="false">{</mo><mi>c</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msup><mo>=</mo><mi>j</mi><mo stretchy="false">}</mo><mi>x</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msup></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mi>l</mi></mstyle><mo stretchy="false">{</mo><mi>c</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msup><mo>=</mo><mi>j</mi><mo stretchy="false">}</mo></mrow></mfrac><mo>, </mo><mi>j</mi><mo>=</mo><mn>1</mn><mo>, </mo><mn>2</mn><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>k</mi><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="49">其中, <i>l</i>{<i>c</i><sup> (<i>i</i>) </sup>=<i>j</i>}表示的函数为:</p>
                </div>
                <div class="p1">
                    <p id="50" class="code-formula">
                        <mathml id="50"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>l</mi><mo stretchy="false">{</mo><mi>c</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msup><mo>=</mo><mi>j</mi><mo stretchy="false">}</mo><mo>=</mo><mrow><mo>{</mo><mrow><mtable><mtr><mtd columnalign="left"><mn>1</mn><mo>, </mo><mspace width="0.25em" /><mi>c</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msup><mo>=</mo><mi>j</mi></mtd></mtr><mtr><mtd columnalign="left"><mn>0</mn><mo>, </mo><mspace width="0.25em" /><mi>c</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msup><mo>≠</mo><mi>j</mi></mtd></mtr></mtable></mrow></mrow></mtd></mtr><mtr><mtd><mi>f</mi><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mo>=</mo><mrow><mi>min</mi></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><mrow></mrow></mstyle></mrow><mtext> </mtext><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>x</mi><mo>∈</mo><mi>μ</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></munder><mrow></mrow></mstyle></mrow><mi>d</mi><mi>i</mi><mi>s</mi><mi>t</mi><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>μ</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="51">聚类操作主要是用于优化RPN网络, 以获取更为精准的Proposal。区域网络示意图如图3所示<citation id="169" type="reference"><link href="25" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>。</p>
                </div>
                <div class="area_img" id="52">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201906003_052.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 区域网络示意图" src="Detail/GetImg?filename=images/JSJC201906003_052.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图3 区域网络示意图</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201906003_052.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="53">当获取的Anchors对应的reference box与所检测物体的尺寸越贴合, 越能够获取更为精准的Proposal。因此, 定位模型中用K-means聚类操作来调整anchor box的尺寸与宽高比。在训练分类器之前标记正负样本, 当Anchor所对应的reference box与Ground Truth的重合度<i>IoU</i>&gt;0.7时, 将其所对应的reference box标记为正样本;对于一些极端情况, 例如所有的Anchor对应的reference box与Groud Truth的<i>IoU</i>不大于0.7的, 取<i>IoU</i>值最大的reference box作为正样本;如果anchor对应的reference box与Ground Truth的<i>IoU</i>&lt;0.3, 将其所对应的reference box标记为负样本。训练RPN的损失函数Loss是由classification loss和regression loss按一定权重组成的, 具体计算如式 (4) 所示。</p>
                </div>
                <div class="p1">
                    <p id="54" class="code-formula">
                        <mathml id="54"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>L</mi><mo stretchy="false"> (</mo><mo stretchy="false">{</mo><mi>p</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">}</mo><mo>, </mo><mo stretchy="false">{</mo><mi>t</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">}</mo><mo stretchy="false">) </mo><mo>=</mo><mfrac><mn>1</mn><mrow><mi>Ν</mi><msub><mrow></mrow><mrow><mtext>c</mtext><mtext>l</mtext><mtext>s</mtext></mrow></msub></mrow></mfrac><mstyle displaystyle="true"><munder><mo>∑</mo><mi>i</mi></munder><mi>L</mi></mstyle><msub><mrow></mrow><mrow><mtext>c</mtext><mtext>l</mtext><mtext>s</mtext></mrow></msub><mo stretchy="false"> (</mo><mi>p</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>p</mi><msubsup><mrow></mrow><mi>i</mi><mo>*</mo></msubsup><mo stretchy="false">) </mo><mo>+</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mi>λ</mi><mfrac><mn>1</mn><mrow><mi>Ν</mi><msub><mrow></mrow><mrow><mtext>r</mtext><mtext>e</mtext><mtext>g</mtext></mrow></msub></mrow></mfrac><mstyle displaystyle="true"><munder><mo>∑</mo><mi>i</mi></munder><mi>p</mi></mstyle><msubsup><mrow></mrow><mi>i</mi><mo>*</mo></msubsup><mi>L</mi><msub><mrow></mrow><mrow><mtext>r</mtext><mtext>e</mtext><mtext>g</mtext></mrow></msub><mo stretchy="false"> (</mo><mi>t</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>t</mi><msubsup><mrow></mrow><mi>i</mi><mo>*</mo></msubsup><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi>L</mi><msub><mrow></mrow><mrow><mtext>c</mtext><mtext>l</mtext><mtext>s</mtext></mrow></msub><mo stretchy="false"> (</mo><mi>p</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>u</mi><mo stretchy="false">) </mo><mo>=</mo><mo>-</mo><mrow><mi>log</mi></mrow><msub><mrow></mrow><mi>a</mi></msub><mi>p</mi><msub><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mi>u</mi></mrow></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>5</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi>L</mi><msub><mrow></mrow><mrow><mtext>r</mtext><mtext>e</mtext><mtext>g</mtext></mrow></msub><mo stretchy="false"> (</mo><mi>t</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>t</mi><msubsup><mrow></mrow><mi>i</mi><mo>*</mo></msubsup><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>j</mi><mo>∈</mo><mo stretchy="false">{</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo>, </mo><mi>w</mi><mo>, </mo><mi>h</mi><mo stretchy="false">}</mo></mrow></munder><mi>s</mi></mstyle><mi>m</mi><mi>o</mi><mi>o</mi><mi>t</mi><mi>h</mi><msub><mrow></mrow><mrow><mi>L</mi><mn>1</mn></mrow></msub><mo stretchy="false"> (</mo><mi>t</mi><msub><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mi>j</mi></mrow></msub><mo>-</mo><mi>t</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mi>j</mi></mrow><mo>*</mo></msubsup><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>6</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi>s</mi><mi>m</mi><mi>o</mi><mi>o</mi><mi>t</mi><mi>h</mi><msub><mrow></mrow><mrow><mi>L</mi><mn>1</mn></mrow></msub><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mrow><mo>{</mo><mrow><mtable><mtr><mtd columnalign="left"><mn>0</mn><mo>.</mo><mn>5</mn><mi>x</mi><msup><mrow></mrow><mn>2</mn></msup><mo>, </mo><mspace width="0.25em" /><mrow><mo>|</mo><mi>x</mi><mo>|</mo></mrow><mo>&lt;</mo><mn>1</mn></mtd></mtr><mtr><mtd columnalign="left"><mrow><mo>|</mo><mi>x</mi><mo>|</mo></mrow><mo>-</mo><mn>0</mn><mo>.</mo><mn>5</mn><mo>, </mo><mspace width="0.25em" /><mtext>其</mtext><mtext>他</mtext></mtd></mtr></mtable></mrow></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>7</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="55">其中, <i>i</i>为anchor的索引, <i>p</i><sub><i>i</i></sub>是指anchor <i>i</i>为前景目标类的概率, <i>p</i><sup>*</sup><sub><i>i</i></sub>表示实际标注Ground Truth的类别概率, 即当anchor为正样本时取值为1, 否则取值为0, <i>t</i><sub><i>i</i></sub>为预测框的参数化坐标, <i>t</i><sup>*</sup><sub><i>i</i></sub>为真实框的参数化坐标, <i>L</i><sub>cls</sub> (<i>p</i><sub><i>i</i></sub>, <i>u</i>) 为分类损失, <i>u</i>表示分类的种类, 主要分为两类 (即目标与非目标) , <i>L</i><sub>reg</sub> (<i>t</i><sub><i>i</i></sub>, <i>t</i><sup>*</sup><sub><i>i</i></sub>) 表示回归损失, <i>λ</i>表示权重, <i>N</i><sub>cls</sub>为mini-batch尺寸, <i>N</i><sub>reg</sub>为anchor数量。回归损失中参数化坐标如下:</p>
                </div>
                <div class="p1">
                    <p id="56" class="code-formula">
                        <mathml id="56"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>t</mi><msub><mrow></mrow><mi>x</mi></msub><mo>=</mo><mo stretchy="false"> (</mo><mi>x</mi><mo>-</mo><mi>x</mi><msub><mrow></mrow><mi>a</mi></msub><mo stretchy="false">) </mo><mo>/</mo><mi>w</mi><msub><mrow></mrow><mi>a</mi></msub></mtd></mtr><mtr><mtd><mi>t</mi><msub><mrow></mrow><mi>y</mi></msub><mo>=</mo><mo stretchy="false"> (</mo><mi>y</mi><mo>-</mo><mi>y</mi><msub><mrow></mrow><mi>a</mi></msub><mo stretchy="false">) </mo><mo>/</mo><mi>h</mi><msub><mrow></mrow><mi>a</mi></msub></mtd></mtr><mtr><mtd><mi>t</mi><msub><mrow></mrow><mi>w</mi></msub><mo>=</mo><mrow><mi>log</mi></mrow><msub><mrow></mrow><mi>a</mi></msub><mrow><mo> (</mo><mrow><mfrac><mi>w</mi><mrow><mi>w</mi><msub><mrow></mrow><mi>a</mi></msub></mrow></mfrac></mrow><mo>) </mo></mrow></mtd></mtr><mtr><mtd><mi>t</mi><msub><mrow></mrow><mi>h</mi></msub><mo>=</mo><mrow><mi>log</mi></mrow><msub><mrow></mrow><mi>a</mi></msub><mrow><mo> (</mo><mrow><mfrac><mi>h</mi><mrow><mi>h</mi><msub><mrow></mrow><mi>a</mi></msub></mrow></mfrac></mrow><mo>) </mo></mrow><mspace width="0.25em" /></mtd></mtr><mtr><mtd><mi>t</mi><msubsup><mrow></mrow><mi>x</mi><mo>*</mo></msubsup><mo>=</mo><mo stretchy="false"> (</mo><mi>x</mi><msup><mrow></mrow><mo>*</mo></msup><mo>-</mo><mi>x</mi><msub><mrow></mrow><mi>a</mi></msub><mo stretchy="false">) </mo><mo>/</mo><mi>w</mi><msub><mrow></mrow><mi>a</mi></msub><mtext> </mtext><mspace width="0.25em" /></mtd></mtr><mtr><mtd><mi>t</mi><msubsup><mrow></mrow><mi>y</mi><mo>*</mo></msubsup><mo>=</mo><mo stretchy="false"> (</mo><mi>y</mi><msup><mrow></mrow><mo>*</mo></msup><mo>-</mo><mi>y</mi><msub><mrow></mrow><mi>a</mi></msub><mo stretchy="false">) </mo><mo>/</mo><mi>h</mi><msub><mrow></mrow><mi>a</mi></msub><mspace width="0.25em" /><mspace width="0.25em" /></mtd></mtr><mtr><mtd><mi>t</mi><msubsup><mrow></mrow><mi>w</mi><mo>*</mo></msubsup><mo>=</mo><mrow><mi>log</mi></mrow><msub><mrow></mrow><mi>a</mi></msub><mrow><mo> (</mo><mrow><mfrac><mrow><mi>w</mi><msup><mrow></mrow><mo>*</mo></msup></mrow><mrow><mi>w</mi><msub><mrow></mrow><mi>a</mi></msub></mrow></mfrac></mrow><mo>) </mo></mrow></mtd></mtr><mtr><mtd><mi>t</mi><msubsup><mrow></mrow><mi>h</mi><mo>*</mo></msubsup><mo>=</mo><mrow><mi>log</mi></mrow><msub><mrow></mrow><mi>a</mi></msub><mrow><mo> (</mo><mrow><mfrac><mrow><mi>h</mi><msup><mrow></mrow><mo>*</mo></msup></mrow><mrow><mi>h</mi><msub><mrow></mrow><mi>a</mi></msub></mrow></mfrac></mrow><mo>) </mo></mrow></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="57">其中, <i>x</i>、<i>y</i>表示框的中心点坐标, <i>w</i>、<i>h</i>表示框的宽、高, <i>x</i><sub><i>a</i></sub>表示anchor对应的reference box, <i>x</i><sup>*</sup>表示Ground Truth框 (<i>y</i><sub><i>a</i></sub>、<i>w</i><sub><i>a</i></sub>、<i>h</i><sub><i>a</i></sub>同理) 。</p>
                </div>
                <div class="p1">
                    <p id="58">特征图送入改进后的RPN网络后, 获取了较为精准的候选区域。由于这一步输出的框较多同时分类上还只是分为目标与非目标两类, 随后将候选区域和第五层卷积层提取的特征图同时输入ROI池化层, 将ROI区域的坐标位置映射到特征图上, 再在特征图的ROI区域进行池化操作, 得到ROI区域 (即Proposal区域) 的特征图。原图与特征图映射关系如式 (8) 所示。</p>
                </div>
                <div class="p1">
                    <p id="59"><i>i</i><sub>0</sub>=<i>g</i><sub><i>L</i></sub> (<i>i</i><sub><i>L</i></sub>) =<i>α</i><sub><i>L</i></sub> (<i>i</i><sub><i>L</i></sub>-1) +<i>β</i><sub><i>L</i></sub>      (8) </p>
                </div>
                <div class="p1">
                    <p id="60">其中:</p>
                </div>
                <div class="p1">
                    <p id="61" class="code-formula">
                        <mathml id="61"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>α</mi><msub><mrow></mrow><mi>L</mi></msub><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∏</mo><mrow><mi>Ρ</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></munderover><mi>S</mi></mstyle><msub><mrow></mrow><mi>p</mi></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>9</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi>β</mi><msub><mrow></mrow><mi>L</mi></msub><mo>=</mo><mn>1</mn><mo>+</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>p</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></munderover><mrow><mrow><mo> (</mo><mrow><mstyle displaystyle="true"><munderover><mo>∏</mo><mrow><mi>q</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>p</mi><mo>-</mo><mn>1</mn></mrow></munderover><mi>S</mi></mstyle><msub><mrow></mrow><mi>q</mi></msub></mrow><mo>) </mo></mrow></mrow></mstyle><mrow><mo> (</mo><mrow><mfrac><mrow><mi>F</mi><msub><mrow></mrow><mi>p</mi></msub><mo>-</mo><mn>1</mn></mrow><mn>2</mn></mfrac><mo>-</mo><mi>Ρ</mi><msub><mrow></mrow><mi>p</mi></msub></mrow><mo>) </mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>0</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="62">其中, <i>i</i><sub>0</sub>为特征图<i>g</i><sub><i>L</i></sub> (<i>i</i><sub><i>L</i></sub>) 的感受野中心, <i>L</i>表示第<i>L</i>层CNN, <i>S</i><sub><i>q</i></sub>表示第<i>q</i>层卷积核步长, <i>F</i><sub><i>p</i></sub>表示第<i>p</i>层卷积核大小, <i>P</i><sub><i>p</i></sub>表示第<i>p</i>层的padding大小。</p>
                </div>
                <div class="p1">
                    <p id="63">经过2个全连接层, 最后送入分类回归层。正负样本的选取同RPN网络中正负样本选取规则, 最终分类回归损失函数同样由classification loss和regression loss按一定权重组成, 损失函数如式 (11) 所示。</p>
                </div>
                <div class="p1">
                    <p id="64"><i>L</i> (<i>p</i>, <i>u</i>, <i>t</i><sup><i>u</i></sup>, <i>v</i>) =<i>L</i><sub>cls</sub> (<i>p</i>, <i>u</i>) +<i>λ</i>[<i>u</i>≥1]<i>L</i><sub>loc</sub> (<i>t</i><sup><i>u</i></sup>, <i>v</i>)      (11) </p>
                </div>
                <div class="p1">
                    <p id="65">其中:</p>
                </div>
                <div class="p1">
                    <p id="66"><i>L</i><sub>cls</sub> (<i>p</i>, <i>u</i>) =-log<sub><i>a</i></sub><i>p</i><sub><i>u</i></sub>      (12) </p>
                </div>
                <div class="p1">
                    <p id="67" class="code-formula">
                        <mathml id="67"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><msub><mrow></mrow><mrow><mtext>l</mtext><mtext>o</mtext><mtext>c</mtext></mrow></msub><mo stretchy="false"> (</mo><mi>t</mi><msup><mrow></mrow><mi>u</mi></msup><mo>, </mo><mi>v</mi><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>i</mi><mo>∈</mo><mo stretchy="false">{</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo>, </mo><mi>w</mi><mo>, </mo><mi>h</mi><mo stretchy="false">}</mo></mrow></munder><mi>s</mi></mstyle><mi>m</mi><mi>o</mi><mi>o</mi><mi>t</mi><mi>h</mi><msub><mrow></mrow><mrow><mi>L</mi><mn>1</mn></mrow></msub><mo stretchy="false"> (</mo><mi>t</mi><msubsup><mrow></mrow><mi>i</mi><mi>u</mi></msubsup><mo>-</mo><mi>v</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="68">其中, <i>p</i>={<i>p</i><sub>0</sub>, <i>p</i><sub>1</sub>, …, <i>p</i><sub><i>k</i></sub>}, 共分为<i>k</i>+1个类别 (<i>k</i>类目标加1类背景) , <i>u</i>为类别索引, <i>t</i><sup><i>k</i></sup>= (<i>t</i><mathml id="69"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>x</mi><mi>k</mi></msubsup></mrow></math></mathml>, <i>t</i><mathml id="70"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>y</mi><mi>k</mi></msubsup></mrow></math></mathml>, <i>t</i><mathml id="71"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>w</mi><mi>k</mi></msubsup></mrow></math></mathml>, <i>t</i><mathml id="72"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>h</mi><mi>k</mi></msubsup></mrow></math></mathml>) 表示第<i>k</i> 个类别的预测框参数化坐标, <i>v</i>= (<i>v</i><sub><i>x</i></sub>, <i>v</i><sub><i>y</i></sub>, <i>v</i><sub><i>w</i></sub>, <i>v</i><sub><i>h</i></sub>) 表示Ground Truth的参数化坐标, 定义背景区域类别为0, 则[<i>u</i>≥1]在背景区域框 (负样本) 时取值为0, 其余为1。参数化坐标参考RPN回归损失处, 其中用ROI框替换anchor对应的reference box。</p>
                </div>
                <h4 class="anchor-tag" id="73" name="73">2.2 特征融合算法</h4>
                <div class="p1">
                    <p id="74">本文运用训练集对预训练的VGG16模型进行微调。卷积神经网络VGG包含有5个卷积操作{Conv1, Conv2, Conv3, Conv4, Conv5}、2个全连接操作{FC6, FC7}以及1个分类层Softmax, 其中一组卷积操作包含有卷积 (Convolution) 以及最大池化 (Max-pooling) , 其原理分别如式 (14) 、式 (15) 所示。</p>
                </div>
                <div class="p1">
                    <p id="75" class="code-formula">
                        <mathml id="75"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>x</mi><msubsup><mrow></mrow><mi>j</mi><mi>l</mi></msubsup><mo>=</mo><mi>f</mi><mo stretchy="false"> (</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>i</mi></munder><mi>x</mi></mstyle><msubsup><mrow></mrow><mi>i</mi><mrow><mi>l</mi><mo>-</mo><mn>1</mn></mrow></msubsup><mo>×</mo><mi>k</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mi>l</mi></msubsup><mo>+</mo><mi>b</mi><msubsup><mrow></mrow><mi>j</mi><mi>l</mi></msubsup><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>4</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi>x</mi><msubsup><mrow></mrow><mi>k</mi><mi>l</mi></msubsup><mo>=</mo><mi>p</mi><mi>o</mi><mi>o</mi><mi>l</mi><mo stretchy="false"> (</mo><mi>x</mi><msubsup><mrow></mrow><mi>k</mi><mrow><mi>l</mi><mo>-</mo><mn>1</mn></mrow></msubsup><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>5</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="76">其中, <i>f</i> (<i>x</i>) =max (0, <i>x</i>) , <i>x</i><mathml id="77"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mi>l</mi></msubsup></mrow></math></mathml>为输出层第l层第j个特征图, x<mathml id="78"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>l</mi></msubsup></mrow></math></mathml>为第l层的第i个特征图, x<mathml id="79"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mi>l</mi></msubsup></mrow></math></mathml>表示第l层上其输入层第i个特征图和输出层第j个特征图的卷积核, b<mathml id="80"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mi>l</mi></msubsup></mrow></math></mathml>为第l层第j个特征图的偏置项, pool (·) 为池化计算函数。</p>
                </div>
                <div class="p1">
                    <p id="81">全连接层计算如式 (16) 所示。</p>
                </div>
                <div class="p1">
                    <p id="82" class="code-formula">
                        <mathml id="82"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>x</mi><msubsup><mrow></mrow><mi>j</mi><mi>l</mi></msubsup><mo>=</mo><mi>f</mi><mo stretchy="false"> (</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>i</mi></munder><mi>ω</mi></mstyle><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mi>l</mi></msubsup><mi>x</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mi>l</mi><mo>-</mo><mn>1</mn></mrow></msubsup><mo>+</mo><mi>b</mi><msubsup><mrow></mrow><mi>j</mi><mi>l</mi></msubsup><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>6</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="83">其中, l层是全连接层, x<mathml id="84"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mi>l</mi></msubsup></mrow></math></mathml>是第l层的第j个神经元, ω<mathml id="85"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mi>l</mi></msubsup></mrow></math></mathml>是第l层第j个神经元与第l-1层的第i个输入特征图中所有神经元连接的参数, b<mathml id="86"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mi>l</mi></msubsup></mrow></math></mathml>是偏置项。</p>
                </div>
                <div class="p1">
                    <p id="87"><i>Softmax</i>层是用于卷积神经网络参数的训练, 一旦训练完成, <i>Softmax</i>层会被移除, <i>Softmax</i>层计算如式 (17) 所示。</p>
                </div>
                <div class="p1">
                    <p id="88" class="code-formula">
                        <mathml id="88"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>σ</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false"> (</mo><mi>z</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mrow><mi>exp</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">z</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mtext>e</mtext></mstyle><mtext>x</mtext><mtext>p</mtext><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">z</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo></mrow></mfrac><mo>, </mo><mi>i</mi><mo>=</mo><mn>1</mn><mo>, </mo><mn>2</mn><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>m</mi><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>7</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="89">其中, i为类别数, <b><i>z</i></b><sub><i>i</i></sub>为<i>m</i>维的输入向量。</p>
                </div>
                <div class="p1">
                    <p id="90">VGG16模型在卷积层结构上已经能够提取较优的特征, 取得较好的分类效果。过多的网络层结构会增加特征训练时间;而层数太少, 提取的特征描述能力相对较差。VGG16模型在ImageNet上训练的模型已经公开, 可以在此基础上对其他数据集进行微调, 且对其他数据集适应能力较好。采用预训练好的模型进行迁移学习, 一方面能够使得网络模型更容易收敛, 另一方面又能在新的数据集上取得较好效果。本文运用训练集对预训练的VGG16模型进行微调, 微调步骤为:在训练阶段, 利用训练集对全连接层的两层参数 (FC6/FC7) 进行微调, 将其余卷积层参数进行固定, 把Softmax层的输出种类设置为需要分类的种类数, 输入图像开始训练, 特征训练示意图如图4所示;在测试阶段, 移除FC7和Softmax层, 将全连接层FC6的输出作为特征向量, 特征提取示意图如图5所示。</p>
                </div>
                <div class="area_img" id="91">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201906003_091.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 特征训练示意图" src="Detail/GetImg?filename=images/JSJC201906003_091.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图4 特征训练示意图</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201906003_091.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="92">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201906003_092.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 特征提取示意图" src="Detail/GetImg?filename=images/JSJC201906003_092.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图5 特征提取示意图</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201906003_092.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="93">为有效地捕获不同部件特征之间的联系, 对不同的部件区域所提取的卷积特征进行融合用于车辆重识别。在提出的融合模型中, 融合阶段将不同部件区域的特征描述子进行串联拼接, 其原理如式 (18) 所示, 特征融合示意图如图6所示。这一融合方式计算简单, 同时能够在融合时尽可能地保留各个部件区域特征的描述能力, 减少因融合操作导致的特征描述能力的衰退, 从而使得新的融合特征同时具有各个部件区域特征的表征能力。在重识别实验中, 分别提取车脸部件以及车窗部件的卷积特征。车脸区域的特征能够学习并表征不同车辆型号之间的差异性;车窗区域的特征能够学习表征不同车辆个体之间的差异性。将两部件区域特征进行融合, 获得新的融合特征用于不同车辆个体之间的特征描述。</p>
                </div>
                <div class="p1">
                    <p id="94"><i>f</i> (<i>X</i><sub>1</sub>, <i>X</i><sub>2</sub>, …, <i>X</i><sub><i>k</i></sub>) =<i>ω</i><sub>1</sub><i>f</i> (<i>X</i><sub>1</sub>) ♁<i>ω</i><sub>2</sub><i>f</i> (<i>X</i><sub>2</sub>) ♁…♁</p>
                </div>
                <div class="p1">
                    <p id="95"><i>ω</i><sub><i>k</i></sub><i>f</i> (<i>X</i><sub><i>k</i></sub>)      (18) </p>
                </div>
                <div class="p1">
                    <p id="96">其中, <i>f</i> (<i>X</i><sub>1</sub>, <i>X</i><sub>2</sub>, …, <i>X</i><sub><i>k</i></sub>) 表示融合操作后得到的融合特征, <i>ω</i><sub><i>k</i></sub>为不同区域特征融合权重, ♁表示串联操作, <i>f</i> (<i>X</i><sub><i>k</i></sub>) 表示不同区域特征。</p>
                </div>
                <div class="area_img" id="97">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201906003_097.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 特征融合示意图" src="Detail/GetImg?filename=images/JSJC201906003_097.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图6 特征融合示意图</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201906003_097.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="98" name="98">2.3 重识别距离度量</h4>
                <div class="p1">
                    <p id="99">重识别问题本质上是一种更为细粒度的图像识别问题, 即个体层面上的微观分类识别问题, 但通常通过分类器的训练来进行图像分类在重识别问题上具有一定的局限性。一方面个体层面上的分类其类别数量众多, 分类器的训练效果不一定好;另一方面, 训练器的训练是针对固定的种类数, 属于硬分类, 新的种类的图像也会被分到之前训练的种类中, 导致识别错误, 不具有扩展性。为增强模型的扩展性, 模型引入距离度量的方式, 通过比较图像特征之间的距离度量, 将距离由大到小排序, 距离越小越有可能属于同一车辆。在计算特征度量距离的具体操作中, 给定一个待识别的车辆图像<i>Q</i>和检索数据集<i>R</i>, 利用融合特征进行比较, 将待识别车辆图像<i>Q</i>的融合特征{<i>q</i><sub>1</sub>, <i>q</i><sub>2</sub>, …, <i>q</i><sub><i>n</i></sub>}与检索数据集<i>R</i>内所有图像的融合特征{<i>r</i><sub>1</sub>, <i>r</i><sub>2</sub>, …, <i>r</i><sub><i>n</i></sub>}进行度量计算, 计算公式如下:</p>
                </div>
                <div class="p1">
                    <p id="100" class="code-formula">
                        <mathml id="100"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>D</mi><mi>Ι</mi><mi>S</mi><mo>=</mo><msqrt><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><mo stretchy="false"> (</mo><mi>q</mi><msub><mrow></mrow><mi>k</mi></msub><mo>-</mo><mi>r</mi><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false">) </mo></mrow></mstyle><msup><mrow></mrow><mn>2</mn></msup></mrow></msqrt><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>9</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <h3 id="101" name="101" class="anchor-tag">3 实验结果与分析</h3>
                <h4 class="anchor-tag" id="102" name="102">3.1 实验设置</h4>
                <div class="p1">
                    <p id="103">为了验证本文车辆重识别算法的有效性, 本文选择对中山大学公开车辆重识别数据集VRID-1 (Vehicle Re-identification Dataset-1) <sup></sup><citation id="170" type="reference"><link href="31" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>进行测试, 该数据集主要用于评测同一款式车辆重识别问题的研究。数据集含有10个道路常见的车辆款式, 每个款式有100个不同的车辆 (IDs) , 共1 000个车辆IDs, 每个ID的车辆有10张图片, 总计10 000张图像。在训练集和测试集的分配上, 选取700个IDs共7 000张图像作为训练集, 剩余的300个作为测试集。测试集分为查询集和检索集, 在测试集的300个IDs中, 每个IDs选取1张图像作为查询集, 共计300张;剩余的2 700张图像作为检索集。部分数据集如图7所示, 数据集分布情况如图8所示。</p>
                </div>
                <div class="area_img" id="104">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201906003_104.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 数据集部分车辆图像" src="Detail/GetImg?filename=images/JSJC201906003_104.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图7 数据集部分车辆图像</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201906003_104.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="105">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201906003_105.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 数据集分布示意图" src="Detail/GetImg?filename=images/JSJC201906003_105.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图8 数据集分布示意图</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201906003_105.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="106">在实验过程中, 部件检测的效果由以下指标判断:</p>
                </div>
                <div class="p1">
                    <p id="107">1) 定位重合度。计算公式如式 (20) 所示。</p>
                </div>
                <div class="p1">
                    <p id="108" class="code-formula">
                        <mathml id="108"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ο</mi><mi>v</mi><mi>e</mi><mi>r</mi><mi>l</mi><mi>a</mi><mi>p</mi><mo stretchy="false"> (</mo><mi>g</mi><msub><mrow></mrow><mi>j</mi></msub><mo>, </mo><mi>Ι</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mfrac><mrow><mi>a</mi><mi>r</mi><mi>e</mi><mi>a</mi><mo stretchy="false"> (</mo><mi>g</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mstyle displaystyle="true"><mo>∩</mo><mi>a</mi></mstyle><mi>r</mi><mi>e</mi><mi>a</mi><mo stretchy="false"> (</mo><mi>Ι</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo></mrow><mrow><mi>a</mi><mi>r</mi><mi>e</mi><mi>a</mi><mo stretchy="false"> (</mo><mi>g</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mstyle displaystyle="true"><mo>∪</mo><mi>a</mi></mstyle><mi>r</mi><mi>e</mi><mi>a</mi><mo stretchy="false"> (</mo><mi>Ι</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mn>0</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="109">其中, <i>j</i>代表不同的部件, <i>g</i><sub><i>j</i></sub>为本文算法得到的提取结果, <i>I</i><sub><i>j</i></sub>为人工定位的车脸部件。</p>
                </div>
                <div class="p1">
                    <p id="110">2) 定位准确率。假设定位重合度大于某阈值<i>T</i>时, 认定为检测正确, 定位准确率则为总样本中检测正确占总样本的比例, 如式 (21) 所示。</p>
                </div>
                <div class="p1">
                    <p id="111" class="code-formula">
                        <mathml id="111"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>a</mi><mi>c</mi><mi>c</mi><mo stretchy="false"> (</mo><mi>j</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mrow><mi>Ν</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false"> (</mo><mi>Ι</mi><mi>o</mi><mi>U</mi><mo>≥</mo><mi>Τ</mi><mo stretchy="false">) </mo></mrow><mrow><mi>Ν</mi><msub><mrow></mrow><mi>j</mi></msub></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="112">其中, <i>j</i>代表不同的部件, <i>T</i>为设定阈值, 即假设定位重合度大于某阈值<i>T</i>时, 认定为检测正确, <i>N</i><sub><i>j</i></sub>为部件<i>j</i>的总样本数。</p>
                </div>
                <div class="p1">
                    <p id="113">3) 识别精准率。计算公式如式 (22) 所示。</p>
                </div>
                <div class="p1">
                    <p id="114" class="code-formula">
                        <mathml id="114"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>c</mi><mo stretchy="false"> (</mo><mi>j</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mrow><mi>Τ</mi><mi>Ρ</mi><msub><mrow></mrow><mi>j</mi></msub></mrow><mrow><mi>Τ</mi><mi>Ρ</mi><msub><mrow></mrow><mi>j</mi></msub><mo>+</mo><mi>F</mi><mi>Ρ</mi><msub><mrow></mrow><mi>j</mi></msub></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="115">其中, <i>TP</i><sub><i>j</i></sub>为第<i>j</i>类目标识别正确的总数, <i>FP</i><sub><i>j</i></sub>为非第<i>j</i>类区域判别为第<i>j</i>类区域的总数。</p>
                </div>
                <div class="p1">
                    <p id="116">本文车辆重识别算法效果评价由以下指标判断:</p>
                </div>
                <div class="p1">
                    <p id="117">1) CMC (Cumulative Matching Characteristic) 曲线。CMC曲线是行人重识别中常用的评价指标, CMC曲线给出的是与检索图像属于同一车辆的图像出现在不同长度的返回结果中的概率。计算公式如式 (23) 所示。</p>
                </div>
                <div class="p1">
                    <p id="118" class="code-formula">
                        <mathml id="118"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>C</mi><mi>Μ</mi><mi>C</mi><mo>@</mo><mi>k</mi><mo>=</mo><mfrac><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>q</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mi>g</mi></mstyle><mi>t</mi><mo stretchy="false"> (</mo><mi>q</mi><mo>, </mo><mi>k</mi><mo stretchy="false">) </mo></mrow><mi>Ν</mi></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="119">其中, <i>gt</i> (<i>q</i>, <i>k</i>) 表示在长度为<i>k</i>的返回结果中出现与检索图像属于同一车辆图像, 出现值取1, 不出现值取0, <i>N</i>为检索集总样本数。</p>
                </div>
                <div class="p1">
                    <p id="120">2) 召回率。表示检索出来的相关图像数量与检索库中所有相关图像数量的比率, 衡量的是检索系统的查全率。计算公式如式 (24) 所示。</p>
                </div>
                <div class="p1">
                    <p id="121" class="code-formula">
                        <mathml id="121"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>R</mi><mi>e</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>l</mi><mo>=</mo><mfrac><mrow><mi>Τ</mi><mi>Ρ</mi></mrow><mrow><mi>Τ</mi><mi>Ρ</mi><mo>+</mo><mi>F</mi><mi>Ρ</mi></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mn>4</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="122">其中, <i>TP</i>为查询图像被检索到并且是同一辆车的数量, <i>FP</i>表示查询图像未被检索到但属于同一辆车的数量。</p>
                </div>
                <h4 class="anchor-tag" id="123" name="123">3.2 结果分析</h4>
                <div class="p1">
                    <p id="124">实验结果分析如下:</p>
                </div>
                <div class="p1">
                    <p id="125">1) 车辆部件检测实验与分析</p>
                </div>
                <div class="p1">
                    <p id="126">好的定位准确率不仅能够提高定位区域的识别率, 同时也能够提升车辆重识别的匹配率在检测阶段, 基于经典定位算法Faster R-CNN进行了一定的改进;在区域生成网络阶段, 通过K-means聚类将训练集中待检测区域的宽高进行聚类, 根据聚类结果调整区域生成网络参数, 从而获取更为精准的候选区域定位框。</p>
                </div>
                <div class="p1">
                    <p id="127">检测算法对车窗、车脸区域的检测准确率如图9所示。横坐标为假定的定位重合度阈值, 即当定位重合度大于此值时, 认定为检测准确。由图9可知, 在测试结果中, 全部图像的车脸、车窗区域定位重合度皆大于0.7, 车窗区域定位重合度大于0.85的比例大于90%, 车脸区域定位重合度大于0.9的比例大于85%。</p>
                </div>
                <div class="area_img" id="128">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201906003_128.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图9 车脸、车窗检测准确率示意图" src="Detail/GetImg?filename=images/JSJC201906003_128.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图9 车脸、车窗检测准确率示意图</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201906003_128.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="129">部件检测平均定位重合度与识别精准率如图10所示。在测试样本中, 检测区域平均定位重合度皆大于0.9, 其部件种类的识别率为1.0。不论是从检测准确率还是平均定位重合度上来看, 部件检测这一部分达到了预定实验要求, 为后续部件区域的特征提取奠定了良好的基础。</p>
                </div>
                <div class="area_img" id="130">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201906003_130.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图10 部件检测效果" src="Detail/GetImg?filename=images/JSJC201906003_130.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图10 部件检测效果</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201906003_130.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="131">2) 特征提取实验与分析</p>
                </div>
                <div class="p1">
                    <p id="132">在计算机视觉中, 不论是分类识别问题还是目标重识别问题, 其最大的核心就是进行图像的特征提取, 一个具有强表征能力的特征对图像目标更具有代表性, 同时也能带来较好的实验效果。因此, 本文通过实验来确定提取何种特征进行算法实验, 此处主要比对传统人工设计特征中的经典特征:HOG特征, SIFT特征以及深度学习中的CNN特征。</p>
                </div>
                <div class="p1">
                    <p id="133">不同的特征所产生的匹配率差异如图11所示, 整体上深度卷积特征的匹配率要远远优于传统的人工设计特征。不同特征召回率的对比如图12所示, 人工设计特征召回率差, 而CNN特征整体上召回率要高得多, 在50个返回结果中, 召回率达到了60%以上。一方面, 深度卷积特征具有更高的维度, 能够更好地表征图像的一些特性;另一方面, 在经过训练微调后, 卷积模型的参数通过训练学习调整到了一个较优的状态, 使得提取出来的特征在后续的匹配问题上更具有描述能力。因此, 本文算法采用深度卷积特征作为实验算法的特征描述子, 用以达到车辆重识别的目的。</p>
                </div>
                <div class="area_img" id="134">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201906003_134.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图11 不同特征匹配率对比" src="Detail/GetImg?filename=images/JSJC201906003_134.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图11 不同特征匹配率对比</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201906003_134.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="135">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201906003_135.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图12 不同特征召回率对比" src="Detail/GetImg?filename=images/JSJC201906003_135.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图12 不同特征召回率对比</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201906003_135.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="136">3) 特征融合实验与分析</p>
                </div>
                <div class="p1">
                    <p id="137">不同的特征融合方式会影响到所获取的融合特征的描述能力。常见的特征融合方式主要有相加、点乘、串联、外积等。车辆图像在经过部件检测步骤后, 获取了车窗与车脸2个区域;随后对两部件区域进行特征提取。在实验中, 提取的特征为全连接层FC6的高维特征, 其维度达到4 096维。在融合阶段, 进行特征的相加、点乘或者串联, 其融合特征维度皆维持在千位级, 但进行外积融合其维度将会涨至4 096×4 096维, 故在外积融合方式中融入了平均池化操作, 用以降低外积融合导致的过大维度。</p>
                </div>
                <div class="p1">
                    <p id="138">不同特征融合方式的CMC曲线及召回率曲线对比分别如图13、图14所示。在所对比的4种融合方式中, 串联融合方式在CMC曲线匹配率和召回率上效果都较优。其主要原因在于串联的融合方式同时保存了车脸及车窗区域的特征, 在进行距离度量计算时, 特征对应的位置较为对齐, 避免了融合造成特征描述力减弱的现象;外积的方式产生维度过高, 进行降维后损失了大量的有效信息, 从而导致该融合方式效果较差。根据实验对比, 最终选取串联作为算法的特征融合方式, 对车脸车窗区域提取的特征进行特征融合, 产生新的融合特征。</p>
                </div>
                <div class="area_img" id="139">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201906003_139.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图13 不同特征融合方法匹配率对比" src="Detail/GetImg?filename=images/JSJC201906003_139.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图13 不同特征融合方法匹配率对比</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201906003_139.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="140">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201906003_140.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图14 不同特征融合方法召回率对比" src="Detail/GetImg?filename=images/JSJC201906003_140.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图14 不同特征融合方法召回率对比</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201906003_140.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="141">4) 基于串联特征融合的车辆重识别实验与分析</p>
                </div>
                <div class="p1">
                    <p id="142">本文算法确定提取部件的CNN特征, 并利用串联的融合方式进行特征融合。为了客观评价本文算法效果, 选择了以下2种特征提取方法与本文算法进行对比:</p>
                </div>
                <div class="p1">
                    <p id="143"> (1) 实验1 (HOG) :对车辆图像提取HOG 特征, 再通过计算查询图像与检索集中的图像HOG特征欧氏距离, 最终得到重识别结果。</p>
                </div>
                <div class="p1">
                    <p id="144"> (2) 实验2 (VGGNET) :使用经典网络VGG16提取图像高层语义特征。在训练阶段, 把1 000辆不同的车作为1 000类目标来训练模型, 得到相应的特征模型以及分类参数, 测试阶段利用训练完成的模型提取深度卷积特征, 再通过计算查询图像与检索集中的图像CNN特征欧氏距离, 最终得到重识别结果。</p>
                </div>
                <div class="p1">
                    <p id="145"> (3) 本文算法:首先经过部件检测算法获得车脸与车窗区域, 随后提取相应区域的CNN特征并进行特征融合, 最终通过计算查询图像与检索集中图像融合特征的欧氏距离, 得到重识别结果。</p>
                </div>
                <div class="p1">
                    <p id="146">实验选择3种特征提取方法进行对比, 实验Rank <i>X</i>下的匹配率和召回率的结果如表1、表2所示, 其中, <i>X</i>是返回结果列表的长度。不同方法的CMC曲线和召回率曲线如图15所示。</p>
                </div>
                <div class="area_img" id="147">
                    <p class="img_tit"><b>表1 不同车辆重识别方法匹配率</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note">%</p>
                    <table id="147" border="1"><tr><td><br />方法</td><td>Rank 1</td><td>Rank 5</td><td>Rank 10</td><td>Rank 15</td></tr><tr><td><br />实验1</td><td>1.67</td><td>4.00</td><td>5.67</td><td>6.00</td></tr><tr><td><br />实验2</td><td>48.67</td><td>75.00</td><td>83.67</td><td>88.67</td></tr><tr><td><br />本文算法</td><td>66.67</td><td>82.67</td><td>90.00</td><td>92.33</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="148">
                    <p class="img_tit"><b>表2 不同车辆重识别方法召回率</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note">%</p>
                    <table id="148" border="1"><tr><td>方法</td><td>Rank 1</td><td>Rank 10</td><td>Rank 20</td><td>Rank 30</td><td>Rank 40</td><td>Rank 50</td></tr><tr><td>实验1</td><td>1.37</td><td>2.26</td><td>3.93</td><td>5.19</td><td>6.22</td><td>7.44</td></tr><tr><td><br />实验2</td><td>2.35</td><td>34.04</td><td>46.19</td><td>53.56</td><td>59.22</td><td>63.67</td></tr><tr><td><br />本文算法</td><td>3.11</td><td>43.26</td><td>50.67</td><td>62.81</td><td>68.11</td><td>72.26</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="149">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201906003_149.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图15 重识别性能曲线" src="Detail/GetImg?filename=images/JSJC201906003_149.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图15 重识别性能曲线</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201906003_149.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="150">从实验结果可知, 实验1效果较差, 主要原因是HOG特征是对灰度图像进行梯度以及方向的统计, 缺乏对颜色信息的考虑, 同时当车辆之间相互的姿态、外观变化时, HOG这种用特定的模式生成的特征鲁棒性不足;实验2重识别效果相较实验1有很大提升, 但相对本文算法较差, 其原因在于不同车辆ID之间大部分区域相似度较高, 差异甚微, 对整张车辆图像进行特征提取容易干扰差异性区域特征的比较, 同时在整张图片情形下, 姿态变化的特征差异会相对较大;本文算法不论是在匹配率还是在召回率上效果都优于对比实验, 在Rank 15值下, 匹配率达到92.33%, 在Rank 50下, 召回率达到72.26%。本文算法利用先验知识, 着眼于不同车辆个体间差异最大的区域, 相对于整体车辆图片的特征提取, 姿态变化导致的特征差异较小, 实验结果证明了本文算法的可行性。</p>
                </div>
                <div class="p1">
                    <p id="151">车辆重识别效果示意图如图16所示, 其中, 粗线代表待识别车辆, 细线代表与待识别车辆属于同一车辆, 其余无加框代表与待识别车辆属于不同车辆。图16 (a) 属于识别效果较好的情形, 图16 (b) 属于识别效果较差的情形。姿态的变化对本文算法的影响较小, 同时从结果上也可以看出, 识别错误的车辆与待识别车辆是属于同颜色同车辆型号的情形, 其相似度极高。光照对本文算法影响相对于姿态来说较大, 特别是在图16 (b) 所示的光照极暗的情形下, 其车窗有效差异特征 (如年检标志、车前摆饰等) 都会消失, 同时光照产生的差异影响较大, 故而在光照差的情形下, 会匹配到同样光照条件下的车辆型号同款但不同车辆的图像。</p>
                </div>
                <div class="area_img" id="152">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201906003_152.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图16 车辆重识别效果示意图" src="Detail/GetImg?filename=images/JSJC201906003_152.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图16 车辆重识别效果示意图</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201906003_152.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h3 id="153" name="153" class="anchor-tag">4 结束语</h3>
                <div class="p1">
                    <p id="154">本文提出一种基于部件融合特征的车辆重识别算法。首先利用部件检测算法检测出区分性较强的车脸与车窗区域, 减少弱差异性部件区域对重识别的干扰;随后提取相应区域的深度卷积特征, 将提取的特征进行特征融合操作, 形成新的融合特征;最后计算查询图像与检索库图片特征之间的距离度量, 将计算的距离度量值从小到大排序, 最终将距离度量最小值所对应的类别标识作为重识别车辆的识别标记。在公开数据集VRID-1上的验证结果表明了本文方法的优越性。但本文方法仍具有一定的局限性, 下一步将从以下方面进行研究:对原始图像预处理, 进行图像增强等操作, 减少光照对车辆重识别的影响;在特征融合中除了不同部件之间深度卷积特征的融合, 进一步融合不同的特征, 如对光照、角度等更具有鲁棒性的人工设计特征, 增强融合特征的表征能力和鲁棒性, 从而提高重识别的匹配精度。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011300328999&amp;v=MTQ0OTVOckk5Rlora0hCWFV3b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VTG5JSVY0WGF4Yz1OaWZPZmJLN0h0RA==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> MEI L T, LAM W H K.Application of automatic vehicle identification technology for real-time journey time estimation[J].Information Fusion, 2011, 12 (1) :11-19.
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSGG201712031&amp;v=Mjc2ODVSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm9GeW5sVUw3Tkx6N01hYkc0SDliTnJZOUdaWVFLREg4NHY=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> 张耿宁, 王家宝, 张亚非, 等.基于特征融合的行人重识别方法[J].计算机工程与应用, 2017, 53 (12) :185-189.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=1017074510.nh&amp;v=Mjg5NjRSTE9lWmVSb0Z5bmxVTDdOVkYyNkdiTy9HdFROcjVFYlBJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1U=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> 刘娜.基于卷积神经网络的行人重识别算法[D].上海:华东师范大学, 2017.
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Person re-identification by multi-channel parts-based cnn with improved triplet loss function">

                                <b>[4]</b> CHENG Deng, GONG Yihong, ZHOU Sanping, et al.Person re-identification by multi-channel parts-based CNN with improved triplet loss function[C]//Proceedings of 2016 IEEE Conference on Computer Vision and Pattern Recognition.Las Vegas, USA:IEEE Press, 2016:1335-1344.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JZUS201405005&amp;v=MTkyOTh0R0ZyQ1VSTE9lWmVSb0Z5bmxVTDdOTHpmZWZiRzRIOVhNcW85RllZUUtESDg0dlI0VDZqNTRPM3pxcUI=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b> TIAN Yin, DONG Honghui, JIA Limin, et al.A vehicle re-identification algorithm based on multi-sensor correlation[J].Journal of Zhejiang University-SCIENCE C (Computers and Electronics) , 2014, 15 (5) :372-382.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Vehicle reidentification and travel time measurement. I. Congested freeways">

                                <b>[6]</b> COIFMAN B.Vehicle reidentification and travel time measurement, part II:uncongested freeways and the onset of congestion[C]//Proceedings of 2001 IEEE Intelligent Transportation Systems.Oakland, USA:IEEE Press, 1999:899-917.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZKK201804005&amp;v=MjY5MTdCdEdGckNVUkxPZVplUm9GeW5sVUw3TklUZkFaYkc0SDluTXE0OUZZWVFLREg4NHZSNFQ2ajU0TzN6cXE=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b> 王盼盼, 李玉惠.基于特征融合和L-M算法的车辆重识别方法[J].电子科技, 2018, 4 (1) :12-15.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Improving triplet-wise training of convolutional neural network for ve-hicle re-identification">

                                <b>[8]</b> ZHANG Yiheng, LIU Dong, ZHA Zhengjun.Improving triplet-wise training of convolutional neural network for vehicle re-identification[C]//Proceedings of IEEE International Conference on Multimedia and Expo.Washington D.C., USA:IEEE Press, 2017:1386-1391.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning a repression network for precise vehicle search">

                                <b>[9]</b> XU Qingtong, YAN Ke, TIAN Yonghong.Learning a repression network for precise vehicle search[C]//Proceedings of IEEE CVPR’17.Washington D.C., USA:IEEE Press, 2017:125-136.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Vehicle Re-Identification for Automatic Video Traffic Surveillance">

                                <b>[10]</b> ZAPLETAL D, HEROUT A.Vehicle re-identification for automatic video traffic surveillance[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition Workshops.[S.1.]:IEEE Computer Society, 2016:1568-1574.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Vehicle re-identification by deep hidden multi-view inference">

                                <b>[11]</b> ZHOU Yi, LIU Li, SHAO Ling.Vehicle re-identification by deep hidden multi-view inference[J].IEEE Transactions on Image Processing, 2018, 27 (7) :3275-3287.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Faster R-CNN:Towards Real-Time Object Detection with Region Proposal Networks">

                                <b>[12]</b> REN Shaoqing, GIRSHICK R.Faster R-CNN:towards real-time object detection with region proposal networks[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017, 39 (6) :1137-1149.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Going deeper with convolutions">

                                <b>[13]</b> SZEGEDY C, LIU Wei, JIA Yangqing, et al.Going deeper with convolutions[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2015:1-9.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_14" >
                                    <b>[14]</b>
                                 MACQUEEN J.Some methods for classification and analysis of multi-variate observations[C]//Proceedings of Berkeley Symposium on Mathematical Statistics and Probability.Berkeley, USA:California University Press, 1967:281-297.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=VRID-1:a basic vehicle re-identification dataset for similar vehicles">

                                <b>[15]</b> LI Xiying, YUAN Minxian, JIANG Qianyin, et al.VRID-1:a basic vehicle re-identification dataset for similar vehicles[C]//Proceedings of IEEE International Conference on Intelligent Transportation Systems.Washington D.C., USA:IEEE Press, 2017:1-8.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJC201906003" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201906003&amp;v=MzE5NjZPZVplUm9GeW5sVUw3Tkx6N0JiYkc0SDlqTXFZOUZaNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkw=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="1" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
