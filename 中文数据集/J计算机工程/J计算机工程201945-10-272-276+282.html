<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637126203924146250%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJC201910045%26RESULT%3d1%26SIGN%3dBxivZcrd97lw43ADX1yieuqEqOk%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201910045&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201910045&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201910045&amp;v=MzIyMzJyNDlCWVlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJ0Rnkza1ZMM0tMejdCYmJHNEg5ak4=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5pRHZtTnQxdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#47" data-title="0 概述 ">0 概述</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#55" data-title="1 非局部稀疏表示 ">1 非局部稀疏表示</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#61" data-title="2 非局部稀疏的彩色图像超分辨率重建 ">2 非局部稀疏的彩色图像超分辨率重建</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#63" data-title="2.1 非局部稀疏模型">2.1 非局部稀疏模型</a></li>
                                                <li><a href="#79" data-title="2.2 伪影消除">2.2 伪影消除</a></li>
                                                <li><a href="#93" data-title="2.3 超分辨率重建">2.3 超分辨率重建</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#102" data-title="3 实验结果与分析 ">3 实验结果与分析</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#115" data-title="4 结束语 ">4 结束语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#104" data-title="&lt;b&gt;图1 测试图像&lt;/b&gt;"><b>图1 测试图像</b></a></li>
                                                <li><a href="#106" data-title="&lt;b&gt;图2 蝴蝶图像超分辨率重建结果对比&lt;/b&gt;"><b>图2 蝴蝶图像超分辨率重建结果对比</b></a></li>
                                                <li><a href="#107" data-title="&lt;b&gt;图3 车辆图像超分辨率重建结果对比&lt;/b&gt;"><b>图3 车辆图像超分辨率重建结果对比</b></a></li>
                                                <li><a href="#108" data-title="&lt;b&gt;表1 5种算法对不同图像的PSNR结果对比&lt;/b&gt;"><b>表1 5种算法对不同图像的PSNR结果对比</b></a></li>
                                                <li><a href="#109" data-title="&lt;b&gt;表2 5种算法对不同图像的SSIM结果对比&lt;/b&gt;"><b>表2 5种算法对不同图像的SSIM结果对比</b></a></li>
                                                <li><a href="#113" data-title="&lt;b&gt;图4 字典尺寸对重建图像PSNR的影响&lt;/b&gt;"><b>图4 字典尺寸对重建图像PSNR的影响</b></a></li>
                                                <li><a href="#114" data-title="&lt;b&gt;图5 字典尺寸对重建图像SSIM的影响&lt;/b&gt;"><b>图5 字典尺寸对重建图像SSIM的影响</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" title=" 徐志刚,李文文,袁飞祥,等.基于稀疏表示和多成分字典学习的超分辨率重建[J].系统工程与电子技术,2018,40(3):699-703." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XTYD201803032&amp;v=MTUxNzQza1ZMM05QVG5TYXJHNEg5bk1ySTlHWm9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJ0Rnk=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5pRHZtTnQxdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         徐志刚,李文文,袁飞祥,等.基于稀疏表示和多成分字典学习的超分辨率重建[J].系统工程与电子技术,2018,40(3):699-703.
                                    </a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" title=" 宋熙煜,周利莉,李中国,等.图像分割中的超像素方法研究综述[J].中国图象图形学报,2015,20(5):599-608." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZGTB201505002&amp;v=MTg1MjdlWmVSdEZ5M2tWTDNOUHlyZmJMRzRIOVRNcW85RlpvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE8=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5pRHZtTnQxdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         宋熙煜,周利莉,李中国,等.图像分割中的超像素方法研究综述[J].中国图象图形学报,2015,20(5):599-608.
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title=" 李云峰,李晟阳,韩茜茜.基于多邻域信息的监控图像超分辨率算法[J].计算机工程,2016,42(6):261-264." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201606047&amp;v=MjEyODZPZVplUnRGeTNrVkwzTkx6N0JiYkc0SDlmTXFZOUJZNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkw=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5pRHZtTnQxdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         李云峰,李晟阳,韩茜茜.基于多邻域信息的监控图像超分辨率算法[J].计算机工程,2016,42(6):261-264.
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" title=" 戚曹,朱桂斌,唐鉴波,等.基于稀疏表示的红外视频图像超分辨率算法[J].计算机工程,2016,42(3):278-282." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201603050&amp;v=MjI0MTE0TzN6cXFCdEdGckNVUkxPZVplUnRGeTNrVkwzTkx6N0JiYkc0SDlmTXJJOUFaSVFLREg4NHZSNFQ2ajU=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5pRHZtTnQxdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                         戚曹,朱桂斌,唐鉴波,等.基于稀疏表示的红外视频图像超分辨率算法[J].计算机工程,2016,42(3):278-282.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" title=" BARTH&#201;LEMY Q,LARUE A,MARS J I.Color sparse representations for image processing:review,models,and prospects[J].IEEE Transactions on Image Processing,2015,24(11):3978-3989." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Color sparse representations for image processing:review,models,and prospects">
                                        <b>[5]</b>
                                         BARTH&#201;LEMY Q,LARUE A,MARS J I.Color sparse representations for image processing:review,models,and prospects[J].IEEE Transactions on Image Processing,2015,24(11):3978-3989.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" title=" YUE Linwei,SHEN Huanfeng,LI Jie,et al.Image super-resolution:the techniques,applications,and future[J].Signal Processing,2016,128:389-408." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image Super-resolution: The Techniques,Applications, and Future">
                                        <b>[6]</b>
                                         YUE Linwei,SHEN Huanfeng,LI Jie,et al.Image super-resolution:the techniques,applications,and future[J].Signal Processing,2016,128:389-408.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" title=" ROMANO Y,ISIDORO J,MILANFAR P.Raisr:rapid and accurate image super resolution[J].IEEE Transactions on Computational Imaging,2017,3(1):110-125." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=RAISR:rapid and accurate image super resolution">
                                        <b>[7]</b>
                                         ROMANO Y,ISIDORO J,MILANFAR P.Raisr:rapid and accurate image super resolution[J].IEEE Transactions on Computational Imaging,2017,3(1):110-125.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" title=" DONG Chao,LOY Chenchange,HE Kaiming,et al.Image super-resolution using deep convolutional networks[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2016,38(2):295-307." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image super-resolution using deep convolutional networks">
                                        <b>[8]</b>
                                         DONG Chao,LOY Chenchange,HE Kaiming,et al.Image super-resolution using deep convolutional networks[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2016,38(2):295-307.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" title=" DONG Weisheng,ZHANG Lei,SHI Guangming,et al.Image deblurring and super-resolution by adaptive sparse domain selection and adaptive regularization[J].IEEE Transactions on Image Processing,2011,20(7):1838-1857." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image Deblurring and Super-Resolution by Adaptive Sparse Domain Selection and Adaptive Regularization">
                                        <b>[9]</b>
                                         DONG Weisheng,ZHANG Lei,SHI Guangming,et al.Image deblurring and super-resolution by adaptive sparse domain selection and adaptive regularization[J].IEEE Transactions on Image Processing,2011,20(7):1838-1857.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_10" title=" YANG Jianchao,WANG Zhaowen,LIN Zhe,et al.Coupled dictionary training for image super-resolution[J].IEEE Transactions on Image Processing,2012,21(8):3467-3478." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Coupled Dictionary Training for Image Super-Resolution">
                                        <b>[10]</b>
                                         YANG Jianchao,WANG Zhaowen,LIN Zhe,et al.Coupled dictionary training for image super-resolution[J].IEEE Transactions on Image Processing,2012,21(8):3467-3478.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_11" title=" ZHANG Yulun,GU Kaiyu,ZHANG Yongbing,et al.Image super-resolution based on dictionary learning and anchored neighborhood regression with mutual incoherence[C]//Proceedings of 2015 IEEE International Conference on Image Processing.Washington D.C.,USA:IEEE Press,2015:591-595." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image super-resolution based on dictionary learning and anchored neighborhood regression with mutual incoherence">
                                        <b>[11]</b>
                                         ZHANG Yulun,GU Kaiyu,ZHANG Yongbing,et al.Image super-resolution based on dictionary learning and anchored neighborhood regression with mutual incoherence[C]//Proceedings of 2015 IEEE International Conference on Image Processing.Washington D.C.,USA:IEEE Press,2015:591-595.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_12" title=" TIMOFTE R,DE-SMET V,VAN GOOL L.A+:adjusted anchored neighborhood regression for fast super-resolution[C]//Proceedings of Asian Conference on Computer Vision.Berlin,Germany:Springer,2014:111-126." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A+:adjusted anchored neighborhood regression for fast super-resolution">
                                        <b>[12]</b>
                                         TIMOFTE R,DE-SMET V,VAN GOOL L.A+:adjusted anchored neighborhood regression for fast super-resolution[C]//Proceedings of Asian Conference on Computer Vision.Berlin,Germany:Springer,2014:111-126.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_13" title=" 杨玲,刘怡光,黄蓉刚,等.新的基于稀疏表示单张彩色超分辨率算法[J].计算机应用,2013,33(2):472-475." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201302045&amp;v=Mjc0ODU3RzRIOUxNclk5QllZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSdEZ5M2tWTDNOTHo3QmQ=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5pRHZtTnQxdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[13]</b>
                                         杨玲,刘怡光,黄蓉刚,等.新的基于稀疏表示单张彩色超分辨率算法[J].计算机应用,2013,33(2):472-475.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_14" title=" CHENG Ming,WANG Cheng,LI Jonathan.Single-image super-resolution in RGB space via group sparse representation[J].IET Image Processing,2014,9(6):461-467." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Single-image super-resolution in RGB space via group sparse representation">
                                        <b>[14]</b>
                                         CHENG Ming,WANG Cheng,LI Jonathan.Single-image super-resolution in RGB space via group sparse representation[J].IET Image Processing,2014,9(6):461-467.
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_15" title=" YANG Jianchao,WRIGHT J,HUANG T S,et al.Image super-resolution via sparse representation[J].IEEE Transactions on Image Processing,2010,19(11):2861-2873." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image super-resolution via sparse representation">
                                        <b>[15]</b>
                                         YANG Jianchao,WRIGHT J,HUANG T S,et al.Image super-resolution via sparse representation[J].IEEE Transactions on Image Processing,2010,19(11):2861-2873.
                                    </a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_16" title=" XU Jian,CHANG Zhiguo,FAN Jiulun,et al.Super-resolution via adaptive combination of color channels[J].Multimedia Tools and Applications,2017,76(1):1553-1584." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Super-resolution via adaptive combination of color channels">
                                        <b>[16]</b>
                                         XU Jian,CHANG Zhiguo,FAN Jiulun,et al.Super-resolution via adaptive combination of color channels[J].Multimedia Tools and Applications,2017,76(1):1553-1584.
                                    </a>
                                </li>
                                <li id="35">


                                    <a id="bibliography_17" title=" 邓承志,田伟,陈盼,等.基于局部约束群稀疏的红外图像超分辨率重建[J].物理学报,2014,63(4):144-151." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=WLXB201404021&amp;v=MjQ0MDFyQ1VSTE9lWmVSdEZ5M2tWTDNOTWlIVGJMRzRIOVhNcTQ5SFpZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0Y=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5pRHZtTnQxdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[17]</b>
                                         邓承志,田伟,陈盼,等.基于局部约束群稀疏的红外图像超分辨率重建[J].物理学报,2014,63(4):144-151.
                                    </a>
                                </li>
                                <li id="37">


                                    <a id="bibliography_18" title=" DONG Weisheng,ZHANG Lei,SHI Guangming,et al.Nonlocally centralized sparse representation for image restoration[J].IEEE Transactions on Image Processing,2013,22(4):1620-1630." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Nonlocally Centralized Sparse Representation for Image Restoration">
                                        <b>[18]</b>
                                         DONG Weisheng,ZHANG Lei,SHI Guangming,et al.Nonlocally centralized sparse representation for image restoration[J].IEEE Transactions on Image Processing,2013,22(4):1620-1630.
                                    </a>
                                </li>
                                <li id="39">


                                    <a id="bibliography_19" title=" ZHANG Yongbing,ZHANG Yulun,ZHANG Jian,et al.CCR:clustering and collaborative representation for fast single image super-resolution[J].IEEE Transactions on Multimedia,2016,18(3):405-417." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=CCR:Clustering and collaborative representation for fast single image super-resolution">
                                        <b>[19]</b>
                                         ZHANG Yongbing,ZHANG Yulun,ZHANG Jian,et al.CCR:clustering and collaborative representation for fast single image super-resolution[J].IEEE Transactions on Multimedia,2016,18(3):405-417.
                                    </a>
                                </li>
                                <li id="41">


                                    <a id="bibliography_20" title=" YANG A Y,ZHOU Zihan,BALASUBRAMANIAN A G,et al.Fast L1-minimization algorithms for robust face recognition[J].IEEE Transactions on Image Processing,2013,22(8):3234-3246." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fast-minimization algorithms for robust face recognition">
                                        <b>[20]</b>
                                         YANG A Y,ZHOU Zihan,BALASUBRAMANIAN A G,et al.Fast L1-minimization algorithms for robust face recognition[J].IEEE Transactions on Image Processing,2013,22(8):3234-3246.
                                    </a>
                                </li>
                                <li id="43">


                                    <a id="bibliography_21" title=" MOUSAVI H,MONGA V.Sparsity-based color image super resolution via exploiting cross channel constraints[J].IEEE Transactions on Image Processing,2017,26(11):5094-5106." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Sparsity-based color image super resolution via exploiting cross channel constraints">
                                        <b>[21]</b>
                                         MOUSAVI H,MONGA V.Sparsity-based color image super resolution via exploiting cross channel constraints[J].IEEE Transactions on Image Processing,2017,26(11):5094-5106.
                                    </a>
                                </li>
                                <li id="45">


                                    <a id="bibliography_22" >
                                        <b>[22]</b>
                                     BECK A,TEBOULLE M.A fast iterative shrinkage-thresholding algorithm for linear inverse problems[J].SIAM Journal on Imaging Sciences,2009,2(1):183-202.</a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJC" target="_blank">计算机工程</a>
                2019,45(10),272-276+282 DOI:10.19678/j.issn.1000-3428.0052065            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于色彩约束与非局部稀疏表示的彩色图像超分辨率重建</b></span>
 <span class="shoufa"></span>                                     </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BE%90%E5%BF%97%E5%88%9A&amp;code=07911419&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5pRHZtTnQxdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">徐志刚</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%A9%AC%E5%BC%BA&amp;code=31226669&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5pRHZtTnQxdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">马强</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9C%B1%E7%BA%A2%E8%95%BE&amp;code=07919531&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5pRHZtTnQxdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">朱红蕾</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BC%A0%E5%A2%A8%E9%80%B8&amp;code=24356815&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5pRHZtTnQxdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">张墨逸</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%85%B0%E5%B7%9E%E7%90%86%E5%B7%A5%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%B8%8E%E9%80%9A%E4%BF%A1%E5%AD%A6%E9%99%A2&amp;code=0167938&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5pRHZtTnQxdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">兰州理工大学计算机与通信学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>基于稀疏表示模型的彩色图像超分辨率重建方法通常采用基于图像块的稀疏编码过程,易导致稀疏表示不稳定、重建彩色图像存在细节模糊和色彩伪影的问题。为此,提出一种非局部稀疏表示与色彩通道约束相结合的重建算法。将待重建的低分辨率彩色图像转换到YCbCr色彩空间,利用非局部稀疏模型对低分辨率彩色图像的亮度信息进行重建,再将重建图像转换回RGB色彩空间,应用色彩通道约束方法去除色彩伪影,从而在保证图像细节信息重建质量的同时提升其色彩伪影的去除能力。实验结果表明,与双三次插值算法、ScSR算法等相比,该算法重建图像的峰值信噪比和结构相似性较高。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%A8%80%E7%96%8F%E8%A1%A8%E7%A4%BA&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5pRHZtTnQxdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">稀疏表示;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5pRHZtTnQxdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">超分辨率;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%BD%A9%E8%89%B2%E5%9B%BE%E5%83%8F&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5pRHZtTnQxdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">彩色图像;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%9D%9E%E5%B1%80%E9%83%A8%E8%87%AA%E7%9B%B8%E4%BC%BC%E6%80%A7&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5pRHZtTnQxdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">非局部自相似性;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%89%B2%E5%BD%A9%E9%80%9A%E9%81%93%E7%BA%A6%E6%9D%9F&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5pRHZtTnQxdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">色彩通道约束;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    徐志刚(1977—),男,副教授、博士,主研方向为图像超分辨率重建、深度学习理论与方法;;
                                </span>
                                <span>
                                    马强,硕士研究生;;
                                </span>
                                <span>
                                    朱红蕾,副教授、硕士;;
                                </span>
                                <span>
                                    张墨逸,讲师、硕士。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-07-10</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金(61761028);</span>
                                <span>模式识别国家重点实验室开放课题基金(201700005);</span>
                    </p>
            </div>
                    <h1><b>Color Image Super-resolution Reconstruction Based on Color Constraint and Nonlocal Sparse Representation</b></h1>
                    <h2>
                    <span>XU Zhigang</span>
                    <span>MA Qiang</span>
                    <span>ZHU Honglei</span>
                    <span>ZHANG Moyi</span>
            </h2>
                    <h2>
                    <span>School of Computer and Communication,Lanzhou University of Technology</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Color image super-resolution reconstruction method based on sparse representation model usually adopts sparse coding process based on image blocks,which easily leads to the instability of sparse representation,and the problems of detail blurring and color artifacts in the reconstruction of color images.Therefore,a reconstruction algorithm combining nonlocal sparse representation with color channel constraints is proposed.The low-resolution color images are converted into YCbCr color space,and the brightness information of low-resolution color image is reconstructed by nonlocal sparse model.Then the reconstructed image is converted back to RGB color space,and the color artifacts are removed by using color channel constraints,thus ensuring the quality of image detail information reconstruction,and improving the ability in removing color artifacts.Experimental results show that compared with Bicubic Interpolation(BI) algorithm,ScSR algorithm and so on,the reconstructed image of the proposed algorithm has higher Peak Signal to Noise Ratio(PSNR) and Structural Similarity Index Measurement(SSIM).</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=sparse%20representation&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5pRHZtTnQxdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">sparse representation;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=super-resolution&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5pRHZtTnQxdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">super-resolution;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=color%20image&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5pRHZtTnQxdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">color image;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=nonlocal%20self-similarity&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5pRHZtTnQxdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">nonlocal self-similarity;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=color%20channel%20constraint&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5pRHZtTnQxdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">color channel constraint;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-07-10</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="47" name="47" class="anchor-tag">0 概述</h3>
                <div class="p1">
                    <p id="48">图像超分辨率重建技术借助退化的一幅或多幅低分辨率(Low Resolution,LR)观测图像恢复原始图像中缺失的高频细节信息,从而使重建的高分辨率(High Resolution,HR)图像呈现出更好的视觉效果。在重建时,仅需要一幅LR观测图像就可以重建出相同场景下的HR图像,这对于实际应用具有重要意义<citation id="137" type="reference"><link href="3" rel="bibliography" /><link href="5" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="49">随着彩色成像和显示设备的快速发展,彩色图像超分辨率重建的需求日益增长。彩色图像相比于灰度图像具有更强的信息表达能力,在卫星遥感监测、医学影像处理、文物保护与展示等领域具有广阔的应用前景<citation id="138" type="reference"><link href="7" rel="bibliography" /><link href="9" rel="bibliography" /><sup>[<a class="sup">3</a>,<a class="sup">4</a>]</sup></citation>。彩色图像的超分辨率重建已成为一个亟待解决的问题<citation id="139" type="reference"><link href="11" rel="bibliography" /><link href="13" rel="bibliography" /><sup>[<a class="sup">5</a>,<a class="sup">6</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="50">基于学习方法的彩色图像超分辨率重建是当前研究的重点,具有代表性的主要有基于机器学习的方法<citation id="140" type="reference"><link href="15" rel="bibliography" /><link href="17" rel="bibliography" /><sup>[<a class="sup">7</a>,<a class="sup">8</a>]</sup></citation>、基于稀疏正则模型的方法<citation id="141" type="reference"><link href="19" rel="bibliography" /><link href="21" rel="bibliography" /><sup>[<a class="sup">9</a>,<a class="sup">10</a>]</sup></citation>和基于锚定领域回归的方法<citation id="142" type="reference"><link href="23" rel="bibliography" /><link href="25" rel="bibliography" /><sup>[<a class="sup">11</a>,<a class="sup">12</a>]</sup></citation>等。基于稀疏正则模型的重建方法具有图像信息表征效果好、计算复杂度低等优点,因此得到了广泛应用。根据处理各通道色彩信息方式的不同,该重建方法主要分为2类:基于联合色彩通道的重建方法和基于转换色彩空间的重建方法。</p>
                </div>
                <div class="p1">
                    <p id="51">在基于联合色彩通道的重建方法方面,文献<citation id="143" type="reference">[<a class="sup">13</a>]</citation>将RGB三通道看成一个整体,并通过新定义的内积强化通道之间的联系,保证重建的图像块能够维持原始三通道颜色的平均值信息。文献<citation id="144" type="reference">[<a class="sup">14</a>]</citation>构建彩色图像稀疏字典,每个字典原子由RGB通道对应的3个原子叠加而成。然而,该方法得到的稀疏字典可能含有较多的灰度原子,导致重建彩色图像中存在一定的色彩伪影。</p>
                </div>
                <div class="p1">
                    <p id="52">在基于转换色彩空间的重建方法方面,文献<citation id="145" type="reference">[<a class="sup">15</a>]</citation>将彩色图像转换到YCbCr色彩空间中,对Y通道进行联合稀疏重建,而对Cb、Cr进行简单的插值。文献<citation id="146" type="reference">[<a class="sup">16</a>]</citation>提出基于自适应组合色彩通道的超分辨率方法,通过RGB色彩通道的自适应组合,获取不同通道内的显著纹理信息并整合到Y通道,再将彩色图像转换回RGB色彩空间进行总变差(Total Variation,TV)正则等相关处理。该方法对彩色通道进行分离,可能存在色彩信息在重建过程中不能有效恢复的情况。</p>
                </div>
                <div class="p1">
                    <p id="53">此外,基于图像块的稀疏编码是一个相对独立的过程,在超分辨率重建过程中可能会造成相似图像块之间的全局相似性丢失,出现细节模糊的情况。</p>
                </div>
                <div class="p1">
                    <p id="54">针对以上问题,本文在转换色彩空间的基础上,构建彩色图像亮度通道的非局部稀疏模型,将RGB色彩通道间的边界相似性约束引入到色彩伪影的消除过程中,从而使重建彩色图像呈现出更好的视觉效果。</p>
                </div>
                <h3 id="55" name="55" class="anchor-tag">1 非局部稀疏表示</h3>
                <div class="p1">
                    <p id="56">图像的非局部稀疏表示模型将局部图像块中存在的冗余信息和非局部图像块间存在的结构自相似性统一在同一框架下<citation id="147" type="reference"><link href="35" rel="bibliography" /><link href="37" rel="bibliography" /><sup>[<a class="sup">17</a>,<a class="sup">18</a>]</sup></citation>。该模型综合考虑了图像中所有相似块之间的全局相似性,从图像块结构稀疏层次上尽可能提升重建算法的性能。</p>
                </div>
                <div class="p1">
                    <p id="57">假设在图像块<i><b>x</b></i><sub><i>i</i></sub>位置利用欧式距离寻找与其纹理结构相似的所有图像块<i><b>x</b></i><sub><i>i</i></sub><sub>,</sub><sub><i>q</i></sub>,并构建一个结构组<i><b>S</b></i><sub><i><b>x</b></i></sub><sub><i>i</i></sub>。求解每个相似图像块对应的稀疏系数<i>α</i><sub><i>i</i></sub>,并利用权重<i>δ</i><sub><i>i</i></sub>将稀疏系数加权平均,可得到非局部估计值<i>β</i><sub><i>i</i></sub>,具体如下:</p>
                </div>
                <div class="p1">
                    <p id="58" class="code-formula">
                        <mathml id="58"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">β</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>q</mi><mo>∈</mo><mi mathvariant="bold-italic">S</mi><msub><mrow></mrow><mrow><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msub></mrow></munder><mi mathvariant="bold-italic">δ</mi></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mo>,</mo><mi>q</mi></mrow></msub><mi mathvariant="bold-italic">α</mi><msub><mrow></mrow><mrow><mi>i</mi><mo>,</mo><mi>q</mi></mrow></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="59">其中,<mathml id="117"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">δ</mi><msub><mrow></mrow><mrow><mi>i</mi><mo>,</mo><mi>q</mi></mrow></msub><mo>=</mo><mfrac><mn>1</mn><mi>w</mi></mfrac><mrow><mi>exp</mi></mrow><mrow><mo>(</mo><mrow><mfrac><mrow><mo>-</mo><mrow><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mrow><mi>i</mi><mo>,</mo><mi>q</mi></mrow></msub><mo stretchy="false">∥</mo></mrow><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup></mrow><mi>h</mi></mfrac></mrow><mo>)</mo></mrow><mo>,</mo><mi>w</mi></mrow></math></mathml>是归一化因子,h是前定参数。</p>
                </div>
                <div class="p1">
                    <p id="60">从理论上讲,<i>α</i><sub><i>i</i></sub>与<i>β</i><sub><i>i</i></sub>应具有一致性。因此,本文通过最小化二者的误差,构建图像的非局部稀疏表示模型,即‖<i>α</i><sub><i>i</i></sub>-<i>β</i><sub><i>i</i></sub>‖<sub>p</sub>。</p>
                </div>
                <h3 id="61" name="61" class="anchor-tag">2 非局部稀疏的彩色图像超分辨率重建</h3>
                <div class="p1">
                    <p id="62">本文基于非局部稀疏模型提出彩色图像超分辨率重建算法,然后应用色彩通道约束方法对重建彩色图像进行色彩伪影消除。</p>
                </div>
                <h4 class="anchor-tag" id="63" name="63">2.1 非局部稀疏模型</h4>
                <div class="p1">
                    <p id="64">基于块的稀疏表示模型如式(2)所示,其实质是求解每个图像块<i><b>y</b></i><sub><i>i</i></sub>对应的稀疏系数<i>α</i><sub><i>i</i></sub>。</p>
                </div>
                <div class="p1">
                    <p id="65" class="code-formula">
                        <mathml id="65"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">α</mi><msub><mrow></mrow><mi>k</mi></msub><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mtext>a</mtext><mtext>r</mtext><mtext>g</mtext><mtext>m</mtext><mtext>i</mtext><mtext>n</mtext></mrow></mstyle><mi mathvariant="bold-italic">α</mi></munder><mrow><mo>{</mo><mrow><mrow><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>k</mi></msub><mo>-</mo><mi mathvariant="bold-italic">D</mi><mi mathvariant="bold-italic">α</mi><mo stretchy="false">∥</mo></mrow><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup><mo>+</mo><mi>λ</mi><mrow><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">α</mi><mo stretchy="false">∥</mo></mrow><msub><mrow></mrow><mn>1</mn></msub></mrow><mo>}</mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="66">其中,<image id="157" type="formula" href="images/JSJC201910045_15700.jpg" display="inline" placement="inline"><alt></alt></image>、M分别表示<i>LR</i>和<i>HR</i>图像块的维数,<i><b>Y</b></i><sup>l</sup>、<i><b>Y</b></i><sup>h</sup>分别为LR、HR图像样本集,<i><b>D</b></i><sup>l</sup>、<i><b>D</b></i><sup>h</sup>分别为LR、HR图像的非局部稀疏字典。然而,式(2)在稀疏编码过程中相对独立地考虑每个图像块,忽略了图像块之间的非局部相关性,导致最终求解的稀疏表示系数不够稳定。因此,找到一个能将自然图像固有的局部稀疏性与非局部自相似性相结合的模型,是提升稀疏表示性能的关键。本文在上述稀疏表示模型的基础上,利用非局部约束方法构建非局部图像块稀疏表示模型。在强化稀疏性的同时,降低稀疏重建的误差,计算过程如下:</p>
                </div>
                <div class="p1">
                    <p id="69" class="code-formula">
                        <mathml id="69"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">α</mi><msub><mrow></mrow><mi>k</mi></msub><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mtext>a</mtext><mtext>r</mtext><mtext>g</mtext><mtext>m</mtext><mtext>i</mtext><mtext>n</mtext></mrow></mstyle><mi mathvariant="bold-italic">α</mi></munder><mrow><mo>{</mo><mrow><mrow><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>k</mi></msub><mo>-</mo><mi mathvariant="bold-italic">D</mi><mi mathvariant="bold-italic">α</mi><mo stretchy="false">∥</mo></mrow><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup><mo>+</mo><mi>λ</mi><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">α</mi><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false">∥</mo><msub><mrow></mrow><mn>1</mn></msub><mo>+</mo><mi>γ</mi><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">α</mi><msub><mrow></mrow><mi>k</mi></msub><mo>-</mo><mi mathvariant="bold-italic">β</mi><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false">∥</mo><msub><mrow></mrow><mtext>p</mtext></msub></mrow><mo>}</mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>3</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="70">其中,<i>β</i><sub><i>k</i></sub>表示图像块<i><b>y</b></i><sub><i>k</i></sub>间的纹理结构相似性,将其值作为权重自适应选择重建邻域块稀疏系数<i>α</i><sub><i>k</i></sub><sub>,</sub><sub><i>q</i></sub>的加权和值,以保证<i>α</i><sub><i>k</i></sub>的稀疏性。由于重建图像块与期望图像块之间存在误差,导致<i>α</i><sub><i>k</i></sub>≠<i>β</i><sub><i>k</i></sub>,因此需要通过最小化两者误差来提升稀疏表示的稳定性。非局部稀疏重建模型可记为:</p>
                </div>
                <div class="p1">
                    <p id="71" class="code-formula">
                        <mathml id="71"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi mathvariant="bold-italic">α</mi></mstyle><mrow><mspace width="0.25em" /><mo>∼</mo></mrow></mover><msub><mrow></mrow><mi>k</mi></msub><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mtext>a</mtext><mtext>r</mtext><mtext>g</mtext><mtext>m</mtext><mtext>i</mtext><mtext>n</mtext></mrow></mstyle><mi mathvariant="bold-italic">α</mi></munder><mrow><mo>{</mo><mrow><mrow><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>k</mi></msub><mo>-</mo><mi mathvariant="bold-italic">D</mi><mi mathvariant="bold-italic">α</mi><mo stretchy="false">∥</mo></mrow><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup><mo>+</mo><mi>λ</mi><mrow><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">α</mi><msub><mrow></mrow><mi>k</mi></msub><mo>-</mo><mi mathvariant="bold-italic">β</mi><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false">∥</mo></mrow><msub><mrow></mrow><mtext>p</mtext></msub></mrow><mo>}</mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>4</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="72">为降低图像结构组的复杂度,提高稀疏表达能力,本文通过K-means算法<citation id="148" type="reference"><link href="39" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>对图像块进行分类,进而对每个聚类进行针对性的稀疏重建,最终实现整个图像的重建。</p>
                </div>
                <div class="p1">
                    <p id="73">在非局部稀疏字典求解过程中,首先通过K-means聚类算法将LR、HR图像样本集<i><b>Y</b></i><sup>l</sup>、<i><b>Y</b></i><sup>h</sup>分为<i>K</i>个聚类<mathml id="118"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mrow><mrow><mrow><mo>{</mo><mrow><mi mathvariant="bold-italic">Y</mi><msup><mrow></mrow><mtext>l</mtext></msup><mo>,</mo><mi mathvariant="bold-italic">Y</mi><msup><mrow></mrow><mtext>h</mtext></msup></mrow><mo>}</mo></mrow></mrow><mo>|</mo></mrow></mrow><msubsup><mrow></mrow><mrow><mi>k</mi><mo>=</mo><mi>i</mi></mrow><mi>Κ</mi></msubsup></mrow></math></mathml>,该聚类中心为<mathml id="119"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mrow><mrow><mi mathvariant="bold-italic">c</mi><msub><mrow></mrow><mi>k</mi></msub></mrow><mo>|</mo></mrow></mrow><msubsup><mrow></mrow><mrow><mi>k</mi><mo>=</mo><mi>i</mi></mrow><mi>Κ</mi></msubsup><mo>=</mo><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">d</mi><msubsup><mrow></mrow><mi>k</mi><mtext>Τ</mtext></msubsup><mi mathvariant="bold-italic">y</mi><msubsup><mrow></mrow><mi>i</mi><mtext>l</mtext></msubsup></mrow><mo>|</mo></mrow></mrow></math></mathml>,其中,<i><b>d</b></i><sub><i>k</i></sub>表示字典<i><b>D</b></i><sub><i>k</i></sub>的第<i>k</i>个原子,<i><b>y</b></i><sup>l</sup><sub><i>i</i></sub>表示在样本图像块对集<mathml id="120"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>{</mo><mrow><mi mathvariant="bold-italic">y</mi><msubsup><mrow></mrow><mi>i</mi><mtext>l</mtext></msubsup><mo>,</mo><mi mathvariant="bold-italic">y</mi><msubsup><mrow></mrow><mi>i</mi><mtext>h</mtext></msubsup></mrow><mo>}</mo></mrow></mrow></math></mathml>中的<i>LR</i>样本图像块。值得注意的是,该聚类过程得到的所有图像块都具有相似的纹理结构,即K个聚类对应K个图像块结构组。为进一步提高稀疏重建字典的表达能力,利用图像块间的非局部自相似性约束进行联合字典学习,得到如下非局部稀疏字典对<mathml id="121"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>{</mo><mrow><mi mathvariant="bold-italic">D</mi><msup><mrow></mrow><mtext>l</mtext></msup><mo>,</mo><mi mathvariant="bold-italic">D</mi><msup><mrow></mrow><mtext>h</mtext></msup></mrow><mo>}</mo></mrow></mrow></math></mathml>:</p>
                </div>
                <div class="p1">
                    <p id="74" class="code-formula">
                        <mathml id="74"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mrow><mo>{</mo><mrow><mi mathvariant="bold-italic">D</mi><msubsup><mrow></mrow><mi>k</mi><mtext>h</mtext></msubsup><mo>,</mo><mi mathvariant="bold-italic">D</mi><msubsup><mrow></mrow><mi>k</mi><mtext>l</mtext></msubsup><mo>,</mo><mi mathvariant="bold-italic">α</mi><msub><mrow></mrow><mi>k</mi></msub></mrow><mo>}</mo></mrow><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mtext>a</mtext><mtext>r</mtext><mtext>g</mtext><mtext>m</mtext><mtext>i</mtext><mtext>n</mtext></mrow></mstyle><mrow><mrow><mo>{</mo><mrow><mi mathvariant="bold-italic">D</mi><msubsup><mrow></mrow><mi>k</mi><mtext>h</mtext></msubsup><mo>,</mo><mi mathvariant="bold-italic">D</mi><msubsup><mrow></mrow><mi>k</mi><mtext>l</mtext></msubsup><mo>,</mo><mi mathvariant="bold-italic">α</mi><msub><mrow></mrow><mi>k</mi></msub></mrow><mo>}</mo></mrow></mrow></munder><mrow><mo>{</mo><mrow><mrow><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">Y</mi><msubsup><mrow></mrow><mi>k</mi><mtext>l</mtext></msubsup><mo>-</mo><mi mathvariant="bold-italic">D</mi><msubsup><mrow></mrow><mi>k</mi><mtext>l</mtext></msubsup><mi mathvariant="bold-italic">α</mi><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false">∥</mo></mrow><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup><mo>+</mo></mrow></mrow></mtd></mtr><mtr><mtd><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mrow><mrow><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">Y</mi><msubsup><mrow></mrow><mi>k</mi><mtext>h</mtext></msubsup><mo>-</mo><mi mathvariant="bold-italic">D</mi><msubsup><mrow></mrow><mi>k</mi><mtext>h</mtext></msubsup><mi mathvariant="bold-italic">α</mi><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false">∥</mo></mrow><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup><mo>+</mo><mi>λ</mi><mrow><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">α</mi><msub><mrow></mrow><mi>k</mi></msub><mo>-</mo><mi mathvariant="bold-italic">β</mi><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false">∥</mo></mrow><msub><mrow></mrow><mn>1</mn></msub></mrow><mo>}</mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>5</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="75">其中,<i><b>D</b></i><sup>l</sup><sub><i>k</i></sub>和<i><b>D</b></i><sup>h</sup><sub><i>k</i></sub>分别表示第<i>k</i>个聚类<mathml id="122"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mrow><mrow><mrow><mo>{</mo><mrow><mi mathvariant="bold-italic">Y</mi><msup><mrow></mrow><mtext>l</mtext></msup><mo>,</mo><mi mathvariant="bold-italic">Y</mi><msup><mrow></mrow><mtext>h</mtext></msup></mrow><mo>}</mo></mrow></mrow><mo>|</mo></mrow></mrow><msubsup><mrow></mrow><mrow><mi>k</mi><mo>=</mo><mi>i</mi></mrow><mi>Κ</mi></msubsup></mrow></math></mathml>所对应的<i>LR</i>/<i>HR</i>子字典对,矩阵<i>α</i><sub><i>k</i></sub>表示第<i>k</i>个结构组的稀疏表示向量。本文采用交替方向乘子算法<citation id="149" type="reference"><link href="41" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>对式(5)进行求解。首先固定字典对<i><b>D</b></i>,求解LR/HR图像样本集在字典对上的稀疏表示<i>α</i>,然后根据稀疏表示<i>α</i>更新字典对<i><b>D</b></i>。</p>
                </div>
                <div class="p1">
                    <p id="76">为提高重建算法的有效性和鲁棒性,对具有相似纹理结构的LR/HR图像样本集<mathml id="123"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>{</mo><mrow><mi mathvariant="bold-italic">Y</mi><msup><mrow></mrow><mtext>l</mtext></msup><mo>,</mo><mi mathvariant="bold-italic">Y</mi><msup><mrow></mrow><mtext>h</mtext></msup></mrow><mo>}</mo></mrow></mrow></math></mathml>进行特征提取,以实现字典学习。为确保<i>LR</i>/<i>HR</i>字典对<mathml id="124"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>{</mo><mrow><mi mathvariant="bold-italic">D</mi><msup><mrow></mrow><mtext>l</mtext></msup><mo>,</mo><mi mathvariant="bold-italic">D</mi><msup><mrow></mrow><mtext>h</mtext></msup></mrow><mo>}</mo></mrow></mrow></math></mathml>间的稀疏共生先验,在图像样本集<mathml id="125"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>{</mo><mrow><mi mathvariant="bold-italic">Y</mi><msup><mrow></mrow><mtext>l</mtext></msup><mo>,</mo><mi mathvariant="bold-italic">Y</mi><msup><mrow></mrow><mtext>h</mtext></msup></mrow><mo>}</mo></mrow></mrow></math></mathml>的构造过程中,<i>LR</i>图像<i><b>y</b></i><sup>l</sup>由HR图像<i><b>y</b></i><sup>h</sup>降采样得到。</p>
                </div>
                <div class="p1">
                    <p id="77">在得到LR/HR稀疏字典对和对应的稀疏表示系数矩阵<mathml id="126"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi mathvariant="bold-italic">α</mi></mstyle><mrow><mspace width="0.25em" /><mo>∼</mo></mrow></mover><msub><mrow></mrow><mi>k</mi></msub></mrow></math></mathml>后,针对任意输入的<i>LR</i>图像块<i><b>y</b></i><sub><i>k</i></sub>利用欧式距离作为度量标准,寻找其与所有聚类中心<mathml id="127"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mrow><mrow><mi mathvariant="bold-italic">c</mi><msub><mrow></mrow><mi>k</mi></msub></mrow><mo>|</mo></mrow></mrow><msubsup><mrow></mrow><mrow><mi>k</mi><mo>=</mo><mi>i</mi></mrow><mi>Κ</mi></msubsup></mrow></math></mathml>最相似的聚类。然后将该聚类对应的<i>LR</i>/<i>HR</i>子字典对<mathml id="128"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>{</mo><mrow><mi mathvariant="bold-italic">D</mi><msubsup><mrow></mrow><mi>k</mi><mtext>l</mtext></msubsup><mo>,</mo><mi mathvariant="bold-italic">D</mi><msubsup><mrow></mrow><mi>k</mi><mtext>h</mtext></msubsup></mrow><mo>}</mo></mrow></mrow></math></mathml>应用于彩色图像的超分辨率重建过程中,重建后的<i>HR</i>图像块<i><b>x</b></i><sub><i>k</i></sub>为:</p>
                </div>
                <div class="p1">
                    <p id="78" class="code-formula">
                        <mathml id="78"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>k</mi></msub><mo>=</mo><mi mathvariant="bold-italic">D</mi><msubsup><mrow></mrow><mi>k</mi><mtext>h</mtext></msubsup><mover><mstyle mathsize="140%" displaystyle="true"><mi mathvariant="bold-italic">α</mi></mstyle><mrow><mspace width="0.25em" /><mo>∼</mo></mrow></mover><msub><mrow></mrow><mi>k</mi></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>6</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <h4 class="anchor-tag" id="79" name="79">2.2 伪影消除</h4>
                <div class="p1">
                    <p id="80">由于第2.1节中提出的算法仅对彩色图像的Y通道信息进行重建,并未考虑色彩通道间的相关性,因此重建图像中易出现色彩伪影的问题。考虑到大多数自然图像的高频成份,例如边界、轮廓等特征,在RGB色彩通道中具有很强的相关性,因此在利用高通滤波方法得到各色彩通道边界信息的基础上,通过RGB色彩通道边界一致性约束强化色彩通道间的相关性,以消除重建图像的色彩伪影。</p>
                </div>
                <div class="p1">
                    <p id="81">本文参考文献<citation id="150" type="reference">[<a class="sup">21</a>]</citation>提出的RGB色彩通道间边界相似性模型,其色彩通道约束如下:</p>
                </div>
                <div class="p1">
                    <p id="82">‖<i><b>S</b></i><sub><i>μ</i></sub><i><b>x</b></i><sub><i>Rμ</i></sub>-<i><b>S</b></i><sub><i>ν</i></sub><i><b>x</b></i><sub><i>Rν</i></sub>‖<sub>2</sub>&lt;<i>ε</i><sub><i>μν</i></sub>      (7)</p>
                </div>
                <div class="p1">
                    <p id="83">其中,<i>μ</i>、<mathml id="129"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>ν</mi><mo>∈</mo><mrow><mo>{</mo><mrow><mi>r</mi><mo>,</mo><mi>g</mi><mo>,</mo><mi>b</mi></mrow><mo>}</mo></mrow><mo>,</mo><mi>μ</mi><mo>≠</mo><mi>ν</mi><mo>,</mo><mi>r</mi></mrow></math></mathml>、g、b分别表示<i>RGB</i>色彩通道,矩阵<i><b>S</b></i>表示高通滤波器,<i><b>S</b></i><sub><i>r</i></sub><i><b>y</b></i><sub><i>Rr</i></sub>表示在高分辨率图像<i>r</i>通道中的边界信息。</p>
                </div>
                <div class="p1">
                    <p id="84">将上述色彩通道约束对应于每个独立色彩通道的稀疏表示向量,具体如下:</p>
                </div>
                <div class="p1">
                    <p id="85" class="code-formula">
                        <mathml id="85"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>{</mo><mrow><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mrow><mi>R</mi><msub><mrow></mrow><mi>r</mi></msub></mrow></msub><mo>=</mo><mi mathvariant="bold-italic">D</mi><msub><mrow></mrow><mrow><mi>R</mi><msub><mrow></mrow><mi>r</mi></msub></mrow></msub><mi mathvariant="bold-italic">α</mi><msub><mrow></mrow><mi>r</mi></msub><mo>;</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mrow><mi>R</mi><msub><mrow></mrow><mi>g</mi></msub></mrow></msub><mo>=</mo><mi mathvariant="bold-italic">D</mi><msub><mrow></mrow><mrow><mi>R</mi><msub><mrow></mrow><mi>g</mi></msub></mrow></msub><mi mathvariant="bold-italic">α</mi><msub><mrow></mrow><mi>g</mi></msub><mo>;</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mrow><mi>R</mi><msub><mrow></mrow><mi>b</mi></msub></mrow></msub><mo>=</mo><mi mathvariant="bold-italic">D</mi><msub><mrow></mrow><mrow><mi>R</mi><msub><mrow></mrow><mi>b</mi></msub></mrow></msub><mi mathvariant="bold-italic">α</mi><msub><mrow></mrow><mi>b</mi></msub></mrow><mo>}</mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>8</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="86">其中,<i><b>D</b></i><sub><i>Ri</i></sub>(<i>i</i>=<i>r</i>,<i>g</i>,<i>b</i>)表示各色彩通道对应的稀疏字典。</p>
                </div>
                <div class="p1">
                    <p id="87">文献<citation id="151" type="reference">[<a class="sup">21</a>]</citation>将交叉通道色彩约束应用于彩色图像超分辨率重建任务中。本文在该模型的基础上,将色彩通道约束应用于彩色图像的色彩伪影消除过程中。在字典训练阶段,只训练高分辨率色彩约束稀疏字典,并借助该字典完成重建彩色图像的优化处理。彩色图像联合通道约束的色彩伪影消除模型可表示为:</p>
                </div>
                <div class="p1">
                    <p id="88" class="code-formula">
                        <mathml id="88"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtable><mtr><mtd columnalign="left"><mo stretchy="false">{</mo><mi mathvariant="bold-italic">α</mi><msub><mrow></mrow><mi>r</mi></msub><mo>,</mo><mi mathvariant="bold-italic">α</mi><msub><mrow></mrow><mi>g</mi></msub><mo>,</mo><mi mathvariant="bold-italic">α</mi><msub><mrow></mrow><mi>b</mi></msub><mo stretchy="false">}</mo><mo>=</mo><mtext>a</mtext><mtext>r</mtext><mtext>g</mtext><mtext>m</mtext><mtext>i</mtext><mtext>n</mtext><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi mathvariant="bold-italic">c</mi><mo>∈</mo><mo stretchy="false">{</mo><mi mathvariant="bold-italic">α</mi><msub><mrow></mrow><mi>r</mi></msub><mo>,</mo><mi mathvariant="bold-italic">α</mi><msub><mrow></mrow><mi>g</mi></msub><mo>,</mo><mi mathvariant="bold-italic">α</mi><msub><mrow></mrow><mi>b</mi></msub><mo stretchy="false">}</mo></mrow></munder><mrow><mrow><mo>{</mo><mrow><mrow><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi mathvariant="bold-italic">c</mi></msub><mo>-</mo><mi mathvariant="bold-italic">D</mi><msub><mrow></mrow><mi>R</mi></msub><mi mathvariant="bold-italic">α</mi><msub><mrow></mrow><mi mathvariant="bold-italic">c</mi></msub><mo stretchy="false">∥</mo></mrow><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup><mo>+</mo></mrow></mrow></mrow></mstyle></mtd></mtr><mtr><mtd columnalign="left"><mtext> </mtext><mtext> </mtext><mrow><mrow><mrow><mi>λ</mi><mo stretchy="false">∥</mo><mi>α</mi><msub><mrow></mrow><mi mathvariant="bold-italic">c</mi></msub><mo stretchy="false">∥</mo></mrow><msub><mrow></mrow><mtext>p</mtext></msub></mrow><mo>}</mo></mrow><mo>+</mo><mi>γ</mi><mrow><mo>[</mo><mrow><mrow><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">S</mi><msub><mrow></mrow><mi>r</mi></msub><mi mathvariant="bold-italic">D</mi><msub><mrow></mrow><mrow><mi>R</mi><msub><mrow></mrow><mi>r</mi></msub></mrow></msub><mi mathvariant="bold-italic">α</mi><msub><mrow></mrow><mi>r</mi></msub><mo>-</mo><mi mathvariant="bold-italic">S</mi><msub><mrow></mrow><mi>g</mi></msub><mi mathvariant="bold-italic">D</mi><msub><mrow></mrow><mrow><mi>R</mi><msub><mrow></mrow><mi>g</mi></msub></mrow></msub><mi mathvariant="bold-italic">α</mi><msub><mrow></mrow><mi>g</mi></msub><mo stretchy="false">∥</mo></mrow><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup><mo>+</mo></mrow></mrow></mtd></mtr><mtr><mtd columnalign="left"><mtext> </mtext><mspace width="0.25em" /><mrow><mrow><mrow><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">S</mi><msub><mrow></mrow><mi>g</mi></msub><mi mathvariant="bold-italic">D</mi><msub><mrow></mrow><mrow><mi>R</mi><msub><mrow></mrow><mi>g</mi></msub></mrow></msub><mi mathvariant="bold-italic">α</mi><msub><mrow></mrow><mi>g</mi></msub><mo>-</mo><mi mathvariant="bold-italic">S</mi><msub><mrow></mrow><mi>b</mi></msub><mi mathvariant="bold-italic">D</mi><msub><mrow></mrow><mrow><mi>R</mi><msub><mrow></mrow><mi>b</mi></msub></mrow></msub><mi mathvariant="bold-italic">α</mi><msub><mrow></mrow><mi>b</mi></msub><mo stretchy="false">∥</mo></mrow><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup><mo>+</mo><mrow><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">S</mi><msub><mrow></mrow><mi>b</mi></msub><mi mathvariant="bold-italic">D</mi><msub><mrow></mrow><mrow><mi>R</mi><msub><mrow></mrow><mi>b</mi></msub></mrow></msub><mi mathvariant="bold-italic">α</mi><msub><mrow></mrow><mi>b</mi></msub><mo>-</mo><mi mathvariant="bold-italic">S</mi><msub><mrow></mrow><mi>r</mi></msub><mi mathvariant="bold-italic">D</mi><msub><mrow></mrow><mrow><mi>R</mi><msub><mrow></mrow><mi>r</mi></msub></mrow></msub><mi mathvariant="bold-italic">α</mi><msub><mrow></mrow><mi>r</mi></msub><mo stretchy="false">∥</mo></mrow><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup></mrow><mo>]</mo></mrow></mtd></mtr></mtable><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>9</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="89">其中,p=1表示范数约束。为简化运算,对每个边缘差异项和色彩通道假定相同的正则化参数<i>γ</i>和<i>λ</i>。</p>
                </div>
                <div class="p1">
                    <p id="90">最小化式(9)所建立的损失函数能量,得到式(10)。</p>
                </div>
                <div class="p1">
                    <p id="91" class="code-formula">
                        <mathml id="91"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mover><mstyle mathsize="140%" displaystyle="true"><mi mathvariant="bold-italic">α</mi></mstyle><mrow><mspace width="0.25em" /><mo>^</mo></mrow></mover><mo>=</mo><mrow><mi>arg</mi></mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mi>α</mi></munder><mspace width="0.25em" /><mi mathvariant="bold-italic">α</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mrow><mo>[</mo><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mi mathvariant="bold-italic">D</mi><msubsup><mrow></mrow><mi>R</mi><mtext>Τ</mtext></msubsup><mi mathvariant="bold-italic">D</mi><msub><mrow></mrow><mi>R</mi></msub><mo>+</mo><mn>2</mn><mi>λ</mi><mi mathvariant="bold-italic">D</mi><msubsup><mrow></mrow><mi>R</mi><mtext>Τ</mtext></msubsup><mi>S</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mrow><mo>(</mo><mrow><mi mathvariant="bold-italic">Ι</mi><mo>-</mo><mi mathvariant="bold-italic">Ρ</mi><msubsup><mrow></mrow><mi>S</mi><mtext>Τ</mtext></msubsup></mrow><mo>)</mo></mrow><mi mathvariant="bold-italic">S</mi><mi mathvariant="bold-italic">D</mi><msub><mrow></mrow><mi>R</mi></msub></mrow><mo>]</mo></mrow><mi mathvariant="bold-italic">α</mi><mo>-</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mi mathvariant="bold-italic">y</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">D</mi><msub><mrow></mrow><mi>R</mi></msub><mi mathvariant="bold-italic">α</mi><mo>+</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mi mathvariant="bold-italic">y</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">y</mi><mo>+</mo><mi>λ</mi><mrow><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">α</mi><mo stretchy="false">∥</mo></mrow><msub><mrow></mrow><mn>1</mn></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>0</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="92">其中,<mathml id="130"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi mathvariant="bold-italic">α</mi></mstyle><mrow><mspace width="0.25em" /><mo>^</mo></mrow></mover></mrow></math></mathml>表示由<i>r</i>、<i>g</i>、<i>b</i>通道联合稀疏编码得到的<i>α</i><sub><i>r</i></sub>、<i>α</i><sub><i>g</i></sub>、<i>α</i><sub><i>b</i></sub>耦合链接得到的稀疏系数矩阵。然后采用快速迭代收敛算法<citation id="152" type="reference"><link href="45" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>求解式(10)。</p>
                </div>
                <h4 class="anchor-tag" id="93" name="93">2.3 超分辨率重建</h4>
                <div class="p1">
                    <p id="94">本文联合非局部稀疏模型与色彩通道约束模型,将LR、HR图像块间的非局部相似性约束与彩色图像色彩通道间的相关性约束进行有效结合,具体步骤如下:</p>
                </div>
                <div class="p1">
                    <p id="95">1)输入<mathml id="131"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>{</mo><mrow><mi mathvariant="bold-italic">D</mi><msup><mrow></mrow><mtext>l</mtext></msup><mo>,</mo><mi mathvariant="bold-italic">D</mi><msup><mrow></mrow><mtext>h</mtext></msup></mrow><mo>}</mo></mrow></mrow></math></mathml>、<i><b>D</b></i><sub><i>R</i></sub>以及待重建的LR彩色图像<i><b>X</b></i><sub>l</sub>。</p>
                </div>
                <div class="p1">
                    <p id="96">2)通过双三次插值对<i><b>X</b></i><sub>l</sub>采样。</p>
                </div>
                <div class="p1">
                    <p id="97">3)在YCbCr色彩空间内分离出图像<i><b>X</b></i><sub>l</sub>的亮度图像<i><b>y</b></i>,对<i><b>y</b></i>分块并提取与图像块<i><b>y</b></i><sub><i>k</i></sub>(<i>k</i>=1,2,…,<i>N</i>)具有相似纹理结构的所有图像块,组成图像块结构组<i><b>S</b></i><sub><i><b>y</b></i></sub><sub><i>k</i></sub>。</p>
                </div>
                <div class="p1">
                    <p id="98">4)利用<i><b>y</b></i><sub><i>k</i></sub>和<i><b>D</b></i><sup>l</sup>求得图像块非局部稀疏系数<mathml id="132"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi mathvariant="bold-italic">α</mi></mstyle><mrow><mspace width="0.25em" /><mo>∼</mo></mrow></mover><msub><mrow></mrow><mi>k</mi></msub></mrow></math></mathml>。</p>
                </div>
                <div class="p1">
                    <p id="99">5)重建彩色图像块的<i>Y</i>通道分量<mathml id="133"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi mathvariant="bold-italic">x</mi></mstyle><mrow><mspace width="0.25em" /><mo>∼</mo></mrow></mover><msub><mrow></mrow><mi>k</mi></msub></mrow></math></mathml>:<mathml id="134"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi mathvariant="bold-italic">x</mi></mstyle><mrow><mspace width="0.25em" /><mo>∼</mo></mrow></mover><msub><mrow></mrow><mi>k</mi></msub><mo>=</mo><mi mathvariant="bold-italic">D</mi><msubsup><mrow></mrow><mi>k</mi><mtext>h</mtext></msubsup><mover><mstyle mathsize="140%" displaystyle="true"><mi mathvariant="bold-italic">α</mi></mstyle><mrow><mspace width="0.25em" /><mo>∼</mo></mrow></mover><msub><mrow></mrow><mi>k</mi></msub></mrow></math></mathml>。通过逐个像素叠加得到重建彩色图像的<i>Y</i>分量信息,对<i><b>X</b></i><sub>l</sub>的Cb和Cr通道分量采用双三次插值方式进行重建。</p>
                </div>
                <div class="p1">
                    <p id="100">6)将步骤5)重建得到的图像通过式(9)求出色彩联合通道约束表示的系数<mathml id="135"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi mathvariant="bold-italic">α</mi></mstyle><mrow><mspace width="0.25em" /><mo>^</mo></mrow></mover></mrow></math></mathml>。</p>
                </div>
                <div class="p1">
                    <p id="101">7)输出最终HR彩色图像<mathml id="136"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi mathvariant="bold-italic">X</mi></mstyle><mrow><mspace width="0.25em" /><mo>^</mo></mrow></mover><msub><mrow></mrow><mtext>h</mtext></msub><mo>=</mo><mi mathvariant="bold-italic">D</mi><msub><mrow></mrow><mi>R</mi></msub><mover><mstyle mathsize="140%" displaystyle="true"><mi mathvariant="bold-italic">α</mi></mstyle><mrow><mspace width="0.25em" /><mo>^</mo></mrow></mover></mrow></math></mathml>。</p>
                </div>
                <h3 id="102" name="102" class="anchor-tag">3 实验结果与分析</h3>
                <div class="p1">
                    <p id="103">本文从标准图像处理数据库中挑选5幅图片进行测试,如图1所示。利用峰值信噪比(Peak Signal to Noise Ratio,PSNR)和结构相似性(Structural Similarity Index Measurement,SSIM)对重建图像的质量进行客观评价,并与双三次插值算法(Bicubic Interpolation,BI)、ScSR算法<citation id="153" type="reference"><link href="31" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>、NCSR算法<citation id="154" type="reference"><link href="37" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>、MCcSR算法<citation id="155" type="reference"><link href="43" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>进行对比。</p>
                </div>
                <div class="area_img" id="104">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201910045_104.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5pRHZtTnQxdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 测试图像" src="Detail/GetImg?filename=images/JSJC201910045_104.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5pRHZtTnQxdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图1 测试图像</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201910045_104.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5pRHZtTnQxdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="105">为验证本文算法的有效性,采用文献<citation id="156" type="reference">[<a class="sup">15</a>]</citation>提供的训练样本库进行实验。在训练和测试时,LR图像块大小为6像素×6像素,图像块之间重叠4像素,上采样因子为2,迭代次数为20,字典大小为512。图2和图3分别为测试图像蝴蝶和车辆通过不同重建方法得到的实验结果。表1、表2分别为5种算法的PSNR、SSIM结果对比。</p>
                </div>
                <div class="area_img" id="106">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201910045_106.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5pRHZtTnQxdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 蝴蝶图像超分辨率重建结果对比" src="Detail/GetImg?filename=images/JSJC201910045_106.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5pRHZtTnQxdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图2 蝴蝶图像超分辨率重建结果对比</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201910045_106.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5pRHZtTnQxdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="107">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201910045_107.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5pRHZtTnQxdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 车辆图像超分辨率重建结果对比" src="Detail/GetImg?filename=images/JSJC201910045_107.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5pRHZtTnQxdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图3 车辆图像超分辨率重建结果对比</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201910045_107.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5pRHZtTnQxdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="108">
                    <p class="img_tit"><b>表1 5种算法对不同图像的PSNR结果对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note">dB</p>
                    <table id="108" border="1"><tr><td>图像</td><td>BI算法</td><td>ScSR<br />算法</td><td>NCSR<br />算法</td><td>MCcSR<br />算法</td><td>本文<br />算法</td></tr><tr><td><br />蝴蝶</td><td>27.42</td><td>30.51</td><td>30.63</td><td>30.23</td><td>30.89</td></tr><tr><td><br />猴子</td><td>24.66</td><td>25.08</td><td>24.94</td><td>24.92</td><td>25.31</td></tr><tr><td><br />鸟</td><td>36.68</td><td>39.31</td><td>38.41</td><td>37.34</td><td>39.65</td></tr><tr><td><br />车辆</td><td>25.66</td><td>27.33</td><td>27.03</td><td>26.94</td><td>27.85</td></tr><tr><td><br />漫画人物</td><td>26.06</td><td>27.28</td><td>27.22</td><td>26.52</td><td>27.81</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="109">
                    <p class="img_tit"><b>表2 5种算法对不同图像的SSIM结果对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="109" border="1"><tr><td>图像</td><td>BI算法</td><td>ScSR<br />算法</td><td>NCSR<br />算法</td><td>MCcSR<br />算法</td><td>本文<br />算法</td></tr><tr><td><br />蝴蝶</td><td>0.915 0</td><td>0.951 2</td><td>0.951 9</td><td>0.950 3</td><td>0.952 7</td></tr><tr><td><br />猴子</td><td>0.696 8</td><td>0.758 1</td><td>0.724 9</td><td>0.721 3</td><td>0.760 2</td></tr><tr><td><br />鸟</td><td>0.971 5</td><td>0.979 6</td><td>0.976 5</td><td>0.974 2</td><td>0.982 4</td></tr><tr><td><br />车辆</td><td>0.850 1</td><td>0.895 9</td><td>0.863 2</td><td>0.864 7</td><td>0.907 0</td></tr><tr><td><br />漫画人物</td><td>0.850 9</td><td>0.899 3</td><td>0.896 1</td><td>0.892 3</td><td>0.903 3</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="110">从图2可以看出,BI算法得到的重建图像质量较差,特别是翅膀部位的纹理边缘区域不够平滑,存在较明显的锯齿和伪影现象。ScSR算法的重建结果较好,但整体细节较为模糊且存在一定的色彩伪影。NCSR算法对于图像平滑区域可实现很好的重建,但其对细节较为丰富的区域过度平滑,导致细节信息丢失。MCcSR算法可对色彩信息实现较好的重建,但对于图像细节的恢复表现较差。本文算法在增强重建图像边缘细节信息的同时,可有效去除伪影,使HR图像的视觉效果较好。</p>
                </div>
                <div class="p1">
                    <p id="111">从图3可以看出,BI算法存在很明显的模糊问题。ScSR算法重建得到的图像细节部分比较清晰,但在边缘区域存在锯齿现象。NCSR算法的重建图像中存在一定的色彩伪影。MCcSR算法对重建图像的色彩信息实现了较好的重建,但对局部细节信息,如头盔部位的文字区域恢复效果不够理想。本文算法在重建图像色彩保持及图像细节恢复方面取得了更好的效果。</p>
                </div>
                <div class="p1">
                    <p id="112">在利用稀疏表示方法对彩色图像进行超分辨率重建时,重建图像的质量与稀疏字典的尺寸有直接关系。以蝴蝶图像作为实验样本,图4和图5分别给出4种算法在不同字典尺寸下取得的重建图像的PSNR与SSIM的对比结果。</p>
                </div>
                <div class="area_img" id="113">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201910045_113.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5pRHZtTnQxdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 字典尺寸对重建图像PSNR的影响" src="Detail/GetImg?filename=images/JSJC201910045_113.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5pRHZtTnQxdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图4 字典尺寸对重建图像PSNR的影响</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201910045_113.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5pRHZtTnQxdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="114">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201910045_114.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5pRHZtTnQxdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 字典尺寸对重建图像SSIM的影响" src="Detail/GetImg?filename=images/JSJC201910045_114.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5pRHZtTnQxdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图5 字典尺寸对重建图像SSIM的影响</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201910045_114.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5pRHZtTnQxdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h3 id="115" name="115" class="anchor-tag">4 结束语</h3>
                <div class="p1">
                    <p id="116">本文提出一种非局部稀疏与色彩约束方法相结合的彩色图像超分辨率重建算法。利用基于非局部稀疏模型对彩色图像亮度通道进行超分辨率重建,以实现较好的细节恢复效果。通过色彩相关性约束模型进行通道约束以消除重建图像的色彩伪影。实验结果表明,该算法能提升重建图像视觉质量并消除重建色彩伪影。下一步将把四元稀疏表示理论应用于非局部稀疏模型,提高彩色图像的重建质量。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XTYD201803032&amp;v=MzIyMzBuTXJJOUdab1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUnRGeTNrVkwzTlBUblNhckc0SDk=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5pRHZtTnQxdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> 徐志刚,李文文,袁飞祥,等.基于稀疏表示和多成分字典学习的超分辨率重建[J].系统工程与电子技术,2018,40(3):699-703.
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZGTB201505002&amp;v=MjI3MjZPZVplUnRGeTNrVkwzTlB5cmZiTEc0SDlUTXFvOUZab1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkw=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5pRHZtTnQxdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> 宋熙煜,周利莉,李中国,等.图像分割中的超像素方法研究综述[J].中国图象图形学报,2015,20(5):599-608.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201606047&amp;v=MjkzMDlqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSdEZ5M2tWTDNOTHo3QmJiRzRIOWZNcVk5Qlk0UUtESDg0dlI0VDY=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5pRHZtTnQxdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> 李云峰,李晟阳,韩茜茜.基于多邻域信息的监控图像超分辨率算法[J].计算机工程,2016,42(6):261-264.
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201603050&amp;v=MzE2ODViRzRIOWZNckk5QVpJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSdEZ5M2tWTDNOTHo3QmI=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5pRHZtTnQxdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b> 戚曹,朱桂斌,唐鉴波,等.基于稀疏表示的红外视频图像超分辨率算法[J].计算机工程,2016,42(3):278-282.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Color sparse representations for image processing:review,models,and prospects">

                                <b>[5]</b> BARTHÉLEMY Q,LARUE A,MARS J I.Color sparse representations for image processing:review,models,and prospects[J].IEEE Transactions on Image Processing,2015,24(11):3978-3989.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image Super-resolution: The Techniques,Applications, and Future">

                                <b>[6]</b> YUE Linwei,SHEN Huanfeng,LI Jie,et al.Image super-resolution:the techniques,applications,and future[J].Signal Processing,2016,128:389-408.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=RAISR:rapid and accurate image super resolution">

                                <b>[7]</b> ROMANO Y,ISIDORO J,MILANFAR P.Raisr:rapid and accurate image super resolution[J].IEEE Transactions on Computational Imaging,2017,3(1):110-125.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image super-resolution using deep convolutional networks">

                                <b>[8]</b> DONG Chao,LOY Chenchange,HE Kaiming,et al.Image super-resolution using deep convolutional networks[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2016,38(2):295-307.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image Deblurring and Super-Resolution by Adaptive Sparse Domain Selection and Adaptive Regularization">

                                <b>[9]</b> DONG Weisheng,ZHANG Lei,SHI Guangming,et al.Image deblurring and super-resolution by adaptive sparse domain selection and adaptive regularization[J].IEEE Transactions on Image Processing,2011,20(7):1838-1857.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Coupled Dictionary Training for Image Super-Resolution">

                                <b>[10]</b> YANG Jianchao,WANG Zhaowen,LIN Zhe,et al.Coupled dictionary training for image super-resolution[J].IEEE Transactions on Image Processing,2012,21(8):3467-3478.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image super-resolution based on dictionary learning and anchored neighborhood regression with mutual incoherence">

                                <b>[11]</b> ZHANG Yulun,GU Kaiyu,ZHANG Yongbing,et al.Image super-resolution based on dictionary learning and anchored neighborhood regression with mutual incoherence[C]//Proceedings of 2015 IEEE International Conference on Image Processing.Washington D.C.,USA:IEEE Press,2015:591-595.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A+:adjusted anchored neighborhood regression for fast super-resolution">

                                <b>[12]</b> TIMOFTE R,DE-SMET V,VAN GOOL L.A+:adjusted anchored neighborhood regression for fast super-resolution[C]//Proceedings of Asian Conference on Computer Vision.Berlin,Germany:Springer,2014:111-126.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_13" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201302045&amp;v=MDcyNTQza1ZMM05MejdCZDdHNEg5TE1yWTlCWVlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJ0Rnk=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5pRHZtTnQxdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[13]</b> 杨玲,刘怡光,黄蓉刚,等.新的基于稀疏表示单张彩色超分辨率算法[J].计算机应用,2013,33(2):472-475.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Single-image super-resolution in RGB space via group sparse representation">

                                <b>[14]</b> CHENG Ming,WANG Cheng,LI Jonathan.Single-image super-resolution in RGB space via group sparse representation[J].IET Image Processing,2014,9(6):461-467.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image super-resolution via sparse representation">

                                <b>[15]</b> YANG Jianchao,WRIGHT J,HUANG T S,et al.Image super-resolution via sparse representation[J].IEEE Transactions on Image Processing,2010,19(11):2861-2873.
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Super-resolution via adaptive combination of color channels">

                                <b>[16]</b> XU Jian,CHANG Zhiguo,FAN Jiulun,et al.Super-resolution via adaptive combination of color channels[J].Multimedia Tools and Applications,2017,76(1):1553-1584.
                            </a>
                        </p>
                        <p id="35">
                            <a id="bibliography_17" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=WLXB201404021&amp;v=MDE2MTdMM05NaUhUYkxHNEg5WE1xNDlIWllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJ0Rnkza1Y=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5pRHZtTnQxdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[17]</b> 邓承志,田伟,陈盼,等.基于局部约束群稀疏的红外图像超分辨率重建[J].物理学报,2014,63(4):144-151.
                            </a>
                        </p>
                        <p id="37">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Nonlocally Centralized Sparse Representation for Image Restoration">

                                <b>[18]</b> DONG Weisheng,ZHANG Lei,SHI Guangming,et al.Nonlocally centralized sparse representation for image restoration[J].IEEE Transactions on Image Processing,2013,22(4):1620-1630.
                            </a>
                        </p>
                        <p id="39">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=CCR:Clustering and collaborative representation for fast single image super-resolution">

                                <b>[19]</b> ZHANG Yongbing,ZHANG Yulun,ZHANG Jian,et al.CCR:clustering and collaborative representation for fast single image super-resolution[J].IEEE Transactions on Multimedia,2016,18(3):405-417.
                            </a>
                        </p>
                        <p id="41">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fast-minimization algorithms for robust face recognition">

                                <b>[20]</b> YANG A Y,ZHOU Zihan,BALASUBRAMANIAN A G,et al.Fast L1-minimization algorithms for robust face recognition[J].IEEE Transactions on Image Processing,2013,22(8):3234-3246.
                            </a>
                        </p>
                        <p id="43">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Sparsity-based color image super resolution via exploiting cross channel constraints">

                                <b>[21]</b> MOUSAVI H,MONGA V.Sparsity-based color image super resolution via exploiting cross channel constraints[J].IEEE Transactions on Image Processing,2017,26(11):5094-5106.
                            </a>
                        </p>
                        <p id="45">
                            <a id="bibliography_22" >
                                    <b>[22]</b>
                                 BECK A,TEBOULLE M.A fast iterative shrinkage-thresholding algorithm for linear inverse problems[J].SIAM Journal on Imaging Sciences,2009,2(1):183-202.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJC201910045" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5pRHZtTnQxdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201910045&amp;v=MzIyMzJyNDlCWVlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJ0Rnkza1ZMM0tMejdCYmJHNEg5ak4=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5pRHZtTnQxdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
