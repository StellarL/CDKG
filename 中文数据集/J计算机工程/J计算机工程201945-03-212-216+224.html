<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637130634556681250%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJC201903036%26RESULT%3d1%26SIGN%3daH0WOPw1BJ0vZwllAle0TurU06c%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201903036&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201903036&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201903036&amp;v=MDc5NTJPM3pxcUJ0R0ZyQ1VSTE9lWmVSb0Z5N2xVcnZJTHo3QmJiRzRIOWpNckk5R1lvUUtESDg0dlI0VDZqNTQ=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#45" data-title="0 概述 ">0 概述</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#50" data-title="1 美观度评价模型 ">1 美观度评价模型</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#69" data-title="1.1 美学信息通道构造">1.1 美学信息通道构造</a></li>
                                                <li><a href="#81" data-title="1.2 场景信息通道设计">1.2 场景信息通道设计</a></li>
                                                <li><a href="#85" data-title="1.3 两通道设计">1.3 两通道设计</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#91" data-title="2 实验结果与分析 ">2 实验结果与分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#93" data-title="2.1 AVA数据库">2.1 AVA数据库</a></li>
                                                <li><a href="#99" data-title="2.2 单通道分类">2.2 单通道分类</a></li>
                                                <li><a href="#102" data-title="2.3 两通道分类实验">2.3 两通道分类实验</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#107" data-title="3 结束语 ">3 结束语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#52" data-title="&lt;b&gt;图1 两通道网络模型结构框图&lt;/b&gt;"><b>图1 两通道网络模型结构框图</b></a></li>
                                                <li><a href="#54" data-title="&lt;b&gt;图2 高美观度图像&lt;/b&gt;"><b>图2 高美观度图像</b></a></li>
                                                <li><a href="#76" data-title="&lt;b&gt;图3 图像处理&lt;/b&gt;"><b>图3 图像处理</b></a></li>
                                                <li><a href="#95" data-title="&lt;b&gt;图4 部分AVA图像库&lt;/b&gt;"><b>图4 部分AVA图像库</b></a></li>
                                                <li><a href="#101" data-title="&lt;b&gt;表1 美学通道在AVA2库上的准确率对比结果&lt;/b&gt; %"><b>表1 美学通道在AVA2库上的准确率对比结果</b> %</a></li>
                                                <li><a href="#104" data-title="&lt;b&gt;表2 各方法在AVA1库上的实验对比结果&lt;/b&gt; %"><b>表2 各方法在AVA1库上的实验对比结果</b> %</a></li>
                                                <li><a href="#105" data-title="&lt;b&gt;表3 各方法在AVA2库上的实验对比结果&lt;/b&gt; %"><b>表3 各方法在AVA2库上的实验对比结果</b> %</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" title=" DONG Z, TIAN X M.Multi-level photo quality assessment with multi-view features[J].Neurocomputing, 2015, 168:308-319." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES445AB3E6FB1CF3EA84061CF5FD68FD06&amp;v=MDU3Mjh1bWNiN2o5N1NReVVxV1JCZjdyaU1icVpDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZDcGJRMzVONWh3cmkreEtnPU5pZk9mYmU4RzZDK3JQcERFcGtPZndvNg==&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         DONG Z, TIAN X M.Multi-level photo quality assessment with multi-view features[J].Neurocomputing, 2015, 168:308-319.
                                    </a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" title=" TONG H, LI M, ZHANG H J, et al.Classification of digital photos taken by photographers or home users[C]//Proceedings of Pacific-Rim Conference on Multimedia.Berlin, Germany:Springer, 2004:198-205." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Classification of digital photos taken by photographers or home users">
                                        <b>[2]</b>
                                         TONG H, LI M, ZHANG H J, et al.Classification of digital photos taken by photographers or home users[C]//Proceedings of Pacific-Rim Conference on Multimedia.Berlin, Germany:Springer, 2004:198-205.
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title=" DATTA R, JOSHI D, LI J, et al.Studying aesthetics in photographic images using a computational approach[C]//Proceedings of the 9th European Conference on Computer Vision.Berlin, Germany:Springer, 2006:288-301." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Studying aesthetics in photographic images using acomputational approach">
                                        <b>[3]</b>
                                         DATTA R, JOSHI D, LI J, et al.Studying aesthetics in photographic images using a computational approach[C]//Proceedings of the 9th European Conference on Computer Vision.Berlin, Germany:Springer, 2006:288-301.
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" title=" LOWE D G.Distinctive image features from scale-invariant keypoints[J].International Journal of Computer Vision, 2004, 60 (2) :91-110." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002830901&amp;v=MzEzMDM0SHRIT3A0eEZiZXNPWTNrNXpCZGg0ajk5U1hxUnJ4b3hjTUg3UjdxZWJ1ZHRGQzdsVmIvSkpGND1OajdCYXJP&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                         LOWE D G.Distinctive image features from scale-invariant keypoints[J].International Journal of Computer Vision, 2004, 60 (2) :91-110.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" title=" MARCHESOTTI L, PERRONNIN F, LARLUS D, et al.Assessing the aesthetic quality of photographs using generic image descriptors[C]//Proceedings of International Conference on Computer Vision.Washington D.C., USA:IEEE Computer Society, 2011:1784-1791." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Assessing the aesthetic quality of photographs using generic image descriptors">
                                        <b>[5]</b>
                                         MARCHESOTTI L, PERRONNIN F, LARLUS D, et al.Assessing the aesthetic quality of photographs using generic image descriptors[C]//Proceedings of International Conference on Computer Vision.Washington D.C., USA:IEEE Computer Society, 2011:1784-1791.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" title=" 高寒.基于图像分类的图像美学评价研究[D].哈尔滨:哈尔滨工业大学, 2013." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=1014001227.nh&amp;v=MTQwNzMzenFxQnRHRnJDVVJMT2VaZVJvRnk3bFVydklWRjI2R3JPNEg5UE9xSkViUElRS0RIODR2UjRUNmo1NE8=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                         高寒.基于图像分类的图像美学评价研究[D].哈尔滨:哈尔滨工业大学, 2013.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" title=" LU X, LIN Z, JIN H, et al.Rating image aesthetics using deep learning[J].IEEE Transactions on Multimedia, 2015, 17 (11) :2021-2034." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Rating image aesthetics using deep learning">
                                        <b>[7]</b>
                                         LU X, LIN Z, JIN H, et al.Rating image aesthetics using deep learning[J].IEEE Transactions on Multimedia, 2015, 17 (11) :2021-2034.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" title=" WANG Z, CHANG S, DOLCOS F, et al.Brain-inspired deep networks for image aesthetics assessment[EB/OL].[2017-08-25].https://arxiv.org/pdf/1601.04155.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Brain-inspired deep networks for image aesthetics assessment">
                                        <b>[8]</b>
                                         WANG Z, CHANG S, DOLCOS F, et al.Brain-inspired deep networks for image aesthetics assessment[EB/OL].[2017-08-25].https://arxiv.org/pdf/1601.04155.pdf.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" title=" TIAN X, DONG Z, YANG K, et al.Query-dependent aesthetic model with deep learning for photo quality assessment[J].IEEE Transactions on Multimedia, 2015, 17 (11) :2035-2048." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Query-Dependent Aesthetic Model With Deep Learning for Photo Quality Assessment">
                                        <b>[9]</b>
                                         TIAN X, DONG Z, YANG K, et al.Query-dependent aesthetic model with deep learning for photo quality assessment[J].IEEE Transactions on Multimedia, 2015, 17 (11) :2035-2048.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_10" title=" ZHOU Y, LU X, ZHANG J, et al.Joint image and text representation for aesthetics analysis[C]//Proceedings of the 24th ACM International Conference on Multimedia.New York, USA:ACM Press, 2016:262-266." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Joint image and text representation for aesthetics analysis">
                                        <b>[10]</b>
                                         ZHOU Y, LU X, ZHANG J, et al.Joint image and text representation for aesthetics analysis[C]//Proceedings of the 24th ACM International Conference on Multimedia.New York, USA:ACM Press, 2016:262-266.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_11" title=" POWER J D, COHEN A L, NELSON S M, et al.Functional network organization of the human brain[J].Neuron, 2011, 72 (4) :665-678." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012300023745&amp;v=MTM3OTJVTG5JSmw0VmJoST1OaWZPZmJLN0h0RE9ySTlGWk9rTUMzZzhvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbQ==&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[11]</b>
                                         POWER J D, COHEN A L, NELSON S M, et al.Functional network organization of the human brain[J].Neuron, 2011, 72 (4) :665-678.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_12" title=" VEERINA P.Learning good taste:classifying aesthetic images[EB/OL].[2017-08-25].http://101.110.118.21/cs231n.stanford.edu/reports/2015/pdfs/pveerina_final.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning good taste:classifying aesthetic images">
                                        <b>[12]</b>
                                         VEERINA P.Learning good taste:classifying aesthetic images[EB/OL].[2017-08-25].http://101.110.118.21/cs231n.stanford.edu/reports/2015/pdfs/pveerina_final.pdf.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_13" title=" MAI L, JIN H, LIU F.Composition-preserving deep photo aesthetics assessment[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2016:497-506." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Composition-Preserving Deep Photo Aesthetics Assessment">
                                        <b>[13]</b>
                                         MAI L, JIN H, LIU F.Composition-preserving deep photo aesthetics assessment[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2016:497-506.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_14" title=" KRIZHEVSKY A, SUTSKEVER I, HINTON G E.Imagenet classification with deep convolutional neural networks[C]//Proceedings of the 25th International Conference on Neural Information Processing Systems.[S.l.]:Curran Associates Inc., 2012:1097-1105." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image Net Classification with Deep Convolutional Neural Networks">
                                        <b>[14]</b>
                                         KRIZHEVSKY A, SUTSKEVER I, HINTON G E.Imagenet classification with deep convolutional neural networks[C]//Proceedings of the 25th International Conference on Neural Information Processing Systems.[S.l.]:Curran Associates Inc., 2012:1097-1105.
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_15" title=" Dpchallenge[EB/OL].[2017-08-25].https://www.dpchallenge.com/." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Dpchallenge">
                                        <b>[15]</b>
                                         Dpchallenge[EB/OL].[2017-08-25].https://www.dpchallenge.com/.
                                    </a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_16" title=" JIA Y, SHELHAMER E, DONAHUE J, et al.Caffe:convolutional architecture for fast feature embedding[EB/OL].[2017-08-25].https://arxiv.org/pdf/1408.5093.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Caffe:convolutional architecture for fast feature embedding">
                                        <b>[16]</b>
                                         JIA Y, SHELHAMER E, DONAHUE J, et al.Caffe:convolutional architecture for fast feature embedding[EB/OL].[2017-08-25].https://arxiv.org/pdf/1408.5093.pdf.
                                    </a>
                                </li>
                                <li id="35">


                                    <a id="bibliography_17" title=" LUO Y, TANG X.Photo and video quality evaluation:focusing on the subject[C]//Proceedings of the 10th European Conference on Computer Vision.Berlin, Germany:Springer, 2008:386-399." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Photo and video quality evaluation: Focusing on the subject">
                                        <b>[17]</b>
                                         LUO Y, TANG X.Photo and video quality evaluation:focusing on the subject[C]//Proceedings of the 10th European Conference on Computer Vision.Berlin, Germany:Springer, 2008:386-399.
                                    </a>
                                </li>
                                <li id="37">


                                    <a id="bibliography_18" title=" LO K Y, LIU K H, CHEN C S.Assessment of photo aesthetics with efficiency[C]//Proceedings of the 21st International Conference on Pattern Recognition.Washington D.C., USA:IEEE Press, 2012:2186-2189." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Assessment of photo aesthetics with efficiency">
                                        <b>[18]</b>
                                         LO K Y, LIU K H, CHEN C S.Assessment of photo aesthetics with efficiency[C]//Proceedings of the 21st International Conference on Pattern Recognition.Washington D.C., USA:IEEE Press, 2012:2186-2189.
                                    </a>
                                </li>
                                <li id="39">


                                    <a id="bibliography_19" title=" WANG W, ZHAO M, WANG L, et al.A multi-scene deep learning model for image aesthetic evaluation[J].Signal Processing:Image Communication, 2016, 47:511-518." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A multi-scene deep learning model for image aesthetic evaluation">
                                        <b>[19]</b>
                                         WANG W, ZHAO M, WANG L, et al.A multi-scene deep learning model for image aesthetic evaluation[J].Signal Processing:Image Communication, 2016, 47:511-518.
                                    </a>
                                </li>
                                <li id="41">


                                    <a id="bibliography_20" title=" KONG S, SHEN X, LIN Z, et al.Photo aesthetics ranking network with attributes and content adaptation[C]//Proceedings of European Conference on Computer Vision.Berlin, Germany:Springer, 2016:662-679." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Photo Aesthetics Ranking Network with Attributes and Content Adaptation">
                                        <b>[20]</b>
                                         KONG S, SHEN X, LIN Z, et al.Photo aesthetics ranking network with attributes and content adaptation[C]//Proceedings of European Conference on Computer Vision.Berlin, Germany:Springer, 2016:662-679.
                                    </a>
                                </li>
                                <li id="43">


                                    <a id="bibliography_21" title=" 王伟凝, 王励, 赵明权, 等.基于并行深度卷积神经网络的图像美感分类[J].自动化学报, 2016, 42 (6) :904-914." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201606011&amp;v=MDQwMDBGckNVUkxPZVplUm9GeTdsVXJ2SUtDTGZZYkc0SDlmTXFZOUVaWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEc=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[21]</b>
                                         王伟凝, 王励, 赵明权, 等.基于并行深度卷积神经网络的图像美感分类[J].自动化学报, 2016, 42 (6) :904-914.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJC" target="_blank">计算机工程</a>
                2019,45(03),212-216+224 DOI:10.19678/j.issn.1000-3428.0048890            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于深度学习的图像美观度评价</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%B4%B9%E5%BB%B6%E4%BD%B3&amp;code=39354001&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">费延佳</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%8E%E7%A6%8F%E7%BF%A0&amp;code=08785947&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">李福翠</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%82%B5%E6%9E%AB&amp;code=22467651&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">邵枫</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%AE%81%E6%B3%A2%E5%A4%A7%E5%AD%A6%E4%BF%A1%E6%81%AF%E4%B8%8E%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0160135&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">宁波大学信息与工程学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>基于深度卷积神经网络的特征提取方法比传统手工特征提取方法更加贴近人类大脑的视觉感受。为此, 建立一种两通道组合图像美观度评价模型。使用美学信息通道和场景信息通道的组合来自动提取图像中美学信息和场景类别信息, 通过融合两类信息最终形成美感分类器。在AVA库上进行训练和测试, 结果表明, 与图像局部特征提取方法相比, 该模型结构较简洁, 且具有较高的分类准确率。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%BE%8E%E8%A7%82%E5%BA%A6%E8%AF%84%E4%BB%B7&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">美观度评价;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">卷积神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%9C%BA%E6%99%AF%E8%AF%86%E5%88%AB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">场景识别;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%B8%A4%E9%80%9A%E9%81%93&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">两通道;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%88%86%E7%B1%BB%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">分类网络;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    费延佳 (1992—) , 男, 硕士研究生, 主研方向为图像质量评价;;
                                </span>
                                <span>
                                    李福翠, 讲师;;
                                </span>
                                <span>
                                    邵枫, 教授。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2017-10-10</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金 (61622109);</span>
                    </p>
            </div>
                    <h1><b>Image Aesthetic Assessment Based on Deep Learning</b></h1>
                    <h2>
                    <span>FEI Yanjia</span>
                    <span>LI Fucui</span>
                    <span>SHAO Feng</span>
            </h2>
                    <h2>
                    <span>Department of Information Science and Engineering, Ningbo University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>The method of extracting features based on deep Convolutional Neural Network (CNN) is closer to the visual perception of the human brain than the traditional manual extraction feature method.Therefore, a two-channel combination model is proposed.The combination of the aesthetic information channel and the scene information channel is used to automatically extract the aesthetic information and scene category information in the image, and finally combine the two types of information to form an aesthetic classifier.The training and testing on the AVA library show that compared with the image local feature extraction method, the model structure is simple and has high classification accuracy.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=aesthetic%20assessment&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">aesthetic assessment;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Convolutional%20Neural%20Network%20(CNN)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Convolutional Neural Network (CNN) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=scene%20recognition&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">scene recognition;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=two-channel&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">two-channel;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=classification%20network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">classification network;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2017-10-10</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="45" name="45" class="anchor-tag">0 概述</h3>
                <div class="p1">
                    <p id="46">随着大数据时代的到来, 海量图像信息充斥着整个网络, 而各种社交媒体、网络搜索引擎等平台的图像信息在美观度上存在较大的差异, 同时用户对图像美观度有较高的体验要求, 因此对图像美观度等级的评价越来越重要。</p>
                </div>
                <div class="p1">
                    <p id="47">图像美观度的客观评价主要是根据图像的相关美学特性或摄影规则等进行相关的评价。文献<citation id="109" type="reference">[<a class="sup">1</a>]</citation>模拟人的审美情趣, 根据图像的艺术、绘画等领域的美学规则来提取图像的相关美学特征, 然后通过特征回归或分类获得高低美观度的评价。文献<citation id="110" type="reference">[<a class="sup">2</a>]</citation>提取图像的低层次特征, 如图像的边缘分布、色彩直方图等进行美观度评价, 但这些特征缺乏美学的相关特性。文献<citation id="111" type="reference">[<a class="sup">3</a>]</citation>从摄影的角度出发, 利用黄金三分法等规则去评价图像的美观度。之后, 研究者从图像的分类和检索中受到启发, 利用图像检索和分类技术提取相关特征<citation id="112" type="reference"><link href="9" rel="bibliography" /><link href="11" rel="bibliography" /><link href="13" rel="bibliography" /><sup>[<a class="sup">4</a>,<a class="sup">5</a>,<a class="sup">6</a>]</sup></citation>, 但是由于这种方法提取的特征不具有图像美学的针对性, 不能将美学的特征表现至最佳。</p>
                </div>
                <div class="p1">
                    <p id="48">近年来, 研究者将深度学习引入图像的美观度评价。文献<citation id="113" type="reference">[<a class="sup">7</a>]</citation>将局部和全局的图像输入到网络中, 结合图像的语义或类型信息, 将三通道模型结构作为美学的评价模型。文献<citation id="114" type="reference">[<a class="sup">8</a>]</citation>仿照人眼视觉特性, 分别用网络通道去提取不同特征, 将其整合后输入到更高层的网络进行处理, 得出美学分类。文献<citation id="115" type="reference">[<a class="sup">9</a>]</citation>利用图像检索技术, 在库中检索与待测图像同类型的图像, 然后利用得到的图像集合训练出相应的模型来验证待测图像, 但是整个过程过于繁琐, 工作量较大。文献<citation id="116" type="reference">[<a class="sup">10</a>]</citation>利用图像的相关文本信息, 把图像的评论转换为相关的向量, 并结合图像的视觉信息进行评价, 但是对于图像评论数量的选择和向量的表达形式要进一步研究。</p>
                </div>
                <div class="p1">
                    <p id="49">本文提出一种两通道组合的网络模型结构。通过提取图像的不同信息, 利用特征组合和处理对图像进行美观度分类。在美学通道中, 将剪裁的输入图像及其对应HSV空间中V通道的图像作为一组图像对共同输入到美学单通道网络中, 以提升网络学习的多元化和稳定性, 最终在大型美学数据库AVA上进行训练和测试。</p>
                </div>
                <h3 id="50" name="50" class="anchor-tag">1 美观度评价模型</h3>
                <div class="p1">
                    <p id="51">本文提出两通道网络模型的结构如图1所示。该模型由美学信息通道和场景信息通道组成, 2个通道分别训练4个卷积层, 在第5个卷积层处相结合, 将2种信息相融合且处理后输出256维特征图, 经过3个全连接层后输出一个2维的特征向量, 用于判断图像美观度。美学信息通道用于提取图像的美学相关信息, 对于一幅美观图像而言, 除了图像本身的质量、色彩等因素外, 另一个影响因素就是图像的场景信息。</p>
                </div>
                <div class="area_img" id="52">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201903036_052.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 两通道网络模型结构框图" src="Detail/GetImg?filename=images/JSJC201903036_052.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图1 两通道网络模型结构框图</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201903036_052.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="53">研究表明, 一幅图像中如果含有笑脸, 在打分时往往倾向于认为该图是美观图像, 因此在美观度评价中考虑图像的场景信息十分必要。图2为高美观度图像。为提高模型的分类准确率, 本文在美学通道结构基础上添加了图像场景信息通道来提取图像的相关场景信息。类似于人类大脑的思考机制, 在将浅层次的视觉信息提取后送入更高层次的神经网络进行融合及处理<citation id="117" type="reference"><link href="23" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>, 最终得出判断结果。</p>
                </div>
                <div class="area_img" id="54">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201903036_054.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 高美观度图像" src="Detail/GetImg?filename=images/JSJC201903036_054.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图2 高美观度图像</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201903036_054.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="55">在网络的2个通道中, 一个输入剪裁图像, 另一个输入图像对, 该图像对由剪裁图像和其对应HSV空间中的V通道图像组成。本文将所有输入图像固定为227×227像素, 经过2个平行通道处理后, 将两通道的信息融合并送到更高层的卷基层处理, 最后通过若干全连接层输出分类结果。在模型中利用使用的标签为0和1, 不同标签对应不同类型的特征, 在训练阶段, 网络根据标签的变化, 通过反向传播调整网络。对于训练集中的图像假设有对应的美学标签、图像场景标签<i>y</i><mathml id="56"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>a</mi></msubsup></mrow></math></mathml>、 y<mathml id="57"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>s</mi></msubsup></mrow></math></mathml>, 则对应的训练过程描述为:</p>
                </div>
                <div class="p1">
                    <p id="58" class="code-formula">
                        <mathml id="58"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mrow><mi>max</mi></mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mo stretchy="false"> (</mo></mstyle><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>c</mi><mo>∈</mo><mi mathvariant="bold-italic">C</mi><msub><mrow></mrow><mi>A</mi></msub></mrow></munder><mi>f</mi></mstyle><mo stretchy="false"> (</mo><mi>y</mi><msubsup><mrow></mrow><mi>i</mi><mi>a</mi></msubsup><mo>=</mo><mi>c</mi><mo stretchy="false">) </mo><mtext>l</mtext><mtext>b</mtext><mspace width="0.25em" /><mi>p</mi><mo stretchy="false"> (</mo><mi>y</mi><msubsup><mrow></mrow><mi>i</mi><mi>a</mi></msubsup><mo stretchy="false">|</mo><mi mathvariant="bold-italic">X</mi><msubsup><mrow></mrow><mi>i</mi><mi>a</mi></msubsup><mo>, </mo><mi mathvariant="bold-italic">W</mi><msubsup><mrow></mrow><mi>c</mi><mi>a</mi></msubsup><mo stretchy="false">) </mo><mo>+</mo></mtd></mtr><mtr><mtd><mrow><mtext> </mtext><mtext> </mtext></mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>c</mi><mo>∈</mo><mi mathvariant="bold-italic">C</mi><msub><mrow></mrow><mi>S</mi></msub></mrow></munder><mi>f</mi></mstyle><mo stretchy="false"> (</mo><mi>y</mi><msubsup><mrow></mrow><mi>i</mi><mi>s</mi></msubsup><mo>=</mo><mi>c</mi><mo stretchy="false">) </mo><mtext>l</mtext><mtext>b</mtext><mspace width="0.25em" /><mi>p</mi><mo stretchy="false"> (</mo><mi>y</mi><msubsup><mrow></mrow><mi>i</mi><mi>s</mi></msubsup><mo stretchy="false">|</mo><mi mathvariant="bold-italic">X</mi><msubsup><mrow></mrow><mi>i</mi><mi>s</mi></msubsup><mo>, </mo><mi mathvariant="bold-italic">W</mi><msubsup><mrow></mrow><mi>c</mi><mi>s</mi></msubsup><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="59">其中, N为图像的总数, <b><i>X</i></b><mathml id="60"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>a</mi></msubsup></mrow></math></mathml>和<b><i>X</i></b><mathml id="61"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>s</mi></msubsup></mrow></math></mathml>为第i幅训练图像的美学特征和场景信息特征, <b><i>C</i></b><sub><i>A</i></sub>为美学质量的标签集合, <b><i>C</i></b><sub><i>S</i></sub>为场景类型的标签集合, <b><i>W</i></b><mathml id="62"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>c</mi><mi>a</mi></msubsup></mrow></math></mathml>和<b><i>W</i></b><mathml id="63"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>c</mi><mi>s</mi></msubsup></mrow></math></mathml>分别为美学通道和场景信息通道的模型参数。当x为真时, f (x) =1;当x为假时, f (x) =0。</p>
                </div>
                <div class="p1">
                    <p id="64">概率p (y<mathml id="65"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>a</mi></msubsup></mrow></math></mathml>|<b><i>X</i></b><mathml id="66"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>a</mi></msubsup></mrow></math></mathml>, <b><i>W</i></b><mathml id="67"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>c</mi><mi>a</mi></msubsup></mrow></math></mathml>) 的计算公式为:</p>
                </div>
                <div class="p1">
                    <p id="68" class="code-formula">
                        <mathml id="68"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>p</mi><mo stretchy="false"> (</mo><mi>y</mi><msubsup><mrow></mrow><mi>i</mi><mi>a</mi></msubsup><mo>=</mo><mi>c</mi><mo stretchy="false">|</mo><mi mathvariant="bold-italic">X</mi><msubsup><mrow></mrow><mi>i</mi><mi>a</mi></msubsup><mo>, </mo><mi mathvariant="bold-italic">W</mi><msubsup><mrow></mrow><mi>c</mi><mi>a</mi></msubsup><mo stretchy="false">) </mo><mo>=</mo><mfrac><mrow><mi>exp</mi><mrow><mo> (</mo><mrow><mrow><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">W</mi><msubsup><mrow></mrow><mi>c</mi><mi>a</mi></msubsup><mo stretchy="false">) </mo></mrow><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">X</mi><msubsup><mrow></mrow><mi>i</mi><mi>a</mi></msubsup></mrow><mo>) </mo></mrow></mrow><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>c</mi><mo>∈</mo><mi>C</mi><msub><mrow></mrow><mi>A</mi></msub></mrow></munder><mtext>e</mtext></mstyle><mtext>x</mtext><mtext>p</mtext><mrow><mo> (</mo><mrow><mrow><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">W</mi><msubsup><mrow></mrow><mi>c</mi><mi>a</mi></msubsup><mo stretchy="false">) </mo></mrow><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">X</mi><msubsup><mrow></mrow><mi>i</mi><mi>a</mi></msubsup></mrow><mo>) </mo></mrow></mrow></mfrac><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <h4 class="anchor-tag" id="69" name="69">1.1 美学信息通道构造</h4>
                <div class="p1">
                    <p id="70">文献<citation id="118" type="reference">[<a class="sup">12</a>]</citation>使用在美学数据库上微调的深度学习分类网络模型来进行图像的美观度分类, 取得较好的分类效果。本文参考其构建和训练网络的方法, 搭建并训练相应的网络。</p>
                </div>
                <div class="p1">
                    <p id="71">研究表明, 影响一幅图像美观度的因素有很多, 例如图像的纹理、亮度、构图等。本文选取输入图像和该图像的亮度信息, 即输入图像和该图<i>HSV</i>空间中的<i>V</i>通道图像来构建输入数据的图像对, 以此来丰富数据的类型, 使得网络学习更加多元化并提升其学习过程的稳定性。</p>
                </div>
                <div class="p1">
                    <p id="72">由于卷积神经网络中的全连接层的神经元的个数是固定的, 因此现有的深度网络结构要求输入图像尺寸固定。一般有3种方法将图像尺寸固定, 如图3所示, 具体分析如下:</p>
                </div>
                <div class="p1">
                    <p id="73">1) 将图像的较长边固定为256像素, 短边则按照与长边的比例进行缩放, 其他部分填充为黑色, 如图3 (<i>b</i>) 所示。采用这种方法可保留图像原来的尺寸信息, 但是会引入噪声, 对图像产生较大的影响。</p>
                </div>
                <div class="p1">
                    <p id="74">2) 将图像的长短边都缩放到256像素, 这样保全了图像的整体信息, 但当图像的长宽尺寸差距过大时, 会导致图像产生变形卷曲, 从而改变图像的相关信息, 如图3 (<i>c</i>) 所示。</p>
                </div>
                <div class="p1">
                    <p id="75">3) 采用随机剪裁可以获得图像的局部信息, 当剪裁的次数较多时, 得到的裁剪图像集合可包含整幅图像的信息, 从而得到图像的全局特性, 如图3 (<i>d</i>) 和图3 (<i>e</i>) 所示。</p>
                </div>
                <div class="area_img" id="76">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201903036_076.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 图像处理" src="Detail/GetImg?filename=images/JSJC201903036_076.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图3 图像处理</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201903036_076.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="77">研究者多数采用第2种方法来处理输入图像。文献<citation id="119" type="reference">[<a class="sup">13</a>]</citation>将整幅未变形图像输入到网络中, 其分类准确率仅比文献<citation id="120" type="reference">[<a class="sup">7</a>]</citation>高1%左右, 通过设计良好的模型结构在大数据量的训练集训练下在一定程度上弥补直接缩放带来的美感损失, 且测试集图像采取与训练集图像相同的缩放处理, 保证了测试时实验变量的控制, 因此对图像进行直接缩放处理并不会影响本文的实验结果。 为更加全面地分析图像的全局特性, 本文采用第2种方法将图像进行直接缩放, 然后结合第3种方法对直接缩放后的图像进行多次随机剪裁以增加训练样本的数量, 以保证训练样本的多样性。</p>
                </div>
                <div class="p1">
                    <p id="78">在图1中, 本文将一幅256×256像素的图像, 随机剪裁成多幅227×227像素的图像, 由于剪裁后图像尺寸与剪裁前并无较大差异, 因此剪裁后并不影响原256×256像素图像的美观度。然后计算每个剪裁图像对应的V通道图像, 最后与该剪裁图像组成图像对共同作为网络的输入图像:第1层卷积层输出96维, 卷积核大小为7×7;第2层卷积层输出256维, 卷积核大小为5×5;第3层卷积层输出432维, 卷积核大小为3×3;第4层输出432维, 卷积核大小为3×3;第5层卷积层输出为256维, 卷积核大小为3×3;前3层相邻卷积层之间及第5层卷积层后均有池化层, 各层间加入相应的归一化层和激活层, 则该单通道网络的训练过程可描述为求<i>I</i> (<i>w</i>) 的最大值, 计算公式为:</p>
                </div>
                <div class="p1">
                    <p id="79" class="code-formula">
                        <mathml id="79"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ι</mi><mo stretchy="false"> (</mo><mi>w</mi><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>c</mi><mo>∈</mo><mi mathvariant="bold-italic">C</mi><msub><mrow></mrow><mi>A</mi></msub></mrow></munder><mi>f</mi></mstyle></mrow></mstyle><mo stretchy="false"> (</mo><mi>y</mi><msubsup><mrow></mrow><mi>i</mi><mi>a</mi></msubsup><mo>=</mo><mi>c</mi><mo stretchy="false">) </mo><mtext>l</mtext><mtext>b</mtext><mspace width="0.25em" /><mi>p</mi><mo stretchy="false"> (</mo><mi>y</mi><msubsup><mrow></mrow><mi>i</mi><mi>a</mi></msubsup><mo>=</mo><mi>c</mi><mo stretchy="false">|</mo><mi mathvariant="bold-italic">X</mi><msubsup><mrow></mrow><mi>i</mi><mi>a</mi></msubsup><mo>, </mo><mi mathvariant="bold-italic">W</mi><msubsup><mrow></mrow><mi>c</mi><mi>a</mi></msubsup><mo stretchy="false">) </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="80">在训练过程中, 为解决深度学习中常出现的模型不收敛问题, 本文借助已收敛的模型参数作为美学信息通道模型的初始化参数, 利用AVA数据库进行训练。在深度学习中该技术过程被称为微调, 即该通道使用文献<citation id="121" type="reference">[<a class="sup">14</a>]</citation>提出的网络结构在大型数据库ImageNet上训练得到收敛模型参数, 并将其前4个卷积层中相对应的参数赋给本文单通道网络作为初始化参数, 其余部分使用高斯随机数进行参数的初始化, 用得到的初始化模型在整个训练集上对模型进行训练。经过反复调参, 得出最佳的网络参数设置, 最终将基础学习率设为0.000 5。</p>
                </div>
                <h4 class="anchor-tag" id="81" name="81">1.2 场景信息通道设计</h4>
                <div class="p1">
                    <p id="82">图像的场景信息对于美观度的评价至关重要, 仅使用美学特征来训练深度模型, 会导致模型的稳定性不高。因此, 为提高模型预测的准确率和稳定性, 本文在美学信息通道基础上加入场景信息通道。受到多标签任务启发, 加入场景信息通道后, 可认为一幅图像有2个标签, 即美学标签和场景信息标签。</p>
                </div>
                <div class="p1">
                    <p id="83">通过对AVA数据库进行分析, 其图像场景可以大致分为动物、建筑、人类、景观、夜景、植物和静物7类。因此, 本文按照此7类的场景将图像进行场景分类, 即该通道独立训练后可以判断图像的场景类别。</p>
                </div>
                <div class="p1">
                    <p id="84">由于AVA库中同时拥有2个标签的图像数量较少, 难以实现多标签任务的训练, 因此本文利用收敛模型参数作为本模型的初始化参数, 在此基础上通过微调来解决这一问题。其过程描述为:利用第1.1节方法得到初始化后的模型, 然后将整个模型在7类场景数据库上进行训练。本文使用的场景数据库中所有图像均从搜索引擎上收集所得, 每一类约1 500幅图像, 7类共约10 500幅图像。该单通道网络结构与第1.1节中结构设置相同, 同时为减少计算复杂度以及减少冗余参数, 本文在该单通道中减少第3个和第4个卷积层的输出特征维数。在训练时, 该通道的学习率设置与第1.1节相同。</p>
                </div>
                <h4 class="anchor-tag" id="85" name="85">1.3 两通道设计</h4>
                <div class="p1">
                    <p id="86">两通道模型结构是在美学信息通道的结构基础上进行的扩展, 扩展的另一个通道为场景信息通道。在训练两通道模型时, 为将两通道融合, 使其有较好的分类效果, 本文将分别训练好的2个单通道模型结合后在训练集上进行微调。从图1可以看出, 两通道网络在获得图像相关信息后在第5个卷积层处融合。本文仅对第5个卷基层以及全连接层在整个数据库上进行微调, 而其他参数皆固定不变。整个网络的基础学习率设为0.000 5, 将拼接层以及第一个全连接层的学习率设为0.005。此时训练过程可描述为:</p>
                </div>
                <div class="p1">
                    <p id="87" class="code-formula">
                        <mathml id="87"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>max</mi></mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>c</mi><mo>∈</mo><mi>C</mi><msub><mrow></mrow><mi>A</mi></msub></mrow></munder><mi>f</mi></mstyle></mrow></mstyle><mo stretchy="false"> (</mo><mi>y</mi><msubsup><mrow></mrow><mi>i</mi><mi>a</mi></msubsup><mo>=</mo><mi>c</mi><mo stretchy="false">) </mo><mtext>l</mtext><mtext>b</mtext><mspace width="0.25em" /><mi>p</mi><mo stretchy="false"> (</mo><mi>y</mi><msubsup><mrow></mrow><mi>i</mi><mi>a</mi></msubsup><mo stretchy="false">|</mo><mi mathvariant="bold-italic">X</mi><msubsup><mrow></mrow><mi>i</mi><mi>a</mi></msubsup><mo>, </mo><mi mathvariant="bold-italic">X</mi><msubsup><mrow></mrow><mi>i</mi><mi>s</mi></msubsup><mo>, </mo><mi mathvariant="bold-italic">W</mi><msubsup><mrow></mrow><mi>c</mi><mi>a</mi></msubsup><mo>, </mo><mi mathvariant="bold-italic">W</mi><msubsup><mrow></mrow><mi>c</mi><mi>s</mi></msubsup><mo stretchy="false">) </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="88">概率公式描述为:</p>
                </div>
                <div class="p1">
                    <p id="89" class="code-formula">
                        <mathml id="89"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>p</mi><mo stretchy="false"> (</mo><mi>y</mi><msubsup><mrow></mrow><mi>i</mi><mi>a</mi></msubsup><mo>=</mo><mi>c</mi><mo stretchy="false">|</mo><mi mathvariant="bold-italic">X</mi><msubsup><mrow></mrow><mi>i</mi><mi>a</mi></msubsup><mo>, </mo><mi mathvariant="bold-italic">X</mi><msubsup><mrow></mrow><mi>i</mi><mi>s</mi></msubsup><mo>, </mo><mi mathvariant="bold-italic">W</mi><msubsup><mrow></mrow><mi>c</mi><mi>a</mi></msubsup><mo>, </mo><mi mathvariant="bold-italic">W</mi><msubsup><mrow></mrow><mi>c</mi><mi>s</mi></msubsup><mo stretchy="false">) </mo><mo>=</mo></mtd></mtr><mtr><mtd><mrow><mtext> </mtext><mtext> </mtext></mrow><mfrac><mrow><mi>exp</mi><mrow><mo> (</mo><mrow><mrow><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">W</mi><msubsup><mrow></mrow><mi>c</mi><mi>a</mi></msubsup><mo stretchy="false">) </mo></mrow><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">X</mi><msubsup><mrow></mrow><mi>i</mi><mi>a</mi></msubsup><mrow><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">W</mi><msubsup><mrow></mrow><mi>c</mi><mi>s</mi></msubsup><mo stretchy="false">) </mo></mrow><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">X</mi><msubsup><mrow></mrow><mi>i</mi><mi>s</mi></msubsup></mrow><mo>) </mo></mrow></mrow><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>c</mi><mo>∈</mo><mi>C</mi><msub><mrow></mrow><mi>A</mi></msub></mrow></munder><mtext>e</mtext></mstyle><mtext>x</mtext><mtext>p</mtext><mrow><mo> (</mo><mrow><mrow><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">W</mi><msubsup><mrow></mrow><mi>c</mi><mi>a</mi></msubsup><mo stretchy="false">) </mo></mrow><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">X</mi><msubsup><mrow></mrow><mi>i</mi><mi>a</mi></msubsup><mrow><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">W</mi><msubsup><mrow></mrow><mi>c</mi><mi>s</mi></msubsup><mo stretchy="false">) </mo></mrow><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">X</mi><msubsup><mrow></mrow><mi>i</mi><mi>s</mi></msubsup></mrow><mo>) </mo></mrow></mrow></mfrac><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>5</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="90">在式 (4) 最大化的过程中并没有涉及场景的标签, 即在加入场景信息通道后的整个模型结构在训练过程中, 只用美学的标签去监督整个网络的反馈传播。</p>
                </div>
                <h3 id="91" name="91" class="anchor-tag">2 实验结果与分析</h3>
                <div class="p1">
                    <p id="92">本文采用CPU训练, CPU型号为core i5-6500, 内存为8 GB, 数据库使用AVA, 所有图像来源于dpchallenge<citation id="122" type="reference"><link href="31" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>, 在AVA1库和AVA2库上进行实验, 平台使用Caffe<citation id="123" type="reference"><link href="33" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>。</p>
                </div>
                <h4 class="anchor-tag" id="93" name="93">2.1 AVA数据库</h4>
                <div class="p1">
                    <p id="94">每幅图像均由210个用户对图像进行打分, 分数范围为从1～10, 打分者不分年龄大小、性别和专业, 以保证评分的客观性, 每幅图像的最终评分是所有打分的平均分数。本文以5为分界点, 高于5的为高美观度图像, 低于5的认为是低美观度图像。AVA数据库共有255 529幅图像, 其中高美观度图像为180 856幅, 低美观度图像为74 673幅。图4是部分AVA库中的图像。</p>
                </div>
                <div class="area_img" id="95">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201903036_095.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 部分AVA图像库" src="Detail/GetImg?filename=images/JSJC201903036_095.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图4 部分AVA图像库</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201903036_095.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="96">AVA1库和AVA2库图像具体设置如下:</p>
                </div>
                <div class="p1">
                    <p id="97">1) AVA1:按照文献<citation id="124" type="reference">[<a class="sup">1</a>]</citation>的做法, 挑选图像数据库中前 10% 高分的图像作为高美观度图像, 并挑选前 10% 低分的图像作为低美观度图像, 高美观度图像、低美观度图像均为25 532幅, 然后随机选取高低美观度图像中的一半作为训练集, 剩余的作为测试集。</p>
                </div>
                <div class="p1">
                    <p id="98">2) AVA2:按照 AVA 数据库<citation id="125" type="reference"><link href="27" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>中的训练集和测试集的划分, 将训练集和测试集中的图像划分为高美观度和低美观度2个类别, 将美学评分5分以上的归类为高美观度图像, 美学评分5 分以下的归类为低美观度图像, 得到180 856幅高美观度图像和74 673幅低美观度图像, 其中, 训练集包含166 689幅高美观度图像和68 910幅低美观度图像, 测试集包含14 167幅高美观度图像和5 763幅低美观度图像。</p>
                </div>
                <h4 class="anchor-tag" id="99" name="99">2.2 单通道分类</h4>
                <div class="p1">
                    <p id="100">为了更加系统地测试本文所提出的单通道网络结构对于美观度图像的分类能力, 此处对美学信息通道在AVA2库上单独进行实验, 结果如表1所示。从表1可以看出, 在单独训练和测试美学通道时, 美学信息的单通道分类准确率可达77.20%, 与文献<citation id="126" type="reference">[<a class="sup">7</a>]</citation>方法相比, 分类准确率较高, 同时单通道网络的学习能力和分类能力较强, 因此本文模型结构冗余度较小。</p>
                </div>
                <div class="area_img" id="101">
                    <p class="img_tit"><b>表1 美学通道在AVA2库上的准确率对比结果</b> % <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="101" border="1"><tr><td><br />方法</td><td>准确率</td></tr><tr><td><br />文献[7]方法</td><td>75.42</td></tr><tr><td><br />本文方法</td><td>77.20</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="102" name="102">2.3 两通道分类实验</h4>
                <div class="p1">
                    <p id="103">在两通道融合得到最终的网络模型后, 为验证其对美观度图像的分类性能, 将其与现有方法进行对比, 在AVA1库、AVA2库上的对比结果如表2、表3所示。</p>
                </div>
                <div class="area_img" id="104">
                    <p class="img_tit"><b>表2 各方法在AVA1库上的实验对比结果</b> % <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="104" border="1"><tr><td><br />方法</td><td>准确率</td></tr><tr><td><br />文献[2]方法</td><td>78.92</td></tr><tr><td><br />文献[3]方法</td><td>71.06</td></tr><tr><td><br />文献[17]方法</td><td>61.49</td></tr><tr><td><br />文献[18]方法</td><td>68.13</td></tr><tr><td><br />本文方法</td><td>83.01</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="105">
                    <p class="img_tit"><b>表3 各方法在AVA2库上的实验对比结果</b> % <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="105" border="1"><tr><td><br />方法</td><td>准确率</td></tr><tr><td><br />文献[7]方法</td><td>75.42</td></tr><tr><td><br />文献[12]方法</td><td>77.09</td></tr><tr><td><br />文献[14]方法</td><td>67.00</td></tr><tr><td><br />文献[19]方法</td><td>76.94</td></tr><tr><td><br />文献[20]方法</td><td>76.80</td></tr><tr><td><br />文献[21]方法</td><td>77.03</td></tr><tr><td><br />本文方法</td><td>79.84</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="106">从表2、表3可以看出, 本文方法在AVA1 库及AVA2库上均取得较好的分类准确率, 与文献<citation id="127" type="reference">[<a class="sup">7</a>]</citation>方法相比有较大的优势;与文献<citation id="129" type="reference">[<a class="sup">3</a>,<a class="sup">5</a>]</citation>方法相比, 在AVA1库上取得超过4%的准确率;与文献<citation id="128" type="reference">[<a class="sup">12</a>]</citation>方法相比, 在AVA2库上至少提高2%。综上所述, 相较于图像局部特征提取以及传统手工提取特征的方法, 深度学习的方法在美观度等级分类准确率上有着明显的优势。而相比于文献<citation id="130" type="reference">[<a class="sup">7</a>,<a class="sup">8</a>,<a class="sup">9</a>]</citation>等利用深度学习网络解决该问题的模型, 本文方法不仅结构较为简单, 而且提高了美观度等级分类的准确率。</p>
                </div>
                <h3 id="107" name="107" class="anchor-tag">3 结束语</h3>
                <div class="p1">
                    <p id="108">深度学习网络解决了传统手工提取特征和局部特征难以对图像美学特征量化建模的问题, 可直接从图像中学习得到相关特征。本文利用深度学习网络的特征提取和学习能力, 综合考虑影响图像美观度特征信息和场景分类信息, 设计两通道深度卷积神经网络。通过大规模图像分类数据集训练得到收敛模型参数, 训练图像信息通道, 最后将特征有效组合, 形成美感分类器。在AVA数据库上的实验结果表明, 本文模型对于图像美观度等级分类有较高的准确率。下一步将研究图像文本信息对网络分类能力的影响, 以提高模型分类准确率。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES445AB3E6FB1CF3EA84061CF5FD68FD06&amp;v=MjQyNDVPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkNwYlEzNU41aHdyaSt4S2c9TmlmT2ZiZThHNkMrclBwREVwa09md282dW1jYjdqOTdTUXlVcVdSQmY3cmlNYnFaQw==&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> DONG Z, TIAN X M.Multi-level photo quality assessment with multi-view features[J].Neurocomputing, 2015, 168:308-319.
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Classification of digital photos taken by photographers or home users">

                                <b>[2]</b> TONG H, LI M, ZHANG H J, et al.Classification of digital photos taken by photographers or home users[C]//Proceedings of Pacific-Rim Conference on Multimedia.Berlin, Germany:Springer, 2004:198-205.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Studying aesthetics in photographic images using acomputational approach">

                                <b>[3]</b> DATTA R, JOSHI D, LI J, et al.Studying aesthetics in photographic images using a computational approach[C]//Proceedings of the 9th European Conference on Computer Vision.Berlin, Germany:Springer, 2006:288-301.
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002830901&amp;v=MzIxMDhSN3FlYnVkdEZDN2xWYi9KSkY0PU5qN0Jhck80SHRIT3A0eEZiZXNPWTNrNXpCZGg0ajk5U1hxUnJ4b3hjTUg3&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b> LOWE D G.Distinctive image features from scale-invariant keypoints[J].International Journal of Computer Vision, 2004, 60 (2) :91-110.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Assessing the aesthetic quality of photographs using generic image descriptors">

                                <b>[5]</b> MARCHESOTTI L, PERRONNIN F, LARLUS D, et al.Assessing the aesthetic quality of photographs using generic image descriptors[C]//Proceedings of International Conference on Computer Vision.Washington D.C., USA:IEEE Computer Society, 2011:1784-1791.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=1014001227.nh&amp;v=MjMxODg2ajU0TzN6cXFCdEdGckNVUkxPZVplUm9GeTdsVXJ2SVZGMjZHck80SDlQT3FKRWJQSVFLREg4NHZSNFQ=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b> 高寒.基于图像分类的图像美学评价研究[D].哈尔滨:哈尔滨工业大学, 2013.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Rating image aesthetics using deep learning">

                                <b>[7]</b> LU X, LIN Z, JIN H, et al.Rating image aesthetics using deep learning[J].IEEE Transactions on Multimedia, 2015, 17 (11) :2021-2034.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Brain-inspired deep networks for image aesthetics assessment">

                                <b>[8]</b> WANG Z, CHANG S, DOLCOS F, et al.Brain-inspired deep networks for image aesthetics assessment[EB/OL].[2017-08-25].https://arxiv.org/pdf/1601.04155.pdf.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Query-Dependent Aesthetic Model With Deep Learning for Photo Quality Assessment">

                                <b>[9]</b> TIAN X, DONG Z, YANG K, et al.Query-dependent aesthetic model with deep learning for photo quality assessment[J].IEEE Transactions on Multimedia, 2015, 17 (11) :2035-2048.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Joint image and text representation for aesthetics analysis">

                                <b>[10]</b> ZHOU Y, LU X, ZHANG J, et al.Joint image and text representation for aesthetics analysis[C]//Proceedings of the 24th ACM International Conference on Multimedia.New York, USA:ACM Press, 2016:262-266.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_11" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012300023745&amp;v=MTYyODdpclJkR2VycVFUTW53WmVadUh5am1VTG5JSmw0VmJoST1OaWZPZmJLN0h0RE9ySTlGWk9rTUMzZzhvQk1UNlQ0UFFILw==&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[11]</b> POWER J D, COHEN A L, NELSON S M, et al.Functional network organization of the human brain[J].Neuron, 2011, 72 (4) :665-678.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning good taste:classifying aesthetic images">

                                <b>[12]</b> VEERINA P.Learning good taste:classifying aesthetic images[EB/OL].[2017-08-25].http://101.110.118.21/cs231n.stanford.edu/reports/2015/pdfs/pveerina_final.pdf.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Composition-Preserving Deep Photo Aesthetics Assessment">

                                <b>[13]</b> MAI L, JIN H, LIU F.Composition-preserving deep photo aesthetics assessment[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2016:497-506.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image Net Classification with Deep Convolutional Neural Networks">

                                <b>[14]</b> KRIZHEVSKY A, SUTSKEVER I, HINTON G E.Imagenet classification with deep convolutional neural networks[C]//Proceedings of the 25th International Conference on Neural Information Processing Systems.[S.l.]:Curran Associates Inc., 2012:1097-1105.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Dpchallenge">

                                <b>[15]</b> Dpchallenge[EB/OL].[2017-08-25].https://www.dpchallenge.com/.
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Caffe:convolutional architecture for fast feature embedding">

                                <b>[16]</b> JIA Y, SHELHAMER E, DONAHUE J, et al.Caffe:convolutional architecture for fast feature embedding[EB/OL].[2017-08-25].https://arxiv.org/pdf/1408.5093.pdf.
                            </a>
                        </p>
                        <p id="35">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Photo and video quality evaluation: Focusing on the subject">

                                <b>[17]</b> LUO Y, TANG X.Photo and video quality evaluation:focusing on the subject[C]//Proceedings of the 10th European Conference on Computer Vision.Berlin, Germany:Springer, 2008:386-399.
                            </a>
                        </p>
                        <p id="37">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Assessment of photo aesthetics with efficiency">

                                <b>[18]</b> LO K Y, LIU K H, CHEN C S.Assessment of photo aesthetics with efficiency[C]//Proceedings of the 21st International Conference on Pattern Recognition.Washington D.C., USA:IEEE Press, 2012:2186-2189.
                            </a>
                        </p>
                        <p id="39">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A multi-scene deep learning model for image aesthetic evaluation">

                                <b>[19]</b> WANG W, ZHAO M, WANG L, et al.A multi-scene deep learning model for image aesthetic evaluation[J].Signal Processing:Image Communication, 2016, 47:511-518.
                            </a>
                        </p>
                        <p id="41">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Photo Aesthetics Ranking Network with Attributes and Content Adaptation">

                                <b>[20]</b> KONG S, SHEN X, LIN Z, et al.Photo aesthetics ranking network with attributes and content adaptation[C]//Proceedings of European Conference on Computer Vision.Berlin, Germany:Springer, 2016:662-679.
                            </a>
                        </p>
                        <p id="43">
                            <a id="bibliography_21" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201606011&amp;v=MjQ5OTlHRnJDVVJMT2VaZVJvRnk3bFVydklLQ0xmWWJHNEg5Zk1xWTlFWllRS0RIODR2UjRUNmo1NE8zenFxQnQ=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[21]</b> 王伟凝, 王励, 赵明权, 等.基于并行深度卷积神经网络的图像美感分类[J].自动化学报, 2016, 42 (6) :904-914.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJC201903036" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201903036&amp;v=MDc5NTJPM3pxcUJ0R0ZyQ1VSTE9lWmVSb0Z5N2xVcnZJTHo3QmJiRzRIOWpNckk5R1lvUUtESDg0dlI0VDZqNTQ=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
