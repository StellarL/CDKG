<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637130386611238750%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJC201906039%26RESULT%3d1%26SIGN%3dLIZ58jzofhxA3lD8IynZekN5hzQ%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201906039&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201906039&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201906039&amp;v=MTAwMjRiYkc0SDlqTXFZOUdiWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm9GeW5sVmJySUx6N0I=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#51" data-title="0 概述 ">0 概述</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#56" data-title="1 相关工作 ">1 相关工作</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#57" data-title="1.1 极限学习机">1.1 极限学习机</a></li>
                                                <li><a href="#74" data-title="1.2 流形学习">1.2 流形学习</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#80" data-title="2 算法介绍 ">2 算法介绍</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#87" data-title="2.1 流形正则化思想">2.1 流形正则化思想</a></li>
                                                <li><a href="#106" data-title="2.2 基于流形正则化极限学习机的文本分类模型">2.2 基于流形正则化极限学习机的文本分类模型</a></li>
                                                <li><a href="#122" data-title="2.3 MRELMT算法">2.3 MRELMT算法</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#138" data-title="3 实验结果与分析 ">3 实验结果与分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#139" data-title="3.1 实验数据集及评价标准">3.1 实验数据集及评价标准</a></li>
                                                <li><a href="#145" data-title="3.2 实验环境及参数">3.2 实验环境及参数</a></li>
                                                <li><a href="#151" data-title="3.3 结果分析">3.3 结果分析</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#162" data-title="4 结束语 ">4 结束语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#86" data-title="&lt;b&gt;图1 基于流形正则化极限学习机的文本分类模型&lt;/b&gt;"><b>图1 基于流形正则化极限学习机的文本分类模型</b></a></li>
                                                <li><a href="#112" data-title="&lt;b&gt;图2 流形正则化极限学习机示意图&lt;/b&gt;"><b>图2 流形正则化极限学习机示意图</b></a></li>
                                                <li><a href="#141" data-title="&lt;b&gt;表1 2个数据集的基本参数&lt;/b&gt;"><b>表1 2个数据集的基本参数</b></a></li>
                                                <li><a href="#148" data-title="&lt;b&gt;图3 MRELMT算法在2个数据集上的&lt;i&gt;F&lt;/i&gt;1-&lt;i&gt;measure&lt;/i&gt;值&lt;/b&gt;"><b>图3 MRELMT算法在2个数据集上的<i>F</i>1-<i>measure</i>值</b></a></li>
                                                <li><a href="#150" data-title="&lt;b&gt;表2 MRELMT算法的训练参数&lt;/b&gt;"><b>表2 MRELMT算法的训练参数</b></a></li>
                                                <li><a href="#155" data-title="&lt;b&gt;表3 2种算法在不同数据集上的分类性能&lt;/b&gt;"><b>表3 2种算法在不同数据集上的分类性能</b></a></li>
                                                <li><a href="#158" data-title="&lt;b&gt;图4 5种算法在Reuters数据集上的分类性能&lt;/b&gt;"><b>图4 5种算法在Reuters数据集上的分类性能</b></a></li>
                                                <li><a href="#160" data-title="&lt;b&gt;图5 5种算法在20newsgroup数据集上的分类性能&lt;/b&gt;"><b>图5 5种算法在20newsgroup数据集上的分类性能</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="5">


                                    <a id="bibliography_1" title=" JIANG Liangxiao, WANG Dianhong, CAI Zhihua.Discriminatively weighted naive bayes and its application in text classification[J].International Journal on Artificial Intelligence Tools, 2012, 21 (1) :1-19." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=DISCRIMINATIVELY WEIGHTED NAIVE BAYES AND ITS APPLICATION IN TEXT CLASSIFICATION">
                                        <b>[1]</b>
                                         JIANG Liangxiao, WANG Dianhong, CAI Zhihua.Discriminatively weighted naive bayes and its application in text classification[J].International Journal on Artificial Intelligence Tools, 2012, 21 (1) :1-19.
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_2" title=" KENEKAYORO P, BUCKLEY K, THELWALL M.Automatic classification of academic Web page types[J].Scientometrics, 2014, 101 (2) :1015-1026." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD14101500001203&amp;v=MDU0NDI3QmFySzhIOUhOcW85RlpPc09Ebnc2b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VTG5JSVY0U2J4ST1Oag==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         KENEKAYORO P, BUCKLEY K, THELWALL M.Automatic classification of academic Web page types[J].Scientometrics, 2014, 101 (2) :1015-1026.
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_3" title=" LILLEBERG J, ZHU Yun, ZHANG Yanqing.Support vector machines and word2vec for text classification with semantic features[C]//Proceedings of the 14th International Conference on Cognitive Informatics and Cognitive Computing.Washington D.C., USA:IEEE Press, 2015:136-140." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Support Vector Machines and Word2vec for Text Classification with Semantic Features">
                                        <b>[3]</b>
                                         LILLEBERG J, ZHU Yun, ZHANG Yanqing.Support vector machines and word2vec for text classification with semantic features[C]//Proceedings of the 14th International Conference on Cognitive Informatics and Cognitive Computing.Washington D.C., USA:IEEE Press, 2015:136-140.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_4" title=" 杨帅华, 张清华.粗糙集近似集的KNN文本分类算法研究[J].小型微型计算机系统, 2017, 38 (10) :2192-2196." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XXWX201710004&amp;v=MDE4NjJYY2RyRzRIOWJOcjQ5RllJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSb0Z5bmxWYnJJUFQ=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                         杨帅华, 张清华.粗糙集近似集的KNN文本分类算法研究[J].小型微型计算机系统, 2017, 38 (10) :2192-2196.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_5" title=" SRIVASTAVA N, SALAKHUTDINOV R R, HINTON G E.Modeling documents with deep boltzmann machines[EB/OL].[2018-02-25].https://arxiv.org/ftp/arxiv/papers/13 09/1309.6865.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Modeling documents with deep boltzmann machines">
                                        <b>[5]</b>
                                         SRIVASTAVA N, SALAKHUTDINOV R R, HINTON G E.Modeling documents with deep boltzmann machines[EB/OL].[2018-02-25].https://arxiv.org/ftp/arxiv/papers/13 09/1309.6865.pdf.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_6" title=" LAI Siwei, XU Liheng, LIU Kang, et al.Recurrent convolutional neural networks for text classification[C]//Proceedings of the 29th AAAI Conference on Artificial Intelligence.Palo Alto, USA:Association for the Advance of Artificial Intelligence, 2015:2267-2273." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Recurrent Convolutional Neural Networks for Text Classification">
                                        <b>[6]</b>
                                         LAI Siwei, XU Liheng, LIU Kang, et al.Recurrent convolutional neural networks for text classification[C]//Proceedings of the 29th AAAI Conference on Artificial Intelligence.Palo Alto, USA:Association for the Advance of Artificial Intelligence, 2015:2267-2273.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_7" title=" ZHOU Peng, QI Zhenyu, ZHENG Suncong, et al.Text classification improved by integrating bidirectional LSTM with two-dimensional max pooling[EB/OL].[2018-02-25].https://arxiv.org/pdf/1611.06639.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Text classification improved by integrating bidirectional LSTM with two-dimensional max pooling">
                                        <b>[7]</b>
                                         ZHOU Peng, QI Zhenyu, ZHENG Suncong, et al.Text classification improved by integrating bidirectional LSTM with two-dimensional max pooling[EB/OL].[2018-02-25].https://arxiv.org/pdf/1611.06639.pdf.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_8" title=" CONNEAU A, SCHWENK H, BARRAULT L, et al.Very deep convolutional networks for text classification[EB/OL].[2018-02-25].https://arxiv.org/pdf/160 6.01781.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Very deep convolutional networks for text classification">
                                        <b>[8]</b>
                                         CONNEAU A, SCHWENK H, BARRAULT L, et al.Very deep convolutional networks for text classification[EB/OL].[2018-02-25].https://arxiv.org/pdf/160 6.01781.pdf.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_9" title=" HUANG Guangbin, ZHU Qinyu, SIEW C K.Extreme learning machine:theory and applications[J].Neurocomputing, 2006, 70 (1/2/3) :489-501." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501913101&amp;v=MDE0NTRNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VTG5JSVY0U2J4ST1OaWZPZmJLN0h0RE5xbzlFYmVvTURYdzRvQg==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                         HUANG Guangbin, ZHU Qinyu, SIEW C K.Extreme learning machine:theory and applications[J].Neurocomputing, 2006, 70 (1/2/3) :489-501.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_10" title=" GURPINAR F, KAYA H, DIBEKLIOGLU H, et al.Kernel ELM and CNN based facial age estimation[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition Workshops.Washington D.C., USA:IEEE Press, 2016:80-86." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Kernel ELM and CNN based facial age estimation">
                                        <b>[10]</b>
                                         GURPINAR F, KAYA H, DIBEKLIOGLU H, et al.Kernel ELM and CNN based facial age estimation[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition Workshops.Washington D.C., USA:IEEE Press, 2016:80-86.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_11" title=" BAI Peng, LIU Huaping, SUN Fuchun, et al.Robotic grasp stability analysis using extreme learning machine[C]//Proceedings of ELM’16.Berlin, Germany:Springer, 2016:37-51." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Robotic grasp stability analysis using extreme learning machine">
                                        <b>[11]</b>
                                         BAI Peng, LIU Huaping, SUN Fuchun, et al.Robotic grasp stability analysis using extreme learning machine[C]//Proceedings of ELM’16.Berlin, Germany:Springer, 2016:37-51.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_12" title=" YANG Chenguang, HUANG Kunxia, CHENG Hong, et al.Haptic identification by ELM-controlled uncertain manipulator[J].IEEE Transactions on Systems, Man, and Cybernetics:Systems, 2017, 47 (8) :2398-2409." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Haptic identication by ELM-controlled uncertain manipulator">
                                        <b>[12]</b>
                                         YANG Chenguang, HUANG Kunxia, CHENG Hong, et al.Haptic identification by ELM-controlled uncertain manipulator[J].IEEE Transactions on Systems, Man, and Cybernetics:Systems, 2017, 47 (8) :2398-2409.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_13" title=" ZHENG Wenbin, QIAN Yuntao, LU Huijuan.Text categorization based on regularization extreme learning machine[J].Neural Computing and Applications, 2013, 22 (3/4) :447-456." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD130409074765&amp;v=MTI0ODc3SHRYTXBvOUNZT3dKQ1JNOHp4VVNtRGQ5U0g3bjN4RTlmYnZuS3JpZlp1OXVGQ3ZqVTc3SUpsc1VOajdCYXJL&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[13]</b>
                                         ZHENG Wenbin, QIAN Yuntao, LU Huijuan.Text categorization based on regularization extreme learning machine[J].Neural Computing and Applications, 2013, 22 (3/4) :447-456.
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_14" title=" ROUL R K, NANDA A, PATEL V, et al.Extreme learning machines in the field of text classification[C]//Proceedings of the 16th IEEE/ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing.Washington D.C., USA:IEEE Press, 2015:1-7." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Extreme learning machines in the field of text classification">
                                        <b>[14]</b>
                                         ROUL R K, NANDA A, PATEL V, et al.Extreme learning machines in the field of text classification[C]//Proceedings of the 16th IEEE/ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing.Washington D.C., USA:IEEE Press, 2015:1-7.
                                    </a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_15" title=" FENG Xiaoyue, LIANG Yanchun, SHI Xiaohu, et al.Overfitting reduction of text classification based on AdaBELM[J].Entropy, 2017, 19 (7) :1-13." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Overfitting reduction of text classification based on AdaBELM">
                                        <b>[15]</b>
                                         FENG Xiaoyue, LIANG Yanchun, SHI Xiaohu, et al.Overfitting reduction of text classification based on AdaBELM[J].Entropy, 2017, 19 (7) :1-13.
                                    </a>
                                </li>
                                <li id="35">


                                    <a id="bibliography_16" title=" SEUNG H S, LEE D D.The manifold ways of perception[J].Science, 2000, 290 (5500) :2268-2269." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The manifold ways of perception">
                                        <b>[16]</b>
                                         SEUNG H S, LEE D D.The manifold ways of perception[J].Science, 2000, 290 (5500) :2268-2269.
                                    </a>
                                </li>
                                <li id="37">


                                    <a id="bibliography_17" title=" TENENBAUM J B, DE SILVA V, LANGFORD J C.A global geometric framework for nonlinear dimensionality reduction[J].Science, 2000, 290 (5500) :2319-2323." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A global geometric framework for nonlinear dimensionality reduction">
                                        <b>[17]</b>
                                         TENENBAUM J B, DE SILVA V, LANGFORD J C.A global geometric framework for nonlinear dimensionality reduction[J].Science, 2000, 290 (5500) :2319-2323.
                                    </a>
                                </li>
                                <li id="39">


                                    <a id="bibliography_18" title=" ROWEIS S T, SAUL L K.Nonlinear dimensionality reduction by locally linear embedding[J].Science, 2000, 290 (5500) :2323-2326." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Nonlinear dimensionality reduction by locally linear embedding">
                                        <b>[18]</b>
                                         ROWEIS S T, SAUL L K.Nonlinear dimensionality reduction by locally linear embedding[J].Science, 2000, 290 (5500) :2323-2326.
                                    </a>
                                </li>
                                <li id="41">


                                    <a id="bibliography_19" title=" 徐嘉明, 张卫强, 杨登舟, 等.基于流形正则化极限学习机的语种识别系统[J].自动化学报, 2015, 41 (9) :1680-1685." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201509014&amp;v=MDQ0NzZxQnRHRnJDVVJMT2VaZVJvRnlubFZicklLQ0xmWWJHNEg5VE1wbzlFWUlRS0RIODR2UjRUNmo1NE8zenE=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[19]</b>
                                         徐嘉明, 张卫强, 杨登舟, 等.基于流形正则化极限学习机的语种识别系统[J].自动化学报, 2015, 41 (9) :1680-1685.
                                    </a>
                                </li>
                                <li id="43">


                                    <a id="bibliography_20" title=" 李冬辉, 闫振林, 姚乐乐, 等.基于改进流形正则化极限学习机的短期电力负荷预测[J].高电压技术, 2016, 42 (7) :2092-2099." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GDYJ201607009&amp;v=MDI3MDVMRzRIOWZNcUk5RmJZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSb0Z5bmxWYnJJSWluU1o=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[20]</b>
                                         李冬辉, 闫振林, 姚乐乐, 等.基于改进流形正则化极限学习机的短期电力负荷预测[J].高电压技术, 2016, 42 (7) :2092-2099.
                                    </a>
                                </li>
                                <li id="45">


                                    <a id="bibliography_21" title=" TOMAR V S, ROSE R C.Manifold regularized deep neural networks[C]//Proceedings of the 15th Annual Conference of the International Speech Communication Association.Grenoble, France:International Speech Communication Association, 2014:348-352." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Manifold regularized deep neural networks">
                                        <b>[21]</b>
                                         TOMAR V S, ROSE R C.Manifold regularized deep neural networks[C]//Proceedings of the 15th Annual Conference of the International Speech Communication Association.Grenoble, France:International Speech Communication Association, 2014:348-352.
                                    </a>
                                </li>
                                <li id="47">


                                    <a id="bibliography_22" title=" GUAN Naiyang, TAO Dacheng, LUO Zhigang, et al.Manifold regularized discriminative nonnegative matrix factorization with fast gradient descent[J].IEEE Transactions on Image Processing, 2011, 20 (7) :2030-2048." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Manifold regularized discriminative nonnegative matrix factorization with fast gradient descent">
                                        <b>[22]</b>
                                         GUAN Naiyang, TAO Dacheng, LUO Zhigang, et al.Manifold regularized discriminative nonnegative matrix factorization with fast gradient descent[J].IEEE Transactions on Image Processing, 2011, 20 (7) :2030-2048.
                                    </a>
                                </li>
                                <li id="49">


                                    <a id="bibliography_23" title=" JIANG Mingyang, LIANG Yanchun, FENG Xiaoyue, et al.Text classification based on deep belief network and softmax regression[J].Neural Computing and Applications, 2018, 29 (1) :61-70." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Text classification based on deep belief network and softmax regression">
                                        <b>[23]</b>
                                         JIANG Mingyang, LIANG Yanchun, FENG Xiaoyue, et al.Text classification based on deep belief network and softmax regression[J].Neural Computing and Applications, 2018, 29 (1) :61-70.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJC" target="_blank">计算机工程</a>
                2019,45(06),242-248 DOI:10.19678/j.issn.1000-3428.0050777            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于流形正则化极限学习机的文本分类算法研究</b></span>
 <span class="shoufa"></span>                                     </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BA%9E%E7%9A%93%E6%98%8E&amp;code=38395665&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">庞皓明</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%86%80%E4%BF%8A%E5%BF%A0&amp;code=06289749&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">冀俊忠</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%88%98%E9%87%91%E9%93%8E&amp;code=33463171&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">刘金铎</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%A7%9A%E5%9E%9A&amp;code=38156432&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">姚垚</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%8C%97%E4%BA%AC%E5%B7%A5%E4%B8%9A%E5%A4%A7%E5%AD%A6%E5%A4%9A%E5%AA%92%E4%BD%93%E4%B8%8E%E6%99%BA%E8%83%BD%E8%BD%AF%E4%BB%B6%E6%8A%80%E6%9C%AF%E5%8C%97%E4%BA%AC%E5%B8%82%E9%87%8D%E7%82%B9%E5%AE%9E%E9%AA%8C%E5%AE%A4&amp;code=0034856&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">北京工业大学多媒体与智能软件技术北京市重点实验室</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>基于极限学习机的文本分类方法在对输入的文本特征进行随机映射时, 会呈现一种非线性的几何结构, 利用最小二乘法无法对其进行求解, 影响文本的分类性能。为此, 引入一种新的流形正则化思想, 提出基于极限学习机的改进算法。利用拉普拉斯特征映射保持输入文本特征的几何结构。基于样本的类别信息对样本点之间的距离进行修正, 优先选择类别相同的样本点, 以改善分类性能。在Reuters和20newsgroup数据集上的实验结果表明, 与正则化极限学习机算法、AdaBELM算法等相比, 该算法分类性能较好, <i>F</i>1-<i>measure</i>值可达91.42%。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">文本分类;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">监督学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%AD%A3%E5%88%99%E5%8C%96%E6%9E%81%E9%99%90%E5%AD%A6%E4%B9%A0%E6%9C%BA&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">正则化极限学习机;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B5%81%E5%BD%A2%E6%AD%A3%E5%88%99%E5%8C%96&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">流形正则化;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%89%B9%E5%BE%81%E6%98%A0%E5%B0%84&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">特征映射;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    庞皓明 (1993—) , 男, 硕士研究生, 主研方向为机器学习、自然语言处理;;
                                </span>
                                <span>
                                    *冀俊忠 (通信作者) , 教授、博士生导师;E-mail: jjz01@bjut.edu.cn;
                                </span>
                                <span>
                                    刘金铎, 博士研究生。;
                                </span>
                                <span>
                                    姚垚, 博士研究生。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-03-14</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金 (61375059, 61672065);</span>
                    </p>
            </div>
                    <h1><b>Research on Text Classification Algorithm Based on Manifold Regularization Extreme Learning Machine</b></h1>
                    <h2>
                    <span>PANG Haoming</span>
                    <span>JI Junzhong</span>
                    <span>LIU Jinduo</span>
                    <span>YAO Yao</span>
            </h2>
                    <h2>
                    <span>Beijing Key Laboratory of Multimedia and Intelligent Software Technology, Beijing University of Technology</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>In the text classification process, the Extreme Learning Machine (ELM) randomly maps the input text features and presents a nonlinear geometric structure.As a result, the least square method cannot solve such nonlinear structures and thus affects the text classification performance.To solve this problem, this paper introduces a new manifold regularization and presents an improved algorithm based on extreme machine learning.The Laplace feature mapping is used to preserve the geometry of input text features.The distance between sample points is modified based on the category information of the sample, and the sample points with the same category are selected first to improve the classification performance.Experimental results on the datasets of Reuters and 20 newsgroup show that, compared with the Regularization Extreme Learning Machine (RELM) , AdaBELM and other algorithms, the proposed algorithm has better classification performance, and the <i>F</i>1-<i>measure</i> can reach 91.42%.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=text%20classification&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">text classification;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=supervise%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">supervise learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Regularization%20Extreme%20Learning%20Machine%20(RELM)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Regularization Extreme Learning Machine (RELM) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=manifold%20regularization&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">manifold regularization;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=feature%20mapping&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">feature mapping;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-03-14</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="51" name="51" class="anchor-tag">0 概述</h3>
                <div class="p1">
                    <p id="52">目前, 很多机器学习方法被应用于文本分类问题中, 例如, 朴素贝叶斯<citation id="164" type="reference"><link href="5" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>、决策树<citation id="165" type="reference"><link href="7" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>、支持向量机<citation id="166" type="reference"><link href="9" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>和K近邻<citation id="167" type="reference"><link href="11" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>等。随着技术的发展, 深度玻尔兹曼机<citation id="168" type="reference"><link href="13" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>、循环卷积神经网络<citation id="169" type="reference"><link href="15" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>、长短期记忆神经网络<citation id="170" type="reference"><link href="17" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>和深度卷积神经网络<citation id="171" type="reference"><link href="19" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>等深度学习方法也逐渐应用到文本分类问题中。</p>
                </div>
                <div class="p1">
                    <p id="53">文献<citation id="172" type="reference">[<a class="sup">9</a>]</citation>提出一种单隐层的前馈神经网络学习算法, 即极限学习机 (Extreme Learning Machine, ELM) , 其已被应用到许多不同的领域<sup></sup><citation id="177" type="reference"><link href="23" rel="bibliography" /><link href="25" rel="bibliography" /><link href="27" rel="bibliography" /><sup>[<a class="sup">10</a>,<a class="sup">11</a>,<a class="sup">12</a>]</sup></citation>。文献<citation id="173" type="reference">[<a class="sup">10</a>]</citation>将正则化极限学习机 (Regularization Extreme Learning Machine, RELM) 应用到文本分类问题中, 该方法通过在ELM的线性方程组中构建正则项以解决ELM计算稳定性不足和过拟合问题<citation id="174" type="reference"><link href="29" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>。文献<citation id="175" type="reference">[<a class="sup">14</a>]</citation>将一种新的特征选择方法与ELM相结合来对文本进行分类。文献<citation id="176" type="reference">[<a class="sup">15</a>]</citation>将集成学习和ELM相结合提出一种新的文本分类模型AdaBELM, 解决了过拟合的问题。虽然ELM在文本分类问题上取得了良好的分类结果, 但是其在映射的过程中会将输入的文本特征随机映射到ELM的特征空间中, 呈现一种非线性的几何结构。由于通用的最小二乘法无法对非线性结构进行恢复和求解, 因此会影响文本的分类性能。</p>
                </div>
                <div class="p1">
                    <p id="54">2000年, 关于流形学习的研究取得重要进展<citation id="178" type="reference"><link href="35" rel="bibliography" /><link href="37" rel="bibliography" /><link href="39" rel="bibliography" /><sup>[<a class="sup">16</a>,<a class="sup">17</a>,<a class="sup">18</a>]</sup></citation>, 至此, 流形学习方法受到机器学习、模式识别、数据挖掘等领域研究者的广泛关注。流形学习不仅能将高维输入数据点映射到一个全局低维坐标系中, 还能保留邻接点之间的关系, 使数据保持原有的几何结构。同时, 该模型还具有平移、旋转不变的特性。已有学者将流形正则化思想和ELM相结合应用于监督学习中<citation id="179" type="reference"><link href="41" rel="bibliography" /><link href="43" rel="bibliography" /><sup>[<a class="sup">19</a>,<a class="sup">20</a>]</sup></citation>, 但其在应用过程中存在一定的缺陷。在有监督流形学习中, 样本的类别信息可对样本点间的距离进行修正, 帮助类别信息融入邻接图的构造中, 但是文献<citation id="180" type="reference">[<a class="sup">19</a>,<a class="sup">20</a>]</citation>所使用的流形学习方法, 仅通过欧式距离对样本点之间的距离进行度量, 忽视了类别信息对分类性能的提升作用。</p>
                </div>
                <div class="p1">
                    <p id="55">综上所述, 在文本分类过程中, ELM对文本特征的随机映射影响了文本的分类性能。流形学习可以使数据在新的特征空间中保持原有的几何结构, 使原特征空间中距离很近的样本在新的投影空间中相互接近<citation id="181" type="reference"><link href="45" rel="bibliography" /><link href="47" rel="bibliography" /><sup>[<a class="sup">21</a>,<a class="sup">22</a>]</sup></citation>。因此, 可以将流形正则化思想引入极限学习机相中, 以保持文本特征的几何结构。另一方面, 监督学习中样本的类别信息有助于提高分类性能, 而已有的流形学习与极限学习机相结合的方法没有考虑样本的类别信息。因此, 本文将一种新的流形正则化思想引入到极限学习机中, 提出基于流形正则化极限学习机的文本分类方法 (Mainfold Regularization Extreme Learning Machine on Text classification, MRELMT) 。</p>
                </div>
                <h3 id="56" name="56" class="anchor-tag">1 相关工作</h3>
                <h4 class="anchor-tag" id="57" name="57">1.1 极限学习机</h4>
                <div class="p1">
                    <p id="58">ELM作为一种单隐层的前馈神经网络, 其输入层和隐藏层之间的权值矩阵, 以及隐藏层节点的偏置是随机产生的。在训练过程中, 只需要设置隐藏层节点的个数, 就可以通过最小二乘法计算出隐藏层和输出层之间的权值矩阵。</p>
                </div>
                <div class="p1">
                    <p id="59">给定<i>N</i>个不同的训练样本 (<b><i>x</i></b><sub><i>i</i></sub>, <b><i>y</i></b><sub><i>i</i></sub>) , <b><i>x</i></b><sub><i>i</i></sub>=[<i>x</i><sub><i>i</i>1</sub>, <i>x</i><sub><i>i</i>2</sub>, …, <i>x</i><sub><i>in</i></sub>]<sup>T</sup>∈<image href="images/JSJC201906039_060.jpg" type="" display="inline" placement="inline"><alt></alt></image><sup><i>n</i></sup>, <b><i>y</i></b><sub><i>i</i></sub>=[<i>y</i><sub><i>i</i>1</sub>, <i>y</i><sub><i>i</i>2</sub>, …, <i>y</i><sub><i>im</i></sub>]<sup>T</sup>∈<image href="images/JSJC201906039_061.jpg" type="" display="inline" placement="inline"><alt></alt></image><sup><i>m</i></sup>, <i>i</i>=1, 2, …, <i>N</i>, 隐藏层节点个数为<i>L</i>, 激活函数为<i>g</i> (<i>x</i>) , 则ELM模型可表示为:</p>
                </div>
                <div class="p1">
                    <p id="62" class="code-formula">
                        <mathml id="62"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mi>j</mi><mi>L</mi></munderover><mi mathvariant="bold-italic">β</mi></mstyle><msub><mrow></mrow><mi>j</mi></msub><mi>g</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mi>j</mi><mi>L</mi></munderover><mi mathvariant="bold-italic">β</mi></mstyle><msub><mrow></mrow><mi>j</mi></msub><mi>g</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mi>j</mi></msub><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>+</mo><mi>b</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>i</mi></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="63">其中, <b><i>w</i></b><sub><i>j</i></sub>=[<i>w</i><sub><i>j</i>1</sub>, <i>w</i><sub><i>j</i>2</sub>, …, <i>w</i><sub><i>jn</i></sub>]<sup>T</sup>, <i>j</i>=1, 2, …, <i>L</i>, <b><i>w</i></b><sub><i>j</i></sub>表示输入层和第<i>j</i>个隐藏层节点之间的权值向量, <i>b</i><sub><i>j</i></sub>表示第<i>j</i>个隐藏层节点的偏置, <i>β</i><sub><i>j</i></sub>=[<i>β</i><sub><i>j</i>1</sub>, <i>β</i><sub><i>j</i>2</sub>, …, <i>β</i><sub><i>jn</i></sub>]<sup>T</sup>表示第<i>j</i>个隐藏层节点与输出层之间的权值向量。ELM也可以通过矩阵的形式表示为:</p>
                </div>
                <div class="p1">
                    <p id="64"><b><i>H</i></b><i>β</i>=<b><i>Y</i></b>      (2) </p>
                </div>
                <div class="p1">
                    <p id="65">其中, <b><i>H</i></b>是隐藏层的输出矩阵, <i>β</i>是隐藏层和输出层之间的权值矩阵, 具体如下:</p>
                </div>
                <div class="p1">
                    <p id="66" class="code-formula">
                        <mathml id="66"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi mathvariant="bold-italic">Η</mi><mo>=</mo></mtd></mtr><mtr><mtd><mrow><mrow><mo>[</mo><mrow><mtable><mtr><mtd><mi>g</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mn>1</mn></msub><mo>, </mo><mi>b</mi><msub><mrow></mrow><mn>1</mn></msub><mo>, </mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">) </mo></mtd><mtd><mi>g</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mn>2</mn></msub><mo>, </mo><mi>b</mi><msub><mrow></mrow><mn>2</mn></msub><mo>, </mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">) </mo></mtd><mtd><mo>⋯</mo></mtd><mtd><mi>g</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mi>n</mi></msub><mo>, </mo><mi>b</mi><msub><mrow></mrow><mi>n</mi></msub><mo>, </mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi>g</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mn>1</mn></msub><mo>, </mo><mi>b</mi><msub><mrow></mrow><mn>1</mn></msub><mo>, </mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">) </mo></mtd><mtd><mi>g</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mn>2</mn></msub><mo>, </mo><mi>b</mi><msub><mrow></mrow><mn>2</mn></msub><mo>, </mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">) </mo></mtd><mtd><mo>⋯</mo></mtd><mtd><mi>g</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mi>n</mi></msub><mo>, </mo><mi>b</mi><msub><mrow></mrow><mi>n</mi></msub><mo>, </mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mo>⋮</mo></mtd><mtd><mo>⋮</mo></mtd><mtd></mtd><mtd><mo>⋮</mo></mtd></mtr><mtr><mtd><mi>g</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mn>1</mn></msub><mo>, </mo><mi>b</mi><msub><mrow></mrow><mn>1</mn></msub><mo>, </mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>L</mi></msub><mo stretchy="false">) </mo></mtd><mtd><mi>g</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mn>2</mn></msub><mo>, </mo><mi>b</mi><msub><mrow></mrow><mn>2</mn></msub><mo>, </mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>L</mi></msub><mo stretchy="false">) </mo></mtd><mtd><mo>⋯</mo></mtd><mtd><mi>g</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mi>n</mi></msub><mo>, </mo><mi>b</mi><msub><mrow></mrow><mi>n</mi></msub><mo>, </mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>L</mi></msub><mo stretchy="false">) </mo></mtd></mtr></mtable></mrow><mo>]</mo></mrow></mrow><msub><mrow></mrow><mrow><mi>Ν</mi><mo>×</mo><mi>L</mi></mrow></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi mathvariant="bold-italic">β</mi><mo>=</mo><mrow><mrow><mo>[</mo><mrow><mtable><mtr><mtd><mi mathvariant="bold-italic">β</mi><msubsup><mrow></mrow><mn>1</mn><mtext>Τ</mtext></msubsup></mtd></mtr><mtr><mtd><mi mathvariant="bold-italic">β</mi><msubsup><mrow></mrow><mn>2</mn><mtext>Τ</mtext></msubsup></mtd></mtr><mtr><mtd><mo>⋮</mo></mtd></mtr><mtr><mtd><mi mathvariant="bold-italic">β</mi><msubsup><mrow></mrow><mi>L</mi><mtext>Τ</mtext></msubsup></mtd></mtr></mtable></mrow><mo>]</mo></mrow></mrow><msub><mrow></mrow><mrow><mi>L</mi><mo>×</mo><mi>Μ</mi></mrow></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="67">极限学习机为了提高预测的准确性, 将训练误差 (经验风险) 和输出矩阵的二次范式 (结构风险) 同时最小化, 获得隐藏层和输出层之间的权值矩阵, 可以得到如下公式:</p>
                </div>
                <div class="p1">
                    <p id="68">min:<mathml id="69"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">Η</mi><mi mathvariant="bold-italic">β</mi><mo>-</mo><mi mathvariant="bold-italic">Y</mi><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup><mo>+</mo><mfrac><mi>C</mi><mn>2</mn></mfrac><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">β</mi><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>5</mn><mo stretchy="false">) </mo></mrow></math></mathml></p>
                </div>
                <div class="p1">
                    <p id="70">其中, <i>C</i>是正则化因子, ‖·‖<mathml id="71"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></math></mathml>表示二范数。通过最小二乘法计算得到隐藏层和输出层之间的权值矩阵:</p>
                </div>
                <div class="p1">
                    <p id="72" class="code-formula">
                        <mathml id="72"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">β</mi><mo>=</mo><mrow><mrow><mo> (</mo><mrow><mfrac><mn>1</mn><mi>C</mi></mfrac><mo>+</mo><mi mathvariant="bold-italic">Η</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">Η</mi></mrow><mo>) </mo></mrow></mrow><msup><mrow></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mi mathvariant="bold-italic">Η</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">Y</mi><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>6</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="73"><i>ELM</i>隐藏层和输出层之间的权值通过最小二乘法直接计算得到, 算法的整个学习过程一次完成, 无需迭代, 因此该模型具有较快的学习速度。但是, 输入层和隐藏层之间的权值矩阵和隐藏层节点的偏置是通过随机初始化得到的, 在计算过程中会使文本特征从输入层随机地映射到隐藏层。因此输入的文本特征在<i>ELM</i>特征空间中的分布具有一定的随机性, 这会使得输入的文本特征出现某种非线性的几何结构, 仅使用最小二乘法无法对其几何结构进行求解和恢复, 进而影响文本的分类性能。</p>
                </div>
                <h4 class="anchor-tag" id="74" name="74">1.2 流形学习</h4>
                <div class="p1">
                    <p id="75">拉普拉斯映射是流形学习中的一种映射方法, 数据在经过拉普拉斯特征映射之后, 依然能保持原数据的特征。具体过程描述如下:</p>
                </div>
                <div class="p1">
                    <p id="76">首先计算每个样本点Z<sub>m</sub>和其他样本点之间的欧氏距离, 使用<i>k</i>近邻方法找到与样本点距离最近的k个样本点作为Z<sub>m</sub>的邻居节点。根据式 (7) 计算样本点之间的权重w<sub>mn</sub>, 得到样本点之间的矩阵<b><i>W</i></b>。</p>
                </div>
                <div class="p1">
                    <p id="77" class="code-formula">
                        <mathml id="77"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>w</mi><msub><mrow></mrow><mrow><mi>m</mi><mi>n</mi></mrow></msub><mo>=</mo><mrow><mo>{</mo><mrow><mtable><mtr><mtd columnalign="left"><mn>1</mn><mo>, </mo><mspace width="0.25em" /><mi>e</mi><mo stretchy="false"> (</mo><mi>Ζ</mi><msub><mrow></mrow><mi>m</mi></msub><mo>, </mo><mi>Ζ</mi><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mn>1</mn></mtd></mtr><mtr><mtd columnalign="left"><mn>0</mn><mo>, </mo><mspace width="0.25em" /><mtext>其</mtext><mtext>他</mtext></mtd></mtr></mtable></mrow></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>7</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="78">其中, <i>e</i> (<i>Z</i><sub><i>m</i></sub>, <i>Z</i><sub><i>n</i></sub>) =1表示样本<i>Z</i><sub><i>m</i></sub>和<i>Z</i><sub><i>n</i></sub>为邻居。通过计算拉普拉斯算子的广义特征向量, 求解广义特征值问题<b><i>Lf</i></b>=<b><i>LDf</i></b>, 其中<b><i>L</i></b>=<b><i>D</i></b>-<b><i>W</i></b>, <i>D</i><sub><i>mn</i></sub>=∑<i>w</i><sub><i>mn</i></sub>。</p>
                </div>
                <div class="p1">
                    <p id="79">拉普拉斯映射方法虽然在映射过程中可以保持原有数据的几何结构, 但是在计算样本点之间的距离时忽略了样本的类别信息。流形假设认为, 如果2个样本属于同一类别, 则它们在实际空间中的距离要比不属于同一类别的2个样本的距离要更近一些<citation id="182" type="reference"><link href="45" rel="bibliography" /><link href="47" rel="bibliography" /><sup>[<a class="sup">21</a>,<a class="sup">22</a>]</sup></citation>。因此, 当样本间的距离相同时, 该方法无法选出属于同一类别的样本点, 只能随机选取。</p>
                </div>
                <h3 id="80" name="80" class="anchor-tag">2 算法介绍</h3>
                <div class="p1">
                    <p id="81">本文将一种新的流形正则化思想引入极限学习机中, 通过流形正则化极限学习机对文本进行分类, 以提高分类性能。如图1所示, 流形正则化极限学习机文本分类模型由4个部分组成:</p>
                </div>
                <div class="p1">
                    <p id="82">1) 预处理:通过预处理操作清除文档数据中无用的信息和噪声等。该部分包括编码转换、移除停用词、大小写转换、英文词干提取、移除低频词、训练集和测试集分割等操作。</p>
                </div>
                <div class="p1">
                    <p id="83">2) 文本表示:使用空间向量模型 (<i>Vector Space Model</i>, <i>VSM</i>) 对文本进行表示, 将一篇文章表示为特征空间的一个向量。</p>
                </div>
                <div class="p1">
                    <p id="84">3) 特征提取:采用奇异值分解 (<i>Singular Value Decomposition</i>, <i>SVD</i>) 的方式对文本特征进行特征提取, 将原始的高维文本特征映射到其低维的子空间中。</p>
                </div>
                <div class="p1">
                    <p id="85">4) 文本分类:使用流形正则化极限学习机文本分类方法对文本进行分类。</p>
                </div>
                <div class="area_img" id="86">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201906039_086.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 基于流形正则化极限学习机的文本分类模型" src="Detail/GetImg?filename=images/JSJC201906039_086.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图1 基于流形正则化极限学习机的文本分类模型</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201906039_086.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="87" name="87">2.1 流形正则化思想</h4>
                <div class="p1">
                    <p id="88">当计算样本间的距离相同时, 现有的流形正则化思想无法选出属于同一类别的样本点。针对这一问题, 新的流形正则化思想使用一种有监督的距离度量方法来计算样本点之间的距离。在距离的度量方法中引入样本的类别信息, 基于这些类别信息对样本点之间的距离进行修正, 进而优先选择类别相同的样本点。</p>
                </div>
                <div class="p1">
                    <p id="89">流形正则化主要利用流形假设的思想, 如果2个样本<i>Z</i><sub><i>m</i></sub>和<i>Z</i><sub><i>n</i></sub>接近, 那么它们的标签<i>p</i><sub><i>m</i></sub>和<i>p</i><sub><i>n</i></sub>也应该接近。为实现这一假设, 将流形正则化的目标函数定义为:</p>
                </div>
                <div class="p1">
                    <p id="90">min:<mathml id="91"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>m</mi><mi>n</mi></mrow></munder><mi>s</mi></mstyle><msub><mrow></mrow><mrow><mi>m</mi><mi>n</mi></mrow></msub><mo stretchy="false">∥</mo><mi>p</mi><msub><mrow></mrow><mi>m</mi></msub><mo>-</mo><mi>p</mi><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>8</mn><mo stretchy="false">) </mo></mrow></math></mathml></p>
                </div>
                <div class="p1">
                    <p id="92">其中, <i>s</i><sub><i>mn</i></sub>表示样本<i>Z</i><sub><i>m</i></sub>和<i>Z</i><sub><i>n</i></sub>之间的相似度。在计算样本点相似度之前, 需要使用k近邻方法找到与样本点距离最近的<i>k</i>个样本点。使用流形学习中有监督的距离度量方法计算样本点之间的距离, 即在距离计算时考虑样本类别信息。该方法认为在样本点的所有近邻点中, 其同类样本点邻近的概率大于异类邻近的概率, 因此在计算2个样本点之间的距离时增加修正项, 使同类样本点间的距离尽可能小于异类样本点间的距离。此方法不仅可以保持同类样本间的距离, 而且还拉大了不同类样本间的距离。有监督的距离度量方法可以保持有监督数据的流形结构, 减少对输入特征的随机映射。样本间距离计算过程如下:</p>
                </div>
                <div class="p1">
                    <p id="93" class="code-formula">
                        <mathml id="93"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>D</mi><mo stretchy="false"> (</mo><mi>z</mi><msub><mrow></mrow><mi>m</mi></msub><mo>, </mo><mi>z</mi><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mrow><mo>{</mo><mrow><mtable><mtr><mtd columnalign="left"><msqrt><mrow><mn>1</mn><mo>-</mo><mtext>e</mtext><msup><mrow></mrow><mrow><mfrac><mrow><mo>-</mo><mi>d</mi><msup><mrow></mrow><mn>2</mn></msup><mo stretchy="false"> (</mo><mi>z</mi><msub><mrow></mrow><mi>m</mi></msub><mo>, </mo><mi>z</mi><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false">) </mo></mrow><mi>α</mi></mfrac></mrow></msup></mrow></msqrt><mo>, </mo><mi>p</mi><msub><mrow></mrow><mi>m</mi></msub><mo>=</mo><mi>p</mi><msub><mrow></mrow><mi>n</mi></msub></mtd></mtr><mtr><mtd columnalign="left"><msqrt><mrow><mtext>e</mtext><msup><mrow></mrow><mrow><mfrac><mrow><mo>-</mo><mi>d</mi><msup><mrow></mrow><mn>2</mn></msup><mo stretchy="false"> (</mo><mi>z</mi><msub><mrow></mrow><mi>m</mi></msub><mo>, </mo><mi>z</mi><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false">) </mo></mrow><mi>α</mi></mfrac></mrow></msup></mrow></msqrt><mo>-</mo><mi>σ</mi><mo>, </mo><mi>p</mi><msub><mrow></mrow><mi>m</mi></msub><mo>≠</mo><mi>p</mi><msub><mrow></mrow><mi>n</mi></msub></mtd></mtr></mtable></mrow></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>9</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="94">其中, <i>α</i>和<i>σ</i>是指定常数, <i>D</i> (<i>z</i><sub><i>m</i></sub>, <i>z</i><sub><i>n</i></sub>) 是结合样本点类别信息之后的距离, <i>d</i> (<i>z</i><sub><i>m</i></sub>, <i>z</i><sub><i>n</i></sub>) 是忽略样本点类别信息的欧式距离, 即<mathml id="95"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>d</mi><mo stretchy="false"> (</mo><mi>z</mi><msub><mrow></mrow><mi>m</mi></msub><mo>, </mo><mi>z</mi><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mrow><mo>|</mo><mrow><mi>z</mi><msub><mrow></mrow><mi>m</mi></msub><mo>-</mo><mi>z</mi><msub><mrow></mrow><mi>n</mi></msub></mrow><mo>|</mo></mrow></mrow></math></mathml>。参数α用来避免指数函数中的d (z<sub>m</sub>, z<sub>n</sub>) 的快速增加, 特别是当其自身比较大的时候, 参数α的调节作用较大。同时, 参数α与数据集中的数据密集程度密切相关, 一般取所有成对数据点的欧氏距离的平均值。参数σ (0≤σ≤1) 是调节不同类别数据点间距离的常数因子。</p>
                </div>
                <div class="p1">
                    <p id="96">在计算样本点间的距离之后, 选择k个样本作为样本z<sub>m</sub>的邻居节点, 并通过高斯核函数计算样本点与邻居节点之间的相似度s<sub>mn</sub>。本文只对属于同一类别并且互为邻居的节点进行计算, 其他互为邻居的数据不进行计算。相似度计算过程如下:</p>
                </div>
                <div class="p1">
                    <p id="97" class="code-formula">
                        <mathml id="97"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>s</mi><msub><mrow></mrow><mrow><mi>m</mi><mi>n</mi></mrow></msub><mo>=</mo><mrow><mo>{</mo><mrow><mtable><mtr><mtd columnalign="left"><mi>exp</mi><mrow><mo> (</mo><mrow><mfrac><mrow><mo>-</mo><mo stretchy="false">∥</mo><mi>z</mi><msub><mrow></mrow><mi>m</mi></msub><mo>-</mo><mi>z</mi><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup></mrow><mi>ρ</mi></mfrac></mrow><mo>) </mo></mrow><mo>, </mo><mi>e</mi><mo stretchy="false"> (</mo><mi>z</mi><msub><mrow></mrow><mi>m</mi></msub><mo>, </mo><mi>z</mi><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mn>1</mn><mo>, </mo><mi>c</mi><mo stretchy="false"> (</mo><mi>z</mi><msub><mrow></mrow><mi>m</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mi>c</mi><mo stretchy="false"> (</mo><mi>z</mi><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd columnalign="left"><mn>0</mn><mo>, </mo><mtext>其</mtext><mtext>他</mtext></mtd></mtr></mtable></mrow></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>0</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="98">其中, c (z<sub>m</sub>) =c (z<sub>n</sub>) 表示样本z<sub>m</sub>和z<sub>n</sub>的类别相同。样本z<sub>m</sub>和z<sub>n</sub>之间的相似度越大, 则s<sub>mn</sub>的值也越大。因此得到相似度矩阵<b><i>S</i></b>如下:</p>
                </div>
                <div class="p1">
                    <p id="99" class="code-formula">
                        <mathml id="99"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">S</mi><mo>=</mo><mrow><mrow><mo>[</mo><mrow><mtable><mtr><mtd><mi>s</mi><msub><mrow></mrow><mrow><mn>1</mn><mn>1</mn></mrow></msub></mtd><mtd><mi>s</mi><msub><mrow></mrow><mrow><mn>1</mn><mn>2</mn></mrow></msub></mtd><mtd><mo>⋯</mo></mtd><mtd><mi>s</mi><msub><mrow></mrow><mrow><mn>1</mn><mi>Ν</mi></mrow></msub></mtd></mtr><mtr><mtd><mi>s</mi><msub><mrow></mrow><mrow><mn>2</mn><mn>1</mn></mrow></msub></mtd><mtd><mi>s</mi><msub><mrow></mrow><mrow><mn>2</mn><mn>2</mn></mrow></msub></mtd><mtd><mo>⋯</mo></mtd><mtd><mi>s</mi><msub><mrow></mrow><mrow><mn>2</mn><mi>Ν</mi></mrow></msub></mtd></mtr><mtr><mtd><mo>⋮</mo></mtd><mtd><mo>⋮</mo></mtd><mtd></mtd><mtd><mo>⋮</mo></mtd></mtr><mtr><mtd><mi>s</mi><msub><mrow></mrow><mrow><mi>Ν</mi><mn>1</mn></mrow></msub></mtd><mtd><mi>s</mi><msub><mrow></mrow><mrow><mi>Ν</mi><mn>2</mn></mrow></msub></mtd><mtd><mo>⋯</mo></mtd><mtd><mi>s</mi><msub><mrow></mrow><mrow><mi>Ν</mi><mi>Ν</mi></mrow></msub></mtd></mtr></mtable></mrow><mo>]</mo></mrow></mrow><msub><mrow></mrow><mrow><mi>Ν</mi><mo>×</mo><mi>Ν</mi></mrow></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="100">相似度矩阵<b><i>S</i></b>是一个对称矩阵。经过变换, 流形正则化思想的目标函数可以表示为:</p>
                </div>
                <div class="p1">
                    <p id="101">min: tr (<b><i>Y</i></b><sup>T</sup><b><i>LY</i></b>)      (12) </p>
                </div>
                <div class="p1">
                    <p id="102">其中, <mathml id="103"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">Y</mi><mo>=</mo><mrow><mrow><mo>[</mo><mrow><mtable><mtr><mtd><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mn>1</mn></msub></mtd><mtd><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mn>2</mn></msub></mtd><mtd><mo>⋯</mo></mtd><mtd><mi mathvariant="bold-italic">y</mi><msub><mrow></mrow><mi>Ν</mi></msub></mtd></mtr></mtable></mrow><mo>]</mo></mrow></mrow><msup><mrow></mrow><mtext>Τ</mtext></msup></mrow></math></mathml>是样本点的输出矩阵, <b><i>L</i></b>是经过计算得到的拉普拉斯矩阵, <b><i>L</i></b>=<b><i>D</i></b>-<b><i>S</i></b>, <b><i>D</i></b>是一个<i>N</i>×<i>N</i>大小的对角矩阵, 计算如下:</p>
                </div>
                <div class="p1">
                    <p id="104" class="code-formula">
                        <mathml id="104"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>D</mi><msub><mrow></mrow><mrow><mi>m</mi><mi>m</mi></mrow></msub><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>s</mi></mstyle><msub><mrow></mrow><mrow><mi>m</mi><mi>n</mi></mrow></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="105">其中, <i>D</i><sub><i>mm</i></sub>表示矩阵<b><i>D</i></b>中第<i>m</i>行的对角值。</p>
                </div>
                <h4 class="anchor-tag" id="106" name="106">2.2 基于流形正则化极限学习机的文本分类模型</h4>
                <div class="p1">
                    <p id="107">基于上述的分析和推导, 本文将流形正则化思想引入极限学习机中, 以克服其在文本分类问题中出现的问题。首先, 对文本数据集进行如下数学描述:文本数据集<i>Ω</i>={ (<b><i>x</i></b><sub><i>i</i></sub>, <b><i>y</i></b><sub><i>i</i></sub>) }, <i>i</i>=1, 2, …, <i>N</i>, 有<i>N</i>篇文档 (<b><i>x</i></b><sub><i>i</i></sub>, <b><i>y</i></b><sub><i>i</i></sub>) , 其中, <b><i>x</i></b><sub><i>i</i></sub>=[<i>x</i><sub><i>i</i>1</sub>, <i>x</i><sub><i>i</i>2</sub>, …, <i>x</i><sub><i>ir</i></sub>]<sup>T</sup>∈<image href="images/JSJC201906039_108.jpg" type="" display="inline" placement="inline"><alt></alt></image><sup><i>r</i></sup>代表文本数据集<i>Ω</i>中的第<i>i</i>篇文档, <i>x</i><sub><i>ir</i></sub>代表第<i>i</i>篇文档中的第<i>r</i>个特征词的TF-IDF权值, <i>r</i>为经过SVD处理之后文本数据集<i>Ω</i>中特征词的个数, <b><i>y</i></b><sub><i>i</i></sub>=[<i>y</i><sub><i>i</i>1</sub>, <i>y</i><sub><i>i</i>2</sub>, …, <i>y</i><sub><i>iq</i></sub>]<sup>T</sup>∈<image href="images/JSJC201906039_109.jpg" type="" display="inline" placement="inline"><alt></alt></image><sup><i>q</i></sup>则代表第<i>i</i>篇文档所属的类别, <i>q</i>表示文本数据集<i>Ω</i>中的类别数。此外, 需要设置隐藏层节点个数<i>o</i>和激活函数<i>g</i> (<b><i>x</i></b>) , 随机设置隐藏层节点参数:权值<i>a</i><sub><i>j</i></sub>∈<image href="images/JSJC201906039_110.jpg" type="" display="inline" placement="inline"><alt></alt></image><sup><i>n</i></sup>, 偏置<i>b</i><sub><i>j</i></sub>∈<image href="images/JSJC201906039_111.jpg" type="" display="inline" placement="inline"><alt></alt></image><sup><i>n</i></sup>, <i>j</i>=1, 2, …, <i>o</i>。流形正则化极限学习机结构如图2所示。</p>
                </div>
                <div class="area_img" id="112">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201906039_112.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 流形正则化极限学习机示意图" src="Detail/GetImg?filename=images/JSJC201906039_112.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图2 流形正则化极限学习机示意图</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201906039_112.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="113">为了避免ELM在训练过程中, 随机设置输入层和隐藏层之间的权值<i>a</i><sub><i>o</i></sub>以及隐藏层节点的偏置<i>b</i><sub><i>o</i></sub>而产生非线性的映射, 本文将流形正则化思想引入正则化极限学习机中, 即将目标函数式 (12) 引入正则化极限学习机中, 使其选出能够保持原有结构的邻接点, 得到以下公式:</p>
                </div>
                <div class="p1">
                    <p id="114">min:<mathml id="115"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mrow><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">Η</mi><mi mathvariant="bold-italic">β</mi><mo>-</mo><mi mathvariant="bold-italic">Y</mi><mo stretchy="false">∥</mo></mrow><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup><mo>+</mo><mfrac><mrow><mi>C</mi><msub><mrow></mrow><mn>1</mn></msub></mrow><mn>2</mn></mfrac><mrow><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">β</mi><mo stretchy="false">∥</mo></mrow><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup><mo>+</mo><mfrac><mrow><mi>C</mi><msub><mrow></mrow><mn>2</mn></msub></mrow><mn>2</mn></mfrac><mtext>t</mtext><mtext>r</mtext><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">Y</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">L</mi><mi mathvariant="bold-italic">Y</mi><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>4</mn><mo stretchy="false">) </mo></mrow></math></mathml></p>
                </div>
                <div class="p1">
                    <p id="116">其中, <i>C</i><sub>1</sub>为RELM的正则化因子, 用来平衡极限学习机的经验风险和结构风险, <i>C</i><sub>2</sub>为流形正则化思想中的正则化因子。通过对目标函数求导并另导数值为0, 得到下式:</p>
                </div>
                <div class="p1">
                    <p id="117"><b><i>H</i></b><sup>T</sup> (<b><i>H</i></b><i>β</i>-<b><i>Y</i></b>) +<i>C</i><sub>1</sub><i>β</i>+<i>C</i><sub>2</sub><b><i>H</i></b><sup>T</sup><b><i>LH</i></b><i>β</i>=0      (15) </p>
                </div>
                <div class="p1">
                    <p id="118">计算隐藏层和输出层之间的矩阵<i>β</i>, 结果如下:</p>
                </div>
                <div class="p1">
                    <p id="119"><i>β</i>= (<b><i>H</i></b><sup>T</sup><b><i>H</i></b>+<i>C</i><sub>1</sub><b><i>I</i></b>+<i>C</i><sub>2</sub><b><i>H</i></b><sup>T</sup><b><i>LH</i></b>) <sup>-1</sup><b><i>H</i></b><sup>T</sup><b><i>Y</i></b>      (16) </p>
                </div>
                <div class="p1">
                    <p id="120"><b><i>H</i></b>为隐藏层的输出矩阵, 即:</p>
                </div>
                <div class="p1">
                    <p id="121" class="code-formula">
                        <mathml id="121"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi mathvariant="bold-italic">Η</mi><mo>=</mo></mtd></mtr><mtr><mtd><mrow><mrow><mo>[</mo><mrow><mtable><mtr><mtd><mi>g</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mn>1</mn></msub><mo>, </mo><mi>b</mi><msub><mrow></mrow><mn>1</mn></msub><mo>, </mo><mtext> </mtext><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">) </mo></mtd><mtd><mi>g</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mn>2</mn></msub><mo>, </mo><mi>b</mi><msub><mrow></mrow><mn>2</mn></msub><mo>, </mo><mtext> </mtext><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">) </mo></mtd><mtd><mo>⋯</mo></mtd><mtd><mi>g</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mi>o</mi></msub><mo>, </mo><mi>b</mi><msub><mrow></mrow><mi>o</mi></msub><mo>, </mo><mtext> </mtext><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi>g</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mn>1</mn></msub><mo>, </mo><mi>b</mi><msub><mrow></mrow><mn>1</mn></msub><mo>, </mo><mtext> </mtext><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">) </mo></mtd><mtd><mi>g</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mn>2</mn></msub><mo>, </mo><mi>b</mi><msub><mrow></mrow><mn>2</mn></msub><mo>, </mo><mtext> </mtext><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">) </mo></mtd><mtd><mo>⋯</mo></mtd><mtd><mi>g</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mi>o</mi></msub><mo>, </mo><mi>b</mi><msub><mrow></mrow><mi>o</mi></msub><mo>, </mo><mtext> </mtext><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mo>⋮</mo></mtd><mtd><mo>⋮</mo></mtd><mtd></mtd><mtd><mo>⋮</mo></mtd></mtr><mtr><mtd><mi>g</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mn>1</mn></msub><mo>, </mo><mi>b</mi><msub><mrow></mrow><mn>1</mn></msub><mo>, </mo><mtext> </mtext><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>Ν</mi></msub><mo stretchy="false">) </mo></mtd><mtd><mi>g</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mn>2</mn></msub><mo>, </mo><mi>b</mi><msub><mrow></mrow><mn>2</mn></msub><mo>, </mo><mtext> </mtext><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>Ν</mi></msub><mo stretchy="false">) </mo></mtd><mtd><mo>⋯</mo></mtd><mtd><mi>g</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mi>o</mi></msub><mo>, </mo><mi>b</mi><msub><mrow></mrow><mi>o</mi></msub><mo>, </mo><mtext> </mtext><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>Ν</mi></msub><mo stretchy="false">) </mo></mtd></mtr></mtable></mrow><mo>]</mo></mrow></mrow><msub><mrow></mrow><mrow><mi>Ν</mi><mo>×</mo><mi>o</mi></mrow></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>7</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <h4 class="anchor-tag" id="122" name="122">2.3 MRELMT算法</h4>
                <div class="p1">
                    <p id="123"><i>MRELMT</i>算法的具体过程如下:</p>
                </div>
                <div class="p1">
                    <p id="124"><b>算法</b> MRELMT算法</p>
                </div>
                <div class="p1">
                    <p id="125"><b>输入</b> 训练样本集<i>Ω</i>={ (<b><i>x</i></b><sub><i>i</i></sub>, <b><i>y</i></b><sub><i>i</i></sub>) }, <i>i</i>=1, 2, …, <i>N</i>, <b><i>x</i></b><sub><i>i</i></sub>∈<image href="images/JSJC201906039_126.jpg" type="" display="inline" placement="inline"><alt></alt></image><sup><i>r</i></sup>, <b><i>y</i></b><sub><i>i</i></sub>∈<image href="images/JSJC201906039_127.jpg" type="" display="inline" placement="inline"><alt></alt></image><sup><i>q</i></sup></p>
                </div>
                <div class="p1">
                    <p id="128"><b>输出</b> 隐藏层节点的输出矩阵<i>β</i></p>
                </div>
                <div class="p1">
                    <p id="129"><b>步骤1</b> 设置MRELMT隐藏层节点个数<i>o</i>和激活函数<i>g</i> (<b><i>x</i></b>) , 随机设置隐藏层节点参数:权值<i>a</i><sub><i>j</i></sub>∈<image href="images/JSJC201906039_130.jpg" type="" display="inline" placement="inline"><alt></alt></image><sup><i>n</i></sup>, 偏置<i>b</i><sub><i>j</i></sub>∈<image href="images/JSJC201906039_131.jpg" type="" display="inline" placement="inline"><alt></alt></image><sup><i>n</i></sup>, <i>j</i>=1, 2, …, <i>o</i>。</p>
                </div>
                <div class="p1">
                    <p id="132"><b>步骤2</b> 将全部数据映射到MRELMT的特征空间中。</p>
                </div>
                <div class="p1">
                    <p id="133"><b>步骤3</b> 根据式 (9) 计算样本<b><i>x</i></b><sub><i>i</i></sub>与其他节点之间的距离, 并使用k近邻方法选择<i>k</i>个样本作为<b><i>x</i></b><sub><i>i</i></sub>的邻居节点。</p>
                </div>
                <div class="p1">
                    <p id="134"><b>步骤4</b> 根据式 (10) 计算样本<b><i>x</i></b><sub><i>i</i></sub>和邻居节点之间的相似度, 得到相似度矩阵<b><i>S</i></b>。</p>
                </div>
                <div class="p1">
                    <p id="135"><b>步骤5</b> 根据<b><i>L</i></b>=<b><i>D</i></b>-<b><i>S</i></b>计算的拉普拉斯矩阵<b><i>L</i></b>。</p>
                </div>
                <div class="p1">
                    <p id="136"><b>步骤6</b> 根据式 (16) 计算隐藏层节点的输出矩阵<i>β</i>。</p>
                </div>
                <div class="p1">
                    <p id="137">MRELMT算法与极限学习机文本分类算法最大的不同在于其隐藏层和输出层之间的权值矩阵求解方式不同。RELM是通过最小二乘法直接计算得到, 时间复杂度为<i>O</i> (1) , 而MRELMT算法的通过引入流形正则化思想计算得到。MRELMT算法在计算权值矩阵<i>β</i>时, 根据式 (9) 计算每个样本点与其他样本间的距离, 并选择出距离最近的<i>k</i>个邻居节点。通过式 (10) 和式 (11) 计算样本点与邻居节点之间的相似度矩阵, 计算相似度矩阵时的时间复杂度为<i>O</i> (<i>N</i><sup>2</sup>) , 其中, <i>N</i>是样本数。然后通过式 (13) 计算得到<i>N</i>×<i>N</i>大小的对角矩阵<b><i>D</i></b>, 其时间复杂度为<i>O</i> (<i>N</i><sup>2</sup>) , 并通过公式<b><i>L</i></b>=<b><i>D</i></b>-<b><i>S</i></b>计算得到样本点的拉普拉斯矩阵<b><i>L</i></b>。最后应用最小二乘法计算隐藏层和输出层之间的矩阵<i>β</i>, 此部分直接通过矩阵的运算完成, 无需迭代, 故时间复杂度为<i>O</i> (1) 。因此, MRELMT算法的时间复杂度为<i>O</i> (<i>N</i><sup>2</sup>) 。与RELM算法相比, MRELMT算法时间复杂度更高, 这主要与算法中计算拉普拉斯矩阵有关, 且训练样本的数目越多, 算法的计算复杂度越高。</p>
                </div>
                <h3 id="138" name="138" class="anchor-tag">3 实验结果与分析</h3>
                <h4 class="anchor-tag" id="139" name="139">3.1 实验数据集及评价标准</h4>
                <div class="p1">
                    <p id="140">本文实验使用了2个常用的标准英文数据集:20<i>newsgroup</i>与<i>Reuters</i>。其中, 20<i>newsgroup</i>数据集是由约20 000篇新闻文本组成的数据集, 这些文本被分为20个不同的组, 每个组对应一个类别, 每个类别的文档数分布均匀。采用标准的<i>ModApte</i>划分方法将数据集分为训练集和测试集。经过移除停用词、小于5的低频词, 大小写转换, 词干还原等预处理操作后, 该数据集包含27 800个特征词、20个文本类别、13 192篇训练集样本和5 654篇测试集样本。本文选取前20%的特征词, 即使用5 560个特征词进行实验。Reuters数据集是由路透社发布的21 578篇财经新闻组成的文本数据集, 这些新闻文本被分为5大类, 135小类。由于Reuters数据集文档分布不均匀, 本文选取其中最常用的10个子类别组成的文本数据集。经过与20newsgroup数据集相同的预处理操作后, 该数据集包含6 429个特征词、10个文本类别、5 271篇训练集文本、2 252篇测试集文本。表1给出20newsgroup和Reuters数据集的基本参数。</p>
                </div>
                <div class="area_img" id="141">
                    <p class="img_tit"><b>表1 2个数据集的基本参数</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="141" border="1"><tr><td><br />数据集</td><td>训练样本</td><td>测试样本</td><td>特征词</td><td>类别</td></tr><tr><td><br />Reuters</td><td>5 271</td><td>2 252</td><td>6 429</td><td>10</td></tr><tr><td><br />20newsgroup</td><td>13 192</td><td>5 654</td><td>5 560</td><td>20</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="142">采用准确率 (Accuracy) 、召回率 (Recall) 、精确率 (Precision) 和F1值 (F1-measure) 4个指标来评估文本分类算法的效果, 其具体计算过程如下:</p>
                </div>
                <div class="p1">
                    <p id="143" class="code-formula">
                        <mathml id="143"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>A</mi><mi>c</mi><mi>c</mi><mi>u</mi><mi>r</mi><mi>a</mi><mi>c</mi><mi>y</mi><mo>=</mo><mfrac><mrow><mi>Ν</mi><msub><mrow></mrow><mrow><mi>Τ</mi><mi>Ρ</mi></mrow></msub><mo>+</mo><mi>Ν</mi><msub><mrow></mrow><mrow><mi>Τ</mi><mi>Ν</mi></mrow></msub></mrow><mrow><mi>Ν</mi><msub><mrow></mrow><mrow><mi>Τ</mi><mi>Ρ</mi></mrow></msub><mo>+</mo><mi>Ν</mi><msub><mrow></mrow><mrow><mi>Τ</mi><mi>Ν</mi></mrow></msub><mo>+</mo><mi>Ν</mi><msub><mrow></mrow><mrow><mi>F</mi><mi>Ν</mi></mrow></msub><mo>+</mo><mi>Ν</mi><msub><mrow></mrow><mrow><mi>F</mi><mi>Ρ</mi></mrow></msub></mrow></mfrac><mo>×</mo><mn>1</mn><mn>0</mn><mn>0</mn><mi>%</mi><mo>=</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mfrac><mrow><mi>Ν</mi><msub><mrow></mrow><mrow><mi>Τ</mi><mi>Ρ</mi></mrow></msub><mo>+</mo><mi>Ν</mi><msub><mrow></mrow><mrow><mi>Τ</mi><mi>Ν</mi></mrow></msub></mrow><mrow><mi>Ν</mi><msub><mrow></mrow><mi>Ρ</mi></msub><mo>+</mo><mi>Ν</mi><msub><mrow></mrow><mi>L</mi></msub></mrow></mfrac><mo>×</mo><mn>1</mn><mn>0</mn><mn>0</mn><mi>%</mi><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>8</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi>Ρ</mi><mi>r</mi><mi>e</mi><mi>c</mi><mi>i</mi><mi>s</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo>=</mo><mfrac><mrow><mi>Ν</mi><msub><mrow></mrow><mrow><mi>Τ</mi><mi>Ρ</mi></mrow></msub></mrow><mrow><mi>Ν</mi><msub><mrow></mrow><mrow><mi>Τ</mi><mi>Ρ</mi></mrow></msub><mo>+</mo><mi>Ν</mi><msub><mrow></mrow><mrow><mi>F</mi><mi>Ρ</mi></mrow></msub></mrow></mfrac><mo>×</mo><mn>1</mn><mn>0</mn><mn>0</mn><mi>%</mi><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>9</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi>R</mi><mi>e</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>l</mi><mo>=</mo><mfrac><mrow><mi>Ν</mi><msub><mrow></mrow><mrow><mi>Τ</mi><mi>Ρ</mi></mrow></msub></mrow><mrow><mi>Ν</mi><msub><mrow></mrow><mrow><mi>Τ</mi><mi>Ρ</mi></mrow></msub><mo>+</mo><mi>Ν</mi><msub><mrow></mrow><mrow><mi>F</mi><mi>Ν</mi></mrow></msub></mrow></mfrac><mo>×</mo><mn>1</mn><mn>0</mn><mn>0</mn><mi>%</mi><mo>=</mo><mfrac><mrow><mi>Ν</mi><msub><mrow></mrow><mrow><mi>Τ</mi><mi>Ρ</mi></mrow></msub></mrow><mrow><mi>Ν</mi><msub><mrow></mrow><mi>Ρ</mi></msub></mrow></mfrac><mo>×</mo><mn>1</mn><mn>0</mn><mn>0</mn><mi>%</mi><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mn>0</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi>F</mi><mn>1</mn><mo>-</mo><mi>m</mi><mi>e</mi><mi>a</mi><mi>s</mi><mi>u</mi><mi>r</mi><mi>e</mi><mo>=</mo><mfrac><mrow><mn>2</mn><mi>R</mi><mi>e</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>l</mi><mo>⋅</mo><mi>Ρ</mi><mi>r</mi><mi>e</mi><mi>c</mi><mi>i</mi><mi>s</mi><mi>i</mi><mi>o</mi><mi>n</mi></mrow><mrow><mi>Ρ</mi><mi>r</mi><mi>e</mi><mi>c</mi><mi>i</mi><mi>s</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo>+</mo><mi>R</mi><mi>e</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>l</mi></mrow></mfrac><mo>×</mo><mn>1</mn><mn>0</mn><mn>0</mn><mi>%</mi><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mn>1</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="144">其中, <i>N</i>表示数据集中的样本个数, <i>N</i><sub><i>P</i></sub>表示正样本个数, <i>N</i><sub><i>L</i></sub>表示负样本个数, <i>N</i><sub><i>TP</i></sub>表示实际为正、预测为正的样本数, <i>N</i><sub><i>FN</i></sub>表示实际为正、预测为负的样本数, <i>N</i><sub><i>TN</i></sub>表示实际为负、预测为负的样本数, <i>N</i><sub><i>FP</i></sub>表示实际为负、预测为正的样本数, 且<i>N</i><sub><i>P</i></sub>=<i>N</i><sub><i>TP</i></sub>+<i>N</i><sub><i>FN</i></sub>, <i>N</i><sub><i>L</i></sub>=<i>N</i><sub><i>TN</i></sub>+<i>N</i><sub><i>FP</i></sub>。</p>
                </div>
                <h4 class="anchor-tag" id="145" name="145">3.2 实验环境及参数</h4>
                <div class="p1">
                    <p id="146">本文实验在以下环境配置下进行:<i>Intel</i> (<i>R</i>) <i>Core</i> (<i>TM</i>) <i>i</i>5-4590<i>CPU</i>@3.30 <i>GHz</i>, 12 <i>GB</i>内存, <i>Windows</i>7 64位操作系统, <i>Matlab</i>2010软件平台。</p>
                </div>
                <div class="p1">
                    <p id="147">首先确定<i>MRELMT</i>算法的隐藏层节点个数, 将其取值范围设定为1 000～11 000, 步长为500。在实验过程中, 每次只改变隐藏层节点的个数, 经过多次试验, 确定隐藏层结点个数为9 000。图3给出<i>MRELMT</i>算法在<i>Reuters</i>和20<i>newsgroup</i>数据集上F1-measure随着隐藏层节点个数变化的折线图。从图3可以看出, <i>MRELMT</i>算法的F1-measure随着隐藏层节点个数的增加而不断提高, 达到峰值后继续增加节点个数会出现过拟合现象。在20<i>newsgroup</i>数据集上, 当隐藏层节点达到4 000时, F1-measure的变化趋势逐渐平缓, 其分类效果趋于稳定, 当节点数达9 000时, 分类效果最佳。在<i>Reuters</i>数据集上, 当隐藏层节点数较少时, <i>MRELMT</i>算法的分类效果较差, 当节点数达到5 000时, 分类效果显著提升, 之后随着隐藏层节点个数的增加, 分类性能持续上升。</p>
                </div>
                <div class="area_img" id="148">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201906039_148.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 MRELMT算法在2个数据集上的F1-measure值" src="Detail/GetImg?filename=images/JSJC201906039_148.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图3 MRELMT算法在2个数据集上的<i>F</i>1-<i>measure</i>值</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201906039_148.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="149">然后, 确定正则项因子<i>C</i><sub>1</sub>和<i>C</i><sub>2</sub>的值, 其取值范围是{10<sup>-4</sup>, 10<sup>-3</sup>, …, 10<sup>3</sup>, 10<sup>4</sup>}, 经过多次实验, 取<i>C</i><sub>1</sub>=1  000, <i>C</i><sub>2</sub>=0.01。在实验过程中发现, 正则化参数<i>C</i><sub>1</sub>对分类性能的影响较大。选取不同的<i>C</i><sub>1</sub>, 算法的分类性能会发生较大变化。当<i>C</i><sub>1</sub>取值偏大时, MRELMT算法的分类性能相对较好。当<i>C</i><sub>1</sub>值为10<sup>3</sup>或10<sup>4</sup>时, 分类效果最好, 因此固定<i>C</i><sub>1</sub>值, 对<i>C</i><sub>2</sub>值进行选取。在实验中, <i>C</i><sub>2</sub>对分类性能的影响没有<i>C</i><sub>1</sub>明显。当<i>C</i><sub>2</sub>值为0.01时, 分类性能最佳。将本文算法与RELM算法进行对比实验, 其参数的设置使用与MRELMT算法相同的策略, RELM的正则化因子取0.01, 隐藏层节点个数为8 000。表2给出MRELMT算法的训练参数。</p>
                </div>
                <div class="area_img" id="150">
                    <p class="img_tit"><b>表2 MRELMT算法的训练参数</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="150" border="1"><tr><td><br />参数</td><td>取值</td></tr><tr><td><br />隐藏层节点个数</td><td>9 000</td></tr><tr><td><br />正则化因子<i>C</i><sub>1</sub></td><td>1 000</td></tr><tr><td><br />正则化因子<i>C</i><sub>2</sub></td><td>0.01</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="151" name="151">3.3 结果分析</h4>
                <div class="p1">
                    <p id="152">为了验证MRELMT算法的有效性, 将其与RELM算法<citation id="183" type="reference"><link href="37" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>在Reuters和20newsgroup数据集上进行对比。此外, 将MRELMT算法与AdaBELM算法<citation id="184" type="reference"><link href="33" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>、ELM-OAA<citation id="185" type="reference"><link href="31" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>和DBN+Softmax (2) <citation id="186" type="reference"><link href="49" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>进行比较, 以进一步评价多隐层极限学习机在文本分类问题中的分类性能。</p>
                </div>
                <h4 class="anchor-tag" id="153" name="153">3.3.1 流形正则化的效果</h4>
                <div class="p1">
                    <p id="154">表3给出本文MRELMT算法和RELM算法在2个数据集上的分类性能评价指标。从表3可以看出, 在Reuters和20newsgroup数据集上MRELMT算法的<i>Accuracy</i>、<i>Precision</i>、<i>Recall</i>、<i>F</i>1-<i>measure</i>明显高于RELM算法, 表明MRELMT算法具有更好的分类性能和分类稳定性。这是因为MRELM算法在极限学习机中引入流形正则化思想, 使文本的分类性能明显提升。由此证明了流形正则化可以使输入的文本特征在经过映射之后保持原有的特征结构。新的流形正则化思想在构建拉普拉斯矩阵的过程中优先选择类别相同的样本点, 在样本间距离相同时, MRELMT算法会优先选择类别相同的临近点, 进一步提升了分类性能。</p>
                </div>
                <div class="area_img" id="155">
                    <p class="img_tit"><b>表3 2种算法在不同数据集上的分类性能</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note">%</p>
                    <table id="155" border="1"><tr><td><br />数据集</td><td>算法</td><td><i>Accuracy</i></td><td><i>Precision</i></td><td><i>Recall</i></td><td><i>F</i>1-<i>measure</i></td></tr><tr><td rowspan="2"><br />Reuters</td><td><br />RELM</td><td>85.43</td><td>92.23</td><td>86.37</td><td>89.23</td></tr><tr><td><br />MRELMT</td><td>90.39</td><td>93.12</td><td>89.78</td><td>91.42</td></tr><tr><td rowspan="2"><br />20newsgroup</td><td><br />RELM</td><td>84.26</td><td>85.28</td><td>84.63</td><td>84.34</td></tr><tr><td><br />MRELMT</td><td>86.91</td><td>86.59</td><td>85.47</td><td>86.03</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="156" name="156">3.3.2 多种算法分类结果的对比</h4>
                <div class="p1">
                    <p id="157">为了进一步验证MRELMT算法的分类性能, 将本文算法与其他4种算法在Reuters和20newsgroup数据集上进行对比。图4给出5种算法在Reuters数据集上的<i>Accuracy</i>、<i>Precision</i>、<i>Recall</i>、<i>F</i>1-<i>measure</i>。从图4可以看出, 在Reuters数据集上MRELMT算法的准确率略低于DBN+Softmax (2) 算法, 其召回率略低于ELM-OAA算法, 但是其精确率和<i>F</i>1-<i>measure</i>取得了较好的结果。</p>
                </div>
                <div class="area_img" id="158">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201906039_158.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 5种算法在Reuters数据集上的分类性能" src="Detail/GetImg?filename=images/JSJC201906039_158.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图4 5种算法在Reuters数据集上的分类性能</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201906039_158.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="159">图5给出5种算法在20newsgroup数据集上分类的<i>Accuracy</i>、<i>Precision</i>、<i>Recall</i>、<i>F</i>1-<i>measure</i>。从图5可以看出, 在20newsgroup数据集上MRELMT算法的准确率和召回率都取得较优结果, 但是其精确率和<i>F</i>1-<i>measure</i>低于DBN+Softmax (2) 算法。</p>
                </div>
                <div class="area_img" id="160">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201906039_160.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 5种算法在20newsgroup数据集上的分类性能" src="Detail/GetImg?filename=images/JSJC201906039_160.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图5 5种算法在20newsgroup数据集上的分类性能</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201906039_160.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="161">从上述结果可以看出, MRELMT算法在Reuters和20newsgroup 2个数据集上的分类性能要优于AdaBELM、ELM-OAA和RELM等其他使用了极限学习机的算法。<i>F</i>1-<i>measure</i>作为一个结合了精确率和召回率的综合评价标准, 可全面反映算法的性能。在<i>F</i>1-<i>measure</i>指标上, MRELMT算法在2个数据集上都要优于AdaBELM算法、ELM-OAA算法和RELM算法, 进一步说明在极限学习机中引入流形正则化思想的有效性。其主要原因是MRELMT算法可以在一定程度上改善输入文本特征随机映射的问题, 使输入的文本特征在极限学习机的特征空间中依然保持原有的几何结构。在引入流形正则化思想后, MRELMT算法在构建拉普拉斯矩阵的过程中, 可通过文本的类别信息对样本间的距离进行修正, 以提升算法的文本分类性能。然而, MRELMT算法在Reuters和20newsgroup 2个数据集上的某些分类性能指标要低于DBN+Softmax (2) 算法, 其主要原因在于DBN+Softmax (2) 算法具有深层网络结构, 特征提取能力较强, 能够获取更多的文本语义特征, 其对文本的分类性能有一定的影响。</p>
                </div>
                <h3 id="162" name="162" class="anchor-tag">4 结束语</h3>
                <div class="p1">
                    <p id="163">本文提出一种基于流形正则化极限学习机的文本分类算法。通过引入流形正则化思想, 保持输入文本特征的几何结构, 并基于文本的类别信息对样本点间的距离进行修正。实验结果表明, 与其他极限学习机的文本分类算法相比, 该算法的分类性能有一定的提升。下一步将引入多隐层结构的极限学习机对文本进行分类, 以改善特征提取能力, 获得更高层的文本特征, 进而提高文本分类的性能。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="5">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=DISCRIMINATIVELY WEIGHTED NAIVE BAYES AND ITS APPLICATION IN TEXT CLASSIFICATION">

                                <b>[1]</b> JIANG Liangxiao, WANG Dianhong, CAI Zhihua.Discriminatively weighted naive bayes and its application in text classification[J].International Journal on Artificial Intelligence Tools, 2012, 21 (1) :1-19.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD14101500001203&amp;v=MTg5ODE5SE5xbzlGWk9zT0RudzZvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVVMbklJVjRTYnhJPU5qN0Jhcks4SA==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> KENEKAYORO P, BUCKLEY K, THELWALL M.Automatic classification of academic Web page types[J].Scientometrics, 2014, 101 (2) :1015-1026.
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Support Vector Machines and Word2vec for Text Classification with Semantic Features">

                                <b>[3]</b> LILLEBERG J, ZHU Yun, ZHANG Yanqing.Support vector machines and word2vec for text classification with semantic features[C]//Proceedings of the 14th International Conference on Cognitive Informatics and Cognitive Computing.Washington D.C., USA:IEEE Press, 2015:136-140.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XXWX201710004&amp;v=MjE5MjBESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSb0Z5bmxWYnJJUFRYY2RyRzRIOWJOcjQ5RllJUUs=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b> 杨帅华, 张清华.粗糙集近似集的KNN文本分类算法研究[J].小型微型计算机系统, 2017, 38 (10) :2192-2196.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Modeling documents with deep boltzmann machines">

                                <b>[5]</b> SRIVASTAVA N, SALAKHUTDINOV R R, HINTON G E.Modeling documents with deep boltzmann machines[EB/OL].[2018-02-25].https://arxiv.org/ftp/arxiv/papers/13 09/1309.6865.pdf.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Recurrent Convolutional Neural Networks for Text Classification">

                                <b>[6]</b> LAI Siwei, XU Liheng, LIU Kang, et al.Recurrent convolutional neural networks for text classification[C]//Proceedings of the 29th AAAI Conference on Artificial Intelligence.Palo Alto, USA:Association for the Advance of Artificial Intelligence, 2015:2267-2273.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Text classification improved by integrating bidirectional LSTM with two-dimensional max pooling">

                                <b>[7]</b> ZHOU Peng, QI Zhenyu, ZHENG Suncong, et al.Text classification improved by integrating bidirectional LSTM with two-dimensional max pooling[EB/OL].[2018-02-25].https://arxiv.org/pdf/1611.06639.pdf.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Very deep convolutional networks for text classification">

                                <b>[8]</b> CONNEAU A, SCHWENK H, BARRAULT L, et al.Very deep convolutional networks for text classification[EB/OL].[2018-02-25].https://arxiv.org/pdf/160 6.01781.pdf.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501913101&amp;v=MTM0MzVHZXJxUVRNbndaZVp1SHlqbVVMbklJVjRTYnhJPU5pZk9mYks3SHRETnFvOUViZW9NRFh3NG9CTVQ2VDRQUUgvaXJSZA==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b> HUANG Guangbin, ZHU Qinyu, SIEW C K.Extreme learning machine:theory and applications[J].Neurocomputing, 2006, 70 (1/2/3) :489-501.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Kernel ELM and CNN based facial age estimation">

                                <b>[10]</b> GURPINAR F, KAYA H, DIBEKLIOGLU H, et al.Kernel ELM and CNN based facial age estimation[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition Workshops.Washington D.C., USA:IEEE Press, 2016:80-86.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Robotic grasp stability analysis using extreme learning machine">

                                <b>[11]</b> BAI Peng, LIU Huaping, SUN Fuchun, et al.Robotic grasp stability analysis using extreme learning machine[C]//Proceedings of ELM’16.Berlin, Germany:Springer, 2016:37-51.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Haptic identication by ELM-controlled uncertain manipulator">

                                <b>[12]</b> YANG Chenguang, HUANG Kunxia, CHENG Hong, et al.Haptic identification by ELM-controlled uncertain manipulator[J].IEEE Transactions on Systems, Man, and Cybernetics:Systems, 2017, 47 (8) :2398-2409.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_13" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD130409074765&amp;v=MTMzNzA3SUpsc1VOajdCYXJLN0h0WE1wbzlDWU93SkNSTTh6eFVTbURkOVNIN24zeEU5ZmJ2bktyaWZadTl1RkN2alU3&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[13]</b> ZHENG Wenbin, QIAN Yuntao, LU Huijuan.Text categorization based on regularization extreme learning machine[J].Neural Computing and Applications, 2013, 22 (3/4) :447-456.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Extreme learning machines in the field of text classification">

                                <b>[14]</b> ROUL R K, NANDA A, PATEL V, et al.Extreme learning machines in the field of text classification[C]//Proceedings of the 16th IEEE/ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing.Washington D.C., USA:IEEE Press, 2015:1-7.
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Overfitting reduction of text classification based on AdaBELM">

                                <b>[15]</b> FENG Xiaoyue, LIANG Yanchun, SHI Xiaohu, et al.Overfitting reduction of text classification based on AdaBELM[J].Entropy, 2017, 19 (7) :1-13.
                            </a>
                        </p>
                        <p id="35">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The manifold ways of perception">

                                <b>[16]</b> SEUNG H S, LEE D D.The manifold ways of perception[J].Science, 2000, 290 (5500) :2268-2269.
                            </a>
                        </p>
                        <p id="37">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A global geometric framework for nonlinear dimensionality reduction">

                                <b>[17]</b> TENENBAUM J B, DE SILVA V, LANGFORD J C.A global geometric framework for nonlinear dimensionality reduction[J].Science, 2000, 290 (5500) :2319-2323.
                            </a>
                        </p>
                        <p id="39">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Nonlinear dimensionality reduction by locally linear embedding">

                                <b>[18]</b> ROWEIS S T, SAUL L K.Nonlinear dimensionality reduction by locally linear embedding[J].Science, 2000, 290 (5500) :2323-2326.
                            </a>
                        </p>
                        <p id="41">
                            <a id="bibliography_19" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201509014&amp;v=MTYyNDllUm9GeW5sVmJySUtDTGZZYkc0SDlUTXBvOUVZSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVo=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[19]</b> 徐嘉明, 张卫强, 杨登舟, 等.基于流形正则化极限学习机的语种识别系统[J].自动化学报, 2015, 41 (9) :1680-1685.
                            </a>
                        </p>
                        <p id="43">
                            <a id="bibliography_20" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GDYJ201607009&amp;v=MDgxODI4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm9GeW5sVmJySUlpblNaTEc0SDlmTXFJOUZiWVFLREg=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[20]</b> 李冬辉, 闫振林, 姚乐乐, 等.基于改进流形正则化极限学习机的短期电力负荷预测[J].高电压技术, 2016, 42 (7) :2092-2099.
                            </a>
                        </p>
                        <p id="45">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Manifold regularized deep neural networks">

                                <b>[21]</b> TOMAR V S, ROSE R C.Manifold regularized deep neural networks[C]//Proceedings of the 15th Annual Conference of the International Speech Communication Association.Grenoble, France:International Speech Communication Association, 2014:348-352.
                            </a>
                        </p>
                        <p id="47">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Manifold regularized discriminative nonnegative matrix factorization with fast gradient descent">

                                <b>[22]</b> GUAN Naiyang, TAO Dacheng, LUO Zhigang, et al.Manifold regularized discriminative nonnegative matrix factorization with fast gradient descent[J].IEEE Transactions on Image Processing, 2011, 20 (7) :2030-2048.
                            </a>
                        </p>
                        <p id="49">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Text classification based on deep belief network and softmax regression">

                                <b>[23]</b> JIANG Mingyang, LIANG Yanchun, FENG Xiaoyue, et al.Text classification based on deep belief network and softmax regression[J].Neural Computing and Applications, 2018, 29 (1) :61-70.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJC201906039" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201906039&amp;v=MTAwMjRiYkc0SDlqTXFZOUdiWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm9GeW5sVmJySUx6N0I=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFQxQTZSZWp6VHBWMkMvWVdDaHZmUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
