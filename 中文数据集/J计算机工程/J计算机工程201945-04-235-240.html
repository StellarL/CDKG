<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637130615259181250%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJC201904039%26RESULT%3d1%26SIGN%3d%252bqd28nSgqidEczLYw%252fV4LJwNdXc%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201904039&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201904039&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201904039&amp;v=MDQ5NDM0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSb0Z5L2dVcjNBTHo3QmJiRzRIOWpNcTQ5R2JZUUtESDg=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#35" data-title="0 概述 ">0 概述</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#41" data-title="1 基于三维重建的高光谱影像拼接流程 ">1 基于三维重建的高光谱影像拼接流程</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#46" data-title="2 特征点提取、匹配与光束法平差 ">2 特征点提取、匹配与光束法平差</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#59" data-title="3 基于密集点云的影像微分纠正 ">3 基于密集点云的影像微分纠正</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#60" data-title="3.1 算法原理">3.1 算法原理</a></li>
                                                <li><a href="#82" data-title="3.2 多视角立体三维重建与点云处理">3.2 多视角立体三维重建与点云处理</a></li>
                                                <li><a href="#90" data-title="3.3 影像拼接">3.3 影像拼接</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#92" data-title="4 实验结果与分析 ">4 实验结果与分析</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#105" data-title="5 结束语 ">5 结束语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#43" data-title="&lt;b&gt;图1 无人机高光谱影像拼接流程&lt;/b&gt;"><b>图1 无人机高光谱影像拼接流程</b></a></li>
                                                <li><a href="#45" data-title="&lt;b&gt;图2 波段配准前后对比&lt;/b&gt;"><b>图2 波段配准前后对比</b></a></li>
                                                <li><a href="#50" data-title="&lt;b&gt;图3 SIFT特征点提取结果&lt;/b&gt;"><b>图3 SIFT特征点提取结果</b></a></li>
                                                <li><a href="#52" data-title="&lt;b&gt;图4 单应性约束的特征点匹配结果&lt;/b&gt;"><b>图4 单应性约束的特征点匹配结果</b></a></li>
                                                <li><a href="#64" data-title="&lt;b&gt;图5 中心投影成像过程&lt;/b&gt;"><b>图5 中心投影成像过程</b></a></li>
                                                <li><a href="#87" data-title="&lt;b&gt;图6 基于CMVS\\PMVS算法点云的生成&lt;/b&gt;"><b>图6 基于CMVS\\PMVS算法点云的生成</b></a></li>
                                                <li><a href="#94" data-title="&lt;b&gt;图7 无人机影像序列 (单波段显示&lt;/b&gt;) "><b>图7 无人机影像序列 (单波段显示</b>) </a></li>
                                                <li><a href="#95" data-title="&lt;b&gt;表1 拼接实验各步骤耗时情况&lt;/b&gt;"><b>表1 拼接实验各步骤耗时情况</b></a></li>
                                                <li><a href="#98" data-title="&lt;b&gt;图8 基于三维重建的影像拼接结果&lt;/b&gt;"><b>图8 基于三维重建的影像拼接结果</b></a></li>
                                                <li><a href="#99" data-title="&lt;b&gt;图9 基于DEM正射校正的影像拼接结果&lt;/b&gt;"><b>图9 基于DEM正射校正的影像拼接结果</b></a></li>
                                                <li><a href="#100" data-title="&lt;b&gt;图10 基于多项式几何校正的影像拼接结果&lt;/b&gt;"><b>图10 基于多项式几何校正的影像拼接结果</b></a></li>
                                                <li><a href="#102" data-title="&lt;b&gt;表2 相邻影像对匹配点残差&lt;/b&gt;"><b>表2 相邻影像对匹配点残差</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" title=" 李德仁, 李明.无人机遥感系统的研究进展与应用前景[J].武汉大学学报 (信息科学版) , 2014, 39 (5) :505-513, 540." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=WHCH201405001&amp;v=MDI0MDk5WE1xbzlGWllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJvRnkvZ1VyckpNaVhJWnJHNEg=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         李德仁, 李明.无人机遥感系统的研究进展与应用前景[J].武汉大学学报 (信息科学版) , 2014, 39 (5) :505-513, 540.
                                    </a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" title=" 张永军.无人驾驶飞艇低空遥感影像的几何处理[J].武汉大学学报 (信息科学版) , 2009, 34 (3) :284-288." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=WHCH200903008&amp;v=MzIyOTdJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSb0Z5L2dVcnJKTWlYSVpyRzRIdGpNckk5RmI=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         张永军.无人驾驶飞艇低空遥感影像的几何处理[J].武汉大学学报 (信息科学版) , 2009, 34 (3) :284-288.
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title=" 韩文超, 周利剑, 贾韶辉, 等.基于POS系统的无人机遥感图像融合方法的研究与实现[J].遥感信息, 2013, 28 (3) :80-84, 90." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=YGXX201303014&amp;v=Mjg1Mjg2ajU0TzN6cXFCdEdGckNVUkxPZVplUm9GeS9nVXJySlBDclRkckc0SDlMTXJJOUVZSVFLREg4NHZSNFQ=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         韩文超, 周利剑, 贾韶辉, 等.基于POS系统的无人机遥感图像融合方法的研究与实现[J].遥感信息, 2013, 28 (3) :80-84, 90.
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" title=" 袁修孝.POS辅助光束法区域网平差[J].测绘学报, 2008, 37 (3) :342-348." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=CHXB200803013&amp;v=Mjg0MDVSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm9GeS9nVXJySkppWFRiTEc0SHRuTXJJOUVaNFFLREg4NHY=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                         袁修孝.POS辅助光束法区域网平差[J].测绘学报, 2008, 37 (3) :342-348.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" title=" 孙家抦.遥感原理与应用[M].武汉:武汉大学出版社, 2013." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CBBD&amp;filename=9787307107618000&amp;v=Mjg4MTFNcUlsRWJPc1BEQk04enhVU21EZDlTSDduM3hFOWZidm5LcmlmWnU5dUZDdmpVN2pOSVZzVlhGcXpHYkM0R2RE&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                         孙家抦.遥感原理与应用[M].武汉:武汉大学出版社, 2013.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" title=" AGARWAL S, FURUKAWA Y, SNAVELY N, et al.Building Rome in a day[J].Communications of the ACM, 2011, 54 (10) :105-112." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000004760&amp;v=MTU1NjBVTG5JSjFzVmJ4TT1OaWZJWTdLN0h0ak5yNDlGWk9zTEMzbzVvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbQ==&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                         AGARWAL S, FURUKAWA Y, SNAVELY N, et al.Building Rome in a day[J].Communications of the ACM, 2011, 54 (10) :105-112.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" title=" LOWE D G.Distinctive image features from scale-invariant keypoints[J].International Journal of Computer Vision, 2004, 60 (2) :91-110." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002830901&amp;v=MTI0ODNCYXJPNEh0SE9wNHhGYmVzT1kzazV6QmRoNGo5OVNYcVJyeG94Y01IN1I3cWVidWR0RkM3bFZMckpKVjg9Tmo3&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                         LOWE D G.Distinctive image features from scale-invariant keypoints[J].International Journal of Computer Vision, 2004, 60 (2) :91-110.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" title=" 张剑清, 潘励, 王树根.摄影测量学[M].2版.武汉:武汉大学出版社, 2009." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CBBD&amp;filename=9787307069558001&amp;v=MTAxNTVuS3JpZlp1OXVGQ3ZqVTdqTklWc1ZYRnF6R2JDNEdkSEtwb3BBYk9zUERSTTh6eFVTbURkOVNIN24zeEU5ZmJ2&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                         张剑清, 潘励, 王树根.摄影测量学[M].2版.武汉:武汉大学出版社, 2009.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" title=" WU C.SiftGPU:a GPU implementation of scale invariant feature transform[EB/OL].[2018-02-12].http://cs.unc.edu/～ccwu/siftgpu." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=SiftGPU:a GPU implementation of scale invariant feature transform">
                                        <b>[9]</b>
                                         WU C.SiftGPU:a GPU implementation of scale invariant feature transform[EB/OL].[2018-02-12].http://cs.unc.edu/～ccwu/siftgpu.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_10" title=" ZIEGLER G, TEVS A, THEOBALT C, et al.GPU point list generation through histogram pyramids[M].[S.1.]:Max Planck Society, 2006." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=GPU point list generation through histogram pyramids">
                                        <b>[10]</b>
                                         ZIEGLER G, TEVS A, THEOBALT C, et al.GPU point list generation through histogram pyramids[M].[S.1.]:Max Planck Society, 2006.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_11" title=" 柯涛, 田一涵, 段延松.基于GPU的 SIFT 特征点快速匹配[J].测绘与空间地理信息, 2014, 37 (12) :14-16." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DBCH201412005&amp;v=Mjk1OTQ5RllZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSb0Z5L2dVcnJKSVMvSVpyRzRIOVhOclk=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[11]</b>
                                         柯涛, 田一涵, 段延松.基于GPU的 SIFT 特征点快速匹配[J].测绘与空间地理信息, 2014, 37 (12) :14-16.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_12" title=" 胡庆武, 艾明耀, 殷万玲, 等.大旋角无人机影像全自动拼接方法研究[J].计算机工程, 2012, 38 (15) :152-155." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201215044&amp;v=MTY5NzBSb0Z5L2dVcnJKTHo3QmJiRzRIOVBOcW85QllJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmU=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[12]</b>
                                         胡庆武, 艾明耀, 殷万玲, 等.大旋角无人机影像全自动拼接方法研究[J].计算机工程, 2012, 38 (15) :152-155.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_13" title=" TRIGGS B, MCLAUCHLAN P F, HARTLEY R I, et al.Bundle adjustment——a modern synthesis[C]//Proceedings of International Workshop on Vision Algorithms.Berlin, Germany:Springer, 1999:298-372." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Bundle Adjustment-A Modern Synthesis">
                                        <b>[13]</b>
                                         TRIGGS B, MCLAUCHLAN P F, HARTLEY R I, et al.Bundle adjustment——a modern synthesis[C]//Proceedings of International Workshop on Vision Algorithms.Berlin, Germany:Springer, 1999:298-372.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_14" title=" WU C, AGARWAL S, CURLESS B, et al.Multicore bundle adjustment[C]//Proceedings of 2011 IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2011:3057-3064." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multicore bundle adjustment">
                                        <b>[14]</b>
                                         WU C, AGARWAL S, CURLESS B, et al.Multicore bundle adjustment[C]//Proceedings of 2011 IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2011:3057-3064.
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_15" title=" ARA S, MOUNT D M, NETANYAHU N S, et al.An optimal algorithm for approximate nearest neighbor searching fixed dimensions[J].Journal of the ACM, 1998, 45 (6) :891-923." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM14010900003649&amp;v=MTI2NThETXBvOUZaT3NNQ25nd29CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVUxuSUoxc1ZieE09TmlmSVk3SzhIdA==&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[15]</b>
                                         ARA S, MOUNT D M, NETANYAHU N S, et al.An optimal algorithm for approximate nearest neighbor searching fixed dimensions[J].Journal of the ACM, 1998, 45 (6) :891-923.
                                    </a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_16" title=" FURUKAWA Y, PONCE J.Accurate, dense, and robust multiview stereopsis[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2010, 32 (8) :1362-1376." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Accurate, Dense, and Robust Multiview Stereopsis">
                                        <b>[16]</b>
                                         FURUKAWA Y, PONCE J.Accurate, dense, and robust multiview stereopsis[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2010, 32 (8) :1362-1376.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJC" target="_blank">计算机工程</a>
                2019,45(04),235-240 DOI:10.19678/j.issn.1000-3428.0049800            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于三维重建的大区域无人机影像全自动拼接方法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%82%B9%E6%9D%BE&amp;code=37801670&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">邹松</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%94%90%E5%A8%89&amp;code=33239930&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">唐娉</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%83%A1%E6%98%8C%E8%8B%97&amp;code=33241679&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">胡昌苗</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%8D%95%E5%B0%8F%E5%86%9B&amp;code=33240402&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">单小军</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E7%A7%91%E5%AD%A6%E9%99%A2%E9%81%A5%E6%84%9F%E4%B8%8E%E6%95%B0%E5%AD%97%E5%9C%B0%E7%90%83%E7%A0%94%E7%A9%B6%E6%89%80&amp;code=1698844&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国科学院遥感与数字地球研究所</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E7%A7%91%E5%AD%A6%E9%99%A2%E5%A4%A7%E5%AD%A6&amp;code=1698842&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国科学院大学</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>低成本无人机由于不具有精密的惯性导航系统, 其拍摄区域有可能是一些难以布设控制点的无人区, 因此无法采用传统的航空摄影测量处理手段获得拍摄区域的拼接影像。为此, 提出一种基于三维重建的无人机影像全自动拼接方法。利用从运动恢复结构和多视角立体算法重建拍摄区域的密集点云, 根据密集点云数据采用一种基于邻点分布约束的点坐标插值算法内插待定点空间坐标, 运用间接微分纠正方法对影像进行几何校正, 从而获得几何一致的拼接影像。实验结果表明, 该方法全程无需人工干预且拼接耗时短, 相邻影像之间几何拼接精度约为2.5个像素。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%97%A0%E4%BA%BA%E6%9C%BA&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">无人机;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%BD%B1%E5%83%8F%E6%8B%BC%E6%8E%A5&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">影像拼接;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">三维重建;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%BB%8E%E8%BF%90%E5%8A%A8%E6%81%A2%E5%A4%8D%E7%BB%93%E6%9E%84&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">从运动恢复结构;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%A4%9A%E8%A7%86%E8%A7%92%E7%AB%8B%E4%BD%93&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">多视角立体;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    邹松 (1993—) , 男, 硕士研究生, 主研方向为遥感图像处理;;
                                </span>
                                <span>
                                    *唐娉 (通信作者) , 研究员、博士生导师, E-mail:tangping@radi.ac.cn;;
                                </span>
                                <span>
                                    胡昌苗, 助理研究员。;
                                </span>
                                <span>
                                    单小军, 助理研究员。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2017-12-21</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金青年基金 (41601384);</span>
                    </p>
            </div>
                    <h1><b>Full-automatic Splicing Method of Unmanned Aerial Vehicle Images in Large Area Based on 3D Reconstruction</b></h1>
                    <h2>
                    <span>ZOU Song</span>
                    <span>TANG Ping</span>
                    <span>HU Changmiao</span>
                    <span>SHAN Xiaojun</span>
            </h2>
                    <h2>
                    <span>Institute of Remote Sensing and Digital Earth, Chinese Academy of Sciences</span>
                    <span>University of Chinese Academy of Sciences</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Because the low-cost Unmanned Aerial Vehicle (UAV) does not have a sophisticated inertial navigation system, the shooting area may be some unmanned areas where it is difficult to lay out control points, the conventional aerial photogrammetric processing method cannot be used to obtain the splicing image of the shooting area.Therefore, a method of full-automatic splicing images based on 3 D reconstruction is proposed.Dense point cloud of the shooting area is generated by using Structure From Motion (SFM) and Multi-view Stereo (MVS) algorithms, a point coordinate interpolation algorithm based on adjacent point distribution constraint is used to interpolate the space coordinates to be undetermined, and images are geometric-rectified by using indirect differential correction algorithm, so a geometric-consistent splicing image is obtained.Experimental result shows that the method requires no human intervention and the process takes a short time, and the geometric splicing accuracy between adjacent images is about 2.5 pixels.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Unmanned%20Aerial%20Vehicle%20(UAV)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Unmanned Aerial Vehicle (UAV) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=image%20splicing&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">image splicing;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=3D%20reconstruction&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">3D reconstruction;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Structure%20From%20Motion%20(SFM)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Structure From Motion (SFM) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Multi-view%20Stereo%20(MVS)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Multi-view Stereo (MVS) ;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2017-12-21</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="35" name="35" class="anchor-tag">0 概述</h3>
                <div class="p1">
                    <p id="36">无人机遥感与传统的卫星遥感、航空遥感相比具有一些不可替代的优点, 传统载人航空遥感受制于长航时、危险拍摄环境以及恶劣气候等因素, 卫星遥感因天气和时间有可能无法获取感兴趣区域遥感信息, 而无人机遥感因其成本低廉、起降灵活、风险小、飞行高度低等优点, 已成为卫星遥感与航空遥感的一种有效获取遥感信息的补充手段<citation id="107" type="reference"><link href="3" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>。无人机遥感在国土资源环境调查、测区整体现状评估与灾害应急响应等领域具有良好的应用前景, 而快速准确的无人机影像拼接是其中重要的一项数据处理步骤。</p>
                </div>
                <div class="p1">
                    <p id="37">常规无人机影像拼接是利用拍摄区域内的数字高程模型 (Digital Elevation Model, DEM) 对区域内影像进行正射校正, 继而根据影像像素地理信息完成拼接<citation id="108" type="reference"><link href="5" rel="bibliography" /><link href="7" rel="bibliography" /><link href="9" rel="bibliography" /><sup>[<a class="sup">2</a>,<a class="sup">3</a>,<a class="sup">4</a>]</sup></citation>。这种拼接方法可以得到高精度的正射影像拼接图, 但是DEM的数据生成是一个繁琐复杂的过程, 利用数字摄影测量方法生成DEM涉及到影像后方交会、立体像对空间前方交会、影像立体匹配、匹配点的手动编辑等步骤。这种拼接方法需要在测区内布设高精度的控制点, 如果拍摄区域是无法布设控制点的无人区, 则这种拼接方法不适用。另外, 这种拼接方法多处需要人工干预, 如后方交会控制点像点的选取、立体匹配点的手动编辑等, 所以, 也不适用于一些实时性要求高的应用。</p>
                </div>
                <div class="p1">
                    <p id="38">另外一种常用的遥感影像拼接方法是通过参考影像对区域内所有影像进行图像配准, 继而镶嵌得到拼接影像<citation id="109" type="reference"><link href="11" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>。通常的做法是通过多项式拟合参考影像与待配准影像之间的几何变换关系。参考影像既可以是拍摄区域的正射影像图, 也可以是该区域的其他带有地理信息的地理图件。这种拼接方法虽然可以较快地获取拼接影像, 但是拼接过程依赖拍摄区域的地理图件, 如果不存在该区域的参考影像, 则该拼接方法不适用。且该方法需要手动提取参考影像与待配准影像之间的同名点, 无法全自动的得到拼接影像。</p>
                </div>
                <div class="p1">
                    <p id="39">利用基于图像的三维重建技术可以在较短时间内重建拍摄区域, 文献<citation id="110" type="reference">[<a class="sup">6</a>]</citation>运用照片分享网站上关于罗马城的照片集, 采用并行分布式的匹配与重建算法在1 d内处理了超过150 000幅图像, 重建了整个罗马城。</p>
                </div>
                <div class="p1">
                    <p id="40">本文提出一种基于三维重建的无人机影像全自动拼接方法, 利用从运动恢复结构 (Structure From Motion, SFM) +多视角立体 (Multi-view Stereo, MVS) 技术以密集点云的形式重建拍摄区域, 采用从物方空间坐标反求像方空间坐标的方法纠正区域内影像, 以实现影像间几何的对齐。</p>
                </div>
                <h3 id="41" name="41" class="anchor-tag">1 基于三维重建的高光谱影像拼接流程</h3>
                <div class="p1">
                    <p id="42">基于三维重建的无人机高光谱影像拼接流程如图1所示, 整个拼接流程按照处理影像波段数可分为3个部分。由于不同波段成像传感器的位置差异以及成像时间差异, 高光谱影像不同波段之间存在着配准误差, 因此在拼接之前需对影像做配准处理, 配准后同一幅影像各波段数据可用一个统一的姿态参数来描述。由于后续的SFM+MVS等步骤的主要目的是计算影像的姿态参数, 以及获取拍摄区域内含空间信息的密集点云, 这些几何信息的获取通过单波段数据处理即可获得。利用数字微分纠正对影像所有波段数据进行几何纠正, 最后镶嵌得到拍摄区域的拼接影像。</p>
                </div>
                <div class="area_img" id="43">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201904039_043.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 无人机高光谱影像拼接流程" src="Detail/GetImg?filename=images/JSJC201904039_043.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图1 无人机高光谱影像拼接流程</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201904039_043.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="44">为了全自动地进行波段配准, 本文采用的方案是以中间波段数据为基准数据, 以二次多项式描述其他波段与基准波段之间的几何关系, 对各波段数据提取特征点并与基准数据的特征点进行匹配, 根据匹配的特征点坐标拟合二次多项式系数, 从而对各波段数据进行几何配准。图2描述了高光谱数据中R、G、B三波段合成的真彩色图像在配准前后的对比, 配准前由于存在配准误差, 影像存在重影现象, 利用二次多项式配准后各波段影像重叠良好, 无重影现象, 同一高光谱数据中各波段可用一个统一的姿态参数描述它在空间中的位置姿态。波段选择的主要目的是在高光谱数据中选择一个波段数据用于特征点提取以及后续的点云生成。通常在纹理丰富的波段上提取的特征点数量较多, 但本文并未将波段数据信息量作为波段选择的评价标准, 这主要是因为对于大区域的影像序列而言, 某一幅影像的某个波段信息量丰富不能表示该区域所有影像在这个波段上信息量都丰富, 因此, 本文取高光谱影像中间波段数据作为后续特征点选择以及点云生成的输入数据。</p>
                </div>
                <div class="area_img" id="45">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201904039_045.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 波段配准前后对比" src="Detail/GetImg?filename=images/JSJC201904039_045.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图2 波段配准前后对比</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201904039_045.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h3 id="46" name="46" class="anchor-tag">2 特征点提取、匹配与光束法平差</h3>
                <div class="p1">
                    <p id="47">本节的主要目的是计算区域内所有影像的姿态参数以及匹配的特征点空间坐标, 为后面的点云生成提供输入。</p>
                </div>
                <div class="p1">
                    <p id="48">SIFT算子因其对图像旋转、尺度缩放、亮度变化甚至仿射变换保持不变的良好特性, 被广泛应用于影像匹配、目标识别等领域<citation id="111" type="reference"><link href="15" rel="bibliography" /><link href="17" rel="bibliography" /><sup>[<a class="sup">7</a>,<a class="sup">8</a>]</sup></citation>。对于低空无人机遥感而言, 在飞行过程中其航高与相机焦距的比值有微小变化, 这会导致不同成像时刻的影像分辨率不同, 同时由于外部气候的影响, 不同成像时刻影像旋偏角不同。而SIFT算子因其对尺度缩放与图像旋转保持不变的特点适合作为无人机影像的点特征描述算子。SIFT算子的缺点是算法复杂度高, 仅利用CPU进行计算很难达到实际应用中实时性的要求, 本文选择基于GPU加速的SIFT特征点提取算法<citation id="112" type="reference"><link href="19" rel="bibliography" /><link href="21" rel="bibliography" /><sup>[<a class="sup">9</a>,<a class="sup">10</a>]</sup></citation>, 发挥了GPU在图形类矩阵运算和并行化的数值计算方面的巨大优势, 提高了特征点提取速度。</p>
                </div>
                <div class="p1">
                    <p id="49">图3是采用了GPU加速的影像SIFT特征点提取结果。实验影像尺寸为1 010×648像素, 对区域内50幅相同尺寸的影像进行特征点提取共耗时约30 s, 平均每幅影像耗时约0.6 s, 每幅影像提取特征点数目约为2 200个。</p>
                </div>
                <div class="area_img" id="50">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201904039_050.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 SIFT特征点提取结果" src="Detail/GetImg?filename=images/JSJC201904039_050.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图3 SIFT特征点提取结果</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201904039_050.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="51">大区域的无人机影像序列, 由于缺少POS数据, 影像之间的相互位置关系未知, 导致每一幅影像都需要与其他所有影像进行影像匹配, 而且SIFT算子特征向量维数是128维, 匹配计算量大, 本文采用了GPU加速和多核CPU并行的方法加快影像匹配速度<citation id="113" type="reference"><link href="23" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>。另外, 为提高影像匹配准确度, 本文采用了基于单应性约束的匹配策略, 以保证匹配的特征点对在几何上满足同一个单应性矩阵变换<citation id="114" type="reference"><link href="25" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>。图4是采用单应性约束的特征点匹配结果, 由图4可知, 匹配特征点对数量较多, 且绝大多数符合同一个单应性矩阵变换。</p>
                </div>
                <div class="area_img" id="52">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201904039_052.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 单应性约束的特征点匹配结果" src="Detail/GetImg?filename=images/JSJC201904039_052.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图4 单应性约束的特征点匹配结果</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201904039_052.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="53">光束法平差的目的是利用已知像素坐标的匹配点集合, 通过最小化投影误差求解相机姿态参数和物方点三维坐标。这种优化问题通常被描述为非线性最小二乘问题, 以<b><i>x</i></b>表示待求解的参数向量, <mathml id="54"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>f</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">) </mo><mo>=</mo><mrow><mo>[</mo><mrow><mi>f</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">) </mo><mo>, </mo><mi>f</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">) </mo><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>f</mi><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">) </mo></mrow><mo>]</mo></mrow></mrow></math></mathml>表示匹配点的投影误差向量, 则该最小二乘问题可通过式 (1) 描述。</p>
                </div>
                <div class="p1">
                    <p id="55" class="code-formula">
                        <mathml id="55"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">x</mi><msup><mrow></mrow><mo>*</mo></msup><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mtext>a</mtext><mtext>r</mtext><mtext>g</mtext><mtext>m</mtext><mtext>i</mtext><mtext>n</mtext></mrow></mstyle><mi mathvariant="bold-italic">x</mi></munder><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><mo stretchy="false">∥</mo></mstyle><mi>f</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">x</mi><mo stretchy="false">) </mo><mo stretchy="false">∥</mo></mrow><msup><mrow></mrow><mn>2</mn></msup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="56"><i>LM</i>算法是解决这类非线性最小二乘问题最常用的算法, 也常被用来解决光束法平差问题<citation id="115" type="reference"><link href="27" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>。在区域网光束法平差中, 法方程的构建以及求解涉及到大量的矩阵分解、求逆, 特别是对于大区域网的平差, 参与平差的影像和匹配的同名点数量多, 因此整个平差过程需要高性能计算能力的支撑。</p>
                </div>
                <div class="p1">
                    <p id="57">文献<citation id="116" type="reference">[<a class="sup">14</a>]</citation>利用预条件共轭梯度法以及不精确牛顿法求解法方程, 大幅地减少了平差所需的内存, 提高了平差的效率, 并在此基础上介绍一种多核光束法平差方法, 最大化地利用了多核<i>CPU</i>和<i>GPU</i>的计算能力, 在平差过程中没有存储法方程系数矩阵, 而是实时地计算法方程。</p>
                </div>
                <div class="p1">
                    <p id="58">本文在文献<citation id="117" type="reference">[<a class="sup">14</a>]</citation>的基础上, 将影像畸变系数作为另一类未知数引入到平差中, 以区域内影像匹配的同名点作为输入, 通过平差整体解求每幅影像的畸变系数、姿态参数以及每个同名点的三维坐标。</p>
                </div>
                <h3 id="59" name="59" class="anchor-tag">3 基于密集点云的影像微分纠正</h3>
                <h4 class="anchor-tag" id="60" name="60">3.1 算法原理</h4>
                <div class="p1">
                    <p id="61">影像微分纠正是摄影测量学上的概念, 指根据有关的参数与数字地面模型 (<i>Digital Terrain Model</i>, <i>DTM</i>) , 利用相应的构像方程式, 从原始非正射投影的数字影像获取正射影像, 这个过程是将影像化为很多微小的区域逐一进行纠正, 所以叫做影像微分纠正<citation id="118" type="reference"><link href="17" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>。在摄影测量学上, 相关参数一般是指影像内外方位元素, 构像方程式指共线方程。</p>
                </div>
                <div class="p1">
                    <p id="62"><i>DTM</i>一般需要经过较复杂的处理手段获取, 耗时耗力大。本文提出的基于密集点云的影像微分纠正方法采取了另外一种策略, 即利用<i>MVS</i>算法生成的点云数据拟合空间点高程对影像进行正射校正。</p>
                </div>
                <div class="p1">
                    <p id="63">图5是2幅具有重叠区域的影像在中心投影下的成像关系。坐标系O-XYZ是空间坐标系, o<sub>1</sub>和o<sub>2</sub>是成像时刻的摄影中心, 空间点P在空间坐标系下的三维坐标是 (X, Y, Z) , 点P在2幅影像上的像点分别是p<sub>1</sub>、p<sub>2</sub>, 像点坐标 (x, y) 可用像点在影像上的像素坐标表示。</p>
                </div>
                <div class="area_img" id="64">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201904039_064.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 中心投影成像过程" src="Detail/GetImg?filename=images/JSJC201904039_064.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图5 中心投影成像过程</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201904039_064.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="65">在中心投影成像模型下, 空间点坐标与对应的像点坐标满足式 (2) 、式 (3) 。</p>
                </div>
                <div class="p1">
                    <p id="66" class="code-formula">
                        <mathml id="66"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mrow><mo>[</mo><mrow><mtable><mtr><mtd><mi>h</mi><msub><mrow></mrow><mi>x</mi></msub></mtd></mtr><mtr><mtd><mi>h</mi><msub><mrow></mrow><mi>y</mi></msub></mtd></mtr><mtr><mtd><mi>h</mi><msub><mrow></mrow><mi>z</mi></msub></mtd></mtr></mtable></mrow><mo>]</mo></mrow><mo>=</mo><mi mathvariant="bold-italic">Κ</mi><mrow><mo> (</mo><mrow><mi mathvariant="bold-italic">R</mi><mrow><mo>[</mo><mrow><mtable><mtr><mtd><mi>X</mi></mtd></mtr><mtr><mtd><mi>Y</mi></mtd></mtr><mtr><mtd><mi>Ζ</mi></mtd></mtr></mtable></mrow><mo>]</mo></mrow><mo>+</mo><mi mathvariant="bold-italic">Τ</mi></mrow><mo>) </mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mrow><mo>[</mo><mrow><mtable><mtr><mtd><mi>x</mi></mtd></mtr><mtr><mtd><mi>y</mi></mtd></mtr></mtable></mrow><mo>]</mo></mrow><mo>=</mo><mrow><mo>[</mo><mrow><mtable><mtr><mtd><mfrac><mrow><mi>h</mi><msub><mrow></mrow><mi>x</mi></msub></mrow><mrow><mi>h</mi><msub><mrow></mrow><mi>z</mi></msub></mrow></mfrac></mtd></mtr><mtr><mtd><mfrac><mrow><mi>h</mi><msub><mrow></mrow><mi>y</mi></msub></mrow><mrow><mi>h</mi><msub><mrow></mrow><mi>z</mi></msub></mrow></mfrac></mtd></mtr></mtable></mrow><mo>]</mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="67">其中, <mathml id="68"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mrow><mo> (</mo><mrow><mtable><mtr><mtd><mi>h</mi><msub><mrow></mrow><mi>x</mi></msub></mtd><mtd><mi>h</mi><msub><mrow></mrow><mi>y</mi></msub></mtd><mtd><mi>h</mi><msub><mrow></mrow><mi>z</mi></msub></mtd></mtr></mtable></mrow><mo>) </mo></mrow></mrow><msup><mrow></mrow><mtext>Τ</mtext></msup></mrow></math></mathml>是点在像方空间的齐次坐标, 它与影像像素坐标之间关系满足式<mathml id="69"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo><mo>, </mo><mrow><mrow><mo> (</mo><mrow><mtable><mtr><mtd><mi>X</mi></mtd><mtd><mi>Y</mi></mtd><mtd><mi>Ζ</mi></mtd></mtr></mtable></mrow><mo>) </mo></mrow></mrow><msup><mrow></mrow><mtext>Τ</mtext></msup></mrow></math></mathml>是点在物方空间的三维坐标, <mathml id="70"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">Κ</mi><mo>=</mo><mrow><mo>[</mo><mrow><mtable><mtr><mtd><mi>f</mi></mtd><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd></mtr><mtr><mtd><mn>0</mn></mtd><mtd><mi>f</mi></mtd><mtd><mn>0</mn></mtd></mtr><mtr><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd><mtd><mn>1</mn></mtd></mtr></mtable></mrow><mo>]</mo></mrow><mo>, </mo><mtext> </mtext><mi>f</mi></mrow></math></mathml>是相机焦距, <b><i>R</i></b>是由影像姿态角度计算出的旋转矩阵, <b><i>T</i></b>是像方坐标系和物方空间坐标系之间的偏移向量。<i>f</i>、<b><i>R</i></b>和<b><i>T</i></b>可由光束法平差计算而得。</p>
                </div>
                <div class="p1">
                    <p id="72">在密集点云已知情况下, 任一空间点<i>P</i><sub><i>i</i></sub>的高程<i>Z</i><sub><i>i</i></sub>值可由其邻点内插得到, 用式 (4) 表示。</p>
                </div>
                <div class="p1">
                    <p id="73"><i>Z</i><sub><i>i</i></sub>=<i>f</i> (<i>N</i> (<i>X</i><sub><i>i</i></sub>, <i>Y</i><sub><i>i</i></sub>) )      (4) </p>
                </div>
                <div class="p1">
                    <p id="74">其中, <i>N</i> (<i>X</i><sub><i>i</i></sub>, <i>Y</i><sub><i>i</i></sub>) 表示点<i>P</i><sub><i>i</i></sub>在<i>X</i>、<i>Y</i>维度上计算得到的邻点集合, 本文以近似最近邻算法作为邻点搜索算法<citation id="119" type="reference"><link href="31" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>, 函数<i>f</i>表示内插函数。</p>
                </div>
                <div class="p1">
                    <p id="75">根据影像姿态参数和点云数据可以计算得到每幅影像的分辨率<i>f</i><sub><i>i</i></sub>和在<i>X</i>、<i>Y</i>维度上对应的空间覆盖范围[<i>X</i><sub><i>i</i>-min</sub>, <i>X</i><sub><i>i</i>-max</sub>]、[<i>Y</i><sub><i>i</i>-min</sub>, <i>Y</i><sub><i>i</i>-max</sub>], 由于无人机拍摄时飞行高度变化幅度不大, 为便于后续处理, 可根据所有影像的分辨率计算出一个中间分辨率<i>f</i>作为正射影像分辨率。</p>
                </div>
                <div class="p1">
                    <p id="76">根据影像的空间覆盖范围以及分辨率, 可计算对应正射影像的任意像元 (<i>i</i>, <i>j</i>) 在<i>X</i>、<i>Y</i>方向上的坐标, 如式 (5) 所示。</p>
                </div>
                <div class="p1">
                    <p id="77" class="code-formula">
                        <mathml id="77"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>{</mo><mrow><mtable><mtr><mtd columnalign="left"><mi>X</mi><mo stretchy="false"> (</mo><mi>i</mi><mo>, </mo><mi>j</mi><mo stretchy="false">) </mo><mo>=</mo><mi>X</mi><msub><mrow></mrow><mrow><mi>min</mi></mrow></msub><mo>+</mo><mi>j</mi><mo>×</mo><mi>f</mi></mtd></mtr><mtr><mtd columnalign="left"><mi>Y</mi><mo stretchy="false"> (</mo><mi>i</mi><mo>, </mo><mi>j</mi><mo stretchy="false">) </mo><mo>=</mo><mi>Y</mi><msub><mrow></mrow><mrow><mi>min</mi></mrow></msub><mo>+</mo><mi>i</mi><mo>×</mo><mi>f</mi></mtd></mtr></mtable></mrow></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>5</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="78">进而利用式 (2) ～式 (4) 可计算该点所对应像点坐标 (<i>x</i>, <i>y</i>) , 在原影像上根据像点坐标内插得到该点灰度值, 最终生成带有空间参考信息的正射影像。</p>
                </div>
                <div class="p1">
                    <p id="79">式 (4) 内插高程值的过程中有一个搜寻邻点的步骤, 邻点搜索算法复杂度高, 若逐点采用反解公式式 (2) 求解像点坐标, 整个正射校正过程将会耗时很长。因此, 以“面元素”替代“点元素”作为校正单元可以提高正射校正速度。根据式 (2) ～式 (5) 计算校正单元 (通常是正方形) 4个“角点”的像点坐标 (<i>x</i><sub>1</sub>, <i>y</i><sub>1</sub>) 、 (<i>x</i><sub>2</sub> , <i>y</i><sub>2</sub>) 、 (<i>x</i><sub>3</sub>, <i>y</i><sub>3</sub>) 和 (<i>x</i><sub>4</sub>, <i>y</i><sub>4</sub>) , 校正单元内的坐标 (<i>x</i><sub><i>ij</i></sub>, <i>y</i><sub><i>ij</i></sub>) 利用双线性内插求得, 校正单元内任意一个像元 (<i>i</i>, <i>j</i>) 所对应的像点坐标由式 (6) 计算所得。</p>
                </div>
                <div class="p1">
                    <p id="80" class="code-formula">
                        <mathml id="80"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>{</mo><mrow><mtable><mtr><mtd columnalign="left"><mi>x</mi><mo stretchy="false"> (</mo><mi>i</mi><mo>, </mo><mtext> </mtext><mi>j</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mn>1</mn><mrow><mi>n</mi><msup><mrow></mrow><mn>2</mn></msup></mrow></mfrac><mo stretchy="false">[</mo><mo stretchy="false"> (</mo><mi>n</mi><mo>-</mo><mi>i</mi><mo stretchy="false">) </mo><mo stretchy="false"> (</mo><mi>n</mi><mo>-</mo><mi>j</mi><mo stretchy="false">) </mo><mi>x</mi><msub><mrow></mrow><mn>1</mn></msub><mo>+</mo><mi>i</mi><mo stretchy="false"> (</mo><mi>n</mi><mo>-</mo><mi>j</mi><mo stretchy="false">) </mo><mi>x</mi><msub><mrow></mrow><mn>2</mn></msub><mo>+</mo></mtd></mtr><mtr><mtd columnalign="left"><mtext> </mtext><mtext> </mtext><mtext> </mtext><mspace width="0.25em" /><mo stretchy="false"> (</mo><mi>n</mi><mo>-</mo><mi>i</mi><mo stretchy="false">) </mo><mi>j</mi><mi>x</mi><msub><mrow></mrow><mn>4</mn></msub><mo>+</mo><mi>i</mi><mi>j</mi><mi>x</mi><msub><mrow></mrow><mn>3</mn></msub><mo stretchy="false">]</mo></mtd></mtr><mtr><mtd columnalign="left"><mi>y</mi><mo stretchy="false"> (</mo><mi>i</mi><mo>, </mo><mtext> </mtext><mi>j</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mn>1</mn><mrow><mi>n</mi><msup><mrow></mrow><mn>2</mn></msup></mrow></mfrac><mo stretchy="false">[</mo><mo stretchy="false"> (</mo><mi>n</mi><mo>-</mo><mi>i</mi><mo stretchy="false">) </mo><mo stretchy="false"> (</mo><mi>n</mi><mo>-</mo><mi>j</mi><mo stretchy="false">) </mo><mi>y</mi><msub><mrow></mrow><mn>1</mn></msub><mo>+</mo><mi>i</mi><mo stretchy="false"> (</mo><mi>n</mi><mo>-</mo><mi>j</mi><mo stretchy="false">) </mo><mi>y</mi><msub><mrow></mrow><mn>2</mn></msub><mo>+</mo></mtd></mtr><mtr><mtd columnalign="left"><mtext> </mtext><mtext> </mtext><mtext> </mtext><mspace width="0.25em" /><mo stretchy="false"> (</mo><mi>n</mi><mo>-</mo><mi>i</mi><mo stretchy="false">) </mo><mi>j</mi><mi>y</mi><msub><mrow></mrow><mn>4</mn></msub><mo>+</mo><mi>i</mi><mi>j</mi><mi>y</mi><msub><mrow></mrow><mn>3</mn></msub><mo stretchy="false">]</mo></mtd></mtr></mtable></mrow></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>6</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="81">其中, <i>n</i>是正方形宽度。</p>
                </div>
                <h4 class="anchor-tag" id="82" name="82">3.2 多视角立体三维重建与点云处理</h4>
                <div class="p1">
                    <p id="83">在利用上述公式获取正射影像前, 需先获取拍摄区域的密集点云数据。另外, 由于在光束法平差中未引入外部绝对空间信息, 最终所计算得到的影像姿态参数存在全局性的歧义, 即光束法平差所计算得到的影像姿态参数是相对的, 因此正射校正前还需对点云以及影像姿态参数进行旋转, 使点云最优平面与空间坐标系X-Y平面平行, 以保证最后的拼接影像是“平铺”的。</p>
                </div>
                <div class="p1">
                    <p id="84">利用聚簇多视图立体 (<i>CMVS</i>) 算法对图像集聚簇分类可以优化<i>SFM</i>输入, 减少后续密集匹配时间和空间代价, 再由基于面片的多视图立体 (<i>PMVS</i>) 算法通过初始特征匹配、膨胀和过滤生成密集的点云<citation id="120" type="reference"><link href="33" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="85"><i>CMVS</i>算法首先对<i>SFM</i>同名点进行过滤以减小输入点集规模, 根据一定的约束条件剔除部分影像, 迭代地对图像进行聚簇分类, 将影像加入簇直至最终结果满足覆盖范围约束和图像大小约束。<i>PMVS</i>算法首先在所有影像上提取<i>Harris</i>和<i>DoG</i>特征点, 然后进行极几何约束的特征点匹配, 利用匹配点三角化重建空间面片;根据相邻条件对种子面片逐步扩散重建周围的面片;对面片过滤剔除掉灰度一致性和几何一致性较弱的面片。</p>
                </div>
                <div class="p1">
                    <p id="86">图6是以<i>SFM</i>同名点和影像姿态参数作为输入利用<i>CMVS</i>\\<i>PMVS</i>算法对拍摄区域重建的点云数据, 该算法生成的点云密集, 精度较高。</p>
                </div>
                <div class="area_img" id="87">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201904039_087.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 基于CMVS\\PMVS算法点云的生成" src="Detail/GetImg?filename=images/JSJC201904039_087.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图6 基于CMVS\\PMVS算法点云的生成</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201904039_087.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="88">在点云获取过程中, 由于特征点匹配不准等原因导致生成的点云不可避免地出现离群点。而点云精度直接影响后续影像正射校正的精度, 因此需要对点云进行滤波去噪处理。本文采用基于统计信息的点云滤波方法, 对每个点计算该点到其邻点距离的均值, 并假定该均值服从高斯分布。若该值在某一区间 (由全局距离均值和标准差确定) 外, 则将该点视为离群点。</p>
                </div>
                <div class="p1">
                    <p id="89">利用随机抽样一致 (RANSAC) 算法可以较好地拟合点云平面, 根据拟合出的平面参数, 计算拟合平面与空间坐标系<i>X</i>-<i>Y</i>平面之间的旋转矩阵, 对点云数据以及影像姿态参数进行旋转。</p>
                </div>
                <h4 class="anchor-tag" id="90" name="90">3.3 影像拼接</h4>
                <div class="p1">
                    <p id="91">利用密集匹配得到的点云数据, 根据式 (2) ～式 (6) 对高光谱影像所有波段采用基于面元的微分纠正后得到拍摄区域的高光谱正射影像序列, 由于正射影像像素带有空间信息, 因此只需根据像素空间信息对区域内影像进行拼接得到拍摄区域的拼接影像。</p>
                </div>
                <h3 id="92" name="92" class="anchor-tag">4 实验结果与分析</h3>
                <div class="p1">
                    <p id="93">基于本文方法编写相应的C++程序对某地区无人机影像进行全自动拼接实验。实验数据是50幅1 010像素×648像素的乱序无人机高光谱影像, 每幅影像有50个波段, 光谱范围约为500 nm～900 nm, 光谱分辨率约为10 nm, 波段数据类型是浮点数, 图7是拍摄区域内待拼接影像序列的单波段显示。实验平台采用8核X64 CPU和Geforce 8600 GT GPU的个人电脑。整个拼接实验共耗时约20 min, 表1为拼接实验中各个步骤的耗时情况。</p>
                </div>
                <div class="area_img" id="94">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201904039_094.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 无人机影像序列 (单波段显示)" src="Detail/GetImg?filename=images/JSJC201904039_094.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图7 无人机影像序列 (单波段显示</b>)   <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201904039_094.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="95">
                    <p class="img_tit"><b>表1 拼接实验各步骤耗时情况</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note">s</p>
                    <table id="95" border="1"><tr><td><br />实验步骤</td><td>耗时</td></tr><tr><td><br />波段灰度拉伸</td><td>23</td></tr><tr><td><br />特征点提取</td><td>30</td></tr><tr><td><br />影像匹配</td><td>662</td></tr><tr><td><br />光束法平差</td><td>43</td></tr><tr><td><br />密集点云重建</td><td>87</td></tr><tr><td><br />点云滤波与旋转</td><td>29</td></tr><tr><td><br />微分纠正</td><td>123</td></tr><tr><td><br />镶嵌</td><td>187</td></tr><tr><td><br />总计</td><td>1 184</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="96">由表1可知, 整个拼接过程中耗时最长的是影像匹配, 主要原因是输入影像的无序性导致了区域内任意2幅影像都要进行匹配以防止发生漏匹配的情况。</p>
                </div>
                <div class="p1">
                    <p id="97">图8～图10分别给出本文提出的基于三维重建的影像自动拼接结果、基于DEM正射校正的影像拼接结果和基于多项式几何校正的影像拼接结果, 拼接结果以高光谱拼接影像中三波段真彩色表示。</p>
                </div>
                <div class="area_img" id="98">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201904039_098.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 基于三维重建的影像拼接结果" src="Detail/GetImg?filename=images/JSJC201904039_098.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图8 基于三维重建的影像拼接结果</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201904039_098.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="99">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201904039_099.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图9 基于DEM正射校正的影像拼接结果" src="Detail/GetImg?filename=images/JSJC201904039_099.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图9 基于DEM正射校正的影像拼接结果</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201904039_099.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="100">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201904039_100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图10 基于多项式几何校正的影像拼接结果" src="Detail/GetImg?filename=images/JSJC201904039_100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图10 基于多项式几何校正的影像拼接结果</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201904039_100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="101">采用本文方法拼接耗时约为20 min, 采用基于DEM正射校正的影像拼接耗时约为3 d, 采用基于多项式几何校正的影像拼接耗时约为5 h。本文以相邻影像间同名点的残差评价拼接精度, 在拍摄区域中挑选若干幅有重叠的校正后影像, 提取一定数量的同名点, 计算这些同名点在各自影像上的空间坐标, 统计坐标差异, 以此对拼接精度进行量化评价, 对这3种拼接方法统计的拼接结果如表2所示。</p>
                </div>
                <div class="area_img" id="102">
                    <p class="img_tit"><b>表2 相邻影像对匹配点残差</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="102" border="1"><tr><td colspan="2"><br />影像编号</td><td colspan="3"><br />匹配点残差/像素</td></tr><tr><td><br />左影像</td><td>右影像</td><td>本文方法</td><td>基于DEM<br />正射校正方法</td><td>基于多项式<br />几何校正方法</td></tr><tr><td>0</td><td>1</td><td>2.24</td><td>2.01</td><td>3.02</td></tr><tr><td><br />1</td><td>4</td><td>2.81</td><td>1.96</td><td>2.98</td></tr><tr><td><br />5</td><td>7</td><td>2.56</td><td>2.11</td><td>4.12</td></tr><tr><td><br />10</td><td>12</td><td>2.57</td><td>1.76</td><td>3.20</td></tr><tr><td><br />14</td><td>16</td><td>2.18</td><td>1.67</td><td>3.32</td></tr><tr><td><br />20</td><td>21</td><td>2.23</td><td>2.32</td><td>2.76</td></tr><tr><td><br />23</td><td>25</td><td>3.01</td><td>2.64</td><td>3.67</td></tr><tr><td><br />30</td><td>33</td><td>2.34</td><td>2.12</td><td>2.67</td></tr><tr><td><br />38</td><td>40</td><td>2.63</td><td>2.01</td><td>2.87</td></tr><tr><td><br />45</td><td>47</td><td>2.39</td><td>1.54</td><td>3.76</td></tr><tr><td colspan="2">中误差</td><td>2.51</td><td>2.04</td><td>3.21</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="103">由图8～图10可见, 由这3种拼接方法得到的拼接结果视觉效果相差不大, 拼接影像均清晰无缝, 几何对齐良好。</p>
                </div>
                <div class="p1">
                    <p id="104">由表2可知, 采用本文提出的基于三维重建影像拼接的匹配点中误差为2.51个像素, 略低于基于DEM正射校正影像拼接的2.04个像素, 高于基于多项式几何校正的影像拼接的3.21个像素。但基于DEM正射校正的影像拼接依赖拍摄区域的高精度控制点等辅助数据, 基于多项式几何校正的影像拼接依赖拍摄区域的正射影像或其他地理图件, 本文的拼接方法不需要外部辅助数据。另外, 本文所提出的拼接方法与另外2种方法相比拼接耗时大大减少, 整个拼接过程无需人工干预, 大幅度地提高了拼接效率。</p>
                </div>
                <h3 id="105" name="105" class="anchor-tag">5 结束语</h3>
                <div class="p1">
                    <p id="106">本文提出一种基于三维重建的无人机影像全自动拼接方法。该方法利用SFM+MVS技术以密集点云的形式重建拍摄区域, 不需要其他辅助数据, 且拼接过程中无需人工干预。实验结果证明, 无人机影像全自动拼接方法能在短时间内获得精度较高的拼接影像。由于本文旨在研究如何快速获取几何一致的拼接影像, 在某些算法选择上更注重速度而忽略了精度。如对于点云滤波算法, 本文选择了实现简单、算法复杂度低的基于统计信息的点云滤波算法, 该算法能在较短时间内过滤显著的离群点, 但对于某些边缘点会误当作离群点过滤掉, 从而影响拼接精度。另外由于拼接数据是高光谱影像, 本文没有考虑拼接中的辐射一致性问题。以上不足将在后续研究过程中进行改进。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=WHCH201405001&amp;v=MTQ5MTJPM3pxcUJ0R0ZyQ1VSTE9lWmVSb0Z5L2dVcnJKTWlYSVpyRzRIOVhNcW85RlpZUUtESDg0dlI0VDZqNTQ=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> 李德仁, 李明.无人机遥感系统的研究进展与应用前景[J].武汉大学学报 (信息科学版) , 2014, 39 (5) :505-513, 540.
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=WHCH200903008&amp;v=MDk1MjhaZVJvRnkvZ1VyckpNaVhJWnJHNEh0ak1ySTlGYklRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2U=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> 张永军.无人驾驶飞艇低空遥感影像的几何处理[J].武汉大学学报 (信息科学版) , 2009, 34 (3) :284-288.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=YGXX201303014&amp;v=MzE2MzlLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm9GeS9nVXJySlBDclRkckc0SDlMTXJJOUVZSVE=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> 韩文超, 周利剑, 贾韶辉, 等.基于POS系统的无人机遥感图像融合方法的研究与实现[J].遥感信息, 2013, 28 (3) :80-84, 90.
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=CHXB200803013&amp;v=MzE4MzA1NE8zenFxQnRHRnJDVVJMT2VaZVJvRnkvZ1VyckpKaVhUYkxHNEh0bk1ySTlFWjRRS0RIODR2UjRUNmo=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b> 袁修孝.POS辅助光束法区域网平差[J].测绘学报, 2008, 37 (3) :342-348.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CBBD&amp;filename=9787307107618000&amp;v=MjA4NTNDdmpVN2pOSVZzVlhGcXpHYkM0R2RETXFJbEViT3NQREJNOHp4VVNtRGQ5U0g3bjN4RTlmYnZuS3JpZlp1OXVG&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b> 孙家抦.遥感原理与应用[M].武汉:武汉大学出版社, 2013.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000004760&amp;v=MjUzOTdIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVVMbklKMXNWYnhNPU5pZklZN0s3SHRqTnI0OUZaT3NMQzNvNW9CTVQ2VDRQUQ==&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b> AGARWAL S, FURUKAWA Y, SNAVELY N, et al.Building Rome in a day[J].Communications of the ACM, 2011, 54 (10) :105-112.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002830901&amp;v=Mjk5NjhiZXNPWTNrNXpCZGg0ajk5U1hxUnJ4b3hjTUg3UjdxZWJ1ZHRGQzdsVkxySkpWOD1OajdCYXJPNEh0SE9wNHhG&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b> LOWE D G.Distinctive image features from scale-invariant keypoints[J].International Journal of Computer Vision, 2004, 60 (2) :91-110.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CBBD&amp;filename=9787307069558001&amp;v=MTUwNjV4VVNtRGQ5U0g3bjN4RTlmYnZuS3JpZlp1OXVGQ3ZqVTdqTklWc1ZYRnF6R2JDNEdkSEtwb3BBYk9zUERSTTh6&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b> 张剑清, 潘励, 王树根.摄影测量学[M].2版.武汉:武汉大学出版社, 2009.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=SiftGPU:a GPU implementation of scale invariant feature transform">

                                <b>[9]</b> WU C.SiftGPU:a GPU implementation of scale invariant feature transform[EB/OL].[2018-02-12].http://cs.unc.edu/～ccwu/siftgpu.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=GPU point list generation through histogram pyramids">

                                <b>[10]</b> ZIEGLER G, TEVS A, THEOBALT C, et al.GPU point list generation through histogram pyramids[M].[S.1.]:Max Planck Society, 2006.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_11" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DBCH201412005&amp;v=MTc3NThySklTL0lackc0SDlYTnJZOUZZWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm9GeS9nVXI=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[11]</b> 柯涛, 田一涵, 段延松.基于GPU的 SIFT 特征点快速匹配[J].测绘与空间地理信息, 2014, 37 (12) :14-16.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_12" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201215044&amp;v=MjY3MjBESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSb0Z5L2dVcnJKTHo3QmJiRzRIOVBOcW85QllJUUs=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[12]</b> 胡庆武, 艾明耀, 殷万玲, 等.大旋角无人机影像全自动拼接方法研究[J].计算机工程, 2012, 38 (15) :152-155.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Bundle Adjustment-A Modern Synthesis">

                                <b>[13]</b> TRIGGS B, MCLAUCHLAN P F, HARTLEY R I, et al.Bundle adjustment——a modern synthesis[C]//Proceedings of International Workshop on Vision Algorithms.Berlin, Germany:Springer, 1999:298-372.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multicore bundle adjustment">

                                <b>[14]</b> WU C, AGARWAL S, CURLESS B, et al.Multicore bundle adjustment[C]//Proceedings of 2011 IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2011:3057-3064.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_15" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM14010900003649&amp;v=Mjk5NzZVTG5JSjFzVmJ4TT1OaWZJWTdLOEh0RE1wbzlGWk9zTUNuZ3dvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbQ==&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[15]</b> ARA S, MOUNT D M, NETANYAHU N S, et al.An optimal algorithm for approximate nearest neighbor searching fixed dimensions[J].Journal of the ACM, 1998, 45 (6) :891-923.
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Accurate, Dense, and Robust Multiview Stereopsis">

                                <b>[16]</b> FURUKAWA Y, PONCE J.Accurate, dense, and robust multiview stereopsis[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2010, 32 (8) :1362-1376.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJC201904039" />
        <input id="dpi" type="hidden" value="600" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201904039&amp;v=MDQ5NDM0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSb0Z5L2dVcjNBTHo3QmJiRzRIOWpNcTQ5R2JZUUtESDg=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
