<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637128044458780000%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJC201909035%26RESULT%3d1%26SIGN%3dxvvgWxu1P8lQCSGrENoMJAp03oc%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201909035&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201909035&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201909035&amp;v=MDQ4MjlqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSckZ5N2xWcjdPTHo3QmJiRzRIOWpNcG85R1lZUUtESDg0dlI0VDY=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2tGeFdRSFd2NXQ4L1RpTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#49" data-title="0 概述 ">0 概述</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#52" data-title="1 相关知识 ">1 相关知识</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#53" data-title="1.1 3D卷积">1.1 3D卷积</a></li>
                                                <li><a href="#56" data-title="1.2 全卷积网络">1.2 全卷积网络</a></li>
                                                <li><a href="#60" data-title="1.3 孪生RPN">1.3 孪生RPN</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#64" data-title="2 锚框掩码孪生RPN ">2 锚框掩码孪生RPN</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#66" data-title="2.1 锚框掩码">2.1 锚框掩码</a></li>
                                                <li><a href="#69" data-title="2.2 锚框掩码网络">2.2 锚框掩码网络</a></li>
                                                <li><a href="#74" data-title="2.3 锚框掩码孪生RPN">2.3 锚框掩码孪生RPN</a></li>
                                                <li><a href="#77" data-title="2.4 损失函数">2.4 损失函数</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#84" data-title="3 实验与结果分析 ">3 实验与结果分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#85" data-title="3.1 实验环境及训练集">3.1 实验环境及训练集</a></li>
                                                <li><a href="#87" data-title="3.2 对比模型">3.2 对比模型</a></li>
                                                <li><a href="#89" data-title="3.3 数据集VOT2016实验结果">3.3 数据集VOT2016实验结果</a></li>
                                                <li><a href="#95" data-title="3.4 数据集OTB100实验结果">3.4 数据集OTB100实验结果</a></li>
                                                <li><a href="#99" data-title="3.5 摄像头数据检验">3.5 摄像头数据检验</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#102" data-title="4 结束语 ">4 结束语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#55" data-title="&lt;b&gt;图1 3D卷积&lt;/b&gt;"><b>图1 3D卷积</b></a></li>
                                                <li><a href="#58" data-title="&lt;b&gt;图2 FCN结构&lt;/b&gt;"><b>图2 FCN结构</b></a></li>
                                                <li><a href="#62" data-title="&lt;b&gt;图3 孪生RPN结构&lt;/b&gt;"><b>图3 孪生RPN结构</b></a></li>
                                                <li><a href="#68" data-title="&lt;b&gt;图4 锚框掩码原理&lt;/b&gt;"><b>图4 锚框掩码原理</b></a></li>
                                                <li><a href="#71" data-title="&lt;b&gt;图5 锚框掩码网络结构&lt;/b&gt;"><b>图5 锚框掩码网络结构</b></a></li>
                                                <li><a href="#73" data-title="&lt;b&gt;图6 VOT2016锚框掩码和跟踪效果&lt;/b&gt;"><b>图6 VOT2016锚框掩码和跟踪效果</b></a></li>
                                                <li><a href="#76" data-title="&lt;b&gt;图7 锚框掩码孪生RPN结构&lt;/b&gt;"><b>图7 锚框掩码孪生RPN结构</b></a></li>
                                                <li><a href="#93" data-title="&lt;b&gt;图8 平均覆盖率及速度对比&lt;/b&gt;"><b>图8 平均覆盖率及速度对比</b></a></li>
                                                <li><a href="#94" data-title="&lt;b&gt;表1 10种模型对比结果&lt;/b&gt;"><b>表1 10种模型对比结果</b></a></li>
                                                <li><a href="#98" data-title="&lt;b&gt;图9 OTB数据集下准确率及成功率对比&lt;/b&gt;"><b>图9 OTB数据集下准确率及成功率对比</b></a></li>
                                                <li><a href="#101" data-title="&lt;b&gt;图10 实验室视频结果&lt;/b&gt;"><b>图10 实验室视频结果</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="130">


                                    <a id="bibliography_1" >
                                        <b>[1]</b>
                                     BERTINETTO L,VALMADRE J,GOLODETZ S,et al.Staple:complementary learners for real-time tracking[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Press,2016:1401-1409.</a>
                                </li>
                                <li id="132">


                                    <a id="bibliography_2" title=" NAM H,HAN B.Learning multi-domain convolutional neural networks for visual tracking[C]//Proceedings of the IEEE Conferenceon Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Press,2016:4293-4302." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning multi-domain convolutional neural networks for visual tracking">
                                        <b>[2]</b>
                                         NAM H,HAN B.Learning multi-domain convolutional neural networks for visual tracking[C]//Proceedings of the IEEE Conferenceon Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Press,2016:4293-4302.
                                    </a>
                                </li>
                                <li id="134">


                                    <a id="bibliography_3" title=" 江维创,张俊为,桂江生.基于改进核相关滤波器的目标跟踪算法[J].计算机工程,2018,44(11):228-233." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201811037&amp;v=MTcyNDM0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSckZ5N2xWcjdPTHo3QmJiRzRIOW5Ocm85R1k0UUtESDg=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2tGeFdRSFd2NXQ4L1RpTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         江维创,张俊为,桂江生.基于改进核相关滤波器的目标跟踪算法[J].计算机工程,2018,44(11):228-233.
                                    </a>
                                </li>
                                <li id="136">


                                    <a id="bibliography_4" title=" 李大湘,吴玲风,李娜,等.改进的SAMF目标跟踪算法[J].计算机工程,2019,45(2):258-264." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201902043&amp;v=MDQ0MDM0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSckZ5N2xWcjdPTHo3QmJiRzRIOWpNclk5Qlo0UUtESDg=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2tGeFdRSFd2NXQ4L1RpTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                         李大湘,吴玲风,李娜,等.改进的SAMF目标跟踪算法[J].计算机工程,2019,45(2):258-264.
                                    </a>
                                </li>
                                <li id="138">


                                    <a id="bibliography_5" title=" DANELLJAN M,ROBINSON A,KHAN F S,et al.Beyond correlation filters:learning continuous convolution operators for visual tracking[C]//Proceedings of European Conference on Computer Vision.Berlin,Germany:Springer,2016:472-488." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Beyond correlation filters:Learning continuous convolution operators for visual tracking">
                                        <b>[5]</b>
                                         DANELLJAN M,ROBINSON A,KHAN F S,et al.Beyond correlation filters:learning continuous convolution operators for visual tracking[C]//Proceedings of European Conference on Computer Vision.Berlin,Germany:Springer,2016:472-488.
                                    </a>
                                </li>
                                <li id="140">


                                    <a id="bibliography_6" title=" KANG Kai,LI Hongsheng,YAN Junjie,et al.T-CNN:tubelets with convolutional neural networks for object detection from videos[J].IEEE Transactions on Circuits and Systems for Video Technology,2017,28(10):2896-2907." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=T-cnn:Tubelets with convolutional neural networks for object detection from videos">
                                        <b>[6]</b>
                                         KANG Kai,LI Hongsheng,YAN Junjie,et al.T-CNN:tubelets with convolutional neural networks for object detection from videos[J].IEEE Transactions on Circuits and Systems for Video Technology,2017,28(10):2896-2907.
                                    </a>
                                </li>
                                <li id="142">


                                    <a id="bibliography_7" title=" ZHU Gao,PORIKLI F,LI Hongdong.Robust visual tracking with deep convolutional neural network based object proposals on PETS[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops.Washington D.C.,USA:IEEE Press,2016:26-33." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Robust visual tracking with deep convolutional neural network based object proposals on pets">
                                        <b>[7]</b>
                                         ZHU Gao,PORIKLI F,LI Hongdong.Robust visual tracking with deep convolutional neural network based object proposals on PETS[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops.Washington D.C.,USA:IEEE Press,2016:26-33.
                                    </a>
                                </li>
                                <li id="144">


                                    <a id="bibliography_8" >
                                        <b>[8]</b>
                                     DANELLJAN M,BHAT G,SHAHBAZ K F,et al.ECO:efficient convolution operators for tracking[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Press,2017:6638-6646.</a>
                                </li>
                                <li id="146">


                                    <a id="bibliography_9" title=" HENRIQUES J F,CASEIRO R,MARTINS P,et al.High-speed tracking with kernelized correlation filters[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2014,37(3):583-596." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=High-speed tracking with kernelized correlation filters">
                                        <b>[9]</b>
                                         HENRIQUES J F,CASEIRO R,MARTINS P,et al.High-speed tracking with kernelized correlation filters[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2014,37(3):583-596.
                                    </a>
                                </li>
                                <li id="148">


                                    <a id="bibliography_10" title=" 朱齐丹,韩瑜,蔡成涛.全景视觉非线性核相关滤波目标跟踪技术[J].哈尔滨工程大学学报,2018,39 (7):102-108." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=HEBG201807015&amp;v=Mjk2MDVSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUnJGeTdsVnI3T0xTakphYkc0SDluTXFJOUVZWVFLREg4NHY=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2tGeFdRSFd2NXQ4L1RpTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[10]</b>
                                         朱齐丹,韩瑜,蔡成涛.全景视觉非线性核相关滤波目标跟踪技术[J].哈尔滨工程大学学报,2018,39 (7):102-108.
                                    </a>
                                </li>
                                <li id="150">


                                    <a id="bibliography_11" title=" LI Bo,YAN Junjie,WU Wei,et al.High performance visual tracking with Siamese region proposal network[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Press,2018:8971-8980." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=High performance visual tracking with Siamese region proposal network">
                                        <b>[11]</b>
                                         LI Bo,YAN Junjie,WU Wei,et al.High performance visual tracking with Siamese region proposal network[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Press,2018:8971-8980.
                                    </a>
                                </li>
                                <li id="152">


                                    <a id="bibliography_12" title=" BERTINETTO L,VALMADRE J,HENRIQUES J F,et al.Fully-convolutional Siamese networks for object tracking[C]//Proceedings of European Conferenceon Computer Vision.Berlin,Germany:Springer,2016:850-865." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fully-convolutional siamese networks for object tracking">
                                        <b>[12]</b>
                                         BERTINETTO L,VALMADRE J,HENRIQUES J F,et al.Fully-convolutional Siamese networks for object tracking[C]//Proceedings of European Conferenceon Computer Vision.Berlin,Germany:Springer,2016:850-865.
                                    </a>
                                </li>
                                <li id="154">


                                    <a id="bibliography_13" title=" LONG J,SHELHAMER E,DARRELL T.Fully convolutional networks for semantic segmentation[C]//Proceedings of the IEEE Conferenceon Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Press,2015:3431-3440." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fully convolutional networks for semantic segmentation">
                                        <b>[13]</b>
                                         LONG J,SHELHAMER E,DARRELL T.Fully convolutional networks for semantic segmentation[C]//Proceedings of the IEEE Conferenceon Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Press,2015:3431-3440.
                                    </a>
                                </li>
                                <li id="156">


                                    <a id="bibliography_14" title=" JI Shuiwang,XU Wei,YANG Ming,et al.3D convolutional neural networks for human action recognition[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2012,35(1):221-231." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=3D Convolutional Neural Networks for Human Action Recognition">
                                        <b>[14]</b>
                                         JI Shuiwang,XU Wei,YANG Ming,et al.3D convolutional neural networks for human action recognition[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2012,35(1):221-231.
                                    </a>
                                </li>
                                <li id="158">


                                    <a id="bibliography_15" title=" TRAN D,BOURDEV L,FERGUS R,et al.Learning spatiotemporal features with 3D convolutional networks[C]//Proceedings of the IEEE International Conferenceon Computer Vision.Washington D.C.,USA:IEEE Press,2015:4489-4497." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning spatiotemporal features with 3Dconvolutional networks">
                                        <b>[15]</b>
                                         TRAN D,BOURDEV L,FERGUS R,et al.Learning spatiotemporal features with 3D convolutional networks[C]//Proceedings of the IEEE International Conferenceon Computer Vision.Washington D.C.,USA:IEEE Press,2015:4489-4497.
                                    </a>
                                </li>
                                <li id="160">


                                    <a id="bibliography_16" title=" DAI Jifeng,LI Yi,HE Kaiming,et al.R-FCN:object detection via region-based fully convolutional networks[C]//Proceedings of Advances in Neural Information Processing Systems.[S.l.]:Neural Information Processing Systems Foundation,Inc.,2016:379-387." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=R-FCN:Object Detection via Region-based Fully Convolutional Networks">
                                        <b>[16]</b>
                                         DAI Jifeng,LI Yi,HE Kaiming,et al.R-FCN:object detection via region-based fully convolutional networks[C]//Proceedings of Advances in Neural Information Processing Systems.[S.l.]:Neural Information Processing Systems Foundation,Inc.,2016:379-387.
                                    </a>
                                </li>
                                <li id="162">


                                    <a id="bibliography_17" title=" HE Kaiming,GKIOXARI G,DOLL&#193;R P,et al.Mask R-CNN[C]//Proceedings of the IEEE International Conferenceon Computer Vision.Washington D.C.,USA:IEEE Press,2017:2961-2969." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Mask R-CNN">
                                        <b>[17]</b>
                                         HE Kaiming,GKIOXARI G,DOLL&#193;R P,et al.Mask R-CNN[C]//Proceedings of the IEEE International Conferenceon Computer Vision.Washington D.C.,USA:IEEE Press,2017:2961-2969.
                                    </a>
                                </li>
                                <li id="164">


                                    <a id="bibliography_18" title=" KRAHENB&#220;HL P,KOLTUN V.Efficient inference in fully connected CRFs with Gaussian edge potentials[C]//Proceedings of Advances in Neural Information Processing Systems.[S.l.]:Neural Information Processing Systems Foundation,Inc.,2011:109-117." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials">
                                        <b>[18]</b>
                                         KRAHENB&#220;HL P,KOLTUN V.Efficient inference in fully connected CRFs with Gaussian edge potentials[C]//Proceedings of Advances in Neural Information Processing Systems.[S.l.]:Neural Information Processing Systems Foundation,Inc.,2011:109-117.
                                    </a>
                                </li>
                                <li id="166">


                                    <a id="bibliography_19" title=" ZHENG Shuai,JAYASUMANA S,ROMERA-PAREDES B,et al.Conditional random fields as recurrent neural networks[C]//Proceedings of the IEEE International Conference on Computer Vision.Washington D.C.,USA:IEEE Press,2015:1529-1537." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Conditional Random Fields as Recurrent Neural Networks">
                                        <b>[19]</b>
                                         ZHENG Shuai,JAYASUMANA S,ROMERA-PAREDES B,et al.Conditional random fields as recurrent neural networks[C]//Proceedings of the IEEE International Conference on Computer Vision.Washington D.C.,USA:IEEE Press,2015:1529-1537.
                                    </a>
                                </li>
                                <li id="168">


                                    <a id="bibliography_20" title=" REN Shaoqing,HE Kaiming,GIRSHICK R,et al.Faster R-CNN:towards real-time object detection with region proposal networks[C]//Proceedings of Advances in Neural Information Processing Systems.[S.l.]:Neural Information Processing Systems Foundation,Inc.,2015:91-99." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Faster R-CNN:Towards real-time object detection with region proposal networks">
                                        <b>[20]</b>
                                         REN Shaoqing,HE Kaiming,GIRSHICK R,et al.Faster R-CNN:towards real-time object detection with region proposal networks[C]//Proceedings of Advances in Neural Information Processing Systems.[S.l.]:Neural Information Processing Systems Foundation,Inc.,2015:91-99.
                                    </a>
                                </li>
                                <li id="170">


                                    <a id="bibliography_21" title=" ZHU Gao,PORIKLI F,LI Hongdong.Beyond local search:Tracking objects everywhere with instance-specific proposals[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Press,2016:943-951." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Beyond Local Search:Tracking Objects Everywhere with Instance-Specific Proposals">
                                        <b>[21]</b>
                                         ZHU Gao,PORIKLI F,LI Hongdong.Beyond local search:Tracking objects everywhere with instance-specific proposals[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Press,2016:943-951.
                                    </a>
                                </li>
                                <li id="172">


                                    <a id="bibliography_22" title=" ZAGORUYKO S,KOMODAKIS N.Learning to compare image patches via convolutional neural networks[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Press,2015:4353-4361." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning to compare image patches via convolutional neural networks">
                                        <b>[22]</b>
                                         ZAGORUYKO S,KOMODAKIS N.Learning to compare image patches via convolutional neural networks[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Press,2015:4353-4361.
                                    </a>
                                </li>
                                <li id="174">


                                    <a id="bibliography_23" title=" FAN Heng,LING Haibin.Parallel tracking and verifying:a framework for real-time and high accuracy visual tracking[C]//Proceedings of the IEEE International Conference on Computer Vision.Washington D.C.,USA:IEEE Press,2017:5486-5494." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Parallel tracking and verifying:a framework for real-time and high accuracy visual tracking">
                                        <b>[23]</b>
                                         FAN Heng,LING Haibin.Parallel tracking and verifying:a framework for real-time and high accuracy visual tracking[C]//Proceedings of the IEEE International Conference on Computer Vision.Washington D.C.,USA:IEEE Press,2017:5486-5494.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJC" target="_blank">计算机工程</a>
                2019,45(09),216-221 DOI:10.19678/j.issn.1000-3428.0053937            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>一种用于单目标跟踪的锚框掩码孪生RPN模型</b></span>
 <span class="shoufa"></span>                                     </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%8E%E6%98%8E%E6%9D%B0&amp;code=39017348&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2tGeFdRSFd2NXQ4L1RpTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">李明杰</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%86%AF%E6%9C%89%E5%89%8D&amp;code=20215009&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2tGeFdRSFd2NXQ4L1RpTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">冯有前</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%B0%B9%E5%BF%A0%E6%B5%B7&amp;code=21131914&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2tGeFdRSFd2NXQ4L1RpTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">尹忠海</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%91%A8%E8%AF%9A&amp;code=33141539&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2tGeFdRSFd2NXQ4L1RpTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">周诚</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%91%A3%E6%96%B9%E6%98%8A&amp;code=40753963&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2tGeFdRSFd2NXQ4L1RpTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">董方昊</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E7%A9%BA%E5%86%9B%E5%B7%A5%E7%A8%8B%E5%A4%A7%E5%AD%A6%E5%9F%BA%E7%A1%80%E9%83%A8&amp;code=0274788&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2tGeFdRSFd2NXQ4L1RpTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">空军工程大学基础部</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>针对孪生区域候选网络(RPN)易受干扰且目标丢失后无法跟踪的问题,引入锚框掩码网络机制,设计一种新型孪生RPN模型。设置多尺度模板图片,并将其与目标图片进行卷积操作,实现全图检测以避免目标丢失。通过对前三帧图片的IOU热度图进行学习,预测连续帧目标锚框掩码,简化计算并排除其他目标干扰。在VOT2016和OTB100数据集中的实验结果显示,该模型对VOT2016数据集检测帧率达到24.6 frame/s,预期平均覆盖率为0.344 5,对OTB100数据集的检测准确率和成功率分别为0.862和0.642。基于摄像头采集数据的目标丢失及干扰测试表明,该模型具有良好的抗干扰性与实时性。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%AD%AA%E7%94%9F%E5%8C%BA%E5%9F%9F%E5%80%99%E9%80%89%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2tGeFdRSFd2NXQ4L1RpTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">孪生区域候选网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%94%9A%E6%A1%86%E6%8E%A9%E7%A0%81&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2tGeFdRSFd2NXQ4L1RpTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">锚框掩码;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%94%9A%E6%A1%86%E6%8E%A9%E7%A0%81%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2tGeFdRSFd2NXQ4L1RpTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">锚框掩码网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%A4%9A%E5%B0%BA%E5%BA%A6%E5%8F%98%E6%8D%A2&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2tGeFdRSFd2NXQ4L1RpTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">多尺度变换;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2tGeFdRSFd2NXQ4L1RpTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">目标跟踪;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    李明杰(1995—),男,硕士研究生,主研方向为计算机视觉、目标检测;;
                                </span>
                                <span>
                                    冯有前,教授、博士生导师;;
                                </span>
                                <span>
                                    尹忠海,教授;;
                                </span>
                                <span>
                                    周诚,博士研究生;;
                                </span>
                                <span>
                                    董方昊,硕士研究生。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-02-19</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金(61472443);</span>
                    </p>
            </div>
                    <h1><b>An Anchor Mask Siamese RPN Model for Single Target Tracking</b></h1>
                    <h2>
                    <span>LI Mingjie</span>
                    <span>FENG Youqian</span>
                    <span>YIN Zhonghai</span>
                    <span>ZHOU Cheng</span>
                    <span>DONG Fanghao</span>
            </h2>
                    <h2>
                    <span>Department of Sciences,Air Force Engineering University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>To address the problem that the Siamese Region Proposal Network(RPN) is susceptible to interference and cannot be tracked after the target is lost,this paper introduces the anchor mask network mechanism to design a new Siamese RPN model.The model sets the multi-scale template images and convolves them with the target image to achieve full-image detection and avoid target loss.By learning the IOU hot maps of the first three frames,the target anchor mask is predicted in the continuous frames to simplify the calculation and exclude other target interference.The experimental results in the VOT2016 and OTB100 datasets show that the model has a detection rate of 24.6 frame/s and an expected average overlap of 0.344 5 for the VOT2016 dataset,and a precision of 0.862 and a success rate of 0.642 for the OTB100 dataset.The target loss and interference tests are carried out on the data collected by the camera.The results show that the model has good anti-interference and real-time performance.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Siamese%20Region%20Proposal%20Network%20(RPN)&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2tGeFdRSFd2NXQ4L1RpTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Siamese Region Proposal Network (RPN);</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=anchor%20mask&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2tGeFdRSFd2NXQ4L1RpTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">anchor mask;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=anchor%20mask%20network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2tGeFdRSFd2NXQ4L1RpTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">anchor mask network;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=multi-scale%20transformation&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2tGeFdRSFd2NXQ4L1RpTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">multi-scale transformation;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=target%20tracking&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2tGeFdRSFd2NXQ4L1RpTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">target tracking;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2019-02-19</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="49" name="49" class="anchor-tag">0 概述</h3>
                <div class="p1">
                    <p id="50">目标跟踪在计算机视觉领域是一项基础任务,对于目标行为分析及确定目标位置等其他任务具有重要的作用。近年来许多针对目标跟踪任务的算法被相继提出<citation id="185" type="reference"><link href="130" rel="bibliography" /><link href="132" rel="bibliography" /><link href="134" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>,<a class="sup">3</a>]</sup></citation>,尤其深度学习的广泛应用使得跟踪效果越来越好。在目标跟踪的主流方法中有一类方法为结合深度特征条件的检测跟踪网络。其中:C-COT<citation id="176" type="reference"><link href="136" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>作为一种基于判别式学习的特征点跟踪方法通过学习连续卷积算子实现目标跟踪;T-CNN<citation id="177" type="reference"><link href="138" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>网络通过拓展静态图像检测框架并融合具有时空特性的Tubelets实现目标跟踪;EBT<citation id="178" type="reference"><link href="140" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>网络通过类物体度量生成少量高质量的建议框,再以现有的跟踪检测方法进行评估实现目标跟踪;文献<citation id="179" type="reference">[<a class="sup">7</a>]</citation>提出一种图像重分块检测机制对SAMF算法进行改进,提高了遮挡目标的跟踪准确率。除此以外,基于相关滤波的跟踪网络也取得较好的效果。其中:ECO<citation id="180" type="reference"><link href="144" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>通过使用相关滤波算法实现特征降维,并利用高斯混合模型生成新的训练集,提高了跟踪精度与速度;KCF<citation id="181" type="reference"><link href="146" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>通过使用目标周围区域的循环矩阵采集正负样本,训练目标检测器提高检测速度,并融合多通道数据,高效地实现目标跟踪;基于非线性核相关滤波的全景视觉目标跟踪算法<citation id="182" type="reference"><link href="148" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>将岭回归与循环样本矩阵和经典的相关滤波进行联系,设计了适用于全景成像的自适应机制和基于极坐标表示的目标搜索机制,提高了跟踪精度。特别地,孪生区域候选网络(Region Proposal Network,RPN)<citation id="183" type="reference"><link href="150" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>在孪生全卷积网络(Fully Convolutional Network,FCN)<citation id="184" type="reference"><link href="152" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>的基础上引入RPN原理,以极快的速度达到了单目标跟踪的高准确率。</p>
                </div>
                <div class="p1">
                    <p id="51">在对图像中目标进行跟踪时,孪生RPN只提取单帧图像的特征,没有结合之前的检测结果,因此,在具有相似图像目标的条件下,可能会产生误识别的情况,并且其检测位置是在上一帧图片目标位置一定范围内进行截图操作,未对整张图片进行处理,若视频中的目标曾被遮挡或消失,会导致跟踪失败。针对上述问题,本文构建一种多尺度模板图片的孪生RPN模型,并结合视频分析中常用的3D卷积操作和FCN<citation id="186" type="reference"><link href="154" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>网络引入固定卷积操作范围的锚框掩码机制,使模型适用于视频图片。</p>
                </div>
                <h3 id="52" name="52" class="anchor-tag">1 相关知识</h3>
                <h4 class="anchor-tag" id="53" name="53">1.1 3D卷积</h4>
                <div class="p1">
                    <p id="54">文献<citation id="187" type="reference">[<a class="sup">14</a>]</citation>将3D卷积用于视频维度的分析,随后文献<citation id="188" type="reference">[<a class="sup">15</a>]</citation>提出适用于视频且帧率极快的C3D(Convolutional 3D)网络,此后基于C3D网络的行为识别方法相继被提出。本文提出的锚框掩码网络借助3D卷积学习了连续3帧图片的IOU热度图信息,与后续的类FCN预测层进行相连,估计下一帧图片的锚框掩码图像。3D卷积原理如图1所示。</p>
                </div>
                <div class="area_img" id="55">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201909035_055.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2tGeFdRSFd2NXQ4L1RpTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 3D卷积" src="Detail/GetImg?filename=images/JSJC201909035_055.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2tGeFdRSFd2NXQ4L1RpTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图1 3D卷积</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201909035_055.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2tGeFdRSFd2NXQ4L1RpTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="56" name="56">1.2 全卷积网络</h4>
                <div class="p1">
                    <p id="57">FCN是一种用于图像分割的经典网络,在其基础上诞生了许多衍生网络,如R-FCN<citation id="189" type="reference"><link href="160" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>、Mask R-CNN<citation id="190" type="reference"><link href="162" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>、全连接CRF<citation id="191" type="reference"><link href="164" rel="bibliography" /><link href="166" rel="bibliography" /><sup>[<a class="sup">18</a>,<a class="sup">19</a>]</sup></citation>等。FCN通过对图像进行特征提取、全连接及分类,最后反卷积至原图大小,使得网络针对原图每个像素点都可进行分类,最终实现图像分割,其结构如图2所示。</p>
                </div>
                <div class="area_img" id="58">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201909035_058.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2tGeFdRSFd2NXQ4L1RpTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 FCN结构" src="Detail/GetImg?filename=images/JSJC201909035_058.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2tGeFdRSFd2NXQ4L1RpTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图2 FCN结构</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201909035_058.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2tGeFdRSFd2NXQ4L1RpTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="59">本文借鉴FCN网络结构,在前段接入3D卷积学习IOU热度图时空信息的基础上,利用类FCN结构,全卷积操作预测一幅锚框掩码图片,用于RPN的定位回归。</p>
                </div>
                <h4 class="anchor-tag" id="60" name="60">1.3 孪生RPN</h4>
                <div class="p1">
                    <p id="61">孪生RPN网络借鉴了用于目标跟踪的全卷积孪生网络思想,将RPN<citation id="192" type="reference"><link href="168" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>与FCN结合,不仅实现了有效的目标跟踪,而且因RPN的回归能力大幅提高了目标位置的检测精度,其结构如图3所示。该方法检测帧率可达到160 frame/s,在VOT2015/2016/2017及OTB100数据集中的各项指标都十分优异。但是该网络进行目标跟踪是基于上帧图片位置进行截图操作,不能对全图进行检测,无法有效解决目标丢失的问题。</p>
                </div>
                <div class="area_img" id="62">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201909035_062.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2tGeFdRSFd2NXQ4L1RpTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 孪生RPN结构" src="Detail/GetImg?filename=images/JSJC201909035_062.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2tGeFdRSFd2NXQ4L1RpTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图3 孪生RPN结构</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201909035_062.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2tGeFdRSFd2NXQ4L1RpTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="63">在孪生RPN网络的基础上,本文重构网络,将整张图片作为输入,在全图范围内使用多尺度的模板图片生成不同大小的卷积核对跟踪图片进行卷积,最终根据生成结果,共同进行非极大抑制操作,寻找图片目标,并在此过程中引入锚框掩码机制,推测目标锚框位置,加快卷积操作及屏蔽不可能锚框位置,去除图片中相似物体干扰。</p>
                </div>
                <h3 id="64" name="64" class="anchor-tag">2 锚框掩码孪生RPN</h3>
                <div class="p1">
                    <p id="65">孪生RPN网络在检测当前帧图片目标时,是基于上一帧目标位置,截取一定大小的图片作为检测图片,即假设上下帧视频图像目标位置变化不大。这样的假设条件,对于高帧率视频及没有目标消失和遮挡的视频十分适用,但容错率较低。为此,本文提出一种锚框掩码孪生RPN网络。</p>
                </div>
                <h4 class="anchor-tag" id="66" name="66">2.1 锚框掩码</h4>
                <div class="p1">
                    <p id="67">RPN是Faster-RCNN中生成目标推荐框的核心机制,可以有效地生成推荐框,以及对前后景目标进行检测。根据其工作流程,本文提出的锚框掩码原理如图4所示。正常的RPN在特征图的每个锚点上会对应生成<i>k</i>个锚框,但是根据IOU热度图可知,并不是所有锚框均有效。因此,可根据先验知识,将生成的锚框掩码图与特征图点乘为一个稀疏矩阵,则其卷积对应的得分和回归框会出现大量的0,从而屏蔽掉很多干扰锚框,加快卷积操作以及后期非极大抑制过程。本文提出的锚框掩码网络借助3D卷积学习连续3帧图片的IOU热度图信息,与后续的类FCN预测层进行相连,估计下一帧图片的锚框掩码图。</p>
                </div>
                <div class="area_img" id="68">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201909035_068.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2tGeFdRSFd2NXQ4L1RpTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 锚框掩码原理" src="Detail/GetImg?filename=images/JSJC201909035_068.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2tGeFdRSFd2NXQ4L1RpTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图4 锚框掩码原理</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201909035_068.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2tGeFdRSFd2NXQ4L1RpTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="69" name="69">2.2 锚框掩码网络</h4>
                <div class="p1">
                    <p id="70">本文用一个考虑了目标时序特征的锚框掩码网络生成锚框掩码图,以屏蔽非追踪目标以及其他相似目标干扰。首先用锚框生成前3帧图像的IOU热度图(在训练时利用真实框,在使用时将前一帧检测得分最高的框作为真实框进行操作。当连续帧数小于3时,热度图设置为全0矩阵)。生成条件为每个锚点取<i>k</i>个锚框中的IOU最高值,如果IOU值小于有效阈值,则该位置设为0,生成本图的IOU 热度图。将前3帧图片的IOU热度图输入图5所示的锚框掩码网络,经3D卷积层综合时空特征后,进行全卷积操作,生成等大小锚框掩码预测图。将每个像素点分为0、1两类,表示是否屏蔽。生成结果综合前3帧图片的时空信息,对目标锚框可能的生成位置进行预测。</p>
                </div>
                <div class="area_img" id="71">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201909035_071.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2tGeFdRSFd2NXQ4L1RpTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 锚框掩码网络结构" src="Detail/GetImg?filename=images/JSJC201909035_071.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2tGeFdRSFd2NXQ4L1RpTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图5 锚框掩码网络结构</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201909035_071.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2tGeFdRSFd2NXQ4L1RpTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="72">为了减少因错误预测导致目标位置被屏蔽,本文将预测结果与前帧图片的IOU热度图转换结果进行叠加,提高连续图片的位置相关性。该网络在VOT2016数据集下的跟踪效果如图6所示。</p>
                </div>
                <div class="area_img" id="73">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201909035_073.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2tGeFdRSFd2NXQ4L1RpTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 VOT2016锚框掩码和跟踪效果" src="Detail/GetImg?filename=images/JSJC201909035_073.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2tGeFdRSFd2NXQ4L1RpTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图6 VOT2016锚框掩码和跟踪效果</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201909035_073.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2tGeFdRSFd2NXQ4L1RpTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="74" name="74">2.3 锚框掩码孪生RPN</h4>
                <div class="p1">
                    <p id="75">如图7所示,在本文锚框掩码孪生RPN结构中将全图调整为448×448,将模板区域按相应比例截取后调整为224×224、128×128、64×64,输入网络进行特征提取。之后将相应特征图再进行卷积尺度扩充为2<i>k</i>、4<i>k</i>两种大小,与经锚框掩码网络操作后的预测图片特征图进行全卷积操作,生成3种尺度锚框特征图,分别代表不同位置和大小的锚框,最后效仿SSD网络进行非极大抑制,得到得分最高的预测区域。通过设置不同尺度的模板图片与检测图片进行滤波操作,可以实现孪生RPN网络的变尺度检测。对全图片进行操作能避免因目标从屏幕中消失或被遮挡而导致的跟踪失败。网络中的锚框掩码图在上帧图像跟踪到目标的条件下,接收锚框掩码网络生成结果,否则锚框掩码重置为全1矩阵,开启类全图检测防止目标跟踪丢失。</p>
                </div>
                <div class="area_img" id="76">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201909035_076.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2tGeFdRSFd2NXQ4L1RpTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 锚框掩码孪生RPN结构" src="Detail/GetImg?filename=images/JSJC201909035_076.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2tGeFdRSFd2NXQ4L1RpTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图7 锚框掩码孪生RPN结构</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201909035_076.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2tGeFdRSFd2NXQ4L1RpTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="77" name="77">2.4 损失函数</h4>
                <div class="p1">
                    <p id="78">本文使用的锚框掩码网络和孪生RPN可以进行分离训练和线上组合,即根据相同的数据集分离训练锚框掩码和孪生RPN。在训练孪生RPN时,将锚框掩码图置为全1矩阵。锚框掩码网络损失函数为:</p>
                </div>
                <div class="p1">
                    <p id="79" class="code-formula">
                        <mathml id="79"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>l</mi><mi>o</mi><mi>s</mi><mi>s</mi><msub><mrow></mrow><mrow><mtext>m</mtext><mtext>a</mtext><mtext>s</mtext><mtext>k</mtext></mrow></msub><mo>=</mo><mfrac><mrow><mo>-</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mi>i</mi><mi>Μ</mi></munderover><mi>l</mi></mstyle><mi>a</mi><mi>b</mi><mi>e</mi><mi>l</mi><mo>_</mo><mi>p</mi><msub><mrow></mrow><mi>i</mi></msub><mo>⋅</mo><mi>lg</mi><mo stretchy="false">(</mo><mi>s</mi><mi>c</mi><mi>o</mi><mi>r</mi><mi>e</mi><mo>_</mo><mi>p</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><mi>Μ</mi></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="80">其中,<i>M</i>是卷积操作前特征图锚点数目,<i>label</i>_<i>p</i><sub><i>i</i></sub>是真实框锚框掩码,<i>scores</i>_<i>p</i><sub><i>i</i></sub>是锚框掩码图上每一个像素点的置信得分,<i>i</i>=1,2,…,<i>M</i>。</p>
                </div>
                <div class="p1">
                    <p id="81">孪生网络损失函数为:</p>
                </div>
                <div class="p1">
                    <p id="82" class="code-formula">
                        <mathml id="82"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>l</mi><mi>o</mi><mi>s</mi><mi>s</mi><msub><mrow></mrow><mrow><mtext>s</mtext><mtext>i</mtext><mtext>a</mtext></mrow></msub><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mi>i</mi><mn>3</mn></munderover><mi>l</mi></mstyle><mi>o</mi><mi>s</mi><mi>s</mi><msub><mrow></mrow><mrow><mi>s</mi><mi>i</mi><mi>a</mi><mo>_</mo><mi>c</mi><mi>l</mi><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msub><mo>+</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mi>i</mi><mn>3</mn></munderover><mi>l</mi></mstyle><mi>o</mi><mi>s</mi><mi>s</mi><msub><mrow></mrow><mrow><mi>s</mi><mi>i</mi><mi>a</mi><mo>_</mo><mi>r</mi><mi>e</mi><mi>g</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mi>l</mi><mi>o</mi><mi>s</mi><mi>s</mi><msub><mrow></mrow><mrow><mtext>s</mtext><mtext>i</mtext><mtext>a</mtext><mo>_</mo><mtext>c</mtext><mtext>l</mtext><mtext>s</mtext></mrow></msub><mo>=</mo><mfrac><mrow><mo>-</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mi>i</mi><mi>n</mi></munderover><mi>l</mi></mstyle><mi>a</mi><mi>b</mi><mi>e</mi><mi>l</mi><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub><mo>⋅</mo><mi>lg</mi><mo stretchy="false">(</mo><mi>s</mi><mi>o</mi><mi>c</mi><mi>r</mi><mi>e</mi><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><mi>n</mi></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>3</mn><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mi>l</mi><mi>o</mi><mi>s</mi><mi>s</mi><msub><mrow></mrow><mrow><mtext>s</mtext><mtext>i</mtext><mtext>a</mtext><mo>_</mo><mtext>r</mtext><mtext>e</mtext><mtext>g</mtext></mrow></msub><mo>=</mo><mfrac><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mi>i</mi><mi>n</mi></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mi>j</mi><mn>4</mn></munderover><mi>R</mi></mstyle></mrow></mstyle><mo stretchy="false">(</mo><mi>t</mi><msub><mrow></mrow><mi>j</mi></msub><mo>-</mo><mi>t</mi><msubsup><mrow></mrow><mi>j</mi><mo>*</mo></msubsup><mo stretchy="false">)</mo></mrow><mi>n</mi></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>4</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="83">其中,<i>loss</i><sub><i>sia</i></sub>表示孪生网络损失,<i>loss</i><sub><i>sia</i></sub>_<i>cls</i><sub><i>i</i></sub>、<i>loss</i><sub><i>sia</i></sub>_<i>reg</i><sub><i>i</i></sub>表示第<i>i</i>个模板图片处理后的网络分类损失和网络回归损失,<i>i</i>=1,2,3,<i>n</i>表示有效的锚框数目,<i>labels</i><sub><i>i</i></sub>表示第<i>i</i>个锚框的真实分类,<i>t</i><sub><i>j</i></sub>是真实框位置和锚框位置转换参数(<i>t</i><sub><i>x</i></sub>,<i>t</i><sub><i>y</i></sub>,<i>t</i><sub><i>h</i></sub>,<i>t</i><sub><i>w</i></sub>)中的一项,<i>t</i><sup>*</sup><sub><i>j</i></sub>是预测框位置和锚框位置转换参数(<i>t</i><sup>*</sup><sub><i>x</i></sub>,<i>t</i><sup>*</sup><sub><i>y</i></sub>,<i>t</i><sup>*</sup><sub><i>h</i></sub>,<i>t</i><sup>*</sup><sub><i>w</i></sub>)中的一项,<i>j</i>=1,2,3,4。</p>
                </div>
                <h3 id="84" name="84" class="anchor-tag">3 实验与结果分析</h3>
                <h4 class="anchor-tag" id="85" name="85">3.1 实验环境及训练集</h4>
                <div class="p1">
                    <p id="86">本文实验的硬件环境为锐龙2600x型 CPU、gtx1080Ti型显卡;软件环境为Linux环境搭载pycharm+tensorflow。训练集为VOT2016和OTB100数据集。其中,VOT2016包含60个通用跟踪视频,OTB100包含100个通用跟踪视频。</p>
                </div>
                <h4 class="anchor-tag" id="87" name="87">3.2 对比模型</h4>
                <div class="p1">
                    <p id="88">本文实验与当前的9种主流模型进行对比,其中包括SiamRPN<citation id="193" type="reference"><link href="150" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>、C-COT<citation id="194" type="reference"><link href="136" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>、T-CNN<citation id="195" type="reference"><link href="138" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>、ECO-HC<citation id="196" type="reference"><link href="144" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>、Staple<citation id="197" type="reference"><link href="130" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>、EBT<citation id="198" type="reference"><link href="170" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>、SiamRN<citation id="199" type="reference"><link href="172" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>、MDNet-N<citation id="200" type="reference"><link href="174" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>和SiamAN<citation id="201" type="reference"><link href="172" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>。</p>
                </div>
                <h4 class="anchor-tag" id="89" name="89">3.3 数据集VOT2016实验结果</h4>
                <div class="p1">
                    <p id="90">本文在数据集某一个视频片段中任取一帧图片真实位置作为模板图片,另一帧图片作为检测图片,生成一个训练样本,以20个训练样本作为一个批次进行训练,训练迭代10 000次。</p>
                </div>
                <div class="p1">
                    <p id="91">本文通过准确率(跟踪成功的平均覆盖率)和鲁棒性(平均失败次数)对模型进行评估。具体指标为预期平均重叠率(Expected Average Overlap,EAO)、准确率(Accuracy)、平均批次失败个数(Failure)、等效过滤操作(Equivalent Filter Operations,EFO)。</p>
                </div>
                <div class="p1">
                    <p id="92">本文实验与当前的9种主流模型进行了对比,结果如图8及表1所示。根据对比结果可以看出,本文模型相较于孪生RPN(SiamRPN),在准确率略有下降的情况下,失误率减低了很多,所以在主流网络中其EAO表现优异。并在相同指标下跟踪速度EFO达到15.4,即对大小为448×448图片,其跟踪帧率为24.6 frame/s,可满足实时性要求。本文模型准确率下降的原因在于模板尺度变换有限,对于极小目标,模板难以匹配或者插值后特征不明显,但对于大部分跟踪目标本模型效果显著。</p>
                </div>
                <div class="area_img" id="93">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201909035_093.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2tGeFdRSFd2NXQ4L1RpTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 平均覆盖率及速度对比" src="Detail/GetImg?filename=images/JSJC201909035_093.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2tGeFdRSFd2NXQ4L1RpTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图8 平均覆盖率及速度对比</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201909035_093.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2tGeFdRSFd2NXQ4L1RpTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="94">
                    <p class="img_tit"><b>表1 10种模型对比结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="94" border="1"><tr><td>模型</td><td>EAO</td><td>Accuracy</td><td>Failure</td><td>EFO</td></tr><tr><td>SiamRPN</td><td>0.344 1</td><td>0.56</td><td>1.08</td><td>23.300</td></tr><tr><td><br />C-COT</td><td>0.331 0</td><td>0.53</td><td>0.85</td><td>0.507</td></tr><tr><td><br />TCNN</td><td>0.327 5</td><td>0.55</td><td>0.89</td><td>1.200</td></tr><tr><td><br />ECO-HC</td><td>0.322 0</td><td>0.53</td><td>1.08</td><td>15.130</td></tr><tr><td><br />Staple</td><td>0.295 2</td><td>0.54</td><td>1.35</td><td>11.140</td></tr><tr><td><br />EBT</td><td>0.291 3</td><td>0.47</td><td>0.90</td><td>3.011</td></tr><tr><td><br />SiamRN</td><td>0.276 6</td><td>0.55</td><td>1.37</td><td>5.440</td></tr><tr><td><br />MDNet-N</td><td>0.257 0</td><td>0.54</td><td>1.20</td><td>0.534</td></tr><tr><td><br />SiamAN</td><td>0.235 2</td><td>0.53</td><td>1.65</td><td>9.210</td></tr><tr><td><br />本文模型</td><td>0.344 5</td><td>0.55</td><td>0.95</td><td>15.400</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="95" name="95">3.4 数据集OTB100实验结果</h4>
                <div class="p1">
                    <p id="96">OTB100数据集采用与VOT2016相同训练方式,以20个训练样本作为一个批次进行训练,训练迭代10 000次。</p>
                </div>
                <div class="p1">
                    <p id="97">本文将预测框与真实框测距小于规定阈值的图片作为正确检测图片,正确图片在数据集中的占比作为准确率;预测框与真实框覆盖率高于设定阈值的图片比例作为成功率,对模型进行验证。实验结果如图9所示,其中,[ ]中的数值为该模型误差范围小于0.5的准确率值和覆盖率大于0.5的成功率值。从图9可以看出,本文模型在跟踪准确率和中心位置准确率上与孪生RPN结果相比皆有一定上升,准确率达到了0.862,成功率达到了0.642。并且本文模型成功率下降梯度小,准确率上升梯度大。这表明模型具有强鲁棒性,适用于单目标检测。</p>
                </div>
                <div class="area_img" id="98">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201909035_098.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2tGeFdRSFd2NXQ4L1RpTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图9 OTB数据集下准确率及成功率对比" src="Detail/GetImg?filename=images/JSJC201909035_098.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2tGeFdRSFd2NXQ4L1RpTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图9 OTB数据集下准确率及成功率对比</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201909035_098.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2tGeFdRSFd2NXQ4L1RpTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="99" name="99">3.5 摄像头数据检验</h4>
                <div class="p1">
                    <p id="100">本文利用摄像头采取手部数据训练,在实验室采集视频输入网络中进行检验,检验效果如图10所示。在图10(a)中,目标在前3帧图像中出现,从第4帧开始目标在屏幕中消失,而在最后2帧图像中另一位置目标重新出现并被跟踪。这说明本文网络在目标丢失后可以重新跟踪目标,具有较强的鲁棒性。在图10(b)中,具有相似特征的左右手轨迹具有交叉相近位置,目标跟踪没有受到干扰。这证明本文模型能对目标位置进行连续有效预测,屏蔽干扰位置,使得网络具有较强的抗干扰能力。上述实验证明,本文模型具有良好的跟踪特性可用于实时跟踪。</p>
                </div>
                <div class="area_img" id="101">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201909035_101.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2tGeFdRSFd2NXQ4L1RpTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图10 实验室视频结果" src="Detail/GetImg?filename=images/JSJC201909035_101.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2tGeFdRSFd2NXQ4L1RpTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图10 实验室视频结果</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201909035_101.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2tGeFdRSFd2NXQ4L1RpTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h3 id="102" name="102" class="anchor-tag">4 结束语</h3>
                <div class="p1">
                    <p id="103">本文构建一种用于单目标跟踪的锚框掩码孪生RPN模型,通过设置多尺度模板对全图进行相关滤波,以实现变尺度跟踪及防止目标丢失,同时采用锚框掩码网络预测目标锚框位置,提高网络的抗干扰能力。经过VOT2016和OBT100数据集实验检测,该模型具有高检测准确率及强鲁棒性。利用摄像头采集数据对模型抗干扰性与实时性的验证也取得了较好的效果。下一步尝试引入不同机制以提高目标的跟踪速度,并将本文方法应用到多目标跟踪领域。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="130">
                            <a id="bibliography_1" >
                                    <b>[1]</b>
                                 BERTINETTO L,VALMADRE J,GOLODETZ S,et al.Staple:complementary learners for real-time tracking[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Press,2016:1401-1409.
                            </a>
                        </p>
                        <p id="132">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning multi-domain convolutional neural networks for visual tracking">

                                <b>[2]</b> NAM H,HAN B.Learning multi-domain convolutional neural networks for visual tracking[C]//Proceedings of the IEEE Conferenceon Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Press,2016:4293-4302.
                            </a>
                        </p>
                        <p id="134">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201811037&amp;v=MjcyNzlHRnJDVVJMT2VaZVJyRnk3bFZyN09MejdCYmJHNEg5bk5ybzlHWTRRS0RIODR2UjRUNmo1NE8zenFxQnQ=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2tGeFdRSFd2NXQ4L1RpTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> 江维创,张俊为,桂江生.基于改进核相关滤波器的目标跟踪算法[J].计算机工程,2018,44(11):228-233.
                            </a>
                        </p>
                        <p id="136">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201902043&amp;v=MjU3MzFyRnk3bFZyN09MejdCYmJHNEg5ak1yWTlCWjRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVI=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2tGeFdRSFd2NXQ4L1RpTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b> 李大湘,吴玲风,李娜,等.改进的SAMF目标跟踪算法[J].计算机工程,2019,45(2):258-264.
                            </a>
                        </p>
                        <p id="138">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Beyond correlation filters:Learning continuous convolution operators for visual tracking">

                                <b>[5]</b> DANELLJAN M,ROBINSON A,KHAN F S,et al.Beyond correlation filters:learning continuous convolution operators for visual tracking[C]//Proceedings of European Conference on Computer Vision.Berlin,Germany:Springer,2016:472-488.
                            </a>
                        </p>
                        <p id="140">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=T-cnn:Tubelets with convolutional neural networks for object detection from videos">

                                <b>[6]</b> KANG Kai,LI Hongsheng,YAN Junjie,et al.T-CNN:tubelets with convolutional neural networks for object detection from videos[J].IEEE Transactions on Circuits and Systems for Video Technology,2017,28(10):2896-2907.
                            </a>
                        </p>
                        <p id="142">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Robust visual tracking with deep convolutional neural network based object proposals on pets">

                                <b>[7]</b> ZHU Gao,PORIKLI F,LI Hongdong.Robust visual tracking with deep convolutional neural network based object proposals on PETS[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops.Washington D.C.,USA:IEEE Press,2016:26-33.
                            </a>
                        </p>
                        <p id="144">
                            <a id="bibliography_8" >
                                    <b>[8]</b>
                                 DANELLJAN M,BHAT G,SHAHBAZ K F,et al.ECO:efficient convolution operators for tracking[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Press,2017:6638-6646.
                            </a>
                        </p>
                        <p id="146">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=High-speed tracking with kernelized correlation filters">

                                <b>[9]</b> HENRIQUES J F,CASEIRO R,MARTINS P,et al.High-speed tracking with kernelized correlation filters[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2014,37(3):583-596.
                            </a>
                        </p>
                        <p id="148">
                            <a id="bibliography_10" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=HEBG201807015&amp;v=MjgyMjdUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJyRnk3bFZyN09MU2pKYWJHNEg5bk1xSTlFWVlRS0RIODR2UjQ=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2tGeFdRSFd2NXQ4L1RpTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[10]</b> 朱齐丹,韩瑜,蔡成涛.全景视觉非线性核相关滤波目标跟踪技术[J].哈尔滨工程大学学报,2018,39 (7):102-108.
                            </a>
                        </p>
                        <p id="150">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=High performance visual tracking with Siamese region proposal network">

                                <b>[11]</b> LI Bo,YAN Junjie,WU Wei,et al.High performance visual tracking with Siamese region proposal network[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Press,2018:8971-8980.
                            </a>
                        </p>
                        <p id="152">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fully-convolutional siamese networks for object tracking">

                                <b>[12]</b> BERTINETTO L,VALMADRE J,HENRIQUES J F,et al.Fully-convolutional Siamese networks for object tracking[C]//Proceedings of European Conferenceon Computer Vision.Berlin,Germany:Springer,2016:850-865.
                            </a>
                        </p>
                        <p id="154">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fully convolutional networks for semantic segmentation">

                                <b>[13]</b> LONG J,SHELHAMER E,DARRELL T.Fully convolutional networks for semantic segmentation[C]//Proceedings of the IEEE Conferenceon Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Press,2015:3431-3440.
                            </a>
                        </p>
                        <p id="156">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=3D Convolutional Neural Networks for Human Action Recognition">

                                <b>[14]</b> JI Shuiwang,XU Wei,YANG Ming,et al.3D convolutional neural networks for human action recognition[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2012,35(1):221-231.
                            </a>
                        </p>
                        <p id="158">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning spatiotemporal features with 3Dconvolutional networks">

                                <b>[15]</b> TRAN D,BOURDEV L,FERGUS R,et al.Learning spatiotemporal features with 3D convolutional networks[C]//Proceedings of the IEEE International Conferenceon Computer Vision.Washington D.C.,USA:IEEE Press,2015:4489-4497.
                            </a>
                        </p>
                        <p id="160">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=R-FCN:Object Detection via Region-based Fully Convolutional Networks">

                                <b>[16]</b> DAI Jifeng,LI Yi,HE Kaiming,et al.R-FCN:object detection via region-based fully convolutional networks[C]//Proceedings of Advances in Neural Information Processing Systems.[S.l.]:Neural Information Processing Systems Foundation,Inc.,2016:379-387.
                            </a>
                        </p>
                        <p id="162">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Mask R-CNN">

                                <b>[17]</b> HE Kaiming,GKIOXARI G,DOLLÁR P,et al.Mask R-CNN[C]//Proceedings of the IEEE International Conferenceon Computer Vision.Washington D.C.,USA:IEEE Press,2017:2961-2969.
                            </a>
                        </p>
                        <p id="164">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials">

                                <b>[18]</b> KRAHENBÜHL P,KOLTUN V.Efficient inference in fully connected CRFs with Gaussian edge potentials[C]//Proceedings of Advances in Neural Information Processing Systems.[S.l.]:Neural Information Processing Systems Foundation,Inc.,2011:109-117.
                            </a>
                        </p>
                        <p id="166">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Conditional Random Fields as Recurrent Neural Networks">

                                <b>[19]</b> ZHENG Shuai,JAYASUMANA S,ROMERA-PAREDES B,et al.Conditional random fields as recurrent neural networks[C]//Proceedings of the IEEE International Conference on Computer Vision.Washington D.C.,USA:IEEE Press,2015:1529-1537.
                            </a>
                        </p>
                        <p id="168">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Faster R-CNN:Towards real-time object detection with region proposal networks">

                                <b>[20]</b> REN Shaoqing,HE Kaiming,GIRSHICK R,et al.Faster R-CNN:towards real-time object detection with region proposal networks[C]//Proceedings of Advances in Neural Information Processing Systems.[S.l.]:Neural Information Processing Systems Foundation,Inc.,2015:91-99.
                            </a>
                        </p>
                        <p id="170">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Beyond Local Search:Tracking Objects Everywhere with Instance-Specific Proposals">

                                <b>[21]</b> ZHU Gao,PORIKLI F,LI Hongdong.Beyond local search:Tracking objects everywhere with instance-specific proposals[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Press,2016:943-951.
                            </a>
                        </p>
                        <p id="172">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning to compare image patches via convolutional neural networks">

                                <b>[22]</b> ZAGORUYKO S,KOMODAKIS N.Learning to compare image patches via convolutional neural networks[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Press,2015:4353-4361.
                            </a>
                        </p>
                        <p id="174">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Parallel tracking and verifying:a framework for real-time and high accuracy visual tracking">

                                <b>[23]</b> FAN Heng,LING Haibin.Parallel tracking and verifying:a framework for real-time and high accuracy visual tracking[C]//Proceedings of the IEEE International Conference on Computer Vision.Washington D.C.,USA:IEEE Press,2017:5486-5494.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJC201909035" />
        <input id="dpi" type="hidden" value="600" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2tGeFdRSFd2NXQ4L1RpTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201909035&amp;v=MDQ4MjlqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSckZ5N2xWcjdPTHo3QmJiRzRIOWpNcG85R1lZUUtESDg0dlI0VDY=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU4rdTgrZ2tGeFdRSFd2NXQ4L1RpTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
