<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637130604065587500%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJC201904035%26RESULT%3d1%26SIGN%3di4UIErs8XgNS8d6Iaaj2NKr4o7s%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201904035&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201904035&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201904035&amp;v=MjM3OTMzenFxQnRHRnJDVVJMT2VaZVJvRnkvbVVyN0xMejdCYmJHNEg5ak1xNDlHWVlRS0RIODR2UjRUNmo1NE8=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#41" data-title="0 概述 ">0 概述</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#46" data-title="1 卷积神经网络 ">1 卷积神经网络</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#49" data-title="1.1 卷积层">1.1 卷积层</a></li>
                                                <li><a href="#62" data-title="1.2 池化层">1.2 池化层</a></li>
                                                <li><a href="#75" data-title="1.3 反向传播">1.3 反向传播</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#91" data-title="2 混合概率随机池化方法 ">2 混合概率随机池化方法</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#92" data-title="2.1 方法原理">2.1 方法原理</a></li>
                                                <li><a href="#99" data-title="2.2 混合概率池化方法">2.2 混合概率池化方法</a></li>
                                                <li><a href="#115" data-title="2.3 方法可行性论证">2.3 方法可行性论证</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#121" data-title="3 实验结果与分析 ">3 实验结果与分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#123" data-title="3.1 MNIST数据集">3.1 MNIST数据集</a></li>
                                                <li><a href="#127" data-title="3.2 CIFAR-10数据集">3.2 CIFAR-10数据集</a></li>
                                                <li><a href="#129" data-title="3.3 CIFAR-100数据集">3.3 CIFAR-100数据集</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#133" data-title="4 超参数选择 ">4 超参数选择</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#136" data-title="5 结束语 ">5 结束语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#48" data-title="&lt;b&gt;图1 卷积神经网络结构&lt;/b&gt;"><b>图1 卷积神经网络结构</b></a></li>
                                                <li><a href="#94" data-title="&lt;b&gt;图2 Stochastic pooling方法池化域元素权重概率&lt;/b&gt;"><b>图2 Stochastic pooling方法池化域元素权重概率</b></a></li>
                                                <li><a href="#97" data-title="&lt;b&gt;图3 Rank-based stochastic pooling方法元素被选中概率&lt;/b&gt;"><b>图3 Rank-based stochastic pooling方法元素被选中概率</b></a></li>
                                                <li><a href="#112" data-title="&lt;b&gt;图4 混合概率池化方法示例&lt;/b&gt;"><b>图4 混合概率池化方法示例</b></a></li>
                                                <li><a href="#114" data-title="&lt;b&gt;图5 3种随机池化概率比较结果&lt;/b&gt;"><b>图5 3种随机池化概率比较结果</b></a></li>
                                                <li><a href="#119" data-title="&lt;b&gt;图6 不同超参数下池化域元素概率&lt;/b&gt;"><b>图6 不同超参数下池化域元素概率</b></a></li>
                                                <li><a href="#125" data-title="&lt;b&gt;表1 不同方法在MNIST数据集上的测试准确率&lt;/b&gt;"><b>表1 不同方法在MNIST数据集上的测试准确率</b></a></li>
                                                <li><a href="#131" data-title="&lt;b&gt;表2 不同方法的测试准确率&lt;/b&gt;"><b>表2 不同方法的测试准确率</b></a></li>
                                                <li><a href="#135" data-title="&lt;b&gt;图7 不同超参数下CIFAR-10数据集的测试准确率&lt;/b&gt;"><b>图7 不同超参数下CIFAR-10数据集的测试准确率</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" title=" 郭宝龙, 孙伟.数字图像处理系统工程导论[M].西安:西安电子科技大学出版社, 2012:17-20." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CBBD&amp;filename=9787560628097001&amp;v=MDI1MzA3bjN4RTlmYnZuS3JpZlp1OXVGQ3ZqVTdqTElWOFdYRnF6R2JhK0h0Zk9wNDlNWStzUERSTTh6eFVTbURkOVNI&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         郭宝龙, 孙伟.数字图像处理系统工程导论[M].西安:西安电子科技大学出版社, 2012:17-20.
                                    </a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" title=" HINTON G E, OSINDERO S, TEH Y W.A fast learning algorithm for deep belief nets[J].Neural Computation, 2006, 18 (7) :1527-1554." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJBK&amp;filename=SJBK15090500012538&amp;v=MTAzMjhKMTBWYXhBPU5pZkpaYks5SHRqTXFvOUZaT29OQ1g4eG9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVUxuSQ==&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         HINTON G E, OSINDERO S, TEH Y W.A fast learning algorithm for deep belief nets[J].Neural Computation, 2006, 18 (7) :1527-1554.
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title=" BENGIO Y, LAMBLIN P, POPOVICI D, et al.Greedy layer-wise training of deep networks[C]//Proceedings of the 19th International Conference on Neural Information Processing Systems.Cambridge, USA:MIT Press, 2007:153-160." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Greedy layer-wise training of deep networks">
                                        <b>[3]</b>
                                         BENGIO Y, LAMBLIN P, POPOVICI D, et al.Greedy layer-wise training of deep networks[C]//Proceedings of the 19th International Conference on Neural Information Processing Systems.Cambridge, USA:MIT Press, 2007:153-160.
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" title=" KRIZHEVSKY A, SUTSKEVER I, HILTON G E.ImageNet classification with deep convolutional neural networks[C]//Proceedings of the 25th International Conference on Neural Information Processing Systems.[S.l.]:Curran Associates Inc., 2012:1097-1105." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image Net Classification with Deep Convolutional Neural Networks">
                                        <b>[4]</b>
                                         KRIZHEVSKY A, SUTSKEVER I, HILTON G E.ImageNet classification with deep convolutional neural networks[C]//Proceedings of the 25th International Conference on Neural Information Processing Systems.[S.l.]:Curran Associates Inc., 2012:1097-1105.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" title=" SCHERER D, MULLER A, BEHNKE S.Evaluation of pooling operation in convolutional architecture for object recognition[C]//Proceedings of the 20th International Conference on Artificial Neural Networks.Berlin, Germany:Springer, 2010:92-101." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Evaluation of pooling operations in convolutional architectures for object recognition">
                                        <b>[5]</b>
                                         SCHERER D, MULLER A, BEHNKE S.Evaluation of pooling operation in convolutional architecture for object recognition[C]//Proceedings of the 20th International Conference on Artificial Neural Networks.Berlin, Germany:Springer, 2010:92-101.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" title=" BOUREAU Y L, PONCE J, LECUN Y.A theoretical analysis of feature pooling in visual recognition [EB/OL].[2017-12-10].http://yann.lecun.com/exdb/publis/pdf/boureau-icml-10.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A theoretical analysis of feature pooling in visual recognition">
                                        <b>[6]</b>
                                         BOUREAU Y L, PONCE J, LECUN Y.A theoretical analysis of feature pooling in visual recognition [EB/OL].[2017-12-10].http://yann.lecun.com/exdb/publis/pdf/boureau-icml-10.pdf.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" title=" WANG T, WU D, COATES A.et al.End-to-end text recognition with convolutional neural networks[C]//Proceedings of the 21st International Conference on Pattern Recognition.Washington D.C., USA:IEEE Press, 2012:3304-3308." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=End-to-end text recognition w ith convolutional neural netw orks">
                                        <b>[7]</b>
                                         WANG T, WU D, COATES A.et al.End-to-end text recognition with convolutional neural networks[C]//Proceedings of the 21st International Conference on Pattern Recognition.Washington D.C., USA:IEEE Press, 2012:3304-3308.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" title=" YANG J C, YU K, GONG Y H, et al.Linear spatial pyramid matching using coding for image classifica-tion[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2009:1794-1801." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Linear spatial pyramid matc-hing using sparse coding for image classification">
                                        <b>[8]</b>
                                         YANG J C, YU K, GONG Y H, et al.Linear spatial pyramid matching using coding for image classifica-tion[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2009:1794-1801.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" title=" RANZATO M, HUANG F J, BOUREAU Y, et al.Unsupervised learning of invariant feature hierarchies with applications to object recognition[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2007." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Unsuper-vised learning of invariant feature hierarchies with applications to ob-ject recognition">
                                        <b>[9]</b>
                                         RANZATO M, HUANG F J, BOUREAU Y, et al.Unsupervised learning of invariant feature hierarchies with applications to object recognition[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2007.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_10" title=" ZEILER M D, FERGUS R.Stochastic pooling for regulariza-tion of deep convolutional neural networks[EB/OL].[2017-12-10].https://arxiv.org/pdf/1301.3557.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Stochastic pooling for regulariza-tion of deep convolutional neural networks">
                                        <b>[10]</b>
                                         ZEILER M D, FERGUS R.Stochastic pooling for regulariza-tion of deep convolutional neural networks[EB/OL].[2017-12-10].https://arxiv.org/pdf/1301.3557.pdf.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_11" title=" SERMANET P, CHINTALA S, LECUN Y.Convolutional neural networks applied to house numbers digit classification[C]//Proceedings of the 21st International Conference on Pattern Recognition.Washington D.C., USA:IEEE Press, 2012." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Convolutional Neural Networks Applied To House Numbers Digit Classification">
                                        <b>[11]</b>
                                         SERMANET P, CHINTALA S, LECUN Y.Convolutional neural networks applied to house numbers digit classification[C]//Proceedings of the 21st International Conference on Pattern Recognition.Washington D.C., USA:IEEE Press, 2012.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_12" title=" 余萍, 赵继生.基于矩阵2-范数的卷积神经网络图像识别算法[J].图学学报, 2016, 37 (5) :694-701." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GCTX201605017&amp;v=MjUzNTVxcUJ0R0ZyQ1VSTE9lWmVSb0Z5L21VcjdLSWk3ZmRyRzRIOWZNcW85RVk0UUtESDg0dlI0VDZqNTRPM3o=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[12]</b>
                                         余萍, 赵继生.基于矩阵2-范数的卷积神经网络图像识别算法[J].图学学报, 2016, 37 (5) :694-701.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_13" title=" YU D J, WANG H L, CHEN P Q, et al.Mixed pooling for convolutional neural network[C]//Proceedings of the 9th International Conference on Rough Sets and Knowledge Technology.Berlin, Germany:Springer, 2014:364-375." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Mixed pooling for convolutional neural networks">
                                        <b>[13]</b>
                                         YU D J, WANG H L, CHEN P Q, et al.Mixed pooling for convolutional neural network[C]//Proceedings of the 9th International Conference on Rough Sets and Knowledge Technology.Berlin, Germany:Springer, 2014:364-375.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_14" title=" SHI Z L, YE Y D, WU Y P.Rank-based pooling for deep convolutional neural network[J].Neural Networks, 2016, 83:21-31." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Rank-based pooling for deep convolutional neural networks">
                                        <b>[14]</b>
                                         SHI Z L, YE Y D, WU Y P.Rank-based pooling for deep convolutional neural network[J].Neural Networks, 2016, 83:21-31.
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_15" title=" HE K, ZHANG X, REN S, et al.Spatial pyramid pooling in deep convolutional networks for visual recognition[C]//Proceedings of the 13th European Conference on Computer Vision.Berlin, Germany:Springer, 2014:346-361." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Spatial pyramid pooling in deep convolutional networks for visual recognition">
                                        <b>[15]</b>
                                         HE K, ZHANG X, REN S, et al.Spatial pyramid pooling in deep convolutional networks for visual recognition[C]//Proceedings of the 13th European Conference on Computer Vision.Berlin, Germany:Springer, 2014:346-361.
                                    </a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_16" title=" 孙志军, 薛磊, 许阳明, 等.深度学习研究综述[J].计算机应用研究, 2012, 29 (8) :2806-2810." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSYJ201208003&amp;v=MDc5MDNVUkxPZVplUm9GeS9tVXI3S0x6N1NaTEc0SDlQTXA0OUZaNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckM=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[16]</b>
                                         孙志军, 薛磊, 许阳明, 等.深度学习研究综述[J].计算机应用研究, 2012, 29 (8) :2806-2810.
                                    </a>
                                </li>
                                <li id="35">


                                    <a id="bibliography_17" title=" ZEILER M D, FERGUS R.Visualizing and understanding convolutional networks[C]//Proceedings of the 13th European Conference on Computer Vision.Berlin, Germany:Springer, 2014:818-833." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Visualizing and understanding convolutional networks">
                                        <b>[17]</b>
                                         ZEILER M D, FERGUS R.Visualizing and understanding convolutional networks[C]//Proceedings of the 13th European Conference on Computer Vision.Berlin, Germany:Springer, 2014:818-833.
                                    </a>
                                </li>
                                <li id="37">


                                    <a id="bibliography_18" title=" 陈玄, 朱荣, 王中元.基于融合卷积神经网络模型的手写数字识别[J].计算机工程, 2017, 42 (11) :187-192." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201711031&amp;v=MjM4MTZVcjdLTHo3QmJiRzRIOWJOcm85R1pZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSb0Z5L20=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[18]</b>
                                         陈玄, 朱荣, 王中元.基于融合卷积神经网络模型的手写数字识别[J].计算机工程, 2017, 42 (11) :187-192.
                                    </a>
                                </li>
                                <li id="39">


                                    <a id="bibliography_19" title=" IOFFE S, SZEGEDY C.Batch normalization:accelerating deep network training by reducing internal covariate shift[C]//Proceedings of the 32nd International Conference on Machine Learning.[S.l.]:JMLR, 2015:448-456." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Batch normalization:Accelerating deep network training by reducing internal covariate shift">
                                        <b>[19]</b>
                                         IOFFE S, SZEGEDY C.Batch normalization:accelerating deep network training by reducing internal covariate shift[C]//Proceedings of the 32nd International Conference on Machine Learning.[S.l.]:JMLR, 2015:448-456.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJC" target="_blank">计算机工程</a>
                2019,45(04),211-216 DOI:10.19678/j.issn.1000-3428.0050129            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>卷积神经网络池化方法研究</b></span>
 <span class="shoufa"></span>                                     </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%91%A8%E6%9E%97%E5%8B%87&amp;code=28710035&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">周林勇</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%B0%A2%E6%99%93%E5%B0%A7&amp;code=06962154&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">谢晓尧</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%88%98%E5%BF%97%E6%9D%B0&amp;code=11023486&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">刘志杰</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E4%BB%BB%E7%AC%94%E5%A2%A8&amp;code=38808285&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">任笔墨</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E8%B4%B5%E5%B7%9E%E5%B8%88%E8%8C%83%E5%A4%A7%E5%AD%A6%E8%B4%B5%E5%B7%9E%E7%9C%81%E4%BF%A1%E6%81%AF%E4%B8%8E%E8%AE%A1%E7%AE%97%E7%A7%91%E5%AD%A6%E9%87%8D%E7%82%B9%E5%AE%9E%E9%AA%8C%E5%AE%A4&amp;code=0186961&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">贵州师范大学贵州省信息与计算科学重点实验室</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E8%B4%B5%E5%B7%9E%E8%B4%A2%E7%BB%8F%E5%A4%A7%E5%AD%A6%E6%95%B0%E5%AD%A6%E4%B8%8E%E7%BB%9F%E8%AE%A1%E5%AD%A6%E9%99%A2&amp;code=1694100&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">贵州财经大学数学与统计学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>为解决随机池化中零元素概率为0导致不能被选择的问题, 提出一种改进的混合概率随机池化方法。将池化域中的元素去重复并按升序排序, 然后加上对应次序的幂次, 得到元素的权重概率。在此基础上, 根据多项分布取样给出池化值。在数据集MNIST、CIFAR-10、CIFAR-100上进行实验, 结果表明, 该方法在3种数据集上的分类准确率分别为99.50%、72.25%、39.05%, 相较于传统池化方法具有较好的分类效果与稳健性。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">卷积神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B1%A0%E5%8C%96%E6%96%B9%E6%B3%95&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">池化方法;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%A4%9A%E9%A1%B9%E5%88%86%E5%B8%83&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">多项分布;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">图像分类;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    周林勇 (1987—) , 男, 博士研究生, 主研方向为深度学习、图像处理, E-mail:gzmtzly@126.com;;
                                </span>
                                <span>
                                    *谢晓尧 (通信作者) , 教授、博士生导师;;
                                </span>
                                <span>
                                    刘志杰, 教授、博士;;
                                </span>
                                <span>
                                    任笔墨, 硕士。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-01-16</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金 (U1631132);</span>
                    </p>
            </div>
                    <h1><b>Research on Pooling Method of Convolution Neural Network</b></h1>
                    <h2>
                    <span>ZHOU Linyong</span>
                    <span>XIE Xiaoyao</span>
                    <span>LIU Zhijie</span>
                    <span>REN Bimo</span>
            </h2>
                    <h2>
                    <span>Key Laboratory of Information and Computing Science of Guizhou Province, Guizhou Normal University</span>
                    <span>School of Mathematics and Statistics, Guizhou University of Finance and Economics</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>In order to solve the problem that the probability of zero elements in random pooling is zero, it cannot be selected.An improved hybrid probability stochastic method is proposed.The elements in the pooled domain are deduplicated and sorted in ascending order, and then the power of the corresponding order is added to obtain the weight probability of the element.On this basis, the pooling value is given based on the multi-distribution sampling.Experimental results on the datasets MNIST, CIFAR-10, and CIFAR-100 show that the method in the classification accuracy of the three datasets is 99.50%, 72.25%, and 39.05%, respectively.Compared with the traditional pooling method, the method has good classification effect and robustness.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Convolutional%20Neural%20Network%20(CNN)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Convolutional Neural Network (CNN) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=deep%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">deep learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=pooling%20method&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">pooling method;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=multinomial%20distribution&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">multinomial distribution;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=image%20classification&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">image classification;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-01-16</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="41" name="41" class="anchor-tag">0 概述</h3>
                <div class="p1">
                    <p id="42">图像特征提取是图像识别、图像分割、图像定位等一系列图像处理的基础, 特征提取的优劣直接影响最终图像处理的效果<citation id="138" type="reference"><link href="3" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>。目前, 基于深度学习的特征提取方法如深度自编码、深度受限玻尔兹曼机、深度置信网、卷积神经网络 (Convolutional Neural Network, CNN) <citation id="140" type="reference"><link href="5" rel="bibliography" /><link href="7" rel="bibliography" /><sup>[<a class="sup">2</a>,<a class="sup">3</a>]</sup></citation>成为特征提取的主要方法, 如文献<citation id="139" type="reference">[<a class="sup">4</a>]</citation>提出的AlexNet网络, 其成功应用于ImageNet图像分类大赛, 使深度卷积神经网络成为特征提取、图像分类的重要方法。</p>
                </div>
                <div class="p1">
                    <p id="43">池化是卷积神经网络特征提取的关键步骤, 它具有保持平移、旋转、伸缩不变性等特点<citation id="148" type="reference"><link href="11" rel="bibliography" /><link href="13" rel="bibliography" /><sup>[<a class="sup">5</a>,<a class="sup">6</a>]</sup></citation>。传统的池化方法包括平均池化<citation id="141" type="reference"><link href="15" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>和最大池化<citation id="149" type="reference"><link href="17" rel="bibliography" /><link href="19" rel="bibliography" /><sup>[<a class="sup">8</a>,<a class="sup">9</a>]</sup></citation>, 并在一定领域取得不错的效果, 但同时也存在一定的缺陷。因此, 研究者提出各种改进的池化方法。文献<citation id="142" type="reference">[<a class="sup">10</a>]</citation>提出一种基于概率权重的Stochastic pooling方法, 由于引入随机性, 因此该方法可以在一定程度上防止过拟合。文献<citation id="143" type="reference">[<a class="sup">11</a>]</citation>提出一种利用高斯核函数进行池化特征提取的Lp池化方法。文献<citation id="144" type="reference">[<a class="sup">12</a>]</citation>提出一种基于矩阵2范数的池化方法, 利用矩阵的最大奇异值作为池化域的池化值。文献<citation id="145" type="reference">[<a class="sup">13</a>]</citation>提出一种混合池化方法, 每次随机的选择最大池化与平均池化中的一种池化方法。文献<citation id="146" type="reference">[<a class="sup">14</a>]</citation>提出一种Rank-based stochastic pooling方法, 该方法是一种基于几何概率的随机池化方法。文献<citation id="147" type="reference">[<a class="sup">15</a>]</citation>提出一种空间域池化方法, 该方法可以对不同尺寸的图片进行训练。</p>
                </div>
                <div class="p1">
                    <p id="44">在改进池化方法中, Stochastic pooling方法与Rank-based stochastic pooling方法比较成功, 2种方法在不同形式上给池化域中元素引入概率, 然后再按照概率随机取样, 这样增加了取样的随机性, 从而可以防止过拟合, 取得更好的识别效果。但2种方法有一定的局限性, Stochastic pooling方法只考虑元素的取值, 此时的概率是一种权重概率, 显然有缺陷, 因为当池化域中只有一个非零元素时, 该元素的概率为1, 其他元素的概率均为0, 这样不仅剥夺了零元素被选取的概率, 而且也让该方法失去了一定的随机性;Rank-based stochastic pooling方法根据池化域中元素排序, 依次以几何概率的方式赋予概率, 该方法只考虑元素的顺序, 而未考虑元素的值, 显然不恰当。当2个元素的值相等时, 被选取的概率应该是相等的。</p>
                </div>
                <div class="p1">
                    <p id="45">基于此, 本文提出一种混合概率随机池化方法。将池化域中的元素去重复并按升序排序, 然后对每个元素进行处理, 得到元素的权重概率, 根据权重概率按多项分布随机抽样, 抽样值即为混合概率随机池化方法输出值。</p>
                </div>
                <h3 id="46" name="46" class="anchor-tag">1 卷积神经网络</h3>
                <div class="p1">
                    <p id="47">卷积神经网络是深度学习和人工智能中的重要方法之一, 在计算机视觉与自然语言处理中发挥越来越重要的作用<citation id="150" type="reference"><link href="33" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>。卷积神经网络将图像的原始数据作为输入进行处理, 因此, 它是更直接、更有效的图像处理技术。卷积神经网络是由卷积层、池化层, 以及归一化层不断堆叠, 累加在一起而形成<citation id="151" type="reference"><link href="35" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>, 堆叠次数越多, 层数越深, 从而形成深度卷积神经网络<citation id="152" type="reference"><link href="37" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>。对于实现分类任务的卷积神经网络, 除了卷积、池化外还有重要全连接层, 该层将二维的卷积池化转换为一维网络, 最终利用分类器实现分类。典型的卷积神经网络结构如图1所示, 其中, C<sub>1</sub>、C<sub>3</sub>表示第1层和第3层都是卷积层, S<sub>2</sub>、S<sub>4</sub>表示第2层和第4层为池化层, FC<sub>5</sub>表示第5层为全连接层。</p>
                </div>
                <div class="area_img" id="48">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201904035_048.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 卷积神经网络结构" src="Detail/GetImg?filename=images/JSJC201904035_048.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图1 卷积神经网络结构</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201904035_048.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="49" name="49">1.1 卷积层</h4>
                <div class="p1">
                    <p id="50">在图1中, C<sub>1</sub>、C<sub>3</sub>均为卷积层, 一般第一个卷积层与输入图像直接相连。设输入为<i>n</i>×<i>n</i>的图像矩阵<b><i>X</i></b>, 卷积核大小为<i>m</i>×<i>m</i>的矩阵<b><i>K</i></b>, 卷积核移动步长<i>F</i>为1, 则经过卷积计算后的特征图的大小为 (<i>n</i>-<i>m</i>+1) × (<i>n</i>-<i>m</i>+1) , 卷积计算公式可表示为:</p>
                </div>
                <div class="p1">
                    <p id="51" class="code-formula">
                        <mathml id="51"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>o</mi><mi>u</mi><mi>t</mi><msubsup><mrow></mrow><mi>j</mi><mi>l</mi></msubsup><mo>=</mo><mi>f</mi><mo stretchy="false"> (</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>i</mi><mo>∈</mo><mi>X</mi><msub><mrow></mrow><mi>j</mi></msub></mrow></munder><mi>X</mi></mstyle><msubsup><mrow></mrow><mi>i</mi><mrow><mi>l</mi><mo>-</mo><mn>1</mn></mrow></msubsup><mo>×</mo><mi>Κ</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mi>l</mi></msubsup><mo>+</mo><mi>b</mi><msubsup><mrow></mrow><mi>j</mi><mi>l</mi></msubsup><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="52">其中, <i>X</i><mathml id="53"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mrow><mi>l</mi><mo>-</mo><mn>1</mn></mrow></msubsup></mrow></math></mathml>表示前一层输出的特征图, b<mathml id="54"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mi>l</mi></msubsup></mrow></math></mathml>为l层第j个特征图的偏置, f (x) 为激活函数, 通常经过卷积计算的值都会经过激活函数进一步处理, 常见的激活函数包括3种, 分别为:</p>
                </div>
                <div class="p1">
                    <p id="55">1) <i>sigmoid</i>函数, 表达式为:</p>
                </div>
                <div class="p1">
                    <p id="56" class="code-formula">
                        <mathml id="56"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>f</mi><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mtext>e</mtext><msup><mrow></mrow><mrow><mo>-</mo><mi>x</mi></mrow></msup></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="57">2) <i>tanh</i>函数, 表达式为:</p>
                </div>
                <div class="p1">
                    <p id="58" class="code-formula">
                        <mathml id="58"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>f</mi><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mrow><mtext>e</mtext><msup><mrow></mrow><mi>x</mi></msup><mo>-</mo><mtext>e</mtext><msup><mrow></mrow><mrow><mo>-</mo><mi>x</mi></mrow></msup></mrow><mrow><mtext>e</mtext><msup><mrow></mrow><mi>x</mi></msup><mo>+</mo><mtext>e</mtext><msup><mrow></mrow><mrow><mo>-</mo><mi>x</mi></mrow></msup></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="59">3) <i>ReLU</i>函数, 表达式为:</p>
                </div>
                <div class="p1">
                    <p id="60">f (x) =<i>max</i> (0, x)      (4) </p>
                </div>
                <div class="p1">
                    <p id="61">对于图像分类, 通常使用<i>ReLU</i>函数作为激活函数, 因为<i>ReLU</i>函数处理后的特征图具有稀疏性并且可以在一定程度上防止梯度消失。</p>
                </div>
                <h4 class="anchor-tag" id="62" name="62">1.2 池化层</h4>
                <div class="p1">
                    <p id="63">在图1中, <i>S</i><sub>2</sub>、<i>S</i><sub>4</sub>均是池化层, 一般情况下, 池化层紧接卷积层, 卷积与池化成一一搭配关系。池化是将卷积产生的特征图先划分为互不相交的区域, 然后在每个区域中找一个统计量作为该区域的池化值。过大的池化域容易造成局部信息的丢失, 过小则不能达到降维的目的, 最恰当的池化域为不带重叠2×2以及3×3带重叠的池化域<citation id="153" type="reference"><link href="3" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>。池化层是图像特征提取的关键步骤, 一方面, 池化操作可以降低卷积计算产生的高维数据, 减小计算复杂度, 从而提高效率, 同时也可以防止过拟合;另一方面, 池化可以保持图像的平移、旋转、伸缩等不变性, 因此, 可大大提高图像分类的准确率。</p>
                </div>
                <div class="p1">
                    <p id="64">传统的池化方法有均值池化、最大池化、<i>Stochastic pooling</i>等, 一些常见的改进池化方法包括<i>Lp</i>池化、<i>Rank</i>-<i>based stochastic pooling</i>等, 具体分析如下:</p>
                </div>
                <div class="p1">
                    <p id="65">1) 均值池化是对每个池化域中元素取平均值作为该区域的统计值, 计算公式如下:</p>
                </div>
                <div class="p1">
                    <p id="66" class="code-formula">
                        <mathml id="66"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>s</mi><mo>=</mo><mfrac><mn>1</mn><mrow><mi>m</mi><mo>×</mo><mi>m</mi></mrow></mfrac><mspace width="0.25em" /><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mi>o</mi></mstyle></mrow></mstyle><mi>u</mi><mi>t</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>5</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="67">2) 最大池化是对每个池化域中元素取该区域的最大值作为该区域的统计值, 计算公式为:</p>
                </div>
                <div class="p1">
                    <p id="68" class="code-formula">
                        <mathml id="68"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>s</mi><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mrow><mi>i</mi><mo>, </mo><mi>j</mi><mo>∈</mo><mo stretchy="false">[</mo><mn>1</mn><mo>, </mo><mi>m</mi><mo stretchy="false">]</mo></mrow></munder><mrow><mo> (</mo><mrow><mi>o</mi><mi>u</mi><mi>t</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow><mo>) </mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>6</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="69">3) <i>Stochastic pooling</i>是以池化域中每个元素的权重为概率进行依概率抽样, 计算方法为:</p>
                </div>
                <div class="p1">
                    <p id="70" class="code-formula">
                        <mathml id="70"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>p</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mfrac><mrow><mi>a</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>k</mi><mo>∈</mo><mi>R</mi><msub><mrow></mrow><mi>j</mi></msub></mrow></munder><mi>a</mi></mstyle><msub><mrow></mrow><mi>k</mi></msub></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>7</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi>s</mi><mo>=</mo><mi>a</mi><msub><mrow></mrow><mi>l</mi></msub><mo>, </mo><mi>l</mi><mo>∼</mo><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>p</mi><msub><mrow></mrow><mn>1</mn></msub><mo>, </mo><mi>p</mi><msub><mrow></mrow><mn>2</mn></msub><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>p</mi><msub><mrow></mrow><mrow><mrow><mo>|</mo><mrow><mi>R</mi><msub><mrow></mrow><mi>j</mi></msub></mrow><mo>|</mo></mrow></mrow></msub><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>8</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="71">4) <i>Rank</i>-<i>based stochastic pooling</i>与<i>Stochastic pooling</i>类似, 但此时的概率采用元素在池化域中的排序来计算, 计算方法如下:</p>
                </div>
                <div class="p1">
                    <p id="72" class="code-formula">
                        <mathml id="72"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>p</mi><msub><mrow></mrow><mi>r</mi></msub><mo>=</mo><mi>α</mi><mrow><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>α</mi><mo stretchy="false">) </mo></mrow><msup><mrow></mrow><mrow><mi>r</mi><mo>-</mo><mn>1</mn></mrow></msup><mo>, </mo><mi>r</mi><mo>=</mo><mn>1</mn><mo>, </mo><mn>2</mn><mo>, </mo><mo>⋯</mo><mo>, </mo><mrow><mo>|</mo><mrow><mi>R</mi><msub><mrow></mrow><mi>j</mi></msub></mrow><mo>|</mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>9</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi>s</mi><mo>=</mo><mi>a</mi><msub><mrow></mrow><mi>l</mi></msub><mo>, </mo><mi>l</mi><mo>∼</mo><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>p</mi><msub><mrow></mrow><mn>1</mn></msub><mo>, </mo><mi>p</mi><msub><mrow></mrow><mn>2</mn></msub><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>p</mi><msub><mrow></mrow><mrow><mrow><mo>|</mo><mrow><mi>R</mi><msub><mrow></mrow><mi>j</mi></msub></mrow><mo>|</mo></mrow></mrow></msub><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>0</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="73">其中, α为超参数, 这时的概率分布实质上是一种几何分布。</p>
                </div>
                <div class="p1">
                    <p id="74">不同的池化方法得到的池化值不同, 根据梯度下降理论, 反向传播传递的大小与方向也不同。</p>
                </div>
                <h4 class="anchor-tag" id="75" name="75">1.3 反向传播</h4>
                <div class="p1">
                    <p id="76">反向传播是减小误差, 提高分类准确率的关键步骤, 卷积神经网络的反向误差传递与传统的<i>BP</i>神经网络的误差传递方式一致, 都是遵循梯度下降原理。在卷积神经网络中, 卷积层和池化层的反向传播稍有不同, 池化层没有参数, 因此该层网络不进行参数更新, 只进行误差的传递。</p>
                </div>
                <div class="p1">
                    <p id="77">对于均值池化, 对一个2×2池化域<mathml id="78"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">x</mi><mo>=</mo><mrow><mo> (</mo><mrow><mtable><mtr><mtd columnalign="left"><mi>x</mi><msub><mrow></mrow><mrow><mn>1</mn><mn>1</mn></mrow></msub><mtext> </mtext><mi>x</mi><msub><mrow></mrow><mrow><mn>1</mn><mn>2</mn></mrow></msub></mtd></mtr><mtr><mtd columnalign="left"><mi>x</mi><msub><mrow></mrow><mrow><mn>2</mn><mn>1</mn></mrow></msub><mtext> </mtext><mi>x</mi><msub><mrow></mrow><mrow><mn>2</mn><mn>2</mn></mrow></msub></mtd></mtr></mtable></mrow><mo>) </mo></mrow></mrow></math></mathml>, 根据均值池化原理, 池化输出值<mathml id="80"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>s</mi><mo>=</mo><mfrac><mn>1</mn><mn>4</mn></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mn>2</mn></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mn>2</mn></munderover><mi>x</mi></mstyle></mrow></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow></math></mathml>, 则<mathml id="81"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mo>∂</mo><mi>s</mi></mrow><mrow><mo>∂</mo><mi>x</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow></mfrac><mo>=</mo><mfrac><mn>1</mn><mn>4</mn></mfrac></mrow></math></mathml>。设d<sub><i>out</i></sub>为池化后的误差, 根据链式求导原理, 池化前的误差为:</p>
                </div>
                <div class="p1">
                    <p id="82" class="code-formula">
                        <mathml id="82"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">d</mi><mi mathvariant="bold-italic">x</mi><mo>=</mo><mi>d</mi><msub><mrow></mrow><mrow><mtext>o</mtext><mtext>u</mtext><mtext>t</mtext></mrow></msub><mo>×</mo><mrow><mo> (</mo><mrow><mtable><mtr><mtd columnalign="left"><mfrac><mrow><mo>∂</mo><mi>s</mi></mrow><mrow><mo>∂</mo><mi>x</mi><msub><mrow></mrow><mrow><mn>1</mn><mn>1</mn></mrow></msub></mrow></mfrac><mspace width="0.25em" /><mfrac><mrow><mo>∂</mo><mi>s</mi></mrow><mrow><mo>∂</mo><mi>x</mi><msub><mrow></mrow><mrow><mn>1</mn><mn>2</mn></mrow></msub></mrow></mfrac></mtd></mtr><mtr><mtd columnalign="left"><mfrac><mrow><mo>∂</mo><mi>s</mi></mrow><mrow><mo>∂</mo><mi>x</mi><msub><mrow></mrow><mrow><mn>2</mn><mn>1</mn></mrow></msub></mrow></mfrac><mspace width="0.25em" /><mfrac><mrow><mo>∂</mo><mi>s</mi></mrow><mrow><mo>∂</mo><mi>x</mi><msub><mrow></mrow><mrow><mn>2</mn><mn>2</mn></mrow></msub></mrow></mfrac></mtd></mtr></mtable></mrow><mo>) </mo></mrow><mo>=</mo><mrow><mo> (</mo><mrow><mtable><mtr><mtd columnalign="left"><mfrac><mrow><mi>d</mi><msub><mrow></mrow><mrow><mtext>o</mtext><mtext>u</mtext><mtext>t</mtext></mrow></msub></mrow><mn>4</mn></mfrac><mspace width="0.25em" /><mfrac><mrow><mi>d</mi><msub><mrow></mrow><mrow><mtext>o</mtext><mtext>u</mtext><mtext>t</mtext></mrow></msub></mrow><mn>4</mn></mfrac></mtd></mtr><mtr><mtd columnalign="left"><mfrac><mrow><mi>d</mi><msub><mrow></mrow><mrow><mtext>o</mtext><mtext>u</mtext><mtext>t</mtext></mrow></msub></mrow><mn>4</mn></mfrac><mspace width="0.25em" /><mfrac><mrow><mi>d</mi><msub><mrow></mrow><mrow><mtext>o</mtext><mtext>u</mtext><mtext>t</mtext></mrow></msub></mrow><mn>4</mn></mfrac></mtd></mtr></mtable></mrow><mo>) </mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="83">同理对于最大池化, 对一个2×2池化域<mathml id="84"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">x</mi><mo>=</mo><mrow><mo> (</mo><mrow><mtable><mtr><mtd columnalign="left"><mi>x</mi><msub><mrow></mrow><mrow><mn>1</mn><mn>1</mn></mrow></msub><mtext> </mtext><mi>x</mi><msub><mrow></mrow><mrow><mn>1</mn><mn>2</mn></mrow></msub></mtd></mtr><mtr><mtd columnalign="left"><mi>x</mi><msub><mrow></mrow><mrow><mn>2</mn><mn>1</mn></mrow></msub><mtext> </mtext><mi>x</mi><msub><mrow></mrow><mrow><mn>2</mn><mn>2</mn></mrow></msub></mtd></mtr></mtable></mrow><mo>) </mo></mrow></mrow></math></mathml>, 设<i>x</i><sub><i>kl</i></sub>是<b><i>x</i></b>中最大值, 根据最大池化原理, 池化输出值<i>s</i>=<i>x</i><sub><i>kl</i></sub>, 则<mathml id="86"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mo>∂</mo><mi>s</mi></mrow><mrow><mo>∂</mo><mi>x</mi><msub><mrow></mrow><mrow><mi>k</mi><mi>l</mi></mrow></msub></mrow></mfrac><mo>=</mo><mn>1</mn><mo>, </mo><mfrac><mrow><mo>∂</mo><mi>s</mi></mrow><mrow><mo>∂</mo><mi>x</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow></mfrac><mo>=</mo><mn>0</mn><mo>, </mo><mo stretchy="false"> (</mo><mi>i</mi><mo>, </mo><mi>j</mi><mo>=</mo><mn>1</mn><mo>, </mo><mn>2</mn></mrow></math></mathml>且<i>i</i>≠<i>k</i>∩<i>j</i>≠<i>l</i>) 。设<i>d</i><sub>out</sub>为池化后的误差, 根据链式求导原理, 池化前的误差为:</p>
                </div>
                <div class="p1">
                    <p id="87" class="code-formula">
                        <mathml id="87"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>d</mi><mi>x</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mi>d</mi><msub><mrow></mrow><mrow><mtext>o</mtext><mtext>u</mtext><mtext>t</mtext></mrow></msub><mo>×</mo><mfrac><mrow><mo>∂</mo><mi>s</mi></mrow><mrow><mo>∂</mo><mi>x</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="88">因此, <mathml id="89"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>d</mi><mi>x</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mrow><mo>{</mo><mrow><mtable><mtr><mtd columnalign="left"><mi>d</mi><msub><mrow></mrow><mrow><mtext>o</mtext><mtext>u</mtext><mtext>t</mtext></mrow></msub><mo>, </mo><mi>i</mi><mo>=</mo><mi>l</mi><mo>, </mo><mi>j</mi><mo>=</mo><mi>k</mi></mtd></mtr><mtr><mtd columnalign="left"><mn>0</mn><mo>, </mo><mtext>其</mtext><mtext>他</mtext></mtd></mtr></mtable></mrow></mrow></mrow></math></mathml></p>
                </div>
                <div class="p1">
                    <p id="90">Stochastic pooling、Rank-based stochastic pooling反向传播原理与最大池化反向传播基本类似。通过梯度反向传播, 将误差一层一层往前传递, 达到更新参数、减小误差、提高分类准确率的效果。</p>
                </div>
                <h3 id="91" name="91" class="anchor-tag">2 混合概率随机池化方法</h3>
                <h4 class="anchor-tag" id="92" name="92">2.1 方法原理</h4>
                <div class="p1">
                    <p id="93">在各种不同的池化改进方法中, 基于概率的改进是一种极为有效的方法。基于概率的池化方法给池化域中每个元素引入概率, 然后再利用多项分布进行随机取样得到池化值, 概率的引入使得池化方法具有一定的随机性, 可以有效地防止过拟合, 从而提高图像分类效果。目前基于概率的池化方法主要有<i>Stochastic pooling</i>以及<i>Rank</i>-<i>based stochastic pooling</i> 2种, 并且都能取得较为理想的效果, 但在池化域元素的概率计算上有一定的缺陷。<i>Stochastic pooling</i>以池化域中元素的取值赋予权重概率, 如图2所示。</p>
                </div>
                <div class="area_img" id="94">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201904035_094.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 Stochastic pooling方法池化域元素权重概率" src="Detail/GetImg?filename=images/JSJC201904035_094.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图2 Stochastic pooling方法池化域元素权重概率</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201904035_094.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="95">当一个池化域中只有一个非零元素, 此时非零元素对应的概率为1, 其他元素对应的概率为0。这时随机池化相当于最大池化, 零元素被选择的概率为0, 因此, 该方法不再具有随机性, 对防止过拟合、提高图像分类效果能也有一定的影响。</p>
                </div>
                <div class="p1">
                    <p id="96">Rank-based stochastic pooling以池化域中元素的排序依次赋予概率, 概率只与元素的排序位置有关, 而与元素的取值无关。元素概率以类似几何概率的形式分布<i>p</i><sub><i>r</i></sub>=<i>α</i> (1-<i>α</i>) <sup><i>r</i>-1</sup>, 其中, <i>r</i>表示元素的排序位置, <i>α</i> (0&lt;<i>α</i>&lt;1) 为超参数, 表示排序为<i>r</i>的元素被选中的概率, 如图3所示 (取超参数<i>α</i>=0.5) 。</p>
                </div>
                <div class="area_img" id="97">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201904035_097.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 Rank-based stochastic pooling方法元素被选中概率" src="Detail/GetImg?filename=images/JSJC201904035_097.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图3 Rank-based stochastic pooling方法元素被选中概率</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201904035_097.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="98">根据该方法池化域中元素概率的计算公式, 当池化域中元素为零时, 被取中的概率不为0, 即任意一个元素都有被取中的可能, 因此Rank-based stochastic pooling方法成功改进Stochastic pooling的缺陷。但该池化方法只根据池化域中元素的排序位置来赋予概率, 也就是当池化域中元素相等时, 它们对应概率差别较大, 这样反向传播时对反向传播的方向有较大影响。</p>
                </div>
                <h4 class="anchor-tag" id="99" name="99">2.2 混合概率池化方法</h4>
                <div class="p1">
                    <p id="100">在2种随机池化方法的基础上, 综合2种随机池化的优点, 本文提出一种新的随机概率池化方法, 该方法在概率的赋予上提出了一种新的计算方法, 该计算方法具体描述如下:</p>
                </div>
                <div class="p1">
                    <p id="101">1) 将<i>n</i>×<i>n</i>的卷积特征图划分为<i>m</i>×<i>m</i>大小的互不相交的池化块, 先将池化域中的元素去重复, 然后对池化块 (无重复) 中的元素按从小到大排序, 设<i>a</i>是升序排列的数组, <i>r</i>为排序后元素的顺序<i>r</i>:<mathml id="102"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Τ</mi><mo>→</mo><mrow><mo>{</mo><mrow><mn>1</mn><mo>, </mo><mn>2</mn><mo>, </mo><mo>⋯</mo><mo>, </mo><mrow><mo>|</mo><mi>a</mi><mo>|</mo></mrow></mrow><mo>}</mo></mrow></mrow></math></mathml>, 且满足<mathml id="103"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>a</mi><mrow><mo> (</mo><mi>i</mi><mo>) </mo></mrow><mo>&lt;</mo><mrow><mi>a</mi><mrow><mo> (</mo><mi>j</mi><mo>) </mo></mrow></mrow><mo>⇒</mo><mi>r</mi><mrow><mo> (</mo><mi>i</mi><mo>) </mo></mrow><mo>&lt;</mo><mi>r</mi><mrow><mo> (</mo><mi>j</mi><mo>) </mo></mrow></mrow></math></mathml>。</p>
                </div>
                <div class="p1">
                    <p id="104">2) 对池化域中元素加入顺序影响因子, 计算如下:</p>
                </div>
                <div class="p1">
                    <p id="105" class="code-formula">
                        <mathml id="105"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>b</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mi>a</mi><msub><mrow></mrow><mi>i</mi></msub><mo>+</mo><mo stretchy="false"> (</mo><mi>β</mi><msup><mrow></mrow><mrow><mi>r</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msup><mo>-</mo><mn>1</mn><mo stretchy="false">) </mo><mo>, </mo><mi>i</mi><mo>∈</mo><mo stretchy="false"> (</mo><mn>1</mn><mo>, </mo><mn>2</mn><mo>, </mo><mo>⋯</mo><mo>, </mo><mrow><mo>|</mo><mi>a</mi><mo>|</mo></mrow><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="106">其中, β (β≥1) 为超参数, r<sub>i</sub>表示元素a<sub>i</sub>的顺序。</p>
                </div>
                <div class="p1">
                    <p id="107">3) 求加入顺序影响因子后的权重概率, 计算公式为:</p>
                </div>
                <div class="p1">
                    <p id="108" class="code-formula">
                        <mathml id="108"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>p</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mfrac><mrow><mi>b</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mrow><mo>|</mo><mi>a</mi><mo>|</mo></mrow></mrow></munderover><mi>b</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub></mrow></mfrac><mo>, </mo><mi>i</mi><mo>∈</mo><mo stretchy="false"> (</mo><mn>1</mn><mo>, </mo><mn>2</mn><mo>, </mo><mo>⋯</mo><mo>, </mo><mrow><mo>|</mo><mi>a</mi><mo>|</mo></mrow><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>4</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="109">4) 按多项分布进行取样, 得到池化后的值。</p>
                </div>
                <div class="p1">
                    <p id="110" class="code-formula">
                        <mathml id="110"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>s</mi><mo>=</mo><mi>a</mi><msub><mrow></mrow><mi>l</mi></msub><mo>, </mo><mi>l</mi><mo>∼</mo><mi>Μ</mi><mi>u</mi><mi>l</mi><mi>t</mi><mi>i</mi><mi>n</mi><mi>o</mi><mi>m</mi><mi>i</mi><mi>a</mi><mi>l</mi><mo stretchy="false"> (</mo><mi>p</mi><msub><mrow></mrow><mn>1</mn></msub><mo>, </mo><mi>p</mi><msub><mrow></mrow><mn>2</mn></msub><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>p</mi><msub><mrow></mrow><mrow><mrow><mo>|</mo><mi>a</mi><mo>|</mo></mrow></mrow></msub><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>5</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="111">图4为混合概率池化方法的示例图。</p>
                </div>
                <div class="area_img" id="112">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201904035_112.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 混合概率池化方法示例" src="Detail/GetImg?filename=images/JSJC201904035_112.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图4 混合概率池化方法示例</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201904035_112.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="113">图5所示为图4中的特征图在Stochastic pooling方法、Rank-based stochastic pooling方法以及本文方法上的概率分布 (其中, Rank-based stochastic pooling中超参数<i>α</i>取0.5, 本文混合概率随机池化中超参数<i>β</i>取1.5) 。</p>
                </div>
                <div class="area_img" id="114">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201904035_114.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 3种随机池化概率比较结果" src="Detail/GetImg?filename=images/JSJC201904035_114.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图5 3种随机池化概率比较结果</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201904035_114.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="115" name="115">2.3 方法可行性论证</h4>
                <div class="p1">
                    <p id="116">首先引入去重复, 这样可以保证对相等的元素具有相等的被选中的概率, 其次在基于权重的随机池化基础上加入了元素顺序影响因子<i>β</i> (<i>β</i>≥1) , 此时元素的选择概率<mathml id="117"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>p</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mfrac><mrow><mi>a</mi><msub><mrow></mrow><mi>i</mi></msub><mo>+</mo><mi>β</mi><msup><mrow></mrow><mrow><mi>r</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msup><mo>-</mo><mn>1</mn></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mrow><mo>|</mo><mi>a</mi><mo>|</mo></mrow></mrow></munderover><mo stretchy="false"> (</mo></mstyle><mi>a</mi><msub><mrow></mrow><mi>i</mi></msub><mo>+</mo><mi>β</mi><msup><mrow></mrow><mrow><mi>r</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msup><mo>-</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></mfrac></mrow></math></mathml>, 影响因子<i>β</i>是混合概率的关键因素, <i>β</i>越大, 表示越侧重元素的顺序, <i>β</i>越小 (接近1) , 表示越侧重元素的取值。该概率计算方法既考虑了元素的取值, 同时也考虑了元素值在池化域中的排序位置, 这样对池化域中任意的元素都赋予了被取中的概率, 同时也解决Rank-based stochastic pooling只考虑顺序而不考虑元素取值的问题。当 <i>β</i>=1时, 有<mathml id="118"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>p</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mfrac><mrow><mi>a</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mrow><mo>|</mo><mi>a</mi><mo>|</mo></mrow></mrow></munderover><mo stretchy="false"> (</mo></mstyle><mi>a</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo></mrow></mfrac></mrow></math></mathml>, 此时混合概率池化退化为Stochastic pooling;当<i>β</i>→∞时, 有<i>p</i><sub>max</sub>→1, 即最大元素被取中的概率接近1, 此时混合概率池化退化为最大池化。因此, 本文方法计算的概率是包含Stochastic pooling与最大池化的一种更一般的池化方法, 理论上应该会有较好的图像分类效果。图6给出<i>β</i>取不同值时, 池化域元素的概率分布情况。</p>
                </div>
                <div class="area_img" id="119">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201904035_119.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 不同超参数下池化域元素概率" src="Detail/GetImg?filename=images/JSJC201904035_119.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图6 不同超参数下池化域元素概率</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201904035_119.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="120">综上可知, 本文提出的混合概率池化的方法在前向、反向传播都是对最大池化和随机池化的综合与扩展, 该方法既综合了最大池化、Stochastic pooling及Rank-based stochastic pooling的优点, 又做了一定的改进, 增加了算法的灵活性与鲁棒性, 具有较好的泛化效果。</p>
                </div>
                <h3 id="121" name="121" class="anchor-tag">3 实验结果与分析</h3>
                <div class="p1">
                    <p id="122">为验证混合概率随机池化方法的有效性, 分别在3种不同的数据集MNIST、CIFAR-10、CIFAR-100上进行实验, 所有的实验卷积网络均采用图1的结构C-S-C-S-200FC, 分别表示第1层、第3层为卷积层, 第2层、第4层为池化层, 第5层全连接层, 此层神经元个数为200个。实验中加入了批量归一化, 在每次卷积后与激活作用之间加入批量归一化, 这样更有利于提高分类准确率<citation id="154" type="reference"><link href="39" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>。激活函数选用ReLU函数, 池化域大小为2×2方形区域。每组试验的权重初始值均采用高斯初始化方法, 反向传播采用随机梯度下降方法, 实验在pytorch框架下, 通过扩展函数实现本文池化方法, 并在GPU下运行。</p>
                </div>
                <h4 class="anchor-tag" id="123" name="123">3.1 MNIST数据集</h4>
                <div class="p1">
                    <p id="124">MNIST手写字体数据集共70 000张, 每张的数字均为0～9, 其中60 000张为训练数据集, 10 000张为测试数据集, 每张均分辨率为28像素×28像素的二维灰度图像。在实验时, 首先将数据归一化为[0, 1], 卷积层第1层、第3层的卷积核大小均为5×5, 卷积核个数分别为16个、40个, 学习率初始值为0.1, 动量为0.9。实验分别用最大池化、均值池化、Stochastic pooling、Rank-based stochastic pooling (超参数<i>α</i>=0.5) 以及本文混合概率随机池化方法 (超参数<i>β</i>=2) 进行, 表1所示为在3种不同初始值下, 各种不同池化方法训练80个周期后在测试集上的最高准确率。</p>
                </div>
                <div class="area_img" id="125">
                    <p class="img_tit"><b>表1 不同方法在MNIST数据集上的测试准确率</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note">%</p>
                    <table id="125" border="1"><tr><td>池化方法</td><td>随机种子1</td><td>随机种子2</td><td>随机种子3</td></tr><tr><td>最大池化</td><td>99.44</td><td>99.40</td><td>99.45</td></tr><tr><td><br />均值池化</td><td>99.28</td><td>99.29</td><td>99.20</td></tr><tr><td><br />L2池化</td><td>99.43</td><td>99.42</td><td>99.44</td></tr><tr><td><br />Stochastic pooling</td><td>99.45</td><td>99.39</td><td>99.47</td></tr><tr><td><br />Rank-based Stochastic pooling</td><td>99.46</td><td>99.41</td><td>99.46</td></tr><tr><td><br />混合概率随机池化</td><td>99.50</td><td>99.49</td><td>99.49</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="126">从表1可以看出, 在各种不同初始条件下, 混合概率随机池化方法比其他池化方法有较好的测试效果。</p>
                </div>
                <h4 class="anchor-tag" id="127" name="127">3.2 CIFAR-10数据集</h4>
                <div class="p1">
                    <p id="128">CIFAR-10是分辨率为32像素×32像素的三维彩色图数据集, 共含有10个类别, 每个类别含有50 000张训练图像, 10 000张测试图像。实验首先将图像数据归一化为[0, 1], 然后进行减均值的操作。由于显存限制, 在CIFAR-10数据集训练中, 卷积层第1层、第3层卷积核的大小5×5, 卷积分别为16个、32个, 学习率大小为0.001, 动量为0.9。</p>
                </div>
                <h4 class="anchor-tag" id="129" name="129">3.3 CIFAR-100数据集</h4>
                <div class="p1">
                    <p id="130">CIFAR-100数据集与CIFAR-10数据集是分辨率为32像素×32像素的三维彩色图数据集, 但它含有100个类别, 每个类别含有500张训练图像, 100张测试图像。与CIFAR-10数据集一样, 总共含有50 000张训练图, 10 000张测试图, 只是此时每个类的训练图像更少, 因此从理论上说, 测试的准确率相对较低。在实验时, CIFAR-100的卷积网络结构、卷积核大小、学习率与CIFAR-10实验完全一致, 但卷结核个数减少为16个、28个。表2所示为80个周期后各种不同池化方法在测试集上的准确率。</p>
                </div>
                <div class="area_img" id="131">
                    <p class="img_tit"><b>表2 不同方法的测试准确率</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note">%</p>
                    <table id="131" border="1"><tr><td><br />池化方法</td><td>CIFAR-10</td><td>CIFAR-100</td></tr><tr><td><br />最大池化</td><td>71.30</td><td>38.83</td></tr><tr><td><br />均值池化</td><td>70.71</td><td>38.51</td></tr><tr><td><br />L2池化</td><td>71.45</td><td>38.87</td></tr><tr><td><br />Stochastic pooling</td><td>71.90</td><td>38.94</td></tr><tr><td><br />Rank-based Stochastic pooling</td><td>72.02</td><td>38.93</td></tr><tr><td><br />混合概率随机池化</td><td>72.25</td><td>39.05</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="132">通过上述实验分析可知, 在不同初始条件下与不同数据集中, 本文混合概率池化均能达到更佳的分类效果, 因此, 本文方法是较为合理的卷积神经网络池化方法。</p>
                </div>
                <h3 id="133" name="133" class="anchor-tag">4 超参数选择</h3>
                <div class="p1">
                    <p id="134">超参数<i>β</i>是混合概率池化方法的关键因子, 它是最大池化和Stochastic pooling这2种方法的调节因素。通过前面的分析可知, <i>β</i>越大, 越倾向于最大池化, <i>β</i>越小 (接近1) 越倾向于Stochastic pooling。图7所示为<i>β</i>分别取{1, 1.3, 1.5, 2, 2.5, 3, 5, 10}时, 在CIFAR-10数据集上训练80个周期后的测试准确率。从图7可以看出, 当超参数取1.5时, 测试准确率最高, 达到72.77%。</p>
                </div>
                <div class="area_img" id="135">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201904035_135.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 不同超参数下CIFAR-10数据集的测试准确率" src="Detail/GetImg?filename=images/JSJC201904035_135.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图7 不同超参数下CIFAR-10数据集的测试准确率</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201904035_135.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h3 id="136" name="136" class="anchor-tag">5 结束语</h3>
                <div class="p1">
                    <p id="137">本文通过分析卷积神经网络池化层的工作原理, 对各种池化方法的优缺点进行对比, 提出一种混合概率随机池化方法, 并从理论上分析了该方法的合理性与有效性, 将该方法与多种经典的池化方法进行比较, 结果证明, 本文方法在不同数据集上均具有较好的分类效果。下一步将对超参数进行讨论, 针对不同数据集自适应调整参数, 以提高分类准确率。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CBBD&amp;filename=9787560628097001&amp;v=Mjc5ODZzUERSTTh6eFVTbURkOVNIN24zeEU5ZmJ2bktyaWZadTl1RkN2alU3akxJVjhXWEZxekdiYStIdGZPcDQ5TVkr&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> 郭宝龙, 孙伟.数字图像处理系统工程导论[M].西安:西安电子科技大学出版社, 2012:17-20.
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJBK&amp;filename=SJBK15090500012538&amp;v=MTU2NDFpZkpaYks5SHRqTXFvOUZaT29OQ1g4eG9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVUxuSUoxMFZheEE9Tg==&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> HINTON G E, OSINDERO S, TEH Y W.A fast learning algorithm for deep belief nets[J].Neural Computation, 2006, 18 (7) :1527-1554.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Greedy layer-wise training of deep networks">

                                <b>[3]</b> BENGIO Y, LAMBLIN P, POPOVICI D, et al.Greedy layer-wise training of deep networks[C]//Proceedings of the 19th International Conference on Neural Information Processing Systems.Cambridge, USA:MIT Press, 2007:153-160.
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image Net Classification with Deep Convolutional Neural Networks">

                                <b>[4]</b> KRIZHEVSKY A, SUTSKEVER I, HILTON G E.ImageNet classification with deep convolutional neural networks[C]//Proceedings of the 25th International Conference on Neural Information Processing Systems.[S.l.]:Curran Associates Inc., 2012:1097-1105.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Evaluation of pooling operations in convolutional architectures for object recognition">

                                <b>[5]</b> SCHERER D, MULLER A, BEHNKE S.Evaluation of pooling operation in convolutional architecture for object recognition[C]//Proceedings of the 20th International Conference on Artificial Neural Networks.Berlin, Germany:Springer, 2010:92-101.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A theoretical analysis of feature pooling in visual recognition">

                                <b>[6]</b> BOUREAU Y L, PONCE J, LECUN Y.A theoretical analysis of feature pooling in visual recognition [EB/OL].[2017-12-10].http://yann.lecun.com/exdb/publis/pdf/boureau-icml-10.pdf.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=End-to-end text recognition w ith convolutional neural netw orks">

                                <b>[7]</b> WANG T, WU D, COATES A.et al.End-to-end text recognition with convolutional neural networks[C]//Proceedings of the 21st International Conference on Pattern Recognition.Washington D.C., USA:IEEE Press, 2012:3304-3308.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Linear spatial pyramid matc-hing using sparse coding for image classification">

                                <b>[8]</b> YANG J C, YU K, GONG Y H, et al.Linear spatial pyramid matching using coding for image classifica-tion[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2009:1794-1801.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Unsuper-vised learning of invariant feature hierarchies with applications to ob-ject recognition">

                                <b>[9]</b> RANZATO M, HUANG F J, BOUREAU Y, et al.Unsupervised learning of invariant feature hierarchies with applications to object recognition[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2007.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Stochastic pooling for regulariza-tion of deep convolutional neural networks">

                                <b>[10]</b> ZEILER M D, FERGUS R.Stochastic pooling for regulariza-tion of deep convolutional neural networks[EB/OL].[2017-12-10].https://arxiv.org/pdf/1301.3557.pdf.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Convolutional Neural Networks Applied To House Numbers Digit Classification">

                                <b>[11]</b> SERMANET P, CHINTALA S, LECUN Y.Convolutional neural networks applied to house numbers digit classification[C]//Proceedings of the 21st International Conference on Pattern Recognition.Washington D.C., USA:IEEE Press, 2012.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_12" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GCTX201605017&amp;v=MjY4ODI3ZmRyRzRIOWZNcW85RVk0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSb0Z5L21VcjdLSWk=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[12]</b> 余萍, 赵继生.基于矩阵2-范数的卷积神经网络图像识别算法[J].图学学报, 2016, 37 (5) :694-701.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Mixed pooling for convolutional neural networks">

                                <b>[13]</b> YU D J, WANG H L, CHEN P Q, et al.Mixed pooling for convolutional neural network[C]//Proceedings of the 9th International Conference on Rough Sets and Knowledge Technology.Berlin, Germany:Springer, 2014:364-375.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Rank-based pooling for deep convolutional neural networks">

                                <b>[14]</b> SHI Z L, YE Y D, WU Y P.Rank-based pooling for deep convolutional neural network[J].Neural Networks, 2016, 83:21-31.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Spatial pyramid pooling in deep convolutional networks for visual recognition">

                                <b>[15]</b> HE K, ZHANG X, REN S, et al.Spatial pyramid pooling in deep convolutional networks for visual recognition[C]//Proceedings of the 13th European Conference on Computer Vision.Berlin, Germany:Springer, 2014:346-361.
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_16" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSYJ201208003&amp;v=MzE3MzlLTHo3U1pMRzRIOVBNcDQ5Rlo0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSb0Z5L21Vcjc=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[16]</b> 孙志军, 薛磊, 许阳明, 等.深度学习研究综述[J].计算机应用研究, 2012, 29 (8) :2806-2810.
                            </a>
                        </p>
                        <p id="35">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Visualizing and understanding convolutional networks">

                                <b>[17]</b> ZEILER M D, FERGUS R.Visualizing and understanding convolutional networks[C]//Proceedings of the 13th European Conference on Computer Vision.Berlin, Germany:Springer, 2014:818-833.
                            </a>
                        </p>
                        <p id="37">
                            <a id="bibliography_18" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201711031&amp;v=MDE0NTBiTnJvOUdaWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm9GeS9tVXI3S0x6N0JiYkc0SDk=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[18]</b> 陈玄, 朱荣, 王中元.基于融合卷积神经网络模型的手写数字识别[J].计算机工程, 2017, 42 (11) :187-192.
                            </a>
                        </p>
                        <p id="39">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Batch normalization:Accelerating deep network training by reducing internal covariate shift">

                                <b>[19]</b> IOFFE S, SZEGEDY C.Batch normalization:accelerating deep network training by reducing internal covariate shift[C]//Proceedings of the 32nd International Conference on Machine Learning.[S.l.]:JMLR, 2015:448-456.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJC201904035" />
        <input id="dpi" type="hidden" value="600" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201904035&amp;v=MjM3OTMzenFxQnRHRnJDVVJMT2VaZVJvRnkvbVVyN0xMejdCYmJHNEg5ak1xNDlHWVlRS0RIODR2UjRUNmo1NE8=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="1" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
