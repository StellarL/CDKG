<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637130604021056250%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJC201904034%26RESULT%3d1%26SIGN%3d9aRTKZV3HlJQmGKgnDgDedXJ3ow%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201904034&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201904034&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201904034&amp;v=MDI2NDc0SDlqTXE0OUdZSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm9GeS9sVzd2Qkx6N0JiYkc=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#35" data-title="0 概述 ">0 概述</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#40" data-title="1 LToT模型 ">1 LToT模型</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#41" data-title="1.1 基于文档标题特征的权重计算改进">1.1 基于文档标题特征的权重计算改进</a></li>
                                                <li><a href="#49" data-title="1.2 模型生成过程">1.2 模型生成过程</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#91" data-title="2 实验结果与分析 ">2 实验结果与分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#92" data-title="2.1 实验环境与实验数据">2.1 实验环境与实验数据</a></li>
                                                <li><a href="#99" data-title="2.2 模型泛化能力分析实验">2.2 模型泛化能力分析实验</a></li>
                                                <li><a href="#111" data-title="2.3 词语权重变化分析实验">2.3 词语权重变化分析实验</a></li>
                                                <li><a href="#117" data-title="2.4 主题演化分析实验">2.4 主题演化分析实验</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#125" data-title="3 结束语 ">3 结束语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#51" data-title="&lt;b&gt;图1 LToT模型概率图模型&lt;/b&gt;"><b>图1 LToT模型概率图模型</b></a></li>
                                                <li><a href="#108" data-title="&lt;b&gt;图2 3种模型在搜狗2008数据集上的困惑度对比&lt;/b&gt;"><b>图2 3种模型在搜狗2008数据集上的困惑度对比</b></a></li>
                                                <li><a href="#109" data-title="&lt;b&gt;图3 3种模型在搜狗2012数据集上的困惑度对比&lt;/b&gt;"><b>图3 3种模型在搜狗2012数据集上的困惑度对比</b></a></li>
                                                <li><a href="#113" data-title="&lt;b&gt;表1 不同模型对教育标签下的词项权重对比结果&lt;/b&gt;"><b>表1 不同模型对教育标签下的词项权重对比结果</b></a></li>
                                                <li><a href="#116" data-title="&lt;b&gt;表2 不同模型的词项权重对比结果&lt;/b&gt;"><b>表2 不同模型的词项权重对比结果</b></a></li>
                                                <li><a href="#119" data-title="&lt;b&gt;图4 加权LToT模型演化趋势&lt;/b&gt;"><b>图4 加权LToT模型演化趋势</b></a></li>
                                                <li><a href="#120" data-title="&lt;b&gt;图5 ToT模型演化趋势&lt;/b&gt;"><b>图5 ToT模型演化趋势</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" >
                                        <b>[1]</b>
                                     BLEI D M, NG A Y, JORDAN M I.Latent Dirichlet allocation[J].Journal of Machine Learning Research, 2003, 3:993-1022.</a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" title=" BLEI D M.Probabilistic topic models[J].Communications of the ACM, 2012, 55 (4) :77-84." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000004805&amp;v=MDEyMzc0OUZaT3NMQkh3OG9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVUxuSUoxNGNiaHM9TmlmSVk3SzdIdGpOcg==&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         BLEI D M.Probabilistic topic models[J].Communications of the ACM, 2012, 55 (4) :77-84.
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title=" BLEI D M, LAFFERTY J D.Dynamic topic models[C]//Proceedings of the 23rd International Conference on Machine Learning.New York, USA:ACM Press, 2006:113-120." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Dynamic topic models">
                                        <b>[3]</b>
                                         BLEI D M, LAFFERTY J D.Dynamic topic models[C]//Proceedings of the 23rd International Conference on Machine Learning.New York, USA:ACM Press, 2006:113-120.
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" title=" WANG X, MCCALLUM A.Topics over time:a non-Markov continuous-time model of topical trends[C]//Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.New York, USA:ACM Press, 2006:424-433." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Topics over Time:A Non-Markov Continuous-Time Model of Topical Trends">
                                        <b>[4]</b>
                                         WANG X, MCCALLUM A.Topics over time:a non-Markov continuous-time model of topical trends[C]//Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.New York, USA:ACM Press, 2006:424-433.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" title=" 刘良选, 黄梦醒.一种面向词汇突发的连续时间主题模型[J].计算机工程, 2016, 42 (11) :195-201." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201611033&amp;v=MDA1NTNvOUdaNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm9GeS9sVzd2Qkx6N0JiYkc0SDlmTnI=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                         刘良选, 黄梦醒.一种面向词汇突发的连续时间主题模型[J].计算机工程, 2016, 42 (11) :195-201.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" title=" RAMAGE D, HALL D, NALLAPATI R, et al.Labeled LDA:a supervised topic model for credit attribution in multi-labeled corpora[C]//Proceedings of 2009 Con-ference on Empirical Methods in Natural Language Processing.New York, USA:ACM Press, 2009:248-256." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Labeled LDA:a supervised topic model for credit attribution in multi-labeled corpora">
                                        <b>[6]</b>
                                         RAMAGE D, HALL D, NALLAPATI R, et al.Labeled LDA:a supervised topic model for credit attribution in multi-labeled corpora[C]//Proceedings of 2009 Con-ference on Empirical Methods in Natural Language Processing.New York, USA:ACM Press, 2009:248-256.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" title=" RAMAGE D, MANNING C D, DUMAIS S.Partially labeled topic models for interpretable text mining[C]//Proceedings of ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.New York, USA:ACM Press, 2011:457-465." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Partially labeled topic models for interpretabletext mining">
                                        <b>[7]</b>
                                         RAMAGE D, MANNING C D, DUMAIS S.Partially labeled topic models for interpretable text mining[C]//Proceedings of ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.New York, USA:ACM Press, 2011:457-465.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" title=" RAMAGE D, HEYMANN P, MANNING C D, et al.Clustering the tagged Web[C]//Proceedings of the 2nd ACM International Conference on Web Search and Data Mining.New York, USA:ACM Press, 2009:54-63." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Clustering the Tagged Web">
                                        <b>[8]</b>
                                         RAMAGE D, HEYMANN P, MANNING C D, et al.Clustering the tagged Web[C]//Proceedings of the 2nd ACM International Conference on Web Search and Data Mining.New York, USA:ACM Press, 2009:54-63.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" title=" 张小平, 周雪忠, 黄厚宽, 等.一种改进的LDA主题模型[J].北京交通大学学报, 2010, 34 (2) :111-114." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=BFJT201002026&amp;v=MzEwMjZHNEg5SE1yWTlIWW9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJvRnkvbFc3dkJKeXZCZXI=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                         张小平, 周雪忠, 黄厚宽, 等.一种改进的LDA主题模型[J].北京交通大学学报, 2010, 34 (2) :111-114.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_10" title=" SOUCY P, MINEAU G W.Beyond TFIDF weighting for text categorization in the vector space model[C]//Proceedings of the 19th International Joint Conference on Artificial Intelligence.San Francisco, USA:Morgan Kaufmann Publishers Inc., 2005:1130-1135." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Beyond TFIDF weighting for text categorization in the vector space model">
                                        <b>[10]</b>
                                         SOUCY P, MINEAU G W.Beyond TFIDF weighting for text categorization in the vector space model[C]//Proceedings of the 19th International Joint Conference on Artificial Intelligence.San Francisco, USA:Morgan Kaufmann Publishers Inc., 2005:1130-1135.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_11" title=" CHEN K, ZHANG Z, LONG J, et al.Turning from TF-IDF to TF-IGM for term weighting in text classification[J].Expert Systems with Applications, 2016, 66:245-260." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Turning from TF-IDF to TFIGM for term weighting in text classification">
                                        <b>[11]</b>
                                         CHEN K, ZHANG Z, LONG J, et al.Turning from TF-IDF to TF-IGM for term weighting in text classification[J].Expert Systems with Applications, 2016, 66:245-260.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_12" title=" 侯汉清, 章成志, 郑红.Web概念挖掘中标引源加权方案初探[J].情报学报, 2005, 24 (1) :87-92." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=QBXB20050100E&amp;v=MjYyOTlLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm9GeS9sVzd2Qk5DL1RiTEc0SHRUTXJvOUZFWVE=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[12]</b>
                                         侯汉清, 章成志, 郑红.Web概念挖掘中标引源加权方案初探[J].情报学报, 2005, 24 (1) :87-92.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_13" title=" 张宏毅, 王立威, 陈瑜希.概率图模型研究进展综述[J].软件学报, 2013, 24 (11) :2476-2497." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=RJXB201311002&amp;v=MDE4MzA1NE8zenFxQnRHRnJDVVJMT2VaZVJvRnkvbFc3dkJOeWZUYkxHNEg5TE5ybzlGWm9RS0RIODR2UjRUNmo=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[13]</b>
                                         张宏毅, 王立威, 陈瑜希.概率图模型研究进展综述[J].软件学报, 2013, 24 (11) :2476-2497.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_14" title=" BOWMAN K O, SHENTON L R.Parameter estimation for the Beta distribution[J].Journal of Statistical Computation and Simulation, 2007, 43 (3/4) :217-228." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=STJD&amp;filename=STJD772448042&amp;v=MDIwNTBYSXA0OUJab1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm9GeS9sVzd2Qk5qbkJhclMvSE4=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[14]</b>
                                         BOWMAN K O, SHENTON L R.Parameter estimation for the Beta distribution[J].Journal of Statistical Computation and Simulation, 2007, 43 (3/4) :217-228.
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_15" title=" GUO X, XIANG Y, CHEN Q, et al.LDA-based online topic detection using tensor factorization[J].Journal of Information Science, 2013, 39 (4) :459-469." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJRS&amp;filename=SJRS13080700012110&amp;v=MTgzMDVJOUZaT29ORFgwNW9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVUxuSUoxNGNiaHM9TmlmWmZiSzdIdG5NcQ==&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[15]</b>
                                         GUO X, XIANG Y, CHEN Q, et al.LDA-based online topic detection using tensor factorization[J].Journal of Information Science, 2013, 39 (4) :459-469.
                                    </a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_16" title=" WEI X, CROFT W B.LDA-based document models for ad-hoc retrieval[C]//Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval.New York, USA:ACM Press, 2006:178-185." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=LDA-based document models for ad-hoc retrieval">
                                        <b>[16]</b>
                                         WEI X, CROFT W B.LDA-based document models for ad-hoc retrieval[C]//Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval.New York, USA:ACM Press, 2006:178-185.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJC" target="_blank">计算机工程</a>
                2019,45(04),205-210+216 DOI:10.19678/j.issn.1000-3428.0050169            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>一种基于标签的改进主题演化模型</b></span>
 <span class="shoufa"></span>                                     </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%A7%9A%E7%AB%8B&amp;code=37558770&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">姚立</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BC%A0%E6%9B%A6%E7%85%8C&amp;code=07770640&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">张曦煌</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%B1%9F%E5%8D%97%E5%A4%A7%E5%AD%A6%E7%89%A9%E8%81%94%E7%BD%91%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0074200&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">江南大学物联网工程学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>传统主题演化 (ToT) 模型通常忽略原始数据中的标签元信息。为此, 建立一种基于标签的改进ToT模型。针对传统权重算法忽略词汇在文档集类别间和类别内的分布对权重产生影响的问题, 结合文档标题特征, 使用改进词频-反重力距算法进行权重分析, 以扩展模型的生成过程。在ToT模型的基础上引入原始文档的标签属性, 构建改进模型并使用吉布斯采样算法估计其参数。实验结果表明, 与ToT模型相比, 该模型具有较高的泛化能力。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%A0%87%E7%AD%BE&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">标签;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%B8%BB%E9%A2%98%E6%BC%94%E5%8C%96%E6%A8%A1%E5%9E%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">主题演化模型;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%9A%90%E7%8B%84%E5%88%A9%E5%85%8B%E9%9B%B7%E5%88%86%E9%85%8D&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">隐狄利克雷分配;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%AF%8D%E9%A2%91-%E5%8F%8D%E9%87%8D%E5%8A%9B%E8%B7%9D%E7%AE%97%E6%B3%95&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">词频-反重力距算法;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%90%89%E5%B8%83%E6%96%AF%E9%87%87%E6%A0%B7&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">吉布斯采样;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    姚立 (1991—) , 男, 硕士, 主研方向为数据挖掘、主题模型, E-mail:yl_jnu@126.com;;
                                </span>
                                <span>
                                    张曦煌, 教授、博士。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-01-18</p>

                    <p>

                            <b>基金：</b>
                                                        <span>江苏省产学研合作项目 (BY2015019-30);</span>
                    </p>
            </div>
                    <h1><b>An Improved Topics over Time Model Based on Label</b></h1>
                    <h2>
                    <span>YAO Li</span>
                    <span>ZHANG Xihuang</span>
            </h2>
                    <h2>
                    <span>School of Internet of Things Engineering, Jiangnan University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Traditional Topics over Time (ToT) models usually ignore label meta-information in the original data.To solve this problem, an improved ToT model based on label is established.Aiming at the problem that traditional weighting algorithms ignore the influence of vocabulary distribution among and within document sets on weights, combined with the characteristics of document titles, an improved TF-IGM algorithm is used to analyze the weights to extend the generation process of the model.Based on the ToT model, the label attributes of the original document are introduced to construct the improved model and estimate its parameters using Gibbs sampling algorithm.Experimental results show that the proposed model has higher generalization ability than the ToT model.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=label&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">label;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Topics%20over%20Time%20(ToT)%20model&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Topics over Time (ToT) model;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Latent%20Dirichlet%20Allocation%20(LDA)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Latent Dirichlet Allocation (LDA) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=TF-IGM%20algorithm&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">TF-IGM algorithm;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Gibbs%20sampling&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Gibbs sampling;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-01-18</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="35" name="35" class="anchor-tag">0 概述</h3>
                <div class="p1">
                    <p id="36">随着信息技术的发展, 网络上汇聚了大量的文本, 有效利用这些文本数据成为当前机器学习领域的一个重要课题。</p>
                </div>
                <div class="p1">
                    <p id="37">文献<citation id="127" type="reference">[<a class="sup">1</a>]</citation>建立一种隐狄利克雷分配 (Latent Dirichlet Allocation, LDA) 模型, 由于其良好的泛化能力, 已经被广泛应用于自然语言处理领域<citation id="128" type="reference"><link href="5" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>。但是, LDA主题模型属于无监督学习, 其在学习训练的过程中会忽略在实际中大量存在的文档附属元信息, 如时间、标签等。如何利用这些元信息来挖掘更有价值的信息已经成为该领域的研究热点。文献<citation id="129" type="reference">[<a class="sup">3</a>]</citation>提出一种满足一阶马尔科夫假设的动态主题模型 (Dynamic Topic Model, DTM) , 但是该模型将时间离散化, 导致其对选取的时间粒度较敏感。文献<citation id="130" type="reference">[<a class="sup">4</a>]</citation>建立一种主题演化 (Topics over Time, ToT) 模型, 该模型不依赖马尔科夫假设, 未将时间离散化, 且其中每个主题都与一个连续的时间戳分布相关, 使模型能够发现主题随时间的变化关系。文献<citation id="131" type="reference">[<a class="sup">5</a>]</citation>借鉴ToT模型思想, 建立一种面向词汇突发的连续时间主题模型。文献<citation id="132" type="reference">[<a class="sup">6</a>]</citation>建立一种标签主题模型, 解决了文档与多个标签之间的关联问题, 但是, 该模型仅将主题数量设置成标签的数量, 忽略了文档中潜在的主题, 造成模型与文本之间的拟合不足, 即其泛化能力较差。文献<citation id="133" type="reference">[<a class="sup">7</a>]</citation>建立一种新的基于标签的主题模型PLDA, 其不仅将文档与标签映射成一对多的关联, 而且将标签与多个主题关联。该模型更加符合真实世界中的文档分布特点, 具有良好的通用性。</p>
                </div>
                <div class="p1">
                    <p id="38">但是, 上述模型都是在标准LDA模型的基础上建立的, 均没有考虑加权策略。针对LDA的加权策略, 文献<citation id="134" type="reference">[<a class="sup">8</a>]</citation>提出一种文档标签词频加权策略, 其目的是将标签中具有相同语义的词项进行合并以表示文档。文献<citation id="135" type="reference">[<a class="sup">9</a>]</citation>指出文档集中的词项符合幂律分布, 其使LDA模型中的主题分布倾向高频词项, 导致能够代表主题的多数词项不能被挖掘出来。因此, 在LDA模型中考虑权重非常有必要。</p>
                </div>
                <div class="p1">
                    <p id="39">本文建立一种基于标签的主题演化模型LToT。在ToT模型的生成过程中, 没有使用标签进行约束导致ToT模型会在不属于自己标签的主题上进行分配, 为此, 本文LToT模型结合PLDA模型的优势, 将标签引入到ToT模型中。传统加权策略忽略了词项在文档集类别间和类别内的分布对权重产生的影响, 以及文档标题词项的重要性。针对该问题, LToT模型在ToT模型的生成过程中结合并改进基于词频-反重力矩 (TF-IGM) 算法, 以提升文档标题词项在模型中的权重。</p>
                </div>
                <h3 id="40" name="40" class="anchor-tag">1 LToT模型</h3>
                <h4 class="anchor-tag" id="41" name="41">1.1 基于文档标题特征的权重计算改进</h4>
                <div class="p1">
                    <p id="42">传统基于词频-逆文档频率 (TF-IDF) 算法是目前被广泛采用的文档权值计算方法, 但是该算法把文档看作一个整体, 没有考虑到词项在文档集类别间与类别内的分布会对权重产生影响<citation id="136" type="reference"><link href="21" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>, 从而导致计算结果准确度较低。针对该问题, 文献<citation id="137" type="reference">[<a class="sup">11</a>]</citation>提出一种TF-IGM算法。TF-IGM是一种有监督的文档权重计算方法, 其利用文档的类别, 充分考虑类内分布与类间分布对权重的影响。该算法的表达式如下:</p>
                </div>
                <div class="p1">
                    <p id="43" class="code-formula">
                        <mathml id="43"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>W</mi><mo stretchy="false"> (</mo><mi>v</mi><mo>, </mo><mi>m</mi><mo stretchy="false">) </mo><mo>=</mo><msqrt><mrow><mi>t</mi><msub><mrow></mrow><mrow><mi>f</mi><msub><mrow></mrow><mrow><mi>v</mi><mi>m</mi></mrow></msub></mrow></msub></mrow></msqrt><mo>⋅</mo><mrow><mo> (</mo><mrow><mn>1</mn><mo>+</mo><mi>λ</mi><mo>⋅</mo><mfrac><mrow><mi>F</mi><msub><mrow></mrow><mn>1</mn></msub></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>r</mi><mo>=</mo><mn>1</mn></mrow><mi>c</mi></munderover><mo stretchy="false"> (</mo></mstyle><mi>F</mi><msub><mrow></mrow><mi>r</mi></msub><mo>⋅</mo><mi>r</mi><mo stretchy="false">) </mo></mrow></mfrac></mrow><mo>) </mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="44">其中, <i>t</i><sub><i>f</i><sub><i>vm</i></sub></sub>表示词<i>v</i>在文档<i>m</i>中的词频, <i>λ</i>为可调节系数, 取值为6.0～7.0。词<i>v</i>在各文本类别中的出现频次按从大到小排序为<i>F</i><sub>1</sub>≥<i>F</i><sub>2</sub>≥…≥<i>F</i><sub><i>m</i></sub>, <i>F</i><sub><i>r</i></sub> (<i>r</i>=1, 2, …, <i>m</i>) 为排序后词项<i>v</i>在第<i>r</i>个文本类别中出现的频次, <i>c</i>表示文本的类别数, (<i>F</i><sub><i>r</i></sub>·<i>r</i>) 表示重力矩元素。</p>
                </div>
                <div class="p1">
                    <p id="45">常见的文档属性除时间和标签外, 还有文档标题。相比在正文中出现的词项, 一个在文档标题中出现的词项对文档有更高的概括, 其所包含的信息量也更高。因此, 文档标题中的词应比正文中的词具有更高的权重。文献<citation id="138" type="reference">[<a class="sup">12</a>]</citation>的研究结果表明, 在文本表示能力的重要性上, 标题中的词项与正文中的词项比例约为5∶2。因此, 应针对词项在文档中的不同位置, 使用不同的权重对其进行赋值。文本给出如下的权重计算公式:</p>
                </div>
                <div class="p1">
                    <p id="46">令<i>TW</i>为标题中的词项集合, <i>BW</i>为正文中的词项集合, <i>W</i><sub>new</sub> (<i>v</i>, <i>m</i>) =<i>μ</i>·<i>W</i> (<i>v</i>, <i>m</i>) , 当<i>v</i>∈<i>TW</i>时, <i>μ</i>=0.7, 当<i>v</i>∈<i>BW</i>时, <i>μ</i>=0.3。</p>
                </div>
                <div class="p1">
                    <p id="47">对<i>W</i><sub>new</sub>进行归一化处理, 组成文档中新的权重值:</p>
                </div>
                <div class="p1">
                    <p id="48" class="code-formula">
                        <mathml id="48"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>w</mi><mo stretchy="false"> (</mo><mi>v</mi><mo>, </mo><mi>m</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mrow><mi>W</mi><msub><mrow></mrow><mrow><mtext>n</mtext><mtext>e</mtext><mtext>w</mtext></mrow></msub><mo stretchy="false"> (</mo><mi>v</mi><mo>, </mo><mi>m</mi><mo stretchy="false">) </mo></mrow><mrow><msqrt><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>v</mi><mo>∈</mo><mi>m</mi></mrow></munder><mi>W</mi></mstyle><msub><mrow></mrow><mrow><mtext>n</mtext><mtext>e</mtext><mtext>w</mtext></mrow></msub><mrow><mo stretchy="false"> (</mo><mi>v</mi><mo>, </mo><mi>m</mi><mo stretchy="false">) </mo></mrow><msup><mrow></mrow><mn>2</mn></msup></mrow></msqrt></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <h4 class="anchor-tag" id="49" name="49">1.2 模型生成过程</h4>
                <div class="p1">
                    <p id="50">LToT模型对文档集合、标签、主题、词项和时间进行联合建模, 其概率图模型如图1所示。其中, 阴影圆形内为已知的观察值, 白色圆形内表示未知的隐含变量, 箭头表示各变量之间的依赖关系, 正方形内的<i>K</i>值表示重复的次数<citation id="139" type="reference"><link href="27" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>, 希腊字母<i>α</i>、<i>β</i>、<i>ω</i>、<i>μ</i>为超参数。</p>
                </div>
                <div class="area_img" id="51">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201904034_051.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 LToT模型概率图模型" src="Detail/GetImg?filename=images/JSJC201904034_051.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图1 LToT模型概率图模型</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201904034_051.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="52">令文档集中一共有<i>M</i>篇文档, <i>M</i>={1, 2, …, <i>m</i>}, 该<i>M</i>篇文档一共有<i>V</i>个词项, <i>V</i>={1, 2, …, <i>v</i>}, 一共存在<i>L</i>个标签, <i>L</i>={1, 2, …, <i>l</i>}。对于文档集中的文档<i>m</i>, 已知其标签<i>Λ</i><sub><i>m</i></sub>∈<i>L</i>, 文档<i>m</i>中的第<i>l</i>个标签对应<i>K</i><sub><i>l</i></sub>个主题, 则LToT模型文档生成过程伪代码如下:</p>
                </div>
                <div class="p1">
                    <p id="53"><b>算法1</b> LToT模型文档生成过程</p>
                </div>
                <div class="p1">
                    <p id="54">1.对于每个标签下隐含的主题K∈{1, 2, …, k}, 从以μ为超参数的狄利克雷先验分布中构建主题关于词项的多项分布:<mathml id="55"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mrow><mtext>ϕ</mtext><msub><mrow></mrow><mrow><mtext>l</mtext><mo>, </mo><mtext>k</mtext></mrow></msub></mrow><mo stretchy="true">→</mo></mover><mo>=</mo><mo stretchy="false"> (</mo><mtext>ϕ</mtext><msub><mrow></mrow><mrow><mtext>l</mtext><mo>, </mo><mtext>k</mtext><mo>, </mo><mn>1</mn></mrow></msub><mo>, </mo><mtext>ϕ</mtext><msub><mrow></mrow><mrow><mtext>l</mtext><mo>, </mo><mtext>k</mtext><mo>, </mo><mn>2</mn></mrow></msub><mo>, </mo><mo>⋯</mo><mo>, </mo><mtext>ϕ</mtext><msub><mrow></mrow><mrow><mtext>l</mtext><mo>, </mo><mtext>k</mtext><mo>, </mo><mtext>V</mtext></mrow></msub><mo stretchy="false">) </mo><mo>∼</mo><mtext>D</mtext><mtext>i</mtext><mtext>r</mtext><mo stretchy="false"> (</mo><mtext>μ</mtext><mo stretchy="false">) </mo></mrow></math></mathml></p>
                </div>
                <div class="p1">
                    <p id="56">2.对于每一篇文档m∈{1, 2, …, M}:</p>
                </div>
                <div class="p1">
                    <p id="57">1) 从以β为超参数的狄利克雷先验分布中构建该文档m关于标签的多项分布:</p>
                </div>
                <div class="p1">
                    <p id="58" class="code-formula">
                        <mathml id="58"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mrow><mtext>ψ</mtext><msub><mrow></mrow><mtext>m</mtext></msub></mrow><mo stretchy="true">→</mo></mover><mo>=</mo><mo stretchy="false"> (</mo><mtext>ψ</mtext><msub><mrow></mrow><mrow><mtext>m</mtext><mo>, </mo><mn>1</mn></mrow></msub><mo>, </mo><mtext>ψ</mtext><msub><mrow></mrow><mrow><mtext>m</mtext><mo>, </mo><mn>2</mn></mrow></msub><mo>, </mo><mo>⋯</mo><mo>, </mo><mtext>ψ</mtext><msub><mrow></mrow><mrow><mtext>m</mtext><mo>, </mo><mtext>L</mtext></mrow></msub><mo stretchy="false">) </mo><mo>∼</mo><mtext>D</mtext><mtext>i</mtext><mtext>r</mtext><mo stretchy="false"> (</mo><mtext>β</mtext><mo>, </mo><mtext>Λ</mtext><msub><mrow></mrow><mtext>m</mtext></msub><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="59">2) 从以α为超参数的狄利克雷先验分布中构建该文档下所有标签关于主题的多项分布:</p>
                </div>
                <div class="p1">
                    <p id="60" class="code-formula">
                        <mathml id="60"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mrow><mtext>θ</mtext><msub><mrow></mrow><mrow><mtext>m</mtext><mo>, </mo><mtext>l</mtext></mrow></msub></mrow><mo stretchy="true">→</mo></mover><mo>=</mo><mo stretchy="false"> (</mo><mtext>θ</mtext><msub><mrow></mrow><mrow><mtext>m</mtext><mo>, </mo><mtext>l</mtext><mo>, </mo><mn>1</mn></mrow></msub><mo>, </mo><mtext>θ</mtext><msub><mrow></mrow><mrow><mtext>m</mtext><mo>, </mo><mtext>l</mtext><mo>, </mo><mn>2</mn></mrow></msub><mo>, </mo><mo>⋯</mo><mo>, </mo><mtext>θ</mtext><msub><mrow></mrow><mrow><mtext>m</mtext><mo>, </mo><mtext>l</mtext><mo>, </mo><mtext>k</mtext></mrow></msub><mo stretchy="false">) </mo><mo>∼</mo><mtext>D</mtext><mtext>i</mtext><mtext>r</mtext><mo stretchy="false"> (</mo><mtext>α</mtext><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="61">3) 对于该文档中的每一个词项n∈{1, 2, …, N<sub>m</sub>}:</p>
                </div>
                <div class="p1">
                    <p id="62">生成第n个词项的标签l～Multinomail<mathml id="63"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false"> (</mo><mover accent="true"><mrow><mtext>ψ</mtext><msub><mrow></mrow><mtext>m</mtext></msub></mrow><mo stretchy="true">→</mo></mover><mo stretchy="false">) </mo></mrow></math></mathml></p>
                </div>
                <div class="p1">
                    <p id="64">生成第n个词项的主题z～Multinomail<mathml id="65"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false"> (</mo><mover accent="true"><mrow><mtext>θ</mtext><msub><mrow></mrow><mrow><mtext>m</mtext><mo>, </mo><mtext>l</mtext></mrow></msub></mrow><mo stretchy="true">→</mo></mover><mo stretchy="false">) </mo></mrow></math></mathml></p>
                </div>
                <div class="p1">
                    <p id="66">生成第n个词项的时间戳t～Beta<mathml id="67"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false"> (</mo><mover accent="true"><mtext>ω</mtext><mo>→</mo></mover><mo stretchy="false">) </mo></mrow></math></mathml></p>
                </div>
                <div class="p1">
                    <p id="68">在高维数据下, 求解含有多个隐变量的概率图模型非常困难, 因此, 不能通过最大似然函数的方法进行求解。目前常用的求解方法包括最大期望算法、变分最大期望算法以及基于马尔科夫链蒙特卡罗 (Markov Chain Monte Carlo, MCMC) 的吉布斯采样算法。相比其他算法, 吉布斯采样算法实现简单、性能优越, 因此, 本文采用该算法估计LToT模型中的参数。</p>
                </div>
                <div class="p1">
                    <p id="69">根据LToT的模型图, 可以得出其联合概率分布公式:</p>
                </div>
                <div class="p1">
                    <p id="70"><i>p</i> (<i>w</i>, <i>l</i>, <i>z</i>, <i>t</i>|<i>α</i>, <i>β</i>, <i>ω</i>, <i>μ</i>, <i>Λ</i>) </p>
                </div>
                <div class="p1">
                    <p id="71">该联合概率分布公式可以分解成:</p>
                </div>
                <div class="p1">
                    <p id="72"><i>p</i> (<i>w</i>|<i>z</i>, <i>μ</i>) <i>p</i> (<i>z</i>|<i>α</i>, <i>l</i>) <i>p</i> (<i>l</i>|<i>β</i>, <i>Λ</i>) <i>p</i> (<i>t</i>|<i>z</i>, <i>ω</i>) </p>
                </div>
                <div class="p1">
                    <p id="73">依次对该公式进行推导:</p>
                </div>
                <div class="p1">
                    <p id="74" class="code-formula">
                        <mathml id="74"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>p</mi><mo stretchy="false"> (</mo><mi>w</mi><mo stretchy="false">|</mo><mi>z</mi><mo>, </mo><mi>μ</mi><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><mrow><mo>∫</mo><mi>p</mi></mrow></mstyle><mo stretchy="false"> (</mo><mi>w</mi><mo stretchy="false">|</mo><mi>z</mi><mo>, </mo><mtext>ϕ</mtext><mo stretchy="false">) </mo><mi>p</mi><mo stretchy="false"> (</mo><mtext>ϕ</mtext><mo stretchy="false">|</mo><mi>μ</mi><mo stretchy="false">) </mo><mtext>d</mtext><mtext>ϕ</mtext><mo>=</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mstyle displaystyle="true"><munderover><mo>∏</mo><mrow><mi>l</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∏</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>Κ</mi><msub><mrow></mrow><mi>l</mi></msub></mrow></munderover><mrow><mrow><mo> (</mo><mrow><mfrac><mrow><mi>Γ</mi><mo stretchy="false"> (</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>v</mi><mo>=</mo><mn>1</mn></mrow><mi>V</mi></munderover><mi>μ</mi></mstyle><msub><mrow></mrow><mrow><mi>l</mi><mo>, </mo><mi>v</mi></mrow></msub><mo stretchy="false">) </mo></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∏</mo><mrow><mi>v</mi><mo>=</mo><mn>1</mn></mrow><mi>V</mi></munderover><mi>Γ</mi></mstyle><mo stretchy="false"> (</mo><mi>μ</mi><msub><mrow></mrow><mrow><mi>l</mi><mo>, </mo><mi>v</mi></mrow></msub><mo stretchy="false">) </mo></mrow></mfrac></mrow><mo>) </mo></mrow></mrow></mstyle></mrow></mstyle><mo>×</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mrow><mo> (</mo><mrow><mfrac><mrow><mstyle displaystyle="true"><munderover><mo>∏</mo><mrow><mi>v</mi><mo>=</mo><mn>1</mn></mrow><mi>V</mi></munderover><mi>Γ</mi></mstyle><mo stretchy="false"> (</mo><mi>n</mi><msub><mrow></mrow><mrow><mi>l</mi><mo>, </mo><mi>k</mi><mo>, </mo><mi>v</mi></mrow></msub><mo>+</mo><mi>μ</mi><msub><mrow></mrow><mrow><mi>l</mi><mo>, </mo><mi>v</mi></mrow></msub><mo stretchy="false">) </mo></mrow><mrow><mi>Γ</mi><mo stretchy="false"> (</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>v</mi><mo>=</mo><mn>1</mn></mrow><mi>V</mi></munderover><mo stretchy="false"> (</mo></mstyle><mi>n</mi><msub><mrow></mrow><mrow><mi>l</mi><mo>, </mo><mi>k</mi><mo>, </mo><mi>v</mi></mrow></msub><mo>+</mo><mi>μ</mi><msub><mrow></mrow><mrow><mi>l</mi><mo>, </mo><mi>v</mi></mrow></msub><mo stretchy="false">) </mo><mo stretchy="false">) </mo></mrow></mfrac></mrow><mo>) </mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="75">其中, <i>n</i><sub><i>l</i>, <i>k</i>, <i>v</i></sub>表示在标签<i>l</i>下主题<i>k</i>分配给词项<i>v</i>的数量。</p>
                </div>
                <div class="area_img" id="77">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJC201904034_07700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="78">其中, <i>n</i><sub><i>m</i>, <i>l</i>, <i>k</i></sub>表示文章<i>m</i>在第<i>l</i>个标签下主题<i>k</i>分配给所有词项的总数量。</p>
                </div>
                <div class="p1">
                    <p id="79" class="code-formula">
                        <mathml id="79"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>p</mi><mo stretchy="false"> (</mo><mi>l</mi><mo stretchy="false">|</mo><mi>β</mi><mo>, </mo><mi>Λ</mi><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><mrow><mo>∫</mo><mi>p</mi></mrow></mstyle><mo stretchy="false"> (</mo><mi>l</mi><mo stretchy="false">|</mo><mi>Λ</mi><mo>, </mo><mi>ψ</mi><mo stretchy="false">) </mo><mi>p</mi><mo stretchy="false"> (</mo><mi>ψ</mi><mo stretchy="false">|</mo><mi>β</mi><mo>, </mo><mi>Λ</mi><mo stretchy="false">) </mo><mtext>d</mtext><mi>ψ</mi><mo>=</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mstyle displaystyle="true"><munderover><mo>∏</mo><mrow><mi>m</mi><mo>=</mo><mn>1</mn></mrow><mi>Μ</mi></munderover><mrow><mrow><mo> (</mo><mrow><mfrac><mrow><mi>Γ</mi><mo stretchy="false"> (</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>l</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>L</mi><msub><mrow></mrow><mi>m</mi></msub></mrow></munderover><mi>β</mi></mstyle><msub><mrow></mrow><mi>l</mi></msub><mo stretchy="false">) </mo></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∏</mo><mrow><mi>l</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>L</mi><msub><mrow></mrow><mi>m</mi></msub></mrow></munderover><mi>Γ</mi></mstyle><mo stretchy="false"> (</mo><mi>β</mi><msub><mrow></mrow><mi>l</mi></msub><mo stretchy="false">) </mo></mrow></mfrac></mrow><mo>) </mo></mrow></mrow></mstyle><mo>×</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mrow><mo> (</mo><mrow><mfrac><mrow><mstyle displaystyle="true"><munderover><mo>∏</mo><mrow><mi>l</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>L</mi><msub><mrow></mrow><mi>m</mi></msub></mrow></munderover><mi>Γ</mi></mstyle><mo stretchy="false"> (</mo><mi>n</mi><msub><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mi>l</mi></mrow></msub><mo>+</mo><mi>β</mi><msub><mrow></mrow><mi>l</mi></msub><mo stretchy="false">) </mo></mrow><mrow><mi>Γ</mi><mo stretchy="false"> (</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>l</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>L</mi><msub><mrow></mrow><mi>m</mi></msub></mrow></munderover><mo stretchy="false"> (</mo></mstyle><mi>n</mi><msub><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mi>l</mi></mrow></msub><mo>+</mo><mi>β</mi><msub><mrow></mrow><mi>l</mi></msub><mo stretchy="false">) </mo><mo stretchy="false">) </mo></mrow></mfrac></mrow><mo>) </mo></mrow><mo>, </mo><mspace width="0.25em" /><mi>l</mi><mo>∈</mo><mi>Λ</mi><msub><mrow></mrow><mi>m</mi></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>5</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="80">其中, <i>n</i><sub><i>m</i>, <i>l</i></sub>表示在第<i>m</i>篇文档中词项分配给标签<i>l</i>的数量。</p>
                </div>
                <div class="p1">
                    <p id="81" class="code-formula">
                        <mathml id="81"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>p</mi><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">|</mo><mi>z</mi><mo>, </mo><mi>ω</mi><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∏</mo><mrow><mi>m</mi><mo>=</mo><mn>1</mn></mrow><mi>Μ</mi></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∏</mo><mrow><mi>l</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>L</mi><msub><mrow></mrow><mi>m</mi></msub></mrow></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∏</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>Ν</mi><msub><mrow></mrow><mrow><mi>l</mi><mo>, </mo><mi>m</mi></mrow></msub></mrow></munderover><mi>p</mi></mstyle></mrow></mstyle></mrow></mstyle><mo stretchy="false"> (</mo><mi>t</mi><msub><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mi>l</mi><mo>, </mo><mi>n</mi></mrow></msub><mo stretchy="false">|</mo><mi>ω</mi><msub><mrow></mrow><mrow><mi>z</mi><msub><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mi>l</mi><mo>, </mo><mi>n</mi></mrow></msub></mrow></msub><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>6</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="82">根据吉布斯采样算法, 可以得到如下公式:</p>
                </div>
                <div class="p1">
                    <p id="83" class="code-formula">
                        <mathml id="83"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>p</mi><mo stretchy="false"> (</mo><mi>z</mi><mo>, </mo><mi>t</mi><mo>, </mo><mi>w</mi><mo>, </mo><mi>l</mi><mo stretchy="false">|</mo><mi>α</mi><mo>, </mo><mi>β</mi><mo>, </mo><mi>ω</mi><mo>, </mo><mi>μ</mi><mo>, </mo><mi>Λ</mi><mo stretchy="false">) </mo><mo>∝</mo><mfrac><mrow><mi>p</mi><mo stretchy="false"> (</mo><mi>z</mi><mo>, </mo><mi>t</mi><mo>, </mo><mi>w</mi><mo>, </mo><mi>l</mi><mo stretchy="false">|</mo><mi>α</mi><mo>, </mo><mi>β</mi><mo>, </mo><mi>ω</mi><mo>, </mo><mi>μ</mi><mo>, </mo><mi>Λ</mi><mo stretchy="false">) </mo></mrow><mrow><mi>p</mi><mo stretchy="false"> (</mo><mi>z</mi><msub><mrow></mrow><mrow><mo>-</mo><mi>v</mi></mrow></msub><mo>, </mo><mi>t</mi><msub><mrow></mrow><mrow><mo>-</mo><mi>v</mi></mrow></msub><mo>, </mo><mi>w</mi><msub><mrow></mrow><mrow><mo>-</mo><mi>v</mi></mrow></msub><mo>, </mo><mi>l</mi><msub><mrow></mrow><mrow><mo>-</mo><mi>v</mi></mrow></msub><mo stretchy="false">|</mo><mi>α</mi><mo>, </mo><mi>β</mi><mo>, </mo><mi>ω</mi><mo>, </mo><mi>μ</mi><mo>, </mo><mi>Λ</mi><mo stretchy="false">) </mo></mrow></mfrac><mo>=</mo><mfrac><mrow><mi>n</mi><msub><mrow></mrow><mrow><mi>l</mi><mo>, </mo><mi>k</mi><mo>, </mo><mi>v</mi></mrow></msub><mo>+</mo><mi>μ</mi><msub><mrow></mrow><mrow><mi>l</mi><mo>, </mo><mi>v</mi></mrow></msub><mo>-</mo><mn>1</mn></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>v</mi><mo>=</mo><mn>1</mn></mrow><mi>V</mi></munderover><mo stretchy="false"> (</mo></mstyle><mi>n</mi><msub><mrow></mrow><mrow><mi>l</mi><mo>, </mo><mi>k</mi><mo>, </mo><mi>v</mi></mrow></msub><mo>+</mo><mi>μ</mi><msub><mrow></mrow><mrow><mi>l</mi><mo>, </mo><mi>v</mi></mrow></msub><mo stretchy="false">) </mo><mo>-</mo><mn>1</mn></mrow></mfrac><mo>×</mo><mfrac><mrow><mi>n</mi><msub><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mi>l</mi><mo>, </mo><mi>k</mi></mrow></msub><mo>+</mo><mi>α</mi><msub><mrow></mrow><mrow><mi>l</mi><mo>, </mo><mi>k</mi></mrow></msub><mo>-</mo><mn>1</mn></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>Κ</mi></munderover><mo stretchy="false"> (</mo></mstyle><mi>n</mi><msub><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mi>l</mi><mo>, </mo><mi>k</mi></mrow></msub><mo>+</mo><mi>α</mi><msub><mrow></mrow><mrow><mi>l</mi><mo>, </mo><mi>k</mi></mrow></msub><mo stretchy="false">) </mo><mo>-</mo><mn>1</mn></mrow></mfrac><mo>×</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mfrac><mrow><mi>n</mi><msub><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mi>l</mi></mrow></msub><mo>+</mo><mi>β</mi><msub><mrow></mrow><mi>l</mi></msub><mo>-</mo><mn>1</mn></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>l</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></munderover><mo stretchy="false"> (</mo></mstyle><mi>n</mi><msub><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mi>l</mi></mrow></msub><mo>+</mo><mi>β</mi><msub><mrow></mrow><mi>l</mi></msub><mo stretchy="false">) </mo><mo>-</mo><mn>1</mn></mrow></mfrac><mo>×</mo><mfrac><mrow><mi>Γ</mi><mo stretchy="false"> (</mo><mi>ω</mi><msub><mrow></mrow><mrow><mi>z</mi><msub><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mi>l</mi><mo>, </mo><mi>v</mi></mrow></msub><mo>, </mo><mn>1</mn></mrow></msub><mo>+</mo><mi>ω</mi><msub><mrow></mrow><mrow><mi>z</mi><msub><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mi>l</mi><mo>, </mo><mi>v</mi></mrow></msub><mo>, </mo><mn>2</mn></mrow></msub><mo stretchy="false">) </mo></mrow><mrow><mi>Γ</mi><mo stretchy="false"> (</mo><mi>ω</mi><msub><mrow></mrow><mrow><mi>z</mi><msub><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mi>l</mi><mo>, </mo><mi>v</mi></mrow></msub><mo>, </mo><mn>1</mn></mrow></msub><mo stretchy="false">) </mo><mi>Γ</mi><mo stretchy="false"> (</mo><mi>ω</mi><msub><mrow></mrow><mrow><mi>z</mi><msub><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mi>l</mi><mo>, </mo><mi>v</mi></mrow></msub><mo>, </mo><mn>2</mn></mrow></msub><mo stretchy="false">) </mo></mrow></mfrac><mi>t</mi><msubsup><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mi>l</mi><mo>, </mo><mi>v</mi></mrow><mrow><mi>ω</mi><msub><mrow></mrow><mrow><mi>z</mi><msub><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mi>l</mi><mo>, </mo><mi>v</mi></mrow></msub><mo>, </mo><mn>1</mn></mrow></msub></mrow></msubsup><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>t</mi><msub><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mi>l</mi><mo>, </mo><mi>v</mi></mrow></msub><mo stretchy="false">) </mo><mrow><mi>ω</mi><msub><mrow></mrow><mrow><mi>z</mi><msub><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mi>l</mi><mo>, </mo><mi>v</mi></mrow></msub><mo>, </mo><mn>2</mn></mrow></msub></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>7</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="84">其中, -<i>v</i>表示去除第<i>v</i>个词项, <i>l</i>∈<i>Λ</i><sub><i>m</i></sub>, <i>k</i>∈1, 2, …, <i>K</i><sub><i>l</i></sub>。</p>
                </div>
                <div class="p1">
                    <p id="85">式 (7) 最后一项为Beta分布, 其参数估计有2种方法<citation id="140" type="reference"><link href="29" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>:矩估计和最大似然估计。最大似然估计无闭式解, 需要靠迭代来获取近似解, 而矩估计有闭式解, 且计算方法简单。因此, 本文采用矩估计来获取Beta分布的参数, 计算方法如式 (8) 、式 (9) 所示。</p>
                </div>
                <div class="p1">
                    <p id="86" class="code-formula">
                        <mathml id="86"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>ω</mi><msub><mrow></mrow><mrow><mi>z</mi><msub><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mi>l</mi><mo>, </mo><mi>v</mi></mrow></msub><mo>, </mo><mn>1</mn></mrow></msub><mo>=</mo><mi>t</mi><msub><mrow></mrow><mrow><mi>l</mi><mo>, </mo><mi>k</mi></mrow></msub><mrow><mo> (</mo><mrow><mfrac><mrow><mi>t</mi><msub><mrow></mrow><mrow><mi>l</mi><mo>, </mo><mi>k</mi></mrow></msub><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>t</mi><msub><mrow></mrow><mrow><mi>l</mi><mo>, </mo><mi>k</mi></mrow></msub><mo stretchy="false">) </mo></mrow><mrow><mi>s</mi><msubsup><mrow></mrow><mrow><mi>l</mi><mo>, </mo><mi>k</mi></mrow><mn>2</mn></msubsup></mrow></mfrac><mo>-</mo><mn>1</mn></mrow><mo>) </mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>8</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi>ω</mi><msub><mrow></mrow><mrow><mi>z</mi><msub><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mi>l</mi><mo>, </mo><mi>v</mi></mrow></msub><mo>, </mo><mn>2</mn></mrow></msub><mo>=</mo><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>t</mi><msub><mrow></mrow><mrow><mi>l</mi><mo>, </mo><mi>k</mi></mrow></msub><mo stretchy="false">) </mo><mrow><mo> (</mo><mrow><mfrac><mrow><mi>t</mi><msub><mrow></mrow><mrow><mi>l</mi><mo>, </mo><mi>k</mi></mrow></msub><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>t</mi><msub><mrow></mrow><mrow><mi>l</mi><mo>, </mo><mi>k</mi></mrow></msub><mo stretchy="false">) </mo></mrow><mrow><mi>s</mi><msubsup><mrow></mrow><mrow><mi>l</mi><mo>, </mo><mi>k</mi></mrow><mn>2</mn></msubsup></mrow></mfrac><mo>-</mo><mn>1</mn></mrow><mo>) </mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>9</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="87">其中, <i>t</i><sub><i>l</i>, <i>k</i></sub>为标签<i>l</i>下主题<i>k</i>的时间戳均值, <i>s</i><mathml id="88"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>l</mi><mo>, </mo><mi>k</mi></mrow><mn>2</mn></msubsup></mrow></math></mathml>为标签l下主题k的时间戳方差。</p>
                </div>
                <div class="p1">
                    <p id="89">结合式 (2) 和式 (7) , 可以得到改进后的采样公式:</p>
                </div>
                <div class="p1">
                    <p id="90" class="code-formula">
                        <mathml id="90"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>p</mi><mo stretchy="false"> (</mo><mi>z</mi><mo>, </mo><mi>t</mi><mo>, </mo><mi>w</mi><mo>, </mo><mi>l</mi><mo stretchy="false">|</mo><mi>α</mi><mo>, </mo><mi>β</mi><mo>, </mo><mi>ω</mi><mo>, </mo><mi>μ</mi><mo>, </mo><mi>Λ</mi><mo stretchy="false">) </mo><mo>∝</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mfrac><mrow><mi>w</mi><mo stretchy="false"> (</mo><mi>v</mi><mo>, </mo><mi>m</mi><mo stretchy="false">) </mo><mo>⋅</mo><mi>n</mi><msub><mrow></mrow><mrow><mi>l</mi><mo>, </mo><mi>k</mi><mo>, </mo><mi>v</mi></mrow></msub><mo>+</mo><mi>μ</mi><msub><mrow></mrow><mrow><mi>l</mi><mo>, </mo><mi>v</mi></mrow></msub><mo>-</mo><mn>1</mn></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>v</mi><mo>=</mo><mn>1</mn></mrow><mi>V</mi></munderover><mo stretchy="false"> (</mo></mstyle><mi>w</mi><mo stretchy="false"> (</mo><mi>v</mi><mo>, </mo><mi>m</mi><mo stretchy="false">) </mo><mo>⋅</mo><mi>n</mi><msub><mrow></mrow><mrow><mi>l</mi><mo>, </mo><mi>k</mi><mo>, </mo><mi>v</mi></mrow></msub><mo>+</mo><mi>μ</mi><msub><mrow></mrow><mrow><mi>l</mi><mo>, </mo><mi>v</mi></mrow></msub><mo stretchy="false">) </mo><mo>-</mo><mn>1</mn></mrow></mfrac><mo>×</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mfrac><mrow><mi>w</mi><mo stretchy="false"> (</mo><mi>v</mi><mo>, </mo><mi>m</mi><mo stretchy="false">) </mo><mo>⋅</mo><mi>n</mi><msub><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mi>l</mi><mo>, </mo><mi>k</mi></mrow></msub><mo>+</mo><mi>α</mi><msub><mrow></mrow><mrow><mi>l</mi><mo>, </mo><mi>k</mi></mrow></msub><mo>-</mo><mn>1</mn></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>Κ</mi></munderover><mo stretchy="false"> (</mo></mstyle><mi>w</mi><mo stretchy="false"> (</mo><mi>v</mi><mo>, </mo><mi>m</mi><mo stretchy="false">) </mo><mo>⋅</mo><mi>n</mi><msub><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mi>l</mi><mo>, </mo><mi>k</mi></mrow></msub><mo>+</mo><mi>α</mi><msub><mrow></mrow><mrow><mi>l</mi><mo>, </mo><mi>k</mi></mrow></msub><mo stretchy="false">) </mo><mo>-</mo><mn>1</mn></mrow></mfrac><mo>×</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mfrac><mrow><mi>w</mi><mo stretchy="false"> (</mo><mi>v</mi><mo>, </mo><mi>m</mi><mo stretchy="false">) </mo><mo>⋅</mo><mi>n</mi><msub><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mi>l</mi></mrow></msub><mo>+</mo><mi>β</mi><msub><mrow></mrow><mi>l</mi></msub><mo>-</mo><mn>1</mn></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>l</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></munderover><mo stretchy="false"> (</mo></mstyle><mi>w</mi><mo stretchy="false"> (</mo><mi>v</mi><mo>, </mo><mi>m</mi><mo stretchy="false">) </mo><mo>⋅</mo><mi>n</mi><msub><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mi>l</mi></mrow></msub><mo>+</mo><mi>β</mi><msub><mrow></mrow><mi>l</mi></msub><mo stretchy="false">) </mo><mo>-</mo><mn>1</mn></mrow></mfrac><mo>×</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mfrac><mrow><mi>Γ</mi><mo stretchy="false"> (</mo><mi>ω</mi><msub><mrow></mrow><mrow><mi>z</mi><msub><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mi>l</mi><mo>, </mo><mi>v</mi></mrow></msub><mo>, </mo><mn>1</mn></mrow></msub><mo>+</mo><mi>ω</mi><msub><mrow></mrow><mrow><mi>z</mi><msub><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mi>l</mi><mo>, </mo><mi>v</mi></mrow></msub><mo>, </mo><mn>2</mn></mrow></msub><mo stretchy="false">) </mo></mrow><mrow><mi>Γ</mi><mo stretchy="false"> (</mo><mi>ω</mi><msub><mrow></mrow><mrow><mi>z</mi><msub><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mi>l</mi><mo>, </mo><mi>v</mi></mrow></msub><mo>, </mo><mn>1</mn></mrow></msub><mo stretchy="false">) </mo><mi>Γ</mi><mo stretchy="false"> (</mo><mi>ω</mi><msub><mrow></mrow><mrow><mi>z</mi><msub><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mi>l</mi><mo>, </mo><mi>v</mi></mrow></msub><mo>, </mo><mn>2</mn></mrow></msub><mo stretchy="false">) </mo></mrow></mfrac><mi>t</mi><msubsup><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mi>l</mi><mo>, </mo><mi>v</mi></mrow><mrow><mi>ω</mi><msub><mrow></mrow><mrow><mi>z</mi><msub><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mi>l</mi><mo>, </mo><mi>v</mi></mrow></msub><mo>, </mo><mn>1</mn></mrow></msub></mrow></msubsup><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>t</mi><msub><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mi>l</mi><mo>, </mo><mi>v</mi></mrow></msub><mo stretchy="false">) </mo><mrow><mi>ω</mi><msub><mrow></mrow><mrow><mi>z</mi><msub><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mi>l</mi><mo>, </mo><mi>v</mi></mrow></msub><mo>, </mo><mn>2</mn></mrow></msub></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>0</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <h3 id="91" name="91" class="anchor-tag">2 实验结果与分析</h3>
                <h4 class="anchor-tag" id="92" name="92">2.1 实验环境与实验数据</h4>
                <h4 class="anchor-tag" id="93" name="93">2.1.1 实验环境</h4>
                <div class="p1">
                    <p id="94">本文实验程序代码全部由<i>Java</i>语言实现, 计算机的<i>CPU</i>为<i>Intel i</i>7-6700<i>HQ CPU</i>, 主频为2.60 <i>GHz</i>, 操作系统是<i>Windows</i> 10, 拥有8 <i>GB</i>内存。</p>
                </div>
                <h4 class="anchor-tag" id="95" name="95">2.1.2 实验参数设置</h4>
                <div class="p1">
                    <p id="96">根据经验, 在本次实验中, 参数设置为<citation id="141" type="reference"><link href="31" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>:α=50/K, β=50/K, 其中, K是主题数量, 需要通过困惑度 (<i>Perplexity</i>) 来设定, μ=0.01。</p>
                </div>
                <h4 class="anchor-tag" id="97" name="97">2.1.3 数据集说明</h4>
                <div class="p1">
                    <p id="98">本实验使用2008年6月、2012年6月—7月的搜狗新闻作为数据集。经过特殊字符处理和<i>NLPIR</i>分词系统处理后, 2008年6月搜狗新闻数据集 (以下简称搜狗2008数据集) 一共有1 754篇文章, 51 258个词项, 其中含有6个标签, 分别是健康、房产、<i>IT</i>、学习、体育和娱乐, 每篇文章属于其中的1个～3个标签。2012年6月—7月搜狗新闻数据集 (以下简称搜狗2012数据集) 一共有2 081篇文章, 61 524个词项, 其中含有7个标签, 分别是商业、教育、金融、科技、社会、体育和股市, 同样, 每篇文章属于其中的1个～3个标签。</p>
                </div>
                <h4 class="anchor-tag" id="99" name="99">2.2 模型泛化能力分析实验</h4>
                <div class="p1">
                    <p id="100">为验证模型的泛化能力, 本文采用目前较公认的常规评价标准<i>Perplexity</i>作为衡量指标, 以度量模型的拟合程度<citation id="142" type="reference"><link href="33" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>。为保证模型能够收敛, 每次训练的迭代次数为1 000。<i>Perplexity</i>计算表达式如下:</p>
                </div>
                <div class="p1">
                    <p id="101">Perplexity (w, t|α, β, μ, ω, Λ) =<i>exp</i><mathml id="103"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo>-</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>m</mi><mo>=</mo><mn>1</mn></mrow><mi>Μ</mi></munderover><mi>p</mi></mstyle><mo stretchy="false"> (</mo><mi>w</mi><mo>, </mo><mi>t</mi><mo stretchy="false">|</mo><mi>α</mi><mo>, </mo><mi>β</mi><mo>, </mo><mi>μ</mi><mo>, </mo><mi>ω</mi><mo>, </mo><mi>Λ</mi><mo stretchy="false">) </mo><mo>×</mo><mi>Ν</mi><msubsup><mrow></mrow><mi>m</mi><mrow><mo>-</mo><mn>1</mn></mrow></msubsup></mrow></math></mathml>      (11) </p>
                </div>
                <div class="p1">
                    <p id="104">其中, p (w, t|α, β, μ, ω, Λ) 的展开公式如下:</p>
                </div>
                <div class="p1">
                    <p id="105" class="code-formula">
                        <mathml id="105"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>p</mi><mo stretchy="false"> (</mo><mi>w</mi><mo>, </mo><mi>t</mi><mo stretchy="false">|</mo><mi>α</mi><mo>, </mo><mi>β</mi><mo>, </mo><mi>μ</mi><mo>, </mo><mi>ω</mi><mo>, </mo><mi>Λ</mi><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>l</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>L</mi><msub><mrow></mrow><mi>m</mi></msub></mrow></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>Ν</mi><msub><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mi>l</mi></mrow></msub></mrow></munderover><mrow><mi>ln</mi></mrow></mstyle></mrow></mstyle><mtext> </mtext><mo stretchy="false"> (</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>z</mi><mo>⊆</mo><mi>m</mi></mrow></munder><mi>θ</mi></mstyle><msub><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mi>l</mi><mo>, </mo><mi>z</mi></mrow></msub><mtext>ϕ</mtext><msub><mrow></mrow><mrow><mi>l</mi><mo>, </mo><mi>z</mi><mo>, </mo><mi>n</mi></mrow></msub><mo>×</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>z</mi><mo>⊆</mo><mi>m</mi></mrow></munder><mi>p</mi></mstyle><mo stretchy="false"> (</mo><mi>t</mi><msub><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mi>l</mi><mo>, </mo><mi>n</mi></mrow></msub><mo stretchy="false">|</mo><mi>ω</mi><msub><mrow></mrow><mrow><mi>z</mi><msub><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mi>l</mi><mo>, </mo><mi>n</mi></mrow></msub></mrow></msub><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo>, </mo><mi>l</mi><mo>∈</mo><mi>Λ</mi><msub><mrow></mrow><mi>m</mi></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>2</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="106">由于动态主题模型<i>DTM</i>对时间选取的粒度敏感, 因此不同的粒度选择会产生不同的实验结果。鉴于以上考虑, 本文仅对加权<i>LToT</i>、未加权<i>LToT</i>和<i>ToT</i>模型进行测试与对比。每次实验取相同的主题数, 主题数量初始值为3, 以3为步长增长。</p>
                </div>
                <div class="p1">
                    <p id="107">图2、图3分别显示在不同数据集下随着主题数量的增加, 3个模型的<i>Perplexity</i>值对比结果。</p>
                </div>
                <div class="area_img" id="108">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201904034_108.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 3种模型在搜狗2008数据集上的困惑度对比" src="Detail/GetImg?filename=images/JSJC201904034_108.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图2 3种模型在搜狗2008数据集上的困惑度对比</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201904034_108.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="109">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201904034_109.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 3种模型在搜狗2012数据集上的困惑度对比" src="Detail/GetImg?filename=images/JSJC201904034_109.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图3 3种模型在搜狗2012数据集上的困惑度对比</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201904034_109.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="110">在一般情况下, Perplexity值越小说明该模型泛化能力越强。从图2、图3可以看出, 加权LToT模型的困惑度明显小于其他2种模型。在搜狗2008数据集下, 加权LToT模型在主题数量达到27之后就开始趋于平稳, 而ToT模型和未加权LToT模型主题数量分别要达到54和39后才趋于平稳。在搜狗2012数据集下, 加权LToT模型在主题数量达到30后开始趋于平稳, 而ToT模型和未加权LToT模型主题数量分别要达到48和42才开始趋于平稳。加权LToT和未加权LToT模型都是在ToT模型的基础上引入文档的标签属性形成监督模型, 相比没有考虑文档标签的ToT模型具有更强的泛化能力。又因为加权LToT模型降低了在文档集中占绝大多数的普通词项的权重, 因此, 其相比未加权LToT模型具有更低的Perplexity值。基于该实验结果, 在下文的实验中, 设置搜狗2008数据集下, 加权LToT模型、未加权LToT模型、ToT模型的主题数量<i>K</i>分别为27、39、54;在搜狗2012数据集下, 设置加权LToT模型、未加权LToT模型、ToT模型的主题数量<i>K</i>分别为30、42、48。</p>
                </div>
                <h4 class="anchor-tag" id="111" name="111">2.3 词语权重变化分析实验</h4>
                <div class="p1">
                    <p id="112">为能够直观地得出本文权重计算改进方法对词项权重的改变作用, 表1列出搜狗2012数据集中教育标签下排名前15个词项的权重变化情况。</p>
                </div>
                <div class="area_img" id="113">
                    <p class="img_tit"><b>表1 不同模型对教育标签下的词项权重对比结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="113" border="1"><tr><td><br />模型</td><td colspan="3">教育标签下的词项权重</td></tr><tr><td rowspan="5"><br />未加权LToT<br />模型</td><td>评卷:0.066 61</td><td>志愿:0.052 09</td><td>哲:0.041 37</td></tr><tr><td><br />考生:0.065 05</td><td>同学:0.048 62</td><td>教师:0.037 16</td></tr><tr><td><br />状元:0.062 90</td><td>专业:0.048 57</td><td>电话:0.036 89</td></tr><tr><td><br />高考:0.058 30</td><td>成绩:0.045 85</td><td>结束:0.035 21</td></tr><tr><td><br />考试:0.055 43</td><td>时代:0.045 77</td><td>给予:0.030 43</td></tr><tr><td rowspan="5"><br />加权LToT<br />模型</td><td>高考:0.228 30</td><td>考试:0.175 43</td><td>教师:0.168 62</td></tr><tr><td><br />考生:0.215 05</td><td>成绩:0.175 15</td><td>状元:0.167 00</td></tr><tr><td><br />评卷:0.203 61</td><td>招生:0.173 75</td><td>阅卷:0.166 89</td></tr><tr><td><br />录取:0.185 77</td><td>专业:0.170 73</td><td>选择:0.165 21</td></tr><tr><td><br />状元:0.182 90</td><td>志愿:0.169 33</td><td>孩子:0.154 34</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="114">从表1可以看出, 在未加权LToT模型下, 词项分布与教育相关, 但是存在一些表意性较差的词语, 比如“时代”“哲”“电话”“结束”和“给予”, 且各词项的权重分布比较平均, 新闻标题中表意性较强的词项并没有得到重点突出。而在本文加权LToT模型下, 词项更具有标签代表性, 从“考生”“高考”“成绩”和“孩子”等词可以看出与教育标签内容相关, 且这类具有较高信息量的词项得到了较大的权重。同时, 在加权LToT模型中, 一些类别代表性明显的词项权重明显高于未加权LToT模型, 比如, “高考”在加权LToT模型中的权重是0.228 30, 而在未加权LToT模型中仅为0.058 30。因此, 本文加权LToT模型在一定程度上能提高词项对文本的表述能力。</p>
                </div>
                <div class="p1">
                    <p id="115">为说明加权LToT模型计算出的词项权重不向高频词倾斜, 并且能够贴近标签含义, 本文选取搜狗2012数据集中2012年伦敦奥运会与2012年温布尔登网球公开赛这2个主题, 将其中的词项按权重进行排序, 结果如表2所示。从表2可以看出, 加权LToT模型中的词项比未加权LToT模型中的词项更能代表体育标签。在未加权LToT模型中, 存在一些与标签意义不相关的词项, 比如“昨天”“结束”“表现”和“参加”等, 这些词项之所以出现在主题中, 是因为其出现频率较高, 使得未加权LToT模型的主题-词项矩阵向这些高频词倾斜, 从而影响了主题描述。而在加权LToT模型的构建过程中, 使用了TF-IGM算法, 考虑到新闻的标签分布, 降低了这些高频词的权重, 最终使得结果中的主题-词项矩阵更具标签代表性。</p>
                </div>
                <div class="area_img" id="116">
                    <p class="img_tit"><b>表2 不同模型的词项权重对比结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="116" border="1"><tr><td><br />模型</td><td colspan="3">词项权重情况</td></tr><tr><td rowspan="5"><br />未加权LToT<br />模型</td><td>火炬:0.053 48</td><td>结束:0.018 95</td><td>参加:0.012 57</td></tr><tr><td><br />伦敦:0.041 97</td><td>小威:0.017 19</td><td>冠军:0.011 74</td></tr><tr><td><br />昨天:0.037 73</td><td>选手:0.016 77</td><td>郑洁:0.011 32</td></tr><tr><td><br />传递:0.021 32</td><td>表现:0.016 35</td><td>奥组委:0.010 83</td></tr><tr><td><br />奥运:0.020 31</td><td>纳达尔:0.015 51</td><td>希望:0.010 48</td></tr><tr><td rowspan="5"><br />加权LToT<br />模型</td><td>温网:0.220 37</td><td>网球:0.200 16</td><td>奥运会:0.170 18</td></tr><tr><td><br />小威:0.210 17</td><td>传递:0.190 20</td><td>大满贯:0.140 12</td></tr><tr><td><br />李娜:0.210 00</td><td>奥运:0.181 21</td><td>郑洁:0.110 11</td></tr><tr><td><br />火炬:0.205 34</td><td>选手:0.180 16</td><td>奥组委:0.100 10</td></tr><tr><td><br />伦敦:0.204 19</td><td>纳达尔:0.180 15</td><td>冠军:0.090 11</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="117" name="117">2.4 主题演化分析实验</h4>
                <div class="p1">
                    <p id="118">为展现LToT模型的优势, 在搜狗2012数据集下, 分别利用加权LToT模型与ToT模型计算得到4个主题, 结果如图4、图5所示。其中, 坐标图下方内容为对应主题下的前8个词项权重。</p>
                </div>
                <div class="area_img" id="119">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201904034_119.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 加权LToT模型演化趋势" src="Detail/GetImg?filename=images/JSJC201904034_119.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图4 加权LToT模型演化趋势</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201904034_119.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="120">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201904034_120.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 ToT模型演化趋势" src="Detail/GetImg?filename=images/JSJC201904034_120.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图5 ToT模型演化趋势</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201904034_120.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="121">通过对比可以看出:</p>
                </div>
                <div class="p1">
                    <p id="122">1) 加权LToT模型的“奥运会”主题演化呈上涨趋势, 而ToT模型则表现为先上涨后下降, 通过词项权重对比发现, ToT模型中存在关于“中国男篮”的相关词项, 即ToT模型在“奥运会”主题下融入了“中国男篮”相关的噪声词项, 从而导致其不能很好地描述“奥运会”主题。</p>
                </div>
                <div class="p1">
                    <p id="123">2) 对于“7.21北京暴雨”主题, LToT模型从7月21日开始呈暴涨趋势, 与真实情况一致, 而ToT模型中从7月初就开始呈增长趋势, 通过词项权重发现, ToT模型比较侧重关注全国的暴雨趋势, 而LToT模型则着重关注北京地区的暴雨。</p>
                </div>
                <div class="p1">
                    <p id="124">3) 对于教育标签下“高考”和“志愿填报”这2个主题的趋势演化情况, 与ToT模型相比, LToT模型表现得较“窄”, 从词项权重可以看出, ToT模型一直关注“高考”, 凭借实际经验可知, “高考”这个关键词项贯穿于整个6月, 从而导致ToT模型较“宽”。而LToT模型借助标签, 有效地突出了“高考”和“志愿填报”这2个主题。</p>
                </div>
                <h3 id="125" name="125" class="anchor-tag">3 结束语</h3>
                <div class="p1">
                    <p id="126">本文建立一种基于标签的主题演化模型LToT, 并使用TF-IGM算法进行权重分析, 从而拓展模型的生成过程。该模型充分利用文档的标签、标题和时间属性, 实验结果验证了其有效性。下一步考虑将LToT模型应用于推荐系统, 以提高系统性能。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" >
                                    <b>[1]</b>
                                 BLEI D M, NG A Y, JORDAN M I.Latent Dirichlet allocation[J].Journal of Machine Learning Research, 2003, 3:993-1022.
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000004805&amp;v=MTg4MTRud1plWnVIeWptVUxuSUoxNGNiaHM9TmlmSVk3SzdIdGpOcjQ5RlpPc0xCSHc4b0JNVDZUNFBRSC9pclJkR2VycVFUTQ==&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> BLEI D M.Probabilistic topic models[J].Communications of the ACM, 2012, 55 (4) :77-84.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Dynamic topic models">

                                <b>[3]</b> BLEI D M, LAFFERTY J D.Dynamic topic models[C]//Proceedings of the 23rd International Conference on Machine Learning.New York, USA:ACM Press, 2006:113-120.
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Topics over Time:A Non-Markov Continuous-Time Model of Topical Trends">

                                <b>[4]</b> WANG X, MCCALLUM A.Topics over time:a non-Markov continuous-time model of topical trends[C]//Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.New York, USA:ACM Press, 2006:424-433.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201611033&amp;v=MTE3ODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJvRnkvbFc3dkJMejdCYmJHNEg5Zk5ybzlHWjRRS0RIODQ=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b> 刘良选, 黄梦醒.一种面向词汇突发的连续时间主题模型[J].计算机工程, 2016, 42 (11) :195-201.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Labeled LDA:a supervised topic model for credit attribution in multi-labeled corpora">

                                <b>[6]</b> RAMAGE D, HALL D, NALLAPATI R, et al.Labeled LDA:a supervised topic model for credit attribution in multi-labeled corpora[C]//Proceedings of 2009 Con-ference on Empirical Methods in Natural Language Processing.New York, USA:ACM Press, 2009:248-256.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Partially labeled topic models for interpretabletext mining">

                                <b>[7]</b> RAMAGE D, MANNING C D, DUMAIS S.Partially labeled topic models for interpretable text mining[C]//Proceedings of ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.New York, USA:ACM Press, 2011:457-465.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Clustering the Tagged Web">

                                <b>[8]</b> RAMAGE D, HEYMANN P, MANNING C D, et al.Clustering the tagged Web[C]//Proceedings of the 2nd ACM International Conference on Web Search and Data Mining.New York, USA:ACM Press, 2009:54-63.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=BFJT201002026&amp;v=MzIwMjNVUkxPZVplUm9GeS9sVzd2Qkp5dkJlckc0SDlITXJZOUhZb1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckM=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b> 张小平, 周雪忠, 黄厚宽, 等.一种改进的LDA主题模型[J].北京交通大学学报, 2010, 34 (2) :111-114.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Beyond TFIDF weighting for text categorization in the vector space model">

                                <b>[10]</b> SOUCY P, MINEAU G W.Beyond TFIDF weighting for text categorization in the vector space model[C]//Proceedings of the 19th International Joint Conference on Artificial Intelligence.San Francisco, USA:Morgan Kaufmann Publishers Inc., 2005:1130-1135.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Turning from TF-IDF to TFIGM for term weighting in text classification">

                                <b>[11]</b> CHEN K, ZHANG Z, LONG J, et al.Turning from TF-IDF to TF-IGM for term weighting in text classification[J].Expert Systems with Applications, 2016, 66:245-260.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_12" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=QBXB20050100E&amp;v=MDk1NTdZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSb0Z5L2xXN3ZCTkMvVGJMRzRIdFRNcm85RkU=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[12]</b> 侯汉清, 章成志, 郑红.Web概念挖掘中标引源加权方案初探[J].情报学报, 2005, 24 (1) :87-92.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_13" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=RJXB201311002&amp;v=MDI1NjhaZVJvRnkvbFc3dkJOeWZUYkxHNEg5TE5ybzlGWm9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2U=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[13]</b> 张宏毅, 王立威, 陈瑜希.概率图模型研究进展综述[J].软件学报, 2013, 24 (11) :2476-2497.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_14" target="_blank" href="/kcms/detail/detail.aspx?dbcode=STJD&amp;filename=STJD772448042&amp;v=MTA5MDVMT2VaZVJvRnkvbFc3dkJOam5CYXJTL0hOWElwNDlCWm9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[14]</b> BOWMAN K O, SHENTON L R.Parameter estimation for the Beta distribution[J].Journal of Statistical Computation and Simulation, 2007, 43 (3/4) :217-228.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_15" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJRS&amp;filename=SJRS13080700012110&amp;v=MjY2NTA0Y2Jocz1OaWZaZmJLN0h0bk1xSTlGWk9vTkRYMDVvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVVMbklKMQ==&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[15]</b> GUO X, XIANG Y, CHEN Q, et al.LDA-based online topic detection using tensor factorization[J].Journal of Information Science, 2013, 39 (4) :459-469.
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=LDA-based document models for ad-hoc retrieval">

                                <b>[16]</b> WEI X, CROFT W B.LDA-based document models for ad-hoc retrieval[C]//Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval.New York, USA:ACM Press, 2006:178-185.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJC201904034" />
        <input id="dpi" type="hidden" value="600" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201904034&amp;v=MDI2NDc0SDlqTXE0OUdZSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm9GeS9sVzd2Qkx6N0JiYkc=&amp;uid=WEEvREcwSlJHSldRa1FhcTdWa2FjcW9vRlJTQ2JLUmN2amRwVFpsWThsTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
