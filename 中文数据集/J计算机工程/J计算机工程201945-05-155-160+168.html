<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637131349093582500%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJC201905025%26RESULT%3d1%26SIGN%3dF8Op7hii7aIxfmRW4vSTdHeSKD0%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201905025&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201905025&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201905025&amp;v=MTI1NzZxQnRHRnJDVVJMT2VaZVJuRnlya1U3dk1MejdCYmJHNEg5ak1xbzlIWVlRS0RIODR2UjRUNmo1NE8zenE=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmF3cmhWVUU1ZkxoamhGOGFaaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#43" data-title="0 概述 ">0 概述</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#48" data-title="1 PL-GAN的设计与实现 ">1 PL-GAN的设计与实现</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#52" data-title="1.1 算法思想">1.1 算法思想</a></li>
                                                <li><a href="#98" data-title="1.2 算法步骤">1.2 算法步骤</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#128" data-title="2 实验结果与分析 ">2 实验结果与分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#129" data-title="2.1 实验数据集">2.1 实验数据集</a></li>
                                                <li><a href="#131" data-title="2.2 参数设置">2.2 参数设置</a></li>
                                                <li><a href="#134" data-title="2.3 性能分析">2.3 性能分析</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#159" data-title="3 结束语 ">3 结束语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#80" data-title="&lt;b&gt;图1 PL-GAN算法流程&lt;/b&gt;"><b>图1 PL-GAN算法流程</b></a></li>
                                                <li><a href="#95" data-title="&lt;b&gt;图2 GAN半监督图像分类流程&lt;/b&gt;"><b>图2 GAN半监督图像分类流程</b></a></li>
                                                <li><a href="#133" data-title="&lt;b&gt;表1 参数设置&lt;/b&gt;"><b>表1 参数设置</b></a></li>
                                                <li><a href="#137" data-title="&lt;b&gt;表2 MNIST数据集网络参数配置&lt;/b&gt;"><b>表2 MNIST数据集网络参数配置</b></a></li>
                                                <li><a href="#143" data-title="&lt;b&gt;图3 MNIST数据集中3种算法的生成样本对比结果&lt;/b&gt;"><b>图3 MNIST数据集中3种算法的生成样本对比结果</b></a></li>
                                                <li><a href="#145" data-title="&lt;b&gt;图4 MNIST数据集中算法的损失变化对比结果&lt;/b&gt;"><b>图4 MNIST数据集中算法的损失变化对比结果</b></a></li>
                                                <li><a href="#147" data-title="&lt;b&gt;图5 MNIST数据集中3种算法的错误率对比结果&lt;/b&gt;"><b>图5 MNIST数据集中3种算法的错误率对比结果</b></a></li>
                                                <li><a href="#149" data-title="&lt;b&gt;表3 MNIST数据集中8种算法分类错误率对比&lt;/b&gt; %"><b>表3 MNIST数据集中8种算法分类错误率对比</b> %</a></li>
                                                <li><a href="#152" data-title="&lt;b&gt;表4 CIFAR-10数据集网络参数配置&lt;/b&gt;"><b>表4 CIFAR-10数据集网络参数配置</b></a></li>
                                                <li><a href="#154" data-title="&lt;b&gt;表5 CIFAR-10数据集中3种算法的IS值对比结果&lt;/b&gt;"><b>表5 CIFAR-10数据集中3种算法的IS值对比结果</b></a></li>
                                                <li><a href="#155" data-title="&lt;b&gt;图6 CIFAR-10数据集中3种算法的生成样本视觉对比&lt;/b&gt;"><b>图6 CIFAR-10数据集中3种算法的生成样本视觉对比</b></a></li>
                                                <li><a href="#157" data-title="&lt;b&gt;图7 CIFAR-10数据集中3种算法的错误率对比结果&lt;/b&gt;"><b>图7 CIFAR-10数据集中3种算法的错误率对比结果</b></a></li>
                                                <li><a href="#158" data-title="&lt;b&gt;表6 CIFAR-10数据集中7种算法的错误率对比&lt;/b&gt; %"><b>表6 CIFAR-10数据集中7种算法的错误率对比</b> %</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" title=" CHAPELLE O, SCHOLKOPF B, ZIEN A.Semi-supervised learning[J].IEEE Transactions on Neural Networks, 2009, 20 (3) :542-546." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Semi-Supervised Learning">
                                        <b>[1]</b>
                                         CHAPELLE O, SCHOLKOPF B, ZIEN A.Semi-supervised learning[J].IEEE Transactions on Neural Networks, 2009, 20 (3) :542-546.
                                    </a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" title=" LEE D H.Pseudo-label:the simple and efficient semi-supervised learning method for deep neural networks[EB/OL].[2018-02-01].http://deeplearning.net/wp-content/uploads/2013/03/pseudo_label_final.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Pseudo-label:the simple and efficient semi-supervised learning method for deep neural networks">
                                        <b>[2]</b>
                                         LEE D H.Pseudo-label:the simple and efficient semi-supervised learning method for deep neural networks[EB/OL].[2018-02-01].http://deeplearning.net/wp-content/uploads/2013/03/pseudo_label_final.pdf.
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title=" RASMUS A, BERGLUND M, HONKALA M, et al.Semi-supervised learning with ladder networks [EB/OL].[2018-02-01].http://papers.nips.cc/paper/5947-semi-supervised-learning-with-ladder-networks.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Semi-supervised learning with ladder networks">
                                        <b>[3]</b>
                                         RASMUS A, BERGLUND M, HONKALA M, et al.Semi-supervised learning with ladder networks [EB/OL].[2018-02-01].http://papers.nips.cc/paper/5947-semi-supervised-learning-with-ladder-networks.pdf.
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" title=" KINGMA D P, MOHAMED S, REZENDE D J, et al.Semi-supervised learning with deep generative models[EB/OL].[2018-02-05].https://arxiv.org/pdf/1406.5298.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Semi-supervised learning with deep generative models">
                                        <b>[4]</b>
                                         KINGMA D P, MOHAMED S, REZENDE D J, et al.Semi-supervised learning with deep generative models[EB/OL].[2018-02-05].https://arxiv.org/pdf/1406.5298.pdf.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" title=" GOODFELLOW I J, POUGET-ABADIE J, MIRZA M, et al.Generative adversarial nets[C]//Proceedings of Inter-national Conference on Neural Information Processing Systems.Cambridge, USA:MIT Press, 2014:2672-2680." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Generative adversarial nets">
                                        <b>[5]</b>
                                         GOODFELLOW I J, POUGET-ABADIE J, MIRZA M, et al.Generative adversarial nets[C]//Proceedings of Inter-national Conference on Neural Information Processing Systems.Cambridge, USA:MIT Press, 2014:2672-2680.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" title=" MAAL∅E L, S∅NDERBY C K, S∅NDERBY S K, et al.Auxiliary deep generative models[EB/OL].[2018-02-06].https://arxiv.org/pdf/1602.05473.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Auxiliary deep generative models">
                                        <b>[6]</b>
                                         MAAL∅E L, S∅NDERBY C K, S∅NDERBY S K, et al.Auxiliary deep generative models[EB/OL].[2018-02-06].https://arxiv.org/pdf/1602.05473.pdf.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" title=" DUMOULIN V, BELGHAZI I, POOLE B, et al.Adver-sarially learned inference[EB/OL].[2018-02-04].https://arxiv.org/pdf/1606.00704.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Adver-sarially learned inference">
                                        <b>[7]</b>
                                         DUMOULIN V, BELGHAZI I, POOLE B, et al.Adver-sarially learned inference[EB/OL].[2018-02-04].https://arxiv.org/pdf/1606.00704.pdf.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" title=" SPRINGENBERG J T.Unsupervised and semi-supervised learning with categorical generative adversarial networks[J].Computer Science, 2015 (6) :2321-2330." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Unsupervised and Semi-supervised Learning with Categorical Generative Adversarial Networks">
                                        <b>[8]</b>
                                         SPRINGENBERG J T.Unsupervised and semi-supervised learning with categorical generative adversarial networks[J].Computer Science, 2015 (6) :2321-2330.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" title=" SALIMANS T, GOODFELLOW I, ZAREMBA W, et al.Improved techniques for training GANs[C]//Proceedings of NIPS’16.Barcelona, Spain:NIPS, 2016:2234-2242." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Improved techniques for training gans">
                                        <b>[9]</b>
                                         SALIMANS T, GOODFELLOW I, ZAREMBA W, et al.Improved techniques for training GANs[C]//Proceedings of NIPS’16.Barcelona, Spain:NIPS, 2016:2234-2242.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_10" title=" LI Chongxuan, XU Kun, ZHU Jun, et al.Triple generative adversarial nets[C]//Proceedings of NIPS’17.Barcelona, Spain:NIPS, 2017:4091-4101." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Triple generative adversarial nets">
                                        <b>[10]</b>
                                         LI Chongxuan, XU Kun, ZHU Jun, et al.Triple generative adversarial nets[C]//Proceedings of NIPS’17.Barcelona, Spain:NIPS, 2017:4091-4101.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_11" title=" 王坤峰, 苟超, 段艳杰, 等.生成式对抗网络 GAN的研究进展与展望[J].自动化学报, 2017, 43 (3) :321-332." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201703001&amp;v=MTA3OTVrVTd2TUtDTGZZYkc0SDliTXJJOUZaWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm5GeXI=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmF3cmhWVUU1ZkxoamhGOGFaaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[11]</b>
                                         王坤峰, 苟超, 段艳杰, 等.生成式对抗网络 GAN的研究进展与展望[J].自动化学报, 2017, 43 (3) :321-332.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_12" title=" ARJOVSKY M, BOTTOU L.Towards principled methods for training generative adversarial networks[EB/OL].[2018-01-25].https://leon.bottou.org/publications/pdf/iclr-2017.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Towards principled methods for training generative adversarial networks">
                                        <b>[12]</b>
                                         ARJOVSKY M, BOTTOU L.Towards principled methods for training generative adversarial networks[EB/OL].[2018-01-25].https://leon.bottou.org/publications/pdf/iclr-2017.pdf.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_13" title=" ARJOVSKY M, CHINTALA S, BOTTOU L.Wasserstein GAN[EB/OL].[2018-02-05].https://arxiv.org/pdf/1701.07875.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Wasserstein GAN">
                                        <b>[13]</b>
                                         ARJOVSKY M, CHINTALA S, BOTTOU L.Wasserstein GAN[EB/OL].[2018-02-05].https://arxiv.org/pdf/1701.07875.pdf.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_14" title=" METZ L, POOLE B, PFAU D, et al.Unrolled generative adversarial networks[EB/OL].[2018-02-07].https://arxiv.org/pdf/1611.02163.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Unrolled generative adversarial networks">
                                        <b>[14]</b>
                                         METZ L, POOLE B, PFAU D, et al.Unrolled generative adversarial networks[EB/OL].[2018-02-07].https://arxiv.org/pdf/1611.02163.pdf.
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_15" >
                                        <b>[15]</b>
                                     LECUN Y, BOTTOU L, BENGIO Y, et al.Gradient-based learning applied to document recognition[J].Proceedings of the IEEE, 1998, 86 (11) :2278-2324.</a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_16" title=" KRIZHEVSKY A, HINTON G.Learning multiple layers of features from tiny images[J].Handbook of Systemic Autoimmune Diseases, 2009, 1 (4) :32-33." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning Multiple Layers of Features from Tiny Images">
                                        <b>[16]</b>
                                         KRIZHEVSKY A, HINTON G.Learning multiple layers of features from tiny images[J].Handbook of Systemic Autoimmune Diseases, 2009, 1 (4) :32-33.
                                    </a>
                                </li>
                                <li id="35">


                                    <a id="bibliography_17" title=" BASTIEN F, LAMBLIN P, PASCANU R, et al.Theano:new features and speed improvements[EB/OL].[2018-01-25].http://export.arxiv.org/pdf/1211.5590." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Theano:new features and speed improvements">
                                        <b>[17]</b>
                                         BASTIEN F, LAMBLIN P, PASCANU R, et al.Theano:new features and speed improvements[EB/OL].[2018-01-25].http://export.arxiv.org/pdf/1211.5590.
                                    </a>
                                </li>
                                <li id="37">


                                    <a id="bibliography_18" title=" SALIMANS T, KINGMA D P.Weight normalization:a simple reparameterization to accelerate training of deep neural networks[C]//Proceedings of NIPS’16.Barcelona, Spain:NIPS, 2016:901-909." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Weight normalization:a simple reparameterization to accelerate training of deep neural networks">
                                        <b>[18]</b>
                                         SALIMANS T, KINGMA D P.Weight normalization:a simple reparameterization to accelerate training of deep neural networks[C]//Proceedings of NIPS’16.Barcelona, Spain:NIPS, 2016:901-909.
                                    </a>
                                </li>
                                <li id="39">


                                    <a id="bibliography_19" title=" MIYATO T, MAEDA S, KOYAMA M, et al.Distributional smoothing with virtual adversarial training[EB/OL].[2018-01-26].https://arxiv.org/pdf/1507.00677.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Distributional smoothing with virtual adversarial training">
                                        <b>[19]</b>
                                         MIYATO T, MAEDA S, KOYAMA M, et al.Distributional smoothing with virtual adversarial training[EB/OL].[2018-01-26].https://arxiv.org/pdf/1507.00677.pdf.
                                    </a>
                                </li>
                                <li id="41">


                                    <a id="bibliography_20" title=" RADFORD A, METZ L, CHINTALA S.Unsupervised representation learning with deep convolutional generative adversarial networks[EB/OL].[2018-02-10].https://arxiv.org/pdf/1511.06434.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks">
                                        <b>[20]</b>
                                         RADFORD A, METZ L, CHINTALA S.Unsupervised representation learning with deep convolutional generative adversarial networks[EB/OL].[2018-02-10].https://arxiv.org/pdf/1511.06434.pdf.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJC" target="_blank">计算机工程</a>
                2019,45(05),155-160+168 DOI:10.19678/j.issn.1000-3428.0050529            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于分段损失的生成对抗网络</b></span>
 <span class="shoufa"></span>                                     </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%88%98%E5%85%B6%E5%BC%80&amp;code=39136423&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmF3cmhWVUU1ZkxoamhGOGFaaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">刘其开</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%A7%9C%E4%BB%A3%E7%BA%A2&amp;code=09282308&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmF3cmhWVUU1ZkxoamhGOGFaaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">姜代红</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%8E%E6%96%87%E5%90%89&amp;code=37695824&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmF3cmhWVUU1ZkxoamhGOGFaaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">李文吉</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E7%9F%BF%E4%B8%9A%E5%A4%A7%E5%AD%A6%E4%BF%A1%E6%81%AF%E4%B8%8E%E6%8E%A7%E5%88%B6%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0041682&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmF3cmhWVUU1ZkxoamhGOGFaaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国矿业大学信息与控制工程学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%BE%90%E5%B7%9E%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2%E4%BF%A1%E7%94%B5%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0164774&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmF3cmhWVUU1ZkxoamhGOGFaaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">徐州工程学院信电工程学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E5%9B%BD%E5%9C%9F%E8%B5%84%E6%BA%90%E8%88%AA%E7%A9%BA%E7%89%A9%E6%8E%A2%E9%81%A5%E6%84%9F%E4%B8%AD%E5%BF%83%E5%9B%BD%E5%9C%9F%E8%B5%84%E6%BA%90%E9%83%A8%E8%88%AA%E7%A9%BA%E5%9C%B0%E7%90%83%E7%89%A9%E7%90%86%E4%B8%8E%E9%81%A5%E6%84%9F%E5%9C%B0%E8%B4%A8%E9%87%8D%E7%82%B9%E5%AE%9E%E9%AA%8C%E5%AE%A4&amp;code=1042135&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmF3cmhWVUU1ZkxoamhGOGFaaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国国土资源航空物探遥感中心国土资源部航空地球物理与遥感地质重点实验室</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>生成对抗网络 (GAN) 在训练过程中未能有效进行生成器与鉴别器间的同步更新, 导致模型训练不稳定并出现模式崩溃的现象。为此, 提出一种基于分段损失的生成对抗网络PL-GAN。生成器在不同的训练时期采用不同形式的损失函数, 同时引入真实样本与生成样本之间的特征级损失, 从而使鉴别器提取的特征更具有鲁棒性。MNIST和CIFAR-10数据集上的实验结果表明, 与regular GAN、feature-wise GAN相比, PL-GAN具有更高的分类精度与运行效率。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmF3cmhWVUU1ZkxoamhGOGFaaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">生成对抗网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%A8%A1%E5%BC%8F%E5%B4%A9%E6%BA%83&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmF3cmhWVUU1ZkxoamhGOGFaaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">模式崩溃;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%89%B9%E5%BE%81%E7%BA%A7%E6%8D%9F%E5%A4%B1&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmF3cmhWVUU1ZkxoamhGOGFaaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">特征级损失;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%88%86%E6%AE%B5%E6%8D%9F%E5%A4%B1&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmF3cmhWVUU1ZkxoamhGOGFaaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">分段损失;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmF3cmhWVUU1ZkxoamhGOGFaaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">半监督学习;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    刘其开 (1992—) , 男, 硕士, 主研方向为深度学习、计算机视觉;E-mail: liuqikai_0127728@ 163. com;
                                </span>
                                <span>
                                    姜代红, 教授、博士;;
                                </span>
                                <span>
                                    李文吉, 工程师、硕士。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-02-27</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金 (51574232);</span>
                                <span>国土资源部航空地球物理与遥感地质重点实验室航遥青年创新基金 (2016YFL02);</span>
                                <span>徐州市科技计划项目 (KC16SQ78);</span>
                    </p>
            </div>
                    <h1><b>Generative Adversarial Network Based on Piecewise Loss</b></h1>
                    <h2>
                    <span>LIU Qikai</span>
                    <span>JIANG Daihong</span>
                    <span>LI Wenji</span>
            </h2>
                    <h2>
                    <span>School of Information and Control Engineering, China University of Mining and Technology</span>
                    <span>School of Information and Electrical Engineering, Xuzhou University of Technology</span>
                    <span>Key Laboratory of Airborne Geophysics and Remote Sensing Geology, Ministry of Land and Resources, China Aerospace Geophysical Survey and Remote Sensing Center for Land and Resources</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Generative Adversarial Network (GAN) fails to effectively execute the synchronous update between generator and discriminator during training, resulting in unstable model training and mode collapse.To solve this problem, a generative adversarial network PL-GAN based on piecewise loss is proposed.The generator uses different loss functions in different training periods, and introduces the feature-wise loss between the real sample and the generated sample, which makes the feature extracted by the discriminator more robust.Experimental results on MNIST and CIFAR-10 datasets show that PL-GAN has higher classification accuracy and operation efficiency than regular GAN and feature-wise GAN.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Generative%20Adversarial%20Network%20(GAN)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmF3cmhWVUU1ZkxoamhGOGFaaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Generative Adversarial Network (GAN) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=mode%20collapse&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmF3cmhWVUU1ZkxoamhGOGFaaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">mode collapse;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=feature-wise%20loss&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmF3cmhWVUU1ZkxoamhGOGFaaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">feature-wise loss;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=piecewise%20loss&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmF3cmhWVUU1ZkxoamhGOGFaaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">piecewise loss;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Semi-Supervised%20Learning%20(SSL)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmF3cmhWVUU1ZkxoamhGOGFaaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Semi-Supervised Learning (SSL) ;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-02-27</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="43" name="43" class="anchor-tag">0 概述</h3>
                <div class="p1">
                    <p id="44">根据训练样本有无标签, 机器学习算法可分为带标签的监督学习和无标签的无监督学习。在实际应用中, 监督学习的标签数据获取成本较高, 无监督学习的性能较差, 因此, 半监督学习 (Semi-Supervised Learning, SSL) <citation id="161" type="reference"><link href="3" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>成为人们广泛关注的研究热点之一。SSL利用海量无标签样本和少量带标签样本学习具有鲁棒性的特征, 在图像分类方面具有较好的性能表现。文献<citation id="162" type="reference">[<a class="sup">2</a>]</citation>提出一种对无标签数据进行伪标签的方法, 以进行模型训练。文献<citation id="163" type="reference">[<a class="sup">3</a>]</citation>构建基于自动编码器的阶梯网络, 其中, 编码器用于监督学习, 解码器的每一层与编码器一一对应形成阶梯, 用于无监督学习训练。</p>
                </div>
                <div class="p1">
                    <p id="45">近年来, 生成对抗网络 (Generative Adversarial Network, GAN) <citation id="164" type="reference"><link href="9" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>与深度生成模型在SSL中得到应用。文献<citation id="165" type="reference">[<a class="sup">5</a>]</citation>提出灵活的深度生成模型算法, 其中包括隐含特征判别模型M1和生成半监督模型M2, 前者学习输入样本在高维空间中的流形, 后者结合标签信息推断样本的后验概率<i>p</i><sub><i>θ</i></sub> (<i>y</i>|<i>x</i>) , 算法最后将两者结合堆叠为生成半监督模型M1+M2。文献<citation id="166" type="reference">[<a class="sup">6</a>]</citation>建立辅助深度生成网络 (Auxiliary Depth Generation Network, ADGN) , 其通过引入辅助的隐变量来改进变分近似算法。文献<citation id="167" type="reference">[<a class="sup">7</a>]</citation>构建对抗学习推理模型ALI, 该模型通过引入一个推理器将样本数据空间映射到隐变量空间, 输出样本与隐变量的联合分布, 相应地, 生成器输出隐变量与生成样本的联合分布, 鉴别器判断输入的联合分布的来源。文献<citation id="168" type="reference">[<a class="sup">8</a>]</citation>中的Cat-GAN在目标函数中引入权衡输入样本与对应预测类别的互信息, 通过最大化生成数据类别的条件交叉熵来训练分类器。文献<citation id="169" type="reference">[<a class="sup">9</a>]</citation>将鉴别器的二分类概率输出扩展到<i>N</i>+1类别概率输出。文献<citation id="170" type="reference">[<a class="sup">10</a>]</citation>提出的Triple GAN通过引入额外的分类器, 以解决生成器与鉴别器在训练时无法同时达到最优的问题。文献<citation id="171" type="reference">[<a class="sup">11</a>]</citation>利用无标签数据对GAN的鉴别器进行预训练, 对少量有标签数据微调后用于分类任务。</p>
                </div>
                <div class="p1">
                    <p id="46">GAN在训练过程中易出现模型不稳定以及模式崩溃现象, 对此, 文献<citation id="172" type="reference">[<a class="sup">12</a>]</citation>从理论上进行了分析。当生成样本分布与真实样本分布之间没有重叠或重叠部分可忽略为0时, 生成器的损失梯度近似常数。常规GAN采用的JS散度要求分布之间没有重叠。Wesserstein-GAN<citation id="173" type="reference"><link href="27" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>通过设计满足一定约束条件的神经网络逼近函数来近似度量2个分布间的距离。文献<citation id="174" type="reference">[<a class="sup">14</a>]</citation>针对模式崩溃问题, 提出一种基于梯度损失的Unrolled GAN, 其通过计算二阶梯度来指示生成器一阶梯度变化的方向。</p>
                </div>
                <div class="p1">
                    <p id="47">本文提出一种基于分段损失 (Piecewise Loss, PL) 的生成对抗网络PL-GAN。通过设置时间参数使生成器在不同的训练阶段采用不同形式的损失函数。此外, 引入生成样本与真实样本在特征空间中的均方差损失, 以增强鉴别器所提取特征的鲁棒性。</p>
                </div>
                <h3 id="48" name="48" class="anchor-tag">1 PL-GAN的设计与实现</h3>
                <div class="p1">
                    <p id="49">GAN可以理解为2个玩家进行相互博弈的游戏, 即生成器 (<i>G</i>) 与鉴别器 (<i>D</i>) 之间的“零和游戏”。<i>G</i>的输入是噪声变量<i>z</i>, 目的是拟合真实样本的数据分布, 输出生成样本<i>G</i> (<i>z</i>) 。<i>D</i>判断输入的样本是来自真实样本还是生成样本, 输出输入样本来自真实样本的概率<i>D</i> (<i>x</i>) 。因此, <i>D</i>的优化目标是使输入为真实样本的概率尽可能为1, 输入为生成样本的概率尽可能为0。<i>G</i>的优化目标是最大化<i>D</i>给出错误判断的概率。算法最终的优化目标是寻找两者之间的一个纳什均衡, 该优化过程是一个极大极小的对抗过程, 目标函数<i>V</i> (<i>D</i>, <i>G</i>) 为:</p>
                </div>
                <div class="p1">
                    <p id="50" class="code-formula">
                        <mathml id="50"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mi>G</mi></munder><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mi>D</mi></munder><mspace width="0.25em" /><mi>V</mi><mo stretchy="false"> (</mo><mi>D</mi><mo>, </mo><mi>G</mi><mo stretchy="false">) </mo><mo>=</mo><mi>E</mi><msub><mrow></mrow><mrow><mi>x</mi><mo>∼</mo><mi>p</mi><msub><mrow></mrow><mrow><mtext>d</mtext><mtext>a</mtext><mtext>t</mtext><mtext>a</mtext></mrow></msub><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo></mrow></msub><mo stretchy="false">[</mo><mrow><mi>ln</mi></mrow><mspace width="0.25em" /><mi>D</mi><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mo>+</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mi>E</mi><msub><mrow></mrow><mrow><mi>z</mi><mo>∼</mo><mi>p</mi><msub><mrow></mrow><mrow><mtext>n</mtext><mtext>o</mtext><mtext>i</mtext><mtext>s</mtext><mtext>e</mtext></mrow></msub><mo stretchy="false"> (</mo><mi>z</mi><mo stretchy="false">) </mo></mrow></msub><mo stretchy="false">[</mo><mrow><mi>ln</mi></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>D</mi><mo stretchy="false"> (</mo><mi>G</mi><mo stretchy="false"> (</mo><mi>z</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="51">其中, <i>p</i><sub>data</sub> (<i>x</i>) 表示真实样本的分布, <i>p</i><sub>noise</sub> (<i>z</i>) 表示随机噪声的分布, <i>E</i>表示期望值。</p>
                </div>
                <h4 class="anchor-tag" id="52" name="52">1.1 算法思想</h4>
                <div class="p1">
                    <p id="53">GAN的最终目标是通过双玩家游戏策略隐式训练一个逼近真实样本的生成器<i>p</i><sub><i>g</i></sub>=<i>G</i><sub><i>θ</i></sub> (<i>z</i>) , 其中, <i>D</i>要尽可能地鉴别出真实样本与生成样本, 其损失函数为:</p>
                </div>
                <div class="p1">
                    <p id="54"><i>D</i><sub>loss</sub>=-<i>E</i><sub><i>x</i>～<i>p</i><sub>data</sub></sub>[ln <i>D</i> (<i>x</i>) ]-</p>
                </div>
                <div class="p1">
                    <p id="55"><i>E</i><sub><i>z</i>～<i>p</i><sub>noise</sub></sub>[ln (1-<i>D</i> (<i>G</i> (<i>z</i>) ) ) ]      (2) </p>
                </div>
                <div class="p1">
                    <p id="56"><i>G</i>的损失函数有以下2种形式:</p>
                </div>
                <div class="p1">
                    <p id="57"><i>G</i><sub>loss_first</sub>=<i>E</i><sub><i>z</i>～<i>p</i><sub>noise</sub></sub>ln (1-<i>D</i> (<i>G</i> (<i>z</i>) ) ) =</p>
                </div>
                <div class="p1">
                    <p id="58"><i>E</i><sub><i>x</i>～<i>p</i><sub><i>g</i></sub></sub>ln (1-<i>D</i> (<i>x</i>) )      (3) </p>
                </div>
                <div class="p1">
                    <p id="59"><i>G</i><sub>loss_second</sub>=-<i>E</i><sub><i>z</i>～<i>p</i><sub>noise</sub></sub>ln <i>D</i> (<i>G</i> (<i>z</i>) )      (4) </p>
                </div>
                <div class="p1">
                    <p id="60">当<i>p</i><sub>data</sub> (<i>x</i>) =<i>p</i><sub><i>g</i></sub> (<i>x</i>) 时, 最优鉴别器表示为:</p>
                </div>
                <div class="p1">
                    <p id="61" class="code-formula">
                        <mathml id="61"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>D</mi><msup><mrow></mrow><mo>*</mo></msup><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mrow><mi>p</mi><msub><mrow></mrow><mrow><mtext>d</mtext><mtext>a</mtext><mtext>t</mtext><mtext>a</mtext></mrow></msub><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo></mrow><mrow><mi>p</mi><msub><mrow></mrow><mrow><mtext>d</mtext><mtext>a</mtext><mtext>t</mtext><mtext>a</mtext></mrow></msub><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mo>+</mo><mi>p</mi><msub><mrow></mrow><mi>g</mi></msub><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo></mrow></mfrac><mo>=</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>5</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="62">在最优鉴别器下, 将式 (5) 代入式 (1) 得:</p>
                </div>
                <div class="p1">
                    <p id="63" class="code-formula">
                        <mathml id="63"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mi>G</mi></munder><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mi>D</mi></munder><mspace width="0.25em" /><mi>V</mi><mo stretchy="false"> (</mo><mi>D</mi><mo>, </mo><mi>G</mi><mo stretchy="false">) </mo><mo>=</mo><mi>E</mi><msub><mrow></mrow><mrow><mi>x</mi><mo>∼</mo><mi>p</mi><msub><mrow></mrow><mi>g</mi></msub></mrow></msub><mrow><mi>ln</mi></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>D</mi><msup><mrow></mrow><mo>*</mo></msup><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo>+</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mi>E</mi><msub><mrow></mrow><mrow><mi>x</mi><mo>∼</mo><mi>p</mi><msub><mrow></mrow><mrow><mtext>d</mtext><mtext>a</mtext><mtext>t</mtext><mtext>a</mtext></mrow></msub></mrow></msub><mrow><mi>ln</mi></mrow><mo stretchy="false"> (</mo><mi>D</mi><msup><mrow></mrow><mo>*</mo></msup><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>6</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="64">将式 (6) 代入<i>D</i><sup>*</sup> (<i>x</i>) 后, 引入2个衡量相似度的指标KL散度与JS散度, 计算如下:</p>
                </div>
                <div class="p1">
                    <p id="65" class="code-formula">
                        <mathml id="65"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>Κ</mi><mi>L</mi><mo stretchy="false"> (</mo><mi>Ρ</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">∥</mo><mi>Ρ</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">) </mo><mo>=</mo><mi>E</mi><msub><mrow></mrow><mrow><mi>x</mi><mo>∼</mo><mi>Ρ</mi><msub><mrow></mrow><mn>1</mn></msub></mrow></msub><mrow><mi>ln</mi></mrow><mfrac><mrow><mi>Ρ</mi><msub><mrow></mrow><mn>1</mn></msub></mrow><mrow><mi>Ρ</mi><msub><mrow></mrow><mn>2</mn></msub></mrow></mfrac></mtd></mtr><mtr><mtd><mi>J</mi><mi>S</mi><mo stretchy="false"> (</mo><mi>Ρ</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">∥</mo><mi>Ρ</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">) </mo><mo>=</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mi>Κ</mi><mi>L</mi><mo stretchy="false"> (</mo><mi>Ρ</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">∥</mo><mfrac><mrow><mi>Ρ</mi><msub><mrow></mrow><mn>1</mn></msub><mo>+</mo><mi>Ρ</mi><msub><mrow></mrow><mn>2</mn></msub></mrow><mn>2</mn></mfrac><mo stretchy="false">) </mo><mo>+</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mi>Κ</mi><mi>L</mi><mo stretchy="false"> (</mo><mi>Ρ</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">∥</mo><mfrac><mrow><mi>Ρ</mi><msub><mrow></mrow><mn>1</mn></msub><mo>+</mo><mi>Ρ</mi><msub><mrow></mrow><mn>2</mn></msub></mrow><mn>2</mn></mfrac><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="66">在最优鉴别器下, <i>G</i>的第1种损失函数等价为:</p>
                </div>
                <div class="p1">
                    <p id="67"><i>G</i><sub>loss_first|<i>D</i><sup>*</sup></sub>=2<i>JS</i> (<i>p</i><sub>data</sub>‖<i>p</i><sub><i>g</i></sub>) -2ln 2      (7) </p>
                </div>
                <div class="p1">
                    <p id="68">由式 (7) 可以看出, 在最优鉴别器下, 最小化生成器的损失等价为最小化生成样本与真实样本之间的JS散度。文献<citation id="175" type="reference">[<a class="sup">12</a>]</citation>研究表明, 在第1种损失函数下, 用JS散度衡量分布差异的前提是2个分布间存在不可忽略的重叠, 否则JS散度将会是一个常数。但网络初始化后的生成样本分布很难与真实样本分布存在不可忽略的重叠。</p>
                </div>
                <div class="p1">
                    <p id="69">同样, 由式 (6) 、式 (7) 可得<i>G</i>的第2种损失函数为:</p>
                </div>
                <div class="p1">
                    <p id="70"><i>G</i><sub>loss_second</sub>=<i>KL</i> (<i>p</i><sub><i>g</i></sub>‖<i>p</i><sub>data</sub>) -<i>E</i><sub><i>x</i>～<i>p</i><sub><i>g</i></sub></sub>ln (1-<i>D</i><sup>*</sup> (<i>x</i>) ) =</p>
                </div>
                <div class="p1">
                    <p id="71"><i>KL</i> (<i>p</i><sub><i>g</i></sub>‖<i>p</i><sub>data</sub>) -2<i>JS</i> (<i>p</i><sub>data</sub>‖<i>p</i><sub><i>g</i></sub>) +2ln 2+</p>
                </div>
                <div class="p1">
                    <p id="72"><i>E</i><sub><i>x</i>～<i>p</i><sub>data</sub></sub>ln (<i>D</i><sup>*</sup> (<i>x</i>) )      (8) </p>
                </div>
                <div class="p1">
                    <p id="73">由于式 (8) 右边的后2项不依赖<i>G</i>, 即最小化式 (4) 等价于最小化式 (9) 。</p>
                </div>
                <div class="p1">
                    <p id="74"><i>G</i><sub>loss_second|<i>D</i><sup>*</sup></sub>=<i>KL</i> (<i>p</i><sub><i>g</i></sub>‖<i>p</i><sub>data</sub>) -2<i>JS</i> (<i>p</i><sub>data</sub>‖<i>p</i><sub><i>g</i></sub>)      (9) </p>
                </div>
                <div class="p1">
                    <p id="75">该目标形式一方面要求最小化生成分布与真实分布间的KL散度, 另一方面又要求最大化两者的JS散度, 即优化目标相互矛盾。此外, <i>KL</i> (<i>p</i><sub><i>g</i></sub>‖<i>p</i><sub>data</sub>) 不是一个对称的度量, 当<i>p</i><sub>data</sub>与<i>p</i><sub><i>g</i></sub>的取值相对改变时, KL散度也会变化, 这就迫使生成器生成大量重复且置信度较高的样本, 最终导致了模式崩溃现象。</p>
                </div>
                <div class="p1">
                    <p id="76">为解决生成器第1种损失函数存在的问题, WGAN提出对生成样本与真实样本添加噪声的方法, 使得原本的2个低维流形弥散到整个高维空间, 迫使它们产生不可忽略的重叠。此时, JS散度将发挥作用, 从而避免了梯度消失。随着训练的进行, 逐渐对噪声退火, JS散度继续产生有意义的梯度, 拉近2个低维流形直到完全重合。</p>
                </div>
                <div class="p1">
                    <p id="77">本文借鉴噪声退火的思想, 通过引入时间参数<i>w</i> (<i>t</i>) =exp[-10 (1-<i>t</i>) <sup>2</sup>]来控制生成器在不同的训练阶段采用不同形式的损失函数。训练前期以<i>E</i><sub><i>z</i>～<i>p</i><sub>noise</sub></sub>[-ln <i>D</i> (<i>G</i> (<i>z</i>) ) ]为主, 随着训练的进行, 真实样本和生成样本将发生重叠, 训练进行到某一阶段再切换到以<i>E</i><sub><i>z</i>～<i>p</i><sub>noise</sub></sub>[ln (1-<i>D</i> (<i>G</i> (<i>z</i>) ) ) ]为主, 此时, JS散度将发挥作用。同时, 为给生成器提供足够的梯度, 引入生成样本与真实样本之间特征级的均方差损失。最终, 生成器的损失函数如下:</p>
                </div>
                <div class="p1">
                    <p id="78" class="code-formula">
                        <mathml id="78"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mi>G</mi></munder></mrow><mspace width="0.25em" /><mi>V</mi><mo stretchy="false"> (</mo><mi>G</mi><mo stretchy="false">) </mo><mo>=</mo><mi>α</mi><mo stretchy="false">{</mo><mi>w</mi><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo><mi>E</mi><msub><mrow></mrow><mrow><mi>z</mi><mo>∼</mo><mi>p</mi><msub><mrow></mrow><mrow><mtext>n</mtext><mtext>o</mtext><mtext>i</mtext><mtext>s</mtext><mtext>e</mtext></mrow></msub></mrow></msub><mo stretchy="false">[</mo><mrow><mi>ln</mi></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>D</mi><mo stretchy="false"> (</mo><mi>G</mi><mo stretchy="false"> (</mo><mi>z</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mo>+</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>w</mi><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mi>E</mi><msub><mrow></mrow><mrow><mi>z</mi><mo>∼</mo><mi>p</mi><msub><mrow></mrow><mrow><mtext>n</mtext><mtext>o</mtext><mtext>i</mtext><mtext>s</mtext><mtext>e</mtext></mrow></msub></mrow></msub><mrow><mi>ln</mi></mrow><mo stretchy="false"> (</mo><mo>-</mo><mi>D</mi><mo stretchy="false"> (</mo><mi>G</mi><mo stretchy="false"> (</mo><mi>z</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo stretchy="false">}</mo><mo>+</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mi>β</mi><mo stretchy="false">{</mo><mo stretchy="false">∥</mo><mi>E</mi><msub><mrow></mrow><mrow><mi>x</mi><mo>∼</mo><mi>p</mi><msub><mrow></mrow><mrow><mtext>d</mtext><mtext>a</mtext><mtext>t</mtext><mtext>a</mtext></mrow></msub></mrow></msub><mi>D</mi><msub><mrow></mrow><mi>f</mi></msub><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mo>-</mo><mi>E</mi><msub><mrow></mrow><mrow><mi>z</mi><mo>∼</mo><mi>p</mi><msub><mrow></mrow><mrow><mtext>n</mtext><mtext>o</mtext><mtext>i</mtext><mtext>s</mtext><mtext>e</mtext></mrow></msub></mrow></msub><mi>D</mi><msub><mrow></mrow><mi>f</mi></msub><mo stretchy="false"> (</mo><mi>G</mi><mo stretchy="false"> (</mo><mi>z</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup><mo stretchy="false">}</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>0</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="79">其中, <i>D</i><sub><i>f</i></sub> (·) 表示鉴别器特征层的输出, <i>α</i>表示对抗损失的权重系数, <i>β</i>表示特征级损失系数。PL-GAN算法流程如图1所示, 其中, <i>epoch</i>表示当前迭代次数, <i>T</i>表示损失切换迭代次数, 实线表示前向传播, 虚线表示后向的参数更新。</p>
                </div>
                <div class="area_img" id="80">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201905025_080.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmF3cmhWVUU1ZkxoamhGOGFaaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 PL-GAN算法流程" src="Detail/GetImg?filename=images/JSJC201905025_080.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmF3cmhWVUU1ZkxoamhGOGFaaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图1 PL-GAN算法流程</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201905025_080.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmF3cmhWVUU1ZkxoamhGOGFaaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="81">考虑到GAN的监督损失, 假设标准的分类器输出是<i>N</i>维向量<b><i>logits</i></b>={<i>l</i><sub>1</sub>, <i>l</i><sub>2</sub>, …, <i>l</i><sub><i>N</i></sub>}, <i>N</i>为样本的类别数。则用softmax计算的输出概率为:</p>
                </div>
                <div class="p1">
                    <p id="82" class="code-formula">
                        <mathml id="82"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><msub><mrow></mrow><mrow><mtext>m</mtext><mtext>o</mtext><mtext>d</mtext><mtext>e</mtext><mtext>l</mtext></mrow></msub><mo stretchy="false"> (</mo><mi>y</mi><mo>=</mo><mi>j</mi><mo stretchy="false">|</mo><mi>x</mi><mo stretchy="false">) </mo><mo>=</mo><mrow><mi>exp</mi></mrow><mo stretchy="false"> (</mo><mi>l</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mo>/</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mrow><mi>exp</mi></mrow></mstyle><mo stretchy="false"> (</mo><mi>l</mi><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="83">将生成样本所属的类别定义为第<i>N</i>+1类, 则来自生成样本的概率为<i>P</i><sub>model</sub> (<i>y</i>=<i>N</i>+1|<i>x</i>) , 对应常规GAN的1-<i>D</i> (<i>x</i>) 。假设鉴别器的训练样本一半来自真实样本, 一半来自生成样本, 则<i>D</i>的损失函数为:</p>
                </div>
                <div class="p1">
                    <p id="84"><i>C</i> (<i>D</i>) =-<i>E</i><sub><i>x</i>, <i>y</i>～<i>p</i><sub>data (<i>x</i>, <i>y</i>) </sub></sub>[ln <i>P</i><sub>model</sub> (<i>y</i>|<i>x</i>) ]-</p>
                </div>
                <div class="p1">
                    <p id="85"><i>E</i><sub><i>x</i>～<i>p</i><sub><i>g</i></sub></sub>[ln <i>P</i><sub>model</sub> (<i>y</i>=<i>N</i>+1|<i>x</i>) ]=</p>
                </div>
                <div class="p1">
                    <p id="86"><i>C</i><sub>sup</sub>+<i>C</i><sub>adv</sub>      (12) </p>
                </div>
                <div class="p1">
                    <p id="87">其中, 真实样本由带标签样本与不带标签样本组成。带标签样本参与的监督损失为:</p>
                </div>
                <div class="p1">
                    <p id="88"><i>C</i><sub>sup</sub>=-<i>E</i><sub><i>x</i>, <i>y</i>～<i>p</i><sub>data (<i>x</i>, <i>y</i>) </sub></sub>[ln <i>P</i><sub>model</sub> (<i>y</i>|<i>x</i>, <i>y</i>&lt;<i>N</i>+1) ]      (13) </p>
                </div>
                <div class="p1">
                    <p id="89">不带标签的真实样本与生成样本参与的无监督对抗损失为:</p>
                </div>
                <div class="p1">
                    <p id="90"><i>C</i><sub>adv</sub>=-<i>E</i><sub><i>x</i>～<i>p</i><sub>data</sub> (<i>x</i>) </sub>[ln (1-<i>P</i><sub>model</sub> (<i>y</i>=<i>N</i>+1|<i>x</i>) ) ]-</p>
                </div>
                <div class="p1">
                    <p id="91"><i>E</i><sub><i>x</i>～<i>p</i><sub><i>g</i></sub></sub>[ln <i>P</i><sub>model</sub> (<i>y</i>=<i>N</i>+1|<i>x</i>) ]      (14) </p>
                </div>
                <div class="p1">
                    <p id="92">令<i>D</i> (<i>x</i>) =1-<i>P</i><sub>model</sub> (<i>y</i>=<i>N</i>+1|<i>x</i>) , 则有:</p>
                </div>
                <div class="p1">
                    <p id="93"><i>C</i><sub>adv</sub>=-<i>E</i><sub><i>x</i>～<i>p</i><sub>data</sub> (<i>x</i>) </sub>ln (1-<i>D</i> (<i>x</i>) ) -<i>E</i><sub><i>x</i>～<i>p</i><sub><i>g</i></sub></sub>ln (1-<i>D</i> (<i>x</i>) )      (15) </p>
                </div>
                <div class="p1">
                    <p id="94">计算监督损失与无监督对抗损失成为算法实现的关键。图2所示为GAN半监督图像分类流程。其中, 标签样本为<i>D</i>贡献监督损失, 无标签样本为<i>D</i>贡献无监督对抗损失。</p>
                </div>
                <div class="area_img" id="95">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201905025_095.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmF3cmhWVUU1ZkxoamhGOGFaaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 GAN半监督图像分类流程" src="Detail/GetImg?filename=images/JSJC201905025_095.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmF3cmhWVUU1ZkxoamhGOGFaaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图2 GAN半监督图像分类流程</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201905025_095.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmF3cmhWVUU1ZkxoamhGOGFaaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="96">从优化目标方面分析, 存在一个未知的映射函数<i>f</i> (<i>x</i>) , 使∀<i>j</i>&lt;<i>N</i>+1, <i>P</i> (<i>y</i>=<i>j</i>, <i>x</i>) =<i>f</i> (<i>x</i>) ·exp[<i>l</i><sub><i>j</i></sub> (<i>x</i>) ]、<i>P</i><sub><i>G</i></sub> (<i>x</i>) =<i>f</i> (<i>x</i>) ·exp[<i>l</i><sub><i>N</i>+1</sub> (<i>x</i>) ]成立<citation id="176" type="reference"><link href="19" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>。由于鉴别器输出维度为<i>N</i>+1的概率是过参数化的, 假设∀<i>x</i>, <i>l</i><sub><i>N</i>+1</sub> (<i>x</i>) =0成立, 则不改变鉴别器softmax的概率值。此时, <mathml id="97"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>D</mi><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mo>=</mo></mrow><mi>z</mi><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mo>/</mo><mo stretchy="false"> (</mo><mi>z</mi><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mo>+</mo><mn>1</mn><mo stretchy="false">) </mo><mo>, </mo><mi>z</mi><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mo>=</mo><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mtext>e</mtext></mstyle><mtext>x</mtext><mtext>p</mtext></mrow><mo stretchy="false">[</mo><mi>l</mi><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mo stretchy="false">]</mo></mrow></math></mathml>。</p>
                </div>
                <h4 class="anchor-tag" id="98" name="98">1.2 算法步骤</h4>
                <div class="p1">
                    <p id="99">本文基于分段损失的生成对抗网络半监督分类算法PL-GAN描述如下。确定最大迭代次数与损失切换迭代次数后, 设时间参数<i>w</i> (<i>t</i>) =exp (-10 (1-<i>t</i>) <sup>2</sup>) , <i>t</i>等于当前迭代次数<i>epoch</i>与损失切换迭代次数<i>T</i>的比值。</p>
                </div>
                <div class="p1">
                    <p id="100"><b>算法1</b> PL-GAN算法</p>
                </div>
                <div class="p1">
                    <p id="101">初始化:设批大小 (Batchsize, 即每一次参数更新时所需的样本数) m=100;iterations为批迭代次数, 表示总训练样本与批大小m的整数比;epoch为迭代次数, 即遍历整个训练集的循环次数;超参数k=1, 表示训练鉴别器k次才训练生成器1次;对抗损失和特征级均方差损失权重系数设置为α=β=0.5;用Xavier方法进行参数初始化;噪声分布P<sub>noise</sub> (z) 满足Z～U[-1, 1];</p>
                </div>
                <div class="area_img" id="182">
                                <img alt="" src="Detail/GetImg?filename=images/JSJC201905025_18200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmF3cmhWVUU1ZkxoamhGOGFaaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                            <p class="img_tit"></p>

                </div>
                <h3 id="128" name="128" class="anchor-tag">2 实验结果与分析</h3>
                <h4 class="anchor-tag" id="129" name="129">2.1 实验数据集</h4>
                <div class="p1">
                    <p id="130">本文采用MNIST<citation id="177" type="reference"><link href="31" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>、CIFAR-10<citation id="178" type="reference"><link href="33" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>数据集进行实验验证。MNIST是深度学习领域常见的手写字体数据集, 共10类 (用数字0～9表示) , 每类包含单通道的6 000个训练样本和1 000个测试样本。CIFAR-10包含60 000张、共10类的三通道样本, 每类有5 000个训练样本和1 000个测试样本。</p>
                </div>
                <h4 class="anchor-tag" id="131" name="131">2.2 参数设置</h4>
                <div class="p1">
                    <p id="132">实验在单块GPU型号为GTX1080的计算机上运行, 基于theano<citation id="179" type="reference"><link href="35" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>深度学习框架实现。对于生成器的损失函数而言, 损失切换迭代次数<i>T</i>以及学习率衰减策略非常重要。本次实验数据集中参数<i>T</i>与学习率衰减因子的设置如表1所示, 其中, 实验所用学习率为初始学习率与衰减因子的乘积。</p>
                </div>
                <div class="area_img" id="133">
                    <p class="img_tit"><b>表1 参数设置</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="133" border="1"><tr><td>数据集</td><td>总迭代次数</td><td><i>T</i></td><td>初始学习率</td><td>衰减因子</td></tr><tr><td>MNIST</td><td>300</td><td>60</td><td>0.003 0</td><td>min (2-<i>epoch</i>/250, 1) </td></tr><tr><td><br />CIFAR-10</td><td>600</td><td>100</td><td>0.000 3</td><td>min (2-<i>epoch</i>/450, 1) </td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="134" name="134">2.3 性能分析</h4>
                <h4 class="anchor-tag" id="135" name="135">2.3.1 MNIST数据集</h4>
                <div class="p1">
                    <p id="136">本次实验的网络结构主要由多层感知机组成, PL-GAN模型的参数配置如表2所示, 为加速模型的训练速度, 在鉴别器结构中使用了权重归一化 (Weight Normalization, WN) <citation id="180" type="reference"><link href="37" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>策略。</p>
                </div>
                <div class="area_img" id="137">
                    <p class="img_tit"><b>表2 MNIST数据集网络参数配置</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="137" border="1"><tr><td><br />鉴别器<i>D</i></td><td>生成器<i>G</i></td></tr><tr><td><br />输入:28×28 gray image<br />one-hot labels<i>y</i>∈<img src="images\JSJC201905025_183.jpg" placement="inline" /><sup>10</sup></td><td>Input Noise∈<img src="images\JSJC201905025_183.jpg" placement="inline" /><sup>100</sup></td></tr><tr><td><br />Denselayer 1 000 Units lReLU, <br />Guassian niose, WN</td><td>Denselayer 500 Units</td></tr><tr><td><br />Denselayer 500 Units, lReLU, <br />Guassian niose, WN</td><td>Softplus, batch norm</td></tr><tr><td><br />Denselayer 250 Units, lReLU, <br />Guassian niose, WN</td><td>Denselayer 500 Units</td></tr><tr><td><br />Denselayer 250 Units, lReLU, <br />Guassian niose, WN</td><td>Softplus, batch norm</td></tr><tr><td><br />Denselayer 10 Units, lReLU, <br />Guassian niose, WN</td><td>Denselayer 784 Units, sigmoid</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="138">将生成样本的质量与半监督的分类精度作为实验结果的评价标准。对比算法主要包括:</p>
                </div>
                <div class="p1">
                    <p id="139">1) 常规GAN (regular GAN) , 其生成器的损失为对抗损失<i>E</i><sub><i>z</i>～<i>p</i><sub>noise</sub></sub>ln (-<i>D</i> (<i>G</i> (<i>z</i>) ) ) 。</p>
                </div>
                <div class="p1">
                    <p id="140">2) 特征级损失GAN (feature-wise GAN) , 其生成器的损失仅为特征级均方差损失‖<i>E</i><sub><i>x</i>～<i>p</i><sub>data</sub> (<i>x</i>) </sub><i>D</i><sub><i>f</i></sub> (<i>x</i>) -<i>E</i><sub><i>z</i>～<i>p</i><sub>noise</sub> (<i>z</i>) </sub><i>D</i><sub><i>f</i></sub> (<i>G</i> (<i>z</i>) ) ‖<mathml id="141"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></math></mathml>。</p>
                </div>
                <div class="p1">
                    <p id="142"><i>PL</i>-<i>GAN</i>在<i>regular GAN</i>的基础上改变对抗损失的形式并引入特征级损失, 经过加权后作为最终损失。图3所示为各算法的生成样本对比结果。由图3可以看出, 与<i>feature</i>-<i>wise GAN</i>相比, <i>PL</i>-<i>GAN</i>生成的样本质量较好, 与<i>regular GAN</i>相比, <i>PL</i>-<i>GAN</i>生成的样本虽然质量稍差, 但其多样性得到明显提升, 即<i>PL</i>-<i>GAN</i>解决了<i>regular GAN</i>的模式崩溃问题。</p>
                </div>
                <div class="area_img" id="143">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201905025_143.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmF3cmhWVUU1ZkxoamhGOGFaaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 MNIST数据集中3种算法的生成样本对比结果" src="Detail/GetImg?filename=images/JSJC201905025_143.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmF3cmhWVUU1ZkxoamhGOGFaaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图3 MNIST数据集中3种算法的生成样本对比结果</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201905025_143.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmF3cmhWVUU1ZkxoamhGOGFaaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="144">图4所示为各算法在训练过程中的损失变化对比。由图4可以看出, 对生成器的损失而言, 相比feature-wise GAN, PL-GAN的损失下降得更加平稳;与regular GAN相比, PL-GAN的损失呈现逐渐下降的趋势, 而并非趋近于一个常数。对鉴别器的损失而言, 相比regular GAN, PL-GAN的损失值较低, 其与feature-wise GAN相当。</p>
                </div>
                <div class="area_img" id="145">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201905025_145.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmF3cmhWVUU1ZkxoamhGOGFaaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 MNIST数据集中算法的损失变化对比结果" src="Detail/GetImg?filename=images/JSJC201905025_145.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmF3cmhWVUU1ZkxoamhGOGFaaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图4 MNIST数据集中算法的损失变化对比结果</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201905025_145.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmF3cmhWVUU1ZkxoamhGOGFaaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="146">在保证模型结构相同的情况下, 当标签样本数为100时, 比较各算法的半监督分类性能, 分类对比结果如图5所示。由图5可以看出, PL-GAN比regular GAN分类错误率低, 其分类性能与feature-wise GAN基本接近, 但收敛速度更快。</p>
                </div>
                <div class="area_img" id="147">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201905025_147.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmF3cmhWVUU1ZkxoamhGOGFaaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 MNIST数据集中3种算法的错误率对比结果" src="Detail/GetImg?filename=images/JSJC201905025_147.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmF3cmhWVUU1ZkxoamhGOGFaaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图5 MNIST数据集中3种算法的错误率对比结果</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201905025_147.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmF3cmhWVUU1ZkxoamhGOGFaaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="148">同样设标签样本数为100, 将PL-GAN与传统的半监督学习算法ADGM、M1+M2、VAT、CatGAN等进行比较, 结果如表3所示。从表3可以看出, PL-GAN具有最低的分类错误率。</p>
                </div>
                <div class="area_img" id="149">
                    <p class="img_tit"><b>表3 MNIST数据集中8种算法分类错误率对比</b> % <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="149" border="1"><tr><td><br />算法</td><td>错误率</td></tr><tr><td><br />Ladder<sup>[3]</sup></td><td>1.06 (±0.37) </td></tr><tr><td><br />M1+M2<sup>[5]</sup></td><td>3.33 (±0.14) </td></tr><tr><td><br />ADGM<sup>[6]</sup></td><td>0.96 (±0.02) </td></tr><tr><td><br />CatGAN<sup>[8]</sup></td><td>1.39 (±0.28) </td></tr><tr><td><br />Improved-GAN<sup>[9]</sup></td><td>0.93 (±0.07) </td></tr><tr><td><br />Triple-GAN<sup>[10]</sup></td><td>0.91 (±0.58) </td></tr><tr><td><br />VAT<sup>[19]</sup></td><td>2.33</td></tr><tr><td><br />PL-GAN</td><td>0.90 (±0.06) </td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="150" name="150">2.3.2 CIFAR-10数据集</h4>
                <div class="p1">
                    <p id="151">本次实验的生成器结构以DCGAN<citation id="181" type="reference"><link href="41" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>框架为基准, 初始学习率设为0.000 3, 为加速模型训练并防止出现过拟合现象, 鉴别器使用WN和Dropout策略。网络模型参数配置如表4所示。</p>
                </div>
                <div class="area_img" id="152">
                    <p class="img_tit"><b>表4 CIFAR-10数据集网络参数配置</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="152" border="1"><tr><td><br />鉴别器<i>D</i></td><td>生成器<i>G</i></td></tr><tr><td><br />输入:32×32 Colored image <br />one-hot labels class<i>y</i>∈<img src="images\JSJC201905025_184.jpg" placement="inline" /><sup>10</sup></td><td>Input Noise∈<img src="images\JSJC201905025_184.jpg" placement="inline" /><sup>100</sup></td></tr><tr><td><br />Dropout=0.2<br />3×3 conv, 96, lReLU, WN<br />3×3 conv, 96, lReLU, WN<br />3×3 conv, 96, lReLU, WN</td><td>MLP 8 192 units<br />ReLU, batch norm<br />Reshape 512×4×4<br />5×5 deconv, 256, stride 2<br />ReLU, batch norm</td></tr><tr><td><br />Dropout=0.2<br />3×3 conv, 192, lReLU, WN<br />3×3 conv, 192, lReLU, WN<br />3×3 conv, 192, lReLU, WN</td><td>5×5 deconv.128.stride 2<br />ReLU, batch norm</td></tr><tr><td><br />Dropout=0.2<br />3×3 conv, 192, lReLU, <br />WN NIN, 192, lReLU, WN<br />NIN, 192, lReLU, WN<br />Global pool layer<br />Denselayer 10 Units with WN</td><td>5×5 deconv.3.stride 2, <br />tanh, weight norm</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="153">为定量衡量生成样本质量的优劣, 利用IS (Inception Score) 指标依次对不同算法生成的5 000个样本进行10次评估, 得到相应的IS值, 结果如表5所示。图6所示为各算法在CIFAR-10数据集上生成样本的视觉对比。</p>
                </div>
                <div class="area_img" id="154">
                    <p class="img_tit"><b>表5 CIFAR-10数据集中3种算法的IS值对比结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="154" border="1"><tr><td><br />算法</td><td>IS值</td></tr><tr><td><br />regular GAN</td><td>6.45±0.22</td></tr><tr><td><br />feature-wise GAN</td><td>5.46±0.20</td></tr><tr><td><br />PL-GAN</td><td>7.30±0.17</td></tr><tr><td><br />真实样本</td><td>10.71±0.44</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="155">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201905025_155.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmF3cmhWVUU1ZkxoamhGOGFaaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 CIFAR-10数据集中3种算法的生成样本视觉对比" src="Detail/GetImg?filename=images/JSJC201905025_155.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmF3cmhWVUU1ZkxoamhGOGFaaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图6 CIFAR-10数据集中3种算法的生成样本视觉对比</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201905025_155.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmF3cmhWVUU1ZkxoamhGOGFaaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="156">在保证模型结构框架相同的情况下, 当标签样本数为4 000时, 各算法的半监督分类错误率对比结果如图7所示。由图7可以看出, PL-GAN比regular GAN分类错误率低, 与feature-wise GAN分类性能基本接近, 但收敛性更好。设带标签的训练样本数为4 000, 将PL-GAN与传统半监督分类算法进行对比, 结果如表6所示, 可以看出, PL-GAN有较好的性能表现, 优于大多数传统半监督分类算法。</p>
                </div>
                <div class="area_img" id="157">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201905025_157.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmF3cmhWVUU1ZkxoamhGOGFaaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 CIFAR-10数据集中3种算法的错误率对比结果" src="Detail/GetImg?filename=images/JSJC201905025_157.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmF3cmhWVUU1ZkxoamhGOGFaaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图7 CIFAR-10数据集中3种算法的错误率对比结果</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201905025_157.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmF3cmhWVUU1ZkxoamhGOGFaaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="158">
                    <p class="img_tit"><b>表6 CIFAR-10数据集中7种算法的错误率对比</b> % <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="158" border="1"><tr><td><br />算法</td><td>错误率</td></tr><tr><td><br />Ladder</td><td>20.40 (±0.47) </td></tr><tr><td><br />ALI<sup>[7]</sup></td><td>18.30</td></tr><tr><td><br />CatGAN</td><td>19.58 (±0.58) </td></tr><tr><td><br />Improved-GAN</td><td>18.63 (±2.32) </td></tr><tr><td><br />Triple GAN</td><td>16.99 (±0.36) </td></tr><tr><td><br />VAT</td><td>24.65</td></tr><tr><td><br />PL-GAN</td><td>17.30 (±0.56) </td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h3 id="159" name="159" class="anchor-tag">3 结束语</h3>
                <div class="p1">
                    <p id="160">本文提出一种基于分段损失的生成对抗网络PL-GAN。通过设置时间参数来改变生成器与鉴别器的训练过程, 使衡量生成分布与真实分布之间差异的JS散度能够更好地发挥作用。在生成器中引入真实样本与生成样本之间的特征级损失, 使训练过程更加稳定。实验结果表明, PL-GAN具有较高的分类精度与较快的收敛速度。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Semi-Supervised Learning">

                                <b>[1]</b> CHAPELLE O, SCHOLKOPF B, ZIEN A.Semi-supervised learning[J].IEEE Transactions on Neural Networks, 2009, 20 (3) :542-546.
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Pseudo-label:the simple and efficient semi-supervised learning method for deep neural networks">

                                <b>[2]</b> LEE D H.Pseudo-label:the simple and efficient semi-supervised learning method for deep neural networks[EB/OL].[2018-02-01].http://deeplearning.net/wp-content/uploads/2013/03/pseudo_label_final.pdf.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Semi-supervised learning with ladder networks">

                                <b>[3]</b> RASMUS A, BERGLUND M, HONKALA M, et al.Semi-supervised learning with ladder networks [EB/OL].[2018-02-01].http://papers.nips.cc/paper/5947-semi-supervised-learning-with-ladder-networks.pdf.
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Semi-supervised learning with deep generative models">

                                <b>[4]</b> KINGMA D P, MOHAMED S, REZENDE D J, et al.Semi-supervised learning with deep generative models[EB/OL].[2018-02-05].https://arxiv.org/pdf/1406.5298.pdf.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Generative adversarial nets">

                                <b>[5]</b> GOODFELLOW I J, POUGET-ABADIE J, MIRZA M, et al.Generative adversarial nets[C]//Proceedings of Inter-national Conference on Neural Information Processing Systems.Cambridge, USA:MIT Press, 2014:2672-2680.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Auxiliary deep generative models">

                                <b>[6]</b> MAAL∅E L, S∅NDERBY C K, S∅NDERBY S K, et al.Auxiliary deep generative models[EB/OL].[2018-02-06].https://arxiv.org/pdf/1602.05473.pdf.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Adver-sarially learned inference">

                                <b>[7]</b> DUMOULIN V, BELGHAZI I, POOLE B, et al.Adver-sarially learned inference[EB/OL].[2018-02-04].https://arxiv.org/pdf/1606.00704.pdf.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Unsupervised and Semi-supervised Learning with Categorical Generative Adversarial Networks">

                                <b>[8]</b> SPRINGENBERG J T.Unsupervised and semi-supervised learning with categorical generative adversarial networks[J].Computer Science, 2015 (6) :2321-2330.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Improved techniques for training gans">

                                <b>[9]</b> SALIMANS T, GOODFELLOW I, ZAREMBA W, et al.Improved techniques for training GANs[C]//Proceedings of NIPS’16.Barcelona, Spain:NIPS, 2016:2234-2242.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Triple generative adversarial nets">

                                <b>[10]</b> LI Chongxuan, XU Kun, ZHU Jun, et al.Triple generative adversarial nets[C]//Proceedings of NIPS’17.Barcelona, Spain:NIPS, 2017:4091-4101.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_11" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201703001&amp;v=MjIwNTJPM3pxcUJ0R0ZyQ1VSTE9lWmVSbkZ5cmtVN3ZNS0NMZlliRzRIOWJNckk5RlpZUUtESDg0dlI0VDZqNTQ=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmF3cmhWVUU1ZkxoamhGOGFaaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[11]</b> 王坤峰, 苟超, 段艳杰, 等.生成式对抗网络 GAN的研究进展与展望[J].自动化学报, 2017, 43 (3) :321-332.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Towards principled methods for training generative adversarial networks">

                                <b>[12]</b> ARJOVSKY M, BOTTOU L.Towards principled methods for training generative adversarial networks[EB/OL].[2018-01-25].https://leon.bottou.org/publications/pdf/iclr-2017.pdf.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Wasserstein GAN">

                                <b>[13]</b> ARJOVSKY M, CHINTALA S, BOTTOU L.Wasserstein GAN[EB/OL].[2018-02-05].https://arxiv.org/pdf/1701.07875.pdf.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Unrolled generative adversarial networks">

                                <b>[14]</b> METZ L, POOLE B, PFAU D, et al.Unrolled generative adversarial networks[EB/OL].[2018-02-07].https://arxiv.org/pdf/1611.02163.pdf.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_15" >
                                    <b>[15]</b>
                                 LECUN Y, BOTTOU L, BENGIO Y, et al.Gradient-based learning applied to document recognition[J].Proceedings of the IEEE, 1998, 86 (11) :2278-2324.
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning Multiple Layers of Features from Tiny Images">

                                <b>[16]</b> KRIZHEVSKY A, HINTON G.Learning multiple layers of features from tiny images[J].Handbook of Systemic Autoimmune Diseases, 2009, 1 (4) :32-33.
                            </a>
                        </p>
                        <p id="35">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Theano:new features and speed improvements">

                                <b>[17]</b> BASTIEN F, LAMBLIN P, PASCANU R, et al.Theano:new features and speed improvements[EB/OL].[2018-01-25].http://export.arxiv.org/pdf/1211.5590.
                            </a>
                        </p>
                        <p id="37">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Weight normalization:a simple reparameterization to accelerate training of deep neural networks">

                                <b>[18]</b> SALIMANS T, KINGMA D P.Weight normalization:a simple reparameterization to accelerate training of deep neural networks[C]//Proceedings of NIPS’16.Barcelona, Spain:NIPS, 2016:901-909.
                            </a>
                        </p>
                        <p id="39">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Distributional smoothing with virtual adversarial training">

                                <b>[19]</b> MIYATO T, MAEDA S, KOYAMA M, et al.Distributional smoothing with virtual adversarial training[EB/OL].[2018-01-26].https://arxiv.org/pdf/1507.00677.pdf.
                            </a>
                        </p>
                        <p id="41">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks">

                                <b>[20]</b> RADFORD A, METZ L, CHINTALA S.Unsupervised representation learning with deep convolutional generative adversarial networks[EB/OL].[2018-02-10].https://arxiv.org/pdf/1511.06434.pdf.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJC201905025" />
        <input id="dpi" type="hidden" value="600" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmF3cmhWVUU1ZkxoamhGOGFaaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201905025&amp;v=MTI1NzZxQnRHRnJDVVJMT2VaZVJuRnlya1U3dk1MejdCYmJHNEg5ak1xbzlIWVlRS0RIODR2UjRUNmo1NE8zenE=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmF3cmhWVUU1ZkxoamhGOGFaaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
