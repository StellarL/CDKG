<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637131276248087500%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJC201902027%26RESULT%3d1%26SIGN%3dlrNoeiXz9p02qEG66LzfKgmUKdA%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201902027&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201902027&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201902027&amp;v=MDE3NDk5ak1yWTlIWTRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJuRnlqa1VMM0tMejdCYmJHNEg=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#159" data-title="0概述 ">0概述</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#163" data-title="1 相关工作 ">1 相关工作</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#168" data-title="2 小样本多元类别数据的深度高斯过程模型 ">2 小样本多元类别数据的深度高斯过程模型</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#169" data-title="2.1 深度高斯过程模型">2.1 深度高斯过程模型</a></li>
                                                <li><a href="#180" data-title="2.2 CLDGP模型核函数处理">2.2 CLDGP模型核函数处理</a></li>
                                                <li><a href="#184" data-title="2.3 基于变分推断的深度高斯过程模型参数优化">2.3 基于变分推断的深度高斯过程模型参数优化</a></li>
                                                <li><a href="#232" data-title="2.4 分布估计整体步骤">2.4 分布估计整体步骤</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#238" data-title="3 实例结果与分析 ">3 实例结果与分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#244" data-title="3.1 小批量多元类别手写字母数据集">3.1 小批量多元类别手写字母数据集</a></li>
                                                <li><a href="#251" data-title="3.2 小批量多元类别医疗诊断数据集">3.2 小批量多元类别医疗诊断数据集</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#255" data-title="4 结束语 ">4 结束语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#167" data-title="图1 各模型与本文模型之间的关系结构">图1 各模型与本文模型之间的关系结构</a></li>
                                                <li><a href="#177" data-title="图2 CLDGP概率图模型">图2 CLDGP概率图模型</a></li>
                                                <li><a href="#231" data-title="图3 CLDGP模型变分推断流程">图3 CLDGP模型变分推断流程</a></li>
                                                <li><a href="#246" data-title="图4 二元手写数字字幕数据集示例">图4 二元手写数字字幕数据集示例</a></li>
                                                <li><a href="#249" data-title="图5 2种模型误差在测试集和检验集上的对比结果">图5 2种模型误差在测试集和检验集上的对比结果</a></li>
                                                <li><a href="#250" data-title="图6 二元隐空间分类绘图">图6 二元隐空间分类绘图</a></li>
                                                <li><a href="#254" data-title="表1 3种模型在3种数据下检验集数据误差结果">表1 3种模型在3种数据下检验集数据误差结果</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="81">


                                    <a id="bibliography_1" title="王盛玉, 曾碧卿, 胡翩翩.基于卷积神经网络参数优化的中文情感分析[J].计算机工程, 2017, 43 (8) :200-207, 214." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201708035&amp;v=MDA1NjM0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSbkZ5amtVTDNLTHo3QmJiRzRIOWJNcDQ5R1lZUUtESDg=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                        王盛玉, 曾碧卿, 胡翩翩.基于卷积神经网络参数优化的中文情感分析[J].计算机工程, 2017, 43 (8) :200-207, 214.
                                    </a>
                                </li>
                                <li id="83">


                                    <a id="bibliography_2" title="BENGIO Y, SCHWENK H, SENCAL J S, et al.Neural probabilistic language models[M].In Innovations in Machine Learning.Berlin, Germany:Springer, 2006:137-186." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Neural probabilistic language models">
                                        <b>[2]</b>
                                        BENGIO Y, SCHWENK H, SENCAL J S, et al.Neural probabilistic language models[M].In Innovations in Machine Learning.Berlin, Germany:Springer, 2006:137-186.
                                    </a>
                                </li>
                                <li id="85">


                                    <a id="bibliography_3" title="COLLOBERT R, WESTON J.A unified architecture for natural language processing:deep neural networks with multitask learning[C]//Proceedings of the 25th International Conference on Machine learning.New York, USA:ACMPress, 2008:160-167." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Unified Architecture for Natural Language Processing:Deep Neural Networks with Multitask Learning">
                                        <b>[3]</b>
                                        COLLOBERT R, WESTON J.A unified architecture for natural language processing:deep neural networks with multitask learning[C]//Proceedings of the 25th International Conference on Machine learning.New York, USA:ACMPress, 2008:160-167.
                                    </a>
                                </li>
                                <li id="87">


                                    <a id="bibliography_4" title="GAL Y, CHEN Y, ZOUBIN G.Latent Gaussian process for distribution estimation of multivariate categorical data[EB/OL].[2017-11-18].https://arxiv.org/pdf/1503.02182.pdf" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Latent Gaussian process for distribution estimation of multivariate categorical data">
                                        <b>[4]</b>
                                        GAL Y, CHEN Y, ZOUBIN G.Latent Gaussian process for distribution estimation of multivariate categorical data[EB/OL].[2017-11-18].https://arxiv.org/pdf/1503.02182.pdf
                                    </a>
                                </li>
                                <li id="89">


                                    <a id="bibliography_5" title="KHAN M E, MOHAMED S, MARLIN B R, et al.A stickbreaking likelihood for categorical data analysis with latent Gaussian models[EB/OL].[2017-11-18].https://www.shakirm.com/papers/catLGM-AIstats 20 12.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A stickbreaking likelihood for categorical data analysis with latent Gaussian models">
                                        <b>[5]</b>
                                        KHAN M E, MOHAMED S, MARLIN B R, et al.A stickbreaking likelihood for categorical data analysis with latent Gaussian models[EB/OL].[2017-11-18].https://www.shakirm.com/papers/catLGM-AIstats 20 12.pdf.
                                    </a>
                                </li>
                                <li id="91">


                                    <a id="bibliography_6" title="何志昆, 刘光斌, 赵曦晶, 等.高斯过程回归方法综述[J].控制与决策, 2013, 28 (8) :1121-1129, 1137." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=KZYC201308002&amp;v=MjI1MjM0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSbkZ5amtVTDNLTGpmU2JiRzRIOUxNcDQ5RlpvUUtESDg=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                        何志昆, 刘光斌, 赵曦晶, 等.高斯过程回归方法综述[J].控制与决策, 2013, 28 (8) :1121-1129, 1137.
                                    </a>
                                </li>
                                <li id="93">


                                    <a id="bibliography_7" title="RASMUSSEN C E, WILLIAM K I.Gaussian process for machine learning[EB/OL].[2017-11-18].http://www.gaussianprocess.org/gpml/." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Gaussian process for machine learning">
                                        <b>[7]</b>
                                        RASMUSSEN C E, WILLIAM K I.Gaussian process for machine learning[EB/OL].[2017-11-18].http://www.gaussianprocess.org/gpml/.
                                    </a>
                                </li>
                                <li id="95">


                                    <a id="bibliography_8" title="KO J, FOX D.GP-Bayes filters:Bayesian filtering using Gaussian process prediction and observation models[C]//Proceedings of IEEE/RSJ Intelligent Robots and Systems.Washington D.C., USA:IEEE Press, 2008:3471-3476." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=GP-Bayes filters:Bayesian filtering using Gaussian process prediction and observation models">
                                        <b>[8]</b>
                                        KO J, FOX D.GP-Bayes filters:Bayesian filtering using Gaussian process prediction and observation models[C]//Proceedings of IEEE/RSJ Intelligent Robots and Systems.Washington D.C., USA:IEEE Press, 2008:3471-3476.
                                    </a>
                                </li>
                                <li id="97">


                                    <a id="bibliography_9" title="DEISENROTH M P, RASMUSSEN C E.PILCO:a modelbased and data-efficient approach to policy search[C]//Proceedings of the 28th International Conference on Machine Learning.[S.l.]:Omnipress, 2011:465-472." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=PILCO:A Model-Based and Data-Efficient Approach to Policy Search">
                                        <b>[9]</b>
                                        DEISENROTH M P, RASMUSSEN C E.PILCO:a modelbased and data-efficient approach to policy search[C]//Proceedings of the 28th International Conference on Machine Learning.[S.l.]:Omnipress, 2011:465-472.
                                    </a>
                                </li>
                                <li id="99">


                                    <a id="bibliography_10" title="CRESSIE N, WIKLE K.Statistics for spatio-temporal data[EB/OL].[2017-11-18].https://www.wiley.com/en-us/Statistics+for+Spatio+Temporal+Datap-9780471692744." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Statistics for spatio-temporal data">
                                        <b>[10]</b>
                                        CRESSIE N, WIKLE K.Statistics for spatio-temporal data[EB/OL].[2017-11-18].https://www.wiley.com/en-us/Statistics+for+Spatio+Temporal+Datap-9780471692744.
                                    </a>
                                </li>
                                <li id="101">


                                    <a id="bibliography_11" title="王鑫, 李红丽.台风最大风速预测的高斯过程回归模型[J].计算机应用研究, 2015, 32 (1) :59-62." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSYJ201501015&amp;v=MjMyMTBSbkZ5amtVTDNLTHo3U1pMRzRIOVRNcm85RVlZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmU=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[11]</b>
                                        王鑫, 李红丽.台风最大风速预测的高斯过程回归模型[J].计算机应用研究, 2015, 32 (1) :59-62.
                                    </a>
                                </li>
                                <li id="103">


                                    <a id="bibliography_12" title="BRIOL F X, OATES C J, GIROLAMI M, et al.Probabilistic integration:a role for statisticians in numerical analysis?[EB/OL].[2017-11-18].https://arxiv.org/pdf/1512.00933v5.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Probabilistic integration:a role for statisticians in numerical analysis?">
                                        <b>[12]</b>
                                        BRIOL F X, OATES C J, GIROLAMI M, et al.Probabilistic integration:a role for statisticians in numerical analysis?[EB/OL].[2017-11-18].https://arxiv.org/pdf/1512.00933v5.pdf.
                                    </a>
                                </li>
                                <li id="105">


                                    <a id="bibliography_13" title="GUESTRIN C, KRAUSE A, SINGH A P.Near-optimal sensor placements in Gaussian processes[C]//Proceedings of the 22nd International Conference on Machine Learning.New York, USA:ACM Press, 2005:265-272." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Near-optimal sensor placements in Gaussian processes">
                                        <b>[13]</b>
                                        GUESTRIN C, KRAUSE A, SINGH A P.Near-optimal sensor placements in Gaussian processes[C]//Proceedings of the 22nd International Conference on Machine Learning.New York, USA:ACM Press, 2005:265-272.
                                    </a>
                                </li>
                                <li id="107">


                                    <a id="bibliography_14" title="孙晓燕, 陈姗姗, 巩敦卫, 等.基于区间适应值交互式遗传算法的加权多输出高斯过程代理模型[J].自动化学报, 2014, 40 (2) :172-184." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201402002&amp;v=Mjk1NjJDVVJMT2VaZVJuRnlqa1VMM0tLQ0xmWWJHNEg5WE1yWTlGWm9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnI=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[14]</b>
                                        孙晓燕, 陈姗姗, 巩敦卫, 等.基于区间适应值交互式遗传算法的加权多输出高斯过程代理模型[J].自动化学报, 2014, 40 (2) :172-184.
                                    </a>
                                </li>
                                <li id="109">


                                    <a id="bibliography_15" title="SNOEK J, LAROCHELLE H, ADAMS R P.Practical bayesian optimization of machine learning algorithms[C]//Proceedings of the 25th International Conference on Neural Information Processing Systems[S.l.]:Curran Associates Inc., 2012:2951-2959." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Practical Bayesian optimization of machine learning algorithms">
                                        <b>[15]</b>
                                        SNOEK J, LAROCHELLE H, ADAMS R P.Practical bayesian optimization of machine learning algorithms[C]//Proceedings of the 25th International Conference on Neural Information Processing Systems[S.l.]:Curran Associates Inc., 2012:2951-2959.
                                    </a>
                                </li>
                                <li id="111">


                                    <a id="bibliography_16" title="REZENDE D J, MOHAMED S, WIERSTRA D.Stochastic back propagation and approximate inference in deep generative models[J].Pattern Recognition and Machine Learning, 2014, 32 (2) :1278-1286." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Stochastic back propagation and approximate inference in deep generative models">
                                        <b>[16]</b>
                                        REZENDE D J, MOHAMED S, WIERSTRA D.Stochastic back propagation and approximate inference in deep generative models[J].Pattern Recognition and Machine Learning, 2014, 32 (2) :1278-1286.
                                    </a>
                                </li>
                                <li id="113">


                                    <a id="bibliography_17" title="GHAHRAMANI Z.Probabilistic machine learning and artificial intelligence[J].Nature, 2015, 521:452-459." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Probabilistic machine learning and artificial intelligence">
                                        <b>[17]</b>
                                        GHAHRAMANI Z.Probabilistic machine learning and artificial intelligence[J].Nature, 2015, 521:452-459.
                                    </a>
                                </li>
                                <li id="115">


                                    <a id="bibliography_18" title="DAMIANOU A C, LAWRENCE N D.Deep Gaussian processes[EB/OL].[2017-11-18].https://core.ac.uk/download/pdf/46564399.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep Gaussian processes">
                                        <b>[18]</b>
                                        DAMIANOU A C, LAWRENCE N D.Deep Gaussian processes[EB/OL].[2017-11-18].https://core.ac.uk/download/pdf/46564399.pdf.
                                    </a>
                                </li>
                                <li id="117">


                                    <a id="bibliography_19" title="WILSON A G, HU Z, SALAKHUTDINOV R, et al.Deep kernel learning[EB/OL].[2017-11-18].https://arxiv.org/pdf/1511.02222.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep kernel learning">
                                        <b>[19]</b>
                                        WILSON A G, HU Z, SALAKHUTDINOV R, et al.Deep kernel learning[EB/OL].[2017-11-18].https://arxiv.org/pdf/1511.02222.pdf.
                                    </a>
                                </li>
                                <li id="119">


                                    <a id="bibliography_20" title="DURRANDE N, GINSBOURGER D, ROUSTANT O.Additive kernels for Gaussian process modeling[EB/OL].[2017-11-18].https://arxiv.org/pdf/1103.4023.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Additive kernels for Gaussian process modeling">
                                        <b>[20]</b>
                                        DURRANDE N, GINSBOURGER D, ROUSTANT O.Additive kernels for Gaussian process modeling[EB/OL].[2017-11-18].https://arxiv.org/pdf/1103.4023.pdf.
                                    </a>
                                </li>
                                <li id="121">


                                    <a id="bibliography_21" title="DAVID D, JAMES R L, ROGER G, et al.Structure discovery in nonparametric regression through compositional kernel search[EB/OL].[2017-11-18].http://www.cs.toronto.edu/~rgrosse/icml2013-gp.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Structure discovery in nonparametric regression through compositional kernel search">
                                        <b>[21]</b>
                                        DAVID D, JAMES R L, ROGER G, et al.Structure discovery in nonparametric regression through compositional kernel search[EB/OL].[2017-11-18].http://www.cs.toronto.edu/~rgrosse/icml2013-gp.pdf.
                                    </a>
                                </li>
                                <li id="123">


                                    <a id="bibliography_22" title="HENSMAN J, LAWRENCE N D.Nested variational compression in deep Gaussian processes[EB/OL].[2017-11-18].https://arxiv.org/pdf/1412.1370.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Nested variational compression in deep Gaussian processes">
                                        <b>[22]</b>
                                        HENSMAN J, LAWRENCE N D.Nested variational compression in deep Gaussian processes[EB/OL].[2017-11-18].https://arxiv.org/pdf/1412.1370.pdf.
                                    </a>
                                </li>
                                <li id="125">


                                    <a id="bibliography_23" title="VAFA K.Training deep Gaussian processes with sampling[EB/OL].[2017-11-18].http://approxi mateinference.org/accepted/Vafa2016.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Training deep Gaussian processes with sampling">
                                        <b>[23]</b>
                                        VAFA K.Training deep Gaussian processes with sampling[EB/OL].[2017-11-18].http://approxi mateinference.org/accepted/Vafa2016.pdf.
                                    </a>
                                </li>
                                <li id="127">


                                    <a id="bibliography_24" title="WANG Y, BRUBAKER M, CHAIB-DRAA B, et al.Sequential inference for deep Gaussian process[EB/OL].[2017-11-18].http://proceedings.mlr.press/v51/wang16c.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Sequential inference for deep Gaussian process">
                                        <b>[24]</b>
                                        WANG Y, BRUBAKER M, CHAIB-DRAA B, et al.Sequential inference for deep Gaussian process[EB/OL].[2017-11-18].http://proceedings.mlr.press/v51/wang16c.pdf.
                                    </a>
                                </li>
                                <li id="129">


                                    <a id="bibliography_25" title="BUI T D, HERNNDEZ-LOBATO D, LI Y, et al.Deep Gaussian processes for regression using approximate expectation propagation[EB/OL].[2017-11-18].http://proceedings.mlr.press/v48/bui16.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep Gaussian processes for regression using approximate expectation propagation">
                                        <b>[25]</b>
                                        BUI T D, HERNNDEZ-LOBATO D, LI Y, et al.Deep Gaussian processes for regression using approximate expectation propagation[EB/OL].[2017-11-18].http://proceedings.mlr.press/v48/bui16.pdf.
                                    </a>
                                </li>
                                <li id="131">


                                    <a id="bibliography_26" title="DAI Z, DAMIANOU A, GONZLEZ J, et al.Variational autoencoded deep Gaussian processes[EB/OL].[2017-11-18].https://arxiv.org/pdf/1511.06455.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Variational autoencoded deep Gaussian processes">
                                        <b>[26]</b>
                                        DAI Z, DAMIANOU A, GONZLEZ J, et al.Variational autoencoded deep Gaussian processes[EB/OL].[2017-11-18].https://arxiv.org/pdf/1511.06455.pdf.
                                    </a>
                                </li>
                                <li id="133">


                                    <a id="bibliography_27" title="DAMIANOU A D, TITSIAS M K, LAWRENCE N D.Variational Gaussian process dynamical systems[EB/OL].[2017-11-18].http://papers.nips.cc/paper/4330-variationalgaussianprocess-dynamical-systems.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Variational Gaussian process dynamical systems">
                                        <b>[27]</b>
                                        DAMIANOU A D, TITSIAS M K, LAWRENCE N D.Variational Gaussian process dynamical systems[EB/OL].[2017-11-18].http://papers.nips.cc/paper/4330-variationalgaussianprocess-dynamical-systems.pdf.
                                    </a>
                                </li>
                                <li id="135">


                                    <a id="bibliography_28" title="CUTAJAR K, BONILLA E V, MICHIARDI P, et al.Practical learning of deep Gaussian processes via random fourier features[EB/OL].[2017-11-18].https://pdfs.semanticscholar.org/bafa/7e2d586e7bfe77d9a55ac1cff4eb2f6ff292.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Practical learning of deep Gaussian processes via random fourier features">
                                        <b>[28]</b>
                                        CUTAJAR K, BONILLA E V, MICHIARDI P, et al.Practical learning of deep Gaussian processes via random fourier features[EB/OL].[2017-11-18].https://pdfs.semanticscholar.org/bafa/7e2d586e7bfe77d9a55ac1cff4eb2f6ff292.pdf.
                                    </a>
                                </li>
                                <li id="137">


                                    <a id="bibliography_29" title="HUGH S, MARC D.Doubly stochastic variational inference for deep Gaussian processes[EB/OL].[2017-11-18].https://arxiv.org/pdf/1705.08933.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Doubly stochastic variational inference for deep Gaussian processes">
                                        <b>[29]</b>
                                        HUGH S, MARC D.Doubly stochastic variational inference for deep Gaussian processes[EB/OL].[2017-11-18].https://arxiv.org/pdf/1705.08933.pdf.
                                    </a>
                                </li>
                                <li id="139">


                                    <a id="bibliography_30" title="TITSIAS M K.Variational learning of inducing variables in sparse Gaussian processes[EB/OL].[2017-11-18].http://proceedings.mlr.press/v5/titsias09a/titsias09a.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Variational learning of inducing variables in sparse Gaussian processes">
                                        <b>[30]</b>
                                        TITSIAS M K.Variational learning of inducing variables in sparse Gaussian processes[EB/OL].[2017-11-18].http://proceedings.mlr.press/v5/titsias09a/titsias09a.pdf.
                                    </a>
                                </li>
                                <li id="141">


                                    <a id="bibliography_31" title="BLEI D M, KUCUKELBIR A, MCAULIFFE J D.Variational inference:a review for statisticians[EB/OL].[2017-11-18].https://arxiv.org/pdf/1601.00670.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Variational inference:a review for statisticians">
                                        <b>[31]</b>
                                        BLEI D M, KUCUKELBIR A, MCAULIFFE J D.Variational inference:a review for statisticians[EB/OL].[2017-11-18].https://arxiv.org/pdf/1601.00670.pdf.
                                    </a>
                                </li>
                                <li id="143">


                                    <a id="bibliography_32" title="JOHN D, ELAD H, YORAM S.Adaptive subgradient methods for online learning and stochastic optimization[J].Journal of Machine Learning Research, 2011, 12:2121-2159." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Adaptive subgradient methods for online learning and stochastic optimization">
                                        <b>[32]</b>
                                        JOHN D, ELAD H, YORAM S.Adaptive subgradient methods for online learning and stochastic optimization[J].Journal of Machine Learning Research, 2011, 12:2121-2159.
                                    </a>
                                </li>
                                <li id="145">


                                    <a id="bibliography_33" title="TIELEMAN T, HINTON G.Lecture 6.5-rmsprop, COURSERA:neural networks for machine learning[EB/OL].[2017-11-18].http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Lecture 6.5-rmsprop,COURSERA:neural networks for machine learning">
                                        <b>[33]</b>
                                        TIELEMAN T, HINTON G.Lecture 6.5-rmsprop, COURSERA:neural networks for machine learning[EB/OL].[2017-11-18].http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf.
                                    </a>
                                </li>
                                <li id="147">


                                    <a id="bibliography_34" title="TOM S, IOANNIS A, DAVID S.Unit tests for stochastic optimization[EB/OL].[2017-11-18].https://arxiv.org/pdf/1312.6055.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Unit tests for stochastic optimization">
                                        <b>[34]</b>
                                        TOM S, IOANNIS A, DAVID S.Unit tests for stochastic optimization[EB/OL].[2017-11-18].https://arxiv.org/pdf/1312.6055.pdf.
                                    </a>
                                </li>
                                <li id="149">


                                    <a id="bibliography_35" title="Perplexity[G/OL].[2017-11-18].https://en.wiki pedia.org/wiki/Perplexity." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Perplexity[G/OL]">
                                        <b>[35]</b>
                                        Perplexity[G/OL].[2017-11-18].https://en.wiki pedia.org/wiki/Perplexity.
                                    </a>
                                </li>
                                <li id="151">


                                    <a id="bibliography_36" title="Tensorflow[EB/OL].[2017-11-18].https://tenso rflow.google.cn/." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Tensorflow">
                                        <b>[36]</b>
                                        Tensorflow[EB/OL].[2017-11-18].https://tenso rflow.google.cn/.
                                    </a>
                                </li>
                                <li id="153">


                                    <a id="bibliography_37" title="MATTHEWS A G D G, MARK V D W, NICKSON T, et al.GPflow:a gaussian process library using tensorflow[EB/OL].[2017-11-18].http://adsabs.harvard.edu/abs/2016arXiv161008733M." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=GPflow:a gaussian process library using tensorflow">
                                        <b>[37]</b>
                                        MATTHEWS A G D G, MARK V D W, NICKSON T, et al.GPflow:a gaussian process library using tensorflow[EB/OL].[2017-11-18].http://adsabs.harvard.edu/abs/2016arXiv161008733M.
                                    </a>
                                </li>
                                <li id="155">


                                    <a id="bibliography_38" title="Handwritten digits[DB/OL].[2017-11-18].https://cs.nyu.edu/~roweis/data.html." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Handwritten digits">
                                        <b>[38]</b>
                                        Handwritten digits[DB/OL].[2017-11-18].https://cs.nyu.edu/~roweis/data.html.
                                    </a>
                                </li>
                                <li id="157">


                                    <a id="bibliography_39" title="MATJAZ Z, MILAN S.Breast cancer data set[DB/OL].[2017-11-18].http://archive.ics.uci.edu/ml/datasets/Breast+Cancer." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Breast cancer data set">
                                        <b>[39]</b>
                                        MATJAZ Z, MILAN S.Breast cancer data set[DB/OL].[2017-11-18].http://archive.ics.uci.edu/ml/datasets/Breast+Cancer.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJC" target="_blank">计算机工程</a>
                2019,45(02),160-166 DOI:10.19678/j.issn.1000-3428.0049671            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm">基于深度高斯过程的多元类别数据分布估计</span>
 <span class="shoufa"></span>                                     </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%88%98%E5%A7%9D%E5%90%9B&amp;code=38617598&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">刘姝君</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%8E%E8%89%B3%E5%A9%B7&amp;code=24984246&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">李艳婷</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%8A%E6%B5%B7%E4%BA%A4%E9%80%9A%E5%A4%A7%E5%AD%A6%E6%9C%BA%E6%A2%B0%E4%B8%8E%E5%8A%A8%E5%8A%9B%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0054402&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">上海交通大学机械与动力工程学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>多元类别数据的可能取值会随向量长度的增长呈指数级增长, 从而造成数据稀疏性问题。通过将观察数据嵌入到连续空间中训练识别数据之间的相似性, 构建多元类别数据的线性高斯隐变量模型和类别隐高斯过程 (CLGP) 。在CLGP模型基础上, 建立小样本多元类别数据分布估计的多元类别深度隐高斯过程模型, 并结合蒙特卡洛采样的变分推断方法对模型进行参数优化。实验结果表明, 与CLGP模型相比, 该模型分布估计精确度有所提升。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%A4%9A%E5%85%83%E7%B1%BB%E5%88%AB%E6%95%B0%E6%8D%AE&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">多元类别数据;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%94%9F%E6%88%90%E5%BC%8F%E6%A8%A1%E5%9E%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">生成式模型;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E9%AB%98%E6%96%AF%E8%BF%87%E7%A8%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度高斯过程;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">无监督学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8F%98%E5%88%86%E6%8E%A8%E6%96%AD&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">变分推断;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    刘姝君 (1994—) , 女, 硕士, 主研方向为智能故障诊断、贝叶斯机器学习;E-mail: liushujun_uestc@ 163. com
;
                                </span>
                                <span>
                                    李艳婷, 副教授、博士。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2017-12-12</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金面上项目“多元复杂时空数据建模与监控方法研究” (71672109);</span>
                    </p>
            </div>
                    <h1>Multivariate Categorical Data Distribution Estimation Based on Deep Gaussian Process</h1>
                    <h2>
                    <span>LIU Shujun</span>
                    <span>LI Yanting</span>
            </h2>
                    <h2>
                    <span>School of Mechanical Engineering, Shanghai Jiaotong University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>The possible value of multivariate categorical data increases exponentially with the length of the vector, resulting in data sparsity. The similarity between the identified data is trained by embedding the observation data into the continuous space, and the linear Gaussian hidden variable model and the Categorical Latent Gaussian Process (CLGP) of the multi-category data are constructed. Based on the CLGP model, a multi-class deep hidden Gaussian process model for small sample multi-class data distribution estimation is proposed, and the parameters are optimized by Monte Carlo sampling. Experimental results show that compared with the CLGP model, this model distribution estimation accuracy has improved.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=multivariate%20categorical%20data&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">multivariate categorical data;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=generative%20model&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">generative model;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Deep%20Gaussian%20Process%20(DGP)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Deep Gaussian Process (DGP) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=unsupervised%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">unsupervised learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=variational%20inference&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">variational inference;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2017-12-12</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="159" name="159" class="anchor-tag">0概述</h3>
                <div class="p1">
                    <p id="160">高维多元类别数据的学习与推断是机器学习领域中重要的问题之一, 其广泛应用于金融行业的离散选择模型、社会关系调查响应分析、推荐系统、自然语言处理<citation id="269" type="reference"><link href="81" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>以及医疗诊断等子领域。</p>
                </div>
                <div class="p1">
                    <p id="161">近年来, 基于类别变量向量相似度原理在大规模有标签数据的研究中取得一定的进展<citation id="271" type="reference"><link href="83" rel="bibliography" /><link href="85" rel="bibliography" /><sup>[<a class="sup">2</a>,<a class="sup">3</a>]</sup></citation>。基于大规模有标签数据集利用贝叶斯非参数模型作为非线性转换函数, 进行小规模无标签数据集的分布估计。现有的离散有标签数据的监督式模型将观察数据嵌入到一个连续的空间中, 从而获得类别变量之间的相似性。为了对稀疏多模态类别数据的分布建模, 首先利用一个简单的隐空间建立连续性分布, 然后对空间中的点做非线性转换得到概率分布。文献<citation id="270" type="reference">[<a class="sup">4</a>]</citation>对隐空间的先验分布设置为标准正态分布, 并且将稀疏高斯过程非线性转换的输出输送到Softmax函数中得到输出概率。</p>
                </div>
                <div class="p1">
                    <p id="162">在文献<citation id="272" type="reference">[<a class="sup">4</a>]</citation>基础上, 本文将高斯过程非线性转换拓展到深度高斯过程 (Deep Gaussian Precess, DGP) 非线性转换, 从而得到针对小批量无标签数据分布估计的生成式深度高斯过程模型。</p>
                </div>
                <h3 id="163" name="163" class="anchor-tag">1 相关工作</h3>
                <div class="p1">
                    <p id="164">对于多元类别数据分布估计, 文献<citation id="273" type="reference">[<a class="sup">5</a>]</citation>提出利用线性高斯模型 (Linear Gaussian Model, L GM) 在隐空间中建立标准正态分布先验分布, 然后进行线性转换, 并输出到Softmax似然函数中。针对线性模型表达能力的不足, 文献<citation id="274" type="reference">[<a class="sup">4</a>]</citation>提出一种类别隐高斯过程 (Categorical Latent Gaussian Process, CL GP) 模型。高斯过程模型<citation id="281" type="reference"><link href="91" rel="bibliography" /><link href="93" rel="bibliography" /><sup>[<a class="sup">6</a>,<a class="sup">7</a>]</sup></citation>是一种灵活的非参数贝叶斯模型, 应用于机器人<citation id="282" type="reference"><link href="95" rel="bibliography" /><link href="97" rel="bibliography" /><sup>[<a class="sup">8</a>,<a class="sup">9</a>]</sup></citation>、地理统计<citation id="283" type="reference"><link href="99" rel="bibliography" /><link href="101" rel="bibliography" /><sup>[<a class="sup">10</a>,<a class="sup">11</a>]</sup></citation>、数值运算<citation id="276" type="reference"><link href="103" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>、激励感知<citation id="275" type="reference"><link href="105" rel="bibliography" /><link href="107" rel="bibliography" /><sup>[<a class="sup">13</a>,<a class="sup">14</a>]</sup></citation>以及参数优化问题<citation id="277" type="reference"><link href="109" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>等领域。由于高斯过程的非线性, CLGP模型较线性版本的LGM更加灵活, 能够较好地学习多模态类别数据的特征。然而相对于深度高斯过程模型, 单层高斯过程模型仍然存在学习能力较弱的缺陷。对于大规模类别数据的表示学习, 深度神经网络如自编码<citation id="278" type="reference"><link href="111" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>或长短时记忆取得较好的效果。但这些模型在小批量类别性数据集上表现并不好<citation id="279" type="reference"><link href="113" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>, 因为深度神经网络包含大量参数, 需要大量数据做参数优化。而高斯过程模型由于其完全贝叶斯的处理方式, 因此不易过拟合, 能够有效处理小批量类别数据集, 同时能够提供预测结果的不确定度。本文所提出的类别隐深度高斯过程 (Categorical Latent Deep Gaussian Process, CL DGP) 模型可以看成深度高斯过程<citation id="280" type="reference"><link href="115" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>的无监督学习模型版本, 也可以看成类似于深度高斯过程模型对于单层高斯过程模型拓展的对于类别隐高斯过程模型的延伸。</p>
                </div>
                <div class="p1">
                    <p id="165">在组合核函数、深度核函数高斯过程模型<citation id="288" type="reference"><link href="117" rel="bibliography" /><link href="119" rel="bibliography" /><link href="121" rel="bibliography" /><sup>[<a class="sup">19</a>,<a class="sup">20</a>,<a class="sup">21</a>]</sup></citation>及深度高斯过程模型的组合型架构中, 基于过程组合的深度高斯过程模型<citation id="284" type="reference"><link href="123" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>通常采用近似推断的方法, 如变分推断<citation id="289" type="reference"><link href="129" rel="bibliography" /><link href="131" rel="bibliography" /><sup>[<a class="sup">25</a>,<a class="sup">26</a>]</sup></citation>、期望传播来进行参数优化。由于基础均值场变分推断法<citation id="290" type="reference"><link href="133" rel="bibliography" /><link href="135" rel="bibliography" /><sup>[<a class="sup">27</a>,<a class="sup">28</a>]</sup></citation>中变量独立性假设过强、变分参数线性增长的缺陷, 利用变分自编码的方法对隐层变量后验分布用认知模型构建<citation id="285" type="reference"><link href="131" rel="bibliography" /><sup>[<a class="sup">26</a>]</sup></citation>, 采用核函数稀疏谱的方法做稀疏化处理<citation id="286" type="reference"><link href="135" rel="bibliography" /><sup>[<a class="sup">28</a>]</sup></citation>等方法相继提出。文献<citation id="287" type="reference">[<a class="sup">29</a>]</citation>提出新的基于伪点利用准确模型作为变分后验分布的推断方法, 并且结合蒙特卡洛随机采样与随机梯度下降法将双重随机变分推断应用到深度高斯过程模型中。本文将利用蒙特卡洛采样的变分推断法应用到无监督的类别隐深度高斯过程模型中, 同时采用单层高斯过程模型中对核函数噪声参数的融合处理的方法减少变分参数数量。</p>
                </div>
                <div class="p1">
                    <p id="166">关于各种基本模型与本文模型之间的关系结构如图1所示, 其中, 箭头旁标注表示4个不同方向的模型变化, 横向表示线性模型至非线性模型的变化, 同理纵向1层至2层, 2层至3层以及前向的含义。</p>
                </div>
                <div class="area_img" id="167">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201902027_16700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 各模型与本文模型之间的关系结构" src="Detail/GetImg?filename=images/JSJC201902027_16700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 各模型与本文模型之间的关系结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201902027_16700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h3 id="168" name="168" class="anchor-tag">2 小样本多元类别数据的深度高斯过程模型</h3>
                <h4 class="anchor-tag" id="169" name="169">2.1 深度高斯过程模型</h4>
                <div class="p1">
                    <p id="170">问题目标为在给定多元类别数据集Y的情况下对数据集进行分布估计, 即给出数据集中不同样本数据的分布概率p (y) 进行数据集分布估计。主要思路通过建立隐输入及深度高斯过程变换中间层及Softmax输出层, 并对模型进行变分推断参数优化, 得到隐变量分布及中间变换层参数, 从而得到数据分布生成过程。</p>
                </div>
                <div class="p1">
                    <p id="171">假设存在数据集Y包含N个观察数据以及D个类别变量, 即第n个观察数据y<sub>n</sub>= (y<sub>n1</sub>, y<sub>n2</sub>, …, y<sub>nD</sub>) , n∈{1, 2, …, N}。第n个观察数据中的第d个类别变量y<sub>nd</sub>是一个取值范围为0到K<sub>d</sub>的类别变量。为了表示方便, 现假设所有的类别变量都有相同的K<sub>d</sub>, 即K<sub>d</sub>≡K, <image id="257" type="formula" href="images/JSJC201902027_25700.jpg" display="inline" placement="inline"><alt></alt></image></p>
                </div>
                <div class="p1">
                    <p id="258">每一个类别变量y<sub>nd</sub>的类别分布概率由一个Softmax函数给出, Softmax函数的输入为权重f<sup>L</sup><sub>nd</sub>= (0, f<sup>L</sup><sub>nd1</sub>, …, f<sup>L</sup><sub>ndK</sub>) , 每一个权重f<sup>L</sup><sub>ndk</sub> (k∈{1, 2, …, K}) 为一个L层深度高斯过程的第L层输出。深度高斯过程模型的输入是一个Q维且服从标准正态分布的隐变量<image id="259" type="formula" href="images/JSJC201902027_25900.jpg" display="inline" placement="inline"><alt></alt></image>, 为表示方便, 本文将输入x用深度高斯过程模型第0层F<sup>0</sup>来表示, <image id="260" type="formula" href="images/JSJC201902027_26000.jpg" display="inline" placement="inline"><alt></alt></image>, 具体模型结构如式 (1) ～式 (6) 所示。</p>
                </div>
                <div class="area_img" id="174">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJC201902027_17400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="area_img" id="174">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJC201902027_17401.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="176">在上述结构中, F<sup>l</sup>为第l层GP非线性转换函数, μ<sup>l</sup>、K (·) <sup>l</sup>为第l层GP的均值与核函数。同时, 为减小高斯过程模型的计算成本, 本文采用文献<citation id="291" type="reference">[<a class="sup">30</a>]</citation>提出的稀疏伪点法, Z<sup>l-1</sup>为第l层的伪点输入, U<sup>l</sup>为第l层的伪点输出。对于第l层, 由于输出为d维, 因此有d个GP模型。模型的概率图模型结构如图2所示。</p>
                </div>
                <div class="area_img" id="177">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201902027_17700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 CLDGP概率图模型" src="Detail/GetImg?filename=images/JSJC201902027_17700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 CLDGP概率图模型  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201902027_17700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="178">由于输出数据Y的分布取决于第L层隐函数变量F<sup>L</sup>。同时根据单层高斯过程隐变量模型的推断方法, 单层隐变量F<sup>L</sup>的分布完全取决于该层伪点U<sup>l</sup>的分布。根据F<sup>L</sup>基于U<sup>l</sup>的条件分布以及U<sup>l</sup>的分布可以得到每一层F<sup>L</sup>与U<sup>l</sup>的联合分布, 从而可以得到DGP模型的联合分布为:</p>
                </div>
                <div class="area_img" id="179">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJC201902027_17900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <h4 class="anchor-tag" id="180" name="180">2.2 CLDGP模型核函数处理</h4>
                <div class="p1">
                    <p id="181">在DGP模型中, 通常对每一层GP输出专门做参数化处理, 本文不对这些变量做参数化处理。根据文献<citation id="292" type="reference">[<a class="sup">29</a>]</citation>方法, 对于每一层的核函数K (·) <sup>l</sup>, 将噪声吸收进核函数中得到噪声化的核函数为:</p>
                </div>
                <div class="area_img" id="182">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJC201902027_18200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="183">其中, δ<sub>ij</sub>为克罗内克δ函数, σ<sub>l</sub><sup>2</sup>是层与层之间的噪声方差。当噪声项被参数化时, 变分变量上的变分分布为因子化的高斯分布, 需要2N (D<sup>1</sup>+D<sup>2</sup>+…+D<sup>L-1</sup>) 个变分参数 (D<sup>l</sup>表示第l层的输出维度) , 在特定形式的核函数下关于对数边际似然可以得到可处理的变分下界。单独参数化噪声项的变分模型的另一个问题是输出的密度是带有相互独立高斯分布输入的单层高斯过程, 会使得变分后验分布丢失所有层与层之间的相关性, 因此不能表达整个模型的复杂性, 可能会低估方差。本文将噪声项吸收到核函数中, 采用保留真实模型的完整条件性结构方式, 尽管关于边际似然损失了处理性功能, 但是可以通过采样方式进行变分下界的估计。</p>
                </div>
                <h4 class="anchor-tag" id="184" name="184">2.3 基于变分推断的深度高斯过程模型参数优化</h4>
                <div class="p1">
                    <p id="185">变分推断是一种通过优化来近似概率密度的机器学习方法。变分推断的基本思想是假设一个简化的分布族, 然后找到分布族中与目标分布最为接近的分布。近似分布与目标分布的接近度用KullbackLeibler距离来度量, 即通过优化的方法近似给定观察变量条件下的隐变量的条件密度, 通过自由变分参数来参数化隐变量的密度族, 转变而成的优化问题将会寻找到分布族的具体变分参数, 使得条件分布与目标分布的KL距离最小, 所拟合的变分分布再被用作准确条件分布的近似来进行模型的预测。变分推断的综述性介绍见文献<citation id="293" type="reference">[<a class="sup">31</a>]</citation>。</p>
                </div>
                <div class="p1">
                    <p id="186">DGP推断的难点在于层内以及层级之间存在着复杂的相关性。根据文献<citation id="294" type="reference">[<a class="sup">29</a>]</citation>, 本文利用稀疏变分推断的方法来简化层级内的相关性, 但保留层级变量之间的相关性, 所得到的变分下界并不能得到可处理结果, 通过利用一元高斯分布得到变分下界的无偏采样样本。本文采用一个带有3个性质的后验分布, 具体分析如下:</p>
                </div>
                <div class="p1">
                    <p id="187">1) 每一层隐函数变量F<sup>l</sup>后验分布基于U<sup>l</sup>保留了准确的模型形式, 即没有做任何对先验分布的近似假设处理。</p>
                </div>
                <div class="p1">
                    <p id="188">2) 假设U= (U<sup>1</sup>, U<sup>l</sup>, …, U<sup>L</sup>) 的分布在层级以及每一维度之间因子化, 所得到的变分后验分布为:</p>
                </div>
                <div class="area_img" id="189">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJC201902027_18900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="190">3) q (U<sup>l</sup>) 设置为均值为m<sup>l</sup>、方差为S<sup>l</sup>的高斯分布。与单层稀疏高斯过程回归模型类似, 能够在每一层中边际化伪点。边际化以后, 得到针对每一层隐函数的变分后验分布为:</p>
                </div>
                <div class="area_img" id="191">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJC201902027_19100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="192">以单层GP为例, (F<sup>l</sup>, U<sup>l</sup>) 的联合变分分布为:</p>
                </div>
                <div class="area_img" id="193">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJC201902027_19300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="194">根据关于伪点的变分分布假设, q (U<sup>l</sup>) =N (U<sup>l</sup>|m<sup>l</sup>, S<sup>l</sup>) 可以得到隐函数变分后验分布为:</p>
                </div>
                <div class="area_img" id="195">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJC201902027_19500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="196">式 (12) 可以通过条件高斯分布等式求出结果。</p>
                </div>
                <div class="p1">
                    <p id="261">其中, 式 (12) 中的<image id="262" type="formula" href="images/JSJC201902027_26200.jpg" display="inline" placement="inline"><alt></alt></image>分别如式 (13) 、式 (14) 所示。</p>
                </div>
                <div class="area_img" id="197">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJC201902027_19700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="198">式 (13) 、式 (14) 中α (f<sub>i</sub><sup>l</sup>) 展开的表达式为:</p>
                </div>
                <div class="area_img" id="199">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJC201902027_19900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="200">根据式 (15) 的结果, 对于每一层GP中的第i个变量的边际分布, 有:</p>
                </div>
                <div class="area_img" id="201">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJC201902027_20100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="202">含有d元变量的最后一层即L层伪点变分分布假设为:</p>
                </div>
                <div class="area_img" id="203">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJC201902027_20300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="204">同理与前L-1层的分布推导, 可得L层隐函数变分分布为:</p>
                </div>
                <div class="area_img" id="205">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJC201902027_20500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="206">其中:</p>
                </div>
                <div class="area_img" id="207">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJC201902027_20700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="208">Softmax输出可表示为:</p>
                </div>
                <div class="area_img" id="209">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJC201902027_20900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="210">根据式 (7) 定义的联合分布函数, 对输入隐变量、各层函数隐变量以及各层伪点变量积分取对数可以得到数据的对数似然函数ln p (Y) , 再依据变分推断的基本思路, 结合式 (9) ～式 (23) 定义的变分分布, 利用Jensen不等式可以得到对数似然函数的变分下界L<sub>CLDGP</sub>为:</p>
                </div>
                <div class="area_img" id="263">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJC201902027_26300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="214">将变量变分分布代入式 (24) , 同时对表达式进行拆分可以得到3项表达式:第1项是关于条件似然函数的积分, 第2项和第3项为关于输入隐变量与各层伪点变量的变分分布与真实分布的KL距离项。具体可表示为:</p>
                </div>
                <div class="area_img" id="215">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJC201902027_21500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="216">对于输入隐变量的变分分布同样假设为相互独立的正态分布, 有:</p>
                </div>
                <div class="area_img" id="217">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJC201902027_21700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="219">在得到变分参数的变分下界后, 通常采用梯度下降法优化变分参数。为了以低方差蒙特卡洛法估计变分下界的梯度, 本文采用再参数化技巧。再参数化使得随机性并不取决于求取梯度的参数, 而是通过再参数化的形式引入另一个随机源。</p>
                </div>
                <div class="p1">
                    <p id="220">对于输入隐变量, 再参数化形式可表示为:</p>
                </div>
                <div class="area_img" id="221">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJC201902027_22100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="222">变量x<sub>iq</sub>的随机性部分全部转移到服从标准正态分布的参数<image id="264" type="formula" href="images/JSJC201902027_26400.jpg" display="inline" placement="inline"><alt></alt></image>上。在变分后验分布的求取过程中, 可以直接通过<image id="265" type="formula" href="images/JSJC201902027_26500.jpg" display="inline" placement="inline"><alt></alt></image>采样得到x<sub>iq</sub>的采样结果。同理, 对于隐函数变量, 由于第l层第i个变量的变分后验分布仅取决于第前l-1层第i个变量的边际分布, 即:</p>
                </div>
                <div class="area_img" id="223">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJC201902027_22300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="224">每一层隐函数变量引入随机参数ε<sub>i</sub><sup>l</sup>, 可以从第0层开始对隐函数变量逐层向前采样, 有:</p>
                </div>
                <div class="area_img" id="225">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJC201902027_22500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="226">式 (29) 中的[μ<sup>～l</sup>]<sub>i</sub>、[Σ<sup>～l</sup>]<sub>ii</sub>是关于第l-1层输出的函数, 具体表达式如式 (13) 、式 (14) 。对于最后一层, 伪点再参数化方法类似, 引入随机参数<image id="266" type="formula" href="images/JSJC201902027_26600.jpg" display="inline" placement="inline"><alt></alt></image></p>
                </div>
                <div class="area_img" id="227">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJC201902027_22700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="228">其中, L<sub>d</sub><sup>L</sup>为S<sub>d</sub><sup>L</sup>的Cholesky分解表达式<image id="267" type="formula" href="images/JSJC201902027_26700.jpg" display="inline" placement="inline"><alt></alt></image></p>
                </div>
                <div class="area_img" id="229">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJC201902027_22900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="230">本文利用随机梯度下降法优化得到最优变分分布。带有噪声梯度的梯度下降法在给定的学习率下可以收敛到局部最优, 但在实际中很难操作。学习率的初始值集合会在算法收敛处影响学习率, 错误设置的初始值会导致偏离。基于上述原因, 一些新的算法被提出来处理噪声梯度, 如AdaGrad<citation id="295" type="reference"><link href="143" rel="bibliography" /><sup>[<a class="sup">32</a>]</sup></citation>、RM SPROP<citation id="296" type="reference"><link href="145" rel="bibliography" /><sup>[<a class="sup">33</a>]</sup></citation>等优化算法, 这些算法在梯度处理上均略有不同。文献<citation id="297" type="reference">[<a class="sup">34</a>]</citation>在不同的单元检测上对这些不同的优化技巧做对比, 表明RMSPROP在多数检验集合上相较其他的优化方法都有较好的表现。因此, 本文在变分下界的优化中选择RMSPROP算法。本文利用带有自动求导机制的Tensorflow平台来进行模型推断、参数优化以及模型预测。模型的推断流程如图3所示。</p>
                </div>
                <div class="area_img" id="231">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201902027_23100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 CLDGP模型变分推断流程" src="Detail/GetImg?filename=images/JSJC201902027_23100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 CLDGP模型变分推断流程  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201902027_23100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="232" name="232">2.4 分布估计整体步骤</h4>
                <div class="p1">
                    <p id="233">分布估计过程的具体步骤如下:</p>
                </div>
                <div class="p1">
                    <p id="234">步骤1建立训练集Y及检验集数据集Y<sup>*</sup>。为与现有文献研究结果进行对比, 采用将检验数据集Y<sup>*</sup>随机抽取部分元素设为缺失值, 即Y<sup>*</sup>= (Y<sub>U</sub><sup>*</sup>, Y<sub>O</sub><sup>*</sup>) 。其中, Y<sub>U</sub><sup>*</sup>为缺失部分, Y<sub>O</sub><sup>*</sup>为观测部分。</p>
                </div>
                <div class="p1">
                    <p id="235">步骤2根据第2.1节和第2.2节所述建立模型, 将部分观测点的输入隐变量与训练集数据结合, 利用本文提供的变分推断架构训练。</p>
                </div>
                <div class="p1">
                    <p id="236">步骤3对检验数据集中的缺失部分进行分布估计, 得到P (Y<sub>U</sub><sup>*</sup>|Y<sub>O</sub><sup>*</sup>, Y<sup>*</sup>) 。</p>
                </div>
                <div class="p1">
                    <p id="237">步骤4利用分布估计指标评估效果, 进行性能对比。</p>
                </div>
                <h3 id="238" name="238" class="anchor-tag">3 实例结果与分析</h3>
                <div class="p1">
                    <p id="239">为与CLGP模型进行性能比对, 本文采用文献<citation id="298" type="reference">[<a class="sup">4</a>]</citation>所采用的部分代表性数据集以及CLGP模型的结果进行实验对比。</p>
                </div>
                <div class="p1">
                    <p id="240">在实验中, LGM模型、CLGP模型、以及本文提出的CLDGP模型都以二维隐空间初始化, CLDGP模型的层数选择为3层。隐输入的均值μ<sub>i</sub>以标准正态分布进行随机初始化, 所有层的伪输出m<sup>l</sup><sub>dk</sub>以标准偏差为10<sup>-2</sup>的正态分布随机初始化, 这种初始化方法等价于对所有值使用一个均匀初始分布。</p>
                </div>
                <div class="p1">
                    <p id="241">每一层隐函数的标准偏差初始化为0.1, 自动相关决策径向基函数协方差函数的尺度参数初始化为0.1, 变分分布优化迭代500次, 每一次迭代优化所有参数变量但是保持伪点u<sup>l</sup><sub>dk</sub>的变分参数固定, 然后再优化伪点u<sup>l</sup><sub>dk</sub>的变分参数且保持其他量固定不变。</p>
                </div>
                <div class="p1">
                    <p id="242">本文模型可用于带有部分观察数据的半监督学习, 部分观察点的隐部分通过训练集合来进行优化, 然后预测缺失值。本文采用文献<citation id="299" type="reference">[<a class="sup">4</a>]</citation>和文献<citation id="300" type="reference">[<a class="sup">5</a>]</citation>相同的评估指标困惑度<citation id="301" type="reference"><link href="149" rel="bibliography" /><sup>[<a class="sup">35</a>]</sup></citation>作为评估模型性能的误差指标。困惑度是一种常用的用来评估样本数据概率分布估计效果的统计量, 该统计量定义为负平均对数预测概率的指数值, 即对于所预测的d元输出概率p (y<sub>nd</sub>) , n=1, 2, …, N, d=1, 2, …, D的困惑度指标定义为<image id="268" type="formula" href="images/JSJC201902027_26800.jpg" display="inline" placement="inline"><alt></alt></image>。例如, 二元数据正确预测结果的误差为0, 随机猜测即对每一个数据点的每一种二元值预测概率0.5会使得困惑度误差为1, 而对任意正确类别值给予概率0的困惑度误差为∞。</p>
                </div>
                <div class="p1">
                    <p id="243">本文实验均用Python语言, 高斯过程模型建立在Tensorflow<citation id="302" type="reference"><link href="151" rel="bibliography" /><sup>[<a class="sup">36</a>]</sup></citation>平台上的GPflow<citation id="303" type="reference"><link href="153" rel="bibliography" /><sup>[<a class="sup">37</a>]</sup></citation>平台, 模型参数优化辅助利用Tensorflow的自动求导方法。</p>
                </div>
                <h4 class="anchor-tag" id="244" name="244">3.1 小批量多元类别手写字母数据集</h4>
                <div class="p1">
                    <p id="245">数据来源于文献<citation id="304" type="reference">[<a class="sup">38</a>]</citation>, 其中, 手写字母数据集包含10个手写数字 (0～9) 以及26个手写大写字母 (A～Z) , 每一张图像分辨率为20×16像素, 每一个类别包含39张图像。将图像重置为10×8像素可以得到1 404 (39×36) 像素个80维的变量数据点。图4所示为10个手写数字与部分手写字母的示例。</p>
                </div>
                <div class="area_img" id="246">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201902027_24600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 二元手写数字字幕数据集示例" src="Detail/GetImg?filename=images/JSJC201902027_24600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 二元手写数字字幕数据集示例  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201902027_24600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="247">本文将36个类别的图像分别划分为30个训练样本与9个检验样本。对于检验样本集合, 随机移除20%的像素做模型检验, 然后训练模型, 评估预测误差。</p>
                </div>
                <div class="p1">
                    <p id="248">图5所示为基于困惑度的检验误差在训练集以及检验集上的变化结果。CLGP模型收敛速度比CLDGP模型快, CLGP模型最终预测误差比CLDGP模型高, CLGP模型的训练集检验集误差分别为0.634、0.705, CLDGP模型的训练集检验集误差分别为0.571、0.658。图6所示为36种类别数据点在优化以后二元隐空间上的投影, 即将不同类别数据点以不同颜色形状区分投影到模型优化所得的二元输入隐变量空间中, 不同类别数据区分度越大表示隐空间优化效果越好。从图6可以看出, CLDGP模型对于不同类别的数据点区分度要大一些, 即区别能力比CLGP模型好。</p>
                </div>
                <div class="area_img" id="249">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201902027_24900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 2种模型误差在测试集和检验集上的对比结果" src="Detail/GetImg?filename=images/JSJC201902027_24900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 2种模型误差在测试集和检验集上的对比结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201902027_24900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="250">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201902027_25000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 二元隐空间分类绘图" src="Detail/GetImg?filename=images/JSJC201902027_25000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 二元隐空间分类绘图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201902027_25000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="251" name="251">3.2 小批量多元类别医疗诊断数据集</h4>
                <div class="p1">
                    <p id="252">威斯康星肺癌数据集<citation id="305" type="reference"><link href="157" rel="bibliography" /><sup>[<a class="sup">39</a>]</sup></citation>由683个数据点组成, 带有9个取值在1～10的类别变量, 以及一个取值0、1的二值类别变量, 共2×10<sup>9</sup>种可能的配置结果。本文利用75%数据集作为训练数据集, 剩余作为检验数据集, 实验重复3次, 且对数据做3次随机划分。在检验集合中, 随机移除10个类别变量中的1个, 然后检验模型对数据的恢复能力。</p>
                </div>
                <div class="p1">
                    <p id="253">本文将CLGP、基准模型、LGM模型以及CLDGP模型作对比。表1所示为3种模型在3种数据划分情况下的检验集数据误差结果。从表1可以看出, 基准模型的检验集合困惑度最高, 尤其在数据集合3上出现了较大的分布估计误差, CLGP模型在3项划分数据集上较LGM模型均有所提升, CLDGP模型在检验数据困惑度与估计结果方差上性能都比较好。</p>
                </div>
                <div class="area_img" id="254">
                                            <p class="img_tit">
                                                表1 3种模型在3种数据下检验集数据误差结果
                                                
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201902027_25400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/JSJC201902027_25400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201902027_25400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表1 3种模型在3种数据下检验集数据误差结果" src="Detail/GetImg?filename=images/JSJC201902027_25400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>
                <h3 id="255" name="255" class="anchor-tag">4 结束语</h3>
                <div class="p1">
                    <p id="256">为提高类别隐高期模型的表达能力, 本文通过深度高斯过程作非线性变换, 建立生成式模型对多元类别数据的分布进行估计。利用小批量手写数字字母、医疗诊断数据集、类别隐高斯过程模型以及隐高斯模型的性能进行了对比, 验证了模型的有效性。下一步将在参数化的过程中加入控制协变量, 以提高参数估计稳定性, 同时将神经网络与高斯过程模型相结合, 以提高模型容量。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="81">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201708035&amp;v=MzI2NzR6cXFCdEdGckNVUkxPZVplUm5GeWprVUwzS0x6N0JiYkc0SDliTXA0OUdZWVFLREg4NHZSNFQ2ajU0TzM=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b>王盛玉, 曾碧卿, 胡翩翩.基于卷积神经网络参数优化的中文情感分析[J].计算机工程, 2017, 43 (8) :200-207, 214.
                            </a>
                        </p>
                        <p id="83">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Neural probabilistic language models">

                                <b>[2]</b>BENGIO Y, SCHWENK H, SENCAL J S, et al.Neural probabilistic language models[M].In Innovations in Machine Learning.Berlin, Germany:Springer, 2006:137-186.
                            </a>
                        </p>
                        <p id="85">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Unified Architecture for Natural Language Processing:Deep Neural Networks with Multitask Learning">

                                <b>[3]</b>COLLOBERT R, WESTON J.A unified architecture for natural language processing:deep neural networks with multitask learning[C]//Proceedings of the 25th International Conference on Machine learning.New York, USA:ACMPress, 2008:160-167.
                            </a>
                        </p>
                        <p id="87">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Latent Gaussian process for distribution estimation of multivariate categorical data">

                                <b>[4]</b>GAL Y, CHEN Y, ZOUBIN G.Latent Gaussian process for distribution estimation of multivariate categorical data[EB/OL].[2017-11-18].https://arxiv.org/pdf/1503.02182.pdf
                            </a>
                        </p>
                        <p id="89">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A stickbreaking likelihood for categorical data analysis with latent Gaussian models">

                                <b>[5]</b>KHAN M E, MOHAMED S, MARLIN B R, et al.A stickbreaking likelihood for categorical data analysis with latent Gaussian models[EB/OL].[2017-11-18].https://www.shakirm.com/papers/catLGM-AIstats 20 12.pdf.
                            </a>
                        </p>
                        <p id="91">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=KZYC201308002&amp;v=Mjc1OTVrVUwzS0xqZlNiYkc0SDlMTXA0OUZab1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm5GeWo=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b>何志昆, 刘光斌, 赵曦晶, 等.高斯过程回归方法综述[J].控制与决策, 2013, 28 (8) :1121-1129, 1137.
                            </a>
                        </p>
                        <p id="93">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Gaussian process for machine learning">

                                <b>[7]</b>RASMUSSEN C E, WILLIAM K I.Gaussian process for machine learning[EB/OL].[2017-11-18].http://www.gaussianprocess.org/gpml/.
                            </a>
                        </p>
                        <p id="95">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=GP-Bayes filters:Bayesian filtering using Gaussian process prediction and observation models">

                                <b>[8]</b>KO J, FOX D.GP-Bayes filters:Bayesian filtering using Gaussian process prediction and observation models[C]//Proceedings of IEEE/RSJ Intelligent Robots and Systems.Washington D.C., USA:IEEE Press, 2008:3471-3476.
                            </a>
                        </p>
                        <p id="97">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=PILCO:A Model-Based and Data-Efficient Approach to Policy Search">

                                <b>[9]</b>DEISENROTH M P, RASMUSSEN C E.PILCO:a modelbased and data-efficient approach to policy search[C]//Proceedings of the 28th International Conference on Machine Learning.[S.l.]:Omnipress, 2011:465-472.
                            </a>
                        </p>
                        <p id="99">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Statistics for spatio-temporal data">

                                <b>[10]</b>CRESSIE N, WIKLE K.Statistics for spatio-temporal data[EB/OL].[2017-11-18].https://www.wiley.com/en-us/Statistics+for+Spatio+Temporal+Datap-9780471692744.
                            </a>
                        </p>
                        <p id="101">
                            <a id="bibliography_11" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSYJ201501015&amp;v=MzA3NTA1NE8zenFxQnRHRnJDVVJMT2VaZVJuRnlqa1VMM0tMejdTWkxHNEg5VE1ybzlFWVlRS0RIODR2UjRUNmo=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[11]</b>王鑫, 李红丽.台风最大风速预测的高斯过程回归模型[J].计算机应用研究, 2015, 32 (1) :59-62.
                            </a>
                        </p>
                        <p id="103">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Probabilistic integration:a role for statisticians in numerical analysis?">

                                <b>[12]</b>BRIOL F X, OATES C J, GIROLAMI M, et al.Probabilistic integration:a role for statisticians in numerical analysis?[EB/OL].[2017-11-18].https://arxiv.org/pdf/1512.00933v5.pdf.
                            </a>
                        </p>
                        <p id="105">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Near-optimal sensor placements in Gaussian processes">

                                <b>[13]</b>GUESTRIN C, KRAUSE A, SINGH A P.Near-optimal sensor placements in Gaussian processes[C]//Proceedings of the 22nd International Conference on Machine Learning.New York, USA:ACM Press, 2005:265-272.
                            </a>
                        </p>
                        <p id="107">
                            <a id="bibliography_14" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201402002&amp;v=MDYzMTlLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm5GeWprVUwzS0tDTGZZYkc0SDlYTXJZOUZab1E=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[14]</b>孙晓燕, 陈姗姗, 巩敦卫, 等.基于区间适应值交互式遗传算法的加权多输出高斯过程代理模型[J].自动化学报, 2014, 40 (2) :172-184.
                            </a>
                        </p>
                        <p id="109">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Practical Bayesian optimization of machine learning algorithms">

                                <b>[15]</b>SNOEK J, LAROCHELLE H, ADAMS R P.Practical bayesian optimization of machine learning algorithms[C]//Proceedings of the 25th International Conference on Neural Information Processing Systems[S.l.]:Curran Associates Inc., 2012:2951-2959.
                            </a>
                        </p>
                        <p id="111">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Stochastic back propagation and approximate inference in deep generative models">

                                <b>[16]</b>REZENDE D J, MOHAMED S, WIERSTRA D.Stochastic back propagation and approximate inference in deep generative models[J].Pattern Recognition and Machine Learning, 2014, 32 (2) :1278-1286.
                            </a>
                        </p>
                        <p id="113">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Probabilistic machine learning and artificial intelligence">

                                <b>[17]</b>GHAHRAMANI Z.Probabilistic machine learning and artificial intelligence[J].Nature, 2015, 521:452-459.
                            </a>
                        </p>
                        <p id="115">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep Gaussian processes">

                                <b>[18]</b>DAMIANOU A C, LAWRENCE N D.Deep Gaussian processes[EB/OL].[2017-11-18].https://core.ac.uk/download/pdf/46564399.pdf.
                            </a>
                        </p>
                        <p id="117">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep kernel learning">

                                <b>[19]</b>WILSON A G, HU Z, SALAKHUTDINOV R, et al.Deep kernel learning[EB/OL].[2017-11-18].https://arxiv.org/pdf/1511.02222.pdf.
                            </a>
                        </p>
                        <p id="119">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Additive kernels for Gaussian process modeling">

                                <b>[20]</b>DURRANDE N, GINSBOURGER D, ROUSTANT O.Additive kernels for Gaussian process modeling[EB/OL].[2017-11-18].https://arxiv.org/pdf/1103.4023.pdf.
                            </a>
                        </p>
                        <p id="121">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Structure discovery in nonparametric regression through compositional kernel search">

                                <b>[21]</b>DAVID D, JAMES R L, ROGER G, et al.Structure discovery in nonparametric regression through compositional kernel search[EB/OL].[2017-11-18].http://www.cs.toronto.edu/~rgrosse/icml2013-gp.pdf.
                            </a>
                        </p>
                        <p id="123">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Nested variational compression in deep Gaussian processes">

                                <b>[22]</b>HENSMAN J, LAWRENCE N D.Nested variational compression in deep Gaussian processes[EB/OL].[2017-11-18].https://arxiv.org/pdf/1412.1370.pdf.
                            </a>
                        </p>
                        <p id="125">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Training deep Gaussian processes with sampling">

                                <b>[23]</b>VAFA K.Training deep Gaussian processes with sampling[EB/OL].[2017-11-18].http://approxi mateinference.org/accepted/Vafa2016.pdf.
                            </a>
                        </p>
                        <p id="127">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Sequential inference for deep Gaussian process">

                                <b>[24]</b>WANG Y, BRUBAKER M, CHAIB-DRAA B, et al.Sequential inference for deep Gaussian process[EB/OL].[2017-11-18].http://proceedings.mlr.press/v51/wang16c.pdf.
                            </a>
                        </p>
                        <p id="129">
                            <a id="bibliography_25" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep Gaussian processes for regression using approximate expectation propagation">

                                <b>[25]</b>BUI T D, HERNNDEZ-LOBATO D, LI Y, et al.Deep Gaussian processes for regression using approximate expectation propagation[EB/OL].[2017-11-18].http://proceedings.mlr.press/v48/bui16.pdf.
                            </a>
                        </p>
                        <p id="131">
                            <a id="bibliography_26" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Variational autoencoded deep Gaussian processes">

                                <b>[26]</b>DAI Z, DAMIANOU A, GONZLEZ J, et al.Variational autoencoded deep Gaussian processes[EB/OL].[2017-11-18].https://arxiv.org/pdf/1511.06455.pdf.
                            </a>
                        </p>
                        <p id="133">
                            <a id="bibliography_27" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Variational Gaussian process dynamical systems">

                                <b>[27]</b>DAMIANOU A D, TITSIAS M K, LAWRENCE N D.Variational Gaussian process dynamical systems[EB/OL].[2017-11-18].http://papers.nips.cc/paper/4330-variationalgaussianprocess-dynamical-systems.pdf.
                            </a>
                        </p>
                        <p id="135">
                            <a id="bibliography_28" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Practical learning of deep Gaussian processes via random fourier features">

                                <b>[28]</b>CUTAJAR K, BONILLA E V, MICHIARDI P, et al.Practical learning of deep Gaussian processes via random fourier features[EB/OL].[2017-11-18].https://pdfs.semanticscholar.org/bafa/7e2d586e7bfe77d9a55ac1cff4eb2f6ff292.pdf.
                            </a>
                        </p>
                        <p id="137">
                            <a id="bibliography_29" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Doubly stochastic variational inference for deep Gaussian processes">

                                <b>[29]</b>HUGH S, MARC D.Doubly stochastic variational inference for deep Gaussian processes[EB/OL].[2017-11-18].https://arxiv.org/pdf/1705.08933.pdf.
                            </a>
                        </p>
                        <p id="139">
                            <a id="bibliography_30" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Variational learning of inducing variables in sparse Gaussian processes">

                                <b>[30]</b>TITSIAS M K.Variational learning of inducing variables in sparse Gaussian processes[EB/OL].[2017-11-18].http://proceedings.mlr.press/v5/titsias09a/titsias09a.pdf.
                            </a>
                        </p>
                        <p id="141">
                            <a id="bibliography_31" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Variational inference:a review for statisticians">

                                <b>[31]</b>BLEI D M, KUCUKELBIR A, MCAULIFFE J D.Variational inference:a review for statisticians[EB/OL].[2017-11-18].https://arxiv.org/pdf/1601.00670.pdf.
                            </a>
                        </p>
                        <p id="143">
                            <a id="bibliography_32" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Adaptive subgradient methods for online learning and stochastic optimization">

                                <b>[32]</b>JOHN D, ELAD H, YORAM S.Adaptive subgradient methods for online learning and stochastic optimization[J].Journal of Machine Learning Research, 2011, 12:2121-2159.
                            </a>
                        </p>
                        <p id="145">
                            <a id="bibliography_33" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Lecture 6.5-rmsprop,COURSERA:neural networks for machine learning">

                                <b>[33]</b>TIELEMAN T, HINTON G.Lecture 6.5-rmsprop, COURSERA:neural networks for machine learning[EB/OL].[2017-11-18].http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf.
                            </a>
                        </p>
                        <p id="147">
                            <a id="bibliography_34" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Unit tests for stochastic optimization">

                                <b>[34]</b>TOM S, IOANNIS A, DAVID S.Unit tests for stochastic optimization[EB/OL].[2017-11-18].https://arxiv.org/pdf/1312.6055.pdf.
                            </a>
                        </p>
                        <p id="149">
                            <a id="bibliography_35" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Perplexity[G/OL]">

                                <b>[35]</b>Perplexity[G/OL].[2017-11-18].https://en.wiki pedia.org/wiki/Perplexity.
                            </a>
                        </p>
                        <p id="151">
                            <a id="bibliography_36" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Tensorflow">

                                <b>[36]</b>Tensorflow[EB/OL].[2017-11-18].https://tenso rflow.google.cn/.
                            </a>
                        </p>
                        <p id="153">
                            <a id="bibliography_37" target="_blank" href="http://scholar.cnki.net/result.aspx?q=GPflow:a gaussian process library using tensorflow">

                                <b>[37]</b>MATTHEWS A G D G, MARK V D W, NICKSON T, et al.GPflow:a gaussian process library using tensorflow[EB/OL].[2017-11-18].http://adsabs.harvard.edu/abs/2016arXiv161008733M.
                            </a>
                        </p>
                        <p id="155">
                            <a id="bibliography_38" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Handwritten digits">

                                <b>[38]</b>Handwritten digits[DB/OL].[2017-11-18].https://cs.nyu.edu/~roweis/data.html.
                            </a>
                        </p>
                        <p id="157">
                            <a id="bibliography_39" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Breast cancer data set">

                                <b>[39]</b>MATJAZ Z, MILAN S.Breast cancer data set[DB/OL].[2017-11-18].http://archive.ics.uci.edu/ml/datasets/Breast+Cancer.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJC201902027" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201902027&amp;v=MDE3NDk5ak1yWTlIWTRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJuRnlqa1VMM0tMejdCYmJHNEg=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VEcmE1Yy9WeVdIeHJGV1pCK3VXND0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
