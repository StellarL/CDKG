<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637130530622530000%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJC201905031%26RESULT%3d1%26SIGN%3dXZlJHO9VgV4hHAM7a2ESU1b0N3E%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201905031&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201905031&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201905031&amp;v=MTEzNzVxcUJ0R0ZyQ1VSTE9lWmVSb0Z5M21VcnZJTHo3QmJiRzRIOWpNcW85R1pZUUtESDg0dlI0VDZqNTRPM3o=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#37" data-title="0 概述 ">0 概述</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#43" data-title="1 投影矩阵 ">1 投影矩阵</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#51" data-title="2 字典训练与图像重建 ">2 字典训练与图像重建</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#53" data-title="2.1 字典离线训练阶段">2.1 字典离线训练阶段</a></li>
                                                <li><a href="#67" data-title="2.2 图像在线重建阶段">2.2 图像在线重建阶段</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#74" data-title="3 实验与结果分析 ">3 实验与结果分析</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#94" data-title="4 结束语 ">4 结束语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#55" data-title="&lt;b&gt;图1 字典离线训练阶段的流程&lt;/b&gt;"><b>图1 字典离线训练阶段的流程</b></a></li>
                                                <li><a href="#73" data-title="&lt;b&gt;图2 在线合成放大阶段流程&lt;/b&gt;"><b>图2 在线合成放大阶段流程</b></a></li>
                                                <li><a href="#77" data-title="&lt;b&gt;图3 训练图像&lt;/b&gt;"><b>图3 训练图像</b></a></li>
                                                <li><a href="#79" data-title="&lt;b&gt;表1 超分辨率重建方法的PSNR对比&lt;/b&gt;"><b>表1 超分辨率重建方法的PSNR对比</b></a></li>
                                                <li><a href="#80" data-title="&lt;b&gt;表2 超分辨率重建方法的SSIM对比&lt;/b&gt;"><b>表2 超分辨率重建方法的SSIM对比</b></a></li>
                                                <li><a href="#82" data-title="&lt;b&gt;表3 超分辨率重建方法的运行时间比较&lt;/b&gt;"><b>表3 超分辨率重建方法的运行时间比较</b></a></li>
                                                <li><a href="#85" data-title="&lt;b&gt;图4 对Lena图像的超分辨率处理结果&lt;/b&gt;"><b>图4 对Lena图像的超分辨率处理结果</b></a></li>
                                                <li><a href="#86" data-title="&lt;b&gt;图5 对Monarch图像的超分辨率处理结果&lt;/b&gt;"><b>图5 对Monarch图像的超分辨率处理结果</b></a></li>
                                                <li><a href="#91" data-title="&lt;b&gt;表4 文献&lt;/b&gt;&lt;b&gt;方法与本文方法对比结果&lt;/b&gt;"><b>表4 文献</b><b>方法与本文方法对比结果</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" title=" ZONTAK M, IRANI M.Internal statistics of a single natural image[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2011:977-984." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Internal statistics of a single natural image">
                                        <b>[1]</b>
                                         ZONTAK M, IRANI M.Internal statistics of a single natural image[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2011:977-984.
                                    </a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" title=" HUANG Jiabin.Exploiting self-similarities for single frame super-resolution[C]//Proceedings of Asian Conference on Computer Vision.Berlin, Germany:Springer, 2011:497-510." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Exploiting self-similarities for single frame super-resolution">
                                        <b>[2]</b>
                                         HUANG Jiabin.Exploiting self-similarities for single frame super-resolution[C]//Proceedings of Asian Conference on Computer Vision.Berlin, Germany:Springer, 2011:497-510.
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title=" FREEDMAN G, FATTAL R.Image and video upscaling from local self-examples[J].ACM Transactions on Graphics, 2011, 30 (2) :474-484." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000004443&amp;v=MTg4MjBIeWptVUxuSUpWMFZiaEk9TmlmSVk3SzdIdGpOcjQ5RlpPc0xDSGc2b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadQ==&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         FREEDMAN G, FATTAL R.Image and video upscaling from local self-examples[J].ACM Transactions on Graphics, 2011, 30 (2) :474-484.
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" title=" ZHANG Kaibing, GAO Xinbo, TAO Dacheng, et al.Single image super-resolution with multiscale similarity learning[J].IEEE Transactions on Neural Networks and Learning Systems, 2013, 24 (10) :1648-1659." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Single image super-resolution with multiscale similarity learning">
                                        <b>[4]</b>
                                         ZHANG Kaibing, GAO Xinbo, TAO Dacheng, et al.Single image super-resolution with multiscale similarity learning[J].IEEE Transactions on Neural Networks and Learning Systems, 2013, 24 (10) :1648-1659.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" title=" CHANG Hong, XIONG Yimin.Super-resolution through neighbor embedding[C]//Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2004:275-282." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Super-Resolution Through Neighbor Embedding">
                                        <b>[5]</b>
                                         CHANG Hong, XIONG Yimin.Super-resolution through neighbor embedding[C]//Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2004:275-282.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" title=" BUADES A, COLL B, MOREL J M.A non-local algorithm for image denoising[C]//Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2005:60-65." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A non-local algorithm for image denoising">
                                        <b>[6]</b>
                                         BUADES A, COLL B, MOREL J M.A non-local algorithm for image denoising[C]//Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2005:60-65.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" title=" BEVILACQUA M, ROUMY A, GUILLEMOT C, et al.Single-image super-resolution via linear mapping of interpolated self-examples[J].IEEE Transactions on Image Processing, 2014, 23 (12) :5334-5347." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Single-image super-resolution via linear mapping of interpolated self-examples">
                                        <b>[7]</b>
                                         BEVILACQUA M, ROUMY A, GUILLEMOT C, et al.Single-image super-resolution via linear mapping of interpolated self-examples[J].IEEE Transactions on Image Processing, 2014, 23 (12) :5334-5347.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" title=" TIMOFTE R, DE SMET V, VAN GOOL L.Anchored neighborhood regression for fast example-based super-resolution[C]//Proceedings of IEEE International Conference on Computer Vision.Washington D.C., USA:IEEE Press, 2013:1920-1927." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Anchored neighborhood regression for fast example-based super-resolution">
                                        <b>[8]</b>
                                         TIMOFTE R, DE SMET V, VAN GOOL L.Anchored neighborhood regression for fast example-based super-resolution[C]//Proceedings of IEEE International Conference on Computer Vision.Washington D.C., USA:IEEE Press, 2013:1920-1927.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" title=" TIMOFTE R, DE SMET V, VAN GOOL L.A+:adjusted anchored neighborhood regression for fast super-resolution[C]//Proceedings of Asian Conference on Computer Vision.Berlin, Germany:Springer, 2014:111-126." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A+:adjusted anchored neighborhood regression for fast super-resolution">
                                        <b>[9]</b>
                                         TIMOFTE R, DE SMET V, VAN GOOL L.A+:adjusted anchored neighborhood regression for fast super-resolution[C]//Proceedings of Asian Conference on Computer Vision.Berlin, Germany:Springer, 2014:111-126.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_10" title=" TIMOFTE R, ROTHE R, VAN GOOL L.Seven ways to improve example-based single image super resolution[C]//Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2016:1865-1873." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Seven ways to improve example-based single image super resolution">
                                        <b>[10]</b>
                                         TIMOFTE R, ROTHE R, VAN GOOL L.Seven ways to improve example-based single image super resolution[C]//Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2016:1865-1873.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_11" title=" DONG Chao, CHEN Chang, HE Kaiming, et al.Learning a deep convolutional network for image super-resolution[C]//Proceedings of European Conference on Computer Vision.Berlin, Germany:Springer, 2014:184-199." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning a deep convolutional network for image super-resolution">
                                        <b>[11]</b>
                                         DONG Chao, CHEN Chang, HE Kaiming, et al.Learning a deep convolutional network for image super-resolution[C]//Proceedings of European Conference on Computer Vision.Berlin, Germany:Springer, 2014:184-199.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_12" title=" 王晓晖, 盛斌, 申瑞民.基于深度学习的深度图超分辨率采样[J].计算机工程, 2017, 43 (11) :252-260." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201711042&amp;v=MjM2MTA1NE8zenFxQnRHRnJDVVJMT2VaZVJvRnkzbVVydklMejdCYmJHNEg5Yk5ybzlCWm9RS0RIODR2UjRUNmo=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[12]</b>
                                         王晓晖, 盛斌, 申瑞民.基于深度学习的深度图超分辨率采样[J].计算机工程, 2017, 43 (11) :252-260.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_13" title=" YANG J, WRIGHT J, HUANG T S, et al.Image super-resolution via sparse representation[J].IEEE Transactions on Image Processing, 2010, 19 (1) :2861-2873." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image super-resolution via sparse representation">
                                        <b>[13]</b>
                                         YANG J, WRIGHT J, HUANG T S, et al.Image super-resolution via sparse representation[J].IEEE Transactions on Image Processing, 2010, 19 (1) :2861-2873.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_14" title=" ZEYDE R, ELAD M, PROTTER M.On single image scale-up using sparse-representations[J].Lecture Notes in Computer Science, 2010, 6920:711-730." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=On single image scaleup using sparse-representations">
                                        <b>[14]</b>
                                         ZEYDE R, ELAD M, PROTTER M.On single image scale-up using sparse-representations[J].Lecture Notes in Computer Science, 2010, 6920:711-730.
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_15" title=" ZHANG Jian, ZHAO Chen, XIONG Ruiqin, et al.Image super-resolution via dual-dictionary learning and sparse representation[C]//Proceedings of IEEE International Symposium on Circuits and Systems.Washington D.C., USA:IEEE Press, 2012:1688-1691." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image super-resolution via dual-dictionary learning and sparse representation">
                                        <b>[15]</b>
                                         ZHANG Jian, ZHAO Chen, XIONG Ruiqin, et al.Image super-resolution via dual-dictionary learning and sparse representation[C]//Proceedings of IEEE International Symposium on Circuits and Systems.Washington D.C., USA:IEEE Press, 2012:1688-1691.
                                    </a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_16" title=" TIAN Yapeng, ZHOU Fei, YANG Wenming.Anchored neighborhood regression based Single image super-resolution from self-examples[C]//Proceedings of IEEE International Conference on Image Processing.Washington D.C., USA :IEEE Press, 2016:2827-2831." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Anchored neighborhood regression based single image super-resolution from self-examples">
                                        <b>[16]</b>
                                         TIAN Yapeng, ZHOU Fei, YANG Wenming.Anchored neighborhood regression based Single image super-resolution from self-examples[C]//Proceedings of IEEE International Conference on Image Processing.Washington D.C., USA :IEEE Press, 2016:2827-2831.
                                    </a>
                                </li>
                                <li id="35">


                                    <a id="bibliography_17" title=" WANG Zhou, BOVIK A, SHEIKH H.Image quality assessment:from error visibility to structural similarity[J].IEEE Transactions on Image Processing, 2004, 13 (4) :600-612." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image quality assessment: from error visibility to structural similarity">
                                        <b>[17]</b>
                                         WANG Zhou, BOVIK A, SHEIKH H.Image quality assessment:from error visibility to structural similarity[J].IEEE Transactions on Image Processing, 2004, 13 (4) :600-612.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJC" target="_blank">计算机工程</a>
                2019,45(05),194-198 DOI:10.19678/j.issn.1000-3428.0050309            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>锚点领域回归与稀疏表示的图像超分辨率方法</b></span>
 <span class="shoufa"></span>                                     </h1>

            </div>
                        <h2>
                                <a href="javascript:;">端木春江</a>
                                <a href="javascript:;">左德遥</a>
                </h2>
                    <h2>

                    <span>浙江师范大学数理与信息工程学院</span>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>结合锚点领域回归与稀疏表示方法, 提出一种改进的图像超分辨率方法。通过对高分辨率图像采用模糊和下采样操作生成低分辨率图像, 基于锚点邻域回归的线性映射函数训练投影矩阵, 利用稀疏表示的方法训练和学习稀疏字典对。在图像放大阶段, 根据训练好的投影矩阵重建主要高频特征, 利用稀疏字典对补充残差高频特征。实验结果表明, 该方法能较好地保持图像的局部细节信息, 减少块效应和伪影效应。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">图像超分辨率;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%AD%97%E5%85%B8%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">字典学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%A8%80%E7%96%8F%E8%A1%A8%E7%A4%BA&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">稀疏表示;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%94%9A%E7%82%B9%E9%82%BB%E5%9F%9F%E5%9B%9E%E5%BD%92&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">锚点邻域回归;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%9B%BE%E5%83%8F%E6%94%BE%E5%A4%A7&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">图像放大;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    端木春江 (1974—) , 男, 副教授, 主研方向为图像处理、视频通信;E-mail: c_duanmu@ aliyun. com;
                                </span>
                                <span>
                                    左德遥, 硕士研究生。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-01-26</p>

                    <p>

                            <b>基金：</b>
                                                        <span>浙江省自然科学基金 (LY15F010007, LY18F010017);</span>
                    </p>
            </div>
                    <h1><b>Image Super-resolution Method via Anchored Neighborhood Regression and Sparse Representation</b></h1>
                    <h2>
                    <span>DUANMU Chunjiang</span>
                    <span>ZUO Deyao</span>
            </h2>
                    <h2>
                    <span>College of Mathematics, Physics and Information Engineering, Zhejiang Normal University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Combining anchored neighborhood regression and sparse representation methods, this paper proposes an image super-resolution method.By blurring and subsampling high-resolution image to generate low-resolution image, the linear mapping function based on anchored neighborhood regression is used to train the projection matrix, and sparse representation is used to train and learn sparse dictionary pairs.In the online image magnification stage, the main high frequency features are generated by using the trained projection matrix.Then, the sparse dictionary pairs are employed to reconstruct the residual high frequency features.Experimental results show that the proposed method can maintain the local detail information of the image, reduce the blocks and aliasing artifacts.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=image%20super-resolution&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">image super-resolution;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=dictionary%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">dictionary learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=sparse%20representation&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">sparse representation;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=anchored%20neighborhood%20regression&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">anchored neighborhood regression;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=image%20magnification&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">image magnification;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-01-26</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="37" name="37" class="anchor-tag">0 概述</h3>
                <div class="p1">
                    <p id="38">单幅图像超分辨率技术是指仅从一幅低分辨率图像中重建出一幅高分辨率图像的过程。近年来, 该技术在实际生活中有着广泛的应用。例如在遥感图像、医学图像、监控图像、图像视频压缩和计算机视觉等领域, 该技术用来提高图像的分辨率和清晰度。</p>
                </div>
                <div class="p1">
                    <p id="39">单幅自然图像自身的内部统计信息包括有用的先验知识<citation id="96" type="reference"><link href="3" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>, 而这些信息可以用来提高单幅图像超分辨率技术的性能。例如, 文献<citation id="97" type="reference">[<a class="sup">2</a>]</citation>使用单幅图像中的图像块的自相似性和图像的分组结构信息重建高分辨率图像。文献<citation id="98" type="reference">[<a class="sup">3</a>]</citation>假定自然图像具有局部自相似性, 从输入图像的局部区域中提取与待放大块最近的邻域, 减少搜索最相似块的时间。文献<citation id="101" type="reference">[<a class="sup">4</a>,<a class="sup">5</a>]</citation>使用领域嵌入的方法实现了基于样例的图像超分辨率。该方法在低分辨率图像自身中交错使用不同尺度的相似冗余信息, 与此同时还采用非局部均值的方法<citation id="99" type="reference"><link href="13" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>在相同的尺度下学习相似性。文献<citation id="100" type="reference">[<a class="sup">7</a>]</citation>使用图像的双层金字塔, 从输入图像自身提取样例, 采用回归法直接将每个低分辨率图像块映射到高分辨率图像空间。</p>
                </div>
                <div class="p1">
                    <p id="40">除了上述基于内在先验学习的方法, 还有一种外在样例学习方法, 通过数据库中成对的低分率和高分辨率图像, 学习它们之间的映射关系, 用学习到的映射关系对图像进行超分辨率重建。文献<citation id="103" type="reference">[<a class="sup">8</a>,<a class="sup">9</a>]</citation>提出锚点邻域回归的方法及其变体A+, 将学习的字典元素作为锚点以计算多个线性回归。文献<citation id="102" type="reference">[<a class="sup">10</a>]</citation>对输入图像翻转、旋转, 从而扩大图像的训练集。深度学习也被引入单幅图像超分辨率技术。文献 <citation id="104" type="reference">[<a class="sup">11</a>,<a class="sup">12</a>]</citation>提出超分辨率卷积神经网络 (Super Resolution Convolutional Neural Network, SRCNN) 模型, 通过低分辨率图像和高分辨率图像直接学习端到端的映射。</p>
                </div>
                <div class="p1">
                    <p id="41">稀疏表示方法<citation id="106" type="reference"><link href="27" rel="bibliography" /><link href="29" rel="bibliography" /><sup>[<a class="sup">13</a>,<a class="sup">14</a>]</sup></citation>是利用高、低分辨率的稀疏字典和稀疏信号表示解决单幅图像的超分辨率问题。文献<citation id="105" type="reference">[<a class="sup">15</a>]</citation>利用残差字典学习恢复残差高频特征。</p>
                </div>
                <div class="p1">
                    <p id="42">采用上述方法对图像进行超分辨率重建, 放大后图像的边缘细节普遍存在模糊问题。为此, 本文在文献<citation id="107" type="reference">[<a class="sup">15</a>,<a class="sup">16</a>]</citation>的基础上, 将锚点回归和稀疏表示结合, 进行图像超分辨重建。对原始的高分辨率图像进行模糊和下采样得到低分辨率图像, 将此低分辨率图像作为训练集中的高分辨率图像生成主要高频 (Main High Frequency, MHF) 特征。只使用一个稀疏字典对, 即低分辨率的残差字典 (Low-resolution Residual Dictionary, LRD) 和高分辨率的残差字典 (High-resolution Residual Dictionary, HRD) , 训练字典以及合成图像。</p>
                </div>
                <h3 id="43" name="43" class="anchor-tag">1 投影矩阵</h3>
                <div class="p1">
                    <p id="44">文献<citation id="108" type="reference">[<a class="sup">16</a>]</citation>使用同一幅低分辨图像进行图像训练和重建。在训练阶段, 将低分辨率图像通过比例因子0.98<sup><i>i</i></sup> (<i>i</i>=0, 1, …, 20) 缩小后的图像作为高分辨率图像, 对高分辨率图像利用点扩展算子进行卷积操作, 下采样得到对应的低分辨率图像。分别对高、低分辨率图像进行分块获得对应的训练集。在低分辨率图像训练集中随机选择<i>K</i>个锚点<i>c</i><sub><i>k</i></sub> (<i>k</i>=1, 2, …, <i>K</i>) , 每个锚点<i>c</i><sub><i>k</i></sub>都对应一个邻域矩阵<b><i>N</i></b><sup><i>k</i></sup><sub>L</sub>, 每个邻域矩阵<b><i>N</i></b><sup><i>k</i></sup><sub>L</sub>由<i>n</i><sub><i>c</i></sub>个最相近的原子组成。这<i>n</i><sub><i>c</i></sub>个最相近的原子是通过计算训练样例中的每个原子与锚点的相关性, 选择相关性最大的原子得到的。其对应的高分辨率图像训练集的邻域矩阵为<b><i>N</i></b><sup><i>k</i></sup><sub>H</sub>。由此, 得到<i>K</i>个矩阵对{<b><i>N</i></b><sup><i>k</i></sup><sub>L</sub>, <b><i>N</i></b><sup><i>k</i></sup><sub>H</sub>}<mathml id="45"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>Κ</mi></msubsup></mrow></math></mathml>。对低分辨率图像块<b><i>x</i></b><sub>l</sub>, 根据图像块<b><i>x</i></b><sub>l</sub>与锚点的相关性找到最近的锚点<i>c</i><sub><i>k</i></sub>, 计算公式如下:</p>
                </div>
                <div class="p1">
                    <p id="46"><i>α</i>= (<b><i>N</i></b><sup><i>k</i>T</sup><sub>L</sub><b><i>N</i></b><sup><i>k</i></sup><sub>L</sub>+<i>λ</i><b><i>I</i></b>) <sup>-1</sup><b><i>N</i></b><sup><i>k</i>T</sup><sub>L</sub><b><i>x</i></b><sub><i>l</i></sub>      (1) </p>
                </div>
                <div class="p1">
                    <p id="47">其中, <b><i>I</i></b>是个单位矩阵, <b><i>N</i></b><sup><i>k</i>T</sup><sub>L</sub>是<b><i>N</i></b><sup><i>k</i></sup><sub>L</sub>的转置, 高分辨图像块<b><i>x</i></b><sub>h</sub>可以使用相同的系数<i>α</i>和高分辨率邻域矩阵<b><i>N</i></b><sup><i>k</i></sup><sub>H</sub>计算, 即:</p>
                </div>
                <div class="p1">
                    <p id="48"><b><i>x</i></b><sub>h</sub>=<b><i>N</i></b><sup><i>k</i></sup><sub>H</sub><i>α</i>      (2) </p>
                </div>
                <div class="p1">
                    <p id="49">则线性映射函数, 或者称为投影矩阵<b><i>P</i></b><sub><i>k</i></sub>的计算公式为:</p>
                </div>
                <div class="p1">
                    <p id="50"><b><i>P</i></b><sub><i>k</i></sub>=<b><i>N</i></b><sup><i>k</i></sup><sub>H</sub> (<b><i>N</i></b><sup><i>k</i>T</sup><sub>L</sub><b><i>N</i></b><sup><i>k</i></sup><sub>L</sub>+<i>λ</i><b><i>I</i></b>) <sup>-1</sup><b><i>N</i></b><sup><i>k</i>T</sup><sub>L</sub>      (3) </p>
                </div>
                <h3 id="51" name="51" class="anchor-tag">2 字典训练与图像重建</h3>
                <div class="p1">
                    <p id="52">本文对文献<citation id="109" type="reference">[<a class="sup">15</a>,<a class="sup">16</a>]</citation>的算法进行了改进, 提出的算法包括2个阶段:字典离线训练阶段和图像在线重建阶段。</p>
                </div>
                <h4 class="anchor-tag" id="53" name="53">2.1 字典离线训练阶段</h4>
                <div class="p1">
                    <p id="54">字典的离线训练流程如图1所示。</p>
                </div>
                <div class="area_img" id="55">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201905031_055.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 字典离线训练阶段的流程" src="Detail/GetImg?filename=images/JSJC201905031_055.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图1 字典离线训练阶段的流程</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201905031_055.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="56">从图1可以看出, 在字典训练阶段, 高分辨率原始图像<b><i>H</i></b><sub>ORG</sub>经过点扩展算子的卷积和下采样操作后得到低分辨率图像<b><i>I</i></b><sub>0</sub>。直接将<b><i>I</i></b><sub>0</sub>作为高分辨率训练图像<b><i>I</i></b><sub><i>H</i></sub>, 对<b><i>I</i></b><sub>H</sub>进行点扩展算子的卷积和下采样后得到低分辨率训练图像<b><i>I</i></b><sub>L</sub>, 并使用双三次插值放大<b><i>I</i></b><sub>L</sub>得到初始插值图像<b><i>I</i></b><sub>bic</sub>, 对<b><i>I</i></b><sub>bic</sub>使用迭代反投影 (Iterative Back Projection, IBP) <citation id="110" type="reference"><link href="21" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>加强得到<b><i>I</i></b><sub>LF</sub>图像, <b><i>I</i></b><sub>H</sub>减去<b><i>I</i></b><sub>LF</sub>得到图像<b><i>I</i></b><sub>HF</sub>。投影矩阵<b><i>P</i></b><sub><i>k</i></sub>的训练集<i>ITD</i>={<i>p</i><sup><i>k</i></sup><sub>h</sub>, <i>p</i><sup><i>k</i></sup><sub>l</sub>}<sub><i>k</i></sub>, 其中, <i>p</i><sup><i>k</i></sup><sub>h</sub>是由<b><i>I</i></b><sub>HF</sub>直接分块得到图像块的集合, <i>p</i><sup><i>k</i></sup><sub>l</sub>是使用式 (4) ～式 (7) 代表的4个高通滤波器逐步滤波处理<b><i>I</i></b><sub>LF</sub>以及使用主成分分析 (Principal Component Analysis, PCA) 降维后得到的图像块集合。</p>
                </div>
                <div class="p1">
                    <p id="57"><b><i>G</i></b>=[1, 0, -1]      (4) </p>
                </div>
                <div class="p1">
                    <p id="58"><b><i>L</i></b>=[1/2, 0, -1, 0, 1/2]      (5) </p>
                </div>
                <div class="p1">
                    <p id="59"><b><i>G</i></b><sup>T</sup>=[1, 0, -1]<sup>T</sup>      (6) </p>
                </div>
                <div class="p1">
                    <p id="60"><b><i>L</i></b><sup>T</sup>=[1/2, 0, -1, 0, 1/2]<sup>T</sup>      (7) </p>
                </div>
                <div class="p1">
                    <p id="61">利用式 (3) 可以得到投影矩阵<b><i>P</i></b><sub><i>k</i></sub>。对低分辨率图像<b><i>I</i></b><sub>0</sub>使用双三次插值放大得到初始插值图像<b><i>I</i></b><sub>bic</sub>, 使用IBP加强<b><i>I</i></b><sub>bic</sub>图像得到<b><i>I</i></b><sub>ibp</sub>图像。对于<b><i>I</i></b><sub>ibp</sub>中的每一个图像块<b><i>x</i></b><sub>l</sub>, 将<b><i>P</i></b><sub><i>k</i></sub>代入式 (2) 计算出高分辨率图像中的<b><i>x</i></b><sub>h</sub>。如图1所示, MHF特征图像<b><i>H</i></b><sub>MHF</sub>可以通过合并所有计算出的高分辨率图像块以及在相邻的重叠块之间取平均求出。<b><i>I</i></b><sub>ibp</sub>加上<b><i>H</i></b><sub>MHF</sub>可以得到高分辨率临时图像<b><i>H</i></b><sub>TMP</sub>。<b><i>H</i></b><sub>ORG</sub>减去<b><i>H</i></b><sub>TMP</sub>得到残差高频图像<b><i>H</i></b><sub>RHF</sub>。采用处理<b><i>I</i></b><sub>HF</sub>和<b><i>I</i></b><sub>LF</sub>同样的方法处理<b><i>H</i></b><sub>RHF</sub>和<b><i>H</i></b><sub>TMP</sub>可以得到外部训练集<mathml id="62"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ο</mi><mi>Τ</mi><mi>D</mi><mo>=</mo><mrow><mo stretchy="false">{</mo><mover><mstyle mathsize="140%" displaystyle="true"><mi>p</mi></mstyle><mrow><mspace width="0.25em" /><mo>∼</mo></mrow></mover><msubsup><mrow></mrow><mtext>h</mtext><mi>k</mi></msubsup><mo>, </mo><mover><mstyle mathsize="140%" displaystyle="true"><mi>p</mi></mstyle><mrow><mspace width="0.25em" /><mo>∼</mo></mrow></mover><msubsup><mrow></mrow><mtext>l</mtext><mi>k</mi></msubsup><mo stretchy="false">}</mo></mrow><msub><mrow></mrow><mi>k</mi></msub></mrow></math></mathml>。使用<i>K</i>-<i>SVD</i>字典训练的方法处理<mathml id="63"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mrow><mo>{</mo><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi>p</mi></mstyle><mrow><mspace width="0.25em" /><mo>∼</mo></mrow></mover><msubsup><mrow></mrow><mtext>l</mtext><mi>k</mi></msubsup></mrow><mo>}</mo></mrow></mrow><msub><mrow></mrow><mi>k</mi></msub></mrow></math></mathml>生成低分辨率残差字典<i>LRD</i>和稀疏表示向量<mathml id="64"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mrow><mo>{</mo><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi>q</mi></mstyle><mrow><mspace width="0.25em" /><mo>∼</mo></mrow></mover><msup><mrow></mrow><mi>k</mi></msup></mrow><mo>}</mo></mrow></mrow><msub><mrow></mrow><mi>k</mi></msub></mrow></math></mathml>, 根据<mathml id="65"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mrow><mo>{</mo><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi>p</mi></mstyle><mrow><mspace width="0.25em" /><mo>∼</mo></mrow></mover><msubsup><mrow></mrow><mi>h</mi><mi>k</mi></msubsup></mrow><mo>}</mo></mrow></mrow><msub><mrow></mrow><mi>k</mi></msub></mrow></math></mathml>和<mathml id="66"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mrow><mo>{</mo><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi>q</mi></mstyle><mrow><mspace width="0.25em" /><mo>∼</mo></mrow></mover><msup><mrow></mrow><mi>k</mi></msup></mrow><mo>}</mo></mrow></mrow><msub><mrow></mrow><mi>k</mi></msub></mrow></math></mathml>便得到高分辨率残差字典<i>HRD</i>。</p>
                </div>
                <h4 class="anchor-tag" id="67" name="67">2.2 图像在线重建阶段</h4>
                <div class="p1">
                    <p id="68">图像重建阶段如图2所示。首先对输入的一幅低分辨率图像<b><i>L</i></b><sub>INPUT</sub>, 按照图1中的处理方法, 生成<b><i>I</i></b><sub>LF</sub>图像, 然后对<b><i>I</i></b><sub>LF</sub>图像中的每一个图像块, 以及训练好的投影矩阵<b><i>P</i></b><sub><i>k</i></sub>, 根据式 (2) 生成MHF特征图像<b><i>H</i></b><sub>MHF</sub>。<b><i>I</i></b><sub>ibp</sub>图像加上<b><i>H</i></b><sub>MHF</sub>图像得到高分辨率临时图像<b><i>H</i></b><sub>TMP</sub>, 使用式 (4) ～式 (7) 代表的4个高通滤波器逐步滤波处理<b><i>H</i></b><sub>TMP</sub>, 并通过PCA降维后得到<mathml id="69"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mrow><mo>{</mo><mrow><mi>p</mi><msubsup><mrow></mrow><mtext>l</mtext><mi>k</mi></msubsup></mrow><mo>}</mo></mrow></mrow><msub><mrow></mrow><mi>k</mi></msub></mrow></math></mathml>。再使用正交匹配跟踪 (<i>Orthogonal Matching Pursuit</i>, <i>OMP</i>) 算法处理<mathml id="70"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mrow><mo>{</mo><mrow><mi>p</mi><msubsup><mrow></mrow><mtext>l</mtext><mi>k</mi></msubsup></mrow><mo>}</mo></mrow></mrow><msub><mrow></mrow><mi>k</mi></msub></mrow></math></mathml>和低分辨残差字典<i>LRD</i>得到<mathml id="71"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mrow><mo>{</mo><mrow><mi>q</mi><msup><mrow></mrow><mi>k</mi></msup></mrow><mo>}</mo></mrow></mrow><msub><mrow></mrow><mi>k</mi></msub></mrow></math></mathml>, 重建高分辨率图像块<mathml id="72"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mrow><mo>{</mo><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi>p</mi></mstyle><mrow><mspace width="0.25em" /><mo>^</mo></mrow></mover><msubsup><mrow></mrow><mtext>h</mtext><mi>k</mi></msubsup></mrow><mo>}</mo></mrow></mrow><msub><mrow></mrow><mi>k</mi></msub><mo>=</mo><mrow><mrow><mo>{</mo><mrow><mi>Η</mi><mi>R</mi><mi>D</mi><mo>⋅</mo><mi>q</mi><msup><mrow></mrow><mi>k</mi></msup></mrow><mo>}</mo></mrow></mrow><msub><mrow></mrow><mi>k</mi></msub></mrow></math></mathml>, 就可以得到高分辨率残差高频图像<b><i>H</i></b><sub>RHF</sub>。最后, <b><i>H</i></b><sub>RHF</sub>图像加上<b><i>H</i></b><sub>TMP</sub>图像便得到高分辨图像<b><i>H</i></b><sub>EST</sub>。</p>
                </div>
                <div class="area_img" id="73">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201905031_073.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 在线合成放大阶段流程" src="Detail/GetImg?filename=images/JSJC201905031_073.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图2 在线合成放大阶段流程</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201905031_073.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h3 id="74" name="74" class="anchor-tag">3 实验与结果分析</h3>
                <div class="p1">
                    <p id="75">本文用Matlab仿真软件在Windows 10的64位操作系统中运行所有算法。计算机的CPU为Intel (R) 、Xeon (R) E5-2650, 内存为64 GB。</p>
                </div>
                <div class="p1">
                    <p id="76">本文实验通过窗口大小为5×5、标准差为1的高斯滤波器输入低分辨率图像, 并对其放大2倍。在实验中, 训练阶段和重建阶段的图像块的大小设置为9×9。由于文献<citation id="111" type="reference">[<a class="sup">15</a>]</citation>在与文献<citation id="112" type="reference">[<a class="sup">14</a>]</citation>的方法进行对比实验时, 主要字典和残差字典的字典原子的个数都设置为500, 因此在本文实验中残差字典原子的个数也设置为500, 锚点个数取<i>K</i>=500, 每个锚点所对应的邻域矩阵中有<i>n</i><sub><i>c</i></sub>个最相近的原子 (<i>n</i><sub><i>c</i></sub>=256) , 正则化参数<i>λ</i>=0.01。这是因为文献<citation id="113" type="reference">[<a class="sup">16</a>]</citation>给出了<i>n</i><sub><i>c</i></sub>和<i>λ</i>选取不同数值的实验结果与分析, 并证明了不同的<i>n</i><sub><i>c</i></sub>和<i>λ</i>的取值所对应的结果在视觉上差异并不大。本文采用与文献<citation id="114" type="reference">[<a class="sup">15</a>]</citation>相同的训练图像, 如图3所示。</p>
                </div>
                <div class="area_img" id="77">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201905031_077.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 训练图像" src="Detail/GetImg?filename=images/JSJC201905031_077.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图3 训练图像</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201905031_077.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="78">表1、表2分别给出文献<citation id="115" type="reference">[<a class="sup">14</a>]</citation>方法、文献<citation id="116" type="reference">[<a class="sup">15</a>]</citation>方法、本文方法的PSNR和SSIM<citation id="117" type="reference"><link href="35" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>对比结果。</p>
                </div>
                <div class="area_img" id="79">
                    <p class="img_tit"><b>表1 超分辨率重建方法的PSNR对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note">dB</p>
                    <table id="79" border="1"><tr><td>图像</td><td>双三次插值方法</td><td>文献[14]方法</td><td>文献[15]方法</td><td>本文方法</td></tr><tr><td>Baby</td><td>33.00</td><td>36.96</td><td>37.07</td><td>38.32</td></tr><tr><td><br />Bird</td><td>31.48</td><td>37.75</td><td>38.44</td><td>40.09</td></tr><tr><td><br />Butterfly</td><td>24.00</td><td>28.32</td><td>29.02</td><td>30.59</td></tr><tr><td><br />Head</td><td>32.64</td><td>34.78</td><td>34.85</td><td>35.57</td></tr><tr><td><br />Woman</td><td>27.94</td><td>32.80</td><td>33.24</td><td>34.64</td></tr><tr><td><br />Baboon</td><td>23.53</td><td>24.79</td><td>24.89</td><td>25.57</td></tr><tr><td><br />Coastguard</td><td>26.94</td><td>29.63</td><td>29.79</td><td>30.49</td></tr><tr><td><br />Foreman</td><td>30.42</td><td>35.48</td><td>35.84</td><td>37.28</td></tr><tr><td><br />Lenna</td><td>31.04</td><td>34.91</td><td>35.15</td><td>36.31</td></tr><tr><td><br />Pepper</td><td>31.49</td><td>35.57</td><td>35.86</td><td>36.74</td></tr><tr><td><br />Zebra</td><td>26.02</td><td>31.21</td><td>31.54</td><td>33.09</td></tr><tr><td><br />平均值</td><td>28.95</td><td>32.93</td><td>33.24</td><td>34.43</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="80">
                    <p class="img_tit"><b>表2 超分辨率重建方法的SSIM对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="80" border="1"><tr><td>图像</td><td>双三次插值方法</td><td>文献[14]方法</td><td>文献[15]方法</td><td>本文方法</td></tr><tr><td>Baby</td><td>0.964</td><td>0.989</td><td>0.989</td><td>0.993</td></tr><tr><td><br />Bird</td><td>0.928</td><td>0.974</td><td>0.976</td><td>0.984</td></tr><tr><td><br />Butterfly</td><td>0.852</td><td>0.926</td><td>0.935</td><td>0.952</td></tr><tr><td><br />Head</td><td>0.812</td><td>0.857</td><td>0.860</td><td>0.882</td></tr><tr><td><br />Woman</td><td>0.899</td><td>0.950</td><td>0.953</td><td>0.966</td></tr><tr><td><br />Baboon</td><td>0.845</td><td>0.924</td><td>0.930</td><td>0.950</td></tr><tr><td><br />Coastguard</td><td>0.676</td><td>0.800</td><td>0.809</td><td>0.849</td></tr><tr><td><br />Foreman</td><td>0.917</td><td>0.952</td><td>0.955</td><td>0.966</td></tr><tr><td><br />Lenna</td><td>0.953</td><td>0.984</td><td>0.985</td><td>0.990</td></tr><tr><td><br />Pepper</td><td>0.969</td><td>0.989</td><td>0.989</td><td>0.992</td></tr><tr><td><br />Zebra</td><td>0.919</td><td>0.977</td><td>0.979</td><td>0.987</td></tr><tr><td><br />平均值</td><td>0.885</td><td>0.939</td><td>0.942</td><td>0.956</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="81">由表1和表2可知, 本文方法比文献<citation id="118" type="reference">[<a class="sup">14</a>]</citation>、文献<citation id="119" type="reference">[<a class="sup">15</a>]</citation>方法的PSNR平均值分别提高1.5 dB、1.19 dB, 其SSIM平均值分别提高0.017、0.014。表3为上述方法的运行时间。本文的运行时间是算法的在线处理时间。</p>
                </div>
                <div class="area_img" id="82">
                    <p class="img_tit"><b>表3 超分辨率重建方法的运行时间比较</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note">s</p>
                    <table id="82" border="1"><tr><td>图像</td><td>文献[14]方法</td><td>文献[15]方法</td><td>本文方法</td></tr><tr><td><br />Baby</td><td>2.95</td><td>4.18</td><td>3.68</td></tr><tr><td><br />Bird</td><td>0.91</td><td>1.35</td><td>1.12</td></tr><tr><td><br />Butterfly</td><td>0.74</td><td>1.05</td><td>0.97</td></tr><tr><td><br />Head</td><td>0.93</td><td>1.23</td><td>1.08</td></tr><tr><td><br />Woman</td><td>0.92</td><td>1.31</td><td>1.09</td></tr><tr><td><br />Baboon</td><td>2.62</td><td>3.74</td><td>3.15</td></tr><tr><td><br />Coastguard</td><td>1.19</td><td>1.66</td><td>1.30</td></tr><tr><td><br />Foreman</td><td>1.12</td><td>1.59</td><td>1.34</td></tr><tr><td><br />Lenna</td><td>2.93</td><td>4.36</td><td>3.44</td></tr><tr><td><br />Pepper</td><td>2.95</td><td>4.15</td><td>3.48</td></tr><tr><td><br />Zebra</td><td>2.54</td><td>3.68</td><td>3.00</td></tr><tr><td><br />平均值</td><td>1.80</td><td>2.57</td><td>2.15</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="83">由表3可知, 本文算法的运行时间比文献<citation id="120" type="reference">[<a class="sup">15</a>]</citation>算法短, 比文献<citation id="121" type="reference">[<a class="sup">14</a>]</citation>算法长。再结合表1和表2可以看出, 本文方法的性能指标比文献<citation id="122" type="reference">[<a class="sup">15</a>]</citation>的方法好, 且运行时间短。</p>
                </div>
                <div class="p1">
                    <p id="84">图4和图5给出了表1中不同方法的视觉效果。从中可以看出, 双三次插值放大方法产生的图像比较模糊和平滑, 视觉效果最差;文献<citation id="123" type="reference">[<a class="sup">14</a>]</citation>方法相对于双三次插值, 视觉效果提高了, 但局部纹理和边缘部分不够清晰;文献<citation id="124" type="reference">[<a class="sup">15</a>]</citation>方法相对于文献<citation id="125" type="reference">[<a class="sup">14</a>]</citation>方法有了较大程度的提高, 但在图像细节信息的恢复上仍然有缺陷, 存在一定的锯齿和伪影的现象;本文方法的效果优于上述方法, 更好地保持了图像的局部细节信息, 而且减少了块效应和伪影效应。</p>
                </div>
                <div class="area_img" id="85">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201905031_085.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 对Lena图像的超分辨率处理结果" src="Detail/GetImg?filename=images/JSJC201905031_085.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图4 对Lena图像的超分辨率处理结果</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201905031_085.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="86">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201905031_086.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 对Monarch图像的超分辨率处理结果" src="Detail/GetImg?filename=images/JSJC201905031_086.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图5 对Monarch图像的超分辨率处理结果</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201905031_086.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="87">本文方法借鉴了文献<citation id="126" type="reference">[<a class="sup">16</a>]</citation>的思想, 将低分辨率图像作为训练图像。其不同点在于, 文献<citation id="127" type="reference">[<a class="sup">16</a>]</citation>没有外部训练图像, 只是将输入的低分辨率图像作为初始的训练图像, 对初始的训练图像进行翻转和旋转得到8幅图像, 并将这8幅图像以及对这8幅图像使用比列因子0.98<sup><i>i</i></sup> (<i>i</i>=0, 1, 2, …, 20) 进行缩放得到的一系列图像作为训练集中的高分辨率图像;本文方法使用外部训练图像, 对每一幅外部训练图像, 使用模糊和下采样变成一幅低分辨率图像, 将这幅低分辨率图像作为训练库中的高分辨率训练图像。因此, 本文方法与文献<citation id="128" type="reference">[<a class="sup">16</a>]</citation>方法最大的不同之处, 就是对训练图像可以离线处理, 训练时间不占用在线放大的时间。也就是说, 对要提高分辨率的任何一幅图像, 都可以利用事先训练好的投影矩阵重建高分辨率图像, 在线处理的时间较快。</p>
                </div>
                <div class="p1">
                    <p id="88">文献<citation id="129" type="reference">[<a class="sup">16</a>]</citation>的对比算法为A+<citation id="130" type="reference"><link href="19" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>和 SRCNN<citation id="131" type="reference"><link href="23" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>算法。这些算法均使用文献<citation id="132" type="reference">[<a class="sup">13</a>]</citation>提供的标准训练库中的91幅图像作为外部训练图像。因此, 本文方法在与文献<citation id="133" type="reference">[<a class="sup">16</a>]</citation>方法作对比时, 也采用同样的图像作为外部训练图像。外部训练集的获取与投影矩阵<b><i>P</i></b><sub><i>k</i></sub>的训练过程如第2.1节所示。对比实验的参数设置采用文献<citation id="134" type="reference">[<a class="sup">16</a>]</citation>中的参数设置:高、低分辨率图像块的大小为5×5, 锚点个数<i>K</i>=5 000, <i>n</i><sub><i>c</i></sub>=2 048, 正则化参数<i>λ</i>=0.01。</p>
                </div>
                <div class="p1">
                    <p id="89">利用训练好的投影矩阵<b><i>P</i></b><sub><i>k</i></sub>, 通过式 (2) 可以求出测试集中每一幅图像的低分辨率图像块所对应的高分辨率图像块, 组合所有的高分辨率图像块便得到输出的目标高分辨率图像。</p>
                </div>
                <div class="p1">
                    <p id="90">本文方法和文献<citation id="135" type="reference">[<a class="sup">16</a>]</citation>方法的对比实验采用了国际上通用的评价超分辨率方法的测试图像集合Set5和Set14, 其结果如表4所示。</p>
                </div>
                <div class="area_img" id="91">
                                            <p class="img_tit">
                                                <b>表4 文献</b><citation id="136" type="reference">[<a class="sup">16</a>]</citation><b>方法与本文方法对比结果</b>
                                                
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201905031_09100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/JSJC201905031_09100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201905031_09100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表4 文献[16]方法与本文方法对比结果" src="Detail/GetImg?filename=images/JSJC201905031_09100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>
                <div class="p1">
                    <p id="92">从表4可以看出, 在Set5图像集中, 本文方法的PSNR、SSIM分别比文献<citation id="137" type="reference">[<a class="sup">16</a>]</citation>小0.14 dB、0.001, 但其运行时间仅为4.64 s, 远小于文献<citation id="138" type="reference">[<a class="sup">16</a>]</citation>方法的1 235 s。同时还看出, 在Set14图像集中, 本文方法的PSNR比文献<citation id="139" type="reference">[<a class="sup">16</a>]</citation>小0.13 dB, 其SSIM和文献<citation id="140" type="reference">[<a class="sup">16</a>]</citation>方法相同, 可是本文方法的运行时间为9.36 s, 同样远小于文献<citation id="141" type="reference">[<a class="sup">16</a>]</citation>方法的2 147 s。</p>
                </div>
                <div class="p1">
                    <p id="93">由此可见, 在PSNR和SSIM指标相差不大的情况下, 本文方法在运行速度指标上有明显的提高, 其运行时间低于文献<citation id="142" type="reference">[<a class="sup">16</a>]</citation>方法运行时间的1%, 因此, 本文方法有其实际的应用价值。</p>
                </div>
                <h3 id="94" name="94" class="anchor-tag">4 结束语</h3>
                <div class="p1">
                    <p id="95">本文提出一种图像超分辨率方法。在字典训练阶段, 用原始的外部高分辨率图像生成的低分辨率图像作为外部训练图像, 使用锚点回归的方法计算投影矩阵, 重建MHF特征图像以提高算法对图像细节部分重构的精度。在图像的在线重建阶段, 通过训练好的投影矩阵生成MHF特征图像。实验结果表明, 本文方法的PSNR、SSIM指标优于文献<citation id="143" type="reference">[<a class="sup">14</a>]</citation>方法和文献<citation id="144" type="reference">[<a class="sup">15</a>]</citation>方法。在PSNR、SSIM指标相近的情况下, 本文方法的运行时间远小于文献<citation id="145" type="reference">[<a class="sup">16</a>]</citation>方法。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Internal statistics of a single natural image">

                                <b>[1]</b> ZONTAK M, IRANI M.Internal statistics of a single natural image[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2011:977-984.
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Exploiting self-similarities for single frame super-resolution">

                                <b>[2]</b> HUANG Jiabin.Exploiting self-similarities for single frame super-resolution[C]//Proceedings of Asian Conference on Computer Vision.Berlin, Germany:Springer, 2011:497-510.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000004443&amp;v=MzA2MzNIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVVMbklKVjBWYmhJPU5pZklZN0s3SHRqTnI0OUZaT3NMQ0hnNm9CTVQ2VDRQUQ==&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> FREEDMAN G, FATTAL R.Image and video upscaling from local self-examples[J].ACM Transactions on Graphics, 2011, 30 (2) :474-484.
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Single image super-resolution with multiscale similarity learning">

                                <b>[4]</b> ZHANG Kaibing, GAO Xinbo, TAO Dacheng, et al.Single image super-resolution with multiscale similarity learning[J].IEEE Transactions on Neural Networks and Learning Systems, 2013, 24 (10) :1648-1659.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Super-Resolution Through Neighbor Embedding">

                                <b>[5]</b> CHANG Hong, XIONG Yimin.Super-resolution through neighbor embedding[C]//Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2004:275-282.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A non-local algorithm for image denoising">

                                <b>[6]</b> BUADES A, COLL B, MOREL J M.A non-local algorithm for image denoising[C]//Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2005:60-65.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Single-image super-resolution via linear mapping of interpolated self-examples">

                                <b>[7]</b> BEVILACQUA M, ROUMY A, GUILLEMOT C, et al.Single-image super-resolution via linear mapping of interpolated self-examples[J].IEEE Transactions on Image Processing, 2014, 23 (12) :5334-5347.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Anchored neighborhood regression for fast example-based super-resolution">

                                <b>[8]</b> TIMOFTE R, DE SMET V, VAN GOOL L.Anchored neighborhood regression for fast example-based super-resolution[C]//Proceedings of IEEE International Conference on Computer Vision.Washington D.C., USA:IEEE Press, 2013:1920-1927.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A+:adjusted anchored neighborhood regression for fast super-resolution">

                                <b>[9]</b> TIMOFTE R, DE SMET V, VAN GOOL L.A+:adjusted anchored neighborhood regression for fast super-resolution[C]//Proceedings of Asian Conference on Computer Vision.Berlin, Germany:Springer, 2014:111-126.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Seven ways to improve example-based single image super resolution">

                                <b>[10]</b> TIMOFTE R, ROTHE R, VAN GOOL L.Seven ways to improve example-based single image super resolution[C]//Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2016:1865-1873.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning a deep convolutional network for image super-resolution">

                                <b>[11]</b> DONG Chao, CHEN Chang, HE Kaiming, et al.Learning a deep convolutional network for image super-resolution[C]//Proceedings of European Conference on Computer Vision.Berlin, Germany:Springer, 2014:184-199.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_12" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201711042&amp;v=MTM5MTh2SUx6N0JiYkc0SDliTnJvOUJab1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm9GeTNtVXI=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[12]</b> 王晓晖, 盛斌, 申瑞民.基于深度学习的深度图超分辨率采样[J].计算机工程, 2017, 43 (11) :252-260.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image super-resolution via sparse representation">

                                <b>[13]</b> YANG J, WRIGHT J, HUANG T S, et al.Image super-resolution via sparse representation[J].IEEE Transactions on Image Processing, 2010, 19 (1) :2861-2873.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=On single image scaleup using sparse-representations">

                                <b>[14]</b> ZEYDE R, ELAD M, PROTTER M.On single image scale-up using sparse-representations[J].Lecture Notes in Computer Science, 2010, 6920:711-730.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image super-resolution via dual-dictionary learning and sparse representation">

                                <b>[15]</b> ZHANG Jian, ZHAO Chen, XIONG Ruiqin, et al.Image super-resolution via dual-dictionary learning and sparse representation[C]//Proceedings of IEEE International Symposium on Circuits and Systems.Washington D.C., USA:IEEE Press, 2012:1688-1691.
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Anchored neighborhood regression based single image super-resolution from self-examples">

                                <b>[16]</b> TIAN Yapeng, ZHOU Fei, YANG Wenming.Anchored neighborhood regression based Single image super-resolution from self-examples[C]//Proceedings of IEEE International Conference on Image Processing.Washington D.C., USA :IEEE Press, 2016:2827-2831.
                            </a>
                        </p>
                        <p id="35">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image quality assessment: from error visibility to structural similarity">

                                <b>[17]</b> WANG Zhou, BOVIK A, SHEIKH H.Image quality assessment:from error visibility to structural similarity[J].IEEE Transactions on Image Processing, 2004, 13 (4) :600-612.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJC201905031" />
        <input id="dpi" type="hidden" value="600" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201905031&amp;v=MTEzNzVxcUJ0R0ZyQ1VSTE9lWmVSb0Z5M21VcnZJTHo3QmJiRzRIOWpNcW85R1pZUUtESDg0dlI0VDZqNTRPM3o=&amp;uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0VNZjJWbHVQbXRmcGxDWVVPcjBMVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
