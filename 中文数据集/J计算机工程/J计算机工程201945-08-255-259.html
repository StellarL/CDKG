<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637129055172775000%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJC201908042%26RESULT%3d1%26SIGN%3dXFqaRjNUZ8FPWZD7VnCTrjPS0vs%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201908042&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201908042&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201908042&amp;v=MjM2NTVCWm9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJxRkNqblVMM0tMejdCYmJHNEg5ak1wNDk=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#39" data-title="0 概述 ">0 概述</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#43" data-title="1 基于DNN的语音增强 ">1 基于DNN的语音增强</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#71" data-title="2 网络结构 ">2 网络结构</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#74" data-title="3 实验与结果分析 ">3 实验与结果分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#75" data-title="3.1 实验配置">3.1 实验配置</a></li>
                                                <li><a href="#79" data-title="3.2 评价指标">3.2 评价指标</a></li>
                                                <li><a href="#81" data-title="3.3 不同输入形式下的语音增强性能比较">3.3 不同输入形式下的语音增强性能比较</a></li>
                                                <li><a href="#94" data-title="3.4 CNN-LSTM结构">3.4 CNN-LSTM结构</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#103" data-title="4 结束语 ">4 结束语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#73" data-title="&lt;b&gt;图1 DCNN结构&lt;/b&gt;"><b>图1 DCNN结构</b></a></li>
                                                <li><a href="#85" data-title="&lt;b&gt;图2 在对称窗下DNN和DCNN的语音增强性能&lt;/b&gt;"><b>图2 在对称窗下DNN和DCNN的语音增强性能</b></a></li>
                                                <li><a href="#89" data-title="&lt;b&gt;图3 在不同输入形式下DNN和DCNN的语音增强性能&lt;/b&gt;"><b>图3 在不同输入形式下DNN和DCNN的语音增强性能</b></a></li>
                                                <li><a href="#98" data-title="&lt;b&gt;表1 在不同输入形式下LSTM的语音增强性能&lt;/b&gt;"><b>表1 在不同输入形式下LSTM的语音增强性能</b></a></li>
                                                <li><a href="#100" data-title="&lt;b&gt;图4 CNN-LSTM结构&lt;/b&gt;"><b>图4 CNN-LSTM结构</b></a></li>
                                                <li><a href="#102" data-title="&lt;b&gt;表2 在因果窗下不同结构网络的语音增强性能&lt;/b&gt;"><b>表2 在因果窗下不同结构网络的语音增强性能</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="119">


                                    <a id="bibliography_1" title=" WANG Dongmei, HANSEN J H L.Single channel speech enhancement based on harmonic estimation combined with statistical based method to improve speech intelligibility for cochlear implant recipients[J].Journal of the Acoustical Society of America, 2017, 141 (5) :3985-3986." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Single channel speech enhancement based on harmonic estimation combined with statistical based method to improve speech intelligibility for cochlear implant recipients">
                                        <b>[1]</b>
                                         WANG Dongmei, HANSEN J H L.Single channel speech enhancement based on harmonic estimation combined with statistical based method to improve speech intelligibility for cochlear implant recipients[J].Journal of the Acoustical Society of America, 2017, 141 (5) :3985-3986.
                                    </a>
                                </li>
                                <li id="121">


                                    <a id="bibliography_2" title=" TU Jingxian, XIA Youshen.Effective Kalman filtering algorithm for distributed multichannel speech enhancement[J].Neurocomputing, 2018, 275:144-154." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES794199E20D35EB9B7A85B5AA8E53D9D1&amp;v=MTU1NzRRVW16ZDRPbnFUM1JwQWZMSGdUTTZlQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQ3BiUTM1TnhpeExxOHdxMD1OaWZPZmJTeEd0REZwdnBIWko4TUNRbEx4bQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         TU Jingxian, XIA Youshen.Effective Kalman filtering algorithm for distributed multichannel speech enhancement[J].Neurocomputing, 2018, 275:144-154.
                                    </a>
                                </li>
                                <li id="123">


                                    <a id="bibliography_3" title=" KANDAGATLA R K, SUBBAIAH P V.Speech enhancement using MMSE estimation of amplitude and complex speech spectral coefficients under phase-uncertainty[J].Speech Communication, 2018, 96:10-27." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES8352F922C8CAEFCDC7DFE488AB02D160&amp;v=MDQwMzFDcGJRMzVOeGl4THE4d3EwPU5pZk9mYnU3RzlPNnBvMUhGK044ZlFsUHZHSmc3VXNMUFh2cXBHTkhlYkRnUkx5ZkNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         KANDAGATLA R K, SUBBAIAH P V.Speech enhancement using MMSE estimation of amplitude and complex speech spectral coefficients under phase-uncertainty[J].Speech Communication, 2018, 96:10-27.
                                    </a>
                                </li>
                                <li id="125">


                                    <a id="bibliography_4" title=" 刘文举, 聂帅, 梁山, 等.基于深度学习语音分离技术的研究现状与进展[J].自动化学报, 2016, 42 (6) :819-833." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201606003&amp;v=MTIwNTJGQ2puVUwzTktDTGZZYkc0SDlmTXFZOUZaNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUnE=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                         刘文举, 聂帅, 梁山, 等.基于深度学习语音分离技术的研究现状与进展[J].自动化学报, 2016, 42 (6) :819-833.
                                    </a>
                                </li>
                                <li id="127">


                                    <a id="bibliography_5" title=" 韩伟, 张雄伟, 闵刚, 等.基于感知掩蔽深度神经网络的单通道语音增强方法[J].自动化学报, 2017, 43 (2) :248-258." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201702007&amp;v=MjczNTBScUZDam5VTDNOS0NMZlliRzRIOWJNclk5Rlk0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmU=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                         韩伟, 张雄伟, 闵刚, 等.基于感知掩蔽深度神经网络的单通道语音增强方法[J].自动化学报, 2017, 43 (2) :248-258.
                                    </a>
                                </li>
                                <li id="129">


                                    <a id="bibliography_6" title=" XU Yong, DU Jun, DAI Lirong, et al.An experimental study on speech enhancement based on deep neural networks[J].IEEE Signal Processing Letters, 2014, 21 (1) :65-68." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=An Experimental Study on Speech Enhancement Based on Deep Neural Networks">
                                        <b>[6]</b>
                                         XU Yong, DU Jun, DAI Lirong, et al.An experimental study on speech enhancement based on deep neural networks[J].IEEE Signal Processing Letters, 2014, 21 (1) :65-68.
                                    </a>
                                </li>
                                <li id="131">


                                    <a id="bibliography_7" title=" XU Yong, DU Jun, DAI Lirong, et al.A regression approach to speech enhancement based on deep neural networks[J].IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2015, 23 (1) :7-19." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM0DE9B37CC176D44F1D9767445384B3EC&amp;v=MDExNTAvc0NPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkNwYlEzNU54aXhMcTh3cTA9TmlmSVk3UE1hOWkrcklnMkYrb0lDZ2c5eTJBU25qWjZUbmptcUJjMmNiYm1Scw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                         XU Yong, DU Jun, DAI Lirong, et al.A regression approach to speech enhancement based on deep neural networks[J].IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2015, 23 (1) :7-19.
                                    </a>
                                </li>
                                <li id="133">


                                    <a id="bibliography_8" title=" CHEN Jitong, WANG Yuxuan, YOHO S E, et al.Large-scale training to increase speech intelligibility for hearing-impaired listeners in novel noises[J].Journal of the Acoustical Society of America, 2016, 139 (5) :2604-2612." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Large-scale training to increase speech intelli-gibility for hearing-impaired listeners in novel noises">
                                        <b>[8]</b>
                                         CHEN Jitong, WANG Yuxuan, YOHO S E, et al.Large-scale training to increase speech intelligibility for hearing-impaired listeners in novel noises[J].Journal of the Acoustical Society of America, 2016, 139 (5) :2604-2612.
                                    </a>
                                </li>
                                <li id="135">


                                    <a id="bibliography_9" title=" CHEN Jitong, WANG Deliang.Long short-term memory for speaker generalization in supervised speech separation[J].Journal of the Acoustical Society of America, 2017, 141 (6) :4705." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Long short-term memory for speaker generalization in supervised speech separation">
                                        <b>[9]</b>
                                         CHEN Jitong, WANG Deliang.Long short-term memory for speaker generalization in supervised speech separation[J].Journal of the Acoustical Society of America, 2017, 141 (6) :4705.
                                    </a>
                                </li>
                                <li id="137">


                                    <a id="bibliography_10" title=" ZHANG Xiaolei, WANG Deliang.A deep ensemble learning method for monaural speech separation[J].IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2016, 24 (5) :967-977." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCMD5136DBFFDF1517932D70938E39D7B42&amp;v=MTY5NDEvMHpFcDk1RFhrNHlCOFE2RXQ2U0hiaHBHYzJjTWFUTjc2ZENPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkNwYlEzNU54aXhMcTh3cTA9TmlmSVk4ZTlIOUxLMg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[10]</b>
                                         ZHANG Xiaolei, WANG Deliang.A deep ensemble learning method for monaural speech separation[J].IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2016, 24 (5) :967-977.
                                    </a>
                                </li>
                                <li id="139">


                                    <a id="bibliography_11" title=" 周飞燕, 金林鹏, 董军.卷积神经网络研究综述[J].计算机学报, 2017, 40 (6) :1229-1251." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJX201706001&amp;v=MjE3MjRkckc0SDliTXFZOUZaWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUnFGQ2puVUwzTkx6N0I=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[11]</b>
                                         周飞燕, 金林鹏, 董军.卷积神经网络研究综述[J].计算机学报, 2017, 40 (6) :1229-1251.
                                    </a>
                                </li>
                                <li id="141">


                                    <a id="bibliography_12" title=" 袁文浩, 孙文珠, 夏斌, 等.利用深度卷积神经网络提高未知噪声下的语音增强性能[J].自动化学报, 2018, 44 (4) :751-759." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201804016&amp;v=MTM5NjZPZVplUnFGQ2puVUwzTktDTGZZYkc0SDluTXE0OUVZb1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkw=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[12]</b>
                                         袁文浩, 孙文珠, 夏斌, 等.利用深度卷积神经网络提高未知噪声下的语音增强性能[J].自动化学报, 2018, 44 (4) :751-759.
                                    </a>
                                </li>
                                <li id="143">


                                    <a id="bibliography_13" title=" YU Dong, EVERSOLE A, SELTZER M, et al.An introduction to computational networks and the computational network toolkit:MSR-TR-2014-112[R].Redmond, USA:Microsoft Research, 2014." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=An Introduction to Computational Networks and the Computational Network Toolkit">
                                        <b>[13]</b>
                                         YU Dong, EVERSOLE A, SELTZER M, et al.An introduction to computational networks and the computational network toolkit:MSR-TR-2014-112[R].Redmond, USA:Microsoft Research, 2014.
                                    </a>
                                </li>
                                <li id="145">


                                    <a id="bibliography_14" title=" GAROFOLO J S, LAMEL L F, FISHER W M, et al.TIMIT acoustic-phonetic continuous speech corpus[EB/OL].[2018-05-30].https://catalog.ldc.upenn.edu/LDC93S1." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=TIMIT acoustic-phonetic continuous speech corpus">
                                        <b>[14]</b>
                                         GAROFOLO J S, LAMEL L F, FISHER W M, et al.TIMIT acoustic-phonetic continuous speech corpus[EB/OL].[2018-05-30].https://catalog.ldc.upenn.edu/LDC93S1.
                                    </a>
                                </li>
                                <li id="147">


                                    <a id="bibliography_15" title=" HU Guoning.100 nonspeech environmental sounds, 2004[EB/OL].[2018-05-30].http://web.cse.ohio-state.edu/pnl/corpus/HuNonspeech/HuCorpus.html." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=&amp;quot;100 nonspeech environmental sounds,2004&amp;quot;">
                                        <b>[15]</b>
                                         HU Guoning.100 nonspeech environmental sounds, 2004[EB/OL].[2018-05-30].http://web.cse.ohio-state.edu/pnl/corpus/HuNonspeech/HuCorpus.html.
                                    </a>
                                </li>
                                <li id="149">


                                    <a id="bibliography_16" title=" VARGA A, STEENEKEN H J M.Assessment for automatic speech recognition:II.NOISEX-92:a database and an experiment to study the effect of additive noise on speech recognition systems[J].Speech Communication, 1993, 12 (3) :247-251." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Assessment for automatic speech recognition: II. NOISEX-92: A database and an experiment to study the effect of additive noise on speech recognition systems">
                                        <b>[16]</b>
                                         VARGA A, STEENEKEN H J M.Assessment for automatic speech recognition:II.NOISEX-92:a database and an experiment to study the effect of additive noise on speech recognition systems[J].Speech Communication, 1993, 12 (3) :247-251.
                                    </a>
                                </li>
                                <li id="151">


                                    <a id="bibliography_17" title=" RIX A W, BEERENDS J G, HOLLIER M P, et al.Perceptual evaluation of speech quality (PESQ) -a new method for speech quality assessment of telephone networks and codes[C]//Proceedings of IEEE International Conference on Acoustics, Speech, and Signal Processing.Washington D.C., USA:IEEE Press, 2001:749-752." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Perceptual evaluation of speech quality (PESQ)-a new method for speech quality assessment of telephone networks and codecs">
                                        <b>[17]</b>
                                         RIX A W, BEERENDS J G, HOLLIER M P, et al.Perceptual evaluation of speech quality (PESQ) -a new method for speech quality assessment of telephone networks and codes[C]//Proceedings of IEEE International Conference on Acoustics, Speech, and Signal Processing.Washington D.C., USA:IEEE Press, 2001:749-752.
                                    </a>
                                </li>
                                <li id="153">


                                    <a id="bibliography_18" title=" TAAL C H, HENDRIKS R C, HEUSDENS R, et al.An algorithm for intelligibility prediction of time-frequency weighted noisy speech[J].IEEE Transactions on Audio, Speech, and Language Processing, 2011, 19 (7) :2125-2136." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=An Algorithm for Intelligibility Prediction of Time-Frequency Weighted Noisy Speech">
                                        <b>[18]</b>
                                         TAAL C H, HENDRIKS R C, HEUSDENS R, et al.An algorithm for intelligibility prediction of time-frequency weighted noisy speech[J].IEEE Transactions on Audio, Speech, and Language Processing, 2011, 19 (7) :2125-2136.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">

    <div class="head-tag">   
            <p>
               <b> 网络首发时间: 2018-10-26 09:52:09</b>
            </p>     
    </div>


        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJC" target="_blank">计算机工程</a>
                2019,45(08),255-259 DOI:10.19678/j.issn.1000-3428.0051780            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于深度神经网络的因果形式语音增强模型</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%A2%81%E6%96%87%E6%B5%A9&amp;code=30853524&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">袁文浩</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%A2%81%E6%98%A5%E7%87%95&amp;code=32377127&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">梁春燕</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%A4%8F%E6%96%8C&amp;code=11459401&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">夏斌</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%B1%B1%E4%B8%9C%E7%90%86%E5%B7%A5%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF%E5%AD%A6%E9%99%A2&amp;code=0245633&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">山东理工大学计算机科学与技术学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>传统的基于深度神经网络 (DNN) 的语音增强方法由于采用非因果形式的输入, 在处理过程中具有固定延时, 不适用于实时性要求较高的场合。针对这一问题, 从网络结构角度展开研究, 通过实验对不同网络结构在不同输入形式下的语音增强性能进行对比, 寻找适用于因果形式输入的网络结构, 在此基础上, 结合卷积神经网络和长短期记忆网络建立一个能充分利用先前帧信息的因果语音增强模型。实验结果表明, 该模型在提高基于DNN的语音增强方法实时性的同时, 保证了语音增强性能, 其PESQ与STOI得分分别为2.25和0.76。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%AF%AD%E9%9F%B3%E5%A2%9E%E5%BC%BA&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">语音增强;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%9B%A0%E6%9E%9C%E5%BD%A2%E5%BC%8F%E8%BE%93%E5%85%A5&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">因果形式输入;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%BB%B6%E6%97%B6&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">延时;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">卷积神经网络;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    袁文浩 (1985—) , 男, 讲师、博士, 主研方向为语音信号处理;;
                                </span>
                                <span>
                                    梁春燕, 讲师、博士;;
                                </span>
                                <span>
                                    夏斌, 副教授、博士。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-06-11</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金 (61701286, 11704229);</span>
                                <span>山东省自然科学基金 (ZR2015FL003, ZR2017MF047, ZR2017LA011);</span>
                    </p>
            </div>
                    <h1><b>Causal Speech Enhancement Model Based on Deep Neural Network</b></h1>
                    <h2>
                    <span>YUAN Wenhao</span>
                    <span>LIANG Chunyan</span>
                    <span>XIA Bin</span>
            </h2>
                    <h2>
                    <span>School of Computer Science and Technology, Shandong University of Technology</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>The traditional speech enhancement method based on Deep Neural Network (DNN) has a fixed delay in processing due to its non-causal input, which is unsuitable for the real-time applications.To solve this problem, studying from the perspective of network structures, comparing the speech enhancement performance of different network structures under different input formats through experiments, the network structure suitable for the causal input is found in this paper.On this basis, by combining Convolutional Neural Network (CNN) and Long Short-Term Memory (LSTM) , a causal speech enhancement model that can fully utilize the information of previous frames is established.Experimental results show that the proposed model is able to improve the real-time performance of the DNN-based speech enhancement method while ensuring the speech enhancement performance, whose PESQ and STOI scores are 2.25 and 0.76.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=speech%20enhancement&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">speech enhancement;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=causal%20input&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">causal input;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=delay&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">delay;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Deep%20Neural%20Network%20(DNN)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Deep Neural Network (DNN) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Convolutional%20Neural%20Network%20(CNN)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Convolutional Neural Network (CNN) ;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-06-11</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="39" name="39" class="anchor-tag">0 概述</h3>
                <div class="p1">
                    <p id="40">语音增强是噪声环境下语音通信和语音信号处理的必要环节<citation id="155" type="reference"><link href="119" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>。传统的基于统计的语音增强方法在进行语音和噪声估计时, 一般采用时域平滑来计算相邻帧之间的相关性, 并且该平滑处理仅利用当前帧 (第<i>n</i>帧) 及其之前的帧 (第<i>n</i>-1帧) , 尽可能地减少由语音增强造成的延时, 保证语音通信的实时性<citation id="156" type="reference"><link href="121" rel="bibliography" /><link href="123" rel="bibliography" /><sup>[<a class="sup">2</a>,<a class="sup">3</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="41">近年来, 随着深度学习技术的发展, 受深度神经网络 (Deep Neural Network, DNN) 在语音识别中的成功应用启发, 基于DNN的语音增强方法受到人们的广泛关注, 其在非平稳噪声处理中相比传统方法具有明显的优势<citation id="158" type="reference"><link href="125" rel="bibliography" /><link href="127" rel="bibliography" /><sup>[<a class="sup">4</a>,<a class="sup">5</a>]</sup></citation>。在基于DNN的语音增强方法中, 为了充分利用含噪语音相邻帧之间的相关性, 一般采用相邻多帧的信号作为输入。当对第<i>n</i>帧进行语音增强时, 会同时用到第<i>n</i>帧之前的<i>N</i>帧先前信号和第<i>n</i>帧之后的<i>N</i>帧后续信号, 即对含噪语音段添加一个以第<i>n</i>帧为中心的对称窗, 而2<i>N</i>+1就是输入窗长。例如, 文献<citation id="159" type="reference">[<a class="sup">6</a>,<a class="sup">7</a>]</citation>使用连续11帧的对数功率谱 (Logarithmic Power Spectra, LPS) 作为DNN的输入来建立语音增强模型, 并分析不同输入窗长对语音增强性能的影响。文献<citation id="160" type="reference">[<a class="sup">8</a>,<a class="sup">9</a>]</citation>分别采用23帧的压缩cochleagram特征作为DNN和长短期记忆网络 (Long Short-Term Memory, LSTM) 的输入, 通过估计含噪语音的理想比率掩模 (Ideal Ratio Mask, IRM) 来进行语音增强。为了更加充分地利用网络输入中的上下文信息, 文献<citation id="157" type="reference">[<a class="sup">10</a>]</citation>通过集成3个DNN来建立语音增强模型, 其中每个DNN采用不同窗长的短时傅里叶变换 (Short-Time Fourier Transform, STFT) 特征作为输入。</p>
                </div>
                <div class="p1">
                    <p id="42">无论网络的输入是何种类型的特征, 由于这种非因果形式的输入利用了第<i>n</i>帧之后的<i>N</i>帧后续信号, 在语音增强中都会带来一个固定的延时。消除该固定延时的最直接方法就是在输入窗中不再包括第<i>n</i>帧的后续帧, 即采用仅包含第<i>n</i>帧及其先前帧的因果窗。然而, 因果窗的采用会造成输入信息的减少, 势必对语音增强性能造成影响。因此, 研究如何在消除该固定延时的同时, 尽可能地保证语音增强性能, 对于有实时性要求的语音增强系统具有重要意义。本文通过讨论不同网络结构下输入窗的大小和形式对语音增强性能的影响, 寻找当采用无后续帧的因果窗时具有更好语音增强性能的网络结构, 并引入LSTM建立能够利用更多先前帧信息的因果语音增强模型, 以在消除由输入窗后续帧引起的固定延迟的同时, 保证更好的语音增强性能。</p>
                </div>
                <h3 id="43" name="43" class="anchor-tag">1 基于DNN的语音增强</h3>
                <div class="p1">
                    <p id="44">假设含噪语音<i>y</i>由纯净语音<i>s</i>和加性噪声<i>d</i>组成, 即:</p>
                </div>
                <div class="p1">
                    <p id="45"><i>y</i>=<i>s</i>+<i>d</i>      (1) </p>
                </div>
                <div class="p1">
                    <p id="46">语音增强可以描述为:在已知含噪语音<i>y</i>的条件下, 对纯净语音<i>s</i>进行估计, 得到估计值<mathml id="47"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi>s</mi></mstyle><mrow><mspace width="0.25em" /><mo>^</mo></mrow></mover></mrow></math></mathml>。在语音增强时, 一般通过STFT将信号转换到时频域进行分析处理。假设<i>y</i>、<i>s</i>和<mathml id="48"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi>s</mi></mstyle><mrow><mspace width="0.25em" /><mo>^</mo></mrow></mover></mrow></math></mathml>在第<i>n</i>帧的STFT形式分别为<i>Y</i><sub><i>n</i>, <i>k</i></sub>exp (j<i>α</i><sub><i>n</i>, <i>k</i></sub>) 、<i>S</i><sub><i>n</i>, <i>k</i></sub>exp (j<i>φ</i><sub><i>n</i>, <i>k</i></sub>) 和<mathml id="49"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi>S</mi></mstyle><mrow><mspace width="0.25em" /><mo>^</mo></mrow></mover><msub><mrow></mrow><mrow><mi>n</mi><mo>, </mo><mi>k</mi></mrow></msub><mrow><mi>exp</mi></mrow><mo stretchy="false"> (</mo><mtext>j</mtext><mover><mstyle mathsize="140%" displaystyle="true"><mi>φ</mi></mstyle><mrow><mspace width="0.25em" /><mo>^</mo></mrow></mover><msub><mrow></mrow><mrow><mi>n</mi><mo>, </mo><mi>k</mi></mrow></msub><mo stretchy="false">) </mo></mrow></math></mathml>, 其中, <i>k</i>=1, 2, …, <i>K</i>是频带序号, 忽略相位信息, 第<i>n</i>帧的语音增强任务就是最小化误差函数:</p>
                </div>
                <div class="p1">
                    <p id="50" class="code-formula">
                        <mathml id="50"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>E</mi><mi>r</mi><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>Κ</mi></munderover><mrow><mrow><mo> (</mo><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi>S</mi></mstyle><mrow><mspace width="0.25em" /><mo>^</mo></mrow></mover><msub><mrow></mrow><mrow><mi>n</mi><mo>, </mo><mi>k</mi></mrow></msub><mo>-</mo><mi>S</mi><msub><mrow></mrow><mrow><mi>n</mi><mo>, </mo><mi>k</mi></mrow></msub></mrow><mo>) </mo></mrow></mrow></mstyle><msup><mrow></mrow><mn>2</mn></msup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="51">令<b><i>S</i></b><sub><i>n</i></sub>和<mathml id="52"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi mathvariant="bold-italic">S</mi></mstyle><mrow><mspace width="0.25em" /><mo>^</mo></mrow></mover><msub><mrow></mrow><mi>n</mi></msub></mrow></math></mathml>分别表示纯净语音第n帧的幅度谱向量及其估计值, 式 (2) 可以改写为:</p>
                </div>
                <div class="p1">
                    <p id="53" class="code-formula">
                        <mathml id="53"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>E</mi><mi>r</mi><mo>=</mo><mrow><mo stretchy="false">∥</mo><mover><mstyle mathsize="140%" displaystyle="true"><mi mathvariant="bold-italic">S</mi></mstyle><mrow><mspace width="0.25em" /><mo>^</mo></mrow></mover><msub><mrow></mrow><mi>n</mi></msub><mo>-</mo><mi mathvariant="bold-italic">S</mi><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false">∥</mo></mrow><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="54">在基于<i>DNN</i>的语音增强方法中, 一般通过一个高度复杂的非线性函数f<sub>θ</sub>计算<mathml id="55"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi mathvariant="bold-italic">S</mi></mstyle><mrow><mspace width="0.25em" /><mo>^</mo></mrow></mover><msub><mrow></mrow><mi>n</mi></msub></mrow></math></mathml>, 其中<i>θ</i>是通过训练得到的网络参数集合, 式 (3) 可以改写为:</p>
                </div>
                <div class="p1">
                    <p id="56" class="code-formula">
                        <mathml id="56"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>E</mi><mi>r</mi><mo>=</mo><mrow><mo stretchy="false">∥</mo><mi>f</mi><msub><mrow></mrow><mi>θ</mi></msub><mrow><mo> (</mo><mrow><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>n</mi></msub></mrow><mo>) </mo></mrow><mo>-</mo><mi mathvariant="bold-italic">S</mi><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false">∥</mo></mrow><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="57">则基于DNN的语音增强就是要计算目标输出:</p>
                </div>
                <div class="p1">
                    <p id="58" class="code-formula">
                        <mathml id="58"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi mathvariant="bold-italic">S</mi></mstyle><mrow><mspace width="0.25em" /><mo>^</mo></mrow></mover><msub><mrow></mrow><mi>n</mi></msub><mo>=</mo><mi>f</mi><msub><mrow></mrow><mi>θ</mi></msub><mrow><mo> (</mo><mrow><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>n</mi></msub></mrow><mo>) </mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>5</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="59">其中, <b><i>X</i></b><sub><i>n</i></sub>为对第<i>n</i>帧进行语音增强时的网络输入。当网络的输入形式为对称窗, 且输入窗长为2<i>N</i>+1时, 网络输入为:</p>
                </div>
                <div class="p1">
                    <p id="60"><b><i>X</i></b><sub><i>n</i></sub>=[<i>Y</i><sub><i>n</i>-<i>N</i></sub>, <i>Y</i><sub><i>n</i>-<i>N</i>+1</sub>, …, <i>Y</i><sub><i>n</i></sub>, …, <i>Y</i><sub><i>n</i>+<i>N</i>-1</sub>, <i>Y</i><sub><i>n</i>+<i>N</i></sub>]      (6) </p>
                </div>
                <div class="p1">
                    <p id="61">而当网络的输入形式为因果窗, 输入窗长为<i>N</i>+1时, 网络输入为:</p>
                </div>
                <div class="p1">
                    <p id="62"><b><i>X</i></b><sub><i>n</i></sub>=[<i>Y</i><sub><i>n</i>-<i>N</i></sub>, <i>Y</i><sub><i>n</i>-<i>N</i>+1</sub>, …, <i>Y</i><sub><i>n</i></sub>]      (7) </p>
                </div>
                <div class="p1">
                    <p id="63">在基于DNN的语音增强方法中, 一般对含噪语音和纯净语音的功率谱进行对数运算, 得到LPS作为网络的输入和输出:</p>
                </div>
                <div class="p1">
                    <p id="64" class="code-formula">
                        <mathml id="64"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi mathvariant="bold-italic">Ζ</mi><msub><mrow></mrow><mi>n</mi></msub><mo>=</mo><mrow><mi>ln</mi></mrow><mrow><mo> (</mo><mrow><mrow><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">X</mi><msub><mrow></mrow><mi>n</mi></msub></mrow><mo>|</mo></mrow></mrow><msup><mrow></mrow><mn>2</mn></msup></mrow><mo>) </mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>8</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi mathvariant="bold-italic">Τ</mi><msub><mrow></mrow><mi>n</mi></msub><mo>=</mo><mrow><mi>ln</mi></mrow><mrow><mo> (</mo><mrow><mrow><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">S</mi><msub><mrow></mrow><mi>n</mi></msub></mrow><mo>|</mo></mrow></mrow><msup><mrow></mrow><mn>2</mn></msup></mrow><mo>) </mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>9</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="65">采用大小为<i>M</i>的mini-batch进行网络训练, 以平均误差 (Mean Square Error, MSE) 计算损失函数:</p>
                </div>
                <div class="p1">
                    <p id="66" class="code-formula">
                        <mathml id="66"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><mo stretchy="false"> (</mo><mi>θ</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mn>1</mn><mi>Μ</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>Μ</mi></munderover><mrow><mo stretchy="false">∥</mo><mi>f</mi><msub><mrow></mrow><mi>θ</mi></msub><mrow><mo> (</mo><mrow><mi mathvariant="bold-italic">Ζ</mi><msub><mrow></mrow><mi>n</mi></msub></mrow><mo>) </mo></mrow><mo>-</mo><mi mathvariant="bold-italic">Τ</mi><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false">∥</mo></mrow></mstyle><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>0</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="67">网络训练完成后, 在进行语音增强时, 对第<i>n</i>帧的纯净语音<b><i>s</i></b><sub><i>n</i></sub>, 使用网络输出的估计值<mathml id="68"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi mathvariant="bold-italic">Τ</mi></mstyle><mrow><mspace width="0.25em" /><mo>^</mo></mrow></mover><msub><mrow></mrow><mi>n</mi></msub></mrow></math></mathml>与含噪语音第n帧的相位谱向量<i>α</i><sub><i>n</i></sub>进行时域信号的重构, 增强后的语音信号为:</p>
                </div>
                <div class="p1">
                    <p id="69" class="code-formula">
                        <mathml id="69"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mover><mstyle mathsize="140%" displaystyle="true"><mi mathvariant="bold-italic">s</mi></mstyle><mrow><mspace width="0.25em" /><mo>^</mo></mrow></mover><msub><mrow></mrow><mi>n</mi></msub><mo>=</mo><mi>Ι</mi><mi>S</mi><mi>Τ</mi><mi>F</mi><mi>Τ</mi><mo stretchy="false"> (</mo><mover><mstyle mathsize="140%" displaystyle="true"><mi mathvariant="bold-italic">S</mi></mstyle><mrow><mspace width="0.25em" /><mo>^</mo></mrow></mover><msub><mrow></mrow><mi>n</mi></msub><mo>⋅</mo><mrow><mi>exp</mi></mrow><mo stretchy="false"> (</mo><mtext>j</mtext><mover><mstyle mathsize="140%" displaystyle="true"><mi>φ</mi></mstyle><mrow><mspace width="0.25em" /><mo>^</mo></mrow></mover><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo>=</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mi>Ι</mi><mi>S</mi><mi>Τ</mi><mi>F</mi><mi>Τ</mi><mo stretchy="false"> (</mo><mrow><mi>exp</mi></mrow><mo stretchy="false"> (</mo><mover><mstyle mathsize="140%" displaystyle="true"><mi mathvariant="bold-italic">Τ</mi></mstyle><mrow><mspace width="0.25em" /><mo>^</mo></mrow></mover><msub><mrow></mrow><mi>n</mi></msub><mo>/</mo><mn>2</mn><mo stretchy="false">) </mo><mo>⋅</mo><mrow><mi>exp</mi></mrow><mo stretchy="false"> (</mo><mtext>j</mtext><mi mathvariant="bold-italic">α</mi><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>1</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="70">其中, <i>ISTFT</i>为STFT的逆变换。</p>
                </div>
                <h3 id="71" name="71" class="anchor-tag">2 网络结构</h3>
                <div class="p1">
                    <p id="72">用于语音增强的常见网络结构主要有DNN和深度卷积神经网络 (Deep Convolutional Neural Network, DCNN) , 下文针对输入窗大小和形式的研究将主要以这2种网络展开。本文所采用的DNN结构与文献<citation id="161" type="reference">[<a class="sup">8</a>]</citation>类似, 隐层个数为5, 每个隐层的节点个数是1 024, 激活函数采用指数线性单元 (Exponential Linear Unit, ELU) 。为了防止过拟合, 在每个隐层后都增加一个Dropout层, 丢弃率为0.5。本文所采用的DCNN包含3个卷积层和2个全连接层, 其中, 卷积层的步长为1, 池化层的步长为2, 每个全连接层后紧接一个丢弃率为0.5的Dropout层, 具体的网络结构如图1所示。文献<citation id="162" type="reference">[<a class="sup">11</a>,<a class="sup">12</a>]</citation>已证明该网络结构比DNN具有更好的语音增强性能。为了训练过程的稳定, 2种网络的输入和输出均进行了均值方差归一化 (Mean and Variance Normalization, MVN) 处理, 但是由于网络结构的不同, DNN的输入需要转换为向量形式, 而DCNN的输入则是矩阵形式, 这样可以更好地保持输入窗中的时频结构信息。</p>
                </div>
                <div class="area_img" id="73">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201908042_073.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 DCNN结构" src="Detail/GetImg?filename=images/JSJC201908042_073.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图1 DCNN结构</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201908042_073.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h3 id="74" name="74" class="anchor-tag">3 实验与结果分析</h3>
                <h4 class="anchor-tag" id="75" name="75">3.1 实验配置</h4>
                <div class="p1">
                    <p id="76">训练集所用的纯净语音数据是TIMIT语音数据库的Traning集的4 620段语音<citation id="163" type="reference"><link href="143" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>, 噪声数据是俄亥俄州立大学Perception and Neurodynamics实验室的100类噪声<citation id="164" type="reference"><link href="145" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>。纯净语音和噪声信号的采样频率均转换为8 kHz, 信号STFT采用的帧长为32 ms (256点) , 帧移为16 ms (128点) , 相应的LPS特征的维度为129。将随机选取的纯净语音段与随机选取的噪声片段按照-10 dB、-5 dB、0 dB、5 dB和10 dB 5种全局信噪比中的随机一种进行混合, 得到含噪语音段, 采用上述方法为训练集合成50 000段含噪语音 (约40 h) 。</p>
                </div>
                <div class="p1">
                    <p id="77">测试集采用的纯净语音数据是TIMIT语音数据库的Core test集的192段语音, 噪声数据是Noisex92噪声库的与训练集噪声完全不同的8类未知噪声, 分别是Factory1、Factory2、Babble、Buccaneer1、Buccaneer2、Destroyer engine、Destroyer operations、HF channel噪声<citation id="165" type="reference"><link href="147" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>。将纯净语音与8类噪声的随机片段分别按照-7 dB、0 dB和7 dB 3种全局信噪比进行混合, 为测试集合成4 608 (192×3×8) 段含噪语音。</p>
                </div>
                <div class="p1">
                    <p id="78">本文所有网络的训练均采用微软的Cognitive Toolkit进行<citation id="166" type="reference"><link href="149" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>, mini-batch的大小均设为1 024, 冲量因子均设为0.9, 迭代次数均为20。</p>
                </div>
                <h4 class="anchor-tag" id="79" name="79">3.2 评价指标</h4>
                <div class="p1">
                    <p id="80">本文采用主观语音质量评估 (Perceptual Evaluation of Speech Quality, PESQ) 和短时客观可懂度 (Short Time Objective Intelligibility, STOI) 2种指标对不同模型的语音增强性能进行评价, 其中, PESQ用于评价增强语音的语音质量<citation id="167" type="reference"><link href="151" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>, 其得分范围为-0.5～4.5, 得分越高代表增强语音的语音质量越高;STOI则用于评价增强语音的可懂度<citation id="168" type="reference"><link href="153" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>, 其得分范围为0～1, 得分越高表示增强语音的可懂度越高。</p>
                </div>
                <h4 class="anchor-tag" id="81" name="81">3.3 不同输入形式下的语音增强性能比较</h4>
                <div class="p1">
                    <p id="82">下文通过语音增强实验对在不同大小和形式的输入窗下, 2种网络的语音增强性能进行比较分析。</p>
                </div>
                <h4 class="anchor-tag" id="83" name="83">3.3.1 对称窗下的语音增强性能</h4>
                <div class="p1">
                    <p id="84">首先分析当网络的输入形式为对称窗时, 不同窗长对DNN和DCNN的语音增强性能的影响。图2给出了输入窗长为11帧、13帧、15帧、17帧和19帧时, 采用2种网络建立的语音增强模型的平均PESQ和平均STOI得分。</p>
                </div>
                <div class="area_img" id="85">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201908042_085.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 在对称窗下DNN和DCNN的语音增强性能" src="Detail/GetImg?filename=images/JSJC201908042_085.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图2 在对称窗下DNN和DCNN的语音增强性能</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201908042_085.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="86">由图2可见, DNN的PESQ和STOI 2个指标并没有随着输入窗长的增大而一直提高, 甚至当输入窗长大于15帧时, 2种指标还出现了一定程度的下降, 这表明DNN的语音增强性能并不会随着输入窗长的增大一直提高。而DCNN的语音增强性能随着输入窗长的增大一直提高。另外, 在2种指标下, DCNN都表现出了明显优于DNN的语音增强性能。DCNN相比DNN在语音增强中的优越性, 得益于其矩阵形式的输入能够更好地反映含噪语音的时频结构信息, 以及DCNN本身具有的局部感知特性使其能够更好地利用相邻帧间的时频相关性。需要注意的是, 虽然在采用DCNN建立语音增强模型时, 可以使用较大的窗长来提高语音增强性能, 但是窗长越大, 由对称窗带来的固定延时会越长, 模型也越复杂, 因此需要平衡三者之间的关系。</p>
                </div>
                <h4 class="anchor-tag" id="87" name="87">3.3.2 因果窗下的语音增强性能</h4>
                <div class="p1">
                    <p id="88">当输入形式为因果窗时, 虽然再大的输入窗长都不会引入固定延迟, 但是越大的输入窗长会使模型的复杂度越高, 因此对于因果窗下DNN和DCNN语音增强性能的讨论, 同样针对11帧、13帧、15帧、17帧和19帧5种窗长展开。图3分别给出5种窗长的因果窗下2种网络建立的语音增强模型的平均PESQ和平均STOI得分, 并给出了相同窗长的对称窗下2种网络的语音增强性能作为对比。</p>
                </div>
                <div class="area_img" id="89">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201908042_089.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 在不同输入形式下DNN和DCNN的语音增强性能" src="Detail/GetImg?filename=images/JSJC201908042_089.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图3 在不同输入形式下DNN和DCNN的语音增强性能</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201908042_089.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="90">由图3可以得到以下结论:</p>
                </div>
                <div class="p1">
                    <p id="91">1) 当采用因果窗时, 输入窗长对语音增强性能的影响较小。不论是DNN还是DCNN, 5种不同窗长下的平均PESQ和平均STOI得分都十分相近。这是因为, 在对当前帧进行语音增强时, 因果窗中与当前帧越近的帧, 与当前帧具有越强的相关性, 在语音增强中起到越大的作用, 而较远的帧不仅与当前帧的相关性较小, 且其信息无法被DNN和DCNN充分利用, 使得这部分先前帧对语音增强结果的影响较小。</p>
                </div>
                <div class="p1">
                    <p id="92">2) 输入窗形式对DCNN的影响要小于DNN。在相同网络结构下, 由于输入信息的减少, 因果窗的语音增强性能要差于对称窗。特别是DNN, 采用因果窗时的2种指标相比采用对称窗时都有了较大幅度的下降, 而DCNN在不同输入窗形式下2种指标的差距则相对较小。</p>
                </div>
                <div class="p1">
                    <p id="93">3) 在因果窗下, DCNN仍然表现出了明显优于DNN的语音增强性能, 甚至要优于采用对称窗的DNN。</p>
                </div>
                <h4 class="anchor-tag" id="94" name="94">3.4 CNN-LSTM结构</h4>
                <div class="p1">
                    <p id="95">综合上述分析可知, 得益于DCNN的矩阵输入形式和局部感知特性, DCNN相比DNN能够更好地利用输入窗中的含噪语音帧在时频域的局部相关性, 在对称窗和因果窗2种输入形式下都具有更好的语音增强性能, 但是其在因果窗下仍然不能充分利用所有先前帧的信息。实际上, 在处理有长期依赖的时间序列问题时, 具有长时记忆特性的LSTM是一种有效的网络结构, 因此为了更好地利用含噪语音的先前帧信息, 本文将卷积神经网络 (Convolutional Neural Network, CNN) 与LSTM相结合, 采用CNN来提取局部特征, 并使用LSTM将不同时间段的局部特征进行关联。</p>
                </div>
                <div class="p1">
                    <p id="96">首先对LSTM网络在对称窗和因果窗下的语音增强性能进行对比, 本文所用的LSTM同样包含5个隐层, Cell维度为1 024, 每个隐层后同样紧接一个丢弃率为0.5的Dropout层。为了便于比较, 本节所有网络的输入窗长均设为15帧。</p>
                </div>
                <div class="p1">
                    <p id="97">表1给出采用对称窗和因果窗时LSTM的平均PESQ和平均STOI得分。由表1可见, 与DNN一样, LSTM的语音增强性能在采用因果窗时有明显的下降, 表明单纯的LSTM并不适用于因果输入形式。</p>
                </div>
                <div class="area_img" id="98">
                    <p class="img_tit"><b>表1 在不同输入形式下LSTM的语音增强性能</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="98" border="1"><tr><td><br />输入形式</td><td>PESQ得分</td><td>STOI得分</td></tr><tr><td><br />对称窗</td><td>2.19</td><td>0.75</td></tr><tr><td><br />因果窗</td><td>2.02</td><td>0.71</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="99">本文将CNN和LSTM进行结合, 得到如图4所示的CNN-LSTM结构, 其中CNN代表与图1DCNN中结构相同的3个卷积层、池化层和激活函数层。</p>
                </div>
                <div class="area_img" id="100">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201908042_100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 CNN-LSTM结构" src="Detail/GetImg?filename=images/JSJC201908042_100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图4 CNN-LSTM结构</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201908042_100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="101">表2给出了CNN-LSTM在输入为因果窗时的平均PESQ和平均STOI得分, 并给出了DNN、DCNN和LSTM的结果作为对比。由表2可以看出, CNN-LSTM在2种指标下都取得了最佳结果, 表明该语音增强模型充分结合了CNN和LSTM的各自特点, 解决了单纯LSTM在因果窗下的性能下降问题, 适用于对实时性要求较高的因果输入形式。</p>
                </div>
                <div class="area_img" id="102">
                    <p class="img_tit"><b>表2 在因果窗下不同结构网络的语音增强性能</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="102" border="1"><tr><td><br />网络结构</td><td>PESQ得分</td><td>STOI得分</td></tr><tr><td><br />DNN</td><td>2.06</td><td>0.72</td></tr><tr><td><br />DCNN</td><td>2.21</td><td>0.75</td></tr><tr><td><br />LSTM</td><td>2.02</td><td>0.71</td></tr><tr><td><br />CNN-LSTM</td><td>2.25</td><td>0.76</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h3 id="103" name="103" class="anchor-tag">4 结束语</h3>
                <div class="p1">
                    <p id="104">本文研究基于DNN的语音增强方法中由非因果输入形式带来的固定延时, 通过分析不同大小和形式的输入窗对DNN和DCNN 2种网络的语音增强性能的影响, 得到更加适用于无固定延迟的因果输入形式的网络结构, 并结合CNN和LSTM提出一种能够利用更多先前帧信息的因果语音增强模型。实验结果表明, 该模型能够提高基于DNN的语音增强方法的实时性。下一步将从压缩网络规模着手, 降低语音增强模型的复杂度, 减小由计算过程带来的延时, 提高基于DNN的语音增强方法的实时性。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="119">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Single channel speech enhancement based on harmonic estimation combined with statistical based method to improve speech intelligibility for cochlear implant recipients">

                                <b>[1]</b> WANG Dongmei, HANSEN J H L.Single channel speech enhancement based on harmonic estimation combined with statistical based method to improve speech intelligibility for cochlear implant recipients[J].Journal of the Acoustical Society of America, 2017, 141 (5) :3985-3986.
                            </a>
                        </p>
                        <p id="121">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES794199E20D35EB9B7A85B5AA8E53D9D1&amp;v=MDM0MzBRVW16ZDRPbnFUM1JwQWZMSGdUTTZlQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQ3BiUTM1TnhpeExxOHdxMD1OaWZPZmJTeEd0REZwdnBIWko4TUNRbEx4bQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> TU Jingxian, XIA Youshen.Effective Kalman filtering algorithm for distributed multichannel speech enhancement[J].Neurocomputing, 2018, 275:144-154.
                            </a>
                        </p>
                        <p id="123">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES8352F922C8CAEFCDC7DFE488AB02D160&amp;v=MzExMDc2cG8xSEYrTjhmUWxQdkdKZzdVc0xQWHZxcEdOSGViRGdSTHlmQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQ3BiUTM1TnhpeExxOHdxMD1OaWZPZmJ1N0c5Tw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> KANDAGATLA R K, SUBBAIAH P V.Speech enhancement using MMSE estimation of amplitude and complex speech spectral coefficients under phase-uncertainty[J].Speech Communication, 2018, 96:10-27.
                            </a>
                        </p>
                        <p id="125">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201606003&amp;v=MzAxOTFNcVk5Rlo0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVScUZDam5VTDNOS0NMZlliRzRIOWY=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b> 刘文举, 聂帅, 梁山, 等.基于深度学习语音分离技术的研究现状与进展[J].自动化学报, 2016, 42 (6) :819-833.
                            </a>
                        </p>
                        <p id="127">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201702007&amp;v=MTgwMDc0SDliTXJZOUZZNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUnFGQ2puVUwzTktDTGZZYkc=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b> 韩伟, 张雄伟, 闵刚, 等.基于感知掩蔽深度神经网络的单通道语音增强方法[J].自动化学报, 2017, 43 (2) :248-258.
                            </a>
                        </p>
                        <p id="129">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=An Experimental Study on Speech Enhancement Based on Deep Neural Networks">

                                <b>[6]</b> XU Yong, DU Jun, DAI Lirong, et al.An experimental study on speech enhancement based on deep neural networks[J].IEEE Signal Processing Letters, 2014, 21 (1) :65-68.
                            </a>
                        </p>
                        <p id="131">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM0DE9B37CC176D44F1D9767445384B3EC&amp;v=MTEzMzdnZzl5MkFTbmpaNlRuam1xQmMyY2JibVJzL3NDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZDcGJRMzVOeGl4THE4d3EwPU5pZklZN1BNYTlpK3JJZzJGK29JQw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b> XU Yong, DU Jun, DAI Lirong, et al.A regression approach to speech enhancement based on deep neural networks[J].IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2015, 23 (1) :7-19.
                            </a>
                        </p>
                        <p id="133">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Large-scale training to increase speech intelli-gibility for hearing-impaired listeners in novel noises">

                                <b>[8]</b> CHEN Jitong, WANG Yuxuan, YOHO S E, et al.Large-scale training to increase speech intelligibility for hearing-impaired listeners in novel noises[J].Journal of the Acoustical Society of America, 2016, 139 (5) :2604-2612.
                            </a>
                        </p>
                        <p id="135">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Long short-term memory for speaker generalization in supervised speech separation">

                                <b>[9]</b> CHEN Jitong, WANG Deliang.Long short-term memory for speaker generalization in supervised speech separation[J].Journal of the Acoustical Society of America, 2017, 141 (6) :4705.
                            </a>
                        </p>
                        <p id="137">
                            <a id="bibliography_10" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCMD5136DBFFDF1517932D70938E39D7B42&amp;v=MTU0NTdpZklZOGU5SDlMSzIvMHpFcDk1RFhrNHlCOFE2RXQ2U0hiaHBHYzJjTWFUTjc2ZENPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkNwYlEzNU54aXhMcTh3cTA9Tg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[10]</b> ZHANG Xiaolei, WANG Deliang.A deep ensemble learning method for monaural speech separation[J].IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2016, 24 (5) :967-977.
                            </a>
                        </p>
                        <p id="139">
                            <a id="bibliography_11" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJX201706001&amp;v=MTIyODdlWmVScUZDam5VTDNOTHo3QmRyRzRIOWJNcVk5RlpZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE8=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[11]</b> 周飞燕, 金林鹏, 董军.卷积神经网络研究综述[J].计算机学报, 2017, 40 (6) :1229-1251.
                            </a>
                        </p>
                        <p id="141">
                            <a id="bibliography_12" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201804016&amp;v=Mjc3NTMzenFxQnRHRnJDVVJMT2VaZVJxRkNqblVMM05LQ0xmWWJHNEg5bk1xNDlFWW9RS0RIODR2UjRUNmo1NE8=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[12]</b> 袁文浩, 孙文珠, 夏斌, 等.利用深度卷积神经网络提高未知噪声下的语音增强性能[J].自动化学报, 2018, 44 (4) :751-759.
                            </a>
                        </p>
                        <p id="143">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=An Introduction to Computational Networks and the Computational Network Toolkit">

                                <b>[13]</b> YU Dong, EVERSOLE A, SELTZER M, et al.An introduction to computational networks and the computational network toolkit:MSR-TR-2014-112[R].Redmond, USA:Microsoft Research, 2014.
                            </a>
                        </p>
                        <p id="145">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=TIMIT acoustic-phonetic continuous speech corpus">

                                <b>[14]</b> GAROFOLO J S, LAMEL L F, FISHER W M, et al.TIMIT acoustic-phonetic continuous speech corpus[EB/OL].[2018-05-30].https://catalog.ldc.upenn.edu/LDC93S1.
                            </a>
                        </p>
                        <p id="147">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=&amp;quot;100 nonspeech environmental sounds,2004&amp;quot;">

                                <b>[15]</b> HU Guoning.100 nonspeech environmental sounds, 2004[EB/OL].[2018-05-30].http://web.cse.ohio-state.edu/pnl/corpus/HuNonspeech/HuCorpus.html.
                            </a>
                        </p>
                        <p id="149">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Assessment for automatic speech recognition: II. NOISEX-92: A database and an experiment to study the effect of additive noise on speech recognition systems">

                                <b>[16]</b> VARGA A, STEENEKEN H J M.Assessment for automatic speech recognition:II.NOISEX-92:a database and an experiment to study the effect of additive noise on speech recognition systems[J].Speech Communication, 1993, 12 (3) :247-251.
                            </a>
                        </p>
                        <p id="151">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Perceptual evaluation of speech quality (PESQ)-a new method for speech quality assessment of telephone networks and codecs">

                                <b>[17]</b> RIX A W, BEERENDS J G, HOLLIER M P, et al.Perceptual evaluation of speech quality (PESQ) -a new method for speech quality assessment of telephone networks and codes[C]//Proceedings of IEEE International Conference on Acoustics, Speech, and Signal Processing.Washington D.C., USA:IEEE Press, 2001:749-752.
                            </a>
                        </p>
                        <p id="153">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=An Algorithm for Intelligibility Prediction of Time-Frequency Weighted Noisy Speech">

                                <b>[18]</b> TAAL C H, HENDRIKS R C, HEUSDENS R, et al.An algorithm for intelligibility prediction of time-frequency weighted noisy speech[J].IEEE Transactions on Audio, Speech, and Language Processing, 2011, 19 (7) :2125-2136.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJC201908042" />
        <input id="dpi" type="hidden" value="600" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201908042&amp;v=MjM2NTVCWm9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJxRkNqblVMM0tMejdCYmJHNEg5ak1wNDk=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYyTTc5ZU1kSFJvVW1uVU9FL1g5cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
