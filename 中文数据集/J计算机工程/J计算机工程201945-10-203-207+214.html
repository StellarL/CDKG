<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637126202979927500%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJC201910034%26RESULT%3d1%26SIGN%3dQ3C7nqAENvmUDiuNOrBJx%252fMPwUk%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201910034&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201910034&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201910034&amp;v=MjY4OTVHWUlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJ0RnkzaFU3cklMejdCYmJHNEg5ak5yNDk=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZhcFFuenlFZmNsZnBPdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#43" data-title="0 概述 ">0 概述</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#47" data-title="1 有偏随机优化 ">1 有偏随机优化</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#56" data-title="2 DA收敛速率证明 ">2 DA收敛速率证明</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#78" data-title="3 梯度有偏差对偶平均方法的个体收敛速率 ">3 梯度有偏差对偶平均方法的个体收敛速率</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#95" data-title="4 实验结果与分析 ">4 实验结果与分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#98" data-title="4.1 人工数据集">4.1 人工数据集</a></li>
                                                <li><a href="#102" data-title="4.2 真实数据集">4.2 真实数据集</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#108" data-title="5 结束语 ">5 结束语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#101" data-title="&lt;b&gt;图1 人工数据集收敛速率对比&lt;/b&gt;"><b>图1 人工数据集收敛速率对比</b></a></li>
                                                <li><a href="#104" data-title="&lt;b&gt;表1 真实数据集描述&lt;/b&gt;"><b>表1 真实数据集描述</b></a></li>
                                                <li><a href="#106" data-title="&lt;b&gt;图2 真实数据集收敛速率对比&lt;/b&gt;"><b>图2 真实数据集收敛速率对比</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" title=" NESTEROV Y.Primal-dual subgradient methods for convex problems[J].Mathematical Programming,2009,120(1):221-259." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SPQD&amp;filename=SPQD00004301960&amp;v=MTA3NjBIdEhJckk5RWJlMFBZM2s1ekJkaDRqOTlTWHFScnhveGNNSDdSN3FlYnVkdEZDdmxWcnZJSlYwPU5qM2Fhck80&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZhcFFuenlFZmNsZnBPdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         NESTEROV Y.Primal-dual subgradient methods for convex problems[J].Mathematical Programming,2009,120(1):221-259.
                                    </a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" title=" BERTSEKAS D P,NEDI A,OZDAGLAR A E.Convex analysis and optimization[M].Berlin,Germany:Springer,2003." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Convex analysis and optimization">
                                        <b>[2]</b>
                                         BERTSEKAS D P,NEDI A,OZDAGLAR A E.Convex analysis and optimization[M].Berlin,Germany:Springer,2003.
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title=" BECK A,TEBOULLE M.Mirror descent and nonlinear projected sub-gradient methods for convex optimization[J].Operations Research Letters,2003,31(3):167-175." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012300467851&amp;v=MTkzMjdGWU8wSUJIazRvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVVMeklKVm9VYnhFPU5pZk9mYks3SHRET3JJOQ==&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZhcFFuenlFZmNsZnBPdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         BECK A,TEBOULLE M.Mirror descent and nonlinear projected sub-gradient methods for convex optimization[J].Operations Research Letters,2003,31(3):167-175.
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" title=" ZHANG Tong.Solving large scale linear prediction problems using stochastic gradient descent algorithms[C]//Proceedings of the 21st International Conference on Machine Learning.New York,USA:ACM Press,2004:919-926." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Solving large scale linear prediction problems using stochastic gradient descent algorithms">
                                        <b>[4]</b>
                                         ZHANG Tong.Solving large scale linear prediction problems using stochastic gradient descent algorithms[C]//Proceedings of the 21st International Conference on Machine Learning.New York,USA:ACM Press,2004:919-926.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" title=" SCHMIDT M,ROUX N L,BACH F.Convergence rates of inexact proximal gradient methods for convex optimization[J].Advances in Neural Information Processing Systems,2011,24:1458-1466." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Convergence rates of inexact proximal-gradient methods for convex optimization">
                                        <b>[5]</b>
                                         SCHMIDT M,ROUX N L,BACH F.Convergence rates of inexact proximal gradient methods for convex optimization[J].Advances in Neural Information Processing Systems,2011,24:1458-1466.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" title=" DEVOLDER O.Stochastic first order methods in smooth convex optimization[EB/OL].[2018-07-20].https://core.ac.uk/download/pdf/34135646.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Stochastic first order methods in smooth convex optimization">
                                        <b>[6]</b>
                                         DEVOLDER O.Stochastic first order methods in smooth convex optimization[EB/OL].[2018-07-20].https://core.ac.uk/download/pdf/34135646.pdf.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" title=" HONORIO J.Convergence rates of biased stochastic optimization for learning sparse ising models[C]//Proceedings of International Conference on Machine Learning.Madison,USA:Omnipress,2012:257-264." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Convergence rates of biased stochastic optimization for learning sparse ising models">
                                        <b>[7]</b>
                                         HONORIO J.Convergence rates of biased stochastic optimization for learning sparse ising models[C]//Proceedings of International Conference on Machine Learning.Madison,USA:Omnipress,2012:257-264.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" title=" DASPREMONT A.Smooth optimization with approximate gradient[J].SIAM Journal on Optimization,2008,19(3):1171-1183." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Smooth optimization with approximate gradient">
                                        <b>[8]</b>
                                         DASPREMONT A.Smooth optimization with approximate gradient[J].SIAM Journal on Optimization,2008,19(3):1171-1183.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" title=" DEVOLDER O,GLINEUR F,NESTEROV Y.First-order methods of smooth convex optimization with inexact oracle[J].Mathematical Programming,2014,146(1/2):37-75." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD14072400004581&amp;v=MzE1OTFHZXJxUVRNbndaZVp1SHlqbVVMeklKVm9VYnhFPU5qN0Jhcks4SHRiT3E0OUZaT3NMQ1hRNG9CTVQ2VDRQUUgvaXJSZA==&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZhcFFuenlFZmNsZnBPdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                         DEVOLDER O,GLINEUR F,NESTEROV Y.First-order methods of smooth convex optimization with inexact oracle[J].Mathematical Programming,2014,146(1/2):37-75.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_10" title=" RAKHLIN A,SHAMIR O,SRIDHARAN K.Making gradient descent optimal for strongly convex stochastic optimiza-tion[C]//Proceedings of International Conference on Machine Learning.Madison,USA:Omnipress,2012:1571-1578." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Making gradient descent optimal for strongly convex stochastic optimization">
                                        <b>[10]</b>
                                         RAKHLIN A,SHAMIR O,SRIDHARAN K.Making gradient descent optimal for strongly convex stochastic optimiza-tion[C]//Proceedings of International Conference on Machine Learning.Madison,USA:Omnipress,2012:1571-1578.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_11" title=" SHAMIR O,ZHANG Tong.Stochastic gradient descent for non-smooth optimization:convergence results and optimal averaging schemes[C]//Proceedings of International Conference on Machine Learning.Madison,USA:Omnipress,2013:71-79." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Stochastic gradient descent for non-smooth optimization:Convergence results and optimal averaging schemes">
                                        <b>[11]</b>
                                         SHAMIR O,ZHANG Tong.Stochastic gradient descent for non-smooth optimization:convergence results and optimal averaging schemes[C]//Proceedings of International Conference on Machine Learning.Madison,USA:Omnipress,2013:71-79.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_12" title=" NESTEROV Y,SHIKHMAN V.Quasi-monotone subgradient methods for nonsmooth convex minimization[J].Journal of Optimization Theory and Applications,2015,165(3):917-940." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Quasi-monotone subgradient methods for nonsmooth convex minimization">
                                        <b>[12]</b>
                                         NESTEROV Y,SHIKHMAN V.Quasi-monotone subgradient methods for nonsmooth convex minimization[J].Journal of Optimization Theory and Applications,2015,165(3):917-940.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_13" title=" 陶蔚,潘志松,储德军,等.使用Nesterov步长策略投影次梯度方法的个体收敛性[J].计算机学报,2018,41(1):164-176." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJX201801010&amp;v=MTA4NzdJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSdEZ5M2hVN3JMTHo3QmRyRzRIOW5Ncm85RVo=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZhcFFuenlFZmNsZnBPdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[13]</b>
                                         陶蔚,潘志松,储德军,等.使用Nesterov步长策略投影次梯度方法的个体收敛性[J].计算机学报,2018,41(1):164-176.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_14" title=" DUCHI J,SHALEV-SHWARTZ S,SINGER Y,et al.Composite objective mirror descent[EB/OL].[2018-07-20].http://web.stanford.edu/～jduchi/projects/DuchiShSiTe10.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Composite objective mirror descent">
                                        <b>[14]</b>
                                         DUCHI J,SHALEV-SHWARTZ S,SINGER Y,et al.Composite objective mirror descent[EB/OL].[2018-07-20].http://web.stanford.edu/～jduchi/projects/DuchiShSiTe10.pdf.
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_15" title=" NESTEROV Y.A method of solving a convex programming problem with convergence rate O(1/k2)[J].Soviet Mathematics Doklady,1983,27(2):372-376." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A method of solving a convex programming problem with convergence rate O (1/k2)">
                                        <b>[15]</b>
                                         NESTEROV Y.A method of solving a convex programming problem with convergence rate O(1/k2)[J].Soviet Mathematics Doklady,1983,27(2):372-376.
                                    </a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_16" title=" HU Chonghai,KWOK J T,PAN W.Accelerated gradient methods for stochastic optimization and online learning[C]//Proceedings of the 22nd International Conference on Neural Information Processing Systems.[S.l.]:Curran Associates Inc.,2009:781-789." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Accelerated gradient methods for stochastic optimization and online learning">
                                        <b>[16]</b>
                                         HU Chonghai,KWOK J T,PAN W.Accelerated gradient methods for stochastic optimization and online learning[C]//Proceedings of the 22nd International Conference on Neural Information Processing Systems.[S.l.]:Curran Associates Inc.,2009:781-789.
                                    </a>
                                </li>
                                <li id="35">


                                    <a id="bibliography_17" title=" TSENG P.Approximation accuracy,gradient methods,and error bound for structured convex optimization[J].Mathematical Programming,2010,125(2):263-295." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00003749915&amp;v=MTE3MTAwPU5qN0Jhck80SHRIUHFJdE1iZW9LWTNrNXpCZGg0ajk5U1hxUnJ4b3hjTUg3UjdxZWJ1ZHRGQ3ZsVnJ2SUpW&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZhcFFuenlFZmNsZnBPdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[17]</b>
                                         TSENG P.Approximation accuracy,gradient methods,and error bound for structured convex optimization[J].Mathematical Programming,2010,125(2):263-295.
                                    </a>
                                </li>
                                <li id="37">


                                    <a id="bibliography_18" title=" 陶蔚,潘志松,朱小辉,等.线性插值投影次梯度方法的最优个体收敛速率[J].计算机研究与发展,2017,54(3):529-536." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201703007&amp;v=MTkzNTZxQnRHRnJDVVJMT2VaZVJ0RnkzaFU3ckxMeXZTZExHNEg5Yk1ySTlGWTRRS0RIODR2UjRUNmo1NE8zenE=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZhcFFuenlFZmNsZnBPdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[18]</b>
                                         陶蔚,潘志松,朱小辉,等.线性插值投影次梯度方法的最优个体收敛速率[J].计算机研究与发展,2017,54(3):529-536.
                                    </a>
                                </li>
                                <li id="39">


                                    <a id="bibliography_19" title=" XIAO Lin,ZHANG Tong.A proximal stochastic gradient method with progressive variance reduction[J].SIAM Journal on Optimization,2014,24(4):2057-2075." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Proximal Stochastic Gradient Method with Progressive Variance Reduction">
                                        <b>[19]</b>
                                         XIAO Lin,ZHANG Tong.A proximal stochastic gradient method with progressive variance reduction[J].SIAM Journal on Optimization,2014,24(4):2057-2075.
                                    </a>
                                </li>
                                <li id="41">


                                    <a id="bibliography_20" title=" 瓦普尼克.统计学习理论的本质[M].张学工,译.北京:清华大学出版社,2000." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CBBD&amp;filename=000730203964X999&amp;v=MTIxNzFQcG9sQkRPSUdCUk04enhVU21EZDlTSDduM3hFOWZidm5LcmlmWnU5dUZDdm1VN3JNSUZzWFZWMjdHYkM0SE5I&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZhcFFuenlFZmNsZnBPdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[20]</b>
                                         瓦普尼克.统计学习理论的本质[M].张学工,译.北京:清华大学出版社,2000.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJC" target="_blank">计算机工程</a>
                2019,45(10),203-207+214 DOI:10.19678/j.issn.1000-3428.0052466            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>梯度有偏随机DA优化方法的个体收敛界分析</b></span>
 <span class="shoufa"></span>                                     </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BC%A0%E6%A2%A6%E6%99%97&amp;code=40364757&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZhcFFuenlFZmNsZnBPdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">张梦晗</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%B1%AA%E6%B5%B7&amp;code=38975700&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZhcFFuenlFZmNsZnBPdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">汪海</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%88%98%E6%AC%A3&amp;code=40455238&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZhcFFuenlFZmNsZnBPdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">刘欣</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%B2%8D%E8%95%BE&amp;code=40455239&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZhcFFuenlFZmNsZnBPdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">鲍蕾</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E4%BA%BA%E6%B0%91%E8%A7%A3%E6%94%BE%E5%86%9B%E9%99%86%E5%86%9B%E7%82%AE%E5%85%B5%E9%98%B2%E7%A9%BA%E5%85%B5%E5%AD%A6%E9%99%A2%E4%BF%A1%E6%81%AF%E5%B7%A5%E7%A8%8B%E7%B3%BB&amp;code=1702679&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZhcFFuenlFZmNsZnBPdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国人民解放军陆军炮兵防空兵学院信息工程系</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E4%BA%BA%E6%B0%91%E8%A7%A3%E6%94%BE%E5%86%9B%E9%99%86%E5%86%9B%E7%82%AE%E5%85%B5%E9%98%B2%E7%A9%BA%E5%85%B5%E5%AD%A6%E9%99%A2%E5%9F%BA%E7%A1%80%E9%83%A8&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZhcFFuenlFZmNsZnBPdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国人民解放军陆军炮兵防空兵学院基础部</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>样本不满足独立同分布会使梯度估计在迭代过程中存在偏差,且最优的个体收敛界在噪声的干扰下无法确定。为此,提出一种线性插值随机对偶平均(DA)优化方法。给出DA方法收敛性的证明,在梯度估计有偏的基础上,求解得到一种线性插值DA随机优化方法不产生累积偏差的个体收敛界,以保证正则化损失函数结构下优化方法的个体收敛精度。实验结果表明,与随机加速方法相比,该方法具有较快的个体收敛速率与较高的收敛精度。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%AF%B9%E5%81%B6%E5%B9%B3%E5%9D%87%E6%96%B9%E6%B3%95&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZhcFFuenlFZmNsZnBPdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">对偶平均方法;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%9A%8F%E6%9C%BA%E4%BC%98%E5%8C%96&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZhcFFuenlFZmNsZnBPdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">随机优化;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%B8%AA%E4%BD%93%E6%94%B6%E6%95%9B%E6%80%A7&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZhcFFuenlFZmNsZnBPdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">个体收敛性;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%A2%AF%E5%BA%A6%E6%9C%89%E5%81%8F%E4%BC%B0%E8%AE%A1&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZhcFFuenlFZmNsZnBPdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">梯度有偏估计;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%9C%80%E4%BC%98%E6%94%B6%E6%95%9B%E9%80%9F%E7%8E%87&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZhcFFuenlFZmNsZnBPdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">最优收敛速率;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    张梦晗(1994—),男,硕士研究生,主研方向为机器学习、模式识别;;
                                </span>
                                <span>
                                    汪海,硕士研究生;;
                                </span>
                                <span>
                                    刘欣,教授;;
                                </span>
                                <span>
                                    鲍蕾,讲师、博士。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-08-22</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金(61673394);</span>
                    </p>
            </div>
                    <h1><b>Analysis of Individual Convergence Bound for Gradient Biased Stochastic DA Optimization Method</b></h1>
                    <h2>
                    <span>ZHANG Menghan</span>
                    <span>WANG Hai</span>
                    <span>LIU Xin</span>
                    <span>BAO Lei</span>
            </h2>
                    <h2>
                    <span>Department of Information Engineering,PLA Army Academy of Artillery and Air Defense</span>
                    <span>Department of Basic Courses,PLA Army Academy of Artillery and Air Defense</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Samples that do not satisfy the independent and identical distribution will lead to deviations of the gradient estimation in the iterative process,and the convergence bound of the optimal individual cannot be determined under the interference of noise.Therefore,a linear interpolation stochastic Dual Averaging(DA) optimization method is proposed.The proof of the convergence of the DA method is given.On the basis of the gradient estimation bias,the individual convergence bounds of the non-cumulative deviation of the linear interpolation DA stochastic optimization method are obtained,and the optimization method of individual convergence precision of regularized loss function structure is assured.Experimental results show that compared with the stochastic accelerate method,the method has a faster individual convergence rate and a higher convergence accuracy.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Dual%20Averaging(DA)%20method&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZhcFFuenlFZmNsZnBPdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Dual Averaging(DA) method;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=stochastic%20optimization&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZhcFFuenlFZmNsZnBPdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">stochastic optimization;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=individual%20convergence&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZhcFFuenlFZmNsZnBPdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">individual convergence;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=gradient%20biased%20estimation&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZhcFFuenlFZmNsZnBPdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">gradient biased estimation;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=optimal%20convergence%20rate&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZhcFFuenlFZmNsZnBPdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">optimal convergence rate;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-08-22</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="43" name="43" class="anchor-tag">0 概述</h3>
                <div class="p1">
                    <p id="44">对偶平均(Dual Averaging,DA)<citation id="136" type="reference"><link href="3" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>是求解非光滑约束问题的一阶梯度优化方法,由经典的投影次梯度方法<citation id="137" type="reference"><link href="5" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>和镜面下降算法<citation id="138" type="reference"><link href="7" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>改进而来。批处理形式的梯度优化方法在迭代过程中通过遍历样本集来计算损失函数的梯度,无法满足数据规模急剧增长的需求。然而,由于机器学习通常假定先验样本集均为独立同分布,使单个样本对应目标函数的梯度就是训练集上目标函数梯度的无偏估计。因此,随机形式的DA优化方法在迭代过程中仅需获知目标函数梯度的无偏估计即可<citation id="139" type="reference"><link href="9" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="45">在实际应用中,样本独立同分布的条件未知且无法验证,被认为是广义的梯度有偏估计问题<citation id="143" type="reference"><link href="11" rel="bibliography" /><link href="13" rel="bibliography" /><link href="15" rel="bibliography" /><sup>[<a class="sup">5</a>,<a class="sup">6</a>,<a class="sup">7</a>]</sup></citation>。对于批处理形式的算法,文献<citation id="140" type="reference">[<a class="sup">8</a>]</citation>指出,当批处理方法中存在固定水平的定性误差时,Nesterov加速算法的收敛速率界随着迭代步数的增加会出现累加定性误差的现象,而非加速算法的收敛速率界为常量。在此基础上,文献<citation id="141" type="reference">[<a class="sup">9</a>]</citation>提出随机优化方法。在处理稀疏优化问题时,使用优化算法的个体解作为最终输出,个体解比平均形式的解具有更好的稀疏性。然而,关于个体解的输出研究较多<citation id="144" type="reference"><link href="21" rel="bibliography" /><link href="23" rel="bibliography" /><sup>[<a class="sup">10</a>,<a class="sup">11</a>]</sup></citation>,但对于单纯的凸问题,目前经典梯度优化算法的最优个体收敛速率仍是机器学习领域研究的重点。文献<citation id="142" type="reference">[<a class="sup">12</a>]</citation>提出DA优化方法,在迭代过程中嵌入线性插值操作,实验结果表明,该方法在一般凸情形下具有最优的个体收敛速率。</p>
                </div>
                <div class="p1">
                    <p id="46">关于个体收敛性的研究较多<citation id="145" type="reference"><link href="27" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>,但较少涉及梯度估计有偏情形。本文研究线性插值随机DA优化方法<citation id="146" type="reference"><link href="25" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>的个体收敛界是否随迭代累积偏差的问题。对于有偏梯度估计,文献<citation id="147" type="reference">[<a class="sup">12</a>]</citation>利用平均输出的方式得到DA优化方法,该方法的收敛界不产生累积偏差。对于线性插值DA的优化算法,由于在收敛性分析过程中对偶空间的复杂度要求,较难结合偏差进行求解,因此文献<citation id="148" type="reference">[<a class="sup">12</a>]</citation>给出其个体收敛界不随噪声偏差累积的预测。本文基于COMID的收敛性分析<citation id="149" type="reference"><link href="29" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>给出DA收敛性证明,在梯度估计有偏差的情形下,得到正则化损失函数问题的线性插值随机DA不产生累积偏差的个体收敛界,从而保证算法的个体收敛精度。</p>
                </div>
                <h3 id="47" name="47" class="anchor-tag">1 有偏随机优化</h3>
                <div class="p1">
                    <p id="48">本文研究二分类正则化优化问题。根据文献<citation id="150" type="reference">[<a class="sup">16</a>]</citation>,有偏随机优化问题描述如下:</p>
                </div>
                <div class="p1">
                    <p id="49" class="code-formula">
                        <mathml id="49"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>min</mi></mrow><mspace width="0.25em" /><mi>F</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">w</mi><mo stretchy="false">)</mo><mo>=</mo><mi>r</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">w</mi><mo stretchy="false">)</mo><mo>+</mo><mi>E</mi><msub><mrow></mrow><mi>ξ</mi></msub><mrow><mo>[</mo><mrow><mi>f</mi><mrow><mo>(</mo><mrow><mi mathvariant="bold-italic">w</mi><mo>,</mo><mi>ξ</mi></mrow><mo>)</mo></mrow></mrow><mo>]</mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="50">其中,<i><b>w</b></i>∈<image href="images/JSJC201910034_110.jpg" type="" display="inline" placement="inline"><alt></alt></image><sup><i>n</i></sup>是优化变量,即学习问题的权重,<i>r</i>(<i><b>w</b></i>)= <i>λ</i>‖<i>w</i>‖<sub>1</sub>为正则化项,其控制优化变量的稀疏程度,<mathml id="111"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>f</mi><mrow><mo>(</mo><mrow><mi mathvariant="bold-italic">w</mi><mo>,</mo><mi>ξ</mi></mrow><mo>)</mo></mrow><mo>=</mo><mfrac><mn>1</mn><mi>n</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>l</mi></mstyle><mo stretchy="false">(</mo><mi mathvariant="bold-italic">w</mi><mo>,</mo><mi>ξ</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></math></mathml>为损失项,控制优化模型的训练精度,λ为正则化项和损失项的折衷参数,损失项中的n为训练集中的样本数目,<mathml id="112"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>ξ</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mrow><mo>(</mo><mrow><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mo>)</mo></mrow></mrow></math></mathml>代表第<i>i</i>个样本,<i>y</i><sub><i>i</i></sub>∈{+1,-1}为样本标签,<i><b>x</b></i><sub><i>i</i></sub>∈<i>R</i><sup><i>n</i></sup>为样本。该优化问题的目标函数可理解为随机样本造成正则化损失的期望,此时损失项为<mathml id="113"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>f</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">w</mi><mo stretchy="false">)</mo><mo>=</mo></mrow><mi>E</mi><msub><mrow></mrow><mi>ξ</mi></msub><mrow><mo>[</mo><mrow><mi>f</mi><mrow><mo>(</mo><mrow><mi mathvariant="bold-italic">w</mi><mo>,</mo><mi>ξ</mi></mrow><mo>)</mo></mrow></mrow><mo>]</mo></mrow></mrow></math></mathml>。由于目标函数是期望形式,无法直接优化,但一阶方法在优化过程中仅使用目标函数的梯度信息,只需对损失的梯度进行估计便可优化求解。根据文献<citation id="151" type="reference">[<a class="sup">7</a>]</citation>,下文描述有偏情形下的随机优化问题。</p>
                </div>
                <div class="p1">
                    <p id="51"><b>定义1</b> 假设训练集样本损失的全梯度为∇<i>f</i>(<i><b>w</b></i>),随机抽取的单个样本损失的函数梯度为∇<i>f</i>(<i><b>w</b></i>,<i>ξ</i>),<i>E</i><sub><i>ξ</i></sub>[∇<i>f</i>(<i><b>w</b></i>,<i>ξ</i>)]=∇<i>f</i>(<i><b>w</b></i>)+<i>ε</i>,<i>E</i><sub><i>ξ</i></sub>[‖∇<i>f</i>(<i><b>w</b></i>,<i>ξ</i>)-∇<i>f</i>(<i><b>w</b></i>)‖<sup>2</sup>]≤<i>σ</i><sup>2</sup>,其中,<i>ε</i>∈<i>R</i>为使用随机样本<i>ξ</i>进行梯度估计而产生的误差,<i>σ</i><sup>2</sup>为相应的方差。如果<i>ε</i>=0,则单个随机样本损失的函数梯度为全梯度∇<i>f</i>(<i><b>w</b></i>)的无偏估计;如果<i>ε</i>≠0,则单个随机样本损失的函数梯度为全梯度∇<i>f</i>(<i><b>w</b></i>)的有偏估计,同时称式(1)为有偏随机优化问题。</p>
                </div>
                <div class="p1">
                    <p id="52">对于式(1)描述的优化目标,文献<citation id="152" type="reference">[<a class="sup">9</a>]</citation>在目标函数光滑情形下给出有偏随机优化的凸函数性质。对于满足(<i>δ</i>,<i>L</i>)型的目标函数<i>f</i>(<i><b>w</b></i>)且<i><b>w</b></i>,<i><b>y</b></i>∈<i>Q</i>,有:</p>
                </div>
                <div class="p1">
                    <p id="53" class="code-formula">
                        <mathml id="53"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mn>0</mn><mo>≤</mo><mi>f</mi><mrow><mo>(</mo><mi mathvariant="bold-italic">y</mi><mo>)</mo></mrow><mo>-</mo><mi>f</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">w</mi><mo stretchy="false">)</mo><mo>-</mo><mo>〈</mo><mi>g</mi><msub><mrow></mrow><mrow><mi>δ</mi><mi>L</mi></mrow></msub><mo stretchy="false">(</mo><mi mathvariant="bold-italic">w</mi><mo stretchy="false">)</mo><mo>,</mo><mi mathvariant="bold-italic">y</mi><mo>-</mo><mi mathvariant="bold-italic">w</mi><mo>〉</mo><mo>≤</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mfrac><mi>L</mi><mn>2</mn></mfrac><mrow><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">y</mi><mo>-</mo><mi mathvariant="bold-italic">w</mi><mo stretchy="false">∥</mo></mrow><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mi>δ</mi><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="54">其中,<i>f</i>(<i><b>w</b></i>)为要优化的目标函数,<i>g</i><sub><i>δL</i></sub>(<i><b>w</b></i>)为(<i>δ</i>,<i>L</i>)型梯度估计,<i>L</i>为光滑系数,<i>δ</i>为偏差。</p>
                </div>
                <div class="p1">
                    <p id="55">近年来,国内外学者研究了非精确求解有偏随机优化的问题。在文献<citation id="153" type="reference">[<a class="sup">8</a>]</citation>中,当批处理方法中存在固定的定性误差时,梯度优化算法收敛速率界为常量,而加速算法收敛速率的界却会出现随着迭代的增加而累加定性误差的现象。文献<citation id="154" type="reference">[<a class="sup">9</a>]</citation>将(<i>δ</i>,<i>L</i>)非精确求解应用到随机优化问题,给出已知随机偏差<i>δ</i>与方差<i>σ</i><sup>2</sup>情形下3种近似梯度下降算法的收敛速率,即随机一般梯度方法(Stochastic Primal Gradient Method,SPGM)、随机对偶梯度方法(Stochastic Dual Gradient Method,SDGM)和随机快速梯度方法(Stochastic Fast Gradient Method,SFGM)的收敛速率分别为<mathml id="114"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ο</mi><mo stretchy="false">(</mo><mo stretchy="false">(</mo><mi>L</mi><mo>/</mo><mi>t</mi><mo>+</mo><mi>σ</mi><mo>/</mo><msqrt><mi>t</mi></msqrt><mo stretchy="false">)</mo><mrow><mi>lg</mi></mrow><mi>t</mi><mo>+</mo><mi>δ</mi><mo stretchy="false">)</mo></mrow></math></mathml>、<mathml id="115"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ο</mi><mo stretchy="false">(</mo><mi>L</mi><mo>/</mo><mi>t</mi><mo>+</mo><mi>σ</mi><mo>/</mo><msqrt><mi>t</mi></msqrt><mo>+</mo><mi>δ</mi><mo stretchy="false">)</mo></mrow></math></mathml>和<mathml id="116"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ο</mi><mo stretchy="false">(</mo><mi>L</mi><mo>/</mo><mi>t</mi><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mi>σ</mi><mo>/</mo><msqrt><mi>t</mi></msqrt><mo>+</mo><mi>t</mi><mi>δ</mi><mo stretchy="false">)</mo><msup><mrow></mrow><mrow><mo stretchy="false">[</mo><mn>9</mn><mo stretchy="false">]</mo></mrow></msup></mrow></math></mathml>。其中,<i>t</i>为迭代步数。对于存在偏差的优化问题,随机方法和批处理方法一样,加速插值策略都会导致偏差<i>δ</i>随迭代累加而达不到任意给定的收敛精度。</p>
                </div>
                <h3 id="56" name="56" class="anchor-tag">2 DA收敛速率证明</h3>
                <div class="p1">
                    <p id="57">对于机器学习约束优化问题,有:</p>
                </div>
                <div class="p1">
                    <p id="58" class="code-formula">
                        <mathml id="58"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mrow><mi>w</mi><mo>∈</mo><mi>Q</mi></mrow></munder><mspace width="0.25em" /><mi>f</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">w</mi><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>3</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="59">其中,<i>Q</i>⊆<image href="images/JSJC201910034_117.jpg" type="" display="inline" placement="inline"><alt></alt></image><sup><i>n</i></sup>是有界闭凸集,<i>f</i>(<i><b>w</b></i>)是凸函数。</p>
                </div>
                <div class="p1">
                    <p id="60">文献<citation id="155" type="reference">[<a class="sup">9</a>]</citation>在投影次梯度方法和镜面下降算法的基础上,提出DA方法,其迭代方式为:</p>
                </div>
                <div class="p1">
                    <p id="61" class="code-formula">
                        <mathml id="61"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mtext>a</mtext><mtext>r</mtext><mtext>g</mtext><mtext>m</mtext><mtext>i</mtext><mtext>n</mtext></mrow></mstyle><mrow><mi>w</mi><mo>∈</mo><mi>Q</mi></mrow></munder><mrow><mo>{</mo><mrow><mo>〈</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>0</mn></mrow><mi>t</mi></munderover><mrow></mrow></mstyle><mi>a</mi><msub><mrow></mrow><mi>k</mi></msub><mo>∇</mo><mi>f</mi><mrow><mo>(</mo><mrow><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mi>k</mi></msub></mrow><mo>)</mo></mrow><mo>,</mo><mi mathvariant="bold-italic">w</mi><mo>〉</mo><mo>+</mo><mi>γ</mi><msub><mrow></mrow><mi>t</mi></msub><mi>d</mi><mrow><mo>(</mo><mi mathvariant="bold-italic">w</mi><mo>)</mo></mrow></mrow><mo>}</mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>4</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="62">其中,<i>α</i><sub><i>t</i></sub>为学习速率,<i>γ</i><sub><i>t</i></sub>为步长参数,其可以使学习速率<i>α</i><sub><i>t</i></sub>的选取更加灵活,能够以多种取值使收敛速率达到最优,<i>d</i>(<i><b>w</b></i>)为近邻函数,满足强凸性质。</p>
                </div>
                <div class="p1">
                    <p id="63" class="code-formula">
                        <mathml id="63"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>d</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">y</mi><mo stretchy="false">)</mo><mo>≥</mo><mi>d</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">w</mi><mo stretchy="false">)</mo><mo>+</mo><mo>〈</mo><mo>∇</mo><mi>d</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">w</mi><mo stretchy="false">)</mo><mo>,</mo><mi mathvariant="bold-italic">y</mi><mo>-</mo><mi mathvariant="bold-italic">w</mi><mo>〉</mo><mo>+</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mrow><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">y</mi><mo>-</mo><mi mathvariant="bold-italic">w</mi><mo stretchy="false">∥</mo></mrow><msup><mrow></mrow><mn>2</mn></msup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>5</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="64">其中,<i><b>w</b></i>∈<i><b>Q</b></i>,<i><b>y</b></i>∈<i><b>Q</b></i>。</p>
                </div>
                <div class="p1">
                    <p id="65"><b>引理1</b> 对于<i>d</i>(<i><b>w</b></i>)满足强凸性质,<i><b>w</b></i><sub><i>k</i></sub>由算法本身产生,<i>f</i>(<i>w</i>)为一般凸函数<citation id="156" type="reference"><link href="35" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>。对任意<i><b>w</b></i>∈<i>Q</i>,强凸极值为:</p>
                </div>
                <div class="p1">
                    <p id="66" class="code-formula">
                        <mathml id="66"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mo>〈</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>0</mn></mrow><mi>t</mi></munderover><mrow></mrow></mstyle><mi>a</mi><msub><mrow></mrow><mi>k</mi></msub><mo>∇</mo><mi>f</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false">)</mo><mo>,</mo><mi mathvariant="bold-italic">w</mi><mo>〉</mo><mo>+</mo><mi>γ</mi><msub><mrow></mrow><mi>t</mi></msub><mi>d</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">w</mi><mo stretchy="false">)</mo><mo>≥</mo><mo>〈</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>0</mn></mrow><mi>t</mi></munderover><mrow></mrow></mstyle><mi>a</mi><msub><mrow></mrow><mi>k</mi></msub><mo>∇</mo><mi>f</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false">)</mo><mo>,</mo><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>〉</mo><mo>+</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mi>γ</mi><msub><mrow></mrow><mi>t</mi></msub><mi>d</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo><mo>+</mo><mtext>γ</mtext><msub><mrow></mrow><mi>t</mi></msub><mo>/</mo><mn>2</mn><mrow><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">w</mi><mo>-</mo><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy="false">∥</mo></mrow><msup><mrow></mrow><mn>2</mn></msup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>6</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="67"><b>引理2</b> 对任意<i><b>w</b></i>∈<i><b>Q</b></i>,有:</p>
                </div>
                <div class="p1">
                    <p id="68" class="code-formula">
                        <mathml id="68"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>0</mn></mrow><mi>t</mi></munderover><mrow></mrow></mstyle><mo>〈</mo><mi>a</mi><msub><mrow></mrow><mi>k</mi></msub><mo>∇</mo><mi>f</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false">)</mo><mo>,</mo><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mi>k</mi></msub><mo>-</mo><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>〉</mo><mo>-</mo><mi>γ</mi><msub><mrow></mrow><mi>t</mi></msub><mi>d</mi><mrow><mo>(</mo><mrow><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><mo>)</mo></mrow><mo>≤</mo><mo>-</mo><mi>γ</mi><msub><mrow></mrow><mn>0</mn></msub><mi>d</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">)</mo><mo>+</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>0</mn></mrow><mi>t</mi></munderover><mrow></mrow></mstyle><mfrac><mrow><mi>a</mi><msub><mrow></mrow><mi>k</mi></msub><msup><mrow></mrow><mn>2</mn></msup></mrow><mrow><mn>2</mn><mi>γ</mi><msub><mrow></mrow><mi>k</mi></msub></mrow></mfrac><mrow><mo stretchy="false">∥</mo><mo>∇</mo><mi>f</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">∥</mo></mrow><msup><mrow></mrow><mn>2</mn></msup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>7</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="69"><b>定理1</b> 上述引理条件不变,<i>d</i>(<i><b>w</b></i>)满足强凸性质,<i><b>w</b></i><sub><i>k</i></sub>由算法本身产生,<i>f</i>(<i>w</i>)为一般凸函数。对任意<i><b>w</b></i>∈<i>Q</i>,有:</p>
                </div>
                <div class="p1">
                    <p id="70" class="code-formula">
                        <mathml id="70"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mfrac><mn>1</mn><mrow><mi>A</mi><msub><mrow></mrow><mi>t</mi></msub></mrow></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>0</mn></mrow><mi>t</mi></munderover><mrow></mrow></mstyle><mi>a</mi><msub><mrow></mrow><mi>k</mi></msub><mi>f</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false">)</mo><mo>-</mo><mi>f</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mo>*</mo></msub><mo stretchy="false">)</mo><mo>≤</mo><mfrac><mn>1</mn><mrow><mi>A</mi><msub><mrow></mrow><mi>t</mi></msub></mrow></mfrac><mo stretchy="false">[</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>t</mi></munderover><mrow></mrow></mstyle><mfrac><mrow><mi>a</mi><msub><mrow></mrow><mi>k</mi></msub><msup><mrow></mrow><mn>2</mn></msup></mrow><mrow><mn>2</mn><mi>γ</mi><msub><mrow></mrow><mrow><mi>k</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow></mfrac><mrow><mo stretchy="false">∥</mo><mo>∇</mo><mi>f</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">∥</mo></mrow><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mi>γ</mi><msub><mrow></mrow><mi>t</mi></msub><mi>d</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mo>*</mo></msub><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>8</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="71">与文献<citation id="157" type="reference">[<a class="sup">9</a>]</citation>方法结论相同,在选取<i>a</i><sub><i>t</i></sub>=1、<mathml id="118"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>γ</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mi>Ο</mi><mo stretchy="false">(</mo><msqrt><mi>t</mi></msqrt><mo stretchy="false">)</mo></mrow></math></mathml>时,对偶平均优化算法收敛速率能达到最优<mathml id="119"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ο</mi><mo stretchy="false">(</mo><mn>1</mn><mo>/</mo><msqrt><mi>t</mi></msqrt><mo stretchy="false">)</mo></mrow></math></mathml>。其中,<i><b>w</b></i><sub>*</sub>为式(1)的最优解。<i>A</i><sub><i>t</i></sub>的计算公式为:</p>
                </div>
                <div class="p1">
                    <p id="72" class="code-formula">
                        <mathml id="72"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>A</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>0</mn></mrow><mi>t</mi></munderover><mrow></mrow></mstyle><mi>a</mi><msub><mrow></mrow><mi>k</mi></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>9</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="73">文献<citation id="158" type="reference">[<a class="sup">9</a>]</citation>在原对偶平均方法的迭代中嵌入一步线性插值操作<citation id="159" type="reference"><link href="25" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>,对于一般凸的情形同样在选取<i>a</i><sub><i>t</i></sub>=1、<mathml id="120"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>γ</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mi>Ο</mi><mrow><mo>(</mo><mrow><msqrt><mi>t</mi></msqrt></mrow><mo>)</mo></mrow></mrow></math></mathml>或者<i>a</i><sub><i>t</i></sub>=<i>t</i>、<mathml id="121"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>γ</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mi>Ο</mi><mo stretchy="false">(</mo><mi>t</mi><msqrt><mi>t</mi></msqrt><mo stretchy="false">)</mo></mrow></math></mathml>时,对偶平均优化算法能得到最优的个体收敛速率<mathml id="122"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ο</mi><mrow><mo>(</mo><mrow><mn>1</mn><mo>/</mo><msqrt><mi>t</mi></msqrt></mrow><mo>)</mo></mrow></mrow></math></mathml>。迭代步骤为:</p>
                </div>
                <div class="p1">
                    <p id="74" class="code-formula">
                        <mathml id="74"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mi mathvariant="bold-italic">w</mi><msubsup><mrow></mrow><mi>t</mi><mo>+</mo></msubsup><mo>=</mo><mtext>a</mtext><mtext>r</mtext><mtext>g</mtext><mtext>m</mtext><mtext>i</mtext><mtext>n</mtext><mrow><mo>{</mo><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>0</mn></mrow><mi>t</mi></munderover><mrow></mrow></mstyle><mo>〈</mo><mi>a</mi><msub><mrow></mrow><mi>k</mi></msub><mo>∇</mo><mi>f</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false">)</mo><mo>,</mo><mi mathvariant="bold-italic">w</mi><mo>〉</mo><mo>+</mo><mi>γ</mi><msub><mrow></mrow><mi>t</mi></msub><mi>d</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">w</mi><mo stretchy="false">)</mo></mrow><mo>}</mo></mrow></mtd></mtr><mtr><mtd><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mfrac><mn>1</mn><mrow><mi>A</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow></mfrac><mrow><mo>[</mo><mrow><mi>a</mi><msub><mrow></mrow><mn>0</mn></msub><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mn>0</mn></msub><mo>+</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>0</mn></mrow><mi>t</mi></munderover><mrow></mrow></mstyle><mi>a</mi><msub><mrow></mrow><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub><mi mathvariant="bold-italic">w</mi><msubsup><mrow></mrow><mi>k</mi><mo>+</mo></msubsup></mrow><mo>]</mo></mrow></mtd></mtr></mtable></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>0</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="75">文献<citation id="160" type="reference">[<a class="sup">18</a>]</citation>将该插值方法推广到投影次梯度方法,得到个体输出形式下的最优收敛速率。但将文献<citation id="161" type="reference">[<a class="sup">12</a>]</citation>直接引入投影次梯度方法<citation id="162" type="reference"><link href="5" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>,无法得到个体输出的最优收敛速率,需要在优化算法中增加一个迭代步并修改步长参数,该方法得到的结论是对投影次梯度方法直接输出个体解问题的一种延伸和拓展,其迭代计算公式为:</p>
                </div>
                <div class="p1">
                    <p id="76" class="code-formula">
                        <mathml id="76"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mi mathvariant="bold-italic">w</mi><msubsup><mrow></mrow><mi>t</mi><mo>+</mo></msubsup><mo>=</mo><mi>Ρ</mi><msub><mrow></mrow><mi>Q</mi></msub><mrow><mo>(</mo><mrow><mi mathvariant="bold-italic">w</mi><msubsup><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow><mo>+</mo></msubsup><mo>-</mo><mi>a</mi><msub><mrow></mrow><mi>t</mi></msub><mi>τ</mi><msub><mrow></mrow><mi>t</mi></msub><mo>∇</mo><mi>f</mi><mrow><mo>(</mo><mrow><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mi>t</mi></msub></mrow><mo>)</mo></mrow></mrow><mo>)</mo></mrow></mtd></mtr><mtr><mtd><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mfrac><mrow><mi>A</mi><msub><mrow></mrow><mi>t</mi></msub></mrow><mrow><mi>A</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow></mfrac><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mi>t</mi></msub><mo>+</mo><mfrac><mrow><mi>a</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><mrow><mi>A</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow></mfrac><mi mathvariant="bold-italic">w</mi><msubsup><mrow></mrow><mi>t</mi><mo>+</mo></msubsup></mtd></mtr></mtable></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>1</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="77">其中,t≥1,P<sub>Q</sub>是<i><b>Q</b></i>上的投影算子,<i>a</i><sub><i>t</i></sub>和<i>τ</i><sub><i>t</i></sub>为学习速率参数。</p>
                </div>
                <h3 id="78" name="78" class="anchor-tag">3 梯度有偏差对偶平均方法的个体收敛速率</h3>
                <div class="p1">
                    <p id="79">正则化损失函数是机器学习监督学习算法的基本框架,不同的分类采用不同的损失,如光滑损失有最小二乘损失和对数损失,非光滑损失即hinge损失。常用正则化项L1、L2进行正则化,考虑到个体解的稀疏性,本文采用L1正则化项。本文基于非光滑稀疏学习问题,对标准的L1正则化hinge损失进行目标优化。</p>
                </div>
                <div class="p1">
                    <p id="80">对于式(1)描述的问题,有:</p>
                </div>
                <div class="p1">
                    <p id="81" class="code-formula">
                        <mathml id="81"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>min</mi></mrow><mspace width="0.25em" /><mi>F</mi><mrow><mo>(</mo><mi mathvariant="bold-italic">w</mi><mo>)</mo></mrow><mo>=</mo><mi>r</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">w</mi><mo stretchy="false">)</mo><mo>+</mo><mi>E</mi><msub><mrow></mrow><mi>ξ</mi></msub><mrow><mo>[</mo><mrow><mi>f</mi><mrow><mo>(</mo><mrow><mi mathvariant="bold-italic">w</mi><mo>,</mo><mi>ξ</mi></mrow><mo>)</mo></mrow></mrow><mo>]</mo></mrow></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="82">本文采用线性插值DA<citation id="163" type="reference"><link href="25" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>的方法进行迭代。</p>
                </div>
                <div class="p1">
                    <p id="83"><b>引理3</b> 设<i><b>Q</b></i>为L1正则化hinge损失优化问题的约束域,则<mathml id="123"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">Q</mi><mo>=</mo><mrow><mrow><mo>[</mo><mrow><mo>-</mo><mn>1</mn><mo>/</mo><mi>λ</mi><mo>,</mo><mn>1</mn><mo>/</mo><mi>λ</mi></mrow><mo>]</mo></mrow></mrow><msup><mrow></mrow><mi>n</mi></msup></mrow></math></mathml>。</p>
                </div>
                <div class="p1">
                    <p id="84"><b>证明</b> 设优化问题的最优解为<mathml id="124"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">w</mi><msup><mrow></mrow><mo>*</mo></msup><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mtext>a</mtext><mtext>r</mtext><mtext>g</mtext><mtext>m</mtext><mtext>i</mtext><mtext>n</mtext></mrow></mstyle><mi>w</mi></munder><mspace width="0.25em" /><mspace width="0.25em" /><mi>F</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">w</mi><mo stretchy="false">)</mo></mrow></math></mathml>。由于正则化项r(<i><b>w</b></i>)= <i>λ</i>‖<i><b>w</b></i>‖<sub>1</sub>,<i>λ</i>为正则化项与损失函数项的折衷参数。hinge损失项<mathml id="125"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>f</mi><mrow><mo>(</mo><mrow><mi mathvariant="bold-italic">w</mi><mo>;</mo><mi>x</mi><mo>,</mo><mi>y</mi></mrow><mo>)</mo></mrow><mo>=</mo><mrow><mi>max</mi></mrow><mrow><mo>{</mo><mrow><mn>0</mn><mo>,</mo><mn>1</mn><mo>-</mo><mi>y</mi><mo>〈</mo><mi mathvariant="bold-italic">w</mi><mo>,</mo><mi>x</mi><mo>〉</mo></mrow><mo>}</mo></mrow><mo>,</mo><mi mathvariant="bold-italic">w</mi></mrow></math></mathml>表示数据样本,<i>y</i>为数据对应的样本标签,(<i>x</i><sub><i>i</i></sub>,<i>y</i><sub><i>i</i></sub>)独立同分布,因此<i>F</i>(<i><b>w</b></i><sup>*</sup>)≤<i>F</i>(0)=1,即‖<i><b>w</b></i><sup>*</sup>‖<sub>1</sub>≤1/<i>λ</i>。由于‖<i><b>w</b></i><sup>*</sup>‖≤‖<i><b>w</b></i><sup>*</sup>‖<sub>1</sub>≤1/<i>λ</i>,因此求解优化问题的优化算法的解序列应满足<mathml id="126"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mi>t</mi></msub><mo>∈</mo><mi mathvariant="bold-italic">Q</mi><mo>=</mo><mrow><mrow><mo>[</mo><mrow><mo>-</mo><mn>1</mn><mo>/</mo><mi>λ</mi><mo>,</mo><mn>1</mn><mo>/</mo><mi>λ</mi></mrow><mo>]</mo></mrow></mrow><msup><mrow></mrow><mi>n</mi></msup></mrow></math></mathml>,即优化问题的约束域为<mathml id="127"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">w</mi><mo>∈</mo><mi mathvariant="bold-italic">Q</mi><mo>=</mo><mrow><mrow><mo>[</mo><mrow><mo>-</mo><mn>1</mn><mo>/</mo><mi>λ</mi><mo>,</mo><mn>1</mn><mo>/</mo><mi>λ</mi></mrow><mo>]</mo></mrow></mrow><msup><mrow></mrow><mi>n</mi></msup></mrow></math></mathml>。</p>
                </div>
                <div class="p1">
                    <p id="85">由引理3可以看出,本文研究的<i>L</i>1正则化<i>hinge</i>损失的随机优化问题具有相应的约束域。此时,令<mathml id="128"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mrow><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mi>a</mi></msub><mo>,</mo><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mi>b</mi></msub><mo>∈</mo><mi>Q</mi></mrow></munder><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mi>a</mi></msub><mo>-</mo><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mi>b</mi></msub><mo stretchy="false">∥</mo><mo>≤</mo><mi>d</mi><mo>,</mo><mi>d</mi><mo>∈</mo></mrow></math></mathml><image href="images/JSJC201910034_129.jpg" type="" display="inline" placement="inline"><alt></alt></image><sup>+</sup>,于是∃δ∈<image href="images/JSJC201910034_130.jpg" type="" display="inline" placement="inline"><alt></alt></image><sup>+</sup>满足〈ε,<i><b>w</b></i>-<i><b>w</b></i><sub><i>t</i></sub>〉≤<i>εd</i>≤<i>δ</i>。设单个样本损失的函数梯度<mathml id="131"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo>∇</mo><mi>f</mi><mrow><mo>(</mo><mrow><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mi>t</mi></msub><mo>,</mo><mi>ξ</mi></mrow><mo>)</mo></mrow><mo>=</mo><mi mathvariant="bold-italic">g</mi><msub><mrow></mrow><mi>t</mi></msub></mrow></math></mathml>,由<mathml id="132"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>E</mi><msub><mrow></mrow><mi>ξ</mi></msub><mrow><mo>[</mo><mrow><mo>∇</mo><mi>f</mi><mrow><mo>(</mo><mrow><mi mathvariant="bold-italic">w</mi><mo>,</mo><mi>ξ</mi></mrow><mo>)</mo></mrow></mrow><mo>]</mo></mrow><mo>=</mo><mo>∇</mo><mi>f</mi><mrow><mo>(</mo><mi mathvariant="bold-italic">w</mi><mo>)</mo></mrow><mo>+</mo><mi>ε</mi></mrow></math></mathml>可得:</p>
                </div>
                <div class="p1">
                    <p id="86" class="code-formula">
                        <mathml id="86"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>f</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">w</mi><mo stretchy="false">)</mo><mo>-</mo><mi>f</mi><mrow><mo>(</mo><mrow><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mi>t</mi></msub></mrow><mo>)</mo></mrow><mo>-</mo><mo>〈</mo><mi mathvariant="bold-italic">g</mi><msub><mrow></mrow><mi>t</mi></msub><mo>,</mo><mi mathvariant="bold-italic">w</mi><mo>-</mo><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mi>t</mi></msub><mo>〉</mo><mo>≤</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mrow><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">w</mi><mo>-</mo><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">∥</mo></mrow><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mi>δ</mi><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>2</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="87">其中,估计误差可以和梯度估计误差ε统一归结为偏差δ。</p>
                </div>
                <div class="p1">
                    <p id="88">根据定义1,对于有偏随机优化形式,嵌入线性插值的对偶平均方法的个体收敛速率如定理2。</p>
                </div>
                <div class="p1">
                    <p id="89"><b>定理2</b> 算法的收敛界分析:</p>
                </div>
                <div class="p1">
                    <p id="90" class="code-formula">
                        <mathml id="90"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>E</mi><mo stretchy="false">[</mo><mi>f</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">)</mo><mo>-</mo><mi>f</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mo>*</mo></msub><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mo>≤</mo><mfrac><mn>1</mn><mrow><mi>A</mi><msub><mrow></mrow><mi>t</mi></msub></mrow></mfrac><mo stretchy="false">[</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>t</mi></munderover><mrow></mrow></mstyle><mfrac><mrow><mi>a</mi><msub><mrow></mrow><mi>k</mi></msub><msup><mrow></mrow><mn>2</mn></msup></mrow><mrow><mn>2</mn><mi>γ</mi><msub><mrow></mrow><mrow><mi>k</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow></mfrac><mi>E</mi><mrow><mo>[</mo><mrow><mrow><mo stretchy="false">∥</mo><mo>∇</mo><mi>f</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">∥</mo></mrow><msup><mrow></mrow><mn>2</mn></msup></mrow><mo>]</mo></mrow><mo>+</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mi>γ</mi><msub><mrow></mrow><mi>t</mi></msub><mi>d</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">w</mi><msub><mrow></mrow><mo>*</mo></msub><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mo>+</mo><mi>δ</mi><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>3</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="91">其中,<i>δ</i>为偏差的固定收敛界。</p>
                </div>
                <div class="p1">
                    <p id="92">由引理3和定理2可知,与文献<citation id="164" type="reference">[<a class="sup">9</a>]</citation>和文献<citation id="165" type="reference">[<a class="sup">15</a>]</citation>结论不同,本文证明了线性插值技巧在梯度估计存在偏差时,偏差并不会随着迭代步数的增加而累积,收敛速率最后会收敛于某一固定界。这使得线性插值求解个体收敛速率的方法具有一定的实际意义,且不会因为现实中样本的分布或噪声与理论不符。</p>
                </div>
                <div class="p1">
                    <p id="93"><b>推论1</b> 在选取<i>a</i><sub><i>t</i></sub>=1、<mathml id="133"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>γ</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mi>Ο</mi><mo stretchy="false">(</mo><msqrt><mi>t</mi></msqrt><mo stretchy="false">)</mo></mrow></math></mathml>时,算法收敛速率能收敛到<mathml id="134"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ο</mi><mo stretchy="false">(</mo><mn>1</mn><mo>/</mo><msqrt><mi>t</mi></msqrt><mo stretchy="false">)</mo><mo>+</mo><mi>δ</mi></mrow></math></mathml>的固定界,从而偏差不会累积。</p>
                </div>
                <div class="p1">
                    <p id="94">文献<citation id="166" type="reference">[<a class="sup">19</a>]</citation>对标准正则化投影次梯度算法定义<i>proximal mapping</i>,将正则化项进行改进,得到与投影算子相似的<i>proximal mapping</i>,利用文献<citation id="167" type="reference">[<a class="sup">2</a>]</citation>方法对其最优收敛速率进行证明,得到正则化形式一阶梯度经典方法的最优收敛速率,并将其应用到对偶平均方法,从而得到正则化对偶平均方法在有偏随机优化情形下的个体收敛速率,进一步验证本文的结论。</p>
                </div>
                <h3 id="95" name="95" class="anchor-tag">4 实验结果与分析</h3>
                <div class="p1">
                    <p id="96">本节通过人工数据集和Libsvm网站上的真实数据集对线性插值随机DA优化方法进行验证。在人工数据集中,嵌入线性插值的对偶平均方法在有偏随机优化中,随机噪声的误差不会随迭代步数增加而无限累积。在真实数据集中,使用文献<citation id="168" type="reference">[<a class="sup">13</a>]</citation>和文献<citation id="169" type="reference">[<a class="sup">15</a>]</citation>算法作为对比,在5个真实库上验证本文算法的收敛性和稳定性。</p>
                </div>
                <div class="p1">
                    <p id="97">本文实验采取随机方法抽取样本,算法分别取相同的约束参数运行10次,取10次运行结果的平均值作为输出。</p>
                </div>
                <h4 class="anchor-tag" id="98" name="98">4.1 人工数据集</h4>
                <div class="p1">
                    <p id="99">为验证算法理论分析的正确性,本文给出人工数据集的描述。参考文献<citation id="170" type="reference">[<a class="sup">5</a>]</citation>和文献<citation id="171" type="reference">[<a class="sup">7</a>]</citation>中建立人工数据集的基本方法,选择解向量<i><b>w</b></i>(每一维符合正态分布<i>N</i>(0,1),再随机抽取10%分量置0),训练样本矩阵<i><b>x</b></i>∈<image href="images/JSJC201910034_135.jpg" type="" display="inline" placement="inline"><alt></alt></image><sup>10 000×10 000</sup>(每个分量也符合正态分布<i>N</i>(0,1),再每列随机抽取10%置0)。令样本<i>i</i>的标签<i>y</i><sub><i>i</i></sub>=Sgn(<i>x</i><sub><i>i</i></sub><i><b>w</b></i>),当<i>y</i><sub><i>i</i></sub>&gt;0时,置<i>y</i><sub><i>i</i></sub>=+1为正样本;当<i>y</i><sub><i>i</i></sub>&lt;0时,置<i>y</i><sub><i>i</i></sub>=-1为负样本,测试数据同理。在理论上设定得到的次梯度<i>E</i><sub><i>ξ</i></sub>[∇<i>f</i>(<i>w</i>,<i>ξ</i>)]=∇<i>f</i>(<i>w</i>)+<i>ε</i>,在实验中相应会在次梯度的值上加一个符合正态分布<i>N</i>(<i>θ</i>,1)的随机噪声<i>ε</i>。</p>
                </div>
                <div class="p1">
                    <p id="100">图1给出嵌入线性插值的对偶平均方法在梯度估计存在不同期望值大小随机噪声的收敛趋势及其不同情形下的收敛界。其中,坐标采用对数坐标系,横坐标表示迭代步数,最大值设为1 000,纵坐标表示相对目标函数值即当前值与最优值之差,最优值通过Matlab中的CVX工具箱针对现有模型计算得到,三角形标注实线RDA_OPJ1、正方形标注实线RDA_OPJ2、光滑实线RDA_OPJ3分别表示梯度估计偏差期望值为10、1和0时,线性插值对偶平均方法的个体解的收敛趋势,3条虚线分别表示其对应的bound线。可以看出,在梯度估计存在随机噪声干扰的人工数据集上,随着迭代步数不断增加,该算法加入3种期望值随机噪声的曲线最终都收敛于某一定值,且低于其收敛界对应的bound线,不会累积或发散。偏差期望值为1的曲线相比无偏差情形在前期波动较大,但最终会收敛,与无偏差相距较小。偏差为10的曲线波动不大,但由于偏差值的影响相比另外2种收敛速率较慢且精度较低。实验结果验证了本文结论的正确性。</p>
                </div>
                <div class="area_img" id="101">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201910034_101.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZhcFFuenlFZmNsZnBPdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 人工数据集收敛速率对比" src="Detail/GetImg?filename=images/JSJC201910034_101.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZhcFFuenlFZmNsZnBPdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图1 人工数据集收敛速率对比</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201910034_101.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZhcFFuenlFZmNsZnBPdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="102" name="102">4.2 真实数据集</h4>
                <div class="p1">
                    <p id="103">本节对线性插值对偶平均方法的个体收敛性和实时稳定性进行验证,使用文献<citation id="172" type="reference">[<a class="sup">13</a>]</citation>提出的嵌入加速算法步长策略的投影次梯度(PGD_step)方法和文献<citation id="173" type="reference">[<a class="sup">15</a>]</citation>提出的随机加速(SFGM)方法进行对比,采用来自台湾大学Libsvm网站的5个真实数据集:a9a,astro,rcv1,covtype,CCAT。表1给出真实数据集的基本描述。</p>
                </div>
                <div class="area_img" id="104">
                    <p class="img_tit"><b>表1 真实数据集描述</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="104" border="1"><tr><td>数据集</td><td>训练样本</td><td>测试样本</td><td>维数</td></tr><tr><td>a9a</td><td>24 703</td><td>7 858</td><td>123</td></tr><tr><td><br />astro</td><td>29 882</td><td>32 487</td><td>99 757</td></tr><tr><td><br />rcv1</td><td>20 242</td><td>677 399</td><td>47 236</td></tr><tr><td><br />covtype</td><td>522 911</td><td>58 101</td><td>54</td></tr><tr><td><br />CCAT</td><td>23 149</td><td>781 265</td><td>47 236</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="105">3种算法的收敛速率对比如图2所示。其中,横、纵坐标含义与图1相同,迭代最大值设为10 000。基准集目标函数最优值未知,选取各算法迭代10次平均的最小函数值作为目标函数最优值。</p>
                </div>
                <div class="area_img" id="106">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201910034_106.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZhcFFuenlFZmNsZnBPdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 真实数据集收敛速率对比" src="Detail/GetImg?filename=images/JSJC201910034_106.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZhcFFuenlFZmNsZnBPdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图2 真实数据集收敛速率对比</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201910034_106.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZhcFFuenlFZmNsZnBPdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="107">从图2可以看出,基于非光滑目标函数的嵌入加速算法步长策略的投影次梯度算法具有更快且精度更高的收敛速率,稳定性较好。与PGD_step方法和SFGM方法相比,线性插值对偶平均方法的收敛速率较快,且收敛精度高,验证了本文方法的正确性。线性插值对偶平均方法对于少数真实数据集,如a9a和covtype,稳定性会出现波动,这是因为使用个体作为输出。综上,实验验证了本文方法理论分析的正确性,且具有实际应用意义<citation id="174" type="reference"><link href="41" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>。</p>
                </div>
                <h3 id="108" name="108" class="anchor-tag">5 结束语</h3>
                <div class="p1">
                    <p id="109">本文提出一种梯度有偏随机DA优化方法。基于递归思想对凸条件下对偶平均方法的最优收敛速率进行证明。结合线性插值,将DA应用于非光滑有偏随机优化方法,得到收敛于固定值的个体收敛界,且偏差不会随迭代步数的增加而累积。对非光滑有偏随机优化问题进行验证,结果表明,线性插值对偶平均方法的收敛速度较快,且收敛精度较高。后续考虑将加速算法应用于有偏随机优化问题,进一步提高个体的收敛速度和精度。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SPQD&amp;filename=SPQD00004301960&amp;v=MjgxMTZZM2s1ekJkaDRqOTlTWHFScnhveGNNSDdSN3FlYnVkdEZDdmxWcnZJSlYwPU5qM2Fhck80SHRISXJJOUViZTBQ&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZhcFFuenlFZmNsZnBPdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> NESTEROV Y.Primal-dual subgradient methods for convex problems[J].Mathematical Programming,2009,120(1):221-259.
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Convex analysis and optimization">

                                <b>[2]</b> BERTSEKAS D P,NEDI A,OZDAGLAR A E.Convex analysis and optimization[M].Berlin,Germany:Springer,2003.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012300467851&amp;v=MTk2MjVycVFUTW53WmVadUh5am1VTHpJSlZvVWJ4RT1OaWZPZmJLN0h0RE9ySTlGWU8wSUJIazRvQk1UNlQ0UFFIL2lyUmRHZQ==&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZhcFFuenlFZmNsZnBPdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> BECK A,TEBOULLE M.Mirror descent and nonlinear projected sub-gradient methods for convex optimization[J].Operations Research Letters,2003,31(3):167-175.
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Solving large scale linear prediction problems using stochastic gradient descent algorithms">

                                <b>[4]</b> ZHANG Tong.Solving large scale linear prediction problems using stochastic gradient descent algorithms[C]//Proceedings of the 21st International Conference on Machine Learning.New York,USA:ACM Press,2004:919-926.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Convergence rates of inexact proximal-gradient methods for convex optimization">

                                <b>[5]</b> SCHMIDT M,ROUX N L,BACH F.Convergence rates of inexact proximal gradient methods for convex optimization[J].Advances in Neural Information Processing Systems,2011,24:1458-1466.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Stochastic first order methods in smooth convex optimization">

                                <b>[6]</b> DEVOLDER O.Stochastic first order methods in smooth convex optimization[EB/OL].[2018-07-20].https://core.ac.uk/download/pdf/34135646.pdf.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Convergence rates of biased stochastic optimization for learning sparse ising models">

                                <b>[7]</b> HONORIO J.Convergence rates of biased stochastic optimization for learning sparse ising models[C]//Proceedings of International Conference on Machine Learning.Madison,USA:Omnipress,2012:257-264.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Smooth optimization with approximate gradient">

                                <b>[8]</b> DASPREMONT A.Smooth optimization with approximate gradient[J].SIAM Journal on Optimization,2008,19(3):1171-1183.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD14072400004581&amp;v=MDY5ODZadUh5am1VTHpJSlZvVWJ4RT1OajdCYXJLOEh0Yk9xNDlGWk9zTENYUTRvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZQ==&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZhcFFuenlFZmNsZnBPdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b> DEVOLDER O,GLINEUR F,NESTEROV Y.First-order methods of smooth convex optimization with inexact oracle[J].Mathematical Programming,2014,146(1/2):37-75.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Making gradient descent optimal for strongly convex stochastic optimization">

                                <b>[10]</b> RAKHLIN A,SHAMIR O,SRIDHARAN K.Making gradient descent optimal for strongly convex stochastic optimiza-tion[C]//Proceedings of International Conference on Machine Learning.Madison,USA:Omnipress,2012:1571-1578.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Stochastic gradient descent for non-smooth optimization:Convergence results and optimal averaging schemes">

                                <b>[11]</b> SHAMIR O,ZHANG Tong.Stochastic gradient descent for non-smooth optimization:convergence results and optimal averaging schemes[C]//Proceedings of International Conference on Machine Learning.Madison,USA:Omnipress,2013:71-79.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Quasi-monotone subgradient methods for nonsmooth convex minimization">

                                <b>[12]</b> NESTEROV Y,SHIKHMAN V.Quasi-monotone subgradient methods for nonsmooth convex minimization[J].Journal of Optimization Theory and Applications,2015,165(3):917-940.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_13" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJX201801010&amp;v=MDE0MDFIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJ0RnkzaFU3ckxMejdCZHJHNEg5bk1ybzlFWklRS0Q=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZhcFFuenlFZmNsZnBPdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[13]</b> 陶蔚,潘志松,储德军,等.使用Nesterov步长策略投影次梯度方法的个体收敛性[J].计算机学报,2018,41(1):164-176.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Composite objective mirror descent">

                                <b>[14]</b> DUCHI J,SHALEV-SHWARTZ S,SINGER Y,et al.Composite objective mirror descent[EB/OL].[2018-07-20].http://web.stanford.edu/～jduchi/projects/DuchiShSiTe10.pdf.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A method of solving a convex programming problem with convergence rate O (1/k2)">

                                <b>[15]</b> NESTEROV Y.A method of solving a convex programming problem with convergence rate O(1/k2)[J].Soviet Mathematics Doklady,1983,27(2):372-376.
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Accelerated gradient methods for stochastic optimization and online learning">

                                <b>[16]</b> HU Chonghai,KWOK J T,PAN W.Accelerated gradient methods for stochastic optimization and online learning[C]//Proceedings of the 22nd International Conference on Neural Information Processing Systems.[S.l.]:Curran Associates Inc.,2009:781-789.
                            </a>
                        </p>
                        <p id="35">
                            <a id="bibliography_17" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00003749915&amp;v=MTU5NzJyeG94Y01IN1I3cWVidWR0RkN2bFZydklKVjA9Tmo3QmFyTzRIdEhQcUl0TWJlb0tZM2s1ekJkaDRqOTlTWHFS&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZhcFFuenlFZmNsZnBPdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[17]</b> TSENG P.Approximation accuracy,gradient methods,and error bound for structured convex optimization[J].Mathematical Programming,2010,125(2):263-295.
                            </a>
                        </p>
                        <p id="37">
                            <a id="bibliography_18" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201703007&amp;v=MDAwNDRSTE9lWmVSdEZ5M2hVN3JMTHl2U2RMRzRIOWJNckk5Rlk0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1U=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZhcFFuenlFZmNsZnBPdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[18]</b> 陶蔚,潘志松,朱小辉,等.线性插值投影次梯度方法的最优个体收敛速率[J].计算机研究与发展,2017,54(3):529-536.
                            </a>
                        </p>
                        <p id="39">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Proximal Stochastic Gradient Method with Progressive Variance Reduction">

                                <b>[19]</b> XIAO Lin,ZHANG Tong.A proximal stochastic gradient method with progressive variance reduction[J].SIAM Journal on Optimization,2014,24(4):2057-2075.
                            </a>
                        </p>
                        <p id="41">
                            <a id="bibliography_20" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CBBD&amp;filename=000730203964X999&amp;v=MTU0NjduS3JpZlp1OXVGQ3ZtVTdyTUlGc1hWVjI3R2JDNEhOSFBwb2xCRE9JR0JSTTh6eFVTbURkOVNIN24zeEU5ZmJ2&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZhcFFuenlFZmNsZnBPdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[20]</b> 瓦普尼克.统计学习理论的本质[M].张学工,译.北京:清华大学出版社,2000.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJC201910034" />
        <input id="dpi" type="hidden" value="600" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZhcFFuenlFZmNsZnBPdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201910034&amp;v=MjY4OTVHWUlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJ0RnkzaFU3cklMejdCYmJHNEg5ak5yNDk=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3U0ZhcFFuenlFZmNsZnBPdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
