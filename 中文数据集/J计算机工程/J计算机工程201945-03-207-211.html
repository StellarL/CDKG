<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637130634533712500%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJC201903035%26RESULT%3d1%26SIGN%3dBkwLMWg146aCO%252bxhVLwUunXeCbc%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201903035&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201903035&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201903035&amp;v=MjQ3NjNVUkxPZVplUm9GeTdsVXJyQUx6N0JiYkc0SDlqTXJJOUdZWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckM=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#45" data-title="0 概述 ">0 概述</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#51" data-title="1 特征提取与融合 ">1 特征提取与融合</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#53" data-title="1.1 行人身体节点定位">1.1 行人身体节点定位</a></li>
                                                <li><a href="#58" data-title="1.2 基于身体节点的区域划分">1.2 基于身体节点的区域划分</a></li>
                                                <li><a href="#68" data-title="1.3 网络架构">1.3 网络架构</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#72" data-title="2 距离度量方法 ">2 距离度量方法</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#74" data-title="2.1 基于&lt;i&gt;k&lt;/i&gt;倒排近邻的马氏距离">2.1 基于<i>k</i>倒排近邻的马氏距离</a></li>
                                                <li><a href="#90" data-title="2.2 基于&lt;i&gt;k&lt;/i&gt;倒排近邻的杰卡德距离">2.2 基于<i>k</i>倒排近邻的杰卡德距离</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#102" data-title="3 实验与结果分析 ">3 实验与结果分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#104" data-title="3.1 训练和测试数据库">3.1 训练和测试数据库</a></li>
                                                <li><a href="#106" data-title="3.2 网络收敛">3.2 网络收敛</a></li>
                                                <li><a href="#108" data-title="3.3 结果分析">3.3 结果分析</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#123" data-title="4 结束语 ">4 结束语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#57" data-title="&lt;b&gt;图1 身体节点定位效果&lt;/b&gt;"><b>图1 身体节点定位效果</b></a></li>
                                                <li><a href="#67" data-title="&lt;b&gt;图2 身体区域划分示意图&lt;/b&gt;"><b>图2 身体区域划分示意图</b></a></li>
                                                <li><a href="#71" data-title="&lt;b&gt;图3 特征提取与特征融合框架&lt;/b&gt;"><b>图3 特征提取与特征融合框架</b></a></li>
                                                <li><a href="#111" data-title="&lt;b&gt;表1 在Market-1501中利用马氏距离排序结果&lt;/b&gt; %"><b>表1 在Market-1501中利用马氏距离排序结果</b> %</a></li>
                                                <li><a href="#112" data-title="&lt;b&gt;表2 在VIPeR中利用马氏距离排序结果&lt;/b&gt; %"><b>表2 在VIPeR中利用马氏距离排序结果</b> %</a></li>
                                                <li><a href="#116" data-title="&lt;b&gt;表3 在VIPeR不同距离度量下的准确率&lt;/b&gt; %"><b>表3 在VIPeR不同距离度量下的准确率</b> %</a></li>
                                                <li><a href="#117" data-title="&lt;b&gt;表4 在Market-1501不同距离度量下的准确率&lt;/b&gt; %"><b>表4 在Market-1501不同距离度量下的准确率</b> %</a></li>
                                                <li><a href="#121" data-title="&lt;b&gt;图4 在VIPeR数据库中&lt;i&gt;k&lt;/i&gt;=5出错样本排序&lt;/b&gt;"><b>图4 在VIPeR数据库中<i>k</i>=5出错样本排序</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="15">


                                    <a id="bibliography_1" title=" 俞婧, 仇春春, 王恬, 等.基于距离匹配的行人再识别技术综述[J].微处理机, 2016, 37 (3) :77-80." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=WCLJ201603021&amp;v=MDc3MjZPZVplUm9GeTdsVXJyQU1pN0haTEc0SDlmTXJJOUhaWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkw=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         俞婧, 仇春春, 王恬, 等.基于距离匹配的行人再识别技术综述[J].微处理机, 2016, 37 (3) :77-80.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_2" title=" OJALA T, PIETIKAINEN M, MAENPAA T.Multi-resolution gray-scale and rotation invariant texture classification with local binary patterns[J].Classification with Local Binary Patterns, 2002, 24 (7) :971-987." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multiresolution gray-scale and rotation invariant texture classification with local binary patterns">
                                        <b>[2]</b>
                                         OJALA T, PIETIKAINEN M, MAENPAA T.Multi-resolution gray-scale and rotation invariant texture classification with local binary patterns[J].Classification with Local Binary Patterns, 2002, 24 (7) :971-987.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_3" title=" FARENZENA M, BAZZANI L, PERINA A, et al.Person re-identification by symmetry-driven accumulation of local features[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2010:2360-2367." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Person reidentification by symmetry-driven accumulation of local features">
                                        <b>[3]</b>
                                         FARENZENA M, BAZZANI L, PERINA A, et al.Person re-identification by symmetry-driven accumulation of local features[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2010:2360-2367.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_4" title=" CHENG D S, CRISTANI M, STOPPA M, et al.Custom pictorial structures for re-identification[C]//Proceedings of the British Machine Vision Conference.Washington D.C., USA:IEEE Press, 2011:1-11." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Custom pictorial structures for re-identification">
                                        <b>[4]</b>
                                         CHENG D S, CRISTANI M, STOPPA M, et al.Custom pictorial structures for re-identification[C]//Proceedings of the British Machine Vision Conference.Washington D.C., USA:IEEE Press, 2011:1-11.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_5" title=" LI W, ZHAO R, XIAO T, et al.DeepReID:deep filter pairing neural network for person re-identification[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C.USA:IEEE Press, 2014:152-159." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep Re ID:deep filter pairing neural network for person re-identification">
                                        <b>[5]</b>
                                         LI W, ZHAO R, XIAO T, et al.DeepReID:deep filter pairing neural network for person re-identification[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C.USA:IEEE Press, 2014:152-159.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_6" title=" AHMED E, JONES M J, MARKS T K.An improved deep learning architecture for person re-identification[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2015:3908-3916." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=An improved deep learning architecture for person re-identification">
                                        <b>[6]</b>
                                         AHMED E, JONES M J, MARKS T K.An improved deep learning architecture for person re-identification[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2015:3908-3916.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_7" title=" SZEGEDY C, LIU W, JIA Y Q, et al.Going deeper with convolutions[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2015:1-9." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Going deeper with convolutions">
                                        <b>[7]</b>
                                         SZEGEDY C, LIU W, JIA Y Q, et al.Going deeper with convolutions[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2015:1-9.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_8" title=" CHENG D, GONG Y H, ZHOU S P, et al.Person re-identification by multi-channel parts-based CNN with improved triplet loss function[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2016:1335-1344." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Person re-identification by multi-channel parts-based cnn with improved triplet loss function">
                                        <b>[8]</b>
                                         CHENG D, GONG Y H, ZHOU S P, et al.Person re-identification by multi-channel parts-based CNN with improved triplet loss function[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2016:1335-1344.
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_9" title=" XIAO T, LI H, OUYANG W, et al.Learning deep feature representations with domain guided dropout for person re-identification[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2016:1249-1258." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning deep feature representations with domain guided dropout for person reidentification">
                                        <b>[9]</b>
                                         XIAO T, LI H, OUYANG W, et al.Learning deep feature representations with domain guided dropout for person re-identification[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2016:1249-1258.
                                    </a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_10" title=" WU S X, CHEN Y C, LI X, et al.An enhanced deep feature representation for person re-identification[C]//Proceedings of IEEE Winter Conference on Applications of Computer Vision.Washington D.C., USA:IEEE Press, 2016:1-8." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=An enhanced deep feature representation for person re-identification">
                                        <b>[10]</b>
                                         WU S X, CHEN Y C, LI X, et al.An enhanced deep feature representation for person re-identification[C]//Proceedings of IEEE Winter Conference on Applications of Computer Vision.Washington D.C., USA:IEEE Press, 2016:1-8.
                                    </a>
                                </li>
                                <li id="35">


                                    <a id="bibliography_11" title=" ZHAO H Y, TIAN M Q, SUN S Y, et al.Spindle net:person re-identification with human body region guided feature decomposition and fusion[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2017:907-915." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Spindle Net:Person Re-identification with Human Body Region Guided Feature Decomposition and Fusion">
                                        <b>[11]</b>
                                         ZHAO H Y, TIAN M Q, SUN S Y, et al.Spindle net:person re-identification with human body region guided feature decomposition and fusion[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2017:907-915.
                                    </a>
                                </li>
                                <li id="37">


                                    <a id="bibliography_12" title=" ZHONG Z, ZHENG L, CAO D L, et al.Re-ranking person re-identification with k-reciprocal encoding[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2017:3652-3661." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Re-ranking person re-identification with k-reciprocal encoding">
                                        <b>[12]</b>
                                         ZHONG Z, ZHENG L, CAO D L, et al.Re-ranking person re-identification with k-reciprocal encoding[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2017:3652-3661.
                                    </a>
                                </li>
                                <li id="39">


                                    <a id="bibliography_13" title=" 宋宗涛, 陈岳林, 蔡晓东.基于多分块三重损失计算的行人识别方法[J].电子与信息学报, 2016, 38 (6) :1528-1535." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DSSS2017Z4040&amp;v=MjY4MThyQUlUN1lmYkc0SDlhbXE0OUJaSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm9GeTdsVXI=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[13]</b>
                                         宋宗涛, 陈岳林, 蔡晓东.基于多分块三重损失计算的行人识别方法[J].电子与信息学报, 2016, 38 (6) :1528-1535.
                                    </a>
                                </li>
                                <li id="41">


                                    <a id="bibliography_14" title=" QIN D F, GAMMETER S, BOSSARD L, et al.Hello neighbor:accurate object retrieval with k-reciprocal nearest neighbors[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2011:777-784." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Hello Neighbor:Accurate Object Retrieval with K-Reciprocal Nearest Neighbors">
                                        <b>[14]</b>
                                         QIN D F, GAMMETER S, BOSSARD L, et al.Hello neighbor:accurate object retrieval with k-reciprocal nearest neighbors[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2011:777-784.
                                    </a>
                                </li>
                                <li id="43">


                                    <a id="bibliography_15" title=" ZHENG L, SHEN L Y, TIAN L, et al.Scalable person re-identification:a benchmark[C]//Proceedings of IEEE Conference on Computer Vision.Washington D.C., USA:IEEE Press, 2015:1116-1124." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Scalable person reidentification:a benchmark">
                                        <b>[15]</b>
                                         ZHENG L, SHEN L Y, TIAN L, et al.Scalable person re-identification:a benchmark[C]//Proceedings of IEEE Conference on Computer Vision.Washington D.C., USA:IEEE Press, 2015:1116-1124.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJC" target="_blank">计算机工程</a>
                2019,45(03),207-211 DOI:10.19678/j.issn.1000-3428.0050802            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于区域特征对齐与<i>k</i>倒排编码的行人再识别方法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BA%93%E6%B5%A9%E5%8D%8E&amp;code=39355507&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">库浩华</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%91%A8%E8%90%8D&amp;code=11151226&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">周萍</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%94%A1%E6%99%93%E4%B8%9C&amp;code=24371077&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">蔡晓东</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%A8%E6%B5%B7%E7%87%95&amp;code=11481791&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">杨海燕</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%A2%81%E6%99%93%E6%9B%A6&amp;code=37810906&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">梁晓曦</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%A1%82%E6%9E%97%E7%94%B5%E5%AD%90%E7%A7%91%E6%8A%80%E5%A4%A7%E5%AD%A6%E4%BF%A1%E6%81%AF%E4%B8%8E%E9%80%9A%E4%BF%A1%E5%AD%A6%E9%99%A2&amp;code=0269119&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">桂林电子科技大学信息与通信学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%A1%82%E6%9E%97%E7%94%B5%E5%AD%90%E7%A7%91%E6%8A%80%E5%A4%A7%E5%AD%A6%E7%94%B5%E5%AD%90%E5%B7%A5%E7%A8%8B%E4%B8%8E%E8%87%AA%E5%8A%A8%E5%8C%96%E5%AD%A6%E9%99%A2&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">桂林电子科技大学电子工程与自动化学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>在行人再识别过程中, 由于行人姿态变化会导致图像之间对应位置存在身体区域不对齐的问题, 从而降低识别准确率。为此, 设计一种新的行人再识别方法。利用卷积神经结构计算行人图像的响应图, 根据响应图中的极值点定位行人身体节点, 并以此划分特征区域, 将提取的各个区域的特征进行融合得到特征表示。在比对距离度量上通过引入<i>k</i>倒排近邻使更多的正样本包含在近邻中, 在杰卡德距离中将<i>k</i>倒排近邻集编码成向量以减少计算量, 使得越近的邻域获得越大的权重。实验结果表明, 相比于对整幅行人图像提取特征方法与单独使用马氏距离的方法, 该方法能有效提高行人再识别的准确率。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%BA%AB%E4%BD%93%E8%8A%82%E7%82%B9&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">身体节点;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%BA%AB%E4%BD%93%E5%8C%BA%E5%9F%9F&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">身体区域;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%3Ci%3Ek%3C%2Fi%3E%E5%80%92%E6%8E%92%E8%BF%91%E9%82%BB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank"><i>k</i>倒排近邻;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%9D%B0%E5%8D%A1%E5%BE%B7%E8%B7%9D%E7%A6%BB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">杰卡德距离;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%A1%8C%E4%BA%BA%E5%86%8D%E8%AF%86%E5%88%AB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">行人再识别;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    库浩华 (1993—) , 男, 硕士研究生, 主研方向为图像处理;;
                                </span>
                                <span>
                                    周萍, 教授;;
                                </span>
                                <span>
                                    *蔡晓东 (通信作者) , 教授、博士;;
                                </span>
                                <span>
                                    杨海燕, 副教授、博士研究生;;
                                </span>
                                <span>
                                    梁晓曦, 硕士研究生。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-03-15</p>

                    <p>

                            <b>基金：</b>
                                                        <span>“认知无线电与信息处理”省部共建教育部重点实验室基金 (CRKL160102);</span>
                                <span>广西重点研发计划 (桂科AB16380264);</span>
                    </p>
            </div>
                    <h1><b>Person Re-identification Method Based on Regional Feature Alignment and <i>k</i>-reciprocal Encoding</b></h1>
                    <h2>
                    <span>KU Haohua</span>
                    <span>ZHOU Ping</span>
                    <span>CAI Xiaodong</span>
                    <span>YANG Haiyan</span>
                    <span>LIANG Xiaoxi</span>
            </h2>
                    <h2>
                    <span>School of Information and Communication, Guilin University of Electronic Technology</span>
                    <span>School of Electronic Engineering and Automation, Guilin University of Electronic Technology</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>In the process of person re-identification, pose variations lead to a problem of misalignment between spatial areas of images, which results in low recognition rate.So this paper proposes a new person re-identification method.Firstly, a convolution neural structure is utilized to calculate responding maps of person images, and the body joints are located according to the extreme points in the responding maps.Secondly, body sub-regions are divided according to the positions of body joints, and they are aligned before further feature extraction.Finally, the features of body sub-regions are fused for identification.By introducing the <i>k</i>-reciprocal nearest neighbor method, more positive samples can be included in the nearest neighbors.With the Jaccard distance, the computation cost is reduced by encoding the <i>k</i>-reciprocal nearest neighbor sets into a vector, which assigns larger weights to closer neighbors.Experimental results show that compared with the feature extraction from the whole person image and using Mahalanobis distance alone, the proposed method can improve the accuracy of person re-identification significantly.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=body%20joints&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">body joints;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=body%20sub-regions&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">body sub-regions;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%3Ci%3Ek%3C%2Fi%3E-reciprocal%20nearest%20neighbor&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank"><i>k</i>-reciprocal nearest neighbor;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Jaccard%20distance&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Jaccard distance;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=person%20re-identification&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">person re-identification;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-03-15</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="45" name="45" class="anchor-tag">0 概述</h3>
                <div class="p1">
                    <p id="46">目前, 行人再识别的研究主要是提取更具鲁棒性的目标行人特征表示, 并通过学习获取一个有判别力的距离度量<citation id="125" type="reference"><link href="15" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>函数。行人再识别是找出2个跨场景摄像头或同一摄像头跨时间下的同一行人, 其需要解决的问题包括拍摄角度改变、光照影响、背景杂乱及遮挡、行人姿势变化、非同一行人高度的相似性等。</p>
                </div>
                <div class="p1">
                    <p id="47">在传统手工特征提取方面, LAB、HSV颜色直方图和高维LBP<citation id="126" type="reference"><link href="17" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>都能较好地描述图像的特征, 而通过将两者相结合<citation id="127" type="reference"><link href="19" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>能够获得更准确、更完整的特征描述。文献<citation id="128" type="reference">[<a class="sup">3</a>]</citation>基于行人身体对称性, 通过预处理将</p>
                </div>
                <div class="p1">
                    <p id="48">行人划分为头部、躯干、腿部以及左右对称中轴, 提取各区域的颜色、纹理等多种特征。文献<citation id="129" type="reference">[<a class="sup">4</a>]</citation>将绘画结构应用于行人再识别, 通过一个自适应的身体外形结构来表示行人, 主要包括头部、胸部、大腿和小腿, 提取每个部分的颜色特征进行匹配。</p>
                </div>
                <div class="p1">
                    <p id="49">随着深度学习的发展, 卷积神经网络 (Convolutional Neural Network, CNN) 在图像识别和分类方面有了较大的进展。文献<citation id="130" type="reference">[<a class="sup">5</a>]</citation>应用深度学习的模型处理行人再识别的特征提取问题, 并通过Patch Matching Layer和Max-out Pooling Layer识别行人的视觉和姿势变化。文献<citation id="131" type="reference">[<a class="sup">6</a>]</citation>使用专门设计的层比较2幅图像对应区域的邻域差, 得到较好的识别效果。文献<citation id="132" type="reference">[<a class="sup">7</a>]</citation>提出Inception结构, 在加深加宽网络结构的同时减少网络参数, 获得了较好的特征提取能力。文献<citation id="133" type="reference">[<a class="sup">8</a>]</citation>提出一种多通道CNN网络模型, 该网络模型能学习到行人的局部和全局特征, 并将2个特征连接在一起。文献<citation id="134" type="reference">[<a class="sup">9</a>]</citation>提出DGD算法, 将不同的数据库放在同一CNN中训练, 得到不同的影响值, 解决了不同数据库背景因素的干扰。文献<citation id="135" type="reference">[<a class="sup">10</a>]</citation>提出FFN结合手工提取的特征, 利用CNN网络的自我学习、反向传播获得更完善的向量空间。文献<citation id="136" type="reference">[<a class="sup">11</a>]</citation>提出一种利用身体节点划分区域的方法, 有效解决了由于行人姿态变化带来的区域不对齐的问题。文献<citation id="137" type="reference">[<a class="sup">12</a>]</citation>提出利用<i>k</i>倒排编码对马氏距离的排序进行优化, 从而提升准确率。</p>
                </div>
                <div class="p1">
                    <p id="50">本文采用一种基于区域特征对齐的行人再识别方法, 对行人的身体节点进行定位, 利用定位好的节点划分身体区域。在距离度量上使用马氏距离和杰卡德距离的加权, 采用<i>k</i>倒排编码以使得更多的正样本靠前排序。</p>
                </div>
                <h3 id="51" name="51" class="anchor-tag">1 特征提取与融合</h3>
                <div class="p1">
                    <p id="52">深度学习模型一般通过增加CNN层数的方式提升网络的性能以提高行人再识别的准确率。CNN能够提取各维度的特征, 网络的层数越多, 提取到各维度的特征越丰富。并且, 越深的网络提取的特征越抽象, 越具有语义信息。但网络层数的加深, 会造成梯度弥散或梯度爆炸, 令随机梯度下降算法的优化变得更加困难, 导致训练难以收敛, 模型不能达到较好的学习效果。同时, 层数过多也会导致训练误差加大。因此, 本文通过区域特征对齐来提取更丰富的行人信息。</p>
                </div>
                <h4 class="anchor-tag" id="53" name="53">1.1 行人身体节点定位</h4>
                <div class="p1">
                    <p id="54">常见的行人再识别方法通过对行人的整幅图像提取特征, 然后进行距离比较。文献<citation id="138" type="reference">[<a class="sup">13</a>]</citation>通过将行人图像按比率划分为上、中、下3个部分, 分别对应行人的头部、身体、腿部, 提取各部分的特征后进行融合。然而由于检测效果和行人姿态变化导致划分的区域存在对应身体部位不对齐的问题, 该方法识别率较低。本文利用卷积结构将行人图像作为输入, 计算每个节点的响应图, 通过响应图的极值点获得身体的14个节点, 计算公式如下:</p>
                </div>
                <div class="p1">
                    <p id="55" class="code-formula">
                        <mathml id="55"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mo stretchy="false">[</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">]</mo><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mtext>a</mtext><mtext>r</mtext><mtext>g</mtext><mtext>m</mtext><mtext>a</mtext><mtext>x</mtext></mrow></mstyle><mrow><mi>x</mi><mo>∈</mo><mo stretchy="false">[</mo><mn>1</mn><mo>, </mo><mi>X</mi><mo stretchy="false">]</mo><mo>, </mo><mi>y</mi><mo>∈</mo><mo stretchy="false">[</mo><mn>1</mn><mo>, </mo><mi>Y</mi><mo stretchy="false">]</mo></mrow></munder><mi>F</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="56">其中, <i>X</i>、<i>Y</i>分别表示特征图的水平和垂直方向的维度, <i>P</i><sub><i>i</i></sub>表示节点位置。图1是通过响应图的极值点计算获得身体节点位置的效果, 包括14个关键节点, 其中节点的分布主要在行人的头部、手臂以及腿部。</p>
                </div>
                <div class="area_img" id="57">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201903035_057.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 身体节点定位效果" src="Detail/GetImg?filename=images/JSJC201903035_057.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图1 身体节点定位效果</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201903035_057.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="58" name="58">1.2 基于身体节点的区域划分</h4>
                <div class="p1">
                    <p id="59">根据图1定位的身体节点, 将行人身体区域划分为3个大块和4个小块。3个大块为<i>S</i><mathml id="60"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>1</mn><mi>A</mi></msubsup></mrow></math></mathml>=[1, 2, 3, 4]、<i>S</i><mathml id="61"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>2</mn><mi>A</mi></msubsup></mrow></math></mathml>=[3, 4, 5, 6, 7, 8, 9, 10]、<i>S</i><mathml id="62"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>3</mn><mi>A</mi></msubsup></mrow></math></mathml>=[9, 10, 11, 12, 13, 14], 分别对应行人的头肩部分、身体上半部分、身体下半部分;4个小块为<i>S</i><mathml id="63"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>1</mn><mi>B</mi></msubsup></mrow></math></mathml>=[3, 5, 6]、<i>S</i><mathml id="64"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>2</mn><mi>B</mi></msubsup></mrow></math></mathml>=[4, 7, 8]、<i>S</i><mathml id="65"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>3</mn><mi>B</mi></msubsup></mrow></math></mathml>=[9, 11, 12]、<i>S</i><sup><i>B</i></sup><sub>4</sub>=[10, 13, 14], 分别对应行人的左、右手臂和左、右腿。</p>
                </div>
                <div class="p1">
                    <p id="66">图2所示为利用行人身体节点将行人划分成7个不同的区域, 各区域间存在重叠现象, 且所有区域覆盖整个行人图像。</p>
                </div>
                <div class="area_img" id="67">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201903035_067.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 身体区域划分示意图" src="Detail/GetImg?filename=images/JSJC201903035_067.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图2 身体区域划分示意图</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201903035_067.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="68" name="68">1.3 网络架构</h4>
                <div class="p1">
                    <p id="69">本文的网络结构包括特征提取和特征融合2个部分。在特征提取阶段, 7个子区域和1个全身区域各自提取特征形成256维的特征向量。在特征融合阶段, 8个256维的特征向量形成一个紧凑的特征表达。最终形成的特征表达融合了行人的全局特征和局部特征, 能更丰富地表征行人。</p>
                </div>
                <div class="p1">
                    <p id="70">由于划分的7个区域携带的信息不同, 需要在不同的阶段进行特征提取。在第一阶段, 经卷积运算将3个大块区域特征 (<i>F</i><sub>1</sub>、<i>F</i><sub>2</sub>、<i>F</i><sub>3</sub>) 从整个特征图中分离;在第二阶段, 经卷积运算将4个小块区域特征 (<i>F</i><sub>4</sub>、<i>F</i><sub>5</sub>、<i>F</i><sub>6</sub>、<i>F</i><sub>7</sub>) 从整个特征图中分离。特征融合与特征提取的顺序相反, 4个小块区域提取的特征先进行融合再与3个大块区域提取的特征结合。整个算法的框架如图3所示。</p>
                </div>
                <div class="area_img" id="71">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201903035_071.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 特征提取与特征融合框架" src="Detail/GetImg?filename=images/JSJC201903035_071.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图3 特征提取与特征融合框架</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201903035_071.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h3 id="72" name="72" class="anchor-tag">2 距离度量方法</h3>
                <div class="p1">
                    <p id="73">行人再识别算法根据获得的特征表达对图像之间匹配度进行判断。度量学习研究表明, 合适的距离度量才能获得较好的匹配效果。</p>
                </div>
                <h4 class="anchor-tag" id="74" name="74">2.1 基于<i>k</i>倒排近邻的马氏距离</h4>
                <div class="p1">
                    <p id="75">设置行人再识别测试集, 其中有<i>a</i>、<i>b</i> 2个子库, 分别存放同一行人的不同图片。选择<i>a</i>库中一张图片为目标行人, 将<i>b</i>库中的图片与其进行距离匹配, 则<i>b</i>库中的图片与目标行人的马氏距离计算公式如下:</p>
                </div>
                <div class="p1">
                    <p id="76"><i>d</i> (<i>p</i>, <i>g</i><sub><i>i</i></sub>) = (<b><i>x</i></b><sub><i>p</i></sub>-<b><i>x</i></b><sub><i>g</i><sub><i>i</i></sub></sub>) <sup>T</sup><b><i>M</i></b> (<b><i>x</i></b><sub><i>p</i></sub>-<b><i>x</i></b><sub><i>g</i><sub><i>i</i></sub></sub>) </p>
                </div>
                <div class="p1">
                    <p id="77">其中, <b><i>x</i></b><sub><i>p</i></sub>和<b><i>x</i></b><sub><i>g</i><sub><i>i</i></sub></sub>分别代表<i>a</i>库中目标行人和<i>b</i>库中行人的特征, <b><i>M</i></b>代表半正定矩阵。按距离从近到远进行排序, 则有如下排列:</p>
                </div>
                <div class="p1">
                    <p id="78" class="code-formula">
                        <mathml id="78"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><mo stretchy="false"> (</mo><mi>p</mi><mo>, </mo><mi>G</mi><mo stretchy="false">) </mo><mo>=</mo><mrow><mo>{</mo><mrow><mi>g</mi><msubsup><mrow></mrow><mn>1</mn><mn>0</mn></msubsup><mo>, </mo><mi>g</mi><msubsup><mrow></mrow><mn>2</mn><mn>0</mn></msubsup><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>g</mi><msubsup><mrow></mrow><mi>Ν</mi><mn>0</mn></msubsup></mrow><mo>}</mo></mrow></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="79">其中, <mathml id="80"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>d</mi><mrow><mo> (</mo><mrow><mi>p</mi><mo>, </mo><mi>g</mi><msubsup><mrow></mrow><mi>i</mi><mn>0</mn></msubsup></mrow><mo>) </mo></mrow><mo>&lt;</mo><mi>d</mi><mrow><mo> (</mo><mrow><mi>p</mi><mo>, </mo><mi>g</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow><mn>0</mn></msubsup></mrow><mo>) </mo></mrow></mrow></math></mathml>。本文引入k倒排近邻重新排序L (p, G) , 使更多的正样本位于排列的前面, 以提高行人再识别的准确率。</p>
                </div>
                <div class="p1">
                    <p id="81">根据文献<citation id="146" type="reference">[<a class="sup">14</a>]</citation>, 目标行人的k近邻如下:</p>
                </div>
                <div class="p1">
                    <p id="82" class="code-formula">
                        <mathml id="82"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ν</mi><mo stretchy="false"> (</mo><mi>p</mi><mo>, </mo><mi>k</mi><mo stretchy="false">) </mo><mo>=</mo><mrow><mo>{</mo><mrow><mi>g</mi><msubsup><mrow></mrow><mn>1</mn><mn>0</mn></msubsup><mo>, </mo><mi>g</mi><msubsup><mrow></mrow><mn>2</mn><mn>0</mn></msubsup><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>g</mi><msubsup><mrow></mrow><mi>k</mi><mn>0</mn></msubsup></mrow><mo>}</mo></mrow><mo>, </mo><mo stretchy="false">|</mo><mi>Ν</mi><mo stretchy="false"> (</mo><mi>p</mi><mo>, </mo><mi>k</mi><mo stretchy="false">) </mo><mo stretchy="false">|</mo><mo>=</mo><mi>k</mi></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="83">其中, <mathml id="84"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>|</mo><mo>⋅</mo><mo>|</mo></mrow></mrow></math></mathml>代表测试集中候选图片的数目。</p>
                </div>
                <div class="p1">
                    <p id="85">定义目标行人的<i>k</i>倒排近邻如下:</p>
                </div>
                <div class="p1">
                    <p id="86"><i>R</i> (<i>p</i>, <i>k</i>) ={<i>g</i><sub><i>i</i></sub>| (<i>g</i><sub><i>i</i></sub>∈<i>N</i> (<i>p</i>, <i>k</i>) ) ∧ (<i>p</i>∈<i>N</i> (<i>g</i><sub><i>i</i></sub>, <i>k</i>) ) }</p>
                </div>
                <div class="p1">
                    <p id="87">由于行人姿态、光照、角度的变化等原因导致一些正样本没有包含在<i>k</i>近邻中, 从而使得正样本被排除在<i>k</i>倒排近邻中, 为此, 本文在<i>k</i>倒排近邻基础上增加候选参数数目的<i>k</i>/2倒排近邻得到稳定的<i>R</i><sup>*</sup> (<i>p</i>, <i>k</i>) 如下:</p>
                </div>
                <div class="p1">
                    <p id="88" class="code-formula">
                        <mathml id="88"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>R</mi><msup><mrow></mrow><mo>*</mo></msup><mo stretchy="false"> (</mo><mi>p</mi><mo>, </mo><mi>k</mi><mo stretchy="false">) </mo><mo>←</mo><mi>R</mi><mo stretchy="false"> (</mo><mi>p</mi><mo>, </mo><mi>k</mi><mo stretchy="false">) </mo><mstyle displaystyle="true"><mo>∪</mo><mi>R</mi></mstyle><mo stretchy="false"> (</mo><mi>q</mi><mo>, </mo><mi>k</mi><mo>/</mo><mn>2</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mtext>s</mtext><mo>.</mo><mtext>t</mtext><mo>.</mo><mo stretchy="false">|</mo><mi>R</mi><mo stretchy="false"> (</mo><mi>p</mi><mo>, </mo><mi>k</mi><mo stretchy="false">) </mo><mstyle displaystyle="true"><mo>∩</mo><mi>R</mi></mstyle><mo stretchy="false"> (</mo><mi>q</mi><mo>, </mo><mi>k</mi><mo>/</mo><mn>2</mn><mo stretchy="false">) </mo><mo stretchy="false">|</mo><mo>≥</mo><mfrac><mn>2</mn><mn>3</mn></mfrac><mo stretchy="false">|</mo><mi>R</mi><mo stretchy="false"> (</mo><mi>q</mi><mo>, </mo><mi>k</mi><mo>/</mo><mn>2</mn><mrow><mo stretchy="false">) </mo><mo>|</mo></mrow></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="89">∀<i>q</i>∈<i>R</i> (<i>p</i>, <i>k</i>) </p>
                </div>
                <h4 class="anchor-tag" id="90" name="90">2.2 基于<i>k</i>倒排近邻的杰卡德距离</h4>
                <div class="p1">
                    <p id="91">在上述k倒排近邻的基础上, 本文定义k倒排近邻的杰卡德距离如下:</p>
                </div>
                <div class="p1">
                    <p id="92" class="code-formula">
                        <mathml id="92"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>d</mi><msub><mrow></mrow><mi>J</mi></msub><mo stretchy="false"> (</mo><mi>p</mi><mo>, </mo><mi>g</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mn>1</mn><mo>-</mo><mfrac><mrow><mo stretchy="false">|</mo><mi>R</mi><msup><mrow></mrow><mo>*</mo></msup><mo stretchy="false"> (</mo><mi>p</mi><mo>, </mo><mi>k</mi><mo stretchy="false">) </mo><mstyle displaystyle="true"><mo>∩</mo><mi>R</mi></mstyle><msup><mrow></mrow><mo>*</mo></msup><mo stretchy="false"> (</mo><mi>g</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>k</mi><mrow><mo stretchy="false">) </mo><mo>|</mo></mrow></mrow><mrow><mo stretchy="false">|</mo><mi>R</mi><msup><mrow></mrow><mo>*</mo></msup><mo stretchy="false"> (</mo><mi>p</mi><mo>, </mo><mi>k</mi><mo stretchy="false">) </mo><mstyle displaystyle="true"><mo>∪</mo><mi>R</mi></mstyle><msup><mrow></mrow><mo>*</mo></msup><mo stretchy="false"> (</mo><mi>g</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>k</mi><mrow><mo stretchy="false">) </mo><mo>|</mo></mrow></mrow></mfrac></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="93">由于R<sup>*</sup> (p, k) 和R<sup>*</sup> (g<sub>i</sub>, k) 的交集和并集运算会带来大量的计算消耗, 同时为了使距离越近的邻域得到的权重越大, 本文将k倒排特征编码成向量如下:</p>
                </div>
                <div class="p1">
                    <p id="94" class="code-formula">
                        <mathml id="94"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi mathvariant="bold-italic">V</mi><msub><mrow></mrow><mi>p</mi></msub><mo>=</mo><mo stretchy="false">[</mo><mi>V</mi><msub><mrow></mrow><mrow><mi>p</mi><mo>, </mo><mi>g</mi><msub><mrow></mrow><mn>1</mn></msub></mrow></msub><mo>, </mo><mi>V</mi><msub><mrow></mrow><mrow><mi>p</mi><mo>, </mo><mi>g</mi><msub><mrow></mrow><mn>2</mn></msub></mrow></msub><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>V</mi><msub><mrow></mrow><mrow><mi>p</mi><mo>, </mo><mi>g</mi><msub><mrow></mrow><mi>Ν</mi></msub></mrow></msub><mo stretchy="false">]</mo></mtd></mtr><mtr><mtd><mi>V</mi><msub><mrow></mrow><mrow><mi>p</mi><mo>, </mo><mi>g</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msub><mo>=</mo><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mtext>e</mtext><msup><mrow></mrow><mrow><mo>-</mo><mi>d</mi><mo stretchy="false"> (</mo><mi>p</mi><mo>, </mo><mi>g</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo></mrow></msup><mo>, </mo><mi>g</mi><msub><mrow></mrow><mi>i</mi></msub><mo>∈</mo><mi>R</mi><msup><mrow></mrow><mo>*</mo></msup><mo stretchy="false"> (</mo><mi>p</mi><mo>, </mo><mi>k</mi><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mn>0</mn><mo>, </mo><mtext>其</mtext><mtext>他</mtext></mtd></mtr></mtable></mrow></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="95">基于上式的定义, 交集和并集候选参数的数目定义如下:</p>
                </div>
                <div class="p1">
                    <p id="96" class="code-formula">
                        <mathml id="96"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false">|</mo><mi mathvariant="bold-italic">R</mi><msup><mrow></mrow><mo>*</mo></msup><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">p</mi><mo>, </mo><mi mathvariant="bold-italic">k</mi><mo stretchy="false">) </mo><mstyle displaystyle="true"><mo>∩</mo><mi mathvariant="bold-italic">R</mi></mstyle><msup><mrow></mrow><mo>*</mo></msup><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">g</mi><msub><mrow></mrow><mi mathvariant="bold-italic">i</mi></msub><mo>, </mo><mi mathvariant="bold-italic">k</mi><mo stretchy="false">) </mo><mo stretchy="false">|</mo><mo>=</mo><mrow><mrow><mo>|</mo><mrow><mi mathvariant="bold">m</mi><mi mathvariant="bold">i</mi><mi mathvariant="bold">n</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">V</mi><msub><mrow></mrow><mi>p</mi></msub><mo>, </mo><mi mathvariant="bold-italic">V</mi><msub><mrow></mrow><mrow><mi>g</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msub><mo stretchy="false">) </mo></mrow><mo>|</mo></mrow></mrow><msub><mrow></mrow><mi>L</mi></msub></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="97">则杰卡德距离计算如下:</p>
                </div>
                <div class="p1">
                    <p id="98" class="code-formula">
                        <mathml id="98"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>d</mi><msub><mrow></mrow><mi>J</mi></msub><mo stretchy="false"> (</mo><mi>p</mi><mo>, </mo><mi>g</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mn>1</mn><mo>-</mo><mfrac><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mrow></mrow></mstyle><mi>min</mi><mo stretchy="false"> (</mo><mi>V</mi><msub><mrow></mrow><mrow><mi>p</mi><mo>, </mo><mi>g</mi><msub><mrow></mrow><mi>j</mi></msub></mrow></msub><mo>, </mo><mi>V</mi><msub><mrow></mrow><mrow><mi>g</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>g</mi><msub><mrow></mrow><mi>j</mi></msub></mrow></msub><mo stretchy="false">) </mo></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mrow></mrow></mstyle><mi>max</mi><mo stretchy="false"> (</mo><mi>V</mi><msub><mrow></mrow><mrow><mi>p</mi><mo>, </mo><mi>g</mi><msub><mrow></mrow><mi>j</mi></msub></mrow></msub><mo>, </mo><mi>V</mi><msub><mrow></mrow><mrow><mi>g</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>g</mi><msub><mrow></mrow><mi>j</mi></msub></mrow></msub><mo stretchy="false">) </mo></mrow></mfrac></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="99">结合马氏距离和杰卡德距离, 得到最终的距离度量<i>d</i><sup>*</sup>:</p>
                </div>
                <div class="p1">
                    <p id="100"><i>d</i><sup>*</sup> (<i>p</i>, <i>g</i><sub><i>i</i></sub>) = (1-<i>λ</i>) <i>d</i><sub><i>J</i></sub> (<i>p</i>, <i>g</i><sub><i>i</i></sub>) +<i>λd</i> (<i>p</i>, <i>g</i><sub><i>i</i></sub>) </p>
                </div>
                <div class="p1">
                    <p id="101">其中, <i>λ</i>∈[0, 1]代表惩罚因子。</p>
                </div>
                <h3 id="102" name="102" class="anchor-tag">3 实验与结果分析</h3>
                <div class="p1">
                    <p id="103">本文实验平台由2个部分组成。训练部分配置:Intel E5-2620 v3 (2.4 GHz处理器) 、128 GB内存、M40显卡、centos操作系统、基于C++编程语言的Caffe深度学习开源框架;测试部分配置:Inter i5-4460 (3.2 GHz处理器) 、GTX960 Ti显卡、ubuntu14.04操作系统、基于C++编程语言的Caffe深度学习开源框架。</p>
                </div>
                <h4 class="anchor-tag" id="104" name="104">3.1 训练和测试数据库</h4>
                <div class="p1">
                    <p id="105">本文使用的训练库包括CUHK01、CUHK02、CUHK03、PRID、VIPeR、3DPeS、i-LIDS、Market-1501;测试数据库有VIPeR、Market-1501。其中, Market-1501数据库包含1 501个人, 共38 195张行人图片, VIPeR数据库总共632人, 每人2张图片分别来自2个不重叠的摄像头。VIPeR数据库的图片清晰度较差, 拍摄角度变化大, 挑战性较大。</p>
                </div>
                <h4 class="anchor-tag" id="106" name="106">3.2 网络收敛</h4>
                <div class="p1">
                    <p id="107">在对网络进行特征提取和特征融合时采用分步策略, 首先训练特征提取网络, 再训练特征融合网络, 并且网络中的参数随机设定, 没有经过预训练。在特征提取过程中, 先对整幅行人图片提取特征, 再依次对7个子区域提取特征, 整个训练过程网络迭代70 000次。特征融合则是将提取到的特征进行结合, 迭代50 000次达到收敛。</p>
                </div>
                <h4 class="anchor-tag" id="108" name="108">3.3 结果分析</h4>
                <h4 class="anchor-tag" id="109" name="109">3.3.1 网络特征提取能力</h4>
                <div class="p1">
                    <p id="110">本文利用马氏距离对身体区域特征进行排序, 在测试数据库Market-1501和VIPeR中, 本文方法与其他方法对比结果如表1和表2所示。</p>
                </div>
                <div class="area_img" id="111">
                    <p class="img_tit"><b>表1 在Market-1501中利用马氏距离排序结果</b> % <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="111" border="1"><tr><td rowspan="2"><br />身体划分方法</td><td colspan="3"><br />准确率</td></tr><tr><td><br /><i>k</i>=1</td><td><i>k</i>=5</td><td><i>k</i>=10</td></tr><tr><td><br />文献[15]方法</td><td>44.4</td><td>63.9</td><td>72.2</td></tr><tr><td><br />文献[11]方法</td><td>76.9</td><td>91.5</td><td>94.6</td></tr><tr><td><br />本文方法</td><td>75.1</td><td>90.3</td><td>92.9</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="112">
                    <p class="img_tit"><b>表2 在VIPeR中利用马氏距离排序结果</b> % <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="112" border="1"><tr><td rowspan="2"><br />身体划分方法</td><td colspan="3"><br />准确率</td></tr><tr><td><br /><i>k</i>=1</td><td><i>k</i>=5</td><td><i>k</i>=10</td></tr><tr><td><br />文献[15]方法</td><td>31.50</td><td>55.70</td><td>67.20</td></tr><tr><td><br />文献[10]方法</td><td>51.06</td><td>81.01</td><td>91.39</td></tr><tr><td><br />文献[11]方法</td><td>53.80</td><td>82.60</td><td>91.50</td></tr><tr><td><br />本文方法</td><td>52.60</td><td>81.70</td><td>91.40</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="113">从表1和表2可以看出, 基于身体节点的区域特征提取较直接对整幅行人图像特征提取的准确率高, 说明基于身体节点的区域划分能有效地解决行人比对时不对齐的问题。由于文献<citation id="147" type="reference">[<a class="sup">11</a>]</citation>未公开其训练库, 而本文采用公开库进行训练, 获得的正确率与其接近, 并且训练时间较短。</p>
                </div>
                <h4 class="anchor-tag" id="114" name="114">3.3.2 <i>k</i>倒排近邻编码的杰卡德距离</h4>
                <div class="p1">
                    <p id="115">本文采用基于身体节点的划分区域进行特征提取, 对比了不同距离度量的测试准确率, 结果如表3和表4所示。</p>
                </div>
                <div class="area_img" id="116">
                    <p class="img_tit"><b>表3 在VIPeR不同距离度量下的准确率</b> % <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="116" border="1"><tr><td rowspan="2"><br />度量距离</td><td colspan="3"><br />准确率</td></tr><tr><td><br /><i>k</i>=1</td><td><i>k</i>=5</td><td><i>k</i>=10</td></tr><tr><td><br />马氏距离</td><td>52.6</td><td>81.7</td><td>91.4</td></tr><tr><td><br />马氏距离+杰卡德距离</td><td>53.9</td><td>83.1</td><td>92.3</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="117">
                    <p class="img_tit"><b>表4 在Market-1501不同距离度量下的准确率</b> % <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="117" border="1"><tr><td rowspan="2"><br />度量距离</td><td colspan="3"><br />准确率</td></tr><tr><td><br /><i>k</i>=1</td><td><i>k</i>=5</td><td><i>k</i>=10</td></tr><tr><td><br />马氏距离</td><td>75.1</td><td>90.3</td><td>92.9</td></tr><tr><td><br />马氏距离+杰卡德距离</td><td>77.2</td><td>92.1</td><td>94.8</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="118">从表3和表4可以看出, 结合马氏距离和杰卡德距离作为度量较单独使用马氏距离的准确率有一定改善, 证明引进杰卡德距离可使更多的正样本排序靠前。</p>
                </div>
                <h4 class="anchor-tag" id="119" name="119">3.3.3 错误排序分析</h4>
                <div class="p1">
                    <p id="120">利用基于身体节点的区域划分在马氏距离和杰卡德距离相结合的情况下进行的实验, 对在VIPeR数据库中的<i>k</i>=5出错样本进行分析, 列出部分出错样本, 如图4所示。其中, 带框的图像表示与目标行人准确匹配, 下方的数字代表与目标图片的相似度。</p>
                </div>
                <div class="area_img" id="121">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201903035_121.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 在VIPeR数据库中k=5出错样本排序" src="Detail/GetImg?filename=images/JSJC201903035_121.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图4 在VIPeR数据库中<i>k</i>=5出错样本排序</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201903035_121.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="122">通过对图4进行分析可以发现, 在错误样本中行人都处于背景复杂的环境中, 并且图片几乎都是行人的侧面。而本文采用的基于身体节点的区域划分需要准确定位行人身体14个关键节点, 对侧面的图片由于自身遮挡导致无法准确获得节点位置。</p>
                </div>
                <h3 id="123" name="123" class="anchor-tag">4 结束语</h3>
                <div class="p1">
                    <p id="124">本文提出基于区域特征对齐的行人再识别方法, 利用卷积结构定位行人身体节点, 根据节点位置划分身体区域并进行特征提取, 从而获得更多的行人信息。在距离度量上采用<i>k</i>倒排近邻使得更多的正样本包含在邻域中, 通过将<i>k</i>倒排特征编码成向量减少计算量, 并利用马氏距离和杰卡德距离的结合进一步改进特征排序。实验结果表明, 该方法具有较高的识别准确率。下一步将通过图像语义分割等技术去除行人图片背景并采用不同的定位方式识别侧面样本。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="15">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=WCLJ201603021&amp;v=MjE3NDI4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm9GeTdsVXJyQU1pN0haTEc0SDlmTXJJOUhaWVFLREg=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> 俞婧, 仇春春, 王恬, 等.基于距离匹配的行人再识别技术综述[J].微处理机, 2016, 37 (3) :77-80.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multiresolution gray-scale and rotation invariant texture classification with local binary patterns">

                                <b>[2]</b> OJALA T, PIETIKAINEN M, MAENPAA T.Multi-resolution gray-scale and rotation invariant texture classification with local binary patterns[J].Classification with Local Binary Patterns, 2002, 24 (7) :971-987.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Person reidentification by symmetry-driven accumulation of local features">

                                <b>[3]</b> FARENZENA M, BAZZANI L, PERINA A, et al.Person re-identification by symmetry-driven accumulation of local features[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2010:2360-2367.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Custom pictorial structures for re-identification">

                                <b>[4]</b> CHENG D S, CRISTANI M, STOPPA M, et al.Custom pictorial structures for re-identification[C]//Proceedings of the British Machine Vision Conference.Washington D.C., USA:IEEE Press, 2011:1-11.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep Re ID:deep filter pairing neural network for person re-identification">

                                <b>[5]</b> LI W, ZHAO R, XIAO T, et al.DeepReID:deep filter pairing neural network for person re-identification[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C.USA:IEEE Press, 2014:152-159.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=An improved deep learning architecture for person re-identification">

                                <b>[6]</b> AHMED E, JONES M J, MARKS T K.An improved deep learning architecture for person re-identification[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2015:3908-3916.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Going deeper with convolutions">

                                <b>[7]</b> SZEGEDY C, LIU W, JIA Y Q, et al.Going deeper with convolutions[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2015:1-9.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Person re-identification by multi-channel parts-based cnn with improved triplet loss function">

                                <b>[8]</b> CHENG D, GONG Y H, ZHOU S P, et al.Person re-identification by multi-channel parts-based CNN with improved triplet loss function[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2016:1335-1344.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning deep feature representations with domain guided dropout for person reidentification">

                                <b>[9]</b> XIAO T, LI H, OUYANG W, et al.Learning deep feature representations with domain guided dropout for person re-identification[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2016:1249-1258.
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=An enhanced deep feature representation for person re-identification">

                                <b>[10]</b> WU S X, CHEN Y C, LI X, et al.An enhanced deep feature representation for person re-identification[C]//Proceedings of IEEE Winter Conference on Applications of Computer Vision.Washington D.C., USA:IEEE Press, 2016:1-8.
                            </a>
                        </p>
                        <p id="35">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Spindle Net:Person Re-identification with Human Body Region Guided Feature Decomposition and Fusion">

                                <b>[11]</b> ZHAO H Y, TIAN M Q, SUN S Y, et al.Spindle net:person re-identification with human body region guided feature decomposition and fusion[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2017:907-915.
                            </a>
                        </p>
                        <p id="37">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Re-ranking person re-identification with k-reciprocal encoding">

                                <b>[12]</b> ZHONG Z, ZHENG L, CAO D L, et al.Re-ranking person re-identification with k-reciprocal encoding[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2017:3652-3661.
                            </a>
                        </p>
                        <p id="39">
                            <a id="bibliography_13" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DSSS2017Z4040&amp;v=MjgzMjNZZmJHNEg5YW1xNDlCWklRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJvRnk3bFVyckFJVDc=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[13]</b> 宋宗涛, 陈岳林, 蔡晓东.基于多分块三重损失计算的行人识别方法[J].电子与信息学报, 2016, 38 (6) :1528-1535.
                            </a>
                        </p>
                        <p id="41">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Hello Neighbor:Accurate Object Retrieval with K-Reciprocal Nearest Neighbors">

                                <b>[14]</b> QIN D F, GAMMETER S, BOSSARD L, et al.Hello neighbor:accurate object retrieval with k-reciprocal nearest neighbors[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C., USA:IEEE Press, 2011:777-784.
                            </a>
                        </p>
                        <p id="43">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Scalable person reidentification:a benchmark">

                                <b>[15]</b> ZHENG L, SHEN L Y, TIAN L, et al.Scalable person re-identification:a benchmark[C]//Proceedings of IEEE Conference on Computer Vision.Washington D.C., USA:IEEE Press, 2015:1116-1124.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJC201903035" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201903035&amp;v=MjQ3NjNVUkxPZVplUm9GeTdsVXJyQUx6N0JiYkc0SDlqTXJJOUdZWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckM=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
