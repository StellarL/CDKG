<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637130639826837500%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJC201903040%26RESULT%3d1%26SIGN%3dUsnoZiMlytB04pY0ZCsGXgIeDNQ%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201903040&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201903040&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201903040&amp;v=MTE1NDg2ajU0TzN6cXFCdEdGckNVUkxPZVplUm9GeTdsVzczQkx6N0JiYkc0SDlqTXJJOUJaSVFLREg4NHZSNFQ=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#37" data-title="0 概述 ">0 概述</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#41" data-title="1 基本原理 ">1 基本原理</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#42" data-title="1.1 基于ViBe算法的背景减除方法">1.1 基于ViBe算法的背景减除方法</a></li>
                                                <li><a href="#61" data-title="1.2 基于DTW算法的序列间相似度衡量方法">1.2 基于DTW算法的序列间相似度衡量方法</a></li>
                                                <li><a href="#76" data-title="1.3 基于LSDA的降维原理">1.3 基于LSDA的降维原理</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#99" data-title="2 LSDA算法实现步骤 ">2 LSDA算法实现步骤</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#109" data-title="3 仿真结果与分析 ">3 仿真结果与分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#111" data-title="3.1 基于ViBe算法的背景减除操作">3.1 基于ViBe算法的背景减除操作</a></li>
                                                <li><a href="#122" data-title="3.2 基于LSDA的降维操作">3.2 基于LSDA的降维操作</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#132" data-title="4 结束语 ">4 结束语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#108" data-title="&lt;b&gt;图1 运用LSDA降维的人体行为识别方法流程&lt;/b&gt;"><b>图1 运用LSDA降维的人体行为识别方法流程</b></a></li>
                                                <li><a href="#113" data-title="&lt;b&gt;图2 在KTH数据集上ViBe算法背景减除前后效果对比&lt;/b&gt;"><b>图2 在KTH数据集上ViBe算法背景减除前后效果对比</b></a></li>
                                                <li><a href="#114" data-title="&lt;b&gt;图3 在UCF101数据集上ViBe算法背景减除前后效果对比&lt;/b&gt;"><b>图3 在UCF101数据集上ViBe算法背景减除前后效果对比</b></a></li>
                                                <li><a href="#117" data-title="&lt;b&gt;图4 背景减除操作前后稠密轨迹提取效果对比&lt;/b&gt;"><b>图4 背景减除操作前后稠密轨迹提取效果对比</b></a></li>
                                                <li><a href="#120" data-title="&lt;b&gt;表1 背景减除操作前后存储空间和识别准确率对比&lt;/b&gt;"><b>表1 背景减除操作前后存储空间和识别准确率对比</b></a></li>
                                                <li><a href="#124" data-title="&lt;b&gt;表2 本文方法在KTH数据集上的分类混淆矩阵&lt;/b&gt;"><b>表2 本文方法在KTH数据集上的分类混淆矩阵</b></a></li>
                                                <li><a href="#127" data-title="&lt;b&gt;表3 在UCF101数据集上5种池化方法的效果对比&lt;/b&gt;"><b>表3 在UCF101数据集上5种池化方法的效果对比</b></a></li>
                                                <li><a href="#130" data-title="&lt;b&gt;表4 不同方法的识别准确率对比&lt;/b&gt; %"><b>表4 不同方法的识别准确率对比</b> %</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" title=" 黄凯奇, 陈晓棠, 康运锋, 等.智能视频监控技术综述[J].计算机学报, 2015, 20 (6) :1093-1118." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJX201506001&amp;v=MjE3MDRSTE9lWmVSb0Z5N2xXNzNCTHo3QmRyRzRIOVRNcVk5RlpZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1U=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         黄凯奇, 陈晓棠, 康运锋, 等.智能视频监控技术综述[J].计算机学报, 2015, 20 (6) :1093-1118.
                                    </a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" title=" 单言虎, 张彰, 黄凯奇.人的视觉行为识别研究回顾、现状及展望[J].计算机研究与发展, 2016, 53 (1) :93-112." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201601010&amp;v=MDI5NzR6cXFCdEdGckNVUkxPZVplUm9GeTdsVzczQkx5dlNkTEc0SDlmTXJvOUVaSVFLREg4NHZSNFQ2ajU0TzM=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         单言虎, 张彰, 黄凯奇.人的视觉行为识别研究回顾、现状及展望[J].计算机研究与发展, 2016, 53 (1) :93-112.
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title=" 李瑞峰, 王亮亮, 王珂.人体动作行为识别研究综述[J].模式识别与人工智能, 2014, 27 (1) :35-48." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201401005&amp;v=MTkxMzQ3bFc3M0JLRDdZYkxHNEg5WE1ybzlGWVlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJvRnk=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         李瑞峰, 王亮亮, 王珂.人体动作行为识别研究综述[J].模式识别与人工智能, 2014, 27 (1) :35-48.
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" title=" TURK M, PENTLAND A.Eigenfaces for recognition[J].Journal of Cognitive Neuroscience, 1991, 3 (1) :71-86." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJBK&amp;filename=SJBK15090500007384&amp;v=MjMyNjhxbzlGWk9zSUQzUTlvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVVMbklKbDRjYUJzPU5pZkpaYks5SHRqTQ==&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                         TURK M, PENTLAND A.Eigenfaces for recognition[J].Journal of Cognitive Neuroscience, 1991, 3 (1) :71-86.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" title=" BELHUMEUR P N, HESPANHA J P, KRIEGMAN D J.Eigenfaces vs.fisherfaces:recognition using class specific linear projection[C]//Proceedings of the 4th European Conference on Computer Vision.Berlin, Germany:Springer, 1996:45-58." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Eigenfaces vs. Fisherfaces: Recognition using class specific linear projection">
                                        <b>[5]</b>
                                         BELHUMEUR P N, HESPANHA J P, KRIEGMAN D J.Eigenfaces vs.fisherfaces:recognition using class specific linear projection[C]//Proceedings of the 4th European Conference on Computer Vision.Berlin, Germany:Springer, 1996:45-58.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" title=" HUANG S, YE J, WANG T, et al.Extracting refined low-rank features of robust pca for human action recognition[J].Arabian Journal for Science and Engineering, 2015, 40 (5) :1427-1441." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Extracting Refined LowRank Features of Robust PCA for Human Action Recognition">
                                        <b>[6]</b>
                                         HUANG S, YE J, WANG T, et al.Extracting refined low-rank features of robust pca for human action recognition[J].Arabian Journal for Science and Engineering, 2015, 40 (5) :1427-1441.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" title=" 王淼, 孙季丰, 余家林.基于特征层融合和随机投影的行为识别算法[J].科学技术与工程, 2017, 17 (13) :210-215." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=KXJS201713039&amp;v=MDYyODhaZVJvRnk3bFc3M0JMalhCZmJHNEg5Yk5ySTlHYllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2U=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                         王淼, 孙季丰, 余家林.基于特征层融合和随机投影的行为识别算法[J].科学技术与工程, 2017, 17 (13) :210-215.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" title=" HE X, NIYOGI P.Locality preserving projections[J].Advances in Neural Information Processing Systems, 2002, 16 (1) :186-197." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Locality preserving projections">
                                        <b>[8]</b>
                                         HE X, NIYOGI P.Locality preserving projections[J].Advances in Neural Information Processing Systems, 2002, 16 (1) :186-197.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" title=" 王鑫, 沃波海, 管秋, 等.基于流形学习的人体动作识别[J].中国图象图形学报, 2014, 19 (6) :914-923." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZGTB201406012&amp;v=MjA3NzBYTXFZOUVab1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm9GeTdsVzczQlB5cmZiTEc0SDk=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                         王鑫, 沃波海, 管秋, 等.基于流形学习的人体动作识别[J].中国图象图形学报, 2014, 19 (6) :914-923.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_10" title=" WRIGHT J, MA Y, MAIRAL J, et al.Sparse representation for computer vision and pattern recognition [J].Proceedings of the IEEE, 2010, 98 (6) :1031-1044." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Sparse representation for computer vision and pattern recognition">
                                        <b>[10]</b>
                                         WRIGHT J, MA Y, MAIRAL J, et al.Sparse representation for computer vision and pattern recognition [J].Proceedings of the IEEE, 2010, 98 (6) :1031-1044.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_11" title=" 张瑞杰, 魏福山.结合Fisher判别分析和稀疏编码的图像场景分类[J].计算机辅助设计与图形学学报, 2015, 27 (5) :808-814." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJF201505005&amp;v=MjQyODVMT2VaZVJvRnk3bFc3M0JMejdCYUxHNEg5VE1xbzlGWVlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[11]</b>
                                         张瑞杰, 魏福山.结合Fisher判别分析和稀疏编码的图像场景分类[J].计算机辅助设计与图形学学报, 2015, 27 (5) :808-814.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_12" title=" 肖玉玲.结合HOG/HOF级联特征和多层分类器的人体行为识别[J].计算机工程与设计, 2017, 38 (9) :2567-2572." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=SJSJ201709049&amp;v=MTA5NTR6cXFCdEdGckNVUkxPZVplUm9GeTdsVzczQk5pZllaTEc0SDliTXBvOUJiWVFLREg4NHZSNFQ2ajU0TzM=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[12]</b>
                                         肖玉玲.结合HOG/HOF级联特征和多层分类器的人体行为识别[J].计算机工程与设计, 2017, 38 (9) :2567-2572.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_13" title=" KEOGH E, RATANAMAHATANA C A.Exact indexing of dynamic time warping[J].Knowledge and Information Systems, 2005, 7 (3) :358-386." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00001681961&amp;v=MjE4NDRZM2s1ekJkaDRqOTlTWHFScnhveGNNSDdSN3FlYnVkdEZDN2xWYi9BSWxjPU5qN0Jhck80SHRITnFZZEViZTBP&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[13]</b>
                                         KEOGH E, RATANAMAHATANA C A.Exact indexing of dynamic time warping[J].Knowledge and Information Systems, 2005, 7 (3) :358-386.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_14" title=" SU B, DING X, WANG H, et al.Discriminative dimensionality reduction for multi-dimensional sequences[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2018, 40 (1) :77-91." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Discriminative dimensionality reduction for multi-dimensional sequences">
                                        <b>[14]</b>
                                         SU B, DING X, WANG H, et al.Discriminative dimensionality reduction for multi-dimensional sequences[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2018, 40 (1) :77-91.
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_15" title=" WANG H, SCHMID C.Action recognition with improved trajectories[C]//Proceedings of International Conference on Computer Vision.Washington D.C., USA:IEEE Press, 2014:3551-3558." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Action recognition with improved trajectories">
                                        <b>[15]</b>
                                         WANG H, SCHMID C.Action recognition with improved trajectories[C]//Proceedings of International Conference on Computer Vision.Washington D.C., USA:IEEE Press, 2014:3551-3558.
                                    </a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_16" title=" SCHULDT C, LAPTEV I, CAPUTO B.Recognizing human actions:a local SVM approach[C]//Proceedings of International Conference on Pattern Recognition.Washington D.C., USA:IEEE Press, 2004:32-36." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Recognizing human actions: a local SVM approach">
                                        <b>[16]</b>
                                         SCHULDT C, LAPTEV I, CAPUTO B.Recognizing human actions:a local SVM approach[C]//Proceedings of International Conference on Pattern Recognition.Washington D.C., USA:IEEE Press, 2004:32-36.
                                    </a>
                                </li>
                                <li id="35">


                                    <a id="bibliography_17" title=" SOOMRO K, ZAMIR A R, SHAH M.UCF101:a dataset of 101 human actions classes from videos in the wild[EB/OL].[2017-12-25].https://arxiv.org/pdf/1212.0402.pdf." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=UCF101:a dataset of 101 human actions classes from videos in the wild">
                                        <b>[17]</b>
                                         SOOMRO K, ZAMIR A R, SHAH M.UCF101:a dataset of 101 human actions classes from videos in the wild[EB/OL].[2017-12-25].https://arxiv.org/pdf/1212.0402.pdf.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJC" target="_blank">计算机工程</a>
                2019,45(03),237-241+249 DOI:10.19678/j.issn.1000-3428.0050160            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>一种基于线性序列差异分析降维的人体行为识别方法</b></span>
 <span class="shoufa"></span>                                     </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%B9%BF%E5%A4%A9%E7%84%B6&amp;code=38702680&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">鹿天然</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E4%BA%8E%E5%87%A4%E8%8A%B9&amp;code=07770591&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">于凤芹</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%99%88%E8%8E%B9&amp;code=10556317&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">陈莹</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%B1%9F%E5%8D%97%E5%A4%A7%E5%AD%A6%E7%89%A9%E8%81%94%E7%BD%91%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0074200&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">江南大学物联网工程学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>在视频数据处理过程中容易出现维数灾难的问题。为此, 提出一种线性序列差异分析方法, 对视频数据降维来进行人体行为识别。运用ViBe算法对视频帧进行背景减除操作获取行为区域, 在该区域内提取稠密轨迹特征从而去除背景数据的干扰。使用Fisher Vector对特征编码后进行线性序列差异分析, 采用动态线性规整算法计算序列类别间相似度, 得到最小化类内残差和最大化类间残差的线性变换, 将特征从高维空间投影至低维空间, 降低特征维数。利用降维后的特征训练支持向量机, 实现人体行为识别。在KTH数据集和UCF101数据集上进行数据仿真, 结果表明, 与主成分分析算法、线性判别分析法等相比, 该方法可有效提高识别准确率。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%BA%BA%E4%BD%93%E8%A1%8C%E4%B8%BA%E8%AF%86%E5%88%AB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">人体行为识别;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%83%8C%E6%99%AF%E5%87%8F%E9%99%A4&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">背景减除;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%A8%A0%E5%AF%86%E8%BD%A8%E8%BF%B9&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">稠密轨迹;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%BA%BF%E6%80%A7%E5%BA%8F%E5%88%97%E5%B7%AE%E5%BC%82%E5%88%86%E6%9E%90&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">线性序列差异分析;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%99%8D%E7%BB%B4&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">降维;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    鹿天然 (1993—) , 女, 硕士研究生, 主研方向为模式识别、智能系统;;
                                </span>
                                <span>
                                    于凤芹, 教授、博士。;
                                </span>
                                <span>
                                    陈莹, 教授、博士。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-01-18</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金 (61573168);</span>
                                <span>中央高校基本科研业务费专项资金 (JUSRP51733B);</span>
                    </p>
            </div>
                    <h1><b>A Human Action Recognition Method Based on LSDA Dimension Reduction</b></h1>
                    <h2>
                    <span>LU Tianran</span>
                    <span>YU Fengqin</span>
                    <span>CHEN Ying</span>
            </h2>
                    <h2>
                    <span>School of Internet of Things Engineering, Jiangnan University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Aiming at the problem that dimensionality disaster easily occurs in the processing of dealing with video data, a dimension reduction method called Linear Sequence Discriminant Analysis (LSDA) is proposed for human action recognition.ViBe algorithm is used to subtract the backgrounds of video frames to get action areas, and dense trajectories are extracted in these areas to suppress the noise caused by camera movements.Fisher Vector is used to encode the features and linear sequence discriminant analysis is conducted on them, the sequence class separability is measured by dynamic time warping distance.In order to reduce the data dimension, a linear discriminative projection of the feature vectors in sequences is mapped to a lower-dimensional subspace by maximizing the between-class separability and minimizing the within-class separability.Support Vector Machine (SVM) is learned from the reduced dimension features, and then get the results of human action recognition.Simulation results on KTH datasets and UCF101 datasets show that compared with Principal Component Analysis (PCA) , Linear Discriminant Analysis (LDA) and other dimension reduction methods, the proposed method can effectively improve the recognition accuracy.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=human%20action%20recognition&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">human action recognition;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=background%20substraction&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">background substraction;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=dense%20trajectories&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">dense trajectories;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Linear%20Sequence%20Discriminant%20Analysis%20(LSDA)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Linear Sequence Discriminant Analysis (LSDA) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=dimension%20reduction&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">dimension reduction;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-01-18</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="37" name="37" class="anchor-tag">0 概述</h3>
                <div class="p1">
                    <p id="38">人体行为识别作为模式识别和机器视觉领域的研究热点, 不仅在智能监控<citation id="134" type="reference"><link href="3" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>、人机交互和运动分析方面被广泛应用, 而且在公共场所人员身份鉴别和异常行为监控等方面具有重要的应用前景<citation id="135" type="reference"><link href="5" rel="bibliography" /><link href="7" rel="bibliography" /><sup>[<a class="sup">2</a>,<a class="sup">3</a>]</sup></citation>。由于人体行为动作的复杂性, 描述目标在视频中的时空运动变化时, 待处理的数据量往往十分庞大, 需要大量的计算时间。如何在不同的光照、视角和背景下对视频中的人体行为进行快速、精确的分类识别, 是目前亟待解决的难题。</p>
                </div>
                <div class="p1">
                    <p id="39">国内外研究者采用降维方法处理视频数据来进行人体行为识别。典型代表有主成分分析 (Principal Component Analysis, PCA) 算法<citation id="136" type="reference"><link href="9" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>和线性判别分析 (Linear Discriminant Analysis, LDA) 算法<citation id="137" type="reference"><link href="11" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>。文献<citation id="138" type="reference">[<a class="sup">6</a>]</citation>将PCA方法用于人体行为识别, 以非监督方式、最大化方差将高维数据映射到低维空间。文献<citation id="139" type="reference">[<a class="sup">7</a>]</citation>利用LDA方法以监督方式得到将高维数据映射到低维空间的投影矩阵。这2种方法均有效地降低了数据维度, 但都是以相同方式处理不同时长的特征向量, 忽略了视频序列间的时间相关性。针对这一问题, 文献<citation id="140" type="reference">[<a class="sup">8</a>]</citation>提出基于流形学习的降维方法, 其中应用最为广泛的是局部保持投影 (Locality Preserving Projections, LPP) 算法, 其假设低维数据来源于高维空间的一个潜在流形上。文献<citation id="141" type="reference">[<a class="sup">9</a>]</citation>将LPP运用于人体行为识别中, 但由于LPP是非监督算法, 没有考虑类别标签, 分类效果不佳。稀疏表示 (Sparse Representation, SR) <citation id="142" type="reference"><link href="21" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>近年来在模式识别领域被广泛应用, 它可以对高维数据进行有效表示, 通常与其他的降维方法相结合在低维空间中表示数据并实现有效分类。然而, 该方法采用的L1范数计算过程复杂, 处理的数据量大, 文献<citation id="143" type="reference">[<a class="sup">11</a>]</citation>将其应用于行为识别, 虽取得了较高的识别准确率, 但实时效果不佳。</p>
                </div>
                <div class="p1">
                    <p id="40">针对稠密轨迹行为识别方法不能很好地区分行为和背景的问题, 本文利用ViBe算法<citation id="144" type="reference"><link href="25" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>进行背景减除获取行为区域, 去除背景冗余数据的干扰。在行为区域提取稠密轨迹特征并对编码后的特征进行线性序列差异分析 (Linear Sequence Discriminant Analysis, LSDA) , 以降低数据维度。由于不同的序列有不同的时续长度, 本文采用动态时间规整 (Dynamic Time Warping, DTW) 算法<citation id="145" type="reference"><link href="27" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>计算序列间的距离, 从而衡量序列类别间差异性, 并将序列的平均方差作为序列类别内差异性的衡量准则。</p>
                </div>
                <h3 id="41" name="41" class="anchor-tag">1 基本原理</h3>
                <h4 class="anchor-tag" id="42" name="42">1.1 基于ViBe算法的背景减除方法</h4>
                <div class="p1">
                    <p id="43">人体行为复杂、背景更新速度慢会导致行为对象部分漏检, 影响识别结果。ViBe算法构建背景速度快, 运算效率高, 其具体思想是:为每个像素点存储一个样本集, 样本集中的采样值为该像素点过去的像素值和其邻居结点的像素值, 然后将每一个新的像素值和样本集进行比较来判断是否为背景点。算法主要包括以下3个步骤。</p>
                </div>
                <h4 class="anchor-tag" id="44" name="44">1) 背景模型初始化</h4>
                <div class="p1">
                    <p id="45">初始背景模型 (<i>t</i>=0) 中像素点 (<i>x</i>, <i>y</i>) 处的像素值可以表示为:</p>
                </div>
                <div class="p1">
                    <p id="46"><i>B</i><sup>0</sup> (<i>x</i>, <i>y</i>) =<i>f</i><sup>0</sup> (<i>x</i><sub><i>N</i></sub>, <i>y</i><sub><i>N</i></sub>)      (1) </p>
                </div>
                <div class="p1">
                    <p id="47">其中, <i>f</i><sup>0</sup> (<i>x</i><sub><i>N</i></sub>, <i>y</i><sub><i>N</i></sub>) 表示当前帧像素点 (<i>x</i><sub><i>N</i></sub>, <i>y</i><sub><i>N</i></sub>) 处的像素值, (<i>x</i><sub><i>N</i></sub>, <i>y</i><sub><i>N</i></sub>) 为在像素点 (<i>x</i>, <i>y</i>) 周围随机选取的像素点。<i>M</i>为选取次数, 即可以构建的背景模型为<i>M</i>个, 本文中取<i>M</i>=20。</p>
                </div>
                <h4 class="anchor-tag" id="48" name="48">2) 前景提取</h4>
                <div class="p1">
                    <p id="49">当前帧的像素点 (<i>x</i>, <i>y</i>) 对应的<i>M</i>个背景模型在当前时刻<i>t</i>的像素值为<i>B</i><mathml id="50"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>t</mi></msubsup></mrow></math></mathml> (<i>x</i>, <i>y</i>) , <i>i</i>=1, 2, …, <i>M</i>, 记:</p>
                </div>
                <div class="p1">
                    <p id="51" class="code-formula">
                        <mathml id="51"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>b</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo><mo>=</mo><mrow><mo>{</mo><mrow><mtable><mtr><mtd columnalign="left"><mn>1</mn><mo>, </mo><mrow><mo>|</mo><mrow><mi>f</mi><mrow></mrow><msup><mrow></mrow><mi>t</mi></msup><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo><mo>-</mo><mi>B</mi><msubsup><mrow></mrow><mi>i</mi><mi>t</mi></msubsup><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo></mrow><mo>|</mo></mrow><mo>≤</mo><mi>Τ</mi></mtd></mtr><mtr><mtd columnalign="left"><mn>0</mn><mo>, </mo><mrow><mo>|</mo><mrow><mi>f</mi><mrow></mrow><msup><mrow></mrow><mi>t</mi></msup><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo><mo>-</mo><mi>B</mi><msubsup><mrow></mrow><mi>i</mi><mi>t</mi></msubsup><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo></mrow><mo>|</mo></mrow><mo>&gt;</mo><mi>Τ</mi></mtd></mtr></mtable></mrow></mrow><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="52">其中, 参数<i>T</i>表示前景与背景像素值差异的下限, 本文中取20。</p>
                </div>
                <div class="p1">
                    <p id="53">利用<i>M</i>个背景模型对像素点 (<i>x</i>, <i>y</i>) 的属性进行投票来判断其所属类别, 判断准则为:</p>
                </div>
                <div class="p1">
                    <p id="54" class="code-formula">
                        <mathml id="54"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>p</mi><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo><mo>=</mo><mrow><mo>{</mo><mrow><mtable><mtr><mtd columnalign="left"><mtext>背</mtext><mtext>景</mtext><mo>, </mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Μ</mi></munderover><mi>b</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo><mo>≥</mo><mi>η</mi></mtd></mtr><mtr><mtd columnalign="left"><mtext>前</mtext><mtext>景</mtext><mo>, </mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Μ</mi></munderover><mi>b</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo><mo>&lt;</mo><mi>η</mi></mtd></mtr></mtable></mrow></mrow><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="55">其中, 参数<i>η</i>表示设置的经验值, 本文中取3。</p>
                </div>
                <h4 class="anchor-tag" id="56" name="56">3) 背景模型更新</h4>
                <div class="p1">
                    <p id="57">从时域和空域两方面考虑ViBe算法在背景更新时的随机性。首先在时域上随机抽取一个模型, 记为<i>B</i><mathml id="58"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>t</mi></msubsup></mrow></math></mathml> (<i>x</i>, <i>y</i>) 。当前帧图像<i>f</i><sup><i>t</i></sup> (<i>x</i>, <i>y</i>) 经过前景提取之后得到一些背景像素点, 在更新背景模型<i>B</i><mathml id="59"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>t</mi></msubsup></mrow></math></mathml> (<i>x</i>, <i>y</i>) 时, 用当前帧图像上对应位置的像素值进行替换。在空域上, 对于<i>B</i><mathml id="60"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>t</mi></msubsup></mrow></math></mathml> (<i>x</i>, <i>y</i>) 中的任一像素点 (<i>x</i>, <i>y</i>) , 从其8邻域中随机抽取一个像素点 (<i>x</i><sub><i>N</i></sub>, <i>y</i><sub><i>N</i></sub>) , 用当前帧图像上像素点 (<i>x</i>, <i>y</i>) 处的像素值<i>f</i><sup><i>t</i></sup> (<i>x</i>, <i>y</i>) 来替换该点处的像素值。</p>
                </div>
                <h4 class="anchor-tag" id="61" name="61">1.2 基于DTW算法的序列间相似度衡量方法</h4>
                <div class="p1">
                    <p id="62">DTW算法可以处理不等长序列间距离的衡量问题。设时间序列<i>S</i><sup>1</sup> (<i>t</i>) ={<i>s</i><mathml id="63"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>1</mn><mn>1</mn></msubsup></mrow></math></mathml>, <i>s</i><mathml id="64"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>2</mn><mn>1</mn></msubsup></mrow></math></mathml>, …, <i>s</i><mathml id="65"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>m</mi><mn>1</mn></msubsup></mrow></math></mathml>}, <i>S</i><sup>2</sup> (<i>t</i>) ={<i>s</i><mathml id="66"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>1</mn><mn>2</mn></msubsup></mrow></math></mathml>, <i>s</i><mathml id="67"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></math></mathml>, …, <i>s</i><mathml id="68"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>n</mi><mn>2</mn></msubsup></mrow></math></mathml>}, 其长度分别为<i>m</i>和<i>n</i>。根据它们的时间位置排序, 构造<i>m</i>×<i>n</i>矩阵<b><i>A</i></b><sub><i>m</i>×<i>n</i></sub>, <b><i>A</i></b><sub><i>m</i>×<i>n</i></sub>中的元素<mathml id="69"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>a</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mi>d</mi><mo stretchy="false"> (</mo><mi>s</mi><msubsup><mrow></mrow><mi>i</mi><mn>1</mn></msubsup><mo>, </mo><mi>s</mi><msubsup><mrow></mrow><mi>j</mi><mn>2</mn></msubsup><mo stretchy="false">) </mo><mo>=</mo><msqrt><mrow><mrow><mo stretchy="false"> (</mo><mi>s</mi><msubsup><mrow></mrow><mi>i</mi><mn>1</mn></msubsup><mo>-</mo><mi>s</mi><msubsup><mrow></mrow><mi>j</mi><mn>2</mn></msubsup><mo stretchy="false">) </mo></mrow><msup><mrow></mrow><mn>2</mn></msup></mrow></msqrt></mrow></math></mathml>, 在矩阵<b><i>A</i></b><sub><i>m</i>×<i>n</i></sub>中, 把一组相邻的矩阵元素集合成规整路径, 记为<b><i>W</i></b>=<i>w</i><sub>1</sub>, <i>w</i><sub>2</sub>, …, <i>w</i><sub><i>K</i></sub>, <b><i>W</i></b>的第<i>k</i>个元素<i>w</i><sub><i>k</i></sub>= (<i>a</i><sub><i>ij</i></sub>) <sub><i>k</i></sub>, 则DTW<mathml id="70"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false"> (</mo><mi>S</mi><msup><mrow></mrow><mn>1</mn></msup><mo>, </mo><mi>S</mi><msup><mrow></mrow><mn>2</mn></msup><mo stretchy="false">) </mo><mo>=</mo><mrow><mi>min</mi></mrow><mo stretchy="false"> (</mo><mfrac><mn>1</mn><mi>Κ</mi></mfrac><msqrt><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Κ</mi></munderover><mi>w</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub></mrow></msqrt><mo stretchy="false">) </mo></mrow></math></mathml>。<i>DTW</i>算法运用动态规划思想寻找一条具有最小规整代价的最佳路径, 即:</p>
                </div>
                <div class="p1">
                    <p id="71" class="code-formula">
                        <mathml id="71"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mi>D</mi><mo stretchy="false"> (</mo><mn>1</mn><mo>, </mo><mn>1</mn><mo stretchy="false">) </mo><mo>=</mo><mi>a</mi><msub><mrow></mrow><mrow><mn>1</mn><mn>1</mn></mrow></msub></mtd></mtr><mtr><mtd><mi>D</mi><mo stretchy="false"> (</mo><mi>i</mi><mo>, </mo><mi>j</mi><mo stretchy="false">) </mo><mo>=</mo><mi>a</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>+</mo><mi>min</mi><mo stretchy="false"> (</mo><mi>D</mi><mo stretchy="false"> (</mo><mi>i</mi><mo>-</mo><mn>1</mn><mo>, </mo><mi>j</mi><mo>-</mo><mn>1</mn><mo stretchy="false">) </mo><mo>, </mo><mi>D</mi><mo stretchy="false"> (</mo><mi>i</mi><mo>, </mo><mi>j</mi><mo>-</mo><mn>1</mn><mo stretchy="false">) </mo><mo>, </mo><mi>D</mi><mo stretchy="false"> (</mo><mi>i</mi><mo>-</mo><mn>1</mn><mo>, </mo><mi>j</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo></mtd></mtr></mtable></mrow><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="72">其中, i=2, 3, …, m, j=2, 3, …, n。D (m, n) 表示<b><i>A</i></b><sub><i>m</i>×<i>n</i></sub>中规整路径的最小累加值。</p>
                </div>
                <div class="p1">
                    <p id="73">对于2个序列样本<b><i>U</i></b><sub><i>n</i></sub>和<b><i>U</i></b><sub><i>n</i>′</sub>, 它们之间的DTW距离可以利用下式计算:</p>
                </div>
                <div class="p1">
                    <p id="74" class="code-formula">
                        <mathml id="74"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mrow><mi>Π</mi><msub><mrow></mrow><mi>n</mi></msub><mo>, </mo><mi>Π</mi><msub><mrow></mrow><msup><mi>n</mi><mo>′</mo></msup></msub></mrow></munder><mtext>t</mtext><mtext>r</mtext><mrow><mo> (</mo><mrow><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">U</mi><msub><mrow></mrow><mi>n</mi></msub><mi>Π</mi><msub><mrow></mrow><mi>n</mi></msub><mo>-</mo><mi mathvariant="bold-italic">U</mi><msub><mrow></mrow><msup><mi>n</mi><mo>′</mo></msup></msub><mi>Π</mi><msub><mrow></mrow><msup><mi>n</mi><mo>′</mo></msup></msub><mo stretchy="false">) </mo><mo>⋅</mo><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">U</mi><msub><mrow></mrow><mi>n</mi></msub><mi>Π</mi><msub><mrow></mrow><mi>n</mi></msub><mo>-</mo><mi mathvariant="bold-italic">U</mi><msub><mrow></mrow><msup><mi>n</mi><mo>′</mo></msup></msub><mi>Π</mi><msub><mrow></mrow><msup><mi>n</mi><mo>′</mo></msup></msub><mo stretchy="false">) </mo><msup><mrow></mrow><mtext>Τ</mtext></msup></mrow><mo>) </mo></mrow><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>5</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="75">其中, <i>Π</i><sub><i>n</i></sub>∈{0, 1}<sup><i>P</i><sub><i>n</i></sub>×<i>T</i><sub><i>nn</i>′</sub></sup>、<i>Π</i><sub><i>n</i>′</sub>∈{0, 1}<sup><i>P</i><sub><i>n</i>′</sub>×<i>T</i><sub><i>nn</i>′</sub></sup>为最佳规整路径, <i>P</i><sub><i>n</i></sub>和<i>P</i><sub><i>n</i>′</sub>分别为2个序列的长度, <i>T</i><sub><i>nn</i>′</sub>为规整路径的长度。</p>
                </div>
                <h4 class="anchor-tag" id="76" name="76">1.3 基于LSDA的降维原理</h4>
                <div class="p1">
                    <p id="77"><i>LSDA</i>算法<citation id="146" type="reference"><link href="29" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>是为了处理序列数据的降维问题, 寻找最佳线性判别变换来最大化序列类别间可分离度并最小化类内可分离度, 同时利用有监督信息来解决时间依赖问题的一种有效方法。降维是为了找到一个线性变换<b><i>W</i></b>∈<image href="images/JSJC201903040_078.jpg" type="" display="inline" placement="inline"><alt></alt></image><sup><i>d</i>×<i>d</i>′</sup>, 将维度为<i>d</i>的原始向量<b><i>u</i></b><sub><i>p</i></sub>∈<image href="images/JSJC201903040_079.jpg" type="" display="inline" placement="inline"><alt></alt></image><sup><i>d</i></sup>降至维度为<i>d</i>′:<b><i>W</i></b><sup>T</sup><b><i>u</i></b><sub><i>p</i></sub>∈<image href="images/JSJC201903040_080.jpg" type="" display="inline" placement="inline"><alt></alt></image><sup><i>d</i>′</sup>。因此, 初始序列<b><i>U</i></b>=[<b><i>u</i></b><sub>1</sub>, <b><i>u</i></b><sub>2</sub>, …, <b><i>u</i></b><sub><i>p</i></sub>]∈<image href="images/JSJC201903040_081.jpg" type="" display="inline" placement="inline"><alt></alt></image><sup><i>d</i>×<i>p</i></sup>变为[<b><i>W</i></b><sup>T</sup><b><i>u</i></b><sub>1</sub>, <b><i>W</i></b><sup>T</sup><b><i>u</i></b><sub>2</sub>, …, <b><i>W</i></b><sup>T</sup><b><i>u</i></b><sub><i>p</i></sub>]∈<image href="images/JSJC201903040_082.jpg" type="" display="inline" placement="inline"><alt></alt></image><sup><i>d</i>′×<i>p</i></sup>。</p>
                </div>
                <div class="p1">
                    <p id="83">经过投影后, 来自不同序列类别的特征尽可能不同, 同时相同类别的特征尽可能相同。类别<i>i</i>的平均序列表示为<b><i>M</i></b><sup><i>i</i></sup>=[<i>m</i><mathml id="84"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>1</mn><mi>i</mi></msubsup></mrow></math></mathml>, <i>m</i><mathml id="85"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>2</mn><mi>i</mi></msubsup></mrow></math></mathml>, …, <i>m</i><mathml id="86"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>L</mi><msup><mrow></mrow><mi>i</mi></msup></mrow><mi>i</mi></msubsup></mrow></math></mathml>]∈<image href="images/JSJC201903040_087.jpg" type="" display="inline" placement="inline"><alt></alt></image><sup><i>d</i>×<i>L</i><sup><i>i</i></sup></sup>, 其中<i>L</i><sup><i>i</i></sup>为<b><i>M</i></b><sup><i>i</i></sup>的长度, 类别<i>i</i>的方差为<i>Γ</i><sup><i>i</i></sup>。所有序列类别间的DTW距离总和视为衡量序列类别间可分离性的标准, 序列类内差异定义为平均方差<mathml id="88"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Γ</mi><msub><mrow></mrow><mi>W</mi></msub><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>C</mi></munderover><mi>p</mi></mstyle><msup><mrow></mrow><mi>i</mi></msup><mi>Γ</mi><msup><mrow></mrow><mi>i</mi></msup></mrow></math></mathml>, 其中, <i>p</i><sup><i>i</i></sup>为序列类别<i>i</i>的先验概率。类别<i>i</i>变换后的平均序列为:</p>
                </div>
                <div class="p1">
                    <p id="89"><b><i>W</i></b><sup>T</sup><b><i>M</i></b><sup><i>i</i></sup>=[<b><i>W</i></b><sup>T</sup><i>m</i><mathml id="90"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>1</mn><mi>i</mi></msubsup></mrow></math></mathml>, <b><i>W</i></b><sup>T</sup><i>m</i><mathml id="91"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>2</mn><mi>i</mi></msubsup></mrow></math></mathml>, …, <b><i>W</i></b><sup>T</sup><i>m</i><mathml id="92"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>L</mi><msup><mrow></mrow><mi>i</mi></msup></mrow><mi>i</mi></msubsup></mrow></math></mathml>]      (6) </p>
                </div>
                <div class="p1">
                    <p id="93"><b><i>W</i></b><sup>T</sup><i>Γ</i><sub><i>W</i></sub><b><i>W</i></b>为类别<i>i</i>变换后的序列类内残差, 序列类内差异包含在<b><i>W</i></b>中。计算DTW:</p>
                </div>
                <div class="p1">
                    <p id="94" class="code-formula">
                        <mathml id="94"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mi>W</mi></munder><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mi>Π</mi></munder><mo stretchy="false"> (</mo><mtext>t</mtext><mtext>r</mtext><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">W</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mi>Γ</mi><msub><mrow></mrow><mi>W</mi></msub><mi mathvariant="bold-italic">W</mi><mo stretchy="false">) </mo><msup><mrow></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo>×</mo><mo stretchy="false"> (</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>i</mi></munder><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mi>j</mi></munder><mtext>t</mtext></mstyle></mrow></mstyle><mtext>r</mtext><mo stretchy="false"> (</mo><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">W</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">Μ</mi><msup><mrow></mrow><mi>i</mi></msup><mi>Π</mi><msup><mrow></mrow><mi>i</mi></msup><mo>-</mo></mtd></mtr><mtr><mtd><mi mathvariant="bold-italic">W</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">Μ</mi><msup><mrow></mrow><mi>j</mi></msup><mi>Π</mi><msup><mrow></mrow><mi>j</mi></msup><mo stretchy="false">) </mo><mo>⋅</mo><mi mathvariant="bold-italic">W</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">Μ</mi><msup><mrow></mrow><mi>i</mi></msup><mi>Π</mi><msup><mrow></mrow><mi>i</mi></msup><mo>-</mo><mi mathvariant="bold-italic">W</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">Μ</mi><msup><mrow></mrow><mi>j</mi></msup><mi>Π</mi><msup><mrow></mrow><mi>j</mi></msup><mo stretchy="false">) </mo><msup><mrow></mrow><mtext>Τ</mtext></msup><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo>⇔</mo></mtd></mtr><mtr><mtd><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mi>W</mi></munder><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mi>Π</mi></munder><mo stretchy="false"> (</mo><mtext>t</mtext><mtext>r</mtext><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">W</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mi>Γ</mi><msub><mrow></mrow><mi>W</mi></msub><mi mathvariant="bold-italic">W</mi><mo stretchy="false">) </mo><msup><mrow></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mtext>t</mtext><mtext>r</mtext><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">W</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">B</mi><msub><mrow></mrow><mi>S</mi></msub><mo stretchy="false"> (</mo><mi>Π</mi><mo stretchy="false">) </mo><mi mathvariant="bold-italic">W</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>7</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="95">其中, <i>Π</i><sup><i>i</i></sup>∈{0, 1}<sup><i>L</i><sup><i>i</i></sup>×<i>T</i><sub><i>ij</i></sub></sup>, <i>Π</i><sup><i>j</i></sup>∈{0, 1}<sup><i>L</i><sup><i>j</i></sup>×<i>T</i><sub><i>ij</i></sub></sup>, <i>T</i><sub><i>ij</i></sub>为对齐2个平均序列<b><i>M</i></b><sup><i>i</i></sup>和<b><i>M</i></b><sup><i>j</i></sup>所需要的步长, <i>Π</i>={ (<i>Π</i><sup><i>i</i></sup>, <i>Π</i><sup><i>j</i></sup>) , 1≤<i>i</i>≤<i>j</i>≤<i>C</i>}, <b><i>B</i></b><sub><i>S</i></sub> (<i>Π</i>) 定义为<b><i>B</i></b><sub><i>S</i></sub> (<i>Π</i>) =∑<sub><i>i</i></sub>∑<sub><i>j</i></sub><b><i>B</i></b> (<i>Π</i><sup><i>i</i></sup>, <i>Π</i><sup><i>j</i></sup>) , <b><i>B</i></b> (<i>Π</i><sup><i>i</i></sup>, <i>Π</i><sup><i>j</i></sup>) = (<b><i>M</i></b><sup><i>i</i></sup><i>Π</i><sup><i>i</i></sup>-<b><i>M</i></b><sup><i>j</i></sup><i>Π</i><sup><i>j</i></sup>) (<b><i>M</i></b><sup><i>i</i></sup><i>Π</i><sup><i>i</i></sup>-<b><i>M</i></b><sup><i>j</i></sup><i>Π</i><sup><i>j</i></sup>) <sup>T</sup>。</p>
                </div>
                <div class="p1">
                    <p id="96">用 (<i>Π</i><sup><i>i</i><sup>*</sup></sup>, <i>Π</i><sup><i>j</i><sup>*</sup></sup>) 表示由DTW在初始空间中找到的最佳规整路径, 则LDA可由下式计算:</p>
                </div>
                <div class="p1">
                    <p id="97" class="code-formula">
                        <mathml id="97"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mi>W</mi></munder><mtext>t</mtext><mtext>r</mtext><mo stretchy="false"> (</mo><mrow><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">W</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mi>Γ</mi><msub><mrow></mrow><mi>W</mi></msub><mi mathvariant="bold-italic">W</mi><mo stretchy="false">) </mo></mrow><msup><mrow></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mi mathvariant="bold-italic">W</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">B</mi><msub><mrow></mrow><mi>S</mi></msub><mo stretchy="false"> (</mo><mi>Π</mi><msup><mrow></mrow><mo>*</mo></msup><mo stretchy="false">) </mo><mi mathvariant="bold-italic">W</mi><mo stretchy="false">) </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>8</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="98">其中, <i>Π</i><sup>*</sup>={ (<i>Π</i><sup><i>i</i><sup>*</sup></sup>, <i>Π</i><sup><i>j</i><sup>*</sup></sup>) , 1≤<i>i</i>≤<i>j</i>≤<i>C</i>}, <b><i>B</i></b><sub><i>S</i></sub> (<i>Π</i><sup>*</sup>) 表示序列间差异矩阵。</p>
                </div>
                <h3 id="99" name="99" class="anchor-tag">2 LSDA算法实现步骤</h3>
                <div class="p1">
                    <p id="100">本文运用<i>LSDA</i>降维的人体行为识别算法, 具体步骤如下:</p>
                </div>
                <div class="p1">
                    <p id="101"><b>步骤1</b> 对初始视频帧运用ViBe算法进行背景减除操作, 获取人体行为区域, 去除背景数据的干扰。</p>
                </div>
                <div class="p1">
                    <p id="102"><b>步骤2</b> 在行为区域内对视频进行密集采样, 提取稠密轨迹<citation id="147" type="reference"><link href="31" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>构建特征描述子, 采集方向梯度直方图 (Histogram of Oriented Gradient, HOG) 和轨迹形状作为形状描述符, 利用光流场方向直方图 (Histograms of Oriented Optical Flow, HOF) 和运动边界直方图 (Motion Boundary Histograms, MBH) 作为运动描述符, 最终得到一个426维的描述符。采用PCA对获取的特征进行降维处理, 减少计算量和数据冗余, 然后采用高斯混合模型对特征进行建模, 高斯聚类个数<i>K</i>=256。</p>
                </div>
                <div class="p1">
                    <p id="103"><b>步骤3</b> 使用Fisher Vector对特征进行编码, 编码后得到的特征维数为109 056维。</p>
                </div>
                <div class="p1">
                    <p id="104"><b>步骤4</b> 对降维后的特征进行线性序列差异分析, 利用DTW算法衡量类别间的可分离性。</p>
                </div>
                <div class="p1">
                    <p id="105"><b>步骤5</b> 计算序列类别的平均方差, 并将其作为类内差异, 将原始数据进行线性变换, 从高维空间投影到低维空间, 使类间残差尽可能大而类内残差尽可能小, 从而降低数据维度, 降维后的数据维度为38 170维。</p>
                </div>
                <div class="p1">
                    <p id="106"><b>步骤6</b> 利用降维后的特征向量训练SVM分类器实现最终的人体行为识别。</p>
                </div>
                <div class="p1">
                    <p id="107">具体流程如图1所示。</p>
                </div>
                <div class="area_img" id="108">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201903040_108.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 运用LSDA降维的人体行为识别方法流程" src="Detail/GetImg?filename=images/JSJC201903040_108.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图1 运用LSDA降维的人体行为识别方法流程</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201903040_108.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h3 id="109" name="109" class="anchor-tag">3 仿真结果与分析</h3>
                <div class="p1">
                    <p id="110">实验所用软件环境为OpenCV和Matlab R2016a, 其中, 采用OpenCV提取稠密轨迹以及相关特征, 采用Matlab实现ViBe算法背景减除、线性序列差异分析以及分类预测。本文采用在动作识别领域广为应用的KTH数据集<citation id="148" type="reference"><link href="33" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>和UCF101数据集<citation id="149" type="reference"><link href="35" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>进行算法效果的检验。KTH数据集包含室外、室内、室外尺度变化、室外着装变化4种场景下的6种人体动作, 每个动作由25个人分别完成, 在大多数场景下背景是同质且静止的。UCF101拥有13 320个视频, 101个分类, 每一类动作由25个人完成, 是目前动作类别数、样本数最多的数据库之一。</p>
                </div>
                <h4 class="anchor-tag" id="111" name="111">3.1 基于ViBe算法的背景减除操作</h4>
                <div class="p1">
                    <p id="112">本文首先对初始视频帧运用ViBe算法进行背景减除操作, 获取行为区域并去除背景数据的干扰。分别对KTH数据集中的“走路”视频和“拳击”视频以及UCF101数据集中的“骑车”视频和“弹钢琴”视频进行背景减除操作, 获取的效果图与原视频对比如图2、图3所示。</p>
                </div>
                <div class="area_img" id="113">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201903040_113.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 在KTH数据集上ViBe算法背景减除前后效果对比" src="Detail/GetImg?filename=images/JSJC201903040_113.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图2 在KTH数据集上ViBe算法背景减除前后效果对比</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201903040_113.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="114">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201903040_114.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 在UCF101数据集上ViBe算法背景减除前后效果对比" src="Detail/GetImg?filename=images/JSJC201903040_114.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图3 在UCF101数据集上ViBe算法背景减除前后效果对比</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201903040_114.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="115">从图2、图3可以看出, 运用ViBe算法进行背景减除操作后, 可以去除一些类似于相机移动等背景变化的影响。在图2中, 由于KTH数据集背景简单且静止, 因此运用ViBe算法进行背景减除后效果较好, 可基本获取人体行为区域。而UCF101数据集背景较为复杂且在变化, 其效果不如KTH数据集上取得的效果。</p>
                </div>
                <div class="p1">
                    <p id="116">分别对KTH数据集中的“走路”视频以及UCF101数据集中的“骑车”视频进行背景减除操作后提取稠密轨迹, 并与未进行背景减除操作的稠密轨迹提取过程进行对比, 获取效果如图4所示。</p>
                </div>
                <div class="area_img" id="117">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201903040_117.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 背景减除操作前后稠密轨迹提取效果对比" src="Detail/GetImg?filename=images/JSJC201903040_117.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图4 背景减除操作前后稠密轨迹提取效果对比</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201903040_117.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="118">从图4的对比效果可以看出, 经过背景减除操作后在获取的运动主体区域内提取稠密轨迹, 可有效去除背景冗余信息的影响, 降低特征维数的同时减少计算量。由于KTH数据集的背景静止、单一, 而UCF101数据集的背景复杂且随人体运动, 本文的背景减除操作在KTH数据集上效果较好。</p>
                </div>
                <div class="p1">
                    <p id="119">在KTH数据集和UCF101数据集上进行背景减除操作, 特征所需的存储空间以及以稠密轨迹为特征获得的识别准确率 (<i>η</i>) 结果如表1所示。</p>
                </div>
                <div class="area_img" id="120">
                                            <p class="img_tit">
                                                <b>表1 背景减除操作前后存储空间和识别准确率对比</b>
                                                
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201903040_12000.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/JSJC201903040_12000.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201903040_12000.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表1 背景减除操作前后存储空间和识别准确率对比" src="Detail/GetImg?filename=images/JSJC201903040_12000.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>
                <div class="p1">
                    <p id="121">从表1可以看出, 对视频进行背景减除操作可以去除背景数据的干扰, 节省存储空间。在KTH数据集上可以节省59.15%的存储空间, 在UCF101数据集上可以节省21.93%的存储空间。这是因为KTH数据集大多数视频背景是同质化且静止的, 较为单一, 进行背景减除操作效果明显。UCF101数据集的背景较为复杂, 且背景大多随人体运动, 因此效果相对不明显。</p>
                </div>
                <h4 class="anchor-tag" id="122" name="122">3.2 基于LSDA的降维操作</h4>
                <div class="p1">
                    <p id="123">在KTH数据集上运用本文方法对特征降维来进行人体行为识别, 各动作之间识别准确率的混淆矩阵如表2所示。</p>
                </div>
                <div class="area_img" id="124">
                    <p class="img_tit"><b>表2 本文方法在KTH数据集上的分类混淆矩阵</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="124" border="1"><tr><td>类别</td><td>拳击</td><td>鼓掌</td><td>挥手</td><td>走路</td><td>慢跑</td><td>奔跑</td></tr><tr><td>拳击</td><td>1.00</td><td>0.00</td><td>0.00</td><td>0.00</td><td>0.00</td><td>0.00</td></tr><tr><td><br />鼓掌</td><td>0.00</td><td>0.97</td><td>0.03</td><td>0.00</td><td>0.00</td><td>0.00</td></tr><tr><td><br />挥手</td><td>0.00</td><td>0.03</td><td>0.97</td><td>0.00</td><td>0.00</td><td>0.00</td></tr><tr><td><br />走路</td><td>0.00</td><td>0.00</td><td>0.00</td><td>0.96</td><td>0.04</td><td>0.00</td></tr><tr><td><br />慢跑</td><td>0.00</td><td>0.00</td><td>0.00</td><td>0.04</td><td>0.87</td><td>0.09</td></tr><tr><td><br />奔跑</td><td>0.00</td><td>0.00</td><td>0.00</td><td>0.00</td><td>0.09</td><td>0.91</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="125">由表2可以看出, 拳击识别率为100%。挥手和鼓掌之间有一定的混淆, 鼓掌识别率为97%, 其中有3%被误识为挥手。走路、慢跑和奔跑由于相似度较大, 混淆度稍大, 慢跑有9%的误差落在奔跑上, 有4%的误差落在走路上, 另外有9%的奔跑被误识为慢跑。</p>
                </div>
                <div class="p1">
                    <p id="126">本文采用不同的降维方法来降低以稠密轨迹特征作为视频特征的数据维度, 在此基础上进行人体行为识别, 并对结果进行对比。在UCF101数据集上进行实验, 得到降维后的识别率以及相关特征数据维度的对比结果如表3所示。</p>
                </div>
                <div class="area_img" id="127">
                    <p class="img_tit"><b>表3 在UCF101数据集上5种池化方法的效果对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="127" border="1"><tr><td><br />方法</td><td>识别率/%</td><td>特征维度</td></tr><tr><td><br />PCA</td><td>81.2</td><td>54 528</td></tr><tr><td><br />LDA</td><td>82.9</td><td>44 614</td></tr><tr><td><br />LPP</td><td>87.8</td><td>54 528</td></tr><tr><td><br />SR</td><td>88.6</td><td>40 896</td></tr><tr><td><br />LSDA</td><td>89.3</td><td>38 170</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="128">由表3可以看出, 经过PCA和LPP降维处理后, 特征维度均降为原特征维度的一半, 但LPP降维后的识别率较PCA稍高。LDA虽然对特征维度的降低效果较好, 但识别率与PCA类似。本文提出的LSDA方法可在获取最佳降维效果的基础上保持较高的识别准确率, 其特征维度比LDA降低14.4%, 同时识别率较LPP高1.5%。</p>
                </div>
                <div class="p1">
                    <p id="129">为进一步验证本文方法的有效性, 采用识别准确率 (<i>η</i>) 为评价指标, 分别与文献<citation id="150" type="reference">[<a class="sup">6</a>,<a class="sup">7</a>,<a class="sup">9</a>,<a class="sup">12</a>]</citation>方法在KTH数据集和UCF101数据集上的实验结果进行对比, 结果如表4所示。</p>
                </div>
                <div class="area_img" id="130">
                    <p class="img_tit"><b>表4 不同方法的识别准确率对比</b> % <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="130" border="1"><tr><td><br />方法</td><td>KTH</td><td>UCF101</td></tr><tr><td><br />文献[6]方法</td><td>94.5</td><td>84.0</td></tr><tr><td><br />文献[7]方法</td><td>94.7</td><td>84.2</td></tr><tr><td><br />文献[9]方法</td><td>94.9</td><td>88.3</td></tr><tr><td><br />文献[12]方法</td><td>95.4</td><td>88.9</td></tr><tr><td><br />本文方法</td><td>95.6</td><td>89.3</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="131">由表4可以看出, 由于KTH数据集中的视频背景静止且简单, 上述5种方法均可取得较高的识别准确率。本文方法在KTH数据集上取得95.6%的识别准确率, 与文献<citation id="153" type="reference">[<a class="sup">6</a>,<a class="sup">7</a>,<a class="sup">9</a>,<a class="sup">12</a>]</citation>相比分别提升1.1%、0.9%、0.7%和0.2%, 在一定程度上提高了人体行为的识别准确率。在UCF101数据集上, 本文方法取得89.3%的识别准确率, 与文献<citation id="154" type="reference">[<a class="sup">6</a>,<a class="sup">7</a>,<a class="sup">9</a>,<a class="sup">12</a>]</citation>相比分别提升5.3%、5.1%、1.0%和0.4%, 尤其是与文献<citation id="151" type="reference">[<a class="sup">6</a>]</citation>的PCA方法和文献<citation id="152" type="reference">[<a class="sup">7</a>]</citation>的LDA方法相比, 本文方法的准确率提升较为明显, 实现了良好的识别效果。</p>
                </div>
                <h3 id="132" name="132" class="anchor-tag">4 结束语</h3>
                <div class="p1">
                    <p id="133">本文应用LSDA的降维方法进行人体行为识别。利用ViBe算法对视频帧进行背景减除操作, 得到人体行为区域并提取该区域的稠密轨迹特征。采用Fisher Vector对特征进行编码及线性序列差异分析, 将序列中的特征向量线性映射到一个低维子空间, 降低特征维数。利用降维后的特征训练SVM实现人体行为识别。仿真结果表明, 该方法在KTH数据集和UCF101数据集上均取得较好的识别效果, 可提高人体行为识别准确率。但是, 该方法在应用场景方面还有一定的局限性, 下一步将降低算法复杂度并拓展其应用场景。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJX201506001&amp;v=MDg2MjdlWmVSb0Z5N2xXNzNCTHo3QmRyRzRIOVRNcVk5RlpZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE8=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> 黄凯奇, 陈晓棠, 康运锋, 等.智能视频监控技术综述[J].计算机学报, 2015, 20 (6) :1093-1118.
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ201601010&amp;v=MTMyNjJ2U2RMRzRIOWZNcm85RVpJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSb0Z5N2xXNzNCTHk=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> 单言虎, 张彰, 黄凯奇.人的视觉行为识别研究回顾、现状及展望[J].计算机研究与发展, 2016, 53 (1) :93-112.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MSSB201401005&amp;v=MTc5ODdlWmVSb0Z5N2xXNzNCS0Q3WWJMRzRIOVhNcm85RllZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE8=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> 李瑞峰, 王亮亮, 王珂.人体动作行为识别研究综述[J].模式识别与人工智能, 2014, 27 (1) :35-48.
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJBK&amp;filename=SJBK15090500007384&amp;v=MjY5NDdJRDNROW9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVUxuSUpsNGNhQnM9TmlmSlpiSzlIdGpNcW85RlpPcw==&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b> TURK M, PENTLAND A.Eigenfaces for recognition[J].Journal of Cognitive Neuroscience, 1991, 3 (1) :71-86.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Eigenfaces vs. Fisherfaces: Recognition using class specific linear projection">

                                <b>[5]</b> BELHUMEUR P N, HESPANHA J P, KRIEGMAN D J.Eigenfaces vs.fisherfaces:recognition using class specific linear projection[C]//Proceedings of the 4th European Conference on Computer Vision.Berlin, Germany:Springer, 1996:45-58.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Extracting Refined LowRank Features of Robust PCA for Human Action Recognition">

                                <b>[6]</b> HUANG S, YE J, WANG T, et al.Extracting refined low-rank features of robust pca for human action recognition[J].Arabian Journal for Science and Engineering, 2015, 40 (5) :1427-1441.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=KXJS201713039&amp;v=MDQ4ODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJvRnk3bFc3M0JMalhCZmJHNEg5Yk5ySTlHYllRS0RIODQ=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b> 王淼, 孙季丰, 余家林.基于特征层融合和随机投影的行为识别算法[J].科学技术与工程, 2017, 17 (13) :210-215.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Locality preserving projections">

                                <b>[8]</b> HE X, NIYOGI P.Locality preserving projections[J].Advances in Neural Information Processing Systems, 2002, 16 (1) :186-197.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZGTB201406012&amp;v=MTQxNzZab1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm9GeTdsVzczQlB5cmZiTEc0SDlYTXFZOUU=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b> 王鑫, 沃波海, 管秋, 等.基于流形学习的人体动作识别[J].中国图象图形学报, 2014, 19 (6) :914-923.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Sparse representation for computer vision and pattern recognition">

                                <b>[10]</b> WRIGHT J, MA Y, MAIRAL J, et al.Sparse representation for computer vision and pattern recognition [J].Proceedings of the IEEE, 2010, 98 (6) :1031-1044.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_11" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJF201505005&amp;v=MDU4MTJGeTdsVzczQkx6N0JhTEc0SDlUTXFvOUZZWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUm8=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[11]</b> 张瑞杰, 魏福山.结合Fisher判别分析和稀疏编码的图像场景分类[J].计算机辅助设计与图形学学报, 2015, 27 (5) :808-814.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_12" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=SJSJ201709049&amp;v=MTYzMjBOaWZZWkxHNEg5Yk1wbzlCYllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJvRnk3bFc3M0I=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[12]</b> 肖玉玲.结合HOG/HOF级联特征和多层分类器的人体行为识别[J].计算机工程与设计, 2017, 38 (9) :2567-2572.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_13" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00001681961&amp;v=MTEzNzRxZWJ1ZHRGQzdsVmIvQUlsYz1OajdCYXJPNEh0SE5xWWRFYmUwT1kzazV6QmRoNGo5OVNYcVJyeG94Y01IN1I3&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[13]</b> KEOGH E, RATANAMAHATANA C A.Exact indexing of dynamic time warping[J].Knowledge and Information Systems, 2005, 7 (3) :358-386.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Discriminative dimensionality reduction for multi-dimensional sequences">

                                <b>[14]</b> SU B, DING X, WANG H, et al.Discriminative dimensionality reduction for multi-dimensional sequences[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2018, 40 (1) :77-91.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Action recognition with improved trajectories">

                                <b>[15]</b> WANG H, SCHMID C.Action recognition with improved trajectories[C]//Proceedings of International Conference on Computer Vision.Washington D.C., USA:IEEE Press, 2014:3551-3558.
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Recognizing human actions: a local SVM approach">

                                <b>[16]</b> SCHULDT C, LAPTEV I, CAPUTO B.Recognizing human actions:a local SVM approach[C]//Proceedings of International Conference on Pattern Recognition.Washington D.C., USA:IEEE Press, 2004:32-36.
                            </a>
                        </p>
                        <p id="35">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=UCF101:a dataset of 101 human actions classes from videos in the wild">

                                <b>[17]</b> SOOMRO K, ZAMIR A R, SHAH M.UCF101:a dataset of 101 human actions classes from videos in the wild[EB/OL].[2017-12-25].https://arxiv.org/pdf/1212.0402.pdf.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJC201903040" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201903040&amp;v=MTE1NDg2ajU0TzN6cXFCdEdGckNVUkxPZVplUm9GeTdsVzczQkx6N0JiYkc0SDlqTXJJOUJaSVFLREg4NHZSNFQ=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU45dm52MzRMT3pvcFZMMjlLRVVpST0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
