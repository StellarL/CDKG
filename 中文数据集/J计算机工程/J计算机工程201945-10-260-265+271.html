<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637126203851646250%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJC201910043%26RESULT%3d1%26SIGN%3djIevND7xIdCTIXqhNmLl7k23GVc%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201910043&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJC201910043&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201910043&amp;v=MjI2ODZHNEg5ak5yNDlCWjRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJ0Rnkza1ZMekFMejdCYmI=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qeGdUQmRhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#45" data-title="0 概述 ">0 概述</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#49" data-title="1 基于Deeplabv3+的遥感影像分割 ">1 基于Deeplabv3+的遥感影像分割</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#50" data-title="1.1 Deeplabv3+原理">1.1 Deeplabv3+原理</a></li>
                                                <li><a href="#55" data-title="1.2 精度评价指标">1.2 精度评价指标</a></li>
                                                <li><a href="#59" data-title="1.3 训练策略">1.3 训练策略</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#65" data-title="2 全连接CRF模型 ">2 全连接CRF模型</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#80" data-title="3 实验与结果分析 ">3 实验与结果分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#82" data-title="3.1 噪声数据处理能力分析">3.1 噪声数据处理能力分析</a></li>
                                                <li><a href="#89" data-title="3.2 大规模训练数据采集及训练精度分析">3.2 大规模训练数据采集及训练精度分析</a></li>
                                                <li><a href="#97" data-title="3.3 全连接CRF参数选择及效果分析">3.3 全连接CRF参数选择及效果分析</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#103" data-title="4 结束语 ">4 结束语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#52" data-title="&lt;b&gt;图1 Deeplabv3+原理图&lt;/b&gt;"><b>图1 Deeplabv3+原理图</b></a></li>
                                                <li><a href="#62" data-title="&lt;b&gt;图2 模型精度随迭代步数的变化规律&lt;/b&gt;"><b>图2 模型精度随迭代步数的变化规律</b></a></li>
                                                <li><a href="#64" data-title="&lt;b&gt;表1 不同训练方法及参数对比&lt;/b&gt;"><b>表1 不同训练方法及参数对比</b></a></li>
                                                <li><a href="#85" data-title="&lt;b&gt;图3 标记数据噪声情况&lt;/b&gt;"><b>图3 标记数据噪声情况</b></a></li>
                                                <li><a href="#87" data-title="&lt;b&gt;表2 2种方法准确率和召回率对比&lt;/b&gt;"><b>表2 2种方法准确率和召回率对比</b></a></li>
                                                <li><a href="#88" data-title="&lt;b&gt;图4 Deeplabv3+与文献&lt;/b&gt;&lt;b&gt;方法分割结果对比&lt;/b&gt;"><b>图4 Deeplabv3+与文献</b><b>方法分割结果对比</b></a></li>
                                                <li><a href="#91" data-title="&lt;b&gt;图5 训练样本数据采集算法流程&lt;/b&gt;"><b>图5 训练样本数据采集算法流程</b></a></li>
                                                <li><a href="#93" data-title="&lt;b&gt;图6 训练样本数据采集效果&lt;/b&gt;"><b>图6 训练样本数据采集效果</b></a></li>
                                                <li><a href="#95" data-title="&lt;b&gt;图7 典型要素分割效果&lt;/b&gt;"><b>图7 典型要素分割效果</b></a></li>
                                                <li><a href="#99" data-title="&lt;b&gt;图8 参数比选结果&lt;/b&gt;"><b>图8 参数比选结果</b></a></li>
                                                <li><a href="#101" data-title="&lt;b&gt;图9 基于全连接CRF的要素边界恢复效果&lt;/b&gt;"><b>图9 基于全连接CRF的要素边界恢复效果</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" title=" 蔺海峰,马宇峰,宋涛.基于SIFT特征目标跟踪算法研究[J].自动化学报,2010,36(8):1204-1208." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201008021&amp;v=MDgyODZHNEg5SE1wNDlIWllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJ0Rnkza1ZMekFLQ0xmWWI=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qeGdUQmRhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         蔺海峰,马宇峰,宋涛.基于SIFT特征目标跟踪算法研究[J].自动化学报,2010,36(8):1204-1208.
                                    </a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" title=" NG H P,ONG S H,FOONG K W C,et al.Medical image segmentation using k-means clustering and improved watershed algorithm[C]//Proceedings of Image Analysis and Interpretatio.Southwest Symposium.Washington D.C.,USA:IEEE Press,2006:61-65." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Medical image segmentation using k-means clustering and improved watershed algorithm">
                                        <b>[2]</b>
                                         NG H P,ONG S H,FOONG K W C,et al.Medical image segmentation using k-means clustering and improved watershed algorithm[C]//Proceedings of Image Analysis and Interpretatio.Southwest Symposium.Washington D.C.,USA:IEEE Press,2006:61-65.
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title=" 陶超,谭毅华,蔡华杰,等.面向对象的高分辨率遥感影像城区建筑物分级提取方法[J].测绘学报,2010,39(1):39-45." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=CHXB201001011&amp;v=MTU5NTN5M2tWTHpBSmlYVGJMRzRIOUhNcm85RVpZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSdEY=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qeGdUQmRhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         陶超,谭毅华,蔡华杰,等.面向对象的高分辨率遥感影像城区建筑物分级提取方法[J].测绘学报,2010,39(1):39-45.
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" title=" KRIZHEVSKY A,SUTSKEVER I,HINTON G E.Image net classification with deep convolutional neural networks[C]//Proceedings of Advances in Neural Information Processing Systems.Lake Tahoe,USA:NIPS,2012:1097-1105." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Imagenet classification with deep convolutional neural networks">
                                        <b>[4]</b>
                                         KRIZHEVSKY A,SUTSKEVER I,HINTON G E.Image net classification with deep convolutional neural networks[C]//Proceedings of Advances in Neural Information Processing Systems.Lake Tahoe,USA:NIPS,2012:1097-1105.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" title=" SIMONYAN K,ZISSERMAN A.Very deep convolutional networks for large-scale image recognition[EB/OL].[2018-12-01].https://arxiv.org/abs/1409.1556." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Very Deep Convolutional Networks for Large-Scale Image Recognition">
                                        <b>[5]</b>
                                         SIMONYAN K,ZISSERMAN A.Very deep convolutional networks for large-scale image recognition[EB/OL].[2018-12-01].https://arxiv.org/abs/1409.1556.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" title=" SZEGEDY C,LIU Wei,JIA Yangqing,et al.Going deeper with convolutions[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Press,2015:1-9." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Going deeper with convolutions">
                                        <b>[6]</b>
                                         SZEGEDY C,LIU Wei,JIA Yangqing,et al.Going deeper with convolutions[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Press,2015:1-9.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" title=" LONG J,SHELHAMER E,DARRELL T.Fully convolutional networks for semantic segmentation[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Press,2015:3431-3440." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fully convolutional networks for semantic segmentation">
                                        <b>[7]</b>
                                         LONG J,SHELHAMER E,DARRELL T.Fully convolutional networks for semantic segmentation[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Press,2015:3431-3440.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" title=" BADRINARAYANAN V,KENDALL A,CIPOLLA R.Segnet:a deep convolutional encoder-decoder architecture for image segmentation[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2017,39(12):2481-2495." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=SegNet:A Deep Convolutional Encoder-Decoder Architecture for Scene Segmentation">
                                        <b>[8]</b>
                                         BADRINARAYANAN V,KENDALL A,CIPOLLA R.Segnet:a deep convolutional encoder-decoder architecture for image segmentation[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2017,39(12):2481-2495.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" title=" CHEN L C,PAPANDREOU G,KOKKINOS I,et al.Deeplab:semantic image segmentation with deep convolutional nets,atrous convolution,and fully connected CRFs[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2017,40(4):834-848." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Deep Lab:Semantic Image Segmentation with Deep Convolutional Nets,Atrous Convolution,and Fully Connected CRFs,&amp;quot;">
                                        <b>[9]</b>
                                         CHEN L C,PAPANDREOU G,KOKKINOS I,et al.Deeplab:semantic image segmentation with deep convolutional nets,atrous convolution,and fully connected CRFs[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2017,40(4):834-848.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_10" title=" LIN Guosheng,MILAN A,SHEN Chuanhua,et al.RefineNet:multi-path refinement networks for high-resolution semantic segmentation[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Press,2017:5168-5177." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=RefineNet:Multi-path Refinement Networks for High-Resolution Semantic Segmentation">
                                        <b>[10]</b>
                                         LIN Guosheng,MILAN A,SHEN Chuanhua,et al.RefineNet:multi-path refinement networks for high-resolution semantic segmentation[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Press,2017:5168-5177.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_11" title=" CHEN L C,ZHU Yunkun,PAPANDREOU G,et al.Encoder-decoder with atrous separable convolution for semantic image segmentation[C]//Proceedings of ECCV’18.Berlin,Germany:Springer,2018:801-818." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Encoder-decoder with atrous separable convolution for semantic image segmentation">
                                        <b>[11]</b>
                                         CHEN L C,ZHU Yunkun,PAPANDREOU G,et al.Encoder-decoder with atrous separable convolution for semantic image segmentation[C]//Proceedings of ECCV’18.Berlin,Germany:Springer,2018:801-818.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_12" title=" 冯丽英.基于深度学习技术的高分辨率遥感影像建设用地信息提取研究[D].杭州:浙江大学,2017." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=1017071473.nh&amp;v=MTMwMDdlWmVSdEZ5M2tWTHpBVkYyNkdiTy9IOVhMckpFYlBJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE8=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qeGdUQmRhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[12]</b>
                                         冯丽英.基于深度学习技术的高分辨率遥感影像建设用地信息提取研究[D].杭州:浙江大学,2017.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_13" title=" 陈洋,范荣双,王竞雪,等.基于深度学习的资源三号卫星遥感影像云检测方法[J].光学学报,2018,38(1):362-367." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201801045&amp;v=MDgzMTh0R0ZyQ1VSTE9lWmVSdEZ5M2tWTHpBSWpYVGJMRzRIOW5Ncm85QllZUUtESDg0dlI0VDZqNTRPM3pxcUI=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qeGdUQmRhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[13]</b>
                                         陈洋,范荣双,王竞雪,等.基于深度学习的资源三号卫星遥感影像云检测方法[J].光学学报,2018,38(1):362-367.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_14" title=" CHENG Guangliang,WANG Ying,XU Shibiao,et al.Automatic road detection and centerline extraction via cascaded end-to-end convolutional neural network[J].IEEE Transactions on Geoscience and Remote Sensing,2017,55(6):3322-3337." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Automatic road detection and centerline extraction via cascaded end-to-end convolutional neural network">
                                        <b>[14]</b>
                                         CHENG Guangliang,WANG Ying,XU Shibiao,et al.Automatic road detection and centerline extraction via cascaded end-to-end convolutional neural network[J].IEEE Transactions on Geoscience and Remote Sensing,2017,55(6):3322-3337.
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_15" title=" KRAHENBUHL P,KOLTUN V.Efficient inference in fully connected CRFs with Gaussian edge potentials[J].Advances in Neural Information Processing Systems,2011,24:109-117." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials">
                                        <b>[15]</b>
                                         KRAHENBUHL P,KOLTUN V.Efficient inference in fully connected CRFs with Gaussian edge potentials[J].Advances in Neural Information Processing Systems,2011,24:109-117.
                                    </a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_16" title=" CHOLLET F.Xception:deep learning with depthwise separable convolutions[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Press,2017:1251-1258." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Xception:Deep learning with depthwise separable convolutions">
                                        <b>[16]</b>
                                         CHOLLET F.Xception:deep learning with depthwise separable convolutions[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Press,2017:1251-1258.
                                    </a>
                                </li>
                                <li id="35">


                                    <a id="bibliography_17" title=" HE Kaiming,ZHANG Xiangyu,REN Shaoqing,et al.Deep residual learning for image recognition[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Press,2016:770-778." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep residual learning for image recognition">
                                        <b>[17]</b>
                                         HE Kaiming,ZHANG Xiangyu,REN Shaoqing,et al.Deep residual learning for image recognition[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Press,2016:770-778.
                                    </a>
                                </li>
                                <li id="37">


                                    <a id="bibliography_18" title=" MNIH V.Machine learning for aerial image labeling[D].Toronto,Canada:University of Toronto,2013." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Machine learning for aerial image labeling">
                                        <b>[18]</b>
                                         MNIH V.Machine learning for aerial image labeling[D].Toronto,Canada:University of Toronto,2013.
                                    </a>
                                </li>
                                <li id="39">


                                    <a id="bibliography_19" title=" HAKLAY M,WEBER P.Openstreetmap:User-generated street maps[J].IEEE Pervas Computing,2008,7(4):12-18." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=OpenStreetMap: User-Generated Street Maps">
                                        <b>[19]</b>
                                         HAKLAY M,WEBER P.Openstreetmap:User-generated street maps[J].IEEE Pervas Computing,2008,7(4):12-18.
                                    </a>
                                </li>
                                <li id="41">


                                    <a id="bibliography_20" title=" 胡正平,杨建秀.HOG特征混合模型结合隐SVM的感兴趣目标检测定位算法[J].信号处理,2011,27(8):1206-1212." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XXCN201108016&amp;v=MDQ5NjllUnRGeTNrVkx6QVBUWElZTEc0SDlETXA0OUVZb1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVo=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qeGdUQmRhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[20]</b>
                                         胡正平,杨建秀.HOG特征混合模型结合隐SVM的感兴趣目标检测定位算法[J].信号处理,2011,27(8):1206-1212.
                                    </a>
                                </li>
                                <li id="43">


                                    <a id="bibliography_21" title=" DE BOER P T,KROESE D P,MANNOR S,et al.A tutorial on the cross-entropy method[J].Annals of Operations Research,2005,134(1):19-67." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00000425224&amp;v=MjgyNzdDdmxWcjdQSTFZPU5qN0Jhck80SHRITXE0MUFadWtMWTNrNXpCZGg0ajk5U1hxUnJ4b3hjTUg3UjdxZWJ1ZHRG&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qeGdUQmRhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[21]</b>
                                         DE BOER P T,KROESE D P,MANNOR S,et al.A tutorial on the cross-entropy method[J].Annals of Operations Research,2005,134(1):19-67.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJC" target="_blank">计算机工程</a>
                2019,45(10),260-265+271 DOI:10.19678/j.issn.1000-3428.0053359            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于Deeplabv3+与CRF的遥感影像典型要素提取方法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E4%BF%8A%E5%BC%BA&amp;code=40953981&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qeGdUQmRhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王俊强</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%8E%E5%BB%BA%E8%83%9C&amp;code=20421498&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qeGdUQmRhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">李建胜</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%91%A8%E5%8D%8E%E6%98%A5&amp;code=42117728&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qeGdUQmRhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">周华春</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BC%A0%E6%97%AD&amp;code=39544270&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qeGdUQmRhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">张旭</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%BF%A1%E6%81%AF%E5%B7%A5%E7%A8%8B%E5%A4%A7%E5%AD%A6%E5%9C%B0%E7%90%86%E7%A9%BA%E9%97%B4%E4%BF%A1%E6%81%AF%E5%AD%A6%E9%99%A2&amp;code=0199248&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qeGdUQmRhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">信息工程大学地理空间信息学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=78123%E9%83%A8%E9%98%9F&amp;code=1745004&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qeGdUQmRhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">78123部队</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>为提取高分辨率遥感影像的典型要素(建筑物及道路),基于深度学习,提出一种语义分割与全连接条件随机场(CRF)相结合的提取方法。以Deeplabv3+作为语义分割模型,提取较完整图像分割信息,并将其作为全连接CRF的一元能量函数的输入,利用平均场近似方法进行推理,实现对分割信息边界的优化。通过分析Deeplabv3+模型在噪声样本集数据的训练效果验证其鲁棒性,并基于公开影像及矢量数据源设计大规模遥感训练样本集智能采集系统。采集罗德岛2 000平方公里遥感影像及相对应典型要素标记数据作为样本进行实验,结果表明,该方法分割精度MIoU值达到80.32%,结合形态学滤波处理,要素边界轮廓明显优于初始分割结果。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%81%A5%E6%84%9F%E5%BD%B1%E5%83%8F&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qeGdUQmRhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">遥感影像;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qeGdUQmRhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">语义分割;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qeGdUQmRhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">条件随机场;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%A6%81%E7%B4%A0%E6%8F%90%E5%8F%96&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qeGdUQmRhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">要素提取;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qeGdUQmRhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度学习;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    王俊强(1990—),男,硕士研究生,主研方向为计算机视觉、导航定位与服务;E-mail:xindawangjunqiang@163.com;
                                </span>
                                <span>
                                    *李建胜(通信作者),教授;;
                                </span>
                                <span>
                                    周华春,工程师;;
                                </span>
                                <span>
                                    张旭,硕士研究生。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-12-10</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金(41876150);</span>
                    </p>
            </div>
                    <h1><b>Typical Element Extraction Method of Remote Sensing Image Based on Deeplabv3+ and CRF</b></h1>
                    <h2>
                    <span>WANG Junqiang</span>
                    <span>LI Jiansheng</span>
                    <span>ZHOU Huachun</span>
                    <span>ZHANG Xu</span>
            </h2>
                    <h2>
                    <span>Institute of Geospatial Information,Information Engineering University</span>
                    <span>78123 Troops</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>In order to extract the typical elements(buildings and roads) of high-resolution remote sensing images,based on deep learning,a extraction method combining semantic segmentation and full-connection Conditional Random Field(CRF) is proposed.Using Deeplabv3+ as the semantic segmentation model,the relatively complete image segmentation information is extracted and used as the input of the unary potential function of the fully connected CRF.The mean field approximation method is used to realize the optimization of the segmentation information boundary.The robustness of Deeplabv3+ model is verified by analyzing the training effect of Deeplabv3+ in noise sample set data and based on public image and vector data source,a large-scale remote sensing training sample set intelligent collection system is designed.Collecting 2 000 square kilometers of remote sensing images of Rhode Island and taking corresponding typical elemental marker data as samples to conduct experiments,the results show that the MIoU value of the proposed method reaches 80.23%.After morphological processing,the boundary of the element is obviously clearer than the intial segmentation raslut.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=remote%20sensing%20image&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qeGdUQmRhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">remote sensing image;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=semantic%20segmentation&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qeGdUQmRhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">semantic segmentation;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Conditional%20Random%20Field(CRF)&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qeGdUQmRhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Conditional Random Field(CRF);</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=element%20extraction&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qeGdUQmRhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">element extraction;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=deep%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qeGdUQmRhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">deep learning;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-12-10</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="45" name="45" class="anchor-tag">0 概述</h3>
                <div class="p1">
                    <p id="46">随着对地观测遥感平台的增多,对地观测周期缩短,高分辨率遥感数据急剧增加。有效利用这些影像数据资源,解译分割关注信息,无论在民用还是军事领域都有着广泛的应用。传统基于像素的影像分割算法如SIFT方法<citation id="105" type="reference"><link href="3" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>、分水岭算法<citation id="106" type="reference"><link href="5" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>等易受“椒盐”噪声影响,难以获得理想的分割结果。面向对象的影像分割方法可克服“椒盐”噪声的影响,但需要调整分割尺度来获得合适的影像分割结果,并且合适的尺度难以确定<citation id="107" type="reference"><link href="7" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>。总而言之,传统方式识别结果严重依赖于初始设定的特征,很难充分挖掘影像中深层特征,且特征提取鲁棒性差,不适应多源遥感影像光照变化、分辨率不一致等情况,无法满足大范围自动化应用的需求。</p>
                </div>
                <div class="p1">
                    <p id="47">近年来,随着计算机计算能力的增强,深度学习得到广泛应用,尤其是2012年AlexNet模型在ILSVRC2012数据集上以绝对优势赢得ImageNet挑战赛第一名后,深度学习算法开始被广泛应用于计算机视觉领域,在图像分类、自然语言处理、目标检测等方面取得较大的成果<citation id="116" type="reference"><link href="9" rel="bibliography" /><link href="11" rel="bibliography" /><link href="13" rel="bibliography" /><sup>[<a class="sup">4</a>,<a class="sup">5</a>,<a class="sup">6</a>]</sup></citation>。例如在图像语义分割领域,FCN<citation id="108" type="reference"><link href="15" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>、SegNet<citation id="109" type="reference"><link href="17" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>、Deeplabv2<citation id="110" type="reference"><link href="19" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>、RefineNet<citation id="111" type="reference"><link href="21" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>以及Deeplabv3+<citation id="112" type="reference"><link href="23" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>等算法在公开自然场景数据集上的检测效果和性能都很出色,尤其是Deeplabv3+算法在公开数据集PASCAL VOC 2012的平均交并比(Mean Intersection Over Union,MIoU)达到89%,较之前算法大幅度提升。因此,不少学者开始研究利用深度学习的方法进行遥感影像分割。文献<citation id="113" type="reference">[<a class="sup">12</a>]</citation>利用深度卷积神经网络实现建设用地的提取,总体精度优于传统的面向对象方法。文献<citation id="114" type="reference">[<a class="sup">13</a>]</citation>建立了自适应池化模型,更好地挖掘影像特征信息,实现资源三号卫星遥感影像云的检测提取。文献<citation id="115" type="reference">[<a class="sup">14</a>]</citation>利用级联式端对端CNN网络,实现道路网及其中心线的提取。然而,上述研究忽略了对大范围的上下文信息捕获,容易造成边界信息的模糊。Deeplabv3+通过引入空洞空间金字塔池化(Atrous Spatial Pyramid Pooling,ASPP)模块,可以在多尺度上捕获信息,增大感受野,提升边界分割效果。尽管Deeplabv3+在提取局部特征和利用较小感受野进行预测方面效果较好,但其缺少利用全局上下文信息的能力。遥感影像相对常规自然场景图片具有分辨率低、幅面大、目标尺度小等特点,仅利用Deeplabv3+进行影像分割,无法满足高精度遥感影像要素轮廓提取需求。</p>
                </div>
                <div class="p1">
                    <p id="48">本文提出一种结合Deeplabv3+深度学习与全连接条件随机场(Conditional Random Field,CRF)<citation id="117" type="reference"><link href="31" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>的典型要素提取方法。以建筑物及道路为例,利用Deeplabv3+方法获得分割图,将分割信息及图像本身特征信息作为全连接CRF的输入,进行分割要素的边界有效推断,从而实现边界信息的完整生成。</p>
                </div>
                <h3 id="49" name="49" class="anchor-tag">1 基于Deeplabv3+的遥感影像分割</h3>
                <h4 class="anchor-tag" id="50" name="50">1.1 Deeplabv3+原理</h4>
                <div class="p1">
                    <p id="51">文献<citation id="118" type="reference">[<a class="sup">11</a>]</citation>提出的Deeplabv3+是Deeplab系列中的最新版本,其采用编码器-解码器的方式,原理如图1所示。</p>
                </div>
                <div class="area_img" id="52">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201910043_052.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qeGdUQmRhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 Deeplabv3+原理图" src="Detail/GetImg?filename=images/JSJC201910043_052.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qeGdUQmRhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图1 Deeplabv3+原理图</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201910043_052.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qeGdUQmRhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="53">编码器以Xception网络<citation id="119" type="reference"><link href="33" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>作为特征提取骨架网络,该网络结构由一系列深度可分离卷积、类似ResNet<citation id="120" type="reference"><link href="35" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>中的残差连接和一些其他常规的操作组成。相比于ResNet-101,Xception网络具有更强的鲁棒性,在ImageNet数据集上Top-1和Top-5的分类精度分别提高了0.75%和0.29%。本文在Xception网络中引入ASPP模块,解决物体鲁棒分割问题。将深度卷积网络得到的图像级特征图(为原图的1/16)输入到一个256通道的1*1卷积层中,卷积后的特征图在解码器中使用。</p>
                </div>
                <div class="p1">
                    <p id="54">解码器借鉴FCN的跳步连接方式,连接低层次特征与高层次特征,首先利用48通道1*1卷积对低层次特征图卷积,减少特征图通道数,再将其与经过4倍放大的双线性内插上采样的高层次特征图融合,最后进行3*3卷积操作后经4倍放大的双线性内插恢复至原图分别率,实现与原图等大的分割图。</p>
                </div>
                <h4 class="anchor-tag" id="55" name="55">1.2 精度评价指标</h4>
                <div class="p1">
                    <p id="56">用于评估语义分割算法性能的标准指标是MIoU。假设图像分割共有<i>k</i>+1个类,其中包含一个背景类,<i>p</i><sub><i>ij</i></sub>表示本属于类<i>i</i>但被预测为类<i>j</i>的像素数量,即<i>p</i><sub><i>ii</i></sub>表示真正例的数量而<i>p</i><sub><i>ij</i></sub>、<i>p</i><sub><i>ji</i></sub>分别假正例和假负例的数量,则MIoU可表示为:</p>
                </div>
                <div class="p1">
                    <p id="57" class="code-formula">
                        <mathml id="57"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Μ</mi><msub><mrow></mrow><mrow><mtext>Μ</mtext><mtext>Ι</mtext><mtext>o</mtext><mtext>U</mtext></mrow></msub><mo>=</mo><mfrac><mn>1</mn><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow><mi>k</mi></munderover><mrow><mfrac><mrow><mi>p</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>i</mi></mrow></msub></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>0</mn></mrow><mi>k</mi></munderover><mi>p</mi></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>+</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow><mi>k</mi></munderover><mi>p</mi></mstyle><msub><mrow></mrow><mrow><mi>j</mi><mi>i</mi></mrow></msub><mo>-</mo><mi>p</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>i</mi></mrow></msub></mrow></mfrac></mrow></mstyle><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="58">通过式(1)可以判断目标的捕获程度(使预测标签与标注尽可能重合),也可以判断模型的精确程度(使并集尽可能重合)。</p>
                </div>
                <h4 class="anchor-tag" id="59" name="59">1.3 训练策略</h4>
                <div class="p1">
                    <p id="60">利用数据增强方式能够提高训练样本的多样性,防止在训练过程中样本不足带来的过拟合,同时增强模型的鲁棒性。依据影像目标色彩、形状、纹理等特征,本文采用5种数据增强方式,分别为随机旋转变换0°～360°、噪声扰动、色彩抖动、随机非等比例缩放0.8倍～1.2倍以及翻转变换90°、180°或270°。</p>
                </div>
                <div class="p1">
                    <p id="61">本文利用迁移学习加快训练速度,并对公开数据集上的预训练模型初始化权值。由于遥感影像相对于自然场景数据特点不一样,为获得最佳训练效果,以马萨诸塞州建筑物数据集为训练集,首先分析迭代步数对模型精度的影响,如图2所示。</p>
                </div>
                <div class="area_img" id="62">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201910043_062.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qeGdUQmRhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 模型精度随迭代步数的变化规律" src="Detail/GetImg?filename=images/JSJC201910043_062.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qeGdUQmRhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图2 模型精度随迭代步数的变化规律</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201910043_062.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qeGdUQmRhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="63">从图2可知,当训练一定步数后,模型精度变化较缓慢。为此,本文尝试双次迁移方式,即先在预训练模型上迁移训练少量步数,再在生成的模型上进行一次迁移。本文设计了4种训练方法分别进行训练,其模型精度如表1所示。显然,方法4模型精度要优于其他方法,达到81.2%。因此,本文选择训练方法4作为遥感影像语义分割训练方法。</p>
                </div>
                <div class="area_img" id="64">
                    <p class="img_tit"><b>表1 不同训练方法及参数对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="64" border="1"><tr><td>编号</td><td>学习率</td><td>衰减步数</td><td>迁移方式</td><td>MIoU/%</td></tr><tr><td>训练方法1</td><td>0.000 1</td><td>2 000</td><td>单次迁移</td><td>76.0</td></tr><tr><td><br />训练方法2</td><td>0.001 0</td><td>10 000</td><td>单次迁移</td><td>80.2</td></tr><tr><td><br />训练方法3</td><td>0.002 0</td><td>20 000</td><td>单次迁移</td><td>80.9</td></tr><tr><td><br />训练方法4</td><td>0.002 0</td><td>20 000</td><td>双次迁移</td><td>81.2</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h3 id="65" name="65" class="anchor-tag">2 全连接CRF模型</h3>
                <div class="p1">
                    <p id="66">基于Deeplabv3+的编码器-解码器方式,可实现要素的分割,并且保持较高精度,但其分割边缘信息细粒度依然不够完整。在图像物体分割领域,本文采用CRF对像素类别进行判断。由于该模型考虑了像素与其相邻像素之间关系,可高效区分不同类别间的界限<citation id="121" type="reference"><link href="31" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="67">假设一幅图像包含<i>N</i>个像素,<i>I</i>=(<i>I</i><sub>1</sub>,<i>I</i><sub>2</sub>,…,<i>I</i><sub><i>N</i></sub>)为观测量,<i>X</i>=(<i>X</i><sub>1</sub>,<i>X</i><sub>2</sub>,…,<i>X</i><sub><i>N</i></sub>)为随机变量集合,<i>X</i>每个变量值域为<i>l</i>=(<i>l</i><sub>1</sub>,<i>l</i><sub>2</sub>,…,<i>l</i><sub><i>k</i></sub>),代表<i>k</i>个标记类别。(<i>X</i>,<i>I</i>)构成CRF,其概率分布服从吉布斯分布,可表示为:</p>
                </div>
                <div class="p1">
                    <p id="68" class="code-formula">
                        <mathml id="68"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><mo stretchy="false">(</mo><mi>X</mi><mo stretchy="false">|</mo><mi>Ι</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><mrow><mi>Ζ</mi><mo stretchy="false">(</mo><mi>Ι</mi><mo stretchy="false">)</mo></mrow></mfrac><mrow><mi>exp</mi></mrow><mo stretchy="false">(</mo><mo>-</mo><mi>E</mi><mo stretchy="false">(</mo><mi>X</mi><mo stretchy="false">|</mo><mi>Ι</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="69">其中,<i>Z</i>(<i>I</i>)为规范化因子,<i>E</i>(<i>X</i>|<i>I</i>)为能量函数。则CRF的最大后验概率问题转化为能量函数的最小化问题。能量函数可表示为:</p>
                </div>
                <div class="p1">
                    <p id="70" class="code-formula">
                        <mathml id="70"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>E</mi><mo stretchy="false">(</mo><mi>X</mi><mo stretchy="false">|</mo><mi>Ι</mi><mo stretchy="false">)</mo><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>i</mi></munder><mi>θ</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo>+</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>i</mi><mi>j</mi></mrow></munder><mi>θ</mi></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mi>x</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>3</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="71">其中,<i>θ</i><sub><i>i</i></sub>(<i>x</i><sub><i>i</i></sub>)为单个随机变量<i>x</i><sub><i>i</i></sub>的一元能量函数项,表示随机变量<i>x</i><sub><i>i</i></sub>为某个类别的代价。本文以Deeplabv3+经过Softmax层输出的结果作为一元能量函数的输入,其计算公式为:</p>
                </div>
                <div class="p1">
                    <p id="72"><i>θ</i><sub><i>i</i></sub>(<i>x</i><sub><i>i</i></sub>)=-lb <i>P</i>(<i>x</i><sub><i>i</i></sub>)</p>
                </div>
                <div class="p1">
                    <p id="73"><i>θ</i><sub><i>ij</i></sub>(<i>x</i><sub><i>i</i></sub>,<i>x</i><sub><i>j</i></sub>)为关于相互连接的2个随机变量(<i>x</i><sub><i>i</i></sub>,<i>x</i><sub><i>j</i></sub>)的二元成对能量函数项,表示2个变量类别一致性代价,可表示为高斯核函数的线性组合:</p>
                </div>
                <div class="p1">
                    <p id="74" class="code-formula">
                        <mathml id="74"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>θ</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mi>x</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mi>μ</mi><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mi>x</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>m</mi><mo>=</mo><mn>1</mn></mrow><mi>Κ</mi></munderover><mi>w</mi></mstyle><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>m</mi><mo stretchy="false">)</mo></mrow></msup><mi>k</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>m</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">(</mo><mi mathvariant="bold-italic">f</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mi mathvariant="bold-italic">f</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>4</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="75">其中,<i>k</i><sup>(</sup><sup><i>m</i></sup><sup>)</sup>为一个高斯核函数,<i>w</i><sup>(</sup><sup><i>m</i></sup><sup>)</sup>为线性组合权值,<i>μ</i>为类标一致性参数,<i><b>f</b></i><sub><i>i</i></sub>和<i><b>f</b></i><sub><i>j</i></sub>分别为变量<i>x</i><sub><i>i</i></sub>和<i>x</i><sub><i>j</i></sub>的特征向量。对于多类别分割任务,文献<citation id="122" type="reference">[<a class="sup">15</a>]</citation>将颜色和位置2种特征组合构建核函数,即:</p>
                </div>
                <div class="p1">
                    <p id="76" class="code-formula">
                        <mathml id="76"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>k</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">f</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mi mathvariant="bold-italic">f</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mi>w</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><mrow><mi>exp</mi></mrow><mo stretchy="false">(</mo><mo>-</mo><mfrac><mrow><mrow><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">Ρ</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mi mathvariant="bold-italic">Ρ</mi><msub><mrow></mrow><mi>j</mi></msub></mrow><mo>|</mo></mrow></mrow><msup><mrow></mrow><mn>2</mn></msup></mrow><mrow><mn>2</mn><mi>θ</mi><msubsup><mrow></mrow><mi>α</mi><mn>2</mn></msubsup></mrow></mfrac><mo>-</mo><mfrac><mrow><mrow><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">Ι</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mi mathvariant="bold-italic">Ι</mi><msub><mrow></mrow><mi>j</mi></msub></mrow><mo>|</mo></mrow></mrow><msup><mrow></mrow><mn>2</mn></msup></mrow><mrow><mn>2</mn><mi>θ</mi><msubsup><mrow></mrow><mi>β</mi><mn>2</mn></msubsup></mrow></mfrac><mo stretchy="false">)</mo><mo>+</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mi>w</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></msup><mrow><mi>exp</mi></mrow><mo stretchy="false">(</mo><mo>-</mo><mfrac><mrow><mrow><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">Ρ</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mi mathvariant="bold-italic">Ρ</mi><msub><mrow></mrow><mi>j</mi></msub></mrow><mo>|</mo></mrow></mrow><msup><mrow></mrow><mn>2</mn></msup></mrow><mrow><mn>2</mn><mi>θ</mi><msubsup><mrow></mrow><mi>γ</mi><mn>2</mn></msubsup></mrow></mfrac><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>5</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="77">其中,<i><b>I</b></i>为3维颜色向量,<i><b>P</b></i>为2维像素点坐标向量,<i>θ</i><sub><i>α</i></sub>、<i>θ</i><sub><i>β</i></sub>和<i>θ</i><sub><i>γ</i></sub>为尺度参数。</p>
                </div>
                <div class="p1">
                    <p id="78">全连接CRF模型将概率图中的每个节点与其他任意节点相连,图中边的数量远大于局部连接,将带来巨大计算量。因此,本文采用文献<citation id="123" type="reference">[<a class="sup">15</a>]</citation>基于平均场近似的方法处理全连接CRF模型的推理,通过最小化平均场近似<i>Q</i>(<i>X</i>|<i>I</i>)与<i>P</i>(<i>X</i>|<i>I</i>)的K-L散度,得到模型最优解。本文采用网格搜索法对式(5)涉及的尺度和权重参数进行搜索确定。式(5)中的<i>w</i><sup>(2)</sup>和<i>θ</i><sub><i>γ</i></sub>作用于过滤小区域,对分类精度影响较小,但可提升可视效果。根据文献<citation id="124" type="reference">[<a class="sup">15</a>]</citation>的结论,设<i>w</i><sup>(2)</sup>=<i>θ</i><sub><i>γ</i></sub>=1,因此,只需要对<i>θ</i><sub><i>α</i></sub>、<i>θ</i><sub><i>β</i></sub>和<i>w</i><sup>(1)</sup>参数进行搜索。</p>
                </div>
                <div class="p1">
                    <p id="79">通过全连接CRF方式推理得到影像像素最终类标之后,使用形态学滤波方式过滤一些孤立的像素点,得到最终要素提取分割结果。</p>
                </div>
                <h3 id="80" name="80" class="anchor-tag">3 实验与结果分析</h3>
                <div class="p1">
                    <p id="81">首先,为验证Deeplabv3+对于噪声数据处理能力,本文采用带噪声的马萨诸塞州航空影像建筑物数据集<citation id="125" type="reference"><link href="37" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>进行训练,并与其结果对比分析。其次,本文基于公开矢量数据集Openstreetmap<citation id="126" type="reference"><link href="39" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>及影像数据设计训练样本生成系统,半自动采集2 000平方公里罗德岛典型要素(建筑物及道路)标记数据及影像数据用于训练,验证本文模型对带有噪声的高分辨率卫星影像大规模数据集的学习能力。最后,利用全连接CRF对分割数据进行处理分析。整个过程通过Python与TensorFlow深度学习库编程实现,实验操作系统为Ubuntu16.04,16 GB内存并配置GTX1080Ti显卡,计算机分辨率为1 920像素×1 200像素。</p>
                </div>
                <h4 class="anchor-tag" id="82" name="82">3.1 噪声数据处理能力分析</h4>
                <div class="p1">
                    <p id="83">相比于人工手动采集标记,依据现有遥感影像与其相对应区域矢量地图数据自动或半自动采集大规模遥感影像语义分割样本,是一种高效方式。但由于遥感影像现势性与矢量地图数据可能不完全一致,或者由于实际坐标的偏差导致遥感影像和矢量地图数据不完全套合,势必会带来数据噪声,因此,将Deeplabv3+模型应用于遥感影像分割,需要验证其基于噪声数据的学习能力。</p>
                </div>
                <div class="p1">
                    <p id="84">文献<citation id="127" type="reference">[<a class="sup">18</a>]</citation>将真实标记数据误差分为遗漏噪声和配准噪声,其表现形式如图3所示。针对这些数据噪声,文献<citation id="128" type="reference">[<a class="sup">18</a>]</citation>提出一种基于卷积神经网络的遥感影像分割模型TABN,通过引入2个鲁棒性强的损失函数来降低2类噪声的影响,并且利用马萨诸塞州建筑物数据集进行实验效果分析。</p>
                </div>
                <div class="area_img" id="85">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201910043_085.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qeGdUQmRhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 标记数据噪声情况" src="Detail/GetImg?filename=images/JSJC201910043_085.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qeGdUQmRhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图3 标记数据噪声情况</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201910043_085.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qeGdUQmRhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="86">马萨诸塞州建筑物数据集由151张1.5 km×1.5 km的影像及对应建筑物标记组成,其每张图像代表2.25 km<sup>2</sup>区域,总数据量覆盖约340 km<sup>2</sup>,数据噪声率不高于5%。本文利用Deeplabv3+模型对该数据集进行训练学习,在测试集上进行评价,获得MIoU值为83.1%,将其与文献<citation id="129" type="reference">[<a class="sup">18</a>]</citation>的结果进行比较,如图4所示。表2为2种方法的准确率和召回率统计。显然,无论召回率还是准确率,Deeplabv3+方法均要优于文献<citation id="130" type="reference">[<a class="sup">18</a>]</citation>方法,这说明Deeplabv3+具有更强的鲁棒性,处理噪声数据能力更优。</p>
                </div>
                <div class="area_img" id="87">
                    <p class="img_tit"><b>表2 2种方法准确率和召回率对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note">%</p>
                    <table id="87" border="1"><tr><td><br />方法</td><td>准确率</td><td>召回率</td></tr><tr><td><br />文献[18]方法</td><td>90.5</td><td>83.5</td></tr><tr><td><br />Deeplabv3+方法</td><td>93.1</td><td>84.6</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="88">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201910043_088.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qeGdUQmRhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 Deeplabv3+与文献[18]方法分割结果对比" src="Detail/GetImg?filename=images/JSJC201910043_088.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qeGdUQmRhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图4 Deeplabv3+与文献</b><citation id="131" type="reference">[<a class="sup">18</a>]</citation><b>方法分割结果对比</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201910043_088.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qeGdUQmRhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="89" name="89">3.2 大规模训练数据采集及训练精度分析</h4>
                <div class="p1">
                    <p id="90">通过上述分析,验证了Deeplabv3+可适应带有少量噪声的数据集。Deeplabv3+模型需要大规模数据集作为训练基础,而常规方法进行标注虽然标注精度高但耗时耗力。由于数据更新的时效性问题,公开矢量数据集Openstreetmap和影像数据不完全套合,存在噪声,但这种噪声可通过人工筛选降低。因此,为解决训练样本数据集获取问题,本文设计大规模训练样本数据采集系统,该系统基于WebGIS实现,通过网格划分的方式实现大区域样本数据采集,其算法流程如图5所示。</p>
                </div>
                <div class="area_img" id="91">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201910043_091.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qeGdUQmRhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 训练样本数据采集算法流程" src="Detail/GetImg?filename=images/JSJC201910043_091.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qeGdUQmRhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图5 训练样本数据采集算法流程</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201910043_091.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qeGdUQmRhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="92">由于数据集部分地区影像和矢量标记完全不匹配,会带来较大数据误差,因此本文将遍历网格改为半自动方式,采集样本为500×500。如图6所示,共采集7 992张图像及相应标记,标记分为3个类别(道路、建筑及背景),采集影像层级为18层级。</p>
                </div>
                <div class="area_img" id="93">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201910043_093.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qeGdUQmRhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 训练样本数据采集效果" src="Detail/GetImg?filename=images/JSJC201910043_093.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qeGdUQmRhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图6 训练样本数据采集效果</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201910043_093.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qeGdUQmRhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="94">将该数据集中7 000张图像作为训练验证集,992张图像作为测试集,采用二次迁移训练策略进行训练,初始学习率设置为0.002,每20 000步衰减0.1,学习冲量为0.9,设置训练图像最高维度为513,APPS模块采用<citation id="133" type="reference">[<a class="sup">6</a>,<a class="sup">12</a>,<a class="sup">18</a>]</citation>3种扩展率。以交叉熵损失函数<citation id="132" type="reference"><link href="41" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>作为目标函数,采用随机梯度下降法进行训练优化,考虑道路、建筑及背景3种标记样本数量不平衡,在训练时对道路、建筑赋予更大权值,三者权值比设定为2∶2∶1。训练50 000步后,在测试集上进行分割精度验证,结果显示:建筑物IoU为82.9%,道路IoU为77.5%,两者MIoU为80.2%。建筑物识别精度优于道路,其原因是区域内道路标记通过线要素缓冲获取,并不能代表真实道路边界。图7为模型在测试集上的典型要素提取效果图。</p>
                </div>
                <div class="area_img" id="95">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201910043_095.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qeGdUQmRhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 典型要素分割效果" src="Detail/GetImg?filename=images/JSJC201910043_095.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qeGdUQmRhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图7 典型要素分割效果</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201910043_095.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qeGdUQmRhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="96">为保证清晰对比,将道路及建筑物分开提取。从对道路的分割来看,路网要素能够完整提取,连通性较好,尤其是图7(a)中第2张输入图像,中间路段有部分被植被遮挡,但提取数据依然能够连通,说明模型的感受野范围大,全局特征好,鲁棒性强,能够适应少量遮挡情况下的要素完整提取。从对建筑物要素的分割来看,大小建筑物均能在一定程度分割,说明模型具备多尺度要素分割能力,并且大型建筑物边缘分割效果要优于小型建筑物,说明分割效果受影像提取要素分辨率影响较大。总的来说,Deeplabv3+能够较好识别典型要素,具有较高精度,但其边界提取效果依然无法满足对高精度边界轮廓要素提取的需求。</p>
                </div>
                <h4 class="anchor-tag" id="97" name="97">3.3 全连接CRF参数选择及效果分析</h4>
                <div class="p1">
                    <p id="98">本文利用网格搜索法对<i>θ</i><sub><i>α</i></sub>、<i>θ</i><sub><i>β</i></sub>和<i>w</i><sup>(1)</sup>参数进行比对实验。实验首先固定<i>θ</i><sub><i>β</i></sub>=1、<i>w</i><sup>(1)</sup>=1,仅考虑<i>θ</i><sub><i>α</i></sub>取值对MIoU的影响,结果如图8(a)所示,当<i>θ</i><sub><i>α</i></sub>=25时,MIoU达到最优。图8(b)显示<i>θ</i><sub><i>α</i></sub>=25时单张图像推断平均耗时下降较慢,综合考虑,确定<i>θ</i><sub><i>α</i></sub>=25。其次固定<i>θ</i><sub><i>α</i></sub>=25、<i>w</i><sup>(1)</sup>=1,考虑<i>θ</i><sub><i>β</i></sub>取值的影响,图8(a)显示,当<i>θ</i><sub><i>β</i></sub>=1时,MIoU为最优,并且随着<i>θ</i><sub><i>β</i></sub>增加下降较快,图8(b)显示<i>θ</i><sub><i>β</i></sub>=1时单张图像推断平均耗时下降迅速,综合考虑,确定<i>θ</i><sub><i>β</i></sub>=1。同理,可比选参数<i>w</i><sup>(1)</sup>=1时最优。最终,确定<i>θ</i><sub><i>α</i></sub>=25、<i>θ</i><sub><i>β</i></sub>=1及<i>w</i><sup>(1)</sup>=1为最优参数组合,MIoU值达到80.32%,较未加入CRF提升0.12%。</p>
                </div>
                <div class="area_img" id="99">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201910043_099.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qeGdUQmRhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 参数比选结果" src="Detail/GetImg?filename=images/JSJC201910043_099.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qeGdUQmRhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图8 参数比选结果</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201910043_099.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qeGdUQmRhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="100">将Deeplabv3+模型经过Softmax函数处理的结果作为全连接CRF一元能量函数的输入,二元能量函数由图像颜色和位置特征构建,进行高分辨率遥感影像建筑物的边界有效推理。由于影像分辨率的问题,本文只针对较大建筑物进行推理分析,并过滤较小目标,恢复完整边界信息。图9为大型建筑物基于全连接CRF边界恢复效果。</p>
                </div>
                <div class="area_img" id="101">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJC201910043_101.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qeGdUQmRhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图9 基于全连接CRF的要素边界恢复效果" src="Detail/GetImg?filename=images/JSJC201910043_101.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qeGdUQmRhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图9 基于全连接CRF的要素边界恢复效果</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJC201910043_101.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qeGdUQmRhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="102">从图9可知,Deeplabv3+分割边界较粗糙,经过全连接CRF推理后,边界轮廓清晰,经过形态学滤波及闭运算操作后,可获得最终要素边界信息,从而实现要素的高精度提取。</p>
                </div>
                <h3 id="103" name="103" class="anchor-tag">4 结束语</h3>
                <div class="p1">
                    <p id="104">本文针对高分辨率遥感影像典型要素(建筑物与道路)的提取,提出一种结合深度学习与概率图模型的方法,将深度学习得到的分割结果,采用全连接CRF进行推理,获取边界信息。通过大规模训练样本数据采集系统获取实验样本并进行实验验证,结果表明,本文方法能从高精度遥感图像中获得典型要素边界信息。由于全连接CRF受图像颜色及位置特征影响,在颜色特征不明显或者要素分辨率较低的情况下,其提取效果不理想,因此下一步将引入HOG特征<citation id="134" type="reference"><link href="43" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>定义的高斯核函数来避免阴影和环境光照对要素颜色的影响。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201008021&amp;v=MTkxNjFIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJ0Rnkza1ZMekFLQ0xmWWJHNEg5SE1wNDlIWllRS0Q=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qeGdUQmRhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> 蔺海峰,马宇峰,宋涛.基于SIFT特征目标跟踪算法研究[J].自动化学报,2010,36(8):1204-1208.
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Medical image segmentation using k-means clustering and improved watershed algorithm">

                                <b>[2]</b> NG H P,ONG S H,FOONG K W C,et al.Medical image segmentation using k-means clustering and improved watershed algorithm[C]//Proceedings of Image Analysis and Interpretatio.Southwest Symposium.Washington D.C.,USA:IEEE Press,2006:61-65.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=CHXB201001011&amp;v=MDQ2Nzh6QUppWFRiTEc0SDlITXJvOUVaWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUnRGeTNrVkw=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qeGdUQmRhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> 陶超,谭毅华,蔡华杰,等.面向对象的高分辨率遥感影像城区建筑物分级提取方法[J].测绘学报,2010,39(1):39-45.
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Imagenet classification with deep convolutional neural networks">

                                <b>[4]</b> KRIZHEVSKY A,SUTSKEVER I,HINTON G E.Image net classification with deep convolutional neural networks[C]//Proceedings of Advances in Neural Information Processing Systems.Lake Tahoe,USA:NIPS,2012:1097-1105.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Very Deep Convolutional Networks for Large-Scale Image Recognition">

                                <b>[5]</b> SIMONYAN K,ZISSERMAN A.Very deep convolutional networks for large-scale image recognition[EB/OL].[2018-12-01].https://arxiv.org/abs/1409.1556.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Going deeper with convolutions">

                                <b>[6]</b> SZEGEDY C,LIU Wei,JIA Yangqing,et al.Going deeper with convolutions[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Press,2015:1-9.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fully convolutional networks for semantic segmentation">

                                <b>[7]</b> LONG J,SHELHAMER E,DARRELL T.Fully convolutional networks for semantic segmentation[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Press,2015:3431-3440.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=SegNet:A Deep Convolutional Encoder-Decoder Architecture for Scene Segmentation">

                                <b>[8]</b> BADRINARAYANAN V,KENDALL A,CIPOLLA R.Segnet:a deep convolutional encoder-decoder architecture for image segmentation[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2017,39(12):2481-2495.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Deep Lab:Semantic Image Segmentation with Deep Convolutional Nets,Atrous Convolution,and Fully Connected CRFs,&amp;quot;">

                                <b>[9]</b> CHEN L C,PAPANDREOU G,KOKKINOS I,et al.Deeplab:semantic image segmentation with deep convolutional nets,atrous convolution,and fully connected CRFs[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2017,40(4):834-848.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=RefineNet:Multi-path Refinement Networks for High-Resolution Semantic Segmentation">

                                <b>[10]</b> LIN Guosheng,MILAN A,SHEN Chuanhua,et al.RefineNet:multi-path refinement networks for high-resolution semantic segmentation[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Press,2017:5168-5177.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Encoder-decoder with atrous separable convolution for semantic image segmentation">

                                <b>[11]</b> CHEN L C,ZHU Yunkun,PAPANDREOU G,et al.Encoder-decoder with atrous separable convolution for semantic image segmentation[C]//Proceedings of ECCV’18.Berlin,Germany:Springer,2018:801-818.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_12" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=1017071473.nh&amp;v=MzAzNzJySkViUElRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJ0Rnkza1ZMekFWRjI2R2JPL0g5WEw=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qeGdUQmRhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[12]</b> 冯丽英.基于深度学习技术的高分辨率遥感影像建设用地信息提取研究[D].杭州:浙江大学,2017.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_13" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201801045&amp;v=MjY3MDJYVGJMRzRIOW5Ncm85QllZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVSdEZ5M2tWTHpBSWo=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qeGdUQmRhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[13]</b> 陈洋,范荣双,王竞雪,等.基于深度学习的资源三号卫星遥感影像云检测方法[J].光学学报,2018,38(1):362-367.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Automatic road detection and centerline extraction via cascaded end-to-end convolutional neural network">

                                <b>[14]</b> CHENG Guangliang,WANG Ying,XU Shibiao,et al.Automatic road detection and centerline extraction via cascaded end-to-end convolutional neural network[J].IEEE Transactions on Geoscience and Remote Sensing,2017,55(6):3322-3337.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials">

                                <b>[15]</b> KRAHENBUHL P,KOLTUN V.Efficient inference in fully connected CRFs with Gaussian edge potentials[J].Advances in Neural Information Processing Systems,2011,24:109-117.
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Xception:Deep learning with depthwise separable convolutions">

                                <b>[16]</b> CHOLLET F.Xception:deep learning with depthwise separable convolutions[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Press,2017:1251-1258.
                            </a>
                        </p>
                        <p id="35">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep residual learning for image recognition">

                                <b>[17]</b> HE Kaiming,ZHANG Xiangyu,REN Shaoqing,et al.Deep residual learning for image recognition[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.Washington D.C.,USA:IEEE Press,2016:770-778.
                            </a>
                        </p>
                        <p id="37">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Machine learning for aerial image labeling">

                                <b>[18]</b> MNIH V.Machine learning for aerial image labeling[D].Toronto,Canada:University of Toronto,2013.
                            </a>
                        </p>
                        <p id="39">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=OpenStreetMap: User-Generated Street Maps">

                                <b>[19]</b> HAKLAY M,WEBER P.Openstreetmap:User-generated street maps[J].IEEE Pervas Computing,2008,7(4):12-18.
                            </a>
                        </p>
                        <p id="41">
                            <a id="bibliography_20" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XXCN201108016&amp;v=MDI4MDNVUkxPZVplUnRGeTNrVkx6QVBUWElZTEc0SDlETXA0OUVZb1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckM=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qeGdUQmRhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[20]</b> 胡正平,杨建秀.HOG特征混合模型结合隐SVM的感兴趣目标检测定位算法[J].信号处理,2011,27(8):1206-1212.
                            </a>
                        </p>
                        <p id="43">
                            <a id="bibliography_21" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00000425224&amp;v=MDAwNzRITXE0MUFadWtMWTNrNXpCZGg0ajk5U1hxUnJ4b3hjTUg3UjdxZWJ1ZHRGQ3ZsVnI3UEkxWT1OajdCYXJPNEh0&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qeGdUQmRhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[21]</b> DE BOER P T,KROESE D P,MANNOR S,et al.A tutorial on the cross-entropy method[J].Annals of Operations Research,2005,134(1):19-67.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJC201910043" />
        <input id="dpi" type="hidden" value="600" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qeGdUQmRhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201910043&amp;v=MjI2ODZHNEg5ak5yNDlCWjRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJ0Rnkza1ZMekFMejdCYmI=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUU40eEV3WFZvejQrSG5qeGdUQmRhZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
