<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637135597884846250%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJYRJ201909042%26RESULT%3d1%26SIGN%3dSqmFZeFOJFcvVakVmZTvpDNH0jY%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JYRJ201909042&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JYRJ201909042&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201909042&amp;v=MTQzOTZVYjdCTHpUWlpMRzRIOWpNcG85QlpvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVadEZ5ams=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#39" data-title="&lt;b&gt;0 引 言&lt;/b&gt; "><b>0 引 言</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#44" data-title="&lt;b&gt;1 相关工作&lt;/b&gt; "><b>1 相关工作</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#46" data-title="&lt;b&gt;2 基于卷积神经网络的模型匹配方法&lt;/b&gt; "><b>2 基于卷积神经网络的模型匹配方法</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#53" data-title="&lt;b&gt;3 实 验&lt;/b&gt; "><b>3 实 验</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#54" data-title="&lt;b&gt;3.1 实验数据&lt;/b&gt;"><b>3.1 实验数据</b></a></li>
                                                <li><a href="#57" data-title="&lt;b&gt;3.2 评价标准&lt;/b&gt;"><b>3.2 评价标准</b></a></li>
                                                <li><a href="#59" data-title="&lt;b&gt;3.3 参数分析&lt;/b&gt;"><b>3.3 参数分析</b></a></li>
                                                <li><a href="#64" data-title="&lt;b&gt;3.4 实验结果&lt;/b&gt;"><b>3.4 实验结果</b></a></li>
                                                <li><a href="#68" data-title="&lt;b&gt;3.5 实验分析&lt;/b&gt;"><b>3.5 实验分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#71" data-title="&lt;b&gt;4 结 语&lt;/b&gt; "><b>4 结 语</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#51" data-title="图1 卷积神经网络结构图">图1 卷积神经网络结构图</a></li>
                                                <li><a href="#56" data-title="图2 NYU数据集样本">图2 NYU数据集样本</a></li>
                                                <li><a href="#62" data-title="&lt;b&gt;表1 不同层数网络结构定量分析&lt;/b&gt;"><b>表1 不同层数网络结构定量分析</b></a></li>
                                                <li><a href="#66" data-title="&lt;b&gt;表2 不同结构精度对比&lt;/b&gt;"><b>表2 不同结构精度对比</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" title=" Dhond U R,Aggarwal J K.Structure from stereo-a review[J].IEEE transactions on systems,man,and cybernetics,1989,19(6):1489-1510." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Structure from stereo—a review">
                                        <b>[1]</b>
                                         Dhond U R,Aggarwal J K.Structure from stereo-a review[J].IEEE transactions on systems,man,and cybernetics,1989,19(6):1489-1510.
                                    </a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" title=" Scharstein D,Szeliski R.A taxonomy and evaluation of dense two-frame stereo correspondence algorithms[J].International journal of computer vision,2002,47(1/3):7-42." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002830724&amp;v=MjczNDdMWTNrNXpCZGg0ajk5U1hxUnJ4b3hjTUg3UjdxZForWnVGaXZsVTc3S0lWYz1OajdCYXJPNEh0SE9wNHhGWStr&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         Scharstein D,Szeliski R.A taxonomy and evaluation of dense two-frame stereo correspondence algorithms[J].International journal of computer vision,2002,47(1/3):7-42.
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title=" Horn B K.Height and gradient from shading[J].International journal of computer vision,1990,5(1):37-75." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002830197&amp;v=MTMyODZveGNNSDdSN3FkWitadUZpdmxVNzdLSVZjPU5qN0Jhck80SHRIT3A0eEZaZUlJWTNrNXpCZGg0ajk5U1hxUnJ4&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         Horn B K.Height and gradient from shading[J].International journal of computer vision,1990,5(1):37-75.
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" title=" Super B J,Bovik A C.Shape from texture using local spectral moments[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,1995,17(4):333-343." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Shape from texture using local spectral moments">
                                        <b>[4]</b>
                                         Super B J,Bovik A C.Shape from texture using local spectral moments[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,1995,17(4):333-343.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" title=" Nayar S K,Nakagawa Y.Shape from focus[J].IEEE Transactions on Pattern analysis and machine intelligence,1994,16(8):824-831." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Shape from focus">
                                        <b>[5]</b>
                                         Nayar S K,Nakagawa Y.Shape from focus[J].IEEE Transactions on Pattern analysis and machine intelligence,1994,16(8):824-831.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" title=" Chen T,Zhu Z,Shamir A,et al.3-sweep:extracting editable objects from a single photo[J].ACM Transactions on Graphics(TOG),2013,32(6):195." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13121600018392&amp;v=MDgzNDRaZVp0RmlubFVyeklJRjhXYXhzPU5pZklZN0s3SDlQTnFZOUZaT29IRDNVN29CTVQ2VDRQUUgvaXJSZEdlcnFRVE1udw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                         Chen T,Zhu Z,Shamir A,et al.3-sweep:extracting editable objects from a single photo[J].ACM Transactions on Graphics(TOG),2013,32(6):195.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" title=" Criminisi A,Reid I,Zisserman A.Single view metrology[J].International Journal of Computer Vision,2000,40(2):123-148." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002830650&amp;v=MDA4ODhVNzdLSVZjPU5qN0Jhck80SHRIT3A0eEZZdTRQWTNrNXpCZGg0ajk5U1hxUnJ4b3hjTUg3UjdxZForWnVGaXZs&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                         Criminisi A,Reid I,Zisserman A.Single view metrology[J].International Journal of Computer Vision,2000,40(2):123-148.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" title=" Chaudhuri S,Koltun V.Data-driven suggestions for creativity support in 3D modeling[J].ACM Transactions on Graphics(TOG),2010,29(6):183." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000002893&amp;v=MDc2NTAvaXJSZEdlcnFRVE1ud1plWnRGaW5sVXJ6SUlGOFdheHM9TmlmSVk3SzdIdGpOcjQ5RlpPc05CSFU2b0JNVDZUNFBRSA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                         Chaudhuri S,Koltun V.Data-driven suggestions for creativity support in 3D modeling[J].ACM Transactions on Graphics(TOG),2010,29(6):183.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" title=" Chaudhuri S,Kalogerakis E,Guibas L,et al.Probabilistic reasoning for assembly-based 3D modeling[J].ACM Transactions on Graphics(TOG),2011,30(4):35." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000002904&amp;v=MDI5OTRmSVk3SzdIdGpOcjQ5RlpPc05CWHc5b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcnpJSUY4V2F4cz1OaQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                         Chaudhuri S,Kalogerakis E,Guibas L,et al.Probabilistic reasoning for assembly-based 3D modeling[J].ACM Transactions on Graphics(TOG),2011,30(4):35.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_10" title=" Xu K,Zheng H,Zhang H,et al.Photo-inspired model-driven 3D object modeling[J].ACM Transactions on Graphics(TOG),2011,30(4):80." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000002949&amp;v=MTQ0NzR6SUlGOFdheHM9TmlmSVk3SzdIdGpOcjQ5RlpPc05CWGd3b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[10]</b>
                                         Xu K,Zheng H,Zhang H,et al.Photo-inspired model-driven 3D object modeling[J].ACM Transactions on Graphics(TOG),2011,30(4):80.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_11" title=" EITZ M,RICHTER R,BOUBEKEUR T,et al.Sketch-based shape retrieval[J].ACM Transactions on Graphics(TOG),2012,31(4):31." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000007667&amp;v=MDE3NDllWnRGaW5sVXJ6SUlGOFdheHM9TmlmSVk3SzdIdGpOcjQ5RlpPc0lDbm8rb0JNVDZUNFBRSC9pclJkR2VycVFUTW53Wg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[11]</b>
                                         EITZ M,RICHTER R,BOUBEKEUR T,et al.Sketch-based shape retrieval[J].ACM Transactions on Graphics(TOG),2012,31(4):31.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_12" title=" Shao T,Xu W,Yin K,et al.Discriminative Sketch-based 3D Model Retrieval via Robust Shape Matching[J].Computer Graphics Forum,2011,30(7):2011-2020." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Discriminative Sketch‐based 3D Model Retrieval via Robust Shape Matching">
                                        <b>[12]</b>
                                         Shao T,Xu W,Yin K,et al.Discriminative Sketch-based 3D Model Retrieval via Robust Shape Matching[J].Computer Graphics Forum,2011,30(7):2011-2020.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_13" title=" Fan L,Wang R,Xu L,et al.Modeling by Drawing with Shadow Guidance[J].Computer Graphics Forum,2013,32(7):157-166." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJWD&amp;filename=SJWD13120500001560&amp;v=MTM5Mzd4cz1OaWZjYXJLN0g5UE1xbzlGWk9zT0NYbzVvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp0RmlubFVyeklJRjhXYQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[13]</b>
                                         Fan L,Wang R,Xu L,et al.Modeling by Drawing with Shadow Guidance[J].Computer Graphics Forum,2013,32(7):157-166.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_14" title=" Xie X,Xu K,Mitra N J,et al.Sketch-to-Design:Context-Based Part Assembly[J].Computer Graphics Forum,2013,32(8):233-245." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJWD&amp;filename=SJWD13120500001540&amp;v=MTk4MDFvOUZaT3NPQ1hnNW9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaW5sVXJ6SUlGOFdheHM9TmlmY2FySzdIOVBNcQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[14]</b>
                                         Xie X,Xu K,Mitra N J,et al.Sketch-to-Design:Context-Based Part Assembly[J].Computer Graphics Forum,2013,32(8):233-245.
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_15" title=" Shen C-H,Fu H,Chen K,et al.Structure recovery by part assembly[J].ACM Transactions on Graphics(TOG),2012,31(6):180." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000007816&amp;v=MzA2NzlOcjQ5RlpPc0lCSDAvb0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcnpJSUY4V2F4cz1OaWZJWTdLN0h0ag==&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[15]</b>
                                         Shen C-H,Fu H,Chen K,et al.Structure recovery by part assembly[J].ACM Transactions on Graphics(TOG),2012,31(6):180.
                                    </a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_16" title=" Shag T,Xu W,Zhou K,et al.An interactive approach to semantic modeling of indoor scenes with an rgbd camera[J].ACM Transactions on Graphics(TOG),2012,31(6):136." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000007772&amp;v=MTg3OTBzN29CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaW5sVXJ6SUlGOFdheHM9TmlmSVk3SzdIdGpOcjQ5RlpPc0lDMw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[16]</b>
                                         Shag T,Xu W,Zhou K,et al.An interactive approach to semantic modeling of indoor scenes with an rgbd camera[J].ACM Transactions on Graphics(TOG),2012,31(6):136.
                                    </a>
                                </li>
                                <li id="35">


                                    <a id="bibliography_17" title=" Nan L,Xie K,Sharf A.A search-classify approach for cluttered indoor scene understanding[J].ACM Transactions on Graphics(TOG),2012,31(6):137." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000007773&amp;v=MTc4MDlGOFdheHM9TmlmSVk3SzdIdGpOcjQ5RlpPc0lDM3M2b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcnpJSQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[17]</b>
                                         Nan L,Xie K,Sharf A.A search-classify approach for cluttered indoor scene understanding[J].ACM Transactions on Graphics(TOG),2012,31(6):137.
                                    </a>
                                </li>
                                <li id="37">


                                    <a id="bibliography_18" title=" Chen K,Lai Y K,Wu Y X,et al.Automatic semantic modeling of indoor scenes from low-quality RGB-D data using contextual information[J].ACM Transactions on Graphics(TOG),2014,33(6):208." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM46506767665C90D5E3BED73CB1253856&amp;v=MDQ3OTVLcUlsQ1l1MEtmM1U1dXhObTZVMElQSGpoMzJBMGU3ZVhUYitaQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQnJMVTA1dHRoeExtOXdhRT1OaWZJWTdlK0c5SA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[18]</b>
                                         Chen K,Lai Y K,Wu Y X,et al.Automatic semantic modeling of indoor scenes from low-quality RGB-D data using contextual information[J].ACM Transactions on Graphics(TOG),2014,33(6):208.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JYRJ" target="_blank">计算机应用与软件</a>
                2019,36(09),232-235 DOI:10.3969/j.issn.1000-386x.2019.09.041            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于卷积神经网络的室内场景三维重建技术研究</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%A7%9A%E6%99%93%E5%B3%B0&amp;code=41165469&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">姚晓峰</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%AD%A6%E5%88%A9%E7%A7%80&amp;code=41165471&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">武利秀</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%AB%A0%E4%BC%9F&amp;code=41165470&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">章伟</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E6%9D%BE&amp;code=42745802&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王松</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%97%A0%E9%94%A1%E5%A4%AA%E6%B9%96%E5%AD%A6%E9%99%A2%E6%B1%9F%E8%8B%8F%E7%9C%81%E7%89%A9%E8%81%94%E7%BD%91%E5%BA%94%E7%94%A8%E6%8A%80%E6%9C%AF%E9%87%8D%E7%82%B9%E5%BB%BA%E8%AE%BE%E5%AE%9E%E9%AA%8C%E5%AE%A4&amp;code=1694421&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">无锡太湖学院江苏省物联网应用技术重点建设实验室</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>三维场景重建技术是计算机视觉领域的十分重要的研究课题。传统三维场景重建大多是专业工程师通过手工制图实现,效率不高且成本较高。对此提出一种基于卷积神经网络的三维场景重建方法。该方法在对2D图像进行语义分割的基础上,提取分割后的室内场景元素图像块,训练一个基于卷积神经网络的三维模型匹配模型;再将匹配得到的三维模型结合深度图构造的残缺三维模型,进一步进行组合,从而完成室内场景的三维重建工作。实验验证了该方法的可行性和优异性。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%B8%89%E7%BB%B4%E5%9C%BA%E6%99%AF%E9%87%8D%E5%BB%BA&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">三维场景重建;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">卷积神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%B8%89%E7%BB%B4%E6%A8%A1%E5%9E%8B%E5%8C%B9%E9%85%8D&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">三维模型匹配;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E5%9B%BE&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度图;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    姚晓峰,讲师,主研领域:图像处理,无线射频识别技术,虚拟现实。;
                                </span>
                                <span>
                                    武利秀,硕士。;
                                </span>
                                <span>
                                    章伟,硕士。;
                                </span>
                                <span>
                                    王松,硕士。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-01-23</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家重点研发项目(子课题)(2018YFD0400902);</span>
                                <span>教育部-中国移动科研基金项目(MCM20170204);</span>
                    </p>
            </div>
                    <h1><b>3D RECONSTRUCTION OF INDOOR SCENE BASED ON CNN</b></h1>
                    <h2>
                    <span>Yao Xiaofeng</span>
                    <span>Wu Lixiu</span>
                    <span>Zhang Wei</span>
                    <span>Wang Song</span>
            </h2>
                    <h2>
                    <span>Jiangsu Key Construction Laboratory of IoT Application Technology, Taihu University of Wuxi</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>3 D scene reconstruction technology is one of the most important research topics in the field of computer vision. Traditional 3 D scene reconstruction is mostly achieved by professional engineers through manual drawing, which is inefficient and costly. This paper proposed 3 D scene reconstruction method based on CNN. Based on the semantic segmentation of 2 D images, we extracted the segmented image blocks of indoor scene elements, and trained a 3 D model matching model based on CNN. Then, the matched 3 D model was combined with the incomplete 3 D model constructed by depth map, and further combined to complete the 3 D reconstruction of indoor scenes. The feasibility and superiority of this method are verified by experiments.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=3D%20scene%20reconstruction&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">3D scene reconstruction;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=CNN&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">CNN;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=3D%20model%20matching&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">3D model matching;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Depth%20map&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Depth map;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2019-01-23</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="39" name="39" class="anchor-tag"><b>0 引 言</b></h3>
                <div class="p1">
                    <p id="40">传统的三维重建方法大部分是借助现有的计算机辅助设计软件(CAD,Maya等),通过专业三维制图工程师使用大量的时间和手工方法对真实世界的场景进行三维几何重建。该方法建造的场景具有较好的交互性,但模型真实感不太强,而且大多数辅助设计软件具有很高的学习成本,工作量大,要想完成一个建模工作需要大量的人机交互工作,使得工作效率极大降低。这对于一些不需要高精度建模的使用者望而却步。为此,如何寻找更快速的三维建模手段成为研究人员的新的研究方向。一种新的角度就是从图像出发,通过对图像信息的分析与处理,提取三维信息,用来实现三维场景的重建技术。为此,本文提出了一种采用RGBD图像进行三维场景重建的方法。用户首先通过Kinect摄像机获取到一张室内场景的RGBD图像,然后利用图像分割技术将图像分割成单一的场景元素块。由于遮挡等问题使得分割结果有可能达不到满意的效果,用户可以手动对分割图像进行相应的调整。分割之后,用每个元素块的深度图去和三维模型库中的室内元素模型进行匹配,从而使其还原到三维场景中。针对大型的场景图像,每个单张的图像只需要覆盖一部分的场景,从而用户可以通过一组图像重建整个场景。</p>
                </div>
                <div class="p1">
                    <p id="41">本文创新点如下:</p>
                </div>
                <div class="p1">
                    <p id="42">1) 通过三维模型渲染的方法获取部分数据以扩展训练数据集,并结合部分真实数据集,得到充足的训练数据以用于模型匹配的三维重建方法的模型训练上。</p>
                </div>
                <div class="p1">
                    <p id="43">2) 通过改进现有的卷积神经网络模型,将图像分类的思想引入本文问题上,把室内场景元素的三维重建问题转化成一个分类问题去解决。</p>
                </div>
                <h3 id="44" name="44" class="anchor-tag"><b>1 相关工作</b></h3>
                <div class="p1">
                    <p id="45">理想情况下的三维场景建模方式是基于单图像的建模方法,采用立体视觉的方式对三维场景进行重建<citation id="85" type="reference"><link href="3" rel="bibliography" /><link href="5" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>]</sup></citation>的研究较多。该方法需要拍摄多幅同一物体的图像作为参考,或者从不同视点对物体进行数据采集,并从中提取相关特征以用来达到物体三维重建的目的。单幅图像的明暗<citation id="73" type="reference"><link href="7" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>、焦距<citation id="74" type="reference"><link href="9" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>、纹理<citation id="75" type="reference"><link href="11" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>等线索是基于单幅图像三维场景建模技术的十分重要的特征来源。但是,这些线索通常对图像质量要求很高,使得在不同光照条件和物体遮挡下的效果变得很差,极大地限制了基于单幅图像进行三维场景建模的发展,使得这项技术只能在某些特定的场合下完成三维场景重建问题。这就促使科研人员找到其他途径去解决该问题。引入人机交互方法应用于三维场景重建问题<citation id="86" type="reference"><link href="13" rel="bibliography" /><link href="15" rel="bibliography" /><sup>[<a class="sup">6</a>,<a class="sup">7</a>]</sup></citation>,恰当的人机交互使得三维重建技术的相关难题得到有效的解决。文献<citation id="76" type="reference">[<a class="sup">8</a>]</citation>提出了数据驱动的三维建模方法,通过图像处理、三维检索等技术把已有的或者自己创作的三维模型结合起来,可以迅速地实现他们的设计创意。针对三维场景的建模问题,文献<citation id="77" type="reference">[<a class="sup">9</a>]</citation>把上述数据驱动的三维重建方法通过进一步的组装,并对之进行建模,极大地推进了三维场景建模问题的进展,使得三维场景建模变得更加方便。这种方法把三维场景模型库中的模型作为单个部件,通过部件之间的有机联系组装起来,从而得到新的三维场景模型。文献<citation id="78" type="reference">[<a class="sup">10</a>]</citation>提出一种基于图像的三维建模方法,通过对单张图像的分析,匹配到三维物体模型库中的模型,然后进行旋转平移等变换,用来作为三维建模的组件。这类方法对三维模型库要进行大量的手工预处理工作。基于草图的三维模型的几何重建是另一种三维场景建模方法。文献<citation id="87" type="reference">[<a class="sup">11</a>,<a class="sup">12</a>]</citation>提出了一种基于草图的模型检索方法,通过输入草图,在三维模型库中匹配与之对应的三维模型的技术。文献<citation id="79" type="reference">[<a class="sup">13</a>]</citation>在此基础上提出了一种基于草图的三维建模方法。随着触屏技术的发展,这项技术有了更加广阔的前景。通过触屏的概念,能够动态的指引用户迅速完成三维建模任务。文献<citation id="80" type="reference">[<a class="sup">14</a>]</citation>把三维建模技术转换成搭积木工作,在系统后台准备大规模的三维模型组件,让用户手动的组装创意,并和用户手绘的元素进行有机的结合,让三维建模技术变得有趣起来。文献<citation id="81" type="reference">[<a class="sup">15</a>]</citation>提出部件组装的三维重建方法,利用这些残缺的点云信息和三维检索技术所得到的高质量的三维模型进行有机结合,对残缺部分进行修复和拼接从而达到三维重建的目的。通过修复三维扫描仪获取残缺的场景点云模型以达到三维场景重建的目的是该方法的核心。文献<citation id="82" type="reference">[<a class="sup">16</a>]</citation>提出了一种基于深度相机的三维场景重建方法。文献<citation id="83" type="reference">[<a class="sup">17</a>]</citation>提出了一种交互式语义建模的方法,通过提取场景的深度信息进行分析与处理,从而达到三维场景重建的目的。文献<citation id="84" type="reference">[<a class="sup">18</a>]</citation>也针对这类方法进行了研究。这类方法主要是通过扫描真实场景获取RGBD信息,然后利用这些信息和三维模型数据库之间的联系,通过语义信息解决三维场景重建问题。研究人员对三维场景的点云信息进行分割,获取单个物体的三维模型,从而恢复到原来的三维场景中去,完成三维场景的建模工作。这类方法虽然能够取得很好的效果,并且有很高的效率,但是场景的点云信息的获取成为了制约技术发展的绊脚石,也许在不久的将来,手持的深度相机的发展能够弥补这个问题,但现阶段这类方法还不能很好的推广。</p>
                </div>
                <h3 id="46" name="46" class="anchor-tag"><b>2 基于卷积神经网络的模型匹配方法</b></h3>
                <div class="p1">
                    <p id="47">在对RGB图片进行语义分割的基础上,本文提出一种基于卷积神经网络的模型匹配方法,在三维模型库中匹配到和已知图像块最类似的三维模型,然后把它放到适当的位置,就可以重建三维场景。模型训练集是图像分割后的图像块,标签是它们对应的精确分类,例如椅子包含各种不同靠背、支撑形式以及形状的椅子。卷积神经网络对图像分类问题中具有很好的精确度。</p>
                </div>
                <div class="p1">
                    <p id="48">由于不同的场景元素所包含的精分类的类别并不相同,针对每个元素主体设计不同的网络结构来达到分类匹配目的。此处不同元素的网络结构大体类似,故本节以椅子作为主要描述对象。我们获取到34个不同的椅子三维模型,即总共有34个椅子的精分类类别。</p>
                </div>
                <div class="p1">
                    <p id="49">如图1所示,本网络结构有三层卷积层以及两层下采样层组成。网络结构中每层卷积层后都添加激活函数,这里选择RELU函数,其公式为:</p>
                </div>
                <div class="p1">
                    <p id="50"><i>f</i>(<i>x</i>)=max(0,<i>x</i>)      (1)</p>
                </div>
                <div class="area_img" id="51">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201909042_051.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 卷积神经网络结构图" src="Detail/GetImg?filename=images/JYRJ201909042_051.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 卷积神经网络结构图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201909042_051.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="52">使用RELU激活函数有三点好处:一是RELU函数在网络训练阶段计算量小,效率高;二是sigmoid激活函数容易出现梯度消失现象,RELU则可以避免这种状况;三是RELU会使得部分神经元输出为0,减少了参数之间的相互依存关系,对防止过拟合有一定作用。</p>
                </div>
                <h3 id="53" name="53" class="anchor-tag"><b>3 实 验</b></h3>
                <h4 class="anchor-tag" id="54" name="54"><b>3.1 实验数据</b></h4>
                <div class="p1">
                    <p id="55">本文模型的输入数据是语义分割后得到的场景元素图像块的深度图,输出是该图像块所属的精分类类别。为了得到该分类模型的训练数据,首先通过一些免费设计素材网站下载并整理室内场景三维模型,并搜索与之对应的真实RGB图像,通过三维投影的方法,手动渲染了带类标的深度图像。然后通过对NYU室内场景图片数据集的语义分割,手动标注了部分真实场景中的数据,混合形成本实验的训练数据集,用来训练网络模型。如图2所示,NYU室内场景数据集由微软的Kinect相机拍摄的各种室内场景的视频序列组成。</p>
                </div>
                <div class="area_img" id="56">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201909042_056.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 NYU数据集样本" src="Detail/GetImg?filename=images/JYRJ201909042_056.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 NYU数据集样本  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201909042_056.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="57" name="57"><b>3.2 评价标准</b></h4>
                <div class="p1">
                    <p id="58">本文所提出的基于模型匹配的三维重建技术其实就是把重建方法转化成一个分类问题。所以,分类问题的评价指标便是本实验的评价标准,即模型匹配的准确率。</p>
                </div>
                <h4 class="anchor-tag" id="59" name="59"><b>3.3 参数分析</b></h4>
                <div class="p1">
                    <p id="60">通过对比现阶段流行的深度学习框架,选择了高效灵活的Caffe框架作为实验环境。实验环境配置如下:core i7处理器、NIVIDIA GTX980 GPU、16 GB内存、Caffe、Python。</p>
                </div>
                <div class="p1">
                    <p id="61">在进行网络层数的选择时,主要考虑耗时和准确率的问题,由于深层的深度神经网络无论是在训练阶段还是测试阶段,网络层数多而导致的计算过程中大量的矩阵运算会增加耗时,所以尽可能在保证准确率的情况下选择浅层卷积神经网络是十分重要的。为此,本文设计一组关于卷积神经网络层数对匹配效果影响的实验,分别是1～5层卷积层的神经网络结构搭配对应的采样层来进行试验。实验效果如表1所示。</p>
                </div>
                <div class="area_img" id="62">
                    <p class="img_tit"><b>表1 不同层数网络结构定量分析</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="62" border="1"><tr><td>网络层数</td><td>1</td><td>2</td><td>3</td><td>4</td><td>5</td></tr><tr><td><br />耗时/ms</td><td>17</td><td>49</td><td>138</td><td>596</td><td>1.3e3</td></tr><tr><td><br />准确率</td><td>0.53</td><td>0.65</td><td>0.89</td><td>0.90</td><td>0.91</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="63">表1中的耗时指标为单张图像块进行一次模型匹配所耗时长。可以看出,当网络深度逐渐增大时,进行模型匹配的准确率有一定的增加,但深层的神经网络又增加了测试时长,使得该算法在实际应用中没有很好的使用价值。且在3层卷积结构,即整体网络层数为7层时,准确率和耗时都在合理范围内,而继续增加网络深度,对准确率的提升并不明显,反而会增加耗时。故在后续实验中,将选取3层卷积层,2层下采样层作为最终网络结构,即前文所提出的卷积神经网络模型。</p>
                </div>
                <h4 class="anchor-tag" id="64" name="64"><b>3.4 实验结果</b></h4>
                <div class="p1">
                    <p id="65">在实际室内场景中地板、天花板、墙和背景不需要参与三维模型匹配。椅子、桌子、沙发、柜子、床和显示器这六种室内场景元素是有很多精分类的对象,故本实验以这六种元素作为讨论对象。为了对比本文所提出的方法和一些传统的图像分类模型的效果,本文使用Cifar-10和LeNet-5网络结构做出相关对比实验,结果如表2所示。</p>
                </div>
                <div class="area_img" id="66">
                    <p class="img_tit"><b>表2 不同结构精度对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="66" border="1"><tr><td><br />室内元素</td><td>椅子</td><td>桌子</td><td>沙发</td><td>柜子</td><td>床</td><td>显示器</td></tr><tr><td><br />本文方法</td><td><b>0.891</b></td><td><b>0.875</b></td><td><b>0.871</b></td><td><b>0.880</b></td><td>0.842</td><td><b>0.846</b></td></tr><tr><td><br />Cifar-10</td><td>0.870</td><td>0.845</td><td>0.863</td><td>0.862</td><td><b>0.859</b></td><td>0.831</td></tr><tr><td><br />LeNet-5</td><td>0.854</td><td>0.830</td><td>0.843</td><td>0.837</td><td>0.824</td><td>0.810</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="67">通过三维匹配得到模型,从而将匹配得到的室内场景元素三维模型和深度图构建的残缺点云模型相结合,得到一个大致的室内场景三维重建模型。</p>
                </div>
                <h4 class="anchor-tag" id="68" name="68"><b>3.5 实验分析</b></h4>
                <div class="p1">
                    <p id="69">由表2可知,本文提出的基于匹配的三维场景重建技术能达到比较好的效果。该算法重建的室内场景三维模型基本能够还原真实场景的场景画面。但是有些区分度不高的元素,例如显示器和床等,大部分显示器都具有雷同的外观,尤其是在对其进行图像分析过程中所产生的误差,使得其最终的模型匹配准确率整体偏低。</p>
                </div>
                <div class="p1">
                    <p id="70">实验选取的Cifar-10网络结构和LeNet-5网络结构,都是传统的图像分类方法上效果比较好的模型,针对这些模型进行改进,使它们适应本文问题的应用场景,训练模型所得结果对本文模型的验证有着很大的参考价值。在大部分场景元素的匹配中,本文算法都能够取得相对较高的水平。通过实验验证了本文所提的基于卷积神经网络的三维场景建模方法是一个高效可行的解决方案。</p>
                </div>
                <h3 id="71" name="71" class="anchor-tag"><b>4 结 语</b></h3>
                <div class="p1">
                    <p id="72">本文提出并实现了一种基于卷积神经网络的三维场景重建方法。在对2D图像进行语义分割的基础上,提取分割后的室内场景元素图像块,训练了一个基于卷积神经网络的三维模型匹配模型,然后将匹配得到的三维模型结合深度图构造的残缺三维模型,进一步进行组合,从而完成室内场景的三维重建工作。并通过实验验证了该方法的可行性和优异性。随着深度学习等技术的发展,一些新的基于深度学习的图像分类算法将会使图像分类技术得到很大的发展。这是本文三维模型匹配算法的核心,在这方面进行进一步研究可以提高三维模型匹配算法的准确率,使得整个三维场景重建的效果变得更好。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Structure from stereo—a review">

                                <b>[1]</b> Dhond U R,Aggarwal J K.Structure from stereo-a review[J].IEEE transactions on systems,man,and cybernetics,1989,19(6):1489-1510.
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002830724&amp;v=MjcwNTVkWitadUZpdmxVNzdLSVZjPU5qN0Jhck80SHRIT3A0eEZZK2tMWTNrNXpCZGg0ajk5U1hxUnJ4b3hjTUg3Ujdx&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> Scharstein D,Szeliski R.A taxonomy and evaluation of dense two-frame stereo correspondence algorithms[J].International journal of computer vision,2002,47(1/3):7-42.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002830197&amp;v=MjQzNjU3cWRaK1p1Rml2bFU3N0tJVmM9Tmo3QmFyTzRIdEhPcDR4RlplSUlZM2s1ekJkaDRqOTlTWHFScnhveGNNSDdS&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> Horn B K.Height and gradient from shading[J].International journal of computer vision,1990,5(1):37-75.
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Shape from texture using local spectral moments">

                                <b>[4]</b> Super B J,Bovik A C.Shape from texture using local spectral moments[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,1995,17(4):333-343.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Shape from focus">

                                <b>[5]</b> Nayar S K,Nakagawa Y.Shape from focus[J].IEEE Transactions on Pattern analysis and machine intelligence,1994,16(8):824-831.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13121600018392&amp;v=MTE5MjM3b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcnpJSUY4V2F4cz1OaWZJWTdLN0g5UE5xWTlGWk9vSEQzVQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b> Chen T,Zhu Z,Shamir A,et al.3-sweep:extracting editable objects from a single photo[J].ACM Transactions on Graphics(TOG),2013,32(6):195.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002830650&amp;v=MDUxMDZadUZpdmxVNzdLSVZjPU5qN0Jhck80SHRIT3A0eEZZdTRQWTNrNXpCZGg0ajk5U1hxUnJ4b3hjTUg3UjdxZFor&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b> Criminisi A,Reid I,Zisserman A.Single view metrology[J].International Journal of Computer Vision,2000,40(2):123-148.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000002893&amp;v=MzI2NTM0OUZaT3NOQkhVNm9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaW5sVXJ6SUlGOFdheHM9TmlmSVk3SzdIdGpOcg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b> Chaudhuri S,Koltun V.Data-driven suggestions for creativity support in 3D modeling[J].ACM Transactions on Graphics(TOG),2010,29(6):183.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000002904&amp;v=MjYxODJmSVk3SzdIdGpOcjQ5RlpPc05CWHc5b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcnpJSUY4V2F4cz1OaQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b> Chaudhuri S,Kalogerakis E,Guibas L,et al.Probabilistic reasoning for assembly-based 3D modeling[J].ACM Transactions on Graphics(TOG),2011,30(4):35.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_10" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000002949&amp;v=MzI2MjJadEZpbmxVcnpJSUY4V2F4cz1OaWZJWTdLN0h0ak5yNDlGWk9zTkJYZ3dvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[10]</b> Xu K,Zheng H,Zhang H,et al.Photo-inspired model-driven 3D object modeling[J].ACM Transactions on Graphics(TOG),2011,30(4):80.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_11" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000007667&amp;v=MTE4MjBVcnpJSUY4V2F4cz1OaWZJWTdLN0h0ak5yNDlGWk9zSUNubytvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp0RmlubA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[11]</b> EITZ M,RICHTER R,BOUBEKEUR T,et al.Sketch-based shape retrieval[J].ACM Transactions on Graphics(TOG),2012,31(4):31.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Discriminative Sketch‐based 3D Model Retrieval via Robust Shape Matching">

                                <b>[12]</b> Shao T,Xu W,Yin K,et al.Discriminative Sketch-based 3D Model Retrieval via Robust Shape Matching[J].Computer Graphics Forum,2011,30(7):2011-2020.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_13" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJWD&amp;filename=SJWD13120500001560&amp;v=MTk2NDRJRjhXYXhzPU5pZmNhcks3SDlQTXFvOUZaT3NPQ1hvNW9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaW5sVXJ6SQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[13]</b> Fan L,Wang R,Xu L,et al.Modeling by Drawing with Shadow Guidance[J].Computer Graphics Forum,2013,32(7):157-166.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_14" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJWD&amp;filename=SJWD13120500001540&amp;v=MjU2OTg4V2F4cz1OaWZjYXJLN0g5UE1xbzlGWk9zT0NYZzVvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp0RmlubFVyeklJRg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[14]</b> Xie X,Xu K,Mitra N J,et al.Sketch-to-Design:Context-Based Part Assembly[J].Computer Graphics Forum,2013,32(8):233-245.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_15" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000007816&amp;v=MTAzMzhmSVk3SzdIdGpOcjQ5RlpPc0lCSDAvb0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcnpJSUY4V2F4cz1OaQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[15]</b> Shen C-H,Fu H,Chen K,et al.Structure recovery by part assembly[J].ACM Transactions on Graphics(TOG),2012,31(6):180.
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_16" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000007772&amp;v=MTk2NDJ6SUlGOFdheHM9TmlmSVk3SzdIdGpOcjQ5RlpPc0lDM3M3b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[16]</b> Shag T,Xu W,Zhou K,et al.An interactive approach to semantic modeling of indoor scenes with an rgbd camera[J].ACM Transactions on Graphics(TOG),2012,31(6):136.
                            </a>
                        </p>
                        <p id="35">
                            <a id="bibliography_17" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000007773&amp;v=MDY5MzdpZklZN0s3SHRqTnI0OUZaT3NJQzNzNm9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaW5sVXJ6SUlGOFdheHM9Tg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[17]</b> Nan L,Xie K,Sharf A.A search-classify approach for cluttered indoor scene understanding[J].ACM Transactions on Graphics(TOG),2012,31(6):137.
                            </a>
                        </p>
                        <p id="37">
                            <a id="bibliography_18" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM46506767665C90D5E3BED73CB1253856&amp;v=MDYyNjMrRzlIS3FJbENZdTBLZjNVNXV4Tm02VTBJUEhqaDMyQTBlN2VYVGIrWkNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkJyTFUwNXR0aHhMbTl3YUU9TmlmSVk3ZQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[18]</b> Chen K,Lai Y K,Wu Y X,et al.Automatic semantic modeling of indoor scenes from low-quality RGB-D data using contextual information[J].ACM Transactions on Graphics(TOG),2014,33(6):208.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JYRJ201909042" />
        <input id="dpi" type="hidden" value="600" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201909042&amp;v=MTQzOTZVYjdCTHpUWlpMRzRIOWpNcG85QlpvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVadEZ5ams=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
