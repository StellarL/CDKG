<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637136397184190000%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJYRJ201903039%26RESULT%3d1%26SIGN%3dcbWdv1muuWf%252bgUOwSFxJNJOA2ks%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JYRJ201903039&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JYRJ201903039&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201903039&amp;v=MzI2NjVSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGaURsVjcvS0x6VFpaTEc0SDlqTXJJOUdiWVFLREg4NHY=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#35" data-title="&lt;b&gt;0 引 言&lt;/b&gt; "><b>0 引 言</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#37" data-title="&lt;b&gt;1 算法描述&lt;/b&gt; "><b>1 算法描述</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#40" data-title="&lt;b&gt;1.1 获取目标区域&lt;/b&gt;"><b>1.1 获取目标区域</b></a></li>
                                                <li><a href="#50" data-title="&lt;b&gt;1.2 基于深度学习进行细粒度分类&lt;/b&gt;"><b>1.2 基于深度学习进行细粒度分类</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#61" data-title="&lt;b&gt;2 实验仿真&lt;/b&gt; "><b>2 实验仿真</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#62" data-title="&lt;b&gt;2.1 实验背景&lt;/b&gt;"><b>2.1 实验背景</b></a></li>
                                                <li><a href="#65" data-title="&lt;b&gt;2.2 实验结果与分析&lt;/b&gt;"><b>2.2 实验结果与分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#77" data-title="&lt;b&gt;3 结 语&lt;/b&gt; "><b>3 结 语</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#39" data-title="图1 算法流程图">图1 算法流程图</a></li>
                                                <li><a href="#42" data-title="图2 获取目标区域算法描述图">图2 获取目标区域算法描述图</a></li>
                                                <li><a href="#54" data-title="图3 改进的B-CNN算法流程图">图3 改进的B-CNN算法流程图</a></li>
                                                <li><a href="#64" data-title="&lt;b&gt;表1 B-CNN主要参数表&lt;/b&gt;"><b>表1 B-CNN主要参数表</b></a></li>
                                                <li><a href="#72" data-title="&lt;b&gt;表2 不同方法在CUB200-2011数据集的识别率&lt;/b&gt;"><b>表2 不同方法在CUB200-2011数据集的识别率</b></a></li>
                                                <li><a href="#74" data-title="&lt;b&gt;表3 不同方法在Standford Dogs数据集的识别率&lt;/b&gt;"><b>表3 不同方法在Standford Dogs数据集的识别率</b></a></li>
                                                <li><a href="#76" data-title="&lt;b&gt;表4 对比实验结果图&lt;/b&gt;"><b>表4 对比实验结果图</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" title=" 彭晏飞, 陶进, 訾玲玲. 基于卷积神经网络和E2LSH的遥感图像检索研究[J].计算机应用与软件, 2018, 35 (7) :250-255." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201807046&amp;v=MTE5NTdvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0ZpRGxWNy9OTHpUWlpMRzRIOW5NcUk5Qlk=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         彭晏飞, 陶进, 訾玲玲. 基于卷积神经网络和E2LSH的遥感图像检索研究[J].计算机应用与软件, 2018, 35 (7) :250-255.
                                    </a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" title=" Dasgupta R, Namboodiri A M. Leveraging multiple tasks to regularize fine-grained classification[C]//International Conference on Pattern Recognition.IEEE, 2017:3476-3481." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Leveraging multiple tasks to regularize fine-grained classification">
                                        <b>[2]</b>
                                         Dasgupta R, Namboodiri A M. Leveraging multiple tasks to regularize fine-grained classification[C]//International Conference on Pattern Recognition.IEEE, 2017:3476-3481.
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title=" Sang N, Chen Y, Gao C, et al. Detection of vehicle parts based on Faster R-CNN and relative position information[C]//Pattern Recognition and Computer Vision. 2018:83." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Detection of vehicle parts based on Faster R-CNN and relative position information">
                                        <b>[3]</b>
                                         Sang N, Chen Y, Gao C, et al. Detection of vehicle parts based on Faster R-CNN and relative position information[C]//Pattern Recognition and Computer Vision. 2018:83.
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" title=" Lin T Y, Roychowdhury A, Maji S. Bilinear CNN Models for Fine-Grained Visual Recognition[C]//IEEE International Conference on Computer Vision.IEEE, 2016:1449-1457." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Bilinear CNNs for Fine-grained Visual Recognition">
                                        <b>[4]</b>
                                         Lin T Y, Roychowdhury A, Maji S. Bilinear CNN Models for Fine-Grained Visual Recognition[C]//IEEE International Conference on Computer Vision.IEEE, 2016:1449-1457.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" title=" Huang S, Xu Z, Tao D, et al. Part-Stacked CNN for Fine-Grained Visual Categorization[C]//Computer Vision and Pattern Recognition. IEEE, 2016:1173-1182." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Part-Stacked CNN for Fine-Grained Visual Categorization">
                                        <b>[5]</b>
                                         Huang S, Xu Z, Tao D, et al. Part-Stacked CNN for Fine-Grained Visual Categorization[C]//Computer Vision and Pattern Recognition. IEEE, 2016:1173-1182.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" title=" Shen Z, Jiang Y G, Wang D, et al. Iterative object and part transfer for fine-grained recognition[C]//IEEE International Conference on Multimedia and Expo.IEEE, 2017:1470-1475." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Iterative object and part transfer for fine-grained recognition">
                                        <b>[6]</b>
                                         Shen Z, Jiang Y G, Wang D, et al. Iterative object and part transfer for fine-grained recognition[C]//IEEE International Conference on Multimedia and Expo.IEEE, 2017:1470-1475.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" title=" Yao H, Zhang S, Zhang Y, et al. Coarse-to-Fine Description for Fine-Grained Visual Categorization[J]. IEEE Transactions on Image Processing, 2016, 25 (10) :4858-4872." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Coarse-to-Fine Description for Fine-Grained Visual Categorization">
                                        <b>[7]</b>
                                         Yao H, Zhang S, Zhang Y, et al. Coarse-to-Fine Description for Fine-Grained Visual Categorization[J]. IEEE Transactions on Image Processing, 2016, 25 (10) :4858-4872.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" title=" Liu X, Xia T, Wang J, et al. Fully Convolutional Attention Networks for Fine-Grained Recognition[EB]. arXiv:1603.06765, 2017." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fully Convolutional Attention Networks for Fine-Grained Recognition[EB]">
                                        <b>[8]</b>
                                         Liu X, Xia T, Wang J, et al. Fully Convolutional Attention Networks for Fine-Grained Recognition[EB]. arXiv:1603.06765, 2017.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" title=" Murabito F, Spampinato C, Palazzo S, et al. Top-Down Saliency Detection Driven by Visual Classification[J]. Computer Vision &amp;amp; Image Understanding, 2018, 40 (7) :1130-1141." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Top-Down Saliency Detection Driven by Visual Classification">
                                        <b>[9]</b>
                                         Murabito F, Spampinato C, Palazzo S, et al. Top-Down Saliency Detection Driven by Visual Classification[J]. Computer Vision &amp;amp; Image Understanding, 2018, 40 (7) :1130-1141.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_10" title=" Shrivastava A, Gupta A, Girshick R. Training Region-Based Object Detectors with Online Hard Example Mining[C]//IEEE Conference on Computer Vision and Pattern Recognition. IEEE Computer Society, 2016:761-769." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Training Region-Based Object Detectors with Online Hard Example Mining">
                                        <b>[10]</b>
                                         Shrivastava A, Gupta A, Girshick R. Training Region-Based Object Detectors with Online Hard Example Mining[C]//IEEE Conference on Computer Vision and Pattern Recognition. IEEE Computer Society, 2016:761-769.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_11" >
                                        <b>[11]</b>
                                     Ren S, He K, Girshick R, et al. Faster R-CNN: Towards Real-Time Object Detection with RegionProposal Networks[C]//Proceedings of the 28th International Conference on Neural Information Processing Systems—Volume 1. MIT Press, 2015:91-99.</a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_12" title=" 杨国亮, 王志元, 张雨, 等. 基于垂直区域回归网络的自然场景文本检测[J]. 计算机工程与科学, 2018, 40 (7) :1256-1263." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJK201807017&amp;v=MzEyOTR6cXFCdEdGckNVUjdxZlp1WnNGaURsVjcvTkx6N0JaYkc0SDluTXFJOUVZNFFLREg4NHZSNFQ2ajU0TzM=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[12]</b>
                                         杨国亮, 王志元, 张雨, 等. 基于垂直区域回归网络的自然场景文本检测[J]. 计算机工程与科学, 2018, 40 (7) :1256-1263.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_13" title=" Yeung S, Russakovsky O, Mori G, et al. End-to-end learning of action detection from frame glimpses in videos[C]//IEEE Conference on Computer Vision and Pattern Recognition. IEEE Computer Society, 2016:2678-2687." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=End-to-End Learning of Action Detection from Frame Glimpses in Videos">
                                        <b>[13]</b>
                                         Yeung S, Russakovsky O, Mori G, et al. End-to-end learning of action detection from frame glimpses in videos[C]//IEEE Conference on Computer Vision and Pattern Recognition. IEEE Computer Society, 2016:2678-2687.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_14" title=" 罗建豪, 吴建鑫. 基于深度卷积特征的细粒度图像分类研究综述[J]. 自动化学报, 2017, 43 (8) :1306-1318." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201708002&amp;v=MzA3MTNpRGxWNy9OS0NMZlliRzRIOWJNcDQ5RlpvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Y=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[14]</b>
                                         罗建豪, 吴建鑫. 基于深度卷积特征的细粒度图像分类研究综述[J]. 自动化学报, 2017, 43 (8) :1306-1318.
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_15" title=" 杨兴. 基于B-CNN模型的细粒度分类算法研究[D]. 北京:中国地质大学 (北京) , 2017." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=1017136056.nh&amp;v=MjM3OTlHRnJDVVI3cWZadVpzRmlEbFY3L05WRjI2R2JLN0dOSEpxWkViUElRS0RIODR2UjRUNmo1NE8zenFxQnQ=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[15]</b>
                                         杨兴. 基于B-CNN模型的细粒度分类算法研究[D]. 北京:中国地质大学 (北京) , 2017.
                                    </a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_16" title=" Yang Z, Yang D, Dyer C, et al. Hierarchical attention networks for document classification[C]//Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2017:1480-1489." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Hierarchical Attention Networks for Document Classification">
                                        <b>[16]</b>
                                         Yang Z, Yang D, Dyer C, et al. Hierarchical attention networks for document classification[C]//Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2017:1480-1489.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JYRJ" target="_blank">计算机应用与软件</a>
                2019,36(03),210-213+264 DOI:10.3969/j.issn.1000-386x.2019.03.038            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于RPN与B- CNN的细粒度图像分类算法研究</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%B5%B5%E6%B5%A9%E5%A6%82&amp;code=37891853&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">赵浩如</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BC%A0%E6%B0%B8&amp;code=40827225&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">张永</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%88%98%E5%9B%BD%E6%9F%B1&amp;code=10521064&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">刘国柱</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E9%9D%92%E5%B2%9B%E7%A7%91%E6%8A%80%E5%A4%A7%E5%AD%A6%E4%BF%A1%E6%81%AF%E7%A7%91%E5%AD%A6%E6%8A%80%E6%9C%AF%E5%AD%A6%E9%99%A2&amp;code=0229824&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">青岛科技大学信息科学技术学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>随着大数据和硬件的快速发展, 细粒度分类任务应运而生, 其目的是对粗粒度的大类别进行子类分类。为利用类间细微差异, 提出基于RPN (Region Proposal Network) 与B-CNN (Bilinear CNN) 的细粒度图像分类算法。利用OHEM (Online Hard Example Mine) 筛选出对识别结果影响大的图像, 防止过拟合;将筛选后的图像输入到由soft-nms (Soft Non Maximum Suppression) 改进的RPN网络中, 得到对象级标注的图像, 同时减少假阴性概率;将带有对象级标注信息的图像输入到改进后的B-CNN中, 改进后的B-CNN可以融合不同层特征并加强空间联系。实验结果表明, 在CUB200-2011和Standford Dogs数据集平均识别精度分别达到85.50%和90.10%。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%BB%86%E7%B2%92%E5%BA%A6%E5%88%86%E7%B1%BB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">细粒度分类;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%B1%BB%E9%97%B4%E5%B7%AE%E5%BC%82&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">类间差异;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8F%8C%E5%90%91%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">双向卷积网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%9D%9E%E6%9E%81%E5%A4%A7%E5%80%BC%E6%8A%91%E5%88%B6&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">非极大值抑制;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%89%B9%E5%BE%81%E8%9E%8D%E5%90%88&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">特征融合;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    赵浩如, 硕士生, 主研领域:计算机视觉与图像工程。;
                                </span>
                                <span>
                                    张永, 硕士生。;
                                </span>
                                <span>
                                    刘国柱, 教授。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-09-30</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金项目 (61472196, 61672305, 61702295);</span>
                                <span>山东省自然科学基金项目 (ZR2014FM015);</span>
                    </p>
            </div>
                    <h1><b>FINE-GRANTED IMAGE CALSSIFICATION ALGORITHM BASED ON RPN AND B-CNN</b></h1>
                    <h2>
                    <span>Zhao Haoru</span>
                    <span>Zhang Yong</span>
                    <span>Liu Guozhu</span>
            </h2>
                    <h2>
                    <span>College of Information Science and Technology, Qingdao University of Science and Technology</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>With the rapid development of big data and hardware, fine-grained classification has emerged. Its purpose is to classify the coarse-granted categories into subclasses. In order to use the subtle differences between similarities, we proposed a fine-granted classification algorithm based on RPN and B-CNN. The online hard example mine (OHEM) algorithm was used to screen out the images which had a great impact on the recognition results to prevent the over-fitting. Then, the selected image was input into the RPN network improved by soft non maximum suppression (soft-nms) . The false negative probability was reduced, and the image with object-level annotation was obtained. The image with object-level annotation was input the improved B-CNN. The improved B-CNN could fuse features of different layers and enhanced their spatial connection. The experimental results demonstrate that the average recognition accuracy of CUB200-2011 and Stanford Dogs datasets is 85.50% and 90.10%.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Fine-granted%20classification&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Fine-granted classification;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Interclass%20difference&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Interclass difference;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=B-CNN&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">B-CNN;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Non-maximum%20suppression&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Non-maximum suppression;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Feature%20fused&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Feature fused;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-09-30</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="35" name="35" class="anchor-tag"><b>0 引 言</b></h3>
                <div class="p1">
                    <p id="36">作为计算机视觉的重要研究方向, 图像分类<citation id="79" type="reference"><link href="3" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>问题一直备受学者关注。图像分类又包括对象级分类, 如对猫和狗进行分类。还包括细粒度分类<citation id="80" type="reference"><link href="5" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>, 如对狗的不同品种进行分类。由于细微的类内差异, 往往只能借助微小的局部差异才能分出不同的子类别, 使得细粒度分类十分具有挑战性。细粒度分类的方法主要包括两种:一种是基于强监督的分类模型, 如Part-based R-CNN<citation id="81" type="reference"><link href="7" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>不仅需要物体级标注, 还需要局部区域的标注, 这大大限制了在实际场景的应用;另一种是基于弱监督的分类模型, 如B-CNN<citation id="82" type="reference"><link href="9" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>仅仅需要图像级别的标注, 不需要局部信息的标注。因此, 基于弱监督的分类模型在识别精度上要比基于强监督的分类模型差一些。Huang等<citation id="83" type="reference"><link href="11" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>提出了Part-Stacked CNN进行细粒度分类。这个网络需要提供对象及部位级标签, 它分为定位网络和分类网络两个子网络, 采用经典的AlexNet网络结构作为整个网络的基本结构。Shen等<citation id="84" type="reference"><link href="13" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>提出一种迭代的传递策略来优化目标框, 借助对象及部分级标注框进行细粒度分类。Yao等<citation id="85" type="reference"><link href="15" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>提出了多级的由粗到细的目标描述方法进行细粒度分类, 不需借助标注框, 但识别率不如最前沿的算法。Liu等<citation id="86" type="reference"><link href="17" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>提出了基于全连接的注意力机制的网络结构进行细粒度分类, 未考虑各层特征间的联系。Murabito等<citation id="87" type="reference"><link href="19" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>提出显著性特征分类网络 (SalClassNet) 。它包括两个子网络, 网络A计算输入图片的显著性特征, 网络B计算网络A输出的显著性特征进行细粒度分类, 计算显著性特征首先要计算图像像素对应正确分类标准化分数梯度的绝对值, 然后取三个颜色通道的最大值, 因此, 计算成本太高。综上, 为避免人工标注部位级标签花费的巨大时间, 以及减少计算成本。本文提出利用soft-nms和OHEM优化RPN算法得到更精确的对象级标注, 以防止背景的干扰, 同时改进B-CNN网络, 加强不同层特征间的空间联系, 提高识别精度。</p>
                </div>
                <h3 id="37" name="37" class="anchor-tag"><b>1 算法描述</b></h3>
                <div class="p1">
                    <p id="38">为利用细微的类内差异, 本文采用OHEM<citation id="88" type="reference"><link href="21" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>筛选出对识别结果影响大的数据, 可以有效防止无关信息的干扰。然后, 利用soft-nms<citation id="89" type="reference"><link href="23" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>优化RPN<citation id="90" type="reference"><link href="25" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>网络, 选择出置信度更高的目标所在区域。最后, 改进B-CNN网络结构对目标区域进行细粒度分类, 具体的算法流程如图1所示。</p>
                </div>
                <div class="area_img" id="39">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201903039_039.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 算法流程图" src="Detail/GetImg?filename=images/JYRJ201903039_039.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 算法流程图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201903039_039.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="40" name="40"><b>1.1 获取目标区域</b></h4>
                <div class="p1">
                    <p id="41">RPN网络的作用是输入一张图像, 输出置信度排名前<i>N</i>个目标可能在的区域。本文利用OHEM筛选出对最终识别结果影响大的样本, 并用筛选后样本进行随机梯度下降。去除了对识别结果影响小的样本后, 有效防止过拟合, 具体算法流程如图2所示。</p>
                </div>
                <div class="area_img" id="42">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201903039_042.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 获取目标区域算法描述图" src="Detail/GetImg?filename=images/JYRJ201903039_042.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 获取目标区域算法描述图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201903039_042.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="43">图2中, OHEM有两个不同的ROI网络。左边的ROI网络只负责前向传播计算误差, 右边的ROI网络从左边的ROI网络中通过对误差排序, 选出误差大的样本作为右边ROI网络的输入。RPN网络输出的矩形目标框<i>D</i><sub><i>i</i></sub>, 其得分<i>f</i><sub><i>i</i></sub>的计算如下:</p>
                </div>
                <div class="p1">
                    <p id="44"><mathml id="45"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>f</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mfrac><mrow><mi>S</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mi>S</mi></mfrac></mrow></math></mathml>      (1) </p>
                </div>
                <div class="p1">
                    <p id="46">式中:<i>S</i><sub><i>i</i></sub>是重叠框的交集的面积;<i>S</i>是重叠框的并集的面积。</p>
                </div>
                <div class="p1">
                    <p id="47">NMS (Non-maximum suppression) 是RPN中重要的组成部分。RPN输出一系列的检测框<i>D</i><sub><i>i</i></sub>以及对应的分数<i>f</i><sub><i>i</i></sub>。NMS设置常数阈值<i>τ</i>, 当检测框的得分大于阈值<i>τ</i>, 将其放入最终的检测结果集合<i>D</i>。同时, 集合<i>D</i>中任何与检测框<i>M</i>的重叠部分大于重叠阈值<i>τ</i>的检测框, 被强制归零并移除。非最大抑制算法中的最大问题就是将相邻检测框的分数均强制归零后, 如果真实的物体在重叠区域出现, 则将导致对该物体的检测失败并降低了算法的平均检测率。soft-nms不将大于阈值<i>τ</i>的相邻目标框得分重置为0, 而是乘以一个衰减函数。选取所有的目标框中得分最高的<i>N</i>个, 这样可以有效减少假阴性的概率, 提高平均识别率。具体计算如下:</p>
                </div>
                <div class="area_img" id="48">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JYRJ201903039_04800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <h4 class="anchor-tag" id="50" name="50"><b>1.2 基于深度学习进行细粒度分类</b></h4>
                <div class="p1">
                    <p id="51">Bilinear CNN模型包括Stream A和Stream B, Stream A和Stream B的网络结构都是采用的VGGNet。Stream A的作用是对物体的局部信息进行定位, 而Stream B则是对Stream A检测到的局部信息进行特征提取。两个网络相互协调作用, 完成了细粒度图像分类过程中两个最重要的任务:物体、局部区域的检测与特征提取。本文在B-CNN基础上增加了两个外积操作, 外积计算如下:</p>
                </div>
                <div class="p1">
                    <p id="52"><i>B</i>=<i>f</i><sup>T</sup><sub><i>A</i></sub>·<i>f</i><sub><i>B</i></sub>      (3) </p>
                </div>
                <div class="p1">
                    <p id="53">双线性特征<i>B</i><sub>2 </sub>、<i>B</i><sub>3</sub>分别是conv4_3的特征与conv5_3的特征, conv5_1的特征与conv5_3的特征进行点乘得到的。然后将双线性特征<i>B</i><sub>2</sub>、<i>B</i><sub>3</sub>与原有的conv5_3层特征与conv5_3层特征点乘得到的双线性特征<i>B</i><sub>1</sub>拼接起来, 以加强不同层特征间的空间联系。最后, 将拼接后的特征<i>B</i>送进全连接层, 进行softmax分类。具体算法流程如图3所示。</p>
                </div>
                <div class="area_img" id="54">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201903039_054.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 改进的B-CNN算法流程图" src="Detail/GetImg?filename=images/JYRJ201903039_054.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 改进的B-CNN算法流程图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201903039_054.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="55"><i>f</i><sub><i>A</i></sub>与<i>f</i><sub><i>B</i></sub>是不同层的特征, 双线性特征<b><i>B</i></b><sub><i>i</i></sub>是一个<b><i>C</i>×<i>W</i>×<i>H</i></b>的三维矩阵, 将其转化为长度为<b><i>C</i>、<i>W</i>、<i>H</i></b>的列向量。然后将双线性特征<i>B</i><sub>1</sub>、<i>B</i><sub>2</sub>、<i>B</i><sub>3</sub>拼接成一个长度为3<b><i>CWH</i></b>的列向量<b><i>B</i></b>, 将其输入到softmax函数进行分类。最后, 模型在端到端<citation id="91" type="reference"><link href="27" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>的训练过程中, 从图3可以看出模型的前半部分是普通的卷积层与池化层。因此, 只要求得后半部分的梯度值, 即可完成对整个模型的训练。假设Stream A与Stream B的特征分别是<i>f</i><sub><i>a</i></sub>与<i>f</i><sub><i>b</i></sub>, 则双线性特征为<i>B</i>=<i>f</i><sup>T</sup><sub><i>a</i></sub>·<i>f</i><sub><i>b</i></sub>。<mathml id="56"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mi>d</mi><msub><mrow></mrow><mi>l</mi></msub></mrow><mrow><mi>d</mi><msub><mrow></mrow><mi>B</mi></msub></mrow></mfrac></mrow></math></mathml>为特征<i>B</i>的梯度值, <i>l</i>是损失函数。根据链式法则结合式 (4) 、式 (5) , 得到两个网络的梯度值, 从而完成端到端的模型训练。</p>
                </div>
                <div class="p1">
                    <p id="57"><mathml id="58"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mi>d</mi><msub><mrow></mrow><mi>l</mi></msub></mrow><mrow><mi>d</mi><msub><mrow></mrow><mrow><mi>f</mi><msub><mrow></mrow><mi>a</mi></msub></mrow></msub></mrow></mfrac><mo>=</mo><mi>f</mi><msub><mrow></mrow><mi>b</mi></msub><mrow><mo> (</mo><mrow><mfrac><mrow><mi>d</mi><msub><mrow></mrow><mi>l</mi></msub></mrow><mrow><mi>d</mi><msub><mrow></mrow><mi>B</mi></msub></mrow></mfrac></mrow><mo>) </mo></mrow><msup><mrow></mrow><mtext>Τ</mtext></msup></mrow></math></mathml>      (4) </p>
                </div>
                <div class="p1">
                    <p id="59"><mathml id="60"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mi>d</mi><msub><mrow></mrow><mi>l</mi></msub></mrow><mrow><mi>d</mi><msub><mrow></mrow><mrow><mi>f</mi><msub><mrow></mrow><mi>b</mi></msub></mrow></msub></mrow></mfrac><mo>=</mo><mi>f</mi><msub><mrow></mrow><mi>a</mi></msub><mrow><mo> (</mo><mrow><mfrac><mrow><mi>d</mi><msub><mrow></mrow><mi>l</mi></msub></mrow><mrow><mi>d</mi><msub><mrow></mrow><mi>B</mi></msub></mrow></mfrac></mrow><mo>) </mo></mrow></mrow></math></mathml>      (5) </p>
                </div>
                <h3 id="61" name="61" class="anchor-tag"><b>2 实验仿真</b></h3>
                <h4 class="anchor-tag" id="62" name="62"><b>2.1 实验背景</b></h4>
                <div class="p1">
                    <p id="63">为验证本算法的有效性, 与文献<citation id="95" type="reference">[<a class="sup">5</a>,<a class="sup">6</a>,<a class="sup">7</a>,<a class="sup">8</a>,<a class="sup">9</a>]</citation>中的算法的结果进行对比。文献<citation id="96" type="reference">[<a class="sup">5</a>,<a class="sup">6</a>,<a class="sup">7</a>,<a class="sup">8</a>,<a class="sup">9</a>]</citation>分别采用CUB200-2011数据集<citation id="92" type="reference"><link href="29" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>与Standford Dogs数据集<citation id="93" type="reference"><link href="31" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>。因此本文也在两组数据集上进行两组实验, 来证明本算法的识别精度比文献<citation id="97" type="reference">[<a class="sup">5</a>,<a class="sup">6</a>,<a class="sup">7</a>,<a class="sup">8</a>,<a class="sup">9</a>]</citation>中的结果高。第一组实验是在CUB200-2011数据集进行的测试和验证。该数据集是最常用和经典的细粒度分类数据集, 包括200中不同类别, 共11 788张不同鸟类图片, 不仅提供了对象级标注框而且还提供了局部级标注框。第二组实验是在Standford Dogs数据集<citation id="94" type="reference"><link href="31" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>进行测试和验证。该数据集包括120类狗的图像数据, 共有20 580张图片, 只提供对象级标注框。基于RPN与B-CNN的细粒度分类过程中所用到的主要参数如表1所示。</p>
                </div>
                <div class="area_img" id="64">
                    <p class="img_tit"><b>表1 B-CNN主要参数表</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="64" border="1"><tr><td>Batch size</td><td>Display</td><td>Momentum</td><td>Base_lr</td></tr><tr><td><br />10</td><td>20</td><td>0.9</td><td>0.001</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="65" name="65"><b>2.2 实验结果与分析</b></h4>
                <div class="p1">
                    <p id="66">本文利用OHEM与soft-nms优化RPN, 获取对象级标注, 然后输入到改进的B-CNN。在RPN阶段, 训练集、验证集和测试集的比例是7∶2∶1。采取的Anchor的尺度是 (128, 256, 512) , 比例为 (0.5, 1, 2) , 共九种。将一张图片输入到RPN就会产生大量的Anchor, 对这些Anchor进行soft-nms, 最终输出得分最高的目标框。在目标框提供的位置上剪贴图片, 剪贴后的图片只含有目标对象, 没有背景的干扰。B-CNN阶段中训练集, 验证集与测试集的比例是7∶1.5∶1.5。在ImageNet中1 000类分类训练好的参数的基础上, 在CUB200-2011数据集进行微调。将图片输入到B-CNN后, Stream A的作用是对图像中对象的特征部位进行定位, 而Stream B则是用来对Stream A检测到的特征区域进行特征提取。两个网络相互协调作用, 完成了细粒度图像分类过程中两个最关键的任务。</p>
                </div>
                <div class="p1">
                    <p id="67">本文采用softmax函数做分类函数输出一个概率值, 计算公式如下:</p>
                </div>
                <div class="p1">
                    <p id="68"><mathml id="69"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>S</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mfrac><mrow><mi>e</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mi>j</mi></munder><mi>e</mi></mstyle><msub><mrow></mrow><mi>j</mi></msub></mrow></mfrac></mrow></math></mathml>      (6) </p>
                </div>
                <div class="p1">
                    <p id="70">式中:<i>S</i><sub><i>i</i></sub>是第<i>i</i>个类别的概率值;<i>e</i><sub><i>i</i></sub>是第<i>i</i>个类别的得分。</p>
                </div>
                <div class="p1">
                    <p id="71">与文献<citation id="100" type="reference">[<a class="sup">5</a>,<a class="sup">6</a>]</citation>借助对象级及部位级标注框进行细粒度分类对比, 本文仅仅采用了对象级标注框。与文献<citation id="98" type="reference">[<a class="sup">7</a>]</citation>利用迭代的方法获取对象级与部位级标注框对比, 本文利用RPN提取目标区域, 并将深度学习框架的注意力<citation id="99" type="reference"><link href="33" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>全放在目标区域, 防止无关信息的干扰, 提高识别速度与精度。实验结果如表2所示。实验表明, 本文的算法识别率为85.5%, 比文献<citation id="101" type="reference">[<a class="sup">5</a>,<a class="sup">6</a>,<a class="sup">7</a>]</citation>中的方法分别高了8.90%、1.5%、3.0%。证明本文提出的基于RPN与B-CNN的细粒度分类算法, 将识别的重心放在目标区域内。利用B-CNN优化目标区域的同时, 在目标区域内提取特征, 不仅不需要提供额外的部位级标注框, 并且准确率有较大提高。</p>
                </div>
                <div class="area_img" id="72">
                    <p class="img_tit"><b>表2 不同方法在CUB200-2011数据集的识别率</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="72" border="1"><tr><td><br />方法</td><td>Huang</td><td>Shen</td><td>Yao</td><td>Our approach</td></tr><tr><td><br />准确率</td><td>76.60%</td><td>84.00%</td><td>82.50%</td><td>85.50%</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="73">Standford Dogs数据集是从ImageNet数据集中提取狗的类别组成的。本文在第一组实验获取的参数基础上进行微调, 实验结果如表3所示。与文献<citation id="102" type="reference">[<a class="sup">8</a>]</citation>基于对象级与部位级标注框与注意力机制相比, 虽然两者都将识别重心放在目标区域, 但本文在仅仅使用对象级标注框的前提下, 利用外积将B-CNN的Strean A与Stream B统一成一个端到端的训练模型。与文献<citation id="103" type="reference">[<a class="sup">9</a>]</citation>使用SalClassNet网络提取显著性特征, 并对显著性特征进行细粒度分类相比, 本文使用对象级标注框在ROI区域上进行特征提取。因此, 识别率分别比文献<citation id="104" type="reference">[<a class="sup">8</a>]</citation>和文献<citation id="105" type="reference">[<a class="sup">9</a>]</citation>的方法高了1.2%和3.9%。这表明同时对标注框与类别进行端到端的训练能有效提高识别率。</p>
                </div>
                <div class="area_img" id="74">
                    <p class="img_tit"><b>表3 不同方法在Standford Dogs数据集的识别率</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="74" border="1"><tr><td>方法</td><td>Liu</td><td>Murabito</td><td>Our approach</td></tr><tr><td><br />准确率</td><td>88.9%</td><td>86.20%</td><td>90.10%</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="75">此外, 对本文提出的算法, 增加了5组对比实验分别为:方案一, 不使用OHEM优化RPN, 不改变B-CNN网络结构;方案二, 不使用soft-nms优化RPN, 不改变B-CNN网络结构;方案三, 在使用OHEM及soft-nms的前提下, 不增加B-CNN 的外积操作;方案四, 仅增加B-CNN的外积操作;方案五, 使用OHEM及soft-nms, 同时增加B-CNN的外积操作。实验对比结果如表4所示。实验结果表明, 方案五的识别率为90.10%, 比方案一、方案二、方案三、方案四分别高了2.9%、2.3%、1.6%、1.1%。方案一仅使用OHEM, 仅有效地防止了过拟合;方案二仅使用soft-nms, 使输出的对象级标注更加准确, 并减少了假阴性概率;方案三则结合了方案一与方案二, 识别率有所提升;方案四仅增加B-CNN的外积操作, 加强了不同层之间的空间联系。这表明使用OHEM与soft-nms改进RPN, 能让获得的对象级标注更加精确, 既可以避免背景的干扰, 减少假阴性, 又能有效防止过拟合。而增加B-CNN的外积操作, 增加了不同层特征间的空间联系。这是因为不同层关注的特征不同并且感受野大小也不同, 这可以有效地提高识别率。</p>
                </div>
                <div class="area_img" id="76">
                    <p class="img_tit"><b>表4 对比实验结果图</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="76" border="1"><tr><td><br />方法</td><td>方案一</td><td>方案二</td><td>方案三</td><td>方案四</td><td>方案五</td></tr><tr><td><br />准确率</td><td>87.20%</td><td>87.90%</td><td>88.50%</td><td>89.00%</td><td>90.10%</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h3 id="77" name="77" class="anchor-tag"><b>3 结 语</b></h3>
                <div class="p1">
                    <p id="78">本文针对细粒度分类子类别间细微的类间差异、较大的类内差异、依赖大量人工标注信息等问题, 提出了基于RPN与B-CNN的细粒度分类算法。本文的主要贡献如下: (1) 利用RPN网络自动输出对象级标注, 不需要部位级标注, 避免标注对象部位花费的精力。 (2) 使用soft-nms和OHEM算法改进RPN, 输出更加精确的区域提议, 可以有效防止过拟合并减少假阴性概率。 (3) 改进B-CNN网络, 增加不同层间的外积操作, 以融合不同层的特征, 并将双线性特征级联在一起加强空间的联系。实验结果证明, 基于RPN与B-CNN的细粒度分类算法能显著提高识别率。但由于增加了RPN网络以及OHEM与soft-nms操作, 程序的运行时间相比其他算法有所增加。并且, 未将RPN网络与B-CNN网络联合起来, 也是本文的不足。接下来, 我们的工作重心将放在使RPN与B-CNN网络联合成一个端到端的模型, 并提取同类物体不同子类的差异特征, 作为深度网络的输入来提高准确率。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201807046&amp;v=MjE5ODdUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRmlEbFY3L05MelRaWkxHNEg5bk1xSTlCWW9RS0RIODR2UjQ=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> 彭晏飞, 陶进, 訾玲玲. 基于卷积神经网络和E2LSH的遥感图像检索研究[J].计算机应用与软件, 2018, 35 (7) :250-255.
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Leveraging multiple tasks to regularize fine-grained classification">

                                <b>[2]</b> Dasgupta R, Namboodiri A M. Leveraging multiple tasks to regularize fine-grained classification[C]//International Conference on Pattern Recognition.IEEE, 2017:3476-3481.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Detection of vehicle parts based on Faster R-CNN and relative position information">

                                <b>[3]</b> Sang N, Chen Y, Gao C, et al. Detection of vehicle parts based on Faster R-CNN and relative position information[C]//Pattern Recognition and Computer Vision. 2018:83.
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Bilinear CNNs for Fine-grained Visual Recognition">

                                <b>[4]</b> Lin T Y, Roychowdhury A, Maji S. Bilinear CNN Models for Fine-Grained Visual Recognition[C]//IEEE International Conference on Computer Vision.IEEE, 2016:1449-1457.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Part-Stacked CNN for Fine-Grained Visual Categorization">

                                <b>[5]</b> Huang S, Xu Z, Tao D, et al. Part-Stacked CNN for Fine-Grained Visual Categorization[C]//Computer Vision and Pattern Recognition. IEEE, 2016:1173-1182.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Iterative object and part transfer for fine-grained recognition">

                                <b>[6]</b> Shen Z, Jiang Y G, Wang D, et al. Iterative object and part transfer for fine-grained recognition[C]//IEEE International Conference on Multimedia and Expo.IEEE, 2017:1470-1475.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Coarse-to-Fine Description for Fine-Grained Visual Categorization">

                                <b>[7]</b> Yao H, Zhang S, Zhang Y, et al. Coarse-to-Fine Description for Fine-Grained Visual Categorization[J]. IEEE Transactions on Image Processing, 2016, 25 (10) :4858-4872.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fully Convolutional Attention Networks for Fine-Grained Recognition[EB]">

                                <b>[8]</b> Liu X, Xia T, Wang J, et al. Fully Convolutional Attention Networks for Fine-Grained Recognition[EB]. arXiv:1603.06765, 2017.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Top-Down Saliency Detection Driven by Visual Classification">

                                <b>[9]</b> Murabito F, Spampinato C, Palazzo S, et al. Top-Down Saliency Detection Driven by Visual Classification[J]. Computer Vision &amp; Image Understanding, 2018, 40 (7) :1130-1141.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Training Region-Based Object Detectors with Online Hard Example Mining">

                                <b>[10]</b> Shrivastava A, Gupta A, Girshick R. Training Region-Based Object Detectors with Online Hard Example Mining[C]//IEEE Conference on Computer Vision and Pattern Recognition. IEEE Computer Society, 2016:761-769.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_11" >
                                    <b>[11]</b>
                                 Ren S, He K, Girshick R, et al. Faster R-CNN: Towards Real-Time Object Detection with RegionProposal Networks[C]//Proceedings of the 28th International Conference on Neural Information Processing Systems—Volume 1. MIT Press, 2015:91-99.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_12" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJK201807017&amp;v=Mjc2NzVFWTRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRmlEbFY3L05MejdCWmJHNEg5bk1xSTk=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[12]</b> 杨国亮, 王志元, 张雨, 等. 基于垂直区域回归网络的自然场景文本检测[J]. 计算机工程与科学, 2018, 40 (7) :1256-1263.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=End-to-End Learning of Action Detection from Frame Glimpses in Videos">

                                <b>[13]</b> Yeung S, Russakovsky O, Mori G, et al. End-to-end learning of action detection from frame glimpses in videos[C]//IEEE Conference on Computer Vision and Pattern Recognition. IEEE Computer Society, 2016:2678-2687.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_14" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201708002&amp;v=MzE4NTNpRGxWNy9OS0NMZlliRzRIOWJNcDQ5RlpvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0Y=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[14]</b> 罗建豪, 吴建鑫. 基于深度卷积特征的细粒度图像分类研究综述[J]. 自动化学报, 2017, 43 (8) :1306-1318.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_15" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=1017136056.nh&amp;v=MDQyODI4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGaURsVjcvTlZGMjZHYks3R05ISnFaRWJQSVFLREg=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[15]</b> 杨兴. 基于B-CNN模型的细粒度分类算法研究[D]. 北京:中国地质大学 (北京) , 2017.
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Hierarchical Attention Networks for Document Classification">

                                <b>[16]</b> Yang Z, Yang D, Dyer C, et al. Hierarchical attention networks for document classification[C]//Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2017:1480-1489.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JYRJ201903039" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201903039&amp;v=MzI2NjVSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGaURsVjcvS0x6VFpaTEc0SDlqTXJJOUdiWVFLREg4NHY=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="1" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
