<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637135647772600000%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJYRJ201906037%26RESULT%3d1%26SIGN%3ddLR9cdbPeqw8IZvChlA%252bPMcKCPY%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JYRJ201906037&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JYRJ201906037&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201906037&amp;v=MDYyOTF0Rnl2bVZMekpMelRaWkxHNEg5ak1xWTlHWTRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVo=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#73" data-title="&lt;b&gt;0 引 言&lt;/b&gt; "><b>0 引 言</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#82" data-title="&lt;b&gt;1 3D-R2N2网络模型&lt;/b&gt; "><b>1 3D-R2N2网络模型</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#84" data-title="&lt;b&gt;1.1 Encoder模块&lt;/b&gt;"><b>1.1 Encoder模块</b></a></li>
                                                <li><a href="#86" data-title="&lt;b&gt;1.2 3D convolutional LSTM模块&lt;/b&gt;"><b>1.2 3D convolutional LSTM模块</b></a></li>
                                                <li><a href="#88" data-title="&lt;b&gt;1.3 Decoder模块&lt;/b&gt;"><b>1.3 Decoder模块</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#90" data-title="&lt;b&gt;2 改进神经网络的&lt;/b&gt;&lt;b&gt;单幅图像物体重建方法&lt;/b&gt; "><b>2 改进神经网络的</b><b>单幅图像物体重建方法</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#91" data-title="&lt;b&gt;2.1 对Encoder模块的优化&lt;/b&gt;"><b>2.1 对Encoder模块的优化</b></a></li>
                                                <li><a href="#103" data-title="&lt;b&gt;2.2 多特征重建网络&lt;/b&gt;"><b>2.2 多特征重建网络</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#107" data-title="&lt;b&gt;3 实验与结果分析&lt;/b&gt; "><b>3 实验与结果分析</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#109" data-title="&lt;b&gt;3.1 实验数据集与评价标准&lt;/b&gt;"><b>3.1 实验数据集与评价标准</b></a></li>
                                                <li><a href="#112" data-title="&lt;b&gt;3.2 网络模型及训练&lt;/b&gt;"><b>3.2 网络模型及训练</b></a></li>
                                                <li><a href="#114" data-title="&lt;b&gt;3.3 ShapeNet数据集实验结果分析&lt;/b&gt;"><b>3.3 ShapeNet数据集实验结果分析</b></a></li>
                                                <li><a href="#118" data-title="&lt;b&gt;3.4 改进的Encoder模块对计算开销的提升&lt;/b&gt;"><b>3.4 改进的Encoder模块对计算开销的提升</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#122" data-title="&lt;b&gt;4 结 语&lt;/b&gt; "><b>4 结 语</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#93" data-title="图1 Inception-resnet-A的结构">图1 Inception-resnet-A的结构</a></li>
                                                <li><a href="#96" data-title="图2 Inception-resnet-B的结构">图2 Inception-resnet-B的结构</a></li>
                                                <li><a href="#102" data-title="图3 改进的Encoder模块网络结构">图3 改进的Encoder模块网络结构</a></li>
                                                <li><a href="#106" data-title="图4 多特征重建网络结构框架">图4 多特征重建网络结构框架</a></li>
                                                <li><a href="#116" data-title="&lt;b&gt;表1 不同重建方法的IoU值对比&lt;/b&gt;"><b>表1 不同重建方法的IoU值对比</b></a></li>
                                                <li><a href="#117" data-title="图5 多特征重建网络与3D-R2N2的重建效果对比">图5 多特征重建网络与3D-R2N2的重建效果对比</a></li>
                                                <li><a href="#120" data-title="&lt;b&gt;表2 不同重建方法训练时间及参数数量比较&lt;/b&gt;"><b>表2 不同重建方法训练时间及参数数量比较</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="144">


                                    <a id="bibliography_1" title=" Kemelmacher-Shlizerman I, Basri R.3D face reconstruction from a single image using a single reference face shape[J].IEEE transactions on pattern analysis and machine intelligence, 2011, 33 (2) :394-405." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=3D Face Reconstruction from a Single Image Using a Single Reference Face Shape">
                                        <b>[1]</b>
                                         Kemelmacher-Shlizerman I, Basri R.3D face reconstruction from a single image using a single reference face shape[J].IEEE transactions on pattern analysis and machine intelligence, 2011, 33 (2) :394-405.
                                    </a>
                                </li>
                                <li id="146">


                                    <a id="bibliography_2" title=" Baka N, Kaptein B L, De Bruijne M, et al.2D-3D shape reconstruction of the distal femur from stereo X-ray imaging using statistical shape models[J].Medical image analysis, 2011, 15 (6) :840-850." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011300711267&amp;v=MjU5MTBmT2ZiSzdIdEROckk5Rlkrb09Ebm8rb0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcnpJSTEwVGFSTT1OaQ==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         Baka N, Kaptein B L, De Bruijne M, et al.2D-3D shape reconstruction of the distal femur from stereo X-ray imaging using statistical shape models[J].Medical image analysis, 2011, 15 (6) :840-850.
                                    </a>
                                </li>
                                <li id="148">


                                    <a id="bibliography_3" title=" Dworzak J, Lamecker H, Von Berg J, et al.3D reconstruction of the human rib cage from 2D projection images using a statistical shape model[J].International journal of computer assisted radiology and surgery, 2010, 5 (2) :111-124." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00003673695&amp;v=Mjk0NTBIUHFZaEdZdUlLWTNrNXpCZGg0ajk5U1hxUnJ4b3hjTUg3UjdxZForWnVGaXZsVUx6UEkxOD1OajdCYXJPNEh0&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         Dworzak J, Lamecker H, Von Berg J, et al.3D reconstruction of the human rib cage from 2D projection images using a statistical shape model[J].International journal of computer assisted radiology and surgery, 2010, 5 (2) :111-124.
                                    </a>
                                </li>
                                <li id="150">


                                    <a id="bibliography_4" title=" 陈加, 张玉麒, 宋鹏, 等.深度学习在基于单幅图像的物体三维重建中的应用[J].自动化学报, 2019, 45 (4) :657-668." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201904002&amp;v=MjIyMDhIOWpNcTQ5RlpvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVadEZ5dm1WTHpKS0NMZlliRzQ=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                         陈加, 张玉麒, 宋鹏, 等.深度学习在基于单幅图像的物体三维重建中的应用[J].自动化学报, 2019, 45 (4) :657-668.
                                    </a>
                                </li>
                                <li id="152">


                                    <a id="bibliography_5" title=" 陈加, 吴晓军.联合LBS和Snake的3D人体外形和运动跟踪方法[J].计算机辅助设计与图形学学报, 2012, 24 (3) :357-363." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJF201203011&amp;v=MDI3NDRSN3FmWnVadEZ5dm1WTHpKTHo3QmFMRzRIOVBNckk5RVpZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1U=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                         陈加, 吴晓军.联合LBS和Snake的3D人体外形和运动跟踪方法[J].计算机辅助设计与图形学学报, 2012, 24 (3) :357-363.
                                    </a>
                                </li>
                                <li id="154">


                                    <a id="bibliography_6" title=" Chen J, Wu X, Wang M Y, et al.Human Body Shape and Motion Tracking by Hierarchical Weighted ICP[C]//Proceedings of the 7th international conference on Advances in visual computing—Volume Part II.Springer-Verlag, 2011:408-417." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Human Body Shape and Motion Tracking by Hierarchical Weighted ICP">
                                        <b>[6]</b>
                                         Chen J, Wu X, Wang M Y, et al.Human Body Shape and Motion Tracking by Hierarchical Weighted ICP[C]//Proceedings of the 7th international conference on Advances in visual computing—Volume Part II.Springer-Verlag, 2011:408-417.
                                    </a>
                                </li>
                                <li id="156">


                                    <a id="bibliography_7" title=" H&#228;ming K, Peters G.The structure-from-motion reconstruction pipeline—a survey with focus on short image sequences[J].Kybernetika, 2010, 46 (5) :926-937." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The structure-from-motion reconstruction pipeline -- a survey with focus on short image sequences">
                                        <b>[7]</b>
                                         H&#228;ming K, Peters G.The structure-from-motion reconstruction pipeline—a survey with focus on short image sequences[J].Kybernetika, 2010, 46 (5) :926-937.
                                    </a>
                                </li>
                                <li id="158">


                                    <a id="bibliography_8" title=" Lhuillier M, Quan L.A quasi-dense approach to surface reconstruction from uncalibrated images[J].IEEE transactions on pattern analysis and machine intelligence, 2005, 27 (3) :418-433." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Quasi-Dense Approach to Surface Reconstruction from Uncalibrated Images">
                                        <b>[8]</b>
                                         Lhuillier M, Quan L.A quasi-dense approach to surface reconstruction from uncalibrated images[J].IEEE transactions on pattern analysis and machine intelligence, 2005, 27 (3) :418-433.
                                    </a>
                                </li>
                                <li id="160">


                                    <a id="bibliography_9" title=" Kar A, H&#228;ne C, Malik J.Learning a multi-view stereo machine[C]//Advances in neural information processing systems, 2017:365-376." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning a multi-view stereo machine">
                                        <b>[9]</b>
                                         Kar A, H&#228;ne C, Malik J.Learning a multi-view stereo machine[C]//Advances in neural information processing systems, 2017:365-376.
                                    </a>
                                </li>
                                <li id="162">


                                    <a id="bibliography_10" title=" Oswald M R, T&#246;ppe E, Nieuwenhuis C, et al.A review of geometry recovery from a single image focusing on curved object reconstruction[M]//Innovations for Shape Analysis, Models and Algorithms, Springer, 2013:343-378." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Review of Geometry Recovery from a Single Image Focusing on Curved Object Reconstruction">
                                        <b>[10]</b>
                                         Oswald M R, T&#246;ppe E, Nieuwenhuis C, et al.A review of geometry recovery from a single image focusing on curved object reconstruction[M]//Innovations for Shape Analysis, Models and Algorithms, Springer, 2013:343-378.
                                    </a>
                                </li>
                                <li id="164">


                                    <a id="bibliography_11" title=" Lim J J, Pirsiavash H, Torralba A.Parsing ikea objects:Fine pose estimation[C]//Proceedings of the IEEE International Conference on Computer Vision, 2013:2992-2999." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Parsing ikea objects:Fine pose estimation">
                                        <b>[11]</b>
                                         Lim J J, Pirsiavash H, Torralba A.Parsing ikea objects:Fine pose estimation[C]//Proceedings of the IEEE International Conference on Computer Vision, 2013:2992-2999.
                                    </a>
                                </li>
                                <li id="166">


                                    <a id="bibliography_12" title=" Satkin S, Rashid M, Lin J, et al.3dnn:3d nearest neighbor[J].International Journal of Computer Vision, 2015, 111 (1) :69-97." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD15011900002695&amp;v=MzIxMTRzTkNuVThvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp0RmlubFVyeklJMTBUYVJNPU5qN0Jhcks5SHRETnBvOUZaTw==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[12]</b>
                                         Satkin S, Rashid M, Lin J, et al.3dnn:3d nearest neighbor[J].International Journal of Computer Vision, 2015, 111 (1) :69-97.
                                    </a>
                                </li>
                                <li id="168">


                                    <a id="bibliography_13" title=" Blanz V, Vetter T.A morphable model for the synthesis of 3D faces[C]//Proceedings of the 26th annual conference on Computer graphics and interactive techniques, 1999:187-194." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A morphable model for the synthesis of 3D faces">
                                        <b>[13]</b>
                                         Blanz V, Vetter T.A morphable model for the synthesis of 3D faces[C]//Proceedings of the 26th annual conference on Computer graphics and interactive techniques, 1999:187-194.
                                    </a>
                                </li>
                                <li id="170">


                                    <a id="bibliography_14" title=" Blanz V, Vetter T.Face recognition based on fitting a 3D morphable model[J].IEEE Transactions on pattern analysis and machine intelligence, 2003, 25 (9) :1063-1074." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Face recognition based on fitting a 3D morphable model">
                                        <b>[14]</b>
                                         Blanz V, Vetter T.Face recognition based on fitting a 3D morphable model[J].IEEE Transactions on pattern analysis and machine intelligence, 2003, 25 (9) :1063-1074.
                                    </a>
                                </li>
                                <li id="172">


                                    <a id="bibliography_15" title=" Cashman T J, Fitzgibbon A W.What shape are dolphins?building 3d morphable models from 2d images[J].IEEE transactions on pattern analysis and machine intelligence, 2013, 35 (1) :232-244." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=What Shape Are Dolphins? Building 3D Morphable Models from 2D Images">
                                        <b>[15]</b>
                                         Cashman T J, Fitzgibbon A W.What shape are dolphins?building 3d morphable models from 2d images[J].IEEE transactions on pattern analysis and machine intelligence, 2013, 35 (1) :232-244.
                                    </a>
                                </li>
                                <li id="174">


                                    <a id="bibliography_16" title=" Vicente S, Carreira J, Agapito L, et al.Reconstructing pascal voc[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2014:41-48." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Reconstructing PASCAL VOC">
                                        <b>[16]</b>
                                         Vicente S, Carreira J, Agapito L, et al.Reconstructing pascal voc[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2014:41-48.
                                    </a>
                                </li>
                                <li id="176">


                                    <a id="bibliography_17" title=" Kar A, Tulsiani S, Carreira J, et al.Category-specific object reconstruction from a single image[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015:1966-1974." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Category-specific object reconstruction from a single image">
                                        <b>[17]</b>
                                         Kar A, Tulsiani S, Carreira J, et al.Category-specific object reconstruction from a single image[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015:1966-1974.
                                    </a>
                                </li>
                                <li id="178">


                                    <a id="bibliography_18" title=" Pentland A.Shape information from shading:a theory about human perception[J].Spatial vision, 1989, 4 (2) :165-182." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Shape information from shading: a theory about human perception">
                                        <b>[18]</b>
                                         Pentland A.Shape information from shading:a theory about human perception[J].Spatial vision, 1989, 4 (2) :165-182.
                                    </a>
                                </li>
                                <li id="180">


                                    <a id="bibliography_19" title=" Robles-Kelly A, Hancock E R.A graph-spectral approach to shape-from-shading[J].IEEE Transactions on Image Processing, 2004, 13 (7) :912-926." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A graph-spectral approach to shape-from-shading">
                                        <b>[19]</b>
                                         Robles-Kelly A, Hancock E R.A graph-spectral approach to shape-from-shading[J].IEEE Transactions on Image Processing, 2004, 13 (7) :912-926.
                                    </a>
                                </li>
                                <li id="182">


                                    <a id="bibliography_20" title=" Aloimonos J.Shape from texture[J].Biological cybernetics, 1988, 58 (5) :345-360." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00003238822&amp;v=MTM3MTJiT2tOWTNrNXpCZGg0ajk5U1hxUnJ4b3hjTUg3UjdxZForWnVGaXZsVUx6UEkxOD1OajdCYXJPNEh0SFByWXhO&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[20]</b>
                                         Aloimonos J.Shape from texture[J].Biological cybernetics, 1988, 58 (5) :345-360.
                                    </a>
                                </li>
                                <li id="184">


                                    <a id="bibliography_21" title=" Marinos C, Blake A.Shape from texture:The homogeneity hypothesis[C]//Proceedings Third International Conference on Computer Vision, 1990:350-353." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Shape from texture:The homogeneity hypothesis">
                                        <b>[21]</b>
                                         Marinos C, Blake A.Shape from texture:The homogeneity hypothesis[C]//Proceedings Third International Conference on Computer Vision, 1990:350-353.
                                    </a>
                                </li>
                                <li id="186">


                                    <a id="bibliography_22" title=" Choy C B, Xu D, Gwak J, et al.3d-r2n2:A unified approach for single and multi-view 3d object reconstruction[C]//European conference on computer vision, 2016:628-644." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=3d-r2n2 A unified approach for single and multi-view 3d object reconstruction">
                                        <b>[22]</b>
                                         Choy C B, Xu D, Gwak J, et al.3d-r2n2:A unified approach for single and multi-view 3d object reconstruction[C]//European conference on computer vision, 2016:628-644.
                                    </a>
                                </li>
                                <li id="188">


                                    <a id="bibliography_23" title=" Fan H, Su H, Guibas L J.A point set generation network for 3d object reconstruction from a single image[C]//Proceedings of the IEEE conference on computer vision and pattern recognition, 2017:605-613." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A point set generation network for 3Dobject reconstruction from a single image">
                                        <b>[23]</b>
                                         Fan H, Su H, Guibas L J.A point set generation network for 3d object reconstruction from a single image[C]//Proceedings of the IEEE conference on computer vision and pattern recognition, 2017:605-613.
                                    </a>
                                </li>
                                <li id="190">


                                    <a id="bibliography_24" title=" Lin C-H, Kong C, Lucey S.Learning efficient point cloud generation for dense 3D object reconstruction[C]//Thirty-Second AAAI Conference on Artificial Intelligence, 2018." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning efficient point cloud generation for dense 3D object reconstruction">
                                        <b>[24]</b>
                                         Lin C-H, Kong C, Lucey S.Learning efficient point cloud generation for dense 3D object reconstruction[C]//Thirty-Second AAAI Conference on Artificial Intelligence, 2018.
                                    </a>
                                </li>
                                <li id="192">


                                    <a id="bibliography_25" title=" Mandikal P, Murthy N, Agarwal M, et al.3D-LMNet:Latent Embedding Matching for Accurate and Diverse 3D Point Cloud Reconstruction from a Single Image[EB].arXiv preprint arXiv:1807.07796, 2018." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=3D-LMNet:Latent Embedding Matching for Accurate and Diverse 3D Point Cloud Reconstruction from a Single Image[EB]">
                                        <b>[25]</b>
                                         Mandikal P, Murthy N, Agarwal M, et al.3D-LMNet:Latent Embedding Matching for Accurate and Diverse 3D Point Cloud Reconstruction from a Single Image[EB].arXiv preprint arXiv:1807.07796, 2018.
                                    </a>
                                </li>
                                <li id="194">


                                    <a id="bibliography_26" title=" Kato H, Ushiku Y, Harada T.Neural 3d mesh renderer[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018:3907-3916." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Neural 3D mesh renderer">
                                        <b>[26]</b>
                                         Kato H, Ushiku Y, Harada T.Neural 3d mesh renderer[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018:3907-3916.
                                    </a>
                                </li>
                                <li id="196">


                                    <a id="bibliography_27" title=" Pontes J K, Kong C, Sridharan S, et al.Image2Mesh:A Learning Framework for Single Image 3D Reconstruction[EB].arXiv preprint arXiv:1711.10669, 2017." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image2Mesh:A Learning Framework for Single Image 3D Reconstruction[EB]">
                                        <b>[27]</b>
                                         Pontes J K, Kong C, Sridharan S, et al.Image2Mesh:A Learning Framework for Single Image 3D Reconstruction[EB].arXiv preprint arXiv:1711.10669, 2017.
                                    </a>
                                </li>
                                <li id="198">


                                    <a id="bibliography_28" title=" Wang N, Zhang Y, Li Z, et al.Pixel2Mesh:Generating 3D Mesh Models from Single RGB Images[EB].arXiv preprint arXiv:1804.01654, 2018." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Pixel2Mesh:Generating 3D Mesh Models from Single RGB Images[EB]">
                                        <b>[28]</b>
                                         Wang N, Zhang Y, Li Z, et al.Pixel2Mesh:Generating 3D Mesh Models from Single RGB Images[EB].arXiv preprint arXiv:1804.01654, 2018.
                                    </a>
                                </li>
                                <li id="200">


                                    <a id="bibliography_29" title=" Szegedy C, Liu W, Jia Y, et al.Going deeper with convolutions[C]//Proceedings of the IEEE conference on computer vision and pattern recognition, 2015:1-9." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Going deeper with convolutions">
                                        <b>[29]</b>
                                         Szegedy C, Liu W, Jia Y, et al.Going deeper with convolutions[C]//Proceedings of the IEEE conference on computer vision and pattern recognition, 2015:1-9.
                                    </a>
                                </li>
                                <li id="202">


                                    <a id="bibliography_30" title=" Szegedy C, Ioffe S, Vanhoucke V, et al.Inception-v4, inception-resnet and the impact of residual connections on learning[EB].arXiv:1602.07261, 2016." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Inception-v4,inception-resnet and the impact of residual connections on learning[EB]">
                                        <b>[30]</b>
                                         Szegedy C, Ioffe S, Vanhoucke V, et al.Inception-v4, inception-resnet and the impact of residual connections on learning[EB].arXiv:1602.07261, 2016.
                                    </a>
                                </li>
                                <li id="204">


                                    <a id="bibliography_31" title=" Krizhevsky A, Sutskever I, Hinton G E.Imagenet classification with deep convolutional neural networks[C]//Advances in neural information processing systems.2012:1097-1105." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=ImageNet Classification with Deep Convolutional Neural Networks">
                                        <b>[31]</b>
                                         Krizhevsky A, Sutskever I, Hinton G E.Imagenet classification with deep convolutional neural networks[C]//Advances in neural information processing systems.2012:1097-1105.
                                    </a>
                                </li>
                                <li id="206">


                                    <a id="bibliography_32" title=" He K, Zhang X, Ren S, et al.Deep residual learning for image recognition[C]//Proceedings of the IEEE conference on computer vision and pattern recognition.2016:770-778." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep residual learning for image recognition">
                                        <b>[32]</b>
                                         He K, Zhang X, Ren S, et al.Deep residual learning for image recognition[C]//Proceedings of the IEEE conference on computer vision and pattern recognition.2016:770-778.
                                    </a>
                                </li>
                                <li id="208">


                                    <a id="bibliography_33" title=" Huang G, Liu Z, Van Der Maaten L, et al.Densely connected convolutional networks[C]//Proceedings of the IEEE conference on computer vision and pattern recognition.2017:4700-4708." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Densely connected convolutional networks">
                                        <b>[33]</b>
                                         Huang G, Liu Z, Van Der Maaten L, et al.Densely connected convolutional networks[C]//Proceedings of the IEEE conference on computer vision and pattern recognition.2017:4700-4708.
                                    </a>
                                </li>
                                <li id="210">


                                    <a id="bibliography_34" title=" Chang A X, Funkhouser T, Guibas L, et al.Shapenet:An information-rich 3d model repository[EB].arXiv preprint arXiv:1512.03012, 2015." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Shapenet:An information-rich 3d model repository[EB]">
                                        <b>[34]</b>
                                         Chang A X, Funkhouser T, Guibas L, et al.Shapenet:An information-rich 3d model repository[EB].arXiv preprint arXiv:1512.03012, 2015.
                                    </a>
                                </li>
                                <li id="212">


                                    <a id="bibliography_35" title=" Kingma D P, Ba J.Adam:A method for stochastic optimization[EB].arXiv preprint arXiv:1412.6980, 2014." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Adam:A method for stochastic optimization[EB]">
                                        <b>[35]</b>
                                         Kingma D P, Ba J.Adam:A method for stochastic optimization[EB].arXiv preprint arXiv:1412.6980, 2014.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JYRJ" target="_blank">计算机应用与软件</a>
                2019,36(06),190-195 DOI:10.3969/j.issn.1000-386x.2019.06.036            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于改进卷积神经网络的单幅图像物体重建方法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BC%A0%E7%8E%89%E9%BA%92&amp;code=41549952&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">张玉麒</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%99%88%E5%8A%A0&amp;code=37804772&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">陈加</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%8F%B6%E7%AB%8B%E5%BF%97&amp;code=41979467&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">叶立志</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%94%B0%E5%85%83&amp;code=29767665&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">田元</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%A4%8F%E4%B8%B9&amp;code=07647051&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">夏丹</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%99%88%E4%BA%9A%E6%9D%BE&amp;code=41979468&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">陈亚松</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%8D%8E%E4%B8%AD%E5%B8%88%E8%8C%83%E5%A4%A7%E5%AD%A6%E6%95%99%E8%82%B2%E4%BF%A1%E6%81%AF%E6%8A%80%E6%9C%AF%E5%AD%A6%E9%99%A2&amp;code=0200298&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">华中师范大学教育信息技术学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>为了提高基于图像的三维重建的重建效果, 基于深度学习的方法已经成为近年来研究的重点。针对目前存在的方法中特征提取效果差、重建细节缺失且计算量巨大的问题, 提出一种改进卷积神经网络的单个物体重建方法。通过加入改进的Inception-resnet模块来提升网络的特征提取能力, 采用多种网络结构提取多特征, 通过多特征依次输入3D-LSTM模块中以增强单幅图像的重建效果。实验结果表明, 该方法不仅能够得到更好的重建效果, 重建出更多的细节, 同时在训练中花费更少的时间。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">卷积神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">三维重建;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8D%95%E5%B9%85%E5%9B%BE%E5%83%8F&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">单幅图像;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">计算机视觉;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    张玉麒, 硕士生, 主研领域:深度学习, 三维重建。;
                                </span>
                                <span>
                                    陈加, 讲师。;
                                </span>
                                <span>
                                    叶立志, 工程师。;
                                </span>
                                <span>
                                    田元, 讲师。;
                                </span>
                                <span>
                                    夏丹, 讲师。;
                                </span>
                                <span>
                                    陈亚松, 硕士生。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-02-26</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金项目 (61605054);</span>
                                <span>湖北省自然科学基金项目 (2014CFB659);</span>
                                <span>华中师范大学中央高校基本科研业务费项目 (CCNU19QD007, CCNU19TD007);</span>
                    </p>
            </div>
                    <h1><b>SINGLE IMAGE OBJECT RECONSTRUCTION METHOD BASED ON IMPROVED CONVOLUTIONAL NEURAL NETWORK</b></h1>
                    <h2>
                    <span>Zhang Yuqi</span>
                    <span>Chen Jia</span>
                    <span>Ye Lizhi</span>
                    <span>Tian Yuan</span>
                    <span>Xia Dan</span>
                    <span>Chen Yasong</span>
            </h2>
                    <h2>
                    <span>College of Educational Information Technology, Central China Normal University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>To improve the performance of 3 D reconstruction, methods based on deep learning have been the main topic of the research. Aiming at the problems of poor feature extraction effect, lack of reconstruction detailsand huge computational load, we proposed a 3 D objects reconstruction method from a single image based on improved convolutional neural network. The feature extraction capability of the network was improved by adding modules that combine residual connectionsand Inception. We used multi-features extracted by multi-network structure, and input it into 3 D-LSTM module in turn to enhance the reconstruction effect of a single image. The experimental results show that our method can not only perform better in reconstruction, but also spend less time in training.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Convolutional%20neural%20network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Convolutional neural network;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=3D%20reconstruction&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">3D reconstruction;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Single%20image&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Single image;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Computer%20vision&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Computer vision;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2019-02-26</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="73" name="73" class="anchor-tag"><b>0 引 言</b></h3>
                <div class="p1">
                    <p id="74">从二维图像中获得三维信息并恢复三维模型是计算机视觉研究的主要方向之一。基于图像的三维重建在诸如商品展示、文物三维修复、智慧城市建设、医疗器官重建等领域也具有重要的应用价值<citation id="215" type="reference"><link href="144" rel="bibliography" /><link href="146" rel="bibliography" /><link href="148" rel="bibliography" /><link href="150" rel="bibliography" /><link href="152" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>,<a class="sup">3</a>,<a class="sup">4</a>,<a class="sup">5</a>]</sup></citation>。基于图像的三维重建问题按照重建图像的数量可以分为基于多幅图像的三维重建方法<citation id="216" type="reference"><link href="154" rel="bibliography" /><link href="156" rel="bibliography" /><link href="158" rel="bibliography" /><link href="160" rel="bibliography" /><sup>[<a class="sup">6</a>,<a class="sup">7</a>,<a class="sup">8</a>,<a class="sup">9</a>]</sup></citation>和基于单幅图像的重建方法, 按照重建的内容可以分为对物体的重建和对场景的重建<citation id="214" type="reference"><link href="162" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>。本文主要针对基于单幅图像的物体重建方法展开研究。</p>
                </div>
                <div class="p1">
                    <p id="75">传统方法按照重建方法的不同可以分为基于模型的重建方法和基于几何外形恢复的重建方法。基于模型重建的方法通过匹配输入图像和模型, 找到模型的最佳参数来进行重建。基于CAD模型的方法<citation id="220" type="reference"><link href="164" rel="bibliography" /><link href="166" rel="bibliography" /><sup>[<a class="sup">11</a>,<a class="sup">12</a>]</sup></citation>, 能够大致地展现物体的近似外形, 在找到一组对应点后, 可以很好地确定相应实例的视点, 但是其获得的模型和实际的模型仍然有很大偏差。形变模型 (Morphable model) 常用来重建人脸部分<citation id="221" type="reference"><link href="168" rel="bibliography" /><link href="170" rel="bibliography" /><sup>[<a class="sup">13</a>,<a class="sup">14</a>]</sup></citation>, 其主要为一些可以变形的线性组合模型, 通常由一些三维扫描的设备获取。Cashman等<citation id="217" type="reference"><link href="172" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>找到了降低三维数据获取门槛的方式, 通过三维模型和辅助的二维信息学习到相应的形变模型来重建物体。Vicente等<citation id="218" type="reference"><link href="174" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>先在图像数据集中匹配一张和输入图像相同类型相似视角的图像, 结合了可视外壳的方法, 但是其无法应用于真实拍摄的图像。Kar等<citation id="219" type="reference"><link href="176" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>在Vicente等的工作基础上, 通过物体的二维注释来学习相应的形变模型, 不但可以重建真实拍摄的图像, 在重建效果上也有了一定的提升。基于模型的方法因为存在设计好的模型, 对固定类别的物体能够取得相对较好的重建效果, 但是无法具有较好的泛化性。基于几何外形恢复的方法主要包括从阴影中恢复外形 (Shape from shading) <citation id="222" type="reference"><link href="178" rel="bibliography" /><link href="180" rel="bibliography" /><sup>[<a class="sup">18</a>,<a class="sup">19</a>]</sup></citation>和从纹理中恢复外形 (Shape from texture) <citation id="223" type="reference"><link href="182" rel="bibliography" /><link href="184" rel="bibliography" /><sup>[<a class="sup">20</a>,<a class="sup">21</a>]</sup></citation>, 基于几何外形恢复的方法能重建较多种类的物体, 但其往往对灰度和光照等要求较高, 对真实场景的物体图片的重建效果欠佳。</p>
                </div>
                <div class="p1">
                    <p id="76">近年来, 随着深度学习技术的不断发展, 出现了基于深度学习的物体重建方法。Choy等<citation id="224" type="reference"><link href="186" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>提出了一套端到端的基于体素的重建网络结构 (3D recurrent reconstruction neural network, 3D-R2N2) , 与传统方法通过匹配一个最近似的模型并不断优化其参数得到最佳模型不同, 3D-R2N2利用大量的训练去学习二维图像和三维模型之间的映射, 其无需对图像加入分类标签。在以IoU (Intersection-over-Union) 值作为评价指标的对比实验中, 3D-R2N2的重建效果优于效果最好的传统方法。但其在重建精度上仍无法达到可以使用的程度, 且部分重建细节如桌腿等会存在缺失的问题, 相较于二维领域也存在着计算量大、训练时间长的缺点。此外也出现了基于深度学习的点云重建方法<citation id="225" type="reference"><link href="188" rel="bibliography" /><link href="190" rel="bibliography" /><link href="192" rel="bibliography" /><sup>[<a class="sup">23</a>,<a class="sup">24</a>,<a class="sup">25</a>]</sup></citation>和网格重建方法<citation id="226" type="reference"><link href="194" rel="bibliography" /><link href="196" rel="bibliography" /><link href="198" rel="bibliography" /><sup>[<a class="sup">26</a>,<a class="sup">27</a>,<a class="sup">28</a>]</sup></citation>, 与传统方法相比也取得了较好的重建效果。</p>
                </div>
                <div class="p1">
                    <p id="77">本文采用与3D-R2N2相似的网络结构来做基于体素的单幅图像物体重建, 并针对相应模块作出改进。</p>
                </div>
                <div class="p1">
                    <p id="78">本文方法具有如下特点:</p>
                </div>
                <div class="p1">
                    <p id="79"> (1) 针对Encoder模块进行了改进。本文的Encoder模块使用改进的inception-resnet网络结构, 并使用全局平均池化来代替全连接层, 与3D-R2N2的重建结果相比能够得到更高的IoU值, 重建精度更高。</p>
                </div>
                <div class="p1">
                    <p id="80"> (2) 针对单幅图像信息量少的问题, 采用多种网络结构提取多特征并依次输入3D-LSTM模块中, 从而增强单幅图像的重建效果。</p>
                </div>
                <div class="p1">
                    <p id="81"> (3) 与3D-R2N2相比, 本文提出的方法具有更低的计算开销。</p>
                </div>
                <h3 id="82" name="82" class="anchor-tag"><b>1 3D-R2N2网络模型</b></h3>
                <div class="p1">
                    <p id="83">3D-R2N2网络采用监督学习的方式, 仅在边界框信息 (Bounding boxes) 的辅助下, 就可以端到端地从单幅或者多幅图像中重建出图像中物体的三维模型。其网络结构由三部分组成:卷积神经网络构成的编码器 (Encoder) 、三维卷积长短期记忆网络 (3D convolutional LSTM) 和解码器 (Decoder) 。</p>
                </div>
                <h4 class="anchor-tag" id="84" name="84"><b>1.1 Encoder模块</b></h4>
                <div class="p1">
                    <p id="85">Encoder模块将输入的图像转化为较低纬度的特征向量。其网络结构包含了12个卷积层, 1个全连接层和5条残余连接 (Residual connection) 。其中残余连接是在每两个卷积层间添加, 为了匹配卷积操作后的通道数, 残余连接使用1×1卷积。Encoder模块最终输出一个维度为1 024的特征向量。</p>
                </div>
                <h4 class="anchor-tag" id="86" name="86"><b>1.2 3D convolutional LSTM模块</b></h4>
                <div class="p1">
                    <p id="87">3D convolutional LSTM模块通过Encoder模块得到的特征向量去更新其中记忆单元的信息。该模块由<i>n</i>×<i>n</i>×<i>n</i>个3D-LSTM单元组成, 其中<i>n</i>为3D-LSTM网格的空间分辨率。3D-LSTM单元在空间上分布在3D网格结构中, 每个单元负责重建特定部分的最终输出, 其都对应最终重建输出矩阵的一个元素 (<i>i</i>, <i>j</i>, <i>k</i>) , 单元中还伴随着一个独立的隐藏状态<i>h</i><sub><i>t</i></sub>。</p>
                </div>
                <h4 class="anchor-tag" id="88" name="88"><b>1.3 Decoder模块</b></h4>
                <div class="p1">
                    <p id="89">Decoder模块接收3D-LSTM传来的隐藏状态<i>h</i><sub><i>t</i></sub>, 并将它们转化为体素块存在与否的概率, 最终通过得到每个体素块概率的值为0或者1来确认是否重建该体素块。与Encoder模块相同, Decoder模块也由卷积神经网络和相应的残余连接组成, 但这里使用了3×3×3的卷积核进行三维卷积, 同时在Decoder模块会进行三维上池化 (3D unpooling) 来对三维信息进行处理。</p>
                </div>
                <h3 id="90" name="90" class="anchor-tag"><b>2 改进神经网络的</b><b>单幅图像物体重建方法</b></h3>
                <h4 class="anchor-tag" id="91" name="91"><b>2.1 对Encoder模块的优化</b></h4>
                <div class="p1">
                    <p id="92">尽管3D-R2N2的Encoder模块使用了带有残余连接的卷积网络, 但仍存在着特征提取效果差导致的重建精度问题。GooleNet<citation id="227" type="reference"><link href="200" rel="bibliography" /><sup>[<a class="sup">29</a>]</sup></citation>首次提出的Inception模块可以一次性使用多个不同尺寸的卷积核, 能够让网络自己选择需要的特征, 如选择大尺寸的卷积核可以抓取到像素与周边像素之间的关系。同时, Inception模块增大了网络宽度, 能够对有噪声的图片提取到更多的深度特征, 让含噪声的图片具有更好的泛化能力。受到Szegedy等<citation id="228" type="reference"><link href="202" rel="bibliography" /><sup>[<a class="sup">30</a>]</sup></citation>提出的方法的启发, 本文在保存残余连接的同时加入了Inception模块, 既能够利用Resnet中的残余连接解决梯度消失的问题, 又能够利用Inception模块提取更深更细节的图像特征。本文设计了两种改进的Inception-resnet模块, 其中Inception-resnet-A结构如图1所示。</p>
                </div>
                <div class="area_img" id="93">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201906037_093.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 Inception-resnet-A的结构" src="Detail/GetImg?filename=images/JYRJ201906037_093.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 Inception-resnet-A的结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201906037_093.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="94">Inception-resnet-A模块包括3个卷积分支, 在第一个卷积分支中将3×3卷积替换为1×3和3×1两种大小的卷积核, 这种非对称卷积核能够替代3×3卷积核的同时节省计算量, 并提取到更多图像的局部特征。放在3×3卷积核前的1×1卷积用于控制输入特征的通道数。第三个卷积分支使用池化来增加整体的非线性特征。同时模块仍保留着残余连接, 整体模块使用较小的卷积核, 但能够较好地提取局部特征。</p>
                </div>
                <div class="p1">
                    <p id="95">Inception-resnet-B模块采用了更大的卷积核尺寸, 5×1和1×5两个卷积核由5×5的卷积核拆分得到, 中间分支仅采用了1×1特征用来对不同通道的特征进行统一。该结构能够更好地抓取像素与像素之间的关系。其结构如图2所示。</p>
                </div>
                <div class="area_img" id="96">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201906037_096.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 Inception-resnet-B的结构" src="Detail/GetImg?filename=images/JYRJ201906037_096.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 Inception-resnet-B的结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201906037_096.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="97">3D-R2N2的Encoder模块中的全连接层所含参数量较大, 极大地增加了训练的计算量, 占用了较多的时间。本文利用全局平均池化来替代全连接层, 全局平均池化不但减少了参数的数量, 同时也对整个模块起到正则化的作用。其主要思想是将卷积层的每个输出的特征图的值统一为一个值, 建立起了特征图与类之间的对应关系。假设最终的特征图大小为<i>a</i>×<i>b</i>, 第<i>k</i>张特征图的值为<i>x</i><mathml id="98"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mrow><mo stretchy="false"> (</mo><mi>k</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>, 则该特征图对应的输出为:</p>
                </div>
                <div class="p1">
                    <p id="99"><mathml id="100"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>y</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>k</mi><mo stretchy="false">) </mo></mrow></msup><mo>=</mo><mfrac><mn>1</mn><mrow><mi>a</mi><mo>×</mo><mi>b</mi></mrow></mfrac><mrow><mo>[</mo><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow><mi>a</mi></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>0</mn></mrow><mi>b</mi></munderover><mi>x</mi></mstyle></mrow></mstyle><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mrow><mo stretchy="false"> (</mo><mi>k</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow><mo>]</mo></mrow></mrow></math></mathml>      (1) </p>
                </div>
                <div class="p1">
                    <p id="101">改进后的Encoder模块的网络结构如图3所示。与3D-R2N2相比, 网络在层数上没有增加, 网络的前四层没有变化, 仍使用带有残余连接的卷积神经网络。但是从第五层开始使用了改进的Inception-resnet模块替换了单纯带有残余连接的卷积网络, 每个Inception的分支后连接有1×1的线性卷积核, 其主要用来统一跳跃连接后的通道维度。输入的图片经过Inception-resnet模块后, 再经过全局平均池化的处理, 最终输出一个维度为1 024的特征向量。</p>
                </div>
                <div class="area_img" id="102">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201906037_102.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 改进的Encoder模块网络结构" src="Detail/GetImg?filename=images/JYRJ201906037_102.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 改进的Encoder模块网络结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201906037_102.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="103" name="103"><b>2.2 多特征重建网络</b></h4>
                <div class="p1">
                    <p id="104">使用不同的Encoder网络结构会得到不同的重建结果, 原因在于不同的网络结构的特征提取能力不同, 某一种网络结构在特定的特征上表现出较好的能力, 但可能在其他特征上表现较差。单幅图像本身就包含较少的信息, 如果只使用一种网络结构用作特征提取, 则会限制重建的性能。</p>
                </div>
                <div class="p1">
                    <p id="105">本文在Encoder模块中使用多种网络结构进行特征提取, 采用的网络结构有AlexNet<citation id="229" type="reference"><link href="204" rel="bibliography" /><sup>[<a class="sup">31</a>]</sup></citation>、ResNet<citation id="230" type="reference"><link href="206" rel="bibliography" /><sup>[<a class="sup">32</a>]</sup></citation>、DenseNet<citation id="231" type="reference"><link href="208" rel="bibliography" /><sup>[<a class="sup">33</a>]</sup></citation>以及本文中改进的Inception-resnet, 几种网络结构均为现有方法中具有开创性且效果较好的网络结构。在本文提出的多特征重建网络框架中, 四种网络结构对同一张图像提取到四种不同的特征, 所提取的四种特征分别依次输入3D-LSTM中并进行整合, 从而达到模拟多幅图像的重建的过程, 进而增强单幅图像的重建效果。多特征重建网络的整体结构如图4所示。</p>
                </div>
                <div class="area_img" id="106">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201906037_106.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 多特征重建网络结构框架" src="Detail/GetImg?filename=images/JYRJ201906037_106.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 多特征重建网络结构框架  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201906037_106.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h3 id="107" name="107" class="anchor-tag"><b>3 实验与结果分析</b></h3>
                <div class="p1">
                    <p id="108">本文使用Theano框架来实现相应的网络模型。实验在配置有 Intel Xeon E3 1230 V5 CPU (3.40 Ghz) , Nvidia GeForce 1080 Ti GPU (11 GB) 的硬件平台上进行。</p>
                </div>
                <h4 class="anchor-tag" id="109" name="109"><b>3.1 实验数据集与评价标准</b></h4>
                <div class="p1">
                    <p id="110">本文采用ShapeNet数据集<citation id="232" type="reference"><link href="210" rel="bibliography" /><sup>[<a class="sup">34</a>]</sup></citation>作为网络训练和测试所使用的数据集。该数据集包括了13个种类共43 783个CAD模型, 本文采用数据集中的37 192个CAD模型作为训练集, 6 591个剩余的CAD模型作为测试集。</p>
                </div>
                <div class="p1">
                    <p id="111">本文采用IoU值, 也即网络输出结果与对应真实模型的交叠率作为评价三维重建精度的评判标准。</p>
                </div>
                <h4 class="anchor-tag" id="112" name="112"><b>3.2 网络模型及训练</b></h4>
                <div class="p1">
                    <p id="113">网络输入的图像大小为127×127, 输出体素大小为32×32×32, 在网络模型中使用了Adam算法<citation id="233" type="reference"><link href="212" rel="bibliography" /><sup>[<a class="sup">35</a>]</sup></citation>来优化梯度下降过程, 加速网络的训练速度, 其中的超参数<i>β</i><sub>1</sub>值为0.9, <i>β</i><sub>2</sub>值为0.999, 权重衰减值为5×10<sup>-6</sup>。LeakyReLU中斜率设置为0.1, 初始的学习率设置为了10<sup>-5</sup>。为了实现实验的公平对照, 本文网络的初始超参数值采用了和3D-R2N2一样的设置。多特征重建网络的四种网络结构均采用与3D-R2N2相同的12层。</p>
                </div>
                <h4 class="anchor-tag" id="114" name="114"><b>3.3 ShapeNet数据集实验结果分析</b></h4>
                <div class="p1">
                    <p id="115">本文针对ShapeNet数据集中13个种类的物体分别进行了实验, 并且针对Encoder模块的改动以及多特征重建网络分别做了单一的对照实验, 仅使用改进后的Inception-resnet模块的网络称为New encoder 3D-R2N2, 使用多种网络结构作为Encoder的多特征网络结构称为Multi-feature 3D-R2N2。表1显示New encoder 3D-R2N2在13个主要类别中有7类的重建效果优于3D-R2N2, Multi-feature 3D-R2N2在13个主要类别中有10类优于3D-R2N2, 且两者整体平均重建表现均优于3D-R2N2, 该结果证明了本文方法的有效性。实验分析可知, 引入改进后的Inception-resnet结构的Encoder模块能够更好地提取图像的细部特征及深度特征, 在面对灯、电话等重建细节较多的物体时有相对较大的效果提升。但在面对诸如桌子、橱柜等结构相对简单的物体, 其表现反而不如3D-R2N2, 这也印证了本文提到的使用不同的网络结构提取特征, 会对不同的特征产生不同的表现。而使用了多种网络结构来提取多特征的多特征重建网络, 对重建效果的提升则是全面的, 仅在飞机、灯和步枪三类上IoU值与3D-R2N2近乎持平, 其余10类均优于3D-R2N2。本文将多特征重建网络的重建模型与3D-R2N2进行了对比, 其重建模型对比如图5所示。通过模型对比可以看出, 本文的重建结果拥有更多细节, 重建效果更好。</p>
                </div>
                <div class="area_img" id="116">
                    <p class="img_tit"><b>表1 不同重建方法的IoU值对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="116" border="1"><tr><td>种类</td><td>3D-R2N2</td><td>New encoder <br />3D-R2N2</td><td>Multi-feature <br />3D-R2N2</td></tr><tr><td><br />飞机</td><td>0.513</td><td>0.530</td><td>0.510</td></tr><tr><td><br />长凳</td><td>0.421</td><td>0.547</td><td>0.450</td></tr><tr><td><br />橱柜</td><td>0.716</td><td>0.638</td><td>0.740</td></tr><tr><td><br />车</td><td>0.798</td><td>0.695</td><td>0.804</td></tr><tr><td><br />椅子</td><td>0.466</td><td>0.695</td><td>0.497</td></tr><tr><td><br />监视器</td><td>0.468</td><td>0.548</td><td>0.497</td></tr><tr><td><br />灯</td><td>0.381</td><td>0.496</td><td>0.350</td></tr><tr><td><br />扬声器</td><td>0.662</td><td>0.642</td><td>0.687</td></tr><tr><td><br />步枪</td><td>0.544</td><td>0.510</td><td>0.509</td></tr><tr><td><br />沙发</td><td>0.628</td><td>0.642</td><td>0.661</td></tr><tr><td><br />桌子</td><td>0.513</td><td>0.497</td><td>0.512</td></tr><tr><td><br />电话</td><td>0.661</td><td>0.557</td><td>0.669</td></tr><tr><td><br />船舶</td><td>0.513</td><td>0.559</td><td>0.569</td></tr><tr><td><br />均值</td><td>0.560</td><td>0.571</td><td>0.580</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="117">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201906037_117.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 多特征重建网络与3D-R2N2的重建效果对比" src="Detail/GetImg?filename=images/JYRJ201906037_117.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 多特征重建网络与3D-R2N2的重建效果对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201906037_117.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="118" name="118"><b>3.4 改进的Encoder模块对计算开销的提升</b></h4>
                <div class="p1">
                    <p id="119">为了验证改进Inception-resnet后的Encoder模块对计算开销的提升, 本文仍使用ShapeNet数据集对模型的训练时间及训练模型的参数数量进行对比, 其结果如表2所示。</p>
                </div>
                <div class="area_img" id="120">
                    <p class="img_tit"><b>表2 不同重建方法训练时间及参数数量比较</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="120" border="1"><tr><td><br />算法</td><td>训练时间/小时</td><td>参数数量/个</td></tr><tr><td><br />3D-R2N2</td><td>7.5</td><td>1 697 219</td></tr><tr><td><br />New encoder 3D-R2N2</td><td>4.5</td><td>723 121</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="121">其中, 由于全局平均池化取代了全连接层大量的参数, 使得参数数量减掉了60%, 同时参数的减少也加快了训练的时长, Inception-resnet模块中加入的非对称卷积核也能省去该模块近1/3的训练时间, 最终改进的网络结构在训练时间上也有了较大的改善。实验结果也证明了本文所提方法对计算开销的提升。</p>
                </div>
                <h3 id="122" name="122" class="anchor-tag"><b>4 结 语</b></h3>
                <div class="p1">
                    <p id="123">本文采用类似于3D-R2N2的网络结构, 通过使用改进的Encoder模块及结合多特征的重建网络结构, 进一步提高了特征的表达能力和网络的优化能力。通过实验对重建的输出模型的IoU值计算, 得到的结果证明了本文方法在重建精度上和计算开销上的提升。下一步的工作是添加一些人为设计的结构信息和几何信息作为网络的辅助信息, 从而提高该类物体的重建精度。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="144">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=3D Face Reconstruction from a Single Image Using a Single Reference Face Shape">

                                <b>[1]</b> Kemelmacher-Shlizerman I, Basri R.3D face reconstruction from a single image using a single reference face shape[J].IEEE transactions on pattern analysis and machine intelligence, 2011, 33 (2) :394-405.
                            </a>
                        </p>
                        <p id="146">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011300711267&amp;v=MjY1NjJxUVRNbndaZVp0RmlubFVyeklJMTBUYVJNPU5pZk9mYks3SHRETnJJOUZZK29PRG5vK29CTVQ2VDRQUUgvaXJSZEdlcg==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> Baka N, Kaptein B L, De Bruijne M, et al.2D-3D shape reconstruction of the distal femur from stereo X-ray imaging using statistical shape models[J].Medical image analysis, 2011, 15 (6) :840-850.
                            </a>
                        </p>
                        <p id="148">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00003673695&amp;v=MjQzNzh6UEkxOD1OajdCYXJPNEh0SFBxWWhHWXVJS1kzazV6QmRoNGo5OVNYcVJyeG94Y01IN1I3cWRaK1p1Rml2bFVM&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> Dworzak J, Lamecker H, Von Berg J, et al.3D reconstruction of the human rib cage from 2D projection images using a statistical shape model[J].International journal of computer assisted radiology and surgery, 2010, 5 (2) :111-124.
                            </a>
                        </p>
                        <p id="150">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201904002&amp;v=MTM3MjRSN3FmWnVadEZ5dm1WTHpKS0NMZlliRzRIOWpNcTQ5RlpvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1U=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b> 陈加, 张玉麒, 宋鹏, 等.深度学习在基于单幅图像的物体三维重建中的应用[J].自动化学报, 2019, 45 (4) :657-668.
                            </a>
                        </p>
                        <p id="152">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJF201203011&amp;v=MDE4NjZHNEg5UE1ySTlFWllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVp0Rnl2bVZMekpMejdCYUw=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b> 陈加, 吴晓军.联合LBS和Snake的3D人体外形和运动跟踪方法[J].计算机辅助设计与图形学学报, 2012, 24 (3) :357-363.
                            </a>
                        </p>
                        <p id="154">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Human Body Shape and Motion Tracking by Hierarchical Weighted ICP">

                                <b>[6]</b> Chen J, Wu X, Wang M Y, et al.Human Body Shape and Motion Tracking by Hierarchical Weighted ICP[C]//Proceedings of the 7th international conference on Advances in visual computing—Volume Part II.Springer-Verlag, 2011:408-417.
                            </a>
                        </p>
                        <p id="156">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The structure-from-motion reconstruction pipeline -- a survey with focus on short image sequences">

                                <b>[7]</b> Häming K, Peters G.The structure-from-motion reconstruction pipeline—a survey with focus on short image sequences[J].Kybernetika, 2010, 46 (5) :926-937.
                            </a>
                        </p>
                        <p id="158">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Quasi-Dense Approach to Surface Reconstruction from Uncalibrated Images">

                                <b>[8]</b> Lhuillier M, Quan L.A quasi-dense approach to surface reconstruction from uncalibrated images[J].IEEE transactions on pattern analysis and machine intelligence, 2005, 27 (3) :418-433.
                            </a>
                        </p>
                        <p id="160">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning a multi-view stereo machine">

                                <b>[9]</b> Kar A, Häne C, Malik J.Learning a multi-view stereo machine[C]//Advances in neural information processing systems, 2017:365-376.
                            </a>
                        </p>
                        <p id="162">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Review of Geometry Recovery from a Single Image Focusing on Curved Object Reconstruction">

                                <b>[10]</b> Oswald M R, Töppe E, Nieuwenhuis C, et al.A review of geometry recovery from a single image focusing on curved object reconstruction[M]//Innovations for Shape Analysis, Models and Algorithms, Springer, 2013:343-378.
                            </a>
                        </p>
                        <p id="164">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Parsing ikea objects:Fine pose estimation">

                                <b>[11]</b> Lim J J, Pirsiavash H, Torralba A.Parsing ikea objects:Fine pose estimation[C]//Proceedings of the IEEE International Conference on Computer Vision, 2013:2992-2999.
                            </a>
                        </p>
                        <p id="166">
                            <a id="bibliography_12" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD15011900002695&amp;v=MTI4NDhhUk09Tmo3QmFySzlIdEROcG85RlpPc05DblU4b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcnpJSTEwVA==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[12]</b> Satkin S, Rashid M, Lin J, et al.3dnn:3d nearest neighbor[J].International Journal of Computer Vision, 2015, 111 (1) :69-97.
                            </a>
                        </p>
                        <p id="168">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A morphable model for the synthesis of 3D faces">

                                <b>[13]</b> Blanz V, Vetter T.A morphable model for the synthesis of 3D faces[C]//Proceedings of the 26th annual conference on Computer graphics and interactive techniques, 1999:187-194.
                            </a>
                        </p>
                        <p id="170">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Face recognition based on fitting a 3D morphable model">

                                <b>[14]</b> Blanz V, Vetter T.Face recognition based on fitting a 3D morphable model[J].IEEE Transactions on pattern analysis and machine intelligence, 2003, 25 (9) :1063-1074.
                            </a>
                        </p>
                        <p id="172">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=What Shape Are Dolphins? Building 3D Morphable Models from 2D Images">

                                <b>[15]</b> Cashman T J, Fitzgibbon A W.What shape are dolphins?building 3d morphable models from 2d images[J].IEEE transactions on pattern analysis and machine intelligence, 2013, 35 (1) :232-244.
                            </a>
                        </p>
                        <p id="174">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Reconstructing PASCAL VOC">

                                <b>[16]</b> Vicente S, Carreira J, Agapito L, et al.Reconstructing pascal voc[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2014:41-48.
                            </a>
                        </p>
                        <p id="176">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Category-specific object reconstruction from a single image">

                                <b>[17]</b> Kar A, Tulsiani S, Carreira J, et al.Category-specific object reconstruction from a single image[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015:1966-1974.
                            </a>
                        </p>
                        <p id="178">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Shape information from shading: a theory about human perception">

                                <b>[18]</b> Pentland A.Shape information from shading:a theory about human perception[J].Spatial vision, 1989, 4 (2) :165-182.
                            </a>
                        </p>
                        <p id="180">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A graph-spectral approach to shape-from-shading">

                                <b>[19]</b> Robles-Kelly A, Hancock E R.A graph-spectral approach to shape-from-shading[J].IEEE Transactions on Image Processing, 2004, 13 (7) :912-926.
                            </a>
                        </p>
                        <p id="182">
                            <a id="bibliography_20" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00003238822&amp;v=MTM5MTVoNGo5OVNYcVJyeG94Y01IN1I3cWRaK1p1Rml2bFVMelBJMTg9Tmo3QmFyTzRIdEhQcll4TmJPa05ZM2s1ekJk&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[20]</b> Aloimonos J.Shape from texture[J].Biological cybernetics, 1988, 58 (5) :345-360.
                            </a>
                        </p>
                        <p id="184">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Shape from texture:The homogeneity hypothesis">

                                <b>[21]</b> Marinos C, Blake A.Shape from texture:The homogeneity hypothesis[C]//Proceedings Third International Conference on Computer Vision, 1990:350-353.
                            </a>
                        </p>
                        <p id="186">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=3d-r2n2 A unified approach for single and multi-view 3d object reconstruction">

                                <b>[22]</b> Choy C B, Xu D, Gwak J, et al.3d-r2n2:A unified approach for single and multi-view 3d object reconstruction[C]//European conference on computer vision, 2016:628-644.
                            </a>
                        </p>
                        <p id="188">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A point set generation network for 3Dobject reconstruction from a single image">

                                <b>[23]</b> Fan H, Su H, Guibas L J.A point set generation network for 3d object reconstruction from a single image[C]//Proceedings of the IEEE conference on computer vision and pattern recognition, 2017:605-613.
                            </a>
                        </p>
                        <p id="190">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning efficient point cloud generation for dense 3D object reconstruction">

                                <b>[24]</b> Lin C-H, Kong C, Lucey S.Learning efficient point cloud generation for dense 3D object reconstruction[C]//Thirty-Second AAAI Conference on Artificial Intelligence, 2018.
                            </a>
                        </p>
                        <p id="192">
                            <a id="bibliography_25" target="_blank" href="http://scholar.cnki.net/result.aspx?q=3D-LMNet:Latent Embedding Matching for Accurate and Diverse 3D Point Cloud Reconstruction from a Single Image[EB]">

                                <b>[25]</b> Mandikal P, Murthy N, Agarwal M, et al.3D-LMNet:Latent Embedding Matching for Accurate and Diverse 3D Point Cloud Reconstruction from a Single Image[EB].arXiv preprint arXiv:1807.07796, 2018.
                            </a>
                        </p>
                        <p id="194">
                            <a id="bibliography_26" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Neural 3D mesh renderer">

                                <b>[26]</b> Kato H, Ushiku Y, Harada T.Neural 3d mesh renderer[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018:3907-3916.
                            </a>
                        </p>
                        <p id="196">
                            <a id="bibliography_27" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image2Mesh:A Learning Framework for Single Image 3D Reconstruction[EB]">

                                <b>[27]</b> Pontes J K, Kong C, Sridharan S, et al.Image2Mesh:A Learning Framework for Single Image 3D Reconstruction[EB].arXiv preprint arXiv:1711.10669, 2017.
                            </a>
                        </p>
                        <p id="198">
                            <a id="bibliography_28" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Pixel2Mesh:Generating 3D Mesh Models from Single RGB Images[EB]">

                                <b>[28]</b> Wang N, Zhang Y, Li Z, et al.Pixel2Mesh:Generating 3D Mesh Models from Single RGB Images[EB].arXiv preprint arXiv:1804.01654, 2018.
                            </a>
                        </p>
                        <p id="200">
                            <a id="bibliography_29" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Going deeper with convolutions">

                                <b>[29]</b> Szegedy C, Liu W, Jia Y, et al.Going deeper with convolutions[C]//Proceedings of the IEEE conference on computer vision and pattern recognition, 2015:1-9.
                            </a>
                        </p>
                        <p id="202">
                            <a id="bibliography_30" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Inception-v4,inception-resnet and the impact of residual connections on learning[EB]">

                                <b>[30]</b> Szegedy C, Ioffe S, Vanhoucke V, et al.Inception-v4, inception-resnet and the impact of residual connections on learning[EB].arXiv:1602.07261, 2016.
                            </a>
                        </p>
                        <p id="204">
                            <a id="bibliography_31" target="_blank" href="http://scholar.cnki.net/result.aspx?q=ImageNet Classification with Deep Convolutional Neural Networks">

                                <b>[31]</b> Krizhevsky A, Sutskever I, Hinton G E.Imagenet classification with deep convolutional neural networks[C]//Advances in neural information processing systems.2012:1097-1105.
                            </a>
                        </p>
                        <p id="206">
                            <a id="bibliography_32" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep residual learning for image recognition">

                                <b>[32]</b> He K, Zhang X, Ren S, et al.Deep residual learning for image recognition[C]//Proceedings of the IEEE conference on computer vision and pattern recognition.2016:770-778.
                            </a>
                        </p>
                        <p id="208">
                            <a id="bibliography_33" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Densely connected convolutional networks">

                                <b>[33]</b> Huang G, Liu Z, Van Der Maaten L, et al.Densely connected convolutional networks[C]//Proceedings of the IEEE conference on computer vision and pattern recognition.2017:4700-4708.
                            </a>
                        </p>
                        <p id="210">
                            <a id="bibliography_34" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Shapenet:An information-rich 3d model repository[EB]">

                                <b>[34]</b> Chang A X, Funkhouser T, Guibas L, et al.Shapenet:An information-rich 3d model repository[EB].arXiv preprint arXiv:1512.03012, 2015.
                            </a>
                        </p>
                        <p id="212">
                            <a id="bibliography_35" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Adam:A method for stochastic optimization[EB]">

                                <b>[35]</b> Kingma D P, Ba J.Adam:A method for stochastic optimization[EB].arXiv preprint arXiv:1412.6980, 2014.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JYRJ201906037" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201906037&amp;v=MDYyOTF0Rnl2bVZMekpMelRaWkxHNEg5ak1xWTlHWTRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVo=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
