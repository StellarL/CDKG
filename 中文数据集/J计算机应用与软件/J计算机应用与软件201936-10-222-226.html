<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637135565995940000%26DBCODE%3dCJFD%26TABLEName%3dCJFDTEMP%26FileName%3dJYRJ201910039%26RESULT%3d1%26SIGN%3dtZZ4JCDzU5kHhTfRWzb0NtUaJhk%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JYRJ201910039&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JYRJ201910039&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201910039&amp;v=MDEyNTZXN3ZBTHpUWlpMRzRIOWpOcjQ5R2JZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVadEZ5bms=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#27" data-title="&lt;b&gt;0 引 言&lt;/b&gt; "><b>0 引 言</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#33" data-title="&lt;b&gt;1 残差网络和密集网络&lt;/b&gt; "><b>1 残差网络和密集网络</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#41" data-title="&lt;b&gt;2 方法设计&lt;/b&gt; "><b>2 方法设计</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#42" data-title="&lt;b&gt;2.1 整体流程&lt;/b&gt;"><b>2.1 整体流程</b></a></li>
                                                <li><a href="#54" data-title="&lt;b&gt;2.2 残差密集块&lt;/b&gt;"><b>2.2 残差密集块</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#61" data-title="&lt;b&gt;3 实 验&lt;/b&gt; "><b>3 实 验</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#62" data-title="&lt;b&gt;3.1 数据集和评价指标&lt;/b&gt;"><b>3.1 数据集和评价指标</b></a></li>
                                                <li><a href="#74" data-title="&lt;b&gt;3.2 实验结果与分析&lt;/b&gt;"><b>3.2 实验结果与分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#82" data-title="&lt;b&gt;4 结 语&lt;/b&gt; "><b>4 结 语</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#35" data-title="图1 残差块">图1 残差块</a></li>
                                                <li><a href="#44" data-title="图2 残差密集网络结构">图2 残差密集网络结构</a></li>
                                                <li><a href="#56" data-title="图3 残差密集块">图3 残差密集块</a></li>
                                                <li><a href="#76" data-title="&lt;b&gt;表1 本文方法和其他先进的方法在Set5数据集上的比较&lt;/b&gt;"><b>表1 本文方法和其他先进的方法在Set5数据集上的比较</b></a></li>
                                                <li><a href="#77" data-title="&lt;b&gt;表2 本文方法和其他先进的方法在Set14数据集上的比较&lt;/b&gt;"><b>表2 本文方法和其他先进的方法在Set14数据集上的比较</b></a></li>
                                                <li><a href="#78" data-title="&lt;b&gt;表3 本文方法和其他先进的方法在B100数据集上的比较&lt;/b&gt;"><b>表3 本文方法和其他先进的方法在B100数据集上的比较</b></a></li>
                                                <li><a href="#79" data-title="&lt;b&gt;表4 本文方法和其他先进的方法在Urban100数据集上的比较&lt;/b&gt;"><b>表4 本文方法和其他先进的方法在Urban100数据集上的比较</b></a></li>
                                                <li><a href="#81" data-title="图4 超分辨率的定性比较">图4 超分辨率的定性比较</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" title=" Dong C,Loy C C,He K,et al.Image super-resolution using deep convolutional networks[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2014,38(2):295-307." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image Super-Resolution Using Deep Convolutional Networks">
                                        <b>[1]</b>
                                         Dong C,Loy C C,He K,et al.Image super-resolution using deep convolutional networks[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2014,38(2):295-307.
                                    </a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" title=" 胡长胜,詹曙,吴从中.基于深度特征学习的图像超分辨率重建[J].自动化学报,2017,43(5):814-821." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201705013&amp;v=MDU4Nzh2QUtDTGZZYkc0SDliTXFvOUVaNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnRGeW5rVzc=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         胡长胜,詹曙,吴从中.基于深度特征学习的图像超分辨率重建[J].自动化学报,2017,43(5):814-821.
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title=" 孙超,吕俊伟,李健伟,等.基于去卷积的快速图像超分辨率方法[J].光学学报,2017(12):142-152." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201712017&amp;v=MjQ0MTN5bmtXN3ZBSWpYVGJMRzRIOWJOclk5RVk0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVadEY=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         孙超,吕俊伟,李健伟,等.基于去卷积的快速图像超分辨率方法[J].光学学报,2017(12):142-152.
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" title=" Ledig C,Theis L,Huszar F,et al.Photo-realistic single image super-resolution using a generative adversarial network[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition(CVPR).IEEE Computer Society,2017." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Photo-Realistic Single Image Super-Resolution U sing a Generative Adversarial Network">
                                        <b>[4]</b>
                                         Ledig C,Theis L,Huszar F,et al.Photo-realistic single image super-resolution using a generative adversarial network[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition(CVPR).IEEE Computer Society,2017.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" title=" Lim B,Son S,Kim H,et al.Enhanced deep residual networks for single image super-resolution[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops(CVPRW).IEEE Computer Society,2017." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Enhanced Deep Residual Networks for Single Image Super-Resolution">
                                        <b>[5]</b>
                                         Lim B,Son S,Kim H,et al.Enhanced deep residual networks for single image super-resolution[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops(CVPRW).IEEE Computer Society,2017.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" title=" Tai Y,Yang J,Liu X.Image super-resolution via deep recursive residual network[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition(CVPR).IEEE Computer Society,2017." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image super-resolution via deep recursive residual network">
                                        <b>[6]</b>
                                         Tai Y,Yang J,Liu X.Image super-resolution via deep recursive residual network[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition(CVPR).IEEE Computer Society,2017.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" title=" 王一宁,秦品乐,李传朋,等.基于残差神经网络的图像超分辨率改进算法[J].计算机应用,2018(1):246-254." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201801044&amp;v=MDcwNzN5bmtXN3ZBTHo3QmQ3RzRIOW5Ncm85QllJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVadEY=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                         王一宁,秦品乐,李传朋,等.基于残差神经网络的图像超分辨率改进算法[J].计算机应用,2018(1):246-254.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" title=" 李云飞,符冉迪,金炜,等.多通道卷积的图像超分辨率方法[J].中国图象图形学报,2018,22(12):1690-1700." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZGTB201712005&amp;v=MTMwMjF5cmZiTEc0SDliTnJZOUZZWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnRGeW5rVzd2QVA=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                         李云飞,符冉迪,金炜,等.多通道卷积的图像超分辨率方法[J].中国图象图形学报,2018,22(12):1690-1700.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" title=" Zhang K,Zuo W,Zhang L.Learning a single convolutional super-resolution network for multiple degradations[C]//2018 IEEE Conference on Computer Vision and Pattern Recognition(CVPR).IEEE Computer Society,2018." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning a single convolutional super-resolution network for multiple degradations">
                                        <b>[9]</b>
                                         Zhang K,Zuo W,Zhang L.Learning a single convolutional super-resolution network for multiple degradations[C]//2018 IEEE Conference on Computer Vision and Pattern Recognition(CVPR).IEEE Computer Society,2018.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_10" title=" 周登文,赵丽娟,段然,等.基于递归残差网络的图像超分辨率重建[J/OL].自动化学报:1-9[2019-01-23].https://doi.org/10.16383/j.aas.c180334." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201906014&amp;v=MDkxNTZxQnRHRnJDVVI3cWZadVp0Rnlua1c3dkFLQ0xmWWJHNEg5ak1xWTlFWUlRS0RIODR2UjRUNmo1NE8zenE=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[10]</b>
                                         周登文,赵丽娟,段然,等.基于递归残差网络的图像超分辨率重建[J/OL].自动化学报:1-9[2019-01-23].https://doi.org/10.16383/j.aas.c180334.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_11" title=" He K,Zhang X,Ren S,et al.Deep residual learning for image recognition[C]//IEEE Conference on Computer Vision and Pattern Recognition.2016." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep residual learning for image recognition">
                                        <b>[11]</b>
                                         He K,Zhang X,Ren S,et al.Deep residual learning for image recognition[C]//IEEE Conference on Computer Vision and Pattern Recognition.2016.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_12" title=" Gao H,Zhuang L,Maaten L V D,et al.Densely connected convolutional networks[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition(CVPR).2017." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Densely Connected Convolutional Networks">
                                        <b>[12]</b>
                                         Gao H,Zhuang L,Maaten L V D,et al.Densely connected convolutional networks[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition(CVPR).2017.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JYRJ" target="_blank">计算机应用与软件</a>
                2019,36(10),222-226 DOI:10.3969/j.issn.1000-386x.2019.10.038            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于残差密集网络的单幅图像超分辨率重建</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%B0%A2%E9%9B%AA%E6%99%B4&amp;code=30608964&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">谢雪晴</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E9%87%8D%E5%BA%86%E5%B7%A5%E4%B8%9A%E8%81%8C%E4%B8%9A%E6%8A%80%E6%9C%AF%E5%AD%A6%E9%99%A2%E4%BF%A1%E6%81%AF%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0084534&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">重庆工业职业技术学院信息工程学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>随着数码相机、手机等电子设备的普及,每天都会产生大量的图像,但通常这些图像的分辨率比较低。针对单幅图像超分辨率(Single Image Super-Resolution,SISR)方法性能较低的问题,提出一种基于残差密集网络的单幅图像超分辨率重建方法。将浅层的卷积特征输入到残差密集块,获得全局和局部的特征;对图像进行超分辨率重建,得到清晰的高分辨率图像。为了验证该方法的有效性,在四个公共的数据集Set5、Set14、B100和Urban10上进行了定性和定量的实验。实验结果表明,该方法能够更好地恢复出高分辨率的图像。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E8%BE%A8%E7%8E%87&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">图像超分辨率;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%BD%8E%E5%88%86%E8%BE%A8%E7%8E%87&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">低分辨率;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%AB%98%E5%88%86%E8%BE%A8%E7%8E%87&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">高分辨率;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度学习;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    谢雪晴，讲师，主研领域:数字媒体应用技术。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-01-23</p>

                    <p>

                            <b>基金：</b>
                                                        <span>重庆市教委科学技术研究项目(KJ1603701);</span>
                    </p>
            </div>
                    <h1><b>SUPER-RESOLUTION RECONSTRUCTION OF SINGLE IMAGE BASED ON RESIDUAL-DENSE NETWORK</b></h1>
                    <h2>
                    <span>Xie Xueqing</span>
            </h2>
                    <h2>
                    <span>College of of Information Engineering,Chongqing Industry Polytechnic College</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>With the popularity of electronic devices such as digital cameras and mobile phones, a large number of images are produced every day, but usually the resolution of these images is low. Aiming at the problem of low performance of single image super-resolution(SISR) method, this paper proposed a super-resolution reconstruction method for single image based on residual-dense network. I input the convolution feature of shallow layer into residual-dense blocks to obtain global and local features, and reconstructed the image with super resolution to get clear and high resolution image. To verify the effectiveness of this method, this paper conducted qualitative and quantitative experiments on four public data sets Set5, Set14, B100 and Urban10. Experimental results show that the proposed method can better restore high resolution images.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Image%20super-resolution&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Image super-resolution;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Low%20resolution&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Low resolution;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=High%20resolution&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">High resolution;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Deep%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Deep learning;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2019-01-23</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="27" name="27" class="anchor-tag"><b>0 引 言</b></h3>
                <div class="p1">
                    <p id="28">图像超分辨率重构是数字图像处理领域中的研究热点之一。图像超分辨率是指把分辨率比较低的图像通过某种算法得到分辨率较高的图像,使获得的高分辨率图像具有更加清晰的颜色、纹理等信息。提高低质量图像质量的方法一般有两种:一是改善硬件设备,二是图像超分辨率重建技术。然而在实际应用中,由于比较高的工程成本和制作流程,大多数场合一般不会采用分辨率较高的相机来采集数字图像。而是利用超分辨率重建技术来得到具有更加丰富的颜色纹理等信息的高分辨率图像。目前,常见的超分辨率重建技术分为三类:基于插值的方法、基于重建的方法、基于学习的方法。</p>
                </div>
                <div class="p1">
                    <p id="29">基于插值的方法包括最近邻插值和三次插值,优点是算法复杂度较低,缺点是很容易出现阶梯锯齿状现象以及产生边缘模糊。</p>
                </div>
                <div class="p1">
                    <p id="30">基于重建的方法使用了强制约束平滑和下采样技术,使得低分辨率图像和高分辨率图像保持一致性,由于该方法过分地依赖高分辨率图像的先验知识,当缩小或放大图像的时候,重建质量就会下降,重建的结果就会缺失重要的细节信息。</p>
                </div>
                <div class="p1">
                    <p id="31">近年来,深度学习在图像超分辨率重建领域表现出巨大的潜力。文献<citation id="87" type="reference">[<a class="sup">1</a>]</citation>首次提出基于卷积神经网络的图像超像素重建方法,第一次用深度学习来解决SISR问题。该网络共三层卷积层:第一层提取输入的低分辨率图像的特征;第二层将提取的低分辨率图像的特征映射到高分辨率图像的特征;第三层为高分辨率图像的重建。损失函数为简单的MSE损失。文献<citation id="88" type="reference">[<a class="sup">2</a>]</citation>提出了基于学习的图像超分辨率算法,利用样本先验知识来重建图像,相较于其他重建方法有明显的优势,也是近年来研究的热点。本文首先分析了影响图像重建质量的原因,然后对基于深度学习的超分辨率重建算法<citation id="89" type="reference"><link href="3" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>提出了两点改进:用随机线性纠正单元(Randomized rectified linear unit,RReLU)来避免原有网络学习中对图像某些重要的信息的过度压缩,同时用NAG(Nesterov's accelerated gradient)方法来加速网络的收敛,进而避免了在梯度更新时产生较大的震荡。目前,图像超分辨率方法难以同时满足运算速度快和生成的图像高质量这一问题,文献<citation id="90" type="reference">[<a class="sup">3</a>]</citation>提出一种基于卷积的快速的图像超分辨率方法。此方法首先提取卷积特征,然后利用去卷积层对提取的特征放大膨胀,再以池化层对特征进行浓缩,提炼更鲁棒的特征,进而获得高分辨率图像。文献<citation id="91" type="reference">[<a class="sup">4</a>]</citation>提出了一种基于GAN网络的超分辨率重建。文中,作者论证了PSNR不能作为评价超分辨的标准,因为它和人对图片的感受结果不一致,并采用了MOS进行评价;另外作者在loss函数上进行了改进,更好地恢复出细节的纹理信息。文献<citation id="92" type="reference">[<a class="sup">5</a>]</citation>提出了用于图像超分辨率任务的新方法,分别是增强深度超分辨率网络EDSR和一种新的多尺度深度超分辨率MDSR,在减小模型大小的同时实现了比当前其他方法更好的性能。文献<citation id="93" type="reference">[<a class="sup">6</a>]</citation>提出了一个非常深的循环残差网络用于图像超分辨率,这个网络高达52层。文献<citation id="94" type="reference">[<a class="sup">7</a>]</citation>提出了一种多阶段级联残差卷积神经网络模型。该模型分为两阶段来进行超分辨率图像重建,首先重建2倍超分辨率图像,然后重建4倍超分辨率图像。第一阶段和第二阶段都使用残差层,对两阶段分别构建多任务损失函数,利用第一阶段的损失来指导第二阶段的损失,进而提高网络训练的速度。文献<citation id="95" type="reference">[<a class="sup">8</a>]</citation>针对经典的基于卷积神经网络的超分辨率(SRCNN)方法存在重建图像纹理结构模糊以及网络模型训练收敛过慢等问题,提出了一种多通道卷积的图像超分辨率(MCSR)方法。该方法通过增加残差链接,加快了模型收敛速度。同时利用多通道映射来提取更加丰富的特征,进而增强超分辨率重构效果。文献<citation id="96" type="reference">[<a class="sup">9</a>]</citation>提出了一种维度拉长策略,将模糊和噪声作为输入,可以应对多倍和空间改变的退化模型,提高了实用性。实验结果表明,提出的网络结构在多种多样的退化采样后,依然能生成比较好的结果。近年来,随着深度学习的快速发展,出现了很多基于神经网络的超分辨率算法。深度卷积网络在图像超分辨率重建方面取得了卓越的成就,但是参数数量往往非常巨大。文献<citation id="97" type="reference">[<a class="sup">10</a>]</citation>提出了一种简洁紧凑型递归残差网络结构。该网络通过局部残差的学习来减轻训练深层网络的困难;引入递归结构保证增加深度的同时控制模型参数数量;采用可调梯度裁剪方法来防止梯度消失。在网络末端使用反卷积层直接上采样图像,进而得到超分辨率图像。</p>
                </div>
                <div class="p1">
                    <p id="32">然而,这些方法只是用了某些卷积层的信息,而没有用全部的卷积层的信息。图像中的物体有不同的尺度、视角和长宽比,深度卷积神经网络中的层次特征对于图像重构有很大的帮助。因此,本文提出了一个残差密集网络结构用于图像超分辨率。</p>
                </div>
                <h3 id="33" name="33" class="anchor-tag"><b>1 残差网络和密集网络</b></h3>
                <div class="p1">
                    <p id="34">在训练的过程中,深度神经网络容易遇到梯度消失问题,在块的归一化中,我们将输入数据由激活函数的收敛区调整到梯度较大的区域,在一定程度上缓解了这种问题。不过,当网络的层数急剧增加时,BP算法导数的累乘效应很容易让梯度慢慢减小直至消失。为了从根本上解决这个问题,文献<citation id="98" type="reference">[<a class="sup">11</a>]</citation>提出了一种深度残差网络,简化那些非常深的网络的训练过程,使得层能根据其输入来学习残差函数而非原始函数。实验表明,这些残差网络的优化过程比较简单,能够使得网络结构有更深的层,而且能获得更加高的性能。文献<citation id="99" type="reference">[<a class="sup">11</a>]</citation>在ImageNet数据集上使用了一个152层的网络结构来评估所提出的残差网络,虽然它相当于8倍深的VGG网络,但是在所提出的框架中仍然有非常低的复杂度。这些残差网络的一个组合模型,在ImageNet测试集上的错误率仅为3.57%。此结果在2015年的ILSVRC分类任务上获得了第一名的好成绩。每一个残差块如图1所示。</p>
                </div>
                <div class="area_img" id="35">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201910039_035.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 残差块" src="Detail/GetImg?filename=images/JYRJ201910039_035.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 残差块  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201910039_035.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="36">其中:<i>x</i>是输入。假设<i>H</i>(<i>x</i>)是映射函数,我们不期望得到近似的<i>H</i>(<i>x</i>),而是把它们近似为一个残差函数<i>F</i>(<i>x</i>):<i>F</i>(<i>x</i>):=<i>H</i>(<i>x</i>)-<i>x</i>。因此,原始的函数<i>H</i>(<i>x</i>):=<i>F</i>(<i>x</i>)+<i>x</i>。</p>
                </div>
                <div class="p1">
                    <p id="37">对每几个堆栈层都应用残差学习,残差块可以被表示为如下形式:</p>
                </div>
                <div class="p1">
                    <p id="38"><i>y</i>=<i>F</i>(<i>x</i>,{<i>W</i><sub><i>i</i></sub>})+<i>x</i>      (1)</p>
                </div>
                <div class="p1">
                    <p id="39">式中:<i>x</i>和<i>y</i>分别为输入和输出;函数<i>F</i>(<i>x</i>,{<i>W</i><sub><i>i</i></sub>})表示学习到的残差映射。对于图1所示的残差块,总共为两层,<i>F</i>=<i>W</i><sub>2</sub><i>σ</i>(<i>W</i><sub>1</sub><i>x</i>),其中,<i>σ</i>为ReLU。</p>
                </div>
                <div class="p1">
                    <p id="40">根据最近的成果来看,如果神经网络的每一层到输入层和输出层采用更短的连接,那么网络结构就可以设计得更深、更准确,并且训练起来也更加高效。文献<citation id="100" type="reference">[<a class="sup">12</a>]</citation>根据这个现象,提出了密集的卷积神经网络(Dense Convolutional Network,DenseNet),该网络以前馈的方式把每一层与其他层分别相连接。传统的<i>L</i>层卷积网络总共有<i>L</i>个连接,而DenseNet的任一层不仅与相邻层有连接,而且与它随后的所有层都有直接连接,所以该网络有<mathml id="84"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mi>L</mi><mo stretchy="false">(</mo><mi>L</mi><mo>+</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><mn>2</mn></mfrac></mrow></math></mathml>个直接连接。DenseNet有如下几个令人信服的优点:(1) 有效解决了梯度消失问题;(2) 增强了特征传播能力;(3) DenseNet把网络中的每一层都直接与其前面的层相连接,实现了特征的重复利用;(4) DenseNet把网络中的每一层设计得非常窄,即只学习很少的特征图,最极端情况就是每一层只学习一个特征图,这样可以降低冗余性。</p>
                </div>
                <h3 id="41" name="41" class="anchor-tag"><b>2 方法设计</b></h3>
                <h4 class="anchor-tag" id="42" name="42"><b>2.1 整体流程</b></h4>
                <div class="p1">
                    <p id="43">本文提出了一种残差密集网络结构用于图像超分辨率,整体流程图如图2所示。该网络结构包括三部分:浅层的特征提取,残差密集块以及上采样操作。</p>
                </div>
                <div class="area_img" id="44">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201910039_044.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 残差密集网络结构" src="Detail/GetImg?filename=images/JYRJ201910039_044.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 残差密集网络结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201910039_044.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="45">我们用两层卷积层来提取浅层的特征,第一层卷积层提取输入的低分辨率图像(Low Resolution,LR)的特征,表示为:</p>
                </div>
                <div class="p1">
                    <p id="46"><i>F</i><sub>-1</sub>=<i>H</i><sub>SF1</sub>(<i>I</i><sub>LR</sub>)      (2)</p>
                </div>
                <div class="p1">
                    <p id="47">式中:<i>H</i><sub>SF1</sub>(·)表示卷积操作。<i>F</i><sub>-1</sub>可以进一步用于浅层特征的提取,因此,可以依次类推得到<i>F</i><sub>0</sub>:</p>
                </div>
                <div class="p1">
                    <p id="48"><i>F</i><sub>0</sub>=<i>H</i><sub>SF2</sub>(<i>F</i><sub>-1</sub>)      (3)</p>
                </div>
                <div class="p1">
                    <p id="49">式中:<i>H</i><sub>SF2</sub>(·)表示第二个特征提取层的卷积操作。特征<i>F</i><sub>0</sub>作为残差密集块的输入。假设我们有<i>D</i>个残差密集块。第<i>d</i>层残差密集块的输入<i>F</i><sub><i>d</i></sub>可以表示为:</p>
                </div>
                <div class="p1">
                    <p id="50"><i>F</i><sub><i>d</i></sub>=<i>H</i><sub>RDB,</sub><sub><i>d</i></sub>(<i>F</i><sub><i>d</i></sub><sub>-1</sub>)=<i>H</i><sub>RDB,</sub><sub><i>d</i></sub>(…<i>H</i><sub>RDB,0</sub>(<i>F</i><sub>0</sub>))      (4)</p>
                </div>
                <div class="p1">
                    <p id="51">式中:<i>H</i><sub>RDB,</sub><sub><i>d</i></sub>(·)表示第<i>d</i>个残差密集块(Residual Dense Block,RDB)的操作,它可以表示为复合函数运算,例如卷积层和Rectified Linear Units(ReLU)层。最后,所有的特征可以表示:</p>
                </div>
                <div class="p1">
                    <p id="52"><i>F</i><sub>all</sub>=<i>H</i>(<i>F</i><sub>-1</sub>,<i>F</i><sub>0</sub>,<i>F</i><sub>1</sub>,…,<i>F</i><sub><i>D</i></sub>)      (5)</p>
                </div>
                <div class="p1">
                    <p id="53">在提取了局部和全局的特征<i>F</i><sub>all</sub>之后,我们用一个上采样的网络,来得到高分辨率图像(High Resolution,HR)。</p>
                </div>
                <h4 class="anchor-tag" id="54" name="54"><b>2.2 残差密集块</b></h4>
                <div class="p1">
                    <p id="55">残差密集网络结构包括密集的连接层、局部特征融合层,如图3所示。</p>
                </div>
                <div class="area_img" id="56">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201910039_056.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 残差密集块" src="Detail/GetImg?filename=images/JYRJ201910039_056.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 残差密集块  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201910039_056.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="57">令<i>F</i><sub><i>d</i></sub><sub>-1</sub>和<i>F</i><sub><i>d</i></sub>分别表示第<i>d</i>个残差密集块的输入和输出。那么,第<i>d</i>个残差密集块中的第<i>c</i>个卷积层的输出可以表示为:</p>
                </div>
                <div class="p1">
                    <p id="58"><i>F</i><sub><i>d</i></sub><sub>,</sub><sub><i>c</i></sub>=<i>σ</i>(<i>W</i><sub><i>d</i></sub><sub>,</sub><sub><i>c</i></sub>[<i>F</i><sub><i>d</i></sub><sub>-1</sub>,<i>F</i><sub><i>d</i></sub><sub>,1</sub>,…,<i>F</i><sub><i>d</i></sub><sub>,</sub><sub><i>c</i></sub><sub>-1</sub>])      (6)</p>
                </div>
                <div class="p1">
                    <p id="59">式中:<i>σ</i>为ReLU激活函数,<i>W</i><sub><i>d</i></sub><sub>,</sub><sub><i>c</i></sub>是第<i>c</i>个卷积层的权重。</p>
                </div>
                <div class="p1">
                    <p id="60">本文所提出的残差密集网络生成高质量的超分辨率图像的方法,充分利用了原始低分辨率图像的每一层的特征。残差密集块不仅可以通过连续记忆机制从前一个残差密集块读取状态,还可以通过局部密集连接充分利用其中的所有层的信息。然后通过局部特征融合自适应地保留累积的特征。此外,还利用全局残差学习,将浅层特征和深层特征结合在一起,从原始LR图像中得到全局密集特征。</p>
                </div>
                <h3 id="61" name="61" class="anchor-tag"><b>3 实 验</b></h3>
                <h4 class="anchor-tag" id="62" name="62"><b>3.1 数据集和评价指标</b></h4>
                <div class="p1">
                    <p id="63">为了评估本文提出的方法的有效性,本文训练模型用文献<citation id="101" type="reference">[<a class="sup">1</a>]</citation>提到的91幅图像。对于测试,本文用四个基准的数据集:Set5、Set14、B100和Urban100。Set5共包括5幅图像,Set14包括14幅图像。</p>
                </div>
                <div class="p1">
                    <p id="64">图像超分辨率重构常用的客观评价指标主要包括峰值信噪比(Peak Signal to Noise Ratio,PSNR)和结构相似性(Structural Similarity Index Method,SSIM)。</p>
                </div>
                <div class="p1">
                    <p id="65">峰值信噪比是一种简单且广泛使用的SISR测量方法,它通过均方差(MSE)进行定义。PSNR可以定义为:</p>
                </div>
                <div class="p1">
                    <p id="66"><mathml id="85"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><mi>S</mi><mi>Ν</mi><mi>R</mi><mo>=</mo><mn>1</mn><mn>0</mn><mo>⋅</mo><mrow><mi>lg</mi></mrow><mrow><mo>(</mo><mrow><mfrac><mrow><mi>Μ</mi><mi>A</mi><mi>X</mi><msup><mrow></mrow><mn>2</mn></msup></mrow><mrow><mi>Μ</mi><mi>S</mi><mi>E</mi></mrow></mfrac></mrow><mo>)</mo></mrow></mrow></math></mathml>      (7)</p>
                </div>
                <div class="p1">
                    <p id="67">式中:<i>MAX</i>表示图像颜色的最大数值,8 bit图像的最大取值为255。<i>MSE</i>为均方差,定义为:</p>
                </div>
                <div class="p1">
                    <p id="68"><mathml id="86"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Μ</mi><mi>S</mi><mi>E</mi><mo>=</mo><mfrac><mn>1</mn><mrow><mi>m</mi><mi>n</mi></mrow></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mrow><mrow><mo>|</mo><mrow><mi>Κ</mi><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo><mo>-</mo><mi>Ι</mi><mo stretchy="false">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow><mo>|</mo></mrow></mrow></mstyle></mrow></mstyle><msup><mrow></mrow><mn>2</mn></msup></mrow></math></mathml>      (8)MSE=1<sub>mn</sub>∑<sup>n</sup><sub>i=1</sub>∑m <sub>j=1</sub>K(i,j)-I(i,j)<sup>2</sup>(8)</p>
                </div>
                <div class="p1">
                    <p id="69">式中:<i>I</i>和<i>K</i>分别是原始图像和处理后的图像,图像的大小为<i>m</i>×<i>n</i>。</p>
                </div>
                <div class="p1">
                    <p id="70">SSIM评价指标采用更加直接的方法来比较重建图像和参考图像的结构,表示为:</p>
                </div>
                <div class="p1">
                    <p id="71" class="code-formula">
                        <mathml id="71"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>S</mi><mi>S</mi><mi>Ι</mi><mi>Μ</mi><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><mo stretchy="false">(</mo><mn>2</mn><mi>μ</mi><msub><mrow></mrow><mi>x</mi></msub><mi>μ</mi><msub><mrow></mrow><mi>y</mi></msub><mo>+</mo><mi>c</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">)</mo><mo stretchy="false">(</mo><mn>2</mn><mi>σ</mi><msub><mrow></mrow><mrow><mi>x</mi><mi>y</mi></mrow></msub><mo>+</mo><mi>c</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">)</mo></mrow><mrow><mo stretchy="false">(</mo><mi>μ</mi><msubsup><mrow></mrow><mi>x</mi><mn>2</mn></msubsup><mo>+</mo><mi>μ</mi><msubsup><mrow></mrow><mi>y</mi><mn>2</mn></msubsup><mo>+</mo><mi>c</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">)</mo><mo stretchy="false">(</mo><mi>σ</mi><msubsup><mrow></mrow><mi>x</mi><mn>2</mn></msubsup><mo>+</mo><mi>σ</mi><msubsup><mrow></mrow><mi>y</mi><mn>2</mn></msubsup><mo>+</mo><mi>c</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">)</mo></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>9</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="72">式中:<i>μ</i><sub><i>x</i></sub>和<i>μ</i><sub><i>y</i></sub>表示图像所有像素的平均值,<i>σ</i><sub><i>x</i></sub>和<i>σ</i><sub><i>y</i></sub>表示图像像素值的方差,<i>C</i><sub>1</sub>和<i>C</i><sub>2</sub>为常数。</p>
                </div>
                <div class="p1">
                    <p id="73">PSNR值越大,说明效果越好;SSIM值越小,说明效果越好。</p>
                </div>
                <h4 class="anchor-tag" id="74" name="74"><b>3.2 实验结果与分析</b></h4>
                <div class="p1">
                    <p id="75">对于每一个超分辨率的尺度(×2、×3和×4),本文分别训练独立的模型。将本文方法与SRCNN<citation id="102" type="reference"><link href="3" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>方法、SRGAN<citation id="103" type="reference"><link href="9" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>方法、SRRRN<citation id="104" type="reference"><link href="13" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>方法、SRMulti<citation id="105" type="reference"><link href="19" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>方法分别在数据集Set5、Set14、B100和Urban100上进行对比,实验结果如表1-表4所示。可以看出,本文方法更加有效。</p>
                </div>
                <div class="area_img" id="76">
                    <p class="img_tit"><b>表1 本文方法和其他先进的方法在Set5数据集上的比较</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="76" border="1"><tr><td>尺度</td><td>指标</td><td>SRCNN</td><td>SRGAN</td><td>SRRRN</td><td>SRMulti</td><td>本文方法</td></tr><tr><td rowspan="2"><br />×2</td><td>PSNR</td><td>36.65</td><td>36.23</td><td>36.98</td><td>36.91</td><td><b>37.24</b></td></tr><tr><td><br />SSIM</td><td>0.954</td><td>0.941</td><td>0.962</td><td>0.959</td><td><b>0.971</b></td></tr><tr><td rowspan="2"><br />×3</td><td>PSNR</td><td>33.15</td><td>33.04</td><td>33.31</td><td>33.41</td><td><b>33.56</b></td></tr><tr><td><br />SSIM</td><td>0.913</td><td>0.910</td><td>0.923</td><td>0.924</td><td><b>0.943</b></td></tr><tr><td rowspan="2"><br />×4</td><td>PSNR</td><td>30.49</td><td>30.26</td><td>30.64</td><td>30.62</td><td><b>30.95</b></td></tr><tr><td><br />SSIM</td><td>0.862</td><td>0.859</td><td>0.868</td><td>0.870</td><td><b>0.882</b></td></tr><tr><td colspan="7"><br /></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="77">
                    <p class="img_tit"><b>表2 本文方法和其他先进的方法在Set14数据集上的比较</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="77" border="1"><tr><td>尺度</td><td>指标</td><td>SRCNN</td><td>SRGAN</td><td>SRRRN</td><td>SRMulti</td><td>本文方法</td></tr><tr><td rowspan="2"><br />×2</td><td>PSNR</td><td>32.29</td><td>32.25</td><td>32.35</td><td>32.43</td><td><b>32.68</b></td></tr><tr><td><br />SSIM</td><td>0.903</td><td>0.896</td><td>0.905</td><td>0.901</td><td><b>0.910</b></td></tr><tr><td rowspan="2"><br />×3</td><td>PSNR</td><td>29.41</td><td>29.35</td><td>29.57</td><td>29.51</td><td><b>29.84</b></td></tr><tr><td><br />SSIM</td><td>0.823</td><td>0.821</td><td>0.826</td><td>0.828</td><td><b>0.831</b></td></tr><tr><td rowspan="2"><br />×4</td><td>PSNR</td><td>27.61</td><td>27.53</td><td>27.80</td><td>27.89</td><td><b>27.96</b></td></tr><tr><td><br />SSIM</td><td>0.754</td><td>0.750</td><td>0.759</td><td>0.756</td><td><b>0.769</b></td></tr><tr><td colspan="7"><br /></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="78">
                    <p class="img_tit"><b>表3 本文方法和其他先进的方法在B100数据集上的比较</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="78" border="1"><tr><td>尺度</td><td>指标</td><td>SRCNN</td><td>SRGAN</td><td>SRRRN</td><td>SRMulti</td><td>本文方法</td></tr><tr><td rowspan="2"><br />×2</td><td>PSNR</td><td>31.22</td><td>31.21</td><td>31.29</td><td>31.33</td><td><b>31.45</b></td></tr><tr><td><br />SSIM</td><td>0.887</td><td>0.885</td><td>0.889</td><td>0.892</td><td><b>0.899</b></td></tr><tr><td rowspan="2"><br />×3</td><td>PSNR</td><td>28.41</td><td>28.32</td><td>28.47</td><td>28.52</td><td><b>28.67</b></td></tr><tr><td><br />SSIM</td><td>0.787</td><td>0.786</td><td>0.789</td><td>0.792</td><td><b>0.806</b></td></tr><tr><td rowspan="2"><br />×4</td><td>PSNR</td><td>26.91</td><td>26.89</td><td>26.93</td><td>26.96</td><td><b>27.12</b></td></tr><tr><td><br />SSIM</td><td>0.712</td><td>0.711</td><td>0.715</td><td>0.719</td><td><b>0.723</b></td></tr><tr><td colspan="7"><br /></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="79">
                    <p class="img_tit"><b>表4 本文方法和其他先进的方法在Urban100数据集上的比较</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="79" border="1"><tr><td>尺度</td><td>指标</td><td>SRCNN</td><td>SRGAN</td><td>SRRRN</td><td>SRMulti</td><td>本文方法</td></tr><tr><td rowspan="2"><br />×2</td><td>PSNR</td><td>29.52</td><td>29.48</td><td>29.58</td><td>29.67</td><td><b>29.78</b></td></tr><tr><td><br />SSIM</td><td>0.895</td><td>0.891</td><td>0.897</td><td>0.906</td><td><b>0.916</b></td></tr><tr><td rowspan="2"><br />×3</td><td>PSNR</td><td>26.24</td><td>26.12</td><td>26.28</td><td>26.38</td><td><b>26.45</b></td></tr><tr><td><br />SSIM</td><td>0.800</td><td>0.796</td><td>0.802</td><td>0.812</td><td><b>0.819</b></td></tr><tr><td rowspan="2"><br />×4</td><td>PSNR</td><td>24.53</td><td>24.43</td><td>24.56</td><td>24.62</td><td><b>24.82</b></td></tr><tr><td><br />SSIM</td><td>0.724</td><td>0.721</td><td>0.729</td><td>0.730</td><td><b>0.733</b></td></tr><tr><td colspan="7"><br /></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="80">本文除了对提出的方法进行定量的比较,还进行了定性的比较,如图4所示,为了得到更好的视觉效果,对标出的长方形框内的区域进行放大。图4中第一列是原始的图像,第二至第五列分别为比较先进的超分辨率方法,最后一列是本文提出的方法。可以看出,本文提出的方法可以更好地恢复出细节,进而得到更加清晰的高分辨率图像。</p>
                </div>
                <div class="area_img" id="81">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201910039_08100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 超分辨率的定性比较" src="Detail/GetImg?filename=images/JYRJ201910039_08100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 超分辨率的定性比较  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201910039_08100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h3 id="82" name="82" class="anchor-tag"><b>4 结 语</b></h3>
                <div class="p1">
                    <p id="83">本文提出了一种残差密集深度网络用于图像超分辨率,该方法充分利用了每一层的特征,得到了纹理等细节更清晰的超分辨率图像。在四个公共的数据集上进行了定量的实验,用常用的评价指标进行评价,实验结果表明,本文的方法更有效。此外,本文进行了定性的比较,通过直观的视觉观察可以看出,本文的方法相比其他相对先进的方法可以得到更加清晰的高分辨率图像。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image Super-Resolution Using Deep Convolutional Networks">

                                <b>[1]</b> Dong C,Loy C C,He K,et al.Image super-resolution using deep convolutional networks[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2014,38(2):295-307.
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201705013&amp;v=MDc0NjlqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVadEZ5bmtXN3ZBS0NMZlliRzRIOWJNcW85RVo0UUtESDg0dlI0VDY=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> 胡长胜,詹曙,吴从中.基于深度特征学习的图像超分辨率重建[J].自动化学报,2017,43(5):814-821.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201712017&amp;v=MTU1NThRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVp0Rnlua1c3dkFJalhUYkxHNEg5Yk5yWTlFWTQ=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> 孙超,吕俊伟,李健伟,等.基于去卷积的快速图像超分辨率方法[J].光学学报,2017(12):142-152.
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Photo-Realistic Single Image Super-Resolution U sing a Generative Adversarial Network">

                                <b>[4]</b> Ledig C,Theis L,Huszar F,et al.Photo-realistic single image super-resolution using a generative adversarial network[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition(CVPR).IEEE Computer Society,2017.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Enhanced Deep Residual Networks for Single Image Super-Resolution">

                                <b>[5]</b> Lim B,Son S,Kim H,et al.Enhanced deep residual networks for single image super-resolution[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops(CVPRW).IEEE Computer Society,2017.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image super-resolution via deep recursive residual network">

                                <b>[6]</b> Tai Y,Yang J,Liu X.Image super-resolution via deep recursive residual network[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition(CVPR).IEEE Computer Society,2017.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201801044&amp;v=MTA3MzJGeW5rVzd2QUx6N0JkN0c0SDluTXJvOUJZSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnQ=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b> 王一宁,秦品乐,李传朋,等.基于残差神经网络的图像超分辨率改进算法[J].计算机应用,2018(1):246-254.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZGTB201712005&amp;v=Mjg3NjFIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVp0Rnlua1c3dkFQeXJmYkxHNEg5Yk5yWTlGWVlRS0Q=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b> 李云飞,符冉迪,金炜,等.多通道卷积的图像超分辨率方法[J].中国图象图形学报,2018,22(12):1690-1700.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning a single convolutional super-resolution network for multiple degradations">

                                <b>[9]</b> Zhang K,Zuo W,Zhang L.Learning a single convolutional super-resolution network for multiple degradations[C]//2018 IEEE Conference on Computer Vision and Pattern Recognition(CVPR).IEEE Computer Society,2018.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_10" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201906014&amp;v=MjQxNjdmWnVadEZ5bmtXN3ZBS0NMZlliRzRIOWpNcVk5RVlJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3E=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[10]</b> 周登文,赵丽娟,段然,等.基于递归残差网络的图像超分辨率重建[J/OL].自动化学报:1-9[2019-01-23].https://doi.org/10.16383/j.aas.c180334.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep residual learning for image recognition">

                                <b>[11]</b> He K,Zhang X,Ren S,et al.Deep residual learning for image recognition[C]//IEEE Conference on Computer Vision and Pattern Recognition.2016.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Densely Connected Convolutional Networks">

                                <b>[12]</b> Gao H,Zhuang L,Maaten L V D,et al.Densely connected convolutional networks[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition(CVPR).2017.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JYRJ201910039" />
        <input id="dpi" type="hidden" value="600" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201910039&amp;v=MDEyNTZXN3ZBTHpUWlpMRzRIOWpOcjQ5R2JZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVadEZ5bms=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="0" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
