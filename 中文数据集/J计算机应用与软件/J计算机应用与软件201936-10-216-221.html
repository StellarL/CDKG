<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637135565904377500%26DBCODE%3dCJFD%26TABLEName%3dCJFDTEMP%26FileName%3dJYRJ201910038%26RESULT%3d1%26SIGN%3diDS%252fC0EOz7uLE%252bwlU5lNtllLCyY%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JYRJ201910038&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JYRJ201910038&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201910038&amp;v=MDgyNTJPM3pxcUJ0R0ZyQ1VSN3FmWnVadEZ5bmtXN3ZKTHpUWlpMRzRIOWpOcjQ5R2JJUUtESDg0dlI0VDZqNTQ=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#57" data-title="&lt;b&gt;0 引 言&lt;/b&gt; "><b>0 引 言</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#63" data-title="&lt;b&gt;1 高斯拉普拉斯算子理论&lt;/b&gt; "><b>1 高斯拉普拉斯算子理论</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#78" data-title="&lt;b&gt;2 算法描述&lt;/b&gt; "><b>2 算法描述</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#79" data-title="&lt;b&gt;2.1 聚焦程度度量&lt;/b&gt;"><b>2.1 聚焦程度度量</b></a></li>
                                                <li><a href="#82" data-title="&lt;b&gt;2.2 决策图形成&lt;/b&gt;"><b>2.2 决策图形成</b></a></li>
                                                <li><a href="#99" data-title="&lt;b&gt;2.3 图像融合&lt;/b&gt;"><b>2.3 图像融合</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#102" data-title="&lt;b&gt;3 实 验&lt;/b&gt; "><b>3 实 验</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#103" data-title="&lt;b&gt;3.1 实验参数设置&lt;/b&gt;"><b>3.1 实验参数设置</b></a></li>
                                                <li><a href="#105" data-title="&lt;b&gt;3.2 评价指标&lt;/b&gt;"><b>3.2 评价指标</b></a></li>
                                                <li><a href="#118" data-title="&lt;b&gt;3.3 实验结果分析&lt;/b&gt;"><b>3.3 实验结果分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#136" data-title="&lt;b&gt;4 结 语&lt;/b&gt; "><b>4 结 语</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#187" data-title="图1 高斯拉普拉斯模板">图1 高斯拉普拉斯模板</a></li>
                                                <li><a href="#86" data-title="图2 聚焦得分示意图">图2 聚焦得分示意图</a></li>
                                                <li><a href="#188" data-title="图3 原始多聚焦图像">图3 原始多聚焦图像</a></li>
                                                <li><a href="#125" data-title="图4 “clock”三方法融合结果">图4 “clock”三方法融合结果</a></li>
                                                <li><a href="#126" data-title="图5 “lab”三方法融合结果">图5 “lab”三方法融合结果</a></li>
                                                <li><a href="#127" data-title="图6 “clocks”三方法融合结果">图6 “clocks”三方法融合结果</a></li>
                                                <li><a href="#128" data-title="图7 “pepsi”三方法融合结果">图7 “pepsi”三方法融合结果</a></li>
                                                <li><a href="#130" data-title="&lt;b&gt;表1&lt;/b&gt; “&lt;b&gt;clock”图像融合指标&lt;/b&gt;"><b>表1</b> “<b>clock”图像融合指标</b></a></li>
                                                <li><a href="#131" data-title="&lt;b&gt;表2&lt;/b&gt; “&lt;b&gt;lab”图像融合指标&lt;/b&gt;"><b>表2</b> “<b>lab”图像融合指标</b></a></li>
                                                <li><a href="#133" data-title="&lt;b&gt;表3&lt;/b&gt; “&lt;b&gt;clocks”图像融合指标&lt;/b&gt;"><b>表3</b> “<b>clocks”图像融合指标</b></a></li>
                                                <li><a href="#134" data-title="&lt;b&gt;表4&lt;/b&gt; “&lt;b&gt;pepsi”图像融合指标&lt;/b&gt;"><b>表4</b> “<b>pepsi”图像融合指标</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" title=" Li S,Kang X,Fang L,et al.Pixel-level image fusion:A survey of the state of the art[J].Information Fusion,2016,33:100-112." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Pixel-level image fusion:A survey of the state of the art">
                                        <b>[1]</b>
                                         Li S,Kang X,Fang L,et al.Pixel-level image fusion:A survey of the state of the art[J].Information Fusion,2016,33:100-112.
                                    </a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" title=" Stathaki T.Image fusion:algorithms and applications[M].Academic Press,2008." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image fusion:algorithms and applications">
                                        <b>[2]</b>
                                         Stathaki T.Image fusion:algorithms and applications[M].Academic Press,2008.
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" >
                                        <b>[3]</b>
                                     Toet A.A morphological pyramidal image decomposition[J].Pattern Recognition Letters,1989,9(4):255-261.</a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" title=" Li H,Manjunath B S,Mitra S K.Multisensor image fusion using the wavelet transform[J].Graphical Models and Image Processing,1995,57(3):235-245." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501082866&amp;v=MjM4NzhETnFvOUVaT01OQkhvL29CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaW5sVXJ6SUlWOGNiaE09TmlmT2ZiSzdIdA==&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                         Li H,Manjunath B S,Mitra S K.Multisensor image fusion using the wavelet transform[J].Graphical Models and Image Processing,1995,57(3):235-245.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" title=" Lewis J J,O’Callaghan R J,Nikolov S G,et al.Pixel-and region-based image fusion with complex wavelets[J].Information Fusion,2007,8(2):119-130." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011300329188&amp;v=MjE1ODhRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcnpJSVY4Y2JoTT1OaWZPZmJLN0h0RE5ySTlGWitrR0RYUXhvQk1UNlQ0UA==&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                         Lewis J J,O’Callaghan R J,Nikolov S G,et al.Pixel-and region-based image fusion with complex wavelets[J].Information Fusion,2007,8(2):119-130.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" title=" Nencini F,Garzelli A,Baronti S,et al.Remote sensing image fusion using the curvelet transform[J].Information Fusion,2007,8(2):143-156." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011300329185&amp;v=MjkzMzlQUUgvaXJSZEdlcnFRVE1ud1plWnRGaW5sVXJ6SUlWOGNiaE09TmlmT2ZiSzdIdEROckk5Rlora0dEWFE4b0JNVDZUNA==&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                         Nencini F,Garzelli A,Baronti S,et al.Remote sensing image fusion using the curvelet transform[J].Information Fusion,2007,8(2):143-156.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" title=" Zhang Q,Guo B L.Multifocus image fusion using the nonsubsampled contourlet transform[J].Signal Processing,2009,89(7):1334-1346." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012300650202&amp;v=MDkxMjY5Rll1NFBEbnc3b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcnpJSVY4Y2JoTT1OaWZPZmJLN0h0RE9ySQ==&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                         Zhang Q,Guo B L.Multifocus image fusion using the nonsubsampled contourlet transform[J].Signal Processing,2009,89(7):1334-1346.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" title=" Piella G.A general framework for multiresolution image fusion:from pixels to regions[J].Information Fusion,2003,4(4):259-280." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011300329348&amp;v=MjcyMTcra0dEM2d4b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcnpJSVY4Y2JoTT1OaWZPZmJLN0h0RE5ySTlGWg==&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                         Piella G.A general framework for multiresolution image fusion:from pixels to regions[J].Information Fusion,2003,4(4):259-280.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" title=" Mitianoudis N,Stathaki T.Pixel-based and region-based image fusion schemes using ICA bases[J].Information Fusion,2007,8(2):131-142." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011300329189&amp;v=MjA2MzFUNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp0RmlubFVyeklJVjhjYmhNPU5pZk9mYks3SHRETnJJOUZaK2tHRFhRd29CTQ==&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                         Mitianoudis N,Stathaki T.Pixel-based and region-based image fusion schemes using ICA bases[J].Information Fusion,2007,8(2):131-142.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_10" title=" Yang B,Li S T.Multifocus image fusion and restoration with sparse representation[J].IEEE Transactions on Instrumentation and Measurement,2010,59(4):884-892." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multifocus image fusion and restoration with sparse representation">
                                        <b>[10]</b>
                                         Yang B,Li S T.Multifocus image fusion and restoration with sparse representation[J].IEEE Transactions on Instrumentation and Measurement,2010,59(4):884-892.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_11" title=" Liu Z,Chai Y,Yin H,et al.A novel multi-focus image fusion approach based on image decomposition[J].Information Fusion,2017,35:102-116." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A novel multi-focus image fusion approach based on image decomposition">
                                        <b>[11]</b>
                                         Liu Z,Chai Y,Yin H,et al.A novel multi-focus image fusion approach based on image decomposition[J].Information Fusion,2017,35:102-116.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_12" title=" Yang B,Li S.Pixel-level image fusion with simultaneous orthogonal matching pursuit[J].Information Fusion,2012,13(1):10-19." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011300328956&amp;v=MjI1MDBJVjhjYmhNPU5pZk9mYks3SHRETnJJOUZaK2tIQlhrL29CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaW5sVXJ6SQ==&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[12]</b>
                                         Yang B,Li S.Pixel-level image fusion with simultaneous orthogonal matching pursuit[J].Information Fusion,2012,13(1):10-19.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_13" title=" Yin H.Sparse representation with learned multiscale dictionary for image fusion[J].Neurocomputting,2015,148:600-610." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESE853A1BF3F1012562CA7CE4F8B881EB0&amp;v=MDYzNTBmT2ZjYXdHOUs5cnYwelo1ME9ESDA3eWhBUm1VNTZPd3JtMmhwSGNicVZNTWlmQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQnJMVTA1dHRoeGJtM3hLaz1OaQ==&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[13]</b>
                                         Yin H.Sparse representation with learned multiscale dictionary for image fusion[J].Neurocomputting,2015,148:600-610.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_14" title=" Kim M,Han D K,Ko H.Multimodal image fusion via sparse representation with local patch dictionaries[C]//IEEE International Conference on Image Processing.IEEE,2014." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multimodal image fusion via sparse representation with local patch dictionaries">
                                        <b>[14]</b>
                                         Kim M,Han D K,Ko H.Multimodal image fusion via sparse representation with local patch dictionaries[C]//IEEE International Conference on Image Processing.IEEE,2014.
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_15" title=" Kim M,Han D K,Ko H.Joint patch clustering-based dictionary learning for multimodal image fusion[J].Information Fusion,2016,27:198-214." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESB3FF6F53F91C3E5BFFFB25B64F6E5686&amp;v=MDA1MjZ1SFlmT0dRbGZCckxVMDV0dGh4Ym0zeEtrPU5pZk9mY0c3YUtmSzJZcEdFdUlPZjM5TXltUmxuRWtQU25xUXFoWkRmOGVSUTdLWkNPTnZGU2lXV3I3SklGcG1hQg==&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[15]</b>
                                         Kim M,Han D K,Ko H.Joint patch clustering-based dictionary learning for multimodal image fusion[J].Information Fusion,2016,27:198-214.
                                    </a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_16" title=" Zhang Q,Liu Y,Blum R S,et al.Sparse Representation based Multi-sensor Image Fusion for Multi-focus and Multi-modality Images:A Review[J].Information Fusion,2017,40:57-75." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESBF68A02760A03BFF96B957EAFB3AB5BB&amp;v=Mjg5MzZDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZCckxVMDV0dGh4Ym0zeEtrPU5pZk9mY0hPR05tOXI0MUNZdXQrREg5THVXQWE3RTEwVFhpWDNXUkhlc1BtUU1qdA==&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[16]</b>
                                         Zhang Q,Liu Y,Blum R S,et al.Sparse Representation based Multi-sensor Image Fusion for Multi-focus and Multi-modality Images:A Review[J].Information Fusion,2017,40:57-75.
                                    </a>
                                </li>
                                <li id="35">


                                    <a id="bibliography_17" title=" Huang W,Jing Z.Evaluation of focus measures in multi-focus image fusion[J].Pattern Recognition Letters,2007,28(4):493-500." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012300414662&amp;v=MjI0ODVNbndaZVp0RmlubFVyeklJVjhjYmhNPU5pZk9mYks3SHRET3JJOUZZT29MQ25vN29CTVQ2VDRQUUgvaXJSZEdlcnFRVA==&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[17]</b>
                                         Huang W,Jing Z.Evaluation of focus measures in multi-focus image fusion[J].Pattern Recognition Letters,2007,28(4):493-500.
                                    </a>
                                </li>
                                <li id="37">


                                    <a id="bibliography_18" title=" Aslantas V,Kurban R.Fusion of multi-focus images using differential evolution algorithm[J].Expert Systems with Applications,2010,37(12):8861-8870." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501640411&amp;v=MTEzNzN1OFBDSDA0b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcnpJSVY4Y2JoTT1OaWZPZmJLN0h0RE5xbzlFWQ==&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[18]</b>
                                         Aslantas V,Kurban R.Fusion of multi-focus images using differential evolution algorithm[J].Expert Systems with Applications,2010,37(12):8861-8870.
                                    </a>
                                </li>
                                <li id="39">


                                    <a id="bibliography_19" title=" De I,Chanda B.Multi-focus image fusion using a morphology-based focus measure in a quad-tree structure[J].Information Fusion,2013,14(2):136-146." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011300329515&amp;v=MjYzOTRETnJJOUZaK2tHQ1gwOG9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaW5sVXJ6SUlWOGNiaE09TmlmT2ZiSzdIdA==&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[19]</b>
                                         De I,Chanda B.Multi-focus image fusion using a morphology-based focus measure in a quad-tree structure[J].Information Fusion,2013,14(2):136-146.
                                    </a>
                                </li>
                                <li id="41">


                                    <a id="bibliography_20" title=" Li S,Yang B.Multifocus image fusion using region segmentation and spatial frequency[J].Image and Vision Computing,2008,26(7):971-979." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012201349240&amp;v=MjEzNDhiaE09TmlmT2ZiSzdIdERPclk5RVorOEdEbmc1b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcnpJSVY4Yw==&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[20]</b>
                                         Li S,Yang B.Multifocus image fusion using region segmentation and spatial frequency[J].Image and Vision Computing,2008,26(7):971-979.
                                    </a>
                                </li>
                                <li id="43">


                                    <a id="bibliography_21" title=" Liu Y,Liu S,Wang Z.Multi-focus image fusion with dense SIFT[J].Information Fusion,2015,23:139-155." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14110700451426&amp;v=MTAzNzhxUVRNbndaZVp0RmlubFVyeklJVjhjYmhNPU5pZk9mYks4SDlETXFJOUZZTzRPQ0g0L29CTVQ2VDRQUUgvaXJSZEdlcg==&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[21]</b>
                                         Liu Y,Liu S,Wang Z.Multi-focus image fusion with dense SIFT[J].Information Fusion,2015,23:139-155.
                                    </a>
                                </li>
                                <li id="45">


                                    <a id="bibliography_22" title=" Liu Y,Liu S,Wang Z.A general framework for image fusion based on multi-scale transform and sparse representation[J].Information Fusion,2015,24:147-164." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14110700451440&amp;v=MjAwMzBkR2VycVFUTW53WmVadEZpbmxVcnpJSVY4Y2JoTT1OaWZPZmJLOEg5RE1xSTlGWU80T0NIZzVvQk1UNlQ0UFFIL2lyUg==&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[22]</b>
                                         Liu Y,Liu S,Wang Z.A general framework for image fusion based on multi-scale transform and sparse representation[J].Information Fusion,2015,24:147-164.
                                    </a>
                                </li>
                                <li id="47">


                                    <a id="bibliography_23" title=" Liu Y,Chen X,Ward R,et al.Image fusion with convolutional sparse representation[J].IEEE Signal Processing Letters,2016,23(12):1882-1886." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image fusion with convolutional sparse representation">
                                        <b>[23]</b>
                                         Liu Y,Chen X,Ward R,et al.Image fusion with convolutional sparse representation[J].IEEE Signal Processing Letters,2016,23(12):1882-1886.
                                    </a>
                                </li>
                                <li id="49">


                                    <a id="bibliography_24" title=" www.escience.cn/people/liuyu1." target="_blank"
                                       href="">
                                        <b>[24]</b>
                                         www.escience.cn/people/liuyu1.
                                    </a>
                                </li>
                                <li id="51">


                                    <a id="bibliography_25" title=" Qu G,Zhang D,Yan P.Information measure for performance of image fusion[J].Electronics Letters,2002,38(7):313-315." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Information Measure for Performance of Image Fusion">
                                        <b>[25]</b>
                                         Qu G,Zhang D,Yan P.Information measure for performance of image fusion[J].Electronics Letters,2002,38(7):313-315.
                                    </a>
                                </li>
                                <li id="53">


                                    <a id="bibliography_26" title=" Xydeas C S,Petrovic V.Objective image fusion performance measure[J].Electronics Letters,2000,36(4):308-309." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Objective image fusion performance measure">
                                        <b>[26]</b>
                                         Xydeas C S,Petrovic V.Objective image fusion performance measure[J].Electronics Letters,2000,36(4):308-309.
                                    </a>
                                </li>
                                <li id="55">


                                    <a id="bibliography_27" title=" http://www.ece.lehigh.edu/SPCRL/IF/image_fusion.htm." target="_blank"
                                       href="">
                                        <b>[27]</b>
                                         http://www.ece.lehigh.edu/SPCRL/IF/image_fusion.htm.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JYRJ" target="_blank">计算机应用与软件</a>
                2019,36(10),216-221 DOI:10.3969/j.issn.1000-386x.2019.10.037            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于高斯拉普拉斯算子的多聚焦图像融合</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%AF%9B%E4%B9%89%E5%9D%AA&amp;code=42249016&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">毛义坪</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%A9%AC%E8%8C%82%E6%BA%90&amp;code=41037208&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">马茂源</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E9%87%8D%E5%BA%86%E5%B8%88%E8%8C%83%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%B8%8E%E4%BF%A1%E6%81%AF%E7%A7%91%E5%AD%A6%E5%AD%A6%E9%99%A2&amp;code=0124704&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">重庆师范大学计算机与信息科学学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>多聚焦图像融合利用图像众多互补信息,获取清晰的融合图像。为了解决基于变换域和空域方法中易丢失信息及时间复杂度高的问题,提出基于高斯拉普拉斯算子(LOG)的多聚焦图像融合算法。利用高斯拉普拉斯算子对原始图像进行掩膜卷积,取其绝对值得到相应原始图像的聚焦程度度量图;用滑动窗口技术,分别对每个度量图进行比较,窗口内和值大的视为聚焦,相应得分图加1,通过得分图与一定策略得到决策图;通过决策图对原始图像加权得到融合图像。从多聚焦图像公开数据库进行实验,LOG平均融合结果指标PSNR、MI、Q<sup>AB/F</sup>为28.08、5.28、0.73,均高于算法MST-SR和CSR。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%AB%98%E6%96%AF%E6%8B%89%E6%99%AE%E6%8B%89%E6%96%AF&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">高斯拉普拉斯;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%A4%9A%E8%81%9A%E7%84%A6%E5%9B%BE%E5%83%8F%E8%9E%8D%E5%90%88&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">多聚焦图像融合;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B4%BB%E8%B7%83%E5%BA%A6%E5%BA%A6%E9%87%8F&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">活跃度度量;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">滑动窗口;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    毛义坪，硕士生，主研领域:图像处理。;
                                </span>
                                <span>
                                    马茂源，硕士生。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-02-15</p>

                    <p>

                            <b>基金：</b>
                                                        <span>重庆市科委基础科学与前沿技术研究项目(cstc2017jcyjAX0106);</span>
                                <span>重庆市教委科技项目(KJ1600306);</span>
                                <span>重庆师范大学国家基金预研项目(14XYY009);</span>
                    </p>
            </div>
                    <h1><b>MULTI-FOCUS IMAGE FUSION BASED ON LAPLACIAN OF GAUSSIAN OPERATOR</b></h1>
                    <h2>
                    <span>Mao Yiping</span>
                    <span>Ma Maoyuan</span>
            </h2>
                    <h2>
                    <span>College of Computer and Information Science, Chongqing Normal University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Multi-focus image fusion uses a variety of complementary information of the image to obtain a clear fused image.In order to solve the problem of losing information and high time complexity in transform domain and spatial domain methods, we proposed a multi-focus image fusion algorithm based on Laplacian of Gaussian operator(LOG).The LOG was used to mask the original image, and its absolute value was obtained to obtain the activity level measurement of the corresponding original image.Then, using the sliding window technique, each metric map was compared separately, and the inside of the window and the large value were regarded as the focus, and the corresponding score map was incremented by one. Decision maps were obtained by scoring diagrams and certain strategies. Finally, the fusion image was obtained by weighting the original image with the decision graph. Experiments were carried out on multi-focus image public database. The average fusion outcome of LOGis 28.08, 5.28 and 0.73 for PSNR, MI and QAB/F. The results are higher than those of MST-SR and CSR.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Laplacian%20of%20Gaussian&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Laplacian of Gaussian;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Multi-focus%20image%20fusion&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Multi-focus image fusion;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Activity%20measurement&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Activity measurement;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Sliding%20window&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Sliding window;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2019-02-15</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="57" name="57" class="anchor-tag"><b>0 引 言</b></h3>
                <div class="p1">
                    <p id="58">近年来,图像融合成为了图像处理的一个特别重要的子领域,同时也是研究者们的热点课题<citation id="159" type="reference"><link href="3" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>。图像融合是将同一场景的两幅或者多幅附带不同信息图像融合成一幅图像的过程。融合后的图像比原始单个图像包含更多的信息,及融合后的图像具有每个原始图像的能量。由于视觉成像的相机景深是有限的,所以不能完美地使相机成像里的每个部分都聚焦。因此只要相机聚焦了某个点,相应的其他部分就不能聚焦。不在聚焦范围内的部分给人的感觉就是模糊不清的。多聚焦图像融合技术就是研究这样的实际的问题,目的是使图像的每个部分清楚。在此技术上,同一场景的每个原始图像聚焦点不同。同时,多聚焦图像融合技术也是图像融合技术的一个子块,大多数多聚焦图像融合算法简单修改或不改也可以用到其他的图像融合上。比如,红外与可见光图像融合、多模态医学图像融合等。反之亦然。从某角度讲,研究多聚焦融合是图像处理中比较热门的问题。</p>
                </div>
                <div class="p1">
                    <p id="59">几十年来,研究者们提出了很多图像融合的可行方法,简单地可以分为两类,即变换域法和空域法<citation id="160" type="reference"><link href="5" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>。最早提出图像融合的变换域法是基于多尺度分解(multi-scale transform,MST)的算法。其中最早基于拉普拉斯金字塔变换<citation id="161" type="reference"><link href="7" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>的方法已经提出三十多年的历史。其他经典的基于多尺度变换方法也相继提了出来,如:基于离散小波的方法(discrete wavelet transform,DWT)<citation id="162" type="reference"><link href="9" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>、基于双树复小波方法<citation id="163" type="reference"><link href="11" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>。在高维情况下,小波分析不能充分利用数据本身的几何特性,不能用最优的或最稀疏的函数去表示信号,不具有平移不变性。因此,二十世纪后学者们提出了多尺度几何分析(multi-scale geometric analysis,MGA),目的是发展最新最优的高维信号表示法。该方法也应用到了图像融合领域。2007年,Nencini等<citation id="164" type="reference"><link href="13" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>提出基于曲波图像融合方法;2009年,Zhang等<citation id="165" type="reference"><link href="15" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>提出了基于无下采样轮廓波方法。通常上述基于多尺度分析方法一般有三个步骤:变换图像、融合系数、反变换重组图像<citation id="166" type="reference"><link href="17" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>。一般被变换后的图像分为高频系数和低频系数,低频是对原始图像的近似,高频是图像的细节。除了分解方法以外,系数融合也对融合质量好坏有比较大的影响。在大多数的基于多尺度分解的方法中,系数的融合规则是高频取绝对值最大,低频取均值。</p>
                </div>
                <div class="p1">
                    <p id="60">近几年来,学者提出了一种新的基于变换域的方法<citation id="173" type="reference"><link href="19" rel="bibliography" /><link href="21" rel="bibliography" /><link href="23" rel="bibliography" /><sup>[<a class="sup">9</a>,<a class="sup">10</a>,<a class="sup">11</a>]</sup></citation>,并且迅速成为了图像融合领域热门方法。与多尺度分析方法不同的是,此方法利用比较先进的信号表示理论把原始图像变换成单一尺度特征空间,如独立成分分析理论、稀疏表示理论(sparse representation,SR)。为了保证融合图像结果的平移不变性,此类方法通常会用到滑动窗口技术。最重要的问题是探索最有效的特征域来表示图像的高频信息。基于稀疏表示理论<citation id="167" type="reference"><link href="21" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>的图像融合技术就是把图像块映射到稀疏领域,用稀疏系数的L<sub>0</sub>范数来表示图像块的重要信息。自从2010年基于稀疏表示的方法提出,很多研究者对其十分感兴趣,纷纷提出一些新的或改进的算法。有些是改进求稀疏解的算法,如正交匹配追踪<citation id="168" type="reference"><link href="25" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>,有些是改进字典的方法,如,Yin<citation id="169" type="reference"><link href="27" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>提出基于多尺度学习字典的方法。组成一个好的字典是融合质量的关键。为了提高字典的有效性,Kim<citation id="170" type="reference"><link href="29" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>提出了基于局部块字典的方法。由于局部块字典是直接提取于源图像,难免有些冗余和相应的效应。为了降低字典的冗余度提高相应的紧凑性。2016年,Kim等<citation id="171" type="reference"><link href="31" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>提出了基于联合块聚类的学习字典,其原理是先对原始图像块聚类(水平边缘、垂直边缘、平坦边缘)分别组成一个子字典,然后对子字典组成的母字典用PCA分析法进行分析,最终得到一个紧凑的字典。此方法比以前的字典效率明显提高。更多基于稀疏表示的方法可以参考文献<citation id="172" type="reference">[<a class="sup">16</a>]</citation>。</p>
                </div>
                <div class="p1">
                    <p id="61">空域法图像融合方法是基于图像空域来处理的,不用对图像进行某种转换或映射到其他空间的形式。最简单的空域法就是均值法,把原始图像对应像素点相加除以原始融合图像的数目。当然这样的方法会丢失很多细节。早期空域法是把图像分块,然后比较原始图像对应位置的聚焦程度,聚焦值大的块作为融合图像的相应块。以此类推,比较所有原始块。聚焦程度一般采用空间频率、拉普拉斯算子和、方差等表示<citation id="174" type="reference"><link href="35" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>。此类方法对融合结果有较大的影响,比如遇到复杂图像,不能良好地区分到底哪块是聚焦图像块,而且很容易引入块效应。由于手动分块会产生上述问题,Aslantas等<citation id="175" type="reference"><link href="37" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>提出了采用差分进化算法自适应分块算法,弥补了手动分块算法的不足。相应的还有基于形态学的四叉树结构聚焦检测法<citation id="176" type="reference"><link href="39" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>,它能灵活地选择原始图像块,比原始手工分块的融合效果提高了不少。其他类型的空域法是基于图像的分割<citation id="177" type="reference"><link href="41" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>。分割方法与基于分块法的道理类似,只是融合结果对分割精度的依赖性比较高,即要求尽量清楚分割聚焦区与非聚焦区域。2015年,Liu等<citation id="178" type="reference"><link href="43" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>提出了基于DSIFT(Dense scale invariant features transform)的多聚焦图像融合,其融合方法是用DSIFT对原图做活跃程度度量,结合滑动窗口技术和一定策略形成决策图,最后通过加权融合得到融合图像。该算法克服了块效应和一些传统算法的缺点,得到了很好的融合效果。</p>
                </div>
                <div class="p1">
                    <p id="62">上述基于多尺度分析方法中,由于低频按平均值来融合,所以结果图像对比度很容易降低,相应的低频信息就丢失了。基于稀疏表示的方法中,字典一般情况表达能力不足,相应的融合图像的纹理易丢失,且此方法时间复杂度较高,不能用到实时项目中。早期空域法是通过分块或基于图像分割,可能引起块效应果区域模糊。为了克服这些问题,本文提出基于高斯拉普拉斯算子(LOG)的多聚焦融合算法。首先利用高斯拉普拉斯算子度量原始图像的活跃度;为了不引入块效应,采用滑动窗口技术得到决策图;最后通过决策图对原始图像加权的方式得到融合图像。实验结果验证了其有效性。</p>
                </div>
                <h3 id="63" name="63" class="anchor-tag"><b>1 高斯拉普拉斯算子理论</b></h3>
                <div class="p1">
                    <p id="64">在图像处理中,高斯拉普拉斯算子主要作为边缘检测之一,对噪声与离散点的图像有一定的鲁棒性。如果图像聚焦,图像中的边缘即那些灰度发生跳变的区域就会更亮,所以把高斯拉普拉斯算子应用在度量图像活跃程度中。</p>
                </div>
                <div class="p1">
                    <p id="65">高斯卷积函数定义为:</p>
                </div>
                <div class="p1">
                    <p id="66"><mathml id="138"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>G</mi><msub><mrow></mrow><mi>σ</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><mrow><msqrt><mrow><mn>2</mn><mtext>π</mtext><mi>σ</mi><msup><mrow></mrow><mn>2</mn></msup></mrow></msqrt></mrow></mfrac><mrow><mi>exp</mi></mrow><mrow><mo>(</mo><mrow><mo>-</mo><mfrac><mrow><mi>x</mi><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mi>y</mi><msup><mrow></mrow><mn>2</mn></msup></mrow><mrow><mn>2</mn><mi>σ</mi><msup><mrow></mrow><mn>2</mn></msup></mrow></mfrac></mrow><mo>)</mo></mrow></mrow></math></mathml>      (1)</p>
                </div>
                <div class="p1">
                    <p id="67">原图像f(x,y)与高斯卷积可以定义为:</p>
                </div>
                <div class="p1">
                    <p id="68">Δ|G<sub>σ</sub>(x,y)*f(x,y)|=|ΔG<sub>σ</sub>(x,y)|*f(x,y)=LOG*f(x,y)      (2)</p>
                </div>
                <div class="p1">
                    <p id="70">LOG可以通过先对高斯函数进行偏导操作,然后进行卷积求解,公式表示为:</p>
                </div>
                <div class="p1">
                    <p id="71"><mathml id="139"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mo>∂</mo><mrow><mo>∂</mo><mi>x</mi></mrow></mfrac><mi>G</mi><msub><mrow></mrow><mi>σ</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mo>∂</mo><mrow><mo>∂</mo><mi>x</mi></mrow></mfrac><mtext>e</mtext><msup><mrow></mrow><mrow><mo>-</mo><mo stretchy="false">(</mo><mi>x</mi><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mi>y</mi><msup><mrow></mrow><mn>2</mn></msup><mo stretchy="false">)</mo><mo>/</mo><mn>2</mn><mi>σ</mi><msup><mrow></mrow><mn>2</mn></msup></mrow></msup><mo>=</mo><mfrac><mi>x</mi><mrow><mi>σ</mi><msup><mrow></mrow><mn>2</mn></msup></mrow></mfrac><mtext>e</mtext><msup><mrow></mrow><mrow><mo>-</mo><mo stretchy="false">(</mo><mi>x</mi><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mi>y</mi><msup><mrow></mrow><mn>2</mn></msup><mo stretchy="false">)</mo><mo>/</mo><mn>2</mn><mi>σ</mi><msup><mrow></mrow><mn>2</mn></msup></mrow></msup></mrow></math></mathml>      (3)</p>
                </div>
                <div class="p1">
                    <p id="72" class="code-formula">
                        <mathml id="72"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mtext> </mtext><mfrac><mrow><mo>∂</mo><msup><mrow></mrow><mn>2</mn></msup></mrow><mrow><mo>∂</mo><msup><mrow></mrow><mn>2</mn></msup><mi>x</mi></mrow></mfrac><mi>G</mi><msub><mrow></mrow><mi>σ</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><mi>x</mi><msup><mrow></mrow><mn>2</mn></msup></mrow><mrow><mi>σ</mi><msup><mrow></mrow><mn>4</mn></msup></mrow></mfrac><mtext>e</mtext><msup><mrow></mrow><mrow><mo>-</mo><mo stretchy="false">(</mo><mi>x</mi><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mi>y</mi><msup><mrow></mrow><mn>2</mn></msup><mo stretchy="false">)</mo><mo>/</mo><mn>2</mn><mi>σ</mi><msup><mrow></mrow><mn>2</mn></msup></mrow></msup><mo>-</mo><mfrac><mn>1</mn><mrow><mi>σ</mi><msup><mrow></mrow><mn>2</mn></msup></mrow></mfrac><mtext>e</mtext><msup><mrow></mrow><mrow><mo>-</mo><mo stretchy="false">(</mo><mi>x</mi><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mi>y</mi><msup><mrow></mrow><mn>2</mn></msup><mo stretchy="false">)</mo><mo>/</mo><mn>2</mn><mi>σ</mi><msup><mrow></mrow><mn>2</mn></msup></mrow></msup><mo>=</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mtext> </mtext><mtext> </mtext><mfrac><mrow><mi>x</mi><msup><mrow></mrow><mn>2</mn></msup><mo>-</mo><mi>σ</mi><msup><mrow></mrow><mn>2</mn></msup></mrow><mrow><mi>σ</mi><msup><mrow></mrow><mn>4</mn></msup></mrow></mfrac><mtext>e</mtext><msup><mrow></mrow><mrow><mo>-</mo><mo stretchy="false">(</mo><mi>x</mi><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mi>y</mi><msup><mrow></mrow><mn>2</mn></msup><mo stretchy="false">)</mo><mo>/</mo><mn>2</mn><mi>σ</mi><msup><mrow></mrow><mn>2</mn></msup></mrow></msup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>4</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="73">因此LOG核函数定义为:</p>
                </div>
                <div class="p1">
                    <p id="74" class="code-formula">
                        <mathml id="74"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>L</mi><mi>Ο</mi><mi>G</mi><mover><mstyle mathsize="140%" displaystyle="true"><mo>=</mo></mstyle><mi>Δ</mi></mover><mi>Δ</mi><mi>G</mi><msub><mrow></mrow><mi>σ</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><mo>∂</mo><msup><mrow></mrow><mn>2</mn></msup></mrow><mrow><mo>∂</mo><mi>x</mi><msup><mrow></mrow><mn>2</mn></msup></mrow></mfrac><mi>G</mi><msub><mrow></mrow><mi>σ</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>+</mo><mfrac><mrow><mo>∂</mo><msup><mrow></mrow><mn>2</mn></msup></mrow><mrow><mo>∂</mo><mi>y</mi><msup><mrow></mrow><mn>2</mn></msup></mrow></mfrac><mi>G</mi><msub><mrow></mrow><mi>σ</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>=</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mtext> </mtext><mtext> </mtext><mfrac><mrow><mi>x</mi><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mi>y</mi><msup><mrow></mrow><mn>2</mn></msup><mo>-</mo><mn>2</mn><mi>σ</mi><msup><mrow></mrow><mn>2</mn></msup></mrow><mrow><mi>σ</mi><msup><mrow></mrow><mn>4</mn></msup></mrow></mfrac><mtext>e</mtext><msup><mrow></mrow><mrow><mo>-</mo><mo stretchy="false">(</mo><mi>x</mi><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mi>y</mi><msup><mrow></mrow><mn>2</mn></msup><mo stretchy="false">)</mo><mo>/</mo><mn>2</mn><mi>σ</mi><msup><mrow></mrow><mn>2</mn></msup></mrow></msup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>5</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="75">为了编程方便,高斯拉普拉斯模板如图1所示。</p>
                </div>
                <div class="area_img" id="187">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201910038_18700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 高斯拉普拉斯模板" src="Detail/GetImg?filename=images/JYRJ201910038_18700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 高斯拉普拉斯模板  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201910038_18700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h3 id="78" name="78" class="anchor-tag"><b>2 算法描述</b></h3>
                <h4 class="anchor-tag" id="79" name="79"><b>2.1 聚焦程度度量</b></h4>
                <div class="p1">
                    <p id="80">由于高斯拉普拉斯算子在图像中的边缘即那些灰度发生跳变的区域十分敏感,于是将其应用到多聚焦图像融合中,用于度量图像活跃程度。块内绝对值越大,证明图像越活跃。</p>
                </div>
                <div class="p1">
                    <p id="81">这里以融合两幅图像为例,多幅图像可以以此类推。(1) 首先对原始图像<i>O</i><sub>1</sub>进行高斯拉普拉斯算子变换,得到高斯拉普拉斯算子变换矩阵<i><b>O</b></i><sub>11</sub>。(2) 取<i><b>O</b></i><sub>11</sub>矩阵的绝对值,得到|<i><b>O</b></i><sub>11</sub>|。(3) 最后|<i><b>O</b></i><sub>11</sub>|为聚焦度量图<i>A</i><sub>1</sub>。同理,对原始图像<i>O</i><sub>2</sub>操作得到其图像的聚焦度量图<i>A</i><sub>2</sub>。然后通过一定策略比较聚焦度量图得到决策图。</p>
                </div>
                <h4 class="anchor-tag" id="82" name="82"><b>2.2 决策图形成</b></h4>
                <div class="p1">
                    <p id="83">文献<citation id="179" type="reference">[<a class="sup">21</a>]</citation>提出了借助滑动窗口技术比较度量图法。单独像素点的比较容易受奇异点的影响,为了进一步提高图像区域的聚焦度量,采用分块与滑动窗口配合法。比较聚焦度量图<i>A</i><sub>1</sub>、<i>A</i><sub>2</sub>相同坐标的块,块内所有值之和大的,相应的得分图区域+1,最终通过得分图来形成决策图。具体形成过程如下:</p>
                </div>
                <div class="p1">
                    <p id="84">(1) 假设源图像的大小为<i>H</i>×<i>W</i>。使用步长为1个像素的滑窗技术从左上到右下顺序将<i>A</i><sub>1</sub>、<i>A</i><sub>2</sub>聚焦程度度量图分为<i>n</i>×<i>n</i>的图像块,共可得到<i>Q</i>=(<i>H</i>-<i>n</i>+1)×(<i>W</i>-<i>n</i>+1)个图像块对,分别记为{<i>P</i><mathml id="140"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>1</mn><mi>i</mi></msubsup></mrow></math></mathml>}<mathml id="141"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Q</mi></msubsup></mrow></math></mathml>和{<i>P</i><mathml id="142"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>2</mn><mi>i</mi></msubsup></mrow></math></mathml>}<mathml id="143"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Q</mi></msubsup></mrow></math></mathml>。</p>
                </div>
                <div class="p1">
                    <p id="85">(2) 新建两个和原始图像大小一样的全零矩阵<i><b>M</b></i><sub>1</sub>、<i><b>M</b></i><sub>2</sub>,用于保存得分值。计算每个<i>P</i><mathml id="144"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>1</mn><mi>i</mi></msubsup></mrow></math></mathml>和<i>P</i><mathml id="145"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>2</mn><mi>i</mi></msubsup></mrow></math></mathml>块内的和,分别记为<i>S</i><mathml id="146"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>1</mn><mi>i</mi></msubsup></mrow></math></mathml>和<i>S</i><mathml id="147"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>2</mn><mi>i</mi></msubsup></mrow></math></mathml>。当<i>S</i><mathml id="148"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>1</mn><mi>i</mi></msubsup></mrow></math></mathml>&gt;<i>S</i><mathml id="149"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>2</mn><mi>i</mi></msubsup></mrow></math></mathml>时,<i><b>M</b></i><sub>1</sub>对应块内所有元素加1,否则,<i><b>M</b></i><sub>2</sub>对应块所有元素加1,如图2所示。</p>
                </div>
                <div class="area_img" id="86">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201910038_086.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 聚焦得分示意图" src="Detail/GetImg?filename=images/JYRJ201910038_086.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 聚焦得分示意图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201910038_086.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="87">聚焦度量图<i>A</i><sub>1</sub>、<i>A</i><sub>2</sub>所有块{<i>p</i><mathml id="150"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>1</mn><mi>i</mi></msubsup></mrow></math></mathml>}<mathml id="151"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Q</mi></msubsup></mrow></math></mathml>、{<i>p</i><mathml id="152"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>2</mn><mi>i</mi></msubsup></mrow></math></mathml>}<mathml id="153"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Q</mi></msubsup></mrow></math></mathml>都进行以上操作后。得到最终的得分图<i>M</i><sub>1</sub>、<i>M</i><sub>2</sub>。由于滑动窗口原因,聚焦度量比较时候,肯定有重复的。得分图大多数点比较次数为<i>n</i>×<i>n</i>次,取值范围为0到<i>n</i>×<i>n</i>。</p>
                </div>
                <div class="p1">
                    <p id="88">(3) 根据聚焦得分图<i>M</i><sub>1</sub>、<i>M</i><sub>2</sub>,可以把原始图像对应像素分为聚焦、散焦、不确定。对于原始图<i>O</i><sub>1</sub>像,分类规则为:</p>
                </div>
                <div class="area_img" id="89">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JYRJ201910038_08900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="91">对于原始图像O<sub>2</sub>,分类规则为:</p>
                </div>
                <div class="area_img" id="92">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JYRJ201910038_09200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="94">对于上述分类规则,O<sub>1</sub>(x,y)、O<sub>2</sub>(x,y)、M<sub>1</sub>(x,y)、M<sub>2</sub>(x,y)表示为对应的像素点。为了严谨,只有同时满足M<sub>1</sub>得分和M<sub>2</sub>不得分的情况下O<sub>1</sub>(x,y)才聚焦,O<sub>2</sub>(x,y)同理,否则为不确定。</p>
                </div>
                <div class="p1">
                    <p id="95">然后对聚焦像素赋值1,对不聚焦或者不确定的赋值为0,由于图像复杂且不确定,赋值后的聚焦图会出现一些小洞,然后用<i>MATLAB</i>自带函数(<i>bwareaopen</i>)修复小洞。</p>
                </div>
                <div class="p1">
                    <p id="96">于是按照上面的分类标准得到融合决策图:</p>
                </div>
                <div class="area_img" id="97">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JYRJ201910038_09700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <h4 class="anchor-tag" id="99" name="99"><b>2.3 图像融合</b></h4>
                <div class="p1">
                    <p id="100">通过上面的决策图规则得到决策图,决策图只含三个值1、0、0.5。最后得到融合图像策略如下:</p>
                </div>
                <div class="p1">
                    <p id="101"><i>F</i>(<i>x</i>,<i>y</i>)=<i>D</i>(<i>x</i>,<i>y</i>)<i>O</i><sub>1</sub>(<i>x</i>,<i>y</i>)+(1-<i>D</i>(<i>x</i>,<i>y</i>))<i>O</i><sub>2</sub>(<i>x</i>,<i>y</i>)      (9)</p>
                </div>
                <h3 id="102" name="102" class="anchor-tag"><b>3 实 验</b></h3>
                <h4 class="anchor-tag" id="103" name="103"><b>3.1 实验参数设置</b></h4>
                <div class="p1">
                    <p id="104">实验比较了近三年提出的基于结合多尺度分析、稀疏表示法(MST_SR)<citation id="180" type="reference"><link href="45" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>和卷积稀疏表示法(Convolutional Sparse Representation,CSR)<citation id="181" type="reference"><link href="47" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>。在文献<citation id="182" type="reference">[<a class="sup">22</a>]</citation>中,多尺度分解用的拉普拉斯金字塔,字典大小256,用的是正交匹配追踪算法求稀疏系数。具体参数见文献<citation id="183" type="reference">[<a class="sup">24</a>]</citation>。仿真计算机参数为Inter(R)Core(TM)i5-3210M CPU@2.5 GHz,内存4 GB,软件为MATLAB 2014a。</p>
                </div>
                <h4 class="anchor-tag" id="105" name="105"><b>3.2 评价指标</b></h4>
                <h4 class="anchor-tag" id="106" name="106">(1) 峰值信噪比(Peak signal-to-noise ratio,PSNR)。</h4>
                <div class="p1">
                    <p id="107">PSNR是信号可能的最大功率与影响信号表示精度破坏性噪音功率之间的比值。PSNR的值越大证明信号保持度越好。其表达式为:</p>
                </div>
                <div class="p1">
                    <p id="108"><mathml id="154"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><mi>S</mi><mi>Ν</mi><mi>R</mi><mo>=</mo><mn>1</mn><mn>0</mn><mrow><mi>log</mi></mrow><mn>1</mn><mn>0</mn><msup><mrow></mrow><mrow><mfrac><mrow><mi>Μ</mi><mi>A</mi><mi>X</mi><msup><mrow></mrow><mn>2</mn></msup></mrow><mrow><mi>Μ</mi><mi>S</mi><mi>E</mi></mrow></mfrac></mrow></msup></mrow></math></mathml>      (10)</p>
                </div>
                <div class="p1">
                    <p id="109">式中:MAX 是信号的最大可能功率,MSE表示均方误差,即各数据的误差平方的平均数。</p>
                </div>
                <h4 class="anchor-tag" id="110" name="110">(2) 互信息量(<i>Mutual Information</i>,<i>MI</i>)。</h4>
                <div class="p1">
                    <p id="111"><i>MI</i><citation id="184" type="reference"><link href="51" rel="bibliography" /><sup>[<a class="sup">25</a>]</sup></citation>的定义是输入和输出后的信息互相包含的总和量,数值越大证明输入输出的交互信息越多,融合效果越好。其定义公式为:</p>
                </div>
                <div class="p1">
                    <p id="112"><mathml id="155"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Μ</mi><mi>Ι</mi><msub><mrow></mrow><mrow><mi>X</mi><mi>Y</mi></mrow></msub><mo stretchy="false">(</mo><mi>x</mi><mi>y</mi><mo stretchy="false">)</mo><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>x</mi><mo>,</mo><mi>y</mi></mrow></munder><mi>p</mi></mstyle><msub><mrow></mrow><mrow><mi>X</mi><mi>Y</mi></mrow></msub><mo stretchy="false">(</mo><mi>x</mi><mi>y</mi><mo stretchy="false">)</mo><mrow><mi>log</mi></mrow><mfrac><mrow><mi>p</mi><msub><mrow></mrow><mrow><mi>X</mi><mi>Y</mi></mrow></msub><mo stretchy="false">(</mo><mi>x</mi><mi>y</mi><mo stretchy="false">)</mo></mrow><mrow><mi>p</mi><msub><mrow></mrow><mi>X</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mi>p</mi><msub><mrow></mrow><mi>Y</mi></msub><mo stretchy="false">(</mo><mi>y</mi><mo stretchy="false">)</mo></mrow></mfrac></mrow></math></mathml>      (11)</p>
                </div>
                <div class="p1">
                    <p id="113">式中:<i>P</i><sub><i>X</i></sub>(<i>x</i>)表示信息<i>X</i>的边缘概率密度,<i>P</i><sub><i>Y</i></sub>(<i>y</i>)表示信息<i>Y</i>的边缘概率密度,<i>P</i><sub><i>XY</i></sub>(<i>xy</i>)表示两个信息量<i>X</i>、<i>Y</i>的联合概率密度,<i>MI</i><sub><i>XY</i></sub>(<i>xy</i>)即为<i>X</i>、<i>Y</i>两信息的互信息量。</p>
                </div>
                <h4 class="anchor-tag" id="114" name="114">(3) 梯度相关指标<i>Q</i><sup><i>AB</i></sup><sup>/</sup><sup><i>F</i></sup>。</h4>
                <div class="p1">
                    <p id="115"><i>Q</i><sup><i>AB</i></sup><sup>/</sup><sup><i>F</i></sup><citation id="185" type="reference"><link href="53" rel="bibliography" /><sup>[<a class="sup">26</a>]</sup></citation>是一个常用的图像融合评价指标,原理是基于梯度信息的,一般用于检测原始图像到处理后图像之间梯度信息保留的程度。定义是:</p>
                </div>
                <div class="p1">
                    <p id="116" class="code-formula">
                        <mathml id="116"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Q</mi><msup><mrow></mrow><mrow><mi>A</mi><mi>B</mi><mo>/</mo><mi>F</mi></mrow></msup><mo>=</mo><mfrac><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>x</mi><mo>=</mo><mn>1</mn></mrow><mi>Η</mi></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>y</mi><mo>=</mo><mn>1</mn></mrow><mi>W</mi></munderover><mo stretchy="false">(</mo></mstyle></mrow></mstyle><mi>Q</mi><msup><mrow></mrow><mrow><mi>A</mi><mi>F</mi></mrow></msup><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo><mi>w</mi><msup><mrow></mrow><mi>A</mi></msup><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>+</mo><mi>Q</mi><msup><mrow></mrow><mrow><mi>B</mi><mi>F</mi></mrow></msup><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo><mi>w</mi><msup><mrow></mrow><mi>B</mi></msup><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>x</mi><mo>=</mo><mn>1</mn></mrow><mi>Η</mi></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>y</mi><mo>=</mo><mn>1</mn></mrow><mi>W</mi></munderover><mo stretchy="false">(</mo></mstyle></mrow></mstyle><mi>w</mi><msup><mrow></mrow><mi>A</mi></msup><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>+</mo><mi>w</mi><msup><mrow></mrow><mi>B</mi></msup><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>2</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="117">式中:<i>Q</i><sup><i>AF</i></sup>(<i>x</i>,<i>y</i>)=<i>Q</i><mathml id="156"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>g</mi><mrow><mi>A</mi><mi>F</mi></mrow></msubsup></mrow></math></mathml>(<i>x</i>,<i>y</i>)<i>Q</i><mathml id="157"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>a</mi><mrow><mi>A</mi><mi>F</mi></mrow></msubsup></mrow></math></mathml>(<i>x</i>,<i>y</i>),<i>Q</i><mathml id="158"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>g</mi><mrow><mi>A</mi><mi>F</mi></mrow></msubsup></mrow></math></mathml>(<i>x</i>,<i>y</i>)、<i>Q</i><sup><i>AF</i></sup><sub><i>a</i></sub>(<i>x</i>,<i>y</i>)分别代表原始图像<i>A</i>与融合后图像<i>F</i>处的相对边缘强度和边缘方向;<i>Q</i><sup><i>BF</i></sup>(<i>x</i>,<i>y</i>)的定义与<i>Q</i><sup><i>AF</i></sup>(<i>x</i>,<i>y</i>)同理;<i>w</i><sup><i>A</i></sup>(<i>x</i>,<i>y</i>)<i>f</i>(<i>x</i>,<i>y</i>)和<i>w</i><sup><i>B</i></sup>(<i>x</i>,<i>y</i>)分别是<i>Q</i><sup><i>AF</i></sup>(<i>x</i>,<i>y</i>)、<i>Q</i><sup><i>BF</i></sup>(<i>x</i>,<i>y</i>)的权重因子。</p>
                </div>
                <h4 class="anchor-tag" id="118" name="118"><b>3.3 实验结果分析</b></h4>
                <div class="p1">
                    <p id="119">实验选取了几对不确定聚焦图像进行测试,如图3所示<citation id="186" type="reference"><link href="55" rel="bibliography" /><sup>[<a class="sup">27</a>]</sup></citation>。</p>
                </div>
                <div class="area_img" id="188">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201910038_18800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 原始多聚焦图像" src="Detail/GetImg?filename=images/JYRJ201910038_18800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 原始多聚焦图像  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201910038_18800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="123">图3(a)中,左图是近景对焦,右图是远景对焦,左图书架看起来模糊,时钟看起来清晰,右图恰好相反。图3(b)中,左图近景聚焦,图像里人物就没有聚焦,看起来十分模糊。图3(c)、(d)与(a)、(b)差不多,都是部分清晰,即图像部分对焦其他区域散焦。</p>
                </div>
                <div class="p1">
                    <p id="124">融合后的图像如图4-图7所示。融合图像从视觉上看,在图像边缘处,LOG算法优于其他算法,原因是在正确判断图像聚焦区域下,LOG算法融合结果就是源图像本身,这样就不会出现失真或边缘模糊。然而多尺度算法因采样方式或融合策略(比如,高频取最大绝对值,低频取平均值)可能会丢失较多信息。稀疏表示因字典表达能力不足,使边缘或纹理出现模糊现象。LPFOG算法也可能出现判断聚焦区域失败而导致平均化失真。</p>
                </div>
                <div class="area_img" id="125">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201910038_125.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 “clock”三方法融合结果" src="Detail/GetImg?filename=images/JYRJ201910038_125.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 “clock”三方法融合结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201910038_125.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="126">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201910038_126.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 “lab”三方法融合结果" src="Detail/GetImg?filename=images/JYRJ201910038_126.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 “lab”三方法融合结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201910038_126.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="127">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201910038_127.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 “clocks”三方法融合结果" src="Detail/GetImg?filename=images/JYRJ201910038_127.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 “clocks”三方法融合结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201910038_127.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="128">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201910038_128.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 “pepsi”三方法融合结果" src="Detail/GetImg?filename=images/JYRJ201910038_128.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 “pepsi”三方法融合结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201910038_128.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="129">“clock”图像融合指标结果如表1所示,可以看出,对于第一个指标PSNR,MST_SR算法融合结果略优于LOG算法。但从其他指标可以看出,LOG算法都优于其他算法。尤其是MI指标,明显优于其他算法。表2“lab”融合指标结果与表1有些类似,在PSNR指标中,都是MST_SR融合算法略高一些,其他指标均是提出的LOG表现最好。</p>
                </div>
                <div class="area_img" id="130">
                    <p class="img_tit"><b>表1</b> “<b>clock”图像融合指标</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="130" border="1"><tr><td>指标</td><td>MST_SR</td><td>CSR</td><td>LOG</td></tr><tr><td><br />PSNR</td><td><b>27.143 7</b></td><td>26.866 2</td><td>27.070 1</td></tr><tr><td><br />MI</td><td>4.340 2</td><td>4.732 9</td><td><b>5.155 7</b></td></tr><tr><td><br />Q<sup>AB/F</sup></td><td>0.708 5</td><td>0.695 0</td><td><b>0.714 3</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="131">
                    <p class="img_tit"><b>表2</b> “<b>lab”图像融合指标</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="131" border="1"><tr><td><br />指标</td><td>MST_SR</td><td>CSR</td><td>LOG</td></tr><tr><td><br />PSNR</td><td><b>28.298 6</b></td><td>28.191 5</td><td>28.101 4</td></tr><tr><td><br />MI</td><td>4.977 4</td><td>5.180 7</td><td><b>5.087 0</b></td></tr><tr><td><br />Q<sup>AB/F</sup></td><td>0.714 3</td><td>0.696 4</td><td><b>0.732 6</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="132">表3中列出的是“clocks”图像融合结果。从结果上看,提出的LOG算法都优于对比算法。相对来说,基于卷积的稀疏表示法在PSNR指标中效果次于其他算法。“pepsi”原始图像融合结果如表4所示。其融合指标结果也是全部优于比较算法。MST_SR算法与提出的LPF算法在<i>Q</i><sup><i>AB</i></sup><sup>/</sup><sup><i>F</i></sup>指标上比较接近,但提出算法还是略高MST_SR算法。</p>
                </div>
                <div class="area_img" id="133">
                    <p class="img_tit"><b>表3</b> “<b>clocks”图像融合指标</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="133" border="1"><tr><td><br />指标</td><td>MST_SR</td><td>CSR</td><td>LOG</td></tr><tr><td><br />PSNR</td><td>27.691 2</td><td>27.351 4</td><td><b>27.813</b></td></tr><tr><td><br />MI</td><td>5.058 7</td><td>5.268 0</td><td><b>5.725 0</b></td></tr><tr><td><br />Q<sup>AB/F</sup></td><td>0.681 1</td><td>0.666 3</td><td><b>0.696 2</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="134">
                    <p class="img_tit"><b>表4</b> “<b>pepsi”图像融合指标</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="134" border="1"><tr><td><br />指标</td><td>MST_SR</td><td>CSR</td><td>LOG</td></tr><tr><td><br />PSNR</td><td>28.878 4</td><td>28.157 0</td><td><b>29.352 0</b></td></tr><tr><td><br />MI</td><td>4.882 5</td><td>4.918 2</td><td><b>5.179 1</b></td></tr><tr><td><br />Q<sup>AB/F</sup></td><td>0.770 8</td><td>0.713 2</td><td><b>0.779 2</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="135">由于图像的不确定性与复杂性,在PSNR指标中,基于多尺度与稀疏表示算法指标在前两对测试图像上稍微高于提出的LPF算法,但总体看,提出的基于LOG算法优于MST-SR和CSR算法。</p>
                </div>
                <h3 id="136" name="136" class="anchor-tag"><b>4 结 语</b></h3>
                <div class="p1">
                    <p id="137">根据高斯拉普拉斯算子,对图像进行掩膜卷积计算。将计算结果绝对值作为聚焦度量图,然后用滑动窗口对聚焦度量图打分,进行一定的策略得到决策图,最后对决策图乘以相应的权重,得到融合图像。本算法通过判断是否为聚焦区域来划分,如果判断是聚焦区域,就把此区域划为最终融合结果。这样得到的融合区域是原始图像,即没有对原始图像采样或某种近似表达,质量是非常好的。所以不论从主观还是客观评价指标上看,基于高斯拉普拉斯算子算法效果优于传统的算法。本算法主要核心是判断图像聚焦区域,类似于划分聚焦于散焦的分界线。如果可以准确划分聚焦区域,融合质量自然理想,如何又快又好划分聚焦区域是要继续研究探讨的问题。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Pixel-level image fusion:A survey of the state of the art">

                                <b>[1]</b> Li S,Kang X,Fang L,et al.Pixel-level image fusion:A survey of the state of the art[J].Information Fusion,2016,33:100-112.
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image fusion:algorithms and applications">

                                <b>[2]</b> Stathaki T.Image fusion:algorithms and applications[M].Academic Press,2008.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" >
                                    <b>[3]</b>
                                 Toet A.A morphological pyramidal image decomposition[J].Pattern Recognition Letters,1989,9(4):255-261.
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501082866&amp;v=MjE1MzFRVE1ud1plWnRGaW5sVXJ6SUlWOGNiaE09TmlmT2ZiSzdIdEROcW85RVpPTU5CSG8vb0JNVDZUNFBRSC9pclJkR2VycQ==&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b> Li H,Manjunath B S,Mitra S K.Multisensor image fusion using the wavelet transform[J].Graphical Models and Image Processing,1995,57(3):235-245.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011300329188&amp;v=MjcyMDJmT2ZiSzdIdEROckk5Rlora0dEWFF4b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcnpJSVY4Y2JoTT1OaQ==&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b> Lewis J J,O’Callaghan R J,Nikolov S G,et al.Pixel-and region-based image fusion with complex wavelets[J].Information Fusion,2007,8(2):119-130.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011300329185&amp;v=MTkwNzdSZEdlcnFRVE1ud1plWnRGaW5sVXJ6SUlWOGNiaE09TmlmT2ZiSzdIdEROckk5Rlora0dEWFE4b0JNVDZUNFBRSC9pcg==&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b> Nencini F,Garzelli A,Baronti S,et al.Remote sensing image fusion using the curvelet transform[J].Information Fusion,2007,8(2):143-156.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012300650202&amp;v=MTMyNTdoTT1OaWZPZmJLN0h0RE9ySTlGWXU0UERudzdvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp0RmlubFVyeklJVjhjYg==&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b> Zhang Q,Guo B L.Multifocus image fusion using the nonsubsampled contourlet transform[J].Signal Processing,2009,89(7):1334-1346.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011300329348&amp;v=MTcyMjUzZ3hvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp0RmlubFVyeklJVjhjYmhNPU5pZk9mYks3SHRETnJJOUZaK2tHRA==&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b> Piella G.A general framework for multiresolution image fusion:from pixels to regions[J].Information Fusion,2003,4(4):259-280.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011300329189&amp;v=MTM5Njg2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaW5sVXJ6SUlWOGNiaE09TmlmT2ZiSzdIdEROckk5Rlora0dEWFF3b0JNVA==&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b> Mitianoudis N,Stathaki T.Pixel-based and region-based image fusion schemes using ICA bases[J].Information Fusion,2007,8(2):131-142.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multifocus image fusion and restoration with sparse representation">

                                <b>[10]</b> Yang B,Li S T.Multifocus image fusion and restoration with sparse representation[J].IEEE Transactions on Instrumentation and Measurement,2010,59(4):884-892.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A novel multi-focus image fusion approach based on image decomposition">

                                <b>[11]</b> Liu Z,Chai Y,Yin H,et al.A novel multi-focus image fusion approach based on image decomposition[J].Information Fusion,2017,35:102-116.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_12" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011300328956&amp;v=MTEzNDZ6SUlWOGNiaE09TmlmT2ZiSzdIdEROckk5Rlora0hCWGsvb0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcg==&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[12]</b> Yang B,Li S.Pixel-level image fusion with simultaneous orthogonal matching pursuit[J].Information Fusion,2012,13(1):10-19.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_13" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESE853A1BF3F1012562CA7CE4F8B881EB0&amp;v=MzIxMjF2MHpaNTBPREgwN3loQVJtVTU2T3dybTJocEhjYnFWTU1pZkNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkJyTFUwNXR0aHhibTN4S2s9TmlmT2ZjYXdHOUs5cg==&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[13]</b> Yin H.Sparse representation with learned multiscale dictionary for image fusion[J].Neurocomputting,2015,148:600-610.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multimodal image fusion via sparse representation with local patch dictionaries">

                                <b>[14]</b> Kim M,Han D K,Ko H.Multimodal image fusion via sparse representation with local patch dictionaries[C]//IEEE International Conference on Image Processing.IEEE,2014.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_15" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESB3FF6F53F91C3E5BFFFB25B64F6E5686&amp;v=MjAyNzBrUFNucVFxaFpEZjhlUlE3S1pDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZCckxVMDV0dGh4Ym0zeEtrPU5pZk9mY0c3YUtmSzJZcEdFdUlPZjM5TXltUmxuRQ==&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[15]</b> Kim M,Han D K,Ko H.Joint patch clustering-based dictionary learning for multimodal image fusion[J].Information Fusion,2016,27:198-214.
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_16" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESBF68A02760A03BFF96B957EAFB3AB5BB&amp;v=MTM2MzJZdXQrREg5THVXQWE3RTEwVFhpWDNXUkhlc1BtUU1qdENPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkJyTFUwNXR0aHhibTN4S2s9TmlmT2ZjSE9HTm05cjQxQw==&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[16]</b> Zhang Q,Liu Y,Blum R S,et al.Sparse Representation based Multi-sensor Image Fusion for Multi-focus and Multi-modality Images:A Review[J].Information Fusion,2017,40:57-75.
                            </a>
                        </p>
                        <p id="35">
                            <a id="bibliography_17" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012300414662&amp;v=MTI2NTBmT2ZiSzdIdERPckk5RllPb0xDbm83b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcnpJSVY4Y2JoTT1OaQ==&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[17]</b> Huang W,Jing Z.Evaluation of focus measures in multi-focus image fusion[J].Pattern Recognition Letters,2007,28(4):493-500.
                            </a>
                        </p>
                        <p id="37">
                            <a id="bibliography_18" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501640411&amp;v=MTI3ODliSzdIdEROcW85RVl1OFBDSDA0b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcnpJSVY4Y2JoTT1OaWZPZg==&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[18]</b> Aslantas V,Kurban R.Fusion of multi-focus images using differential evolution algorithm[J].Expert Systems with Applications,2010,37(12):8861-8870.
                            </a>
                        </p>
                        <p id="39">
                            <a id="bibliography_19" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011300329515&amp;v=MDQyNjVlWnRGaW5sVXJ6SUlWOGNiaE09TmlmT2ZiSzdIdEROckk5Rlora0dDWDA4b0JNVDZUNFBRSC9pclJkR2VycVFUTW53Wg==&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[19]</b> De I,Chanda B.Multi-focus image fusion using a morphology-based focus measure in a quad-tree structure[J].Information Fusion,2013,14(2):136-146.
                            </a>
                        </p>
                        <p id="41">
                            <a id="bibliography_20" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012201349240&amp;v=MjE4MjNjYmhNPU5pZk9mYks3SHRET3JZOUVaKzhHRG5nNW9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaW5sVXJ6SUlWOA==&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[20]</b> Li S,Yang B.Multifocus image fusion using region segmentation and spatial frequency[J].Image and Vision Computing,2008,26(7):971-979.
                            </a>
                        </p>
                        <p id="43">
                            <a id="bibliography_21" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14110700451426&amp;v=MDgyODFpbmxVcnpJSVY4Y2JoTT1OaWZPZmJLOEg5RE1xSTlGWU80T0NINC9vQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp0Rg==&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[21]</b> Liu Y,Liu S,Wang Z.Multi-focus image fusion with dense SIFT[J].Information Fusion,2015,23:139-155.
                            </a>
                        </p>
                        <p id="45">
                            <a id="bibliography_22" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14110700451440&amp;v=MDA5NDFyeklJVjhjYmhNPU5pZk9mYks4SDlETXFJOUZZTzRPQ0hnNW9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaW5sVQ==&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[22]</b> Liu Y,Liu S,Wang Z.A general framework for image fusion based on multi-scale transform and sparse representation[J].Information Fusion,2015,24:147-164.
                            </a>
                        </p>
                        <p id="47">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image fusion with convolutional sparse representation">

                                <b>[23]</b> Liu Y,Chen X,Ward R,et al.Image fusion with convolutional sparse representation[J].IEEE Signal Processing Letters,2016,23(12):1882-1886.
                            </a>
                        </p>
                        <p id="49">
                            <a id="bibliography_24" target="_blank" href="">

                                <b>[24]</b> www.escience.cn/people/liuyu1.
                            </a>
                        </p>
                        <p id="51">
                            <a id="bibliography_25" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Information Measure for Performance of Image Fusion">

                                <b>[25]</b> Qu G,Zhang D,Yan P.Information measure for performance of image fusion[J].Electronics Letters,2002,38(7):313-315.
                            </a>
                        </p>
                        <p id="53">
                            <a id="bibliography_26" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Objective image fusion performance measure">

                                <b>[26]</b> Xydeas C S,Petrovic V.Objective image fusion performance measure[J].Electronics Letters,2000,36(4):308-309.
                            </a>
                        </p>
                        <p id="55">
                            <a id="bibliography_27" target="_blank" href="">

                                <b>[27]</b> http://www.ece.lehigh.edu/SPCRL/IF/image_fusion.htm.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JYRJ201910038" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201910038&amp;v=MDgyNTJPM3pxcUJ0R0ZyQ1VSN3FmWnVadEZ5bmtXN3ZKTHpUWlpMRzRIOWpOcjQ5R2JJUUtESDg0dlI0VDZqNTQ=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="0" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
