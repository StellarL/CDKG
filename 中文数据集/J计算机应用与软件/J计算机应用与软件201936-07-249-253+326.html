<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637135626672346250%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJYRJ201907043%26RESULT%3d1%26SIGN%3dDFjmwE2zCX9gx3d8xVUvMsssneA%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JYRJ201907043&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JYRJ201907043&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201907043&amp;v=MjQxOTlLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnRGeWpoVTc3T0x6VFpaTEc0SDlqTXFJOUJaNFE=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#33" data-title="&lt;b&gt;0 引 言&lt;/b&gt; "><b>0 引 言</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#36" data-title="&lt;b&gt;1 多特征提取&lt;/b&gt; "><b>1 多特征提取</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#37" data-title="&lt;b&gt;1.1 全局Gist特征提取&lt;/b&gt;"><b>1.1 全局Gist特征提取</b></a></li>
                                                <li><a href="#51" data-title="&lt;b&gt;1.2 Dense-SIFT特征提取&lt;/b&gt;"><b>1.2 Dense-SIFT特征提取</b></a></li>
                                                <li><a href="#58" data-title="&lt;b&gt;1.3 方向梯度直方图&lt;/b&gt;"><b>1.3 方向梯度直方图</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#62" data-title="&lt;b&gt;2 多稀疏表示分类器构成&lt;/b&gt; "><b>2 多稀疏表示分类器构成</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#63" data-title="&lt;b&gt;2.1 稀疏表示分类 (SRC) 方法&lt;/b&gt;"><b>2.1 稀疏表示分类 (SRC) 方法</b></a></li>
                                                <li><a href="#75" data-title="&lt;b&gt;2.2 多分类器权重融合&lt;/b&gt;"><b>2.2 多分类器权重融合</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#106" data-title="&lt;b&gt;3 实验及结果分析&lt;/b&gt; "><b>3 实验及结果分析</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#107" data-title="&lt;b&gt;3.1 实验环境&lt;/b&gt;"><b>3.1 实验环境</b></a></li>
                                                <li><a href="#111" data-title="&lt;b&gt;3.2 单个特征下的图像分类对比实验&lt;/b&gt;"><b>3.2 单个特征下的图像分类对比实验</b></a></li>
                                                <li><a href="#120" data-title="&lt;b&gt;3.3 基于不同分类器的分类对比实验&lt;/b&gt;"><b>3.3 基于不同分类器的分类对比实验</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#124" data-title="&lt;b&gt;4 结 语&lt;/b&gt; "><b>4 结 语</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#57" data-title="图1 Dense-SIFT滑动窗口的特征采样">图1 Dense-SIFT滑动窗口的特征采样</a></li>
                                                <li><a href="#60" data-title="图2 图像灰度图的HOG特征对应轮廓图">图2 图像灰度图的HOG特征对应轮廓图</a></li>
                                                <li><a href="#79" data-title="图3 基于多特征稀疏表示分类器的图像识别算法流程图">图3 基于多特征稀疏表示分类器的图像识别算法流程图</a></li>
                                                <li><a href="#110" data-title="图4 Cifar-10数据库部分预处理后的图像">图4 Cifar-10数据库部分预处理后的图像</a></li>
                                                <li><a href="#114" data-title="&lt;b&gt;表1 单特征稀疏表示分类器在不同训练集识别率&lt;/b&gt; %"><b>表1 单特征稀疏表示分类器在不同训练集识别率</b> %</a></li>
                                                <li><a href="#116" data-title="&lt;b&gt;表2 本文方法在不同训练集下的各特征权重与识别率&lt;/b&gt;"><b>表2 本文方法在不同训练集下的各特征权重与识别率</b></a></li>
                                                <li><a href="#118" data-title="图5 奖惩因子变化所对应分类准确率">图5 奖惩因子变化所对应分类准确率</a></li>
                                                <li><a href="#122" data-title="&lt;b&gt;表3 本文方法与其他分类器方法在不同图集准确率&lt;/b&gt; %"><b>表3 本文方法与其他分类器方法在不同图集准确率</b> %</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" title=" 黄凯奇, 任伟强, 谭铁牛.图像物体分类与检测算法综述[J].计算机学报, 2014, 36 (6) :1-18." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJX201406001&amp;v=MTY5MTN5amhVNzdPTHo3QmRyRzRIOVhNcVk5RlpZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVadEY=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         黄凯奇, 任伟强, 谭铁牛.图像物体分类与检测算法综述[J].计算机学报, 2014, 36 (6) :1-18.
                                    </a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" >
                                        <b>[2]</b>
                                     Wright J, Yang A Y, Sastry S S, et al.Robust Face Recognition via Sparse Representation[J].IEEE Trans Pattern Anal Mach Intell, 2009, 31 (2) :210-227.</a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title=" Aharon M, Elad M, Bruckstein A.K-SVD:An Algorithm for Designing Overcomplete Dictionaries for Sparse Representation[J].IEEE Transactions on Signal Processing, 2006, 54 (11) :4311-4322." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=-SVD: An Algorithm for Designing Overcomplete Dictionaries for Sparse Representation">
                                        <b>[3]</b>
                                         Aharon M, Elad M, Bruckstein A.K-SVD:An Algorithm for Designing Overcomplete Dictionaries for Sparse Representation[J].IEEE Transactions on Signal Processing, 2006, 54 (11) :4311-4322.
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" title=" Jiang Z, Lin Z, Davis L S.Label Consistent K-SVD:Learning a Discriminative Dictionary for Recognition[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2013, 35 (11) :2651-2664." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Label consistent KSVD:Learning a discriminative dictionary for recognition">
                                        <b>[4]</b>
                                         Jiang Z, Lin Z, Davis L S.Label Consistent K-SVD:Learning a Discriminative Dictionary for Recognition[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2013, 35 (11) :2651-2664.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" title=" Zhang Z, Xu Y, Yang J, et al.A Survey of Sparse Representation:Algorithms and Applications[J].IEEE Access, 2015, 3:490-530." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Survey of Sparse Representation:Algorithms and Applications">
                                        <b>[5]</b>
                                         Zhang Z, Xu Y, Yang J, et al.A Survey of Sparse Representation:Algorithms and Applications[J].IEEE Access, 2015, 3:490-530.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" title=" 顾鑫, 王海涛, 汪凌峰, 等.基于不确定性度量的多特征融合跟踪[J].自动化学报, 2011, 37 (5) :550-559." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201105006&amp;v=MjcwODlqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVadEZ5amhVNzdPS0NMZlliRzRIOURNcW85RllvUUtESDg0dlI0VDY=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                         顾鑫, 王海涛, 汪凌峰, 等.基于不确定性度量的多特征融合跟踪[J].自动化学报, 2011, 37 (5) :550-559.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" title=" Zhu X K, Jing X Y, Wu F, et al.Multi-Kernel Low-Rank Dictionary Pair Learning for Multiple Features Based Image Cclassification[C]// Thirty-First AAAI Conference on Artificial Intelligence, 2017." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multi-Kernel Low-Rank Dictionary Pair Learning for Multiple Features Based Image Cclassification">
                                        <b>[7]</b>
                                         Zhu X K, Jing X Y, Wu F, et al.Multi-Kernel Low-Rank Dictionary Pair Learning for Multiple Features Based Image Cclassification[C]// Thirty-First AAAI Conference on Artificial Intelligence, 2017.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" title=" Friedman A.Framing pictures:The role of knowledge in automatized encoding and memory for gist [J].Journal of Experimental Psychology General, 1979, 108 (3) :316-355." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Framing pictures: the role of knowledge in automatized encoding and memory for gist">
                                        <b>[8]</b>
                                         Friedman A.Framing pictures:The role of knowledge in automatized encoding and memory for gist [J].Journal of Experimental Psychology General, 1979, 108 (3) :316-355.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" title=" Oliva A, Torralba A.Building the gist of a scene:the role of global image features in recognition[J].Progress in Brain Research:Visual Perception, 2006, 155:23-36." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011702289969&amp;v=MzIyMzE9TmlmT2ZiSzdIdEROcUk5SFp1TUdCWG93b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcnpJSUZvVWF4UQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                         Oliva A, Torralba A.Building the gist of a scene:the role of global image features in recognition[J].Progress in Brain Research:Visual Perception, 2006, 155:23-36.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_10" title=" Lowe D G.Distinctive image features from scale invariant key points[J].International Journal of Computer Vision, 2004, 60 (2) :91-110." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002830901&amp;v=MjI2MTBzT1kzazV6QmRoNGo5OVNYcVJyeG94Y01IN1I3cWRaK1p1Rml2bFU3dklJVmc9Tmo3QmFyTzRIdEhPcDR4RmJl&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[10]</b>
                                         Lowe D G.Distinctive image features from scale invariant key points[J].International Journal of Computer Vision, 2004, 60 (2) :91-110.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_11" title=" Lindeberg T.Scale-space for discrete signals[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 1990, 12 (3) :234-254." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Scale-space for discrete signals">
                                        <b>[11]</b>
                                         Lindeberg T.Scale-space for discrete signals[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 1990, 12 (3) :234-254.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_12" title=" Dalal N, Triggs B.Histograms of Oriented Gradients for Human Detection[C]// IEEE Computer Society Conference on Computer Vision and Pattern Recognition.IEEE, 2005:886-893." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Histograms of oriented gradients for humandetection">
                                        <b>[12]</b>
                                         Dalal N, Triggs B.Histograms of Oriented Gradients for Human Detection[C]// IEEE Computer Society Conference on Computer Vision and Pattern Recognition.IEEE, 2005:886-893.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_13" title=" Patel V M, Chellappa R.Compressive Sensing[M]// Sparse Representations and Compressive Sensing for Imaging and Vision.Springer New York, 2013:132-136." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Compressive Sensing">
                                        <b>[13]</b>
                                         Patel V M, Chellappa R.Compressive Sensing[M]// Sparse Representations and Compressive Sensing for Imaging and Vision.Springer New York, 2013:132-136.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_14" title=" Corchado E.A survey of multiple classifier systems as hybrid systems[M].Elsevier Science Publishers B.V.2014." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A survey of multiple classifier systems as hybrid systems">
                                        <b>[14]</b>
                                         Corchado E.A survey of multiple classifier systems as hybrid systems[M].Elsevier Science Publishers B.V.2014.
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_15" title=" Yang M, Zhang L.Gabor Feature Based Sparse Representation for Face Recognition with Gabor Occlusion Dictionary[C]// European Conference on Computer Vision.Springer, Berlin, Heidelberg, 2010." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Gabor feature based sparse representation for facerecognition with gabor occlusion dictionary">
                                        <b>[15]</b>
                                         Yang M, Zhang L.Gabor Feature Based Sparse Representation for Face Recognition with Gabor Occlusion Dictionary[C]// European Conference on Computer Vision.Springer, Berlin, Heidelberg, 2010.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JYRJ" target="_blank">计算机应用与软件</a>
                2019,36(07),249-253+326 DOI:10.3969/j.issn.1000-386x.2019.07.042            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于稀疏表示和决策融合的图像分类方法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%82%A8%E5%B2%B3%E4%B8%AD&amp;code=06149280&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">储岳中</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%8E%E5%AE%B6%E6%B5%A9&amp;code=42231778&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">李家浩</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BC%A0%E5%AD%A6%E9%94%8B&amp;code=06151672&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">张学锋</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%BA%AA%E6%BB%A8&amp;code=11168724&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">纪滨</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%AE%89%E5%BE%BD%E5%B7%A5%E4%B8%9A%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF%E5%AD%A6%E9%99%A2&amp;code=0255231&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">安徽工业大学计算机科学与技术学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>利用多个稀疏表示分类器融合的决策信息对图像进行分类, 可避免单个特征对图像分类的影响。提出一种自适应调节权重的多稀疏分类器融合图像分类方法。对原始图像分别提取3组不同特征, 并训练出各自稀疏表示分类器;根据各个子分类器的准确率, 通过迭代计算自适应确定各分类器最终权重;融合各子分类器的输出结果进行最终类别判断。基于Cifar-10图像数据集进行多组实验, 结果表明, 相对仅提取单特征的图像分类方法, 该方法有效提高了图像分类准确率。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">图像分类;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%86%B3%E7%AD%96%E8%9E%8D%E5%90%88&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">决策融合;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%A8%80%E7%96%8F%E8%A1%A8%E7%A4%BA&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">稀疏表示;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    储岳中, 副教授, 主研领域:模式识别。;
                                </span>
                                <span>
                                    李家浩, 硕士生。;
                                </span>
                                <span>
                                    张学锋, 教授。;
                                </span>
                                <span>
                                    纪滨, 副教授。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-12-18</p>

                    <p>

                            <b>基金：</b>
                                                        <span>安徽高校自然科学研究重大项目 (KJ2017ZD05, KJ2015ZD09);</span>
                                <span>安徽高校自然科学研究重点项目 (KJ2017A069);</span>
                    </p>
            </div>
                    <h1><b>IMAGE CLASSIFICATION METHOD BASED ON SPARSE REPRESENTATION AND DECISION FUSION</b></h1>
                    <h2>
                    <span>Chu Yuezhong</span>
                    <span>Li Jiahao</span>
                    <span>Zhang Xuefeng</span>
                    <span>Ji Bing</span>
            </h2>
                    <h2>
                    <span>School of Computer Science and Technology, Anhui University of Technology</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Using multiple sparse representation classifiers to combine decision information to classify images can avoid the impact of individual features on image classification. Thus, this paper proposed a multi-sparse classifier fusion image classification method with adaptive adjustment weights. Three sets of different features were extracted from the original image, and the respective sparse representation classifiers were trained. According to the accuracy of each sub-classifier, the final weight of each classifier was determined adaptively by iterative calculation. Finally, the output of each sub-classifier was merged for final category judgment. Multiple sets of experiments were performed based on the Cifar-10 image data set. The results show that the proposed can effectively improve the image classification accuracy, compared with the image classification method only extracting single feature.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Image%20classification&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Image classification;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Decision%20fusion&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Decision fusion;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Sparse%20representation&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Sparse representation;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-12-18</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="33" name="33" class="anchor-tag"><b>0 引 言</b></h3>
                <div class="p1">
                    <p id="34">图像分类是当前计算机视觉、深度学习和人工智能等领域的研究热点。在图像分类这一研究课题下, 文献<citation id="126" type="reference">[<a class="sup">1</a>]</citation>对该课题近些年的发展作出总结和展望, 表示多特征融合这一策略对图像分类具有非常关键的研究意义, 并且在选取特征与分类器这两个关键环节的处理对最终分类效果具有深远影响, 往往多特征伴随着高维度数据且伴有各类噪声影响。压缩感知技术研究的热潮下, 文献<citation id="127" type="reference">[<a class="sup">2</a>]</citation>提出稀疏表示下的人脸识别算法, 通过训练样本来计算稀疏系数与字典, 然后计算残差来获得最终分类的方法, 取得很高的识别率。从字典学习的角度, 文献<citation id="128" type="reference">[<a class="sup">3</a>]</citation>提出KSVD字典学习算法, 通过<i>k</i>次迭代, 每步计算SVD分解来更新字典原子与对应系数的方法, 从而获得优化的解。文献<citation id="129" type="reference">[<a class="sup">4</a>]</citation>在其基础上改进并提出LC-KSVD算法, 通过整合构建字典与优化线性分类器到重建的目标函数中, 联合获取更优化的学习字典。近些年各类基于稀疏表示的改进方法被不断提出<citation id="130" type="reference"><link href="11" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>, 但很少有多特征融合结合稀疏表示分类器展开研究的方法, 由于单一特征对图像描述不足以及分类器性能存在的差异性, 导致传统的分类方法在最后的分类效果上性能的局限性较大。多特征融合的分类算法近些年不断被提出, 文献<citation id="131" type="reference">[<a class="sup">6</a>]</citation>根据特征不确定性提出一种新度量方式去对各类特征贡献进行调整, 以保证融合算法的鲁棒性和稳定性。由于多特征融合所带来的特征维度过高以及特征表现不明确等问题, 文献<citation id="132" type="reference">[<a class="sup">7</a>]</citation>提出多内核低秩字典学习方法 (MKLDPL) , 能够有效通过对多特征进行多核字典学习。围绕图像分类这一主题, 结合多特征融合思想, 稀疏表示分类器可有效减少遮挡、旋转等因素对图像识别的影响, 为图像分类提供了新的手段。</p>
                </div>
                <div class="p1">
                    <p id="35">针对特征提取与分类器融合等问题, 本文通过自适应的融合多特征所生成的稀疏表示分类器结果, 并结合整体特征与局部轮廓特征在不同分类情景下的权重比等因素, 通过对多特征分类器融合决策的方法充分发挥各特征在不同分类情景下的最大优势。实验表明, 多特征稀疏表示分类器融合的分类方法, 较单一特征分类器的效果有显著提升, 构成了一种容错性高且鲁棒性强的多特征融合图像分类系统。</p>
                </div>
                <h3 id="36" name="36" class="anchor-tag"><b>1 多特征提取</b></h3>
                <h4 class="anchor-tag" id="37" name="37"><b>1.1 全局Gist特征提取</b></h4>
                <div class="p1">
                    <p id="38">Gist最早是由文献<citation id="133" type="reference">[<a class="sup">8</a>]</citation>提出, 而后在2006年文献<citation id="134" type="reference">[<a class="sup">9</a>]</citation>提出对图像使用多尺度多方向Gabor滤波器组进行处理的Gist特征提取方法, 全局Gist特征提取一般采用4个尺度、8个方向的Gabor滤波器组进行全局特征的提取。对图像进行全局Gist特征提取的具体步骤如下:</p>
                </div>
                <div class="p1">
                    <p id="39"> (1) 将一张大小为<i>m</i>×<i>n</i>的灰度图片划分成<i>m</i><sub><i>a</i></sub>×<i>n</i><sub><i>b</i></sub>个分块图片, 每张分块图片的大小为<i>m</i><sup>*</sup>×<i>n</i><sup>*</sup>, 其中<i>m</i><sup>*</sup>=<i>m</i>/<i>m</i><sub><i>a</i></sub>, <i>n</i><sup>*</sup>=<i>n</i>/<i>n</i><sub><i>b</i></sub>。</p>
                </div>
                <div class="p1">
                    <p id="40"> (2) 使用<i>J</i><sub><i>c</i></sub>个通道的Gabor滤波器对每块局部图像进行卷积滤波, 其中<i>J</i><sub><i>c</i></sub>个数等于滤波器尺度与方向数的乘积, 滤波产生的各网格各通道后级联的结果为该图像块Gist特征, 公式为:</p>
                </div>
                <div class="p1">
                    <p id="41"><mathml id="42"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>G</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>c</mi><mi>a</mi><mi>t</mi></mrow></mstyle><mrow><mi>J</mi><msub><mrow></mrow><mi>c</mi></msub></mrow></munder><mo stretchy="false"> (</mo><mi>f</mi><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo><mo>×</mo><mi>g</mi><msub><mrow></mrow><mrow><mi>m</mi><mi>n</mi></mrow></msub><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo></mrow></math></mathml>      (1) </p>
                </div>
                <div class="p1">
                    <p id="43">式中:<i>G</i><sub><i>i</i></sub>的维数是<i>J</i><sub><i>c</i></sub>×<i>m</i><sup>*</sup>×<i>n</i><sup>*</sup>, 由于默认尺度为4个尺度、8个方向, 所以<i>i</i>=[1, 2, …, 32]。</p>
                </div>
                <div class="p1">
                    <p id="44"> (3) 将<i>G</i><sub><i>i</i></sub> (<i>x</i>, <i>y</i>) 经滤波器处理后的特征值取均值, 再将均值化以后的特征值组合起来, 融合形成最终整张图片的Gist特征, 即:</p>
                </div>
                <div class="p1">
                    <p id="45"><mathml id="46"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>G</mi><msup><mrow></mrow><mi>g</mi></msup><mo>=</mo><mrow><mo stretchy="false">[</mo><mover accent="true"><mrow><mi>G</mi><msubsup><mrow></mrow><mn>1</mn><mi>Ρ</mi></msubsup></mrow><mo stretchy="true">¯</mo></mover></mrow><mspace width="0.25em" /><mover accent="true"><mrow><mi>G</mi><msubsup><mrow></mrow><mn>2</mn><mi>Ρ</mi></msubsup></mrow><mo stretchy="true">¯</mo></mover><mo>⋯</mo><mo>⋯</mo><mover accent="true"><mrow><mi>G</mi><msubsup><mrow></mrow><mrow><mn>3</mn><mn>2</mn></mrow><mi>Ρ</mi></msubsup></mrow><mo stretchy="true">¯</mo></mover><mo stretchy="false">]</mo></mrow></math></mathml>      (2) </p>
                </div>
                <div class="p1">
                    <p id="47">式中:<mathml id="48"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mrow><mi>G</mi><msubsup><mrow></mrow><mi>i</mi><mi>p</mi></msubsup></mrow><mo stretchy="true">¯</mo></mover><mo>=</mo><mfrac><mn>1</mn><mrow><mi>m</mi><msup><mrow></mrow><mo>*</mo></msup><mo>×</mo><mi>n</mi><mo>*</mo></mrow></mfrac><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo><mo>∈</mo><mi>Ρ</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></munder><mi>G</mi></mstyle><msubsup><mrow></mrow><mi>i</mi><mi>p</mi></msubsup><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo></mrow></math></mathml>。最终Gist特征向量<i>G</i><sup><i>g</i></sup>的维数是<i>J</i><sub><i>c</i></sub>×<i>m</i><sup>*</sup>×<i>n</i><sup>*</sup>。</p>
                </div>
                <div class="p1">
                    <p id="49">本文在提取研究目标图像的Gist特征时, 选择4×4的网格对整张图像进行划分, 即最后得到的特征维数为32×16=512维。该特征很好地保留了整张照片的全局特征信息, </p>
                </div>
                <div class="p1">
                    <p id="50">由于PCA 降维方法能够对数据进行压缩, 消除冗余和数据噪声, 尽可能保留原始数据中的重要特征信息, 最大限度地减少降维带来的损失, 所以采用PCA方法对Gist特征进行降维, 以达到最优化效果, 缩短分类时间。</p>
                </div>
                <h4 class="anchor-tag" id="51" name="51"><b>1.2 Dense-SIFT特征提取</b></h4>
                <div class="p1">
                    <p id="52">SIFT特征描述子是由Lowe<citation id="135" type="reference"><link href="21" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>于2004年提出的一种对图像缩放、旋转和仿射变换具有不变形的特征描述子, 根据Koendetink<citation id="136" type="reference"><link href="23" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>建立的高斯核为唯一线性核理论, 提取该特征首先通过对样本二维平面空间与DoG (Difference of Gaussian) 中同时检测局部极值点, DoG算子如下:</p>
                </div>
                <div class="p1">
                    <p id="53"><i>D</i> (<i>x</i>, <i>y</i>, <i>σ</i>) = (<i>G</i> (<i>x</i>, <i>y</i>, <i>kσ</i>) -<i>G</i> (<i>x</i>, <i>y</i>, <i>σ</i>) ) ×<i>I</i> (<i>x</i>, <i>y</i>) =</p>
                </div>
                <div class="p1">
                    <p id="54"><i>L</i> (<i>x</i>, <i>y</i>, <i>kσ</i>) -<i>L</i> (<i>x</i>, <i>y</i>, <i>σ</i>)      (3) </p>
                </div>
                <div class="p1">
                    <p id="55">式中:<i>L</i>代表图像的尺度空间, <i>I</i> (<i>x</i>, <i>y</i>) 代表图像 (<i>x</i>, <i>y</i>) 位置的像素值, <i>G</i> (<i>x</i>, <i>y</i>, <i>σ</i>) 为二维高斯核函数。Lowe在原论文中建议SIFT描述子使用在特征点尺度空间内4×4的窗口中计算的8个方向的梯度信息, 共4×4×8=128维向量表示, 最终各训练样本生成的特征向量为<i>m</i>×<i>n</i>, 其中<i>n</i>为图像生成的特征向量个数。</p>
                </div>
                <div class="p1">
                    <p id="56">Dense SIFT特征提取是由SIFT特征提取演变而来, 相较SIFT特征描述子更加均匀平衡地分布在图像的各区域。由于其特征点检测这一部分不同于SIFT特征, 通过对图像进行等大小的网格划分, 设定一个固定大小<i>n</i>×<i>n</i>的正方形滑动窗口, 如图1所示。按照Bin size进行左右上下滑动, 截取生成采样空间块, 生成8个方向上的梯度直方图, 然后将每个采样空间块<i>n</i>×<i>n</i>的8位维度连接起来, 形成Dense-Sift描述符。</p>
                </div>
                <div class="area_img" id="57">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201907043_057.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 Dense-SIFT滑动窗口的特征采样" src="Detail/GetImg?filename=images/JYRJ201907043_057.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 Dense-SIFT滑动窗口的特征采样  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201907043_057.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="58" name="58"><b>1.3 方向梯度直方图</b></h4>
                <div class="p1">
                    <p id="59">方向梯度直方图 (histofram orented gradient, HOG) 是由Dalal<citation id="137" type="reference"><link href="25" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>在2005年提出, 具有很强的图像特征描述能力, 对图像几何与光照的形变都具有较强的鲁棒性。HOG特征对边缘尤为敏感, 在作为融合决策中能够充分发挥局部特征对分类所产生的重要影响, 所以本文选取该特征作为多特征稀疏表示分类之一。HOG算法首先将像素大小为<i>m</i>×<i>n</i>的测试图像样本划分成大小为<i>m</i><sub><i>i</i></sub>×<i>n</i><sub><i>i</i></sub>的细胞, 并将2×2个细胞组成一个块, 水平方向和垂直方向分别按照一个细胞大小的步长为滑动窗口, 计算各像素点的梯度大小和方向, 每个细胞内按照像素的梯度方向形成的方向直方图, 方向直方图的组数<i>b</i>, 最后获得36× (<i>m</i>/<i>m</i><sub><i>i</i></sub>-1) × (<i>n</i>/<i>n</i><sub><i>i</i></sub>-1) 个特征向量, 图2中对样本图像的边缘信息进行描述。</p>
                </div>
                <div class="area_img" id="60">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201907043_060.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 图像灰度图的HOG特征对应轮廓图" src="Detail/GetImg?filename=images/JYRJ201907043_060.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 图像灰度图的HOG特征对应轮廓图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201907043_060.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="61">由图2可发现, HOG特征对样本几何变化性具有较强的鲁棒性, 对样本局部轮廓特征具有良好表现, 类似于其他特征提取生成特征向量的高维数问题, 本文同样对生成的HOG特征向量进行降维处理, 最大程度保存其特征表达的完整性。</p>
                </div>
                <h3 id="62" name="62" class="anchor-tag"><b>2 多稀疏表示分类器构成</b></h3>
                <h4 class="anchor-tag" id="63" name="63"><b>2.1 稀疏表示分类 (SRC) 方法</b></h4>
                <div class="p1">
                    <p id="64">设有<i>M</i>类训练样本, <b><i>K</i></b><sub><i>i</i></sub>=[<b><i>k</i></b><sub><i>i</i>1</sub>, <b><i>k</i></b><sub><i>i</i>2</sub>, …, <b><i>k</i></b><sub><i>in</i><sub><i>i</i></sub></sub>]∈<b><i>R</i></b><sup><i>m</i>×<i>n</i><sub><i>i</i></sub></sup>表示第<i>i</i>类训练样本所构成的样本集合, <b><i>k</i></b><sub><i>ij</i></sub>∈<b><i>R</i></b><sup><i>m</i></sup>表示第<i>i</i>类第<i>j</i>个训练样本图像向量, 其中<i>m</i>为训练样本图像向量的维数, <i>n</i><sub><i>i</i></sub>为第<i>i</i>类训练样本的个数。<b><i>D</i></b>=[<i>D</i><sub>1</sub>, <i>D</i><sub>1</sub>, …, <i>D</i><sub><i>M</i></sub>]∈<b><i>R</i></b><sup><i>m</i>×<i>n</i></sup>表示整个训练样本图像的训练样本矩阵, <i>n</i>为训练样本图像的总数, 令矩阵<b><i>D</i></b>为字典, 给定测试样本<i>y</i>可由字典<b><i>D</i></b>表示, 即<i>y</i>=<b><i>D</i></b><i>x</i>。其中, <i>y</i>可由所在类组成的图像向量线性表示, 即<i>y</i>=<b><i>k</i></b><sub><i>i</i>1</sub><i>x</i><sub><i>i</i>1</sub>+<b><i>k</i></b><sub><i>i</i>2</sub><i>x</i><sub><i>i</i>2</sub>+…+<b><i>k</i></b><sub><i>in</i></sub><i>x</i><sub><i>in</i></sub>, <i>x</i>为该测试样本对应字典<b><i>D</i></b>的稀疏表示编码, 若训练样本数量足够多, <i>y</i>对应字典<b><i>D</i></b>所生成的稀疏表示编码<i>x</i>稀疏, 由于<i>m</i>&lt;&lt;<i>n</i>, 这使得<i>y</i>=<b><i>D</i></b><i>x</i>具有无穷多解, 求解该问题可变为以下表达式:</p>
                </div>
                <div class="p1">
                    <p id="65"><mathml id="66"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>x</mi><mo>^</mo></mover><msub><mrow></mrow><mrow><mn>0</mn><mspace width="0.25em" /></mrow></msub><mo>=</mo><mtext>a</mtext><mtext>r</mtext><mtext>g</mtext><mtext>m</mtext><mtext>i</mtext><mtext>n</mtext></mrow></math></mathml>‖x‖<sub>0 </sub><i>s</i>.<i>t</i>.<b><i>D</i></b><i>x</i>=<i>y</i>      (4) </p>
                </div>
                <div class="p1">
                    <p id="67">式 (4) 为NP问题, 由文献<citation id="138" type="reference">[<a class="sup">13</a>]</citation>提出的可将<i>l</i><sub>0 </sub>范数问题转化为求解<i>l</i><sub>1</sub>范数最优化问题, 由于数据量过多且存在不同程度噪音等影响, 加入误差约束项:</p>
                </div>
                <div class="p1">
                    <p id="68"><mathml id="69"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>x</mi><mo>^</mo></mover><msub><mrow></mrow><mrow><mn>1</mn><mspace width="0.25em" /></mrow></msub><mo>=</mo><mi>a</mi><mi>r</mi><mi>g</mi><mi>m</mi><mi>i</mi><mi>n</mi><mo stretchy="false">∥</mo><mi>x</mi><mo stretchy="false">∥</mo><msub><mrow></mrow><mrow><mn>1</mn><mspace width="0.25em" /></mrow></msub><mtext> </mtext><mtext>s</mtext><mo>.</mo><mtext>t</mtext><mo>.</mo><mo stretchy="false">∥</mo><mi mathvariant="bold-italic">D</mi><mi>x</mi><mo>-</mo><mi>y</mi><mo stretchy="false">∥</mo><msub><mrow></mrow><mn>2</mn></msub><mo>≤</mo><mi>ε</mi></mrow></math></mathml>      (5) </p>
                </div>
                <div class="p1">
                    <p id="70">最后通过计算残差<mathml id="71"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>r</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false"> (</mo><mi>y</mi><mo stretchy="false">) </mo><mo>=</mo><mo stretchy="false">∥</mo><mi>y</mi><mo>-</mo><mi mathvariant="bold-italic">D</mi><mi>δ</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false"> (</mo><mover accent="true"><mi>x</mi><mo>^</mo></mover><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">) </mo><mo stretchy="false">∥</mo><msub><mrow></mrow><mn>2</mn></msub></mrow></math></mathml>, 其中<mathml id="72"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>i</mi><mo>=</mo><mn>1</mn><mo>, </mo><mn>2</mn><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>Μ</mi><mo>, </mo><mi>δ</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false"> (</mo><mover accent="true"><mi>x</mi><mo>^</mo></mover><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">) </mo><mo>=</mo><mo stretchy="false">[</mo><mn>0</mn><mo>, </mo><mo>⋯</mo><mo>, </mo><mn>0</mn><mo>, </mo><mi mathvariant="bold-italic">k</mi><msub><mrow></mrow><mrow><mi>i</mi><mn>1</mn></mrow></msub><mo>, </mo><mi mathvariant="bold-italic">k</mi><msub><mrow></mrow><mrow><mi>i</mi><mn>2</mn></mrow></msub><mo>, </mo><mo>⋯</mo><mo>, </mo><mi mathvariant="bold-italic">k</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>n</mi></mrow></msub><mo>, </mo><mn>0</mn><mo>, </mo><mo>⋯</mo><mo>, </mo><mn>0</mn><mo stretchy="false">]</mo><msup><mrow></mrow><mtext>Τ</mtext></msup></mrow></math></mathml>基于测试样本<i>y</i>的各类重构误差分类规则如下:</p>
                </div>
                <div class="p1">
                    <p id="73"><i>class</i> (<i>y</i>) =<i>argmin r</i><sub><i>i</i></sub> (<i>y</i>)      (6) </p>
                </div>
                <div class="p1">
                    <p id="74">最终基于式 (6) 的分类规则对测试图像<i>y</i>进行分类。</p>
                </div>
                <h4 class="anchor-tag" id="75" name="75"><b>2.2 多分类器权重融合</b></h4>
                <div class="p1">
                    <p id="76">通过对不同特征生成的分类器进行准确率验证, 分别构造多个稀疏分类器, 根据不同的分类器在所有特征情况下的准确率进行权重分配, 通过迭代更新不同特征分类器的权重占比, 最后通过最终决策分类器输出结果。</p>
                </div>
                <div class="p1">
                    <p id="77">文献<citation id="139" type="reference">[<a class="sup">14</a>]</citation>提出关于多分类器融合判别系统, 多分类器融合决策的关键问题是特征多样性以及决策融合方法合理性, 不同多分类器融合决策系统对不同的需求模型有不同的性能表现。基于此概念, 结合在图像分类这一多因素影响的复杂课题下, 具备单一特征信息的分类模型很难具有很良好表现, 例如全局Gist特征虽然能够在全局上很好把握整体轮廓图像特征完整性, 但对于局部特征具有较高决策权重比的分类任务下, 仅单靠全局Gist特征来对图像进行决策分类, 效果并不理想。Hog特征着重提取图像轮廓与方向梯度特征, 对表现图像局部特征有着显著的效果, 但整体旋转变化鲁棒性并不强, 在多分类器融合决策下, 能够很好应对复杂情况下的图像分类问题。</p>
                </div>
                <div class="p1">
                    <p id="78">本文通过引入自适应调节各分类器在最终决策结果的影响因子权重比, 通过各特征稀疏表示分类器对不同样本存在不同程度的准确性;通过奖惩因子∂合理地对各分类器进行自适应权重调节, ∂的参数大小选取对分类准确率也有一定影响;通过不断迭代与自适应更新参数获得最优多分类器融合决策模型。整个分类决策建模过程如图3所示。</p>
                </div>
                <div class="area_img" id="79">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201907043_079.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 基于多特征稀疏表示分类器的图像识别算法流程图" src="Detail/GetImg?filename=images/JYRJ201907043_079.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 基于多特征稀疏表示分类器的图像识别算法流程图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201907043_079.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="80">具体过程如下:</p>
                </div>
                <div class="p1">
                    <p id="81"><b>步骤1</b> 首先根据第2节提出的三种特征提取的方法分别对整个训练样本进行特征提取, 获得三组训练样本特征矩阵:</p>
                </div>
                <div class="p1">
                    <p id="82"><b><i>D</i></b><sup><i>k</i></sup>=[<b><i>D</i></b><sub>1</sub>, <b><i>D</i></b><sub>2</sub>, …, <b><i>D</i></b><sub><i>M</i></sub>]∈<b><i>R</i></b><sup><i>d</i>×<i>n</i></sup>      (7) </p>
                </div>
                <div class="p1">
                    <p id="83">式中:<i>d</i>为该特征维数, <i>M</i>为样本总类别数, <i>k</i>为第<i>k</i>类特征, <b><i>D</i></b><sub><i>i</i><sup><i>k</i></sup></sub>为第<i>i</i>类样本的第<i>k</i>类特征向量。</p>
                </div>
                <div class="p1">
                    <p id="84"><b>步骤2</b> 分别对步骤1中所产生的三种特征向量矩阵, 根据2.1节方法, 生成各自的稀疏表示分类器, 得到三种各自的稀疏表示分类结果<i>Q</i><sub><i>M</i></sub>, 通过计算得到的残差<mathml id="85"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>r</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false"> (</mo><mi>y</mi><mo stretchy="false">) </mo><mo>=</mo><mo stretchy="false">∥</mo><mi>y</mi><mo>-</mo><mi mathvariant="bold-italic">D</mi><mi>δ</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false"> (</mo><mover accent="true"><mi>x</mi><mo>^</mo></mover><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">) </mo><mo stretchy="false">∥</mo><msub><mrow></mrow><mn>2</mn></msub></mrow></math></mathml>, 提出该稀疏分类器验证类间分离程度:</p>
                </div>
                <div class="p1">
                    <p id="86"><i>P</i><mathml id="87"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>k</mi></msubsup></mrow></math></mathml>=<mathml id="88"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>z</mi><mo>=</mo><mn>1</mn></mrow><mi>Μ</mi></munderover><mrow></mrow></mstyle></mrow></math></mathml><i>r</i><sub><i>z</i></sub> (<i>y</i>) -<i>r</i><sub><i>i</i></sub> (<i>y</i>)      (8) </p>
                </div>
                <div class="p1">
                    <p id="89">式中:<i>P</i><mathml id="90"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>k</mi></msubsup></mrow></math></mathml>代表第<i>k</i>类分类器的第<i>i</i>类样本与整个训练样本其他类别之间的分离程度, 分离程度越大, 代表该分类器在该训练样本上的分类效果越明显。</p>
                </div>
                <div class="p1">
                    <p id="91"><b>步骤3</b> 构造分配各稀疏表示分类器初始权重, 按照首次各子分类器输出的分离程度<i>P</i>, 获得初始化特征权重系数为:</p>
                </div>
                <div class="p1">
                    <p id="92"><i>W</i><sup><i>i</i></sup>=<mathml id="93"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mi>max</mi><mo stretchy="false">{</mo><mi>Ρ</mi><msubsup><mrow></mrow><mi>j</mi><mi>i</mi></msubsup><mo stretchy="false">}</mo></mrow><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mi>k</mi></munder><mi>Ρ</mi></mstyle><msup><mrow></mrow><mi>k</mi></msup></mrow></mfrac></mrow></math></mathml>      (9) </p>
                </div>
                <div class="p1">
                    <p id="94"><i>W</i><sup><i>i</i></sup>代表第<i>i</i>类稀疏表示分类所分配的初始化权重比。</p>
                </div>
                <div class="p1">
                    <p id="95"><b>步骤4</b> 融合各加权稀疏表示分类器的分类结果, 确定图像所属类别:</p>
                </div>
                <div class="p1">
                    <p id="96"><i>w</i><sub><i>j</i></sub>=<mathml id="97"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mi>L</mi><msub><mrow></mrow><mi>j</mi></msub></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>Η</mi></munderover><mi>L</mi></mstyle><msub><mrow></mrow><mi>j</mi></msub></mrow></mfrac></mrow></math></mathml>      (10) </p>
                </div>
                <div class="p1">
                    <p id="98">式中:<i>L</i><sub><i>j</i></sub>=<mathml id="99"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>Μ</mi></munderover><mrow></mrow></mstyle></mrow></math></mathml><i>W</i><sup><i>i</i></sup><i>P</i><mathml id="100"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>k</mi><mi>i</mi></msubsup></mrow></math></mathml>, <i>j</i>=1, 2, …, <i>M</i>为第<i>j</i>个分类器在多分类器融合决策影响下对所有样本所占据的影响比重值, 由步骤2得到的三种分类结果<i>Q</i><sub><i>K</i></sub>和其权重比<i>w</i><sub><i>j</i></sub>进行线性加权, 最终获得分类结果如下:</p>
                </div>
                <div class="p1">
                    <p id="101"><i>class</i> (<i>y</i>) =argmax<mathml id="102"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>j</mi></munderover><mrow></mrow></mstyle></mrow></math></mathml><i>w</i><sub><i>i</i></sub><i>Q</i><sub><i>i</i></sub>      (11) </p>
                </div>
                <div class="p1">
                    <p id="103"><b>步骤5</b> 根据测试样本的验证数据不断迭代各分类器的<i>W</i><sup><i>i</i></sup>, 本文通过引入奖惩因子∂对各分类器对样本所存在的合理权重比进行调整, 各分类器对自身权重分别抽取出<mathml id="104"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mo>∂</mo><mn>3</mn></mfrac></mrow></math></mathml>的权重作为总奖惩因子, 之后对验证正确的分类器数量<i>i</i>, 分别分配<mathml id="105"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mo>∂</mo><mi>i</mi></mfrac></mrow></math></mathml>的权重。若所有分类器均未正确检测出该样本, 则抛弃, 对所有训练样本进行以上步骤操作, 整个多稀疏表示分类器融合模型训练完毕。</p>
                </div>
                <h3 id="106" name="106" class="anchor-tag"><b>3 实验及结果分析</b></h3>
                <h4 class="anchor-tag" id="107" name="107"><b>3.1 实验环境</b></h4>
                <div class="p1">
                    <p id="108">仿真环境为Windows 7操作系统, Intel Core i7 CPU, 内存16 GB, 编程环境为MATLAB 2016a。</p>
                </div>
                <div class="p1">
                    <p id="109">为了验证本文方法相较单个特征提取之后的分类效果有较高提升, 基于cifar-10图像数据数据集进行验证实验, 本实验所有数据来自CIFAR-10数据集。该数据集共有60 000幅样本图样, 样本大小均为32×32, 像素分为10类, 每类6 000幅图。这里训练样本集分5批, 每批样本集包括带标签的样本10 000幅, 另剩下10 000幅作为一批用于测试样本集。由于该数据集中样本均为彩色图片, 本文进行所有实验之前, 先进行预处理, 均对图片进行了灰度化与归一化处理, 转换为灰度图进行特征提取, 如图4所示。</p>
                </div>
                <div class="area_img" id="110">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201907043_110.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 Cifar-10数据库部分预处理后的图像" src="Detail/GetImg?filename=images/JYRJ201907043_110.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 Cifar-10数据库部分预处理后的图像  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201907043_110.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="111" name="111"><b>3.2 单个特征下的图像分类对比实验</b></h4>
                <div class="p1">
                    <p id="112">验证根据Gist、Dense-SIFT和HOG三种特征, 分别对其结合各自生成的稀疏表示器进行单一决策与联合决策的图像分类实验。通过对比单一特征的稀疏表示分类器在不同图集的正确率, 来显示不同图像分类任务下各特征对分类结果有不同的表现。</p>
                </div>
                <div class="p1">
                    <p id="113">表1显示了不同特征生成的稀疏表示分类器在各测试样本图集上的识别正确率。由表1可以明显发现不同特征所生成的稀疏表示分类在不同子集上的识别率有着明显的差异, 由此可看出, 仅对图像进行单一的特征提取并不具备良好的分类能力。其中Gist特征稀疏表示分类相比HOG特征稀疏表示分类, 在分类准确率上有3%～5%的提升, 而Dense-SIFT特征描述子所生成的稀疏表示分类从识别率上明显优于其他两种, Dense-SIFT特征更加注重对全局特征信息的均衡表示。</p>
                </div>
                <div class="area_img" id="114">
                    <p class="img_tit"><b>表1 单特征稀疏表示分类器在不同训练集识别率</b> % <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="114" border="1"><tr><td><br />集</td><td>HOG</td><td>GIST</td><td>Dense-SIFT</td></tr><tr><td><br />子集1</td><td>50.49</td><td>53.42</td><td>57.3</td></tr><tr><td><br />子集2</td><td>49.56</td><td>54.11</td><td>55.45</td></tr><tr><td><br />子集3</td><td>47.32</td><td>56.29</td><td>53.78</td></tr><tr><td><br />子集4</td><td>51.23</td><td>52.77</td><td>59.58</td></tr><tr><td><br />子集5</td><td>49.68</td><td>53.47</td><td>54.75</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="115">由第2.2节提出的融合决策方法, 对各特征进行自适应权重融合后, 不同特征在不同训练集合上图像识别率以及权重比如表2所示。</p>
                </div>
                <div class="area_img" id="116">
                    <p class="img_tit"><b>表2 本文方法在不同训练集下的各特征权重与识别率</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="116" border="1"><tr><td><br />集</td><td>HOG</td><td>GIST</td><td>Dense-SIFT</td><td>准确率</td></tr><tr><td><br />子集1</td><td>0.315</td><td>0.358</td><td>0.327</td><td>0.69</td></tr><tr><td><br />子集2</td><td>0.281</td><td>0.335</td><td>0.384</td><td>0.72</td></tr><tr><td><br />子集3</td><td>0.296</td><td>0.362</td><td>0.342</td><td>0.73</td></tr><tr><td><br />子集4</td><td>0.309</td><td>0.338</td><td>0.353</td><td>0.68</td></tr><tr><td><br />子集5</td><td>0.286</td><td>0.325</td><td>0.389</td><td>0.70</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="117">由表2可看出, Gist特征与Dense-SIFT所占比重较大, 通过结合三类特征各自的稀疏表示分类器的分类结果联合决策, 图像识别的准确率在五个不同图集上有着不同程度的提升, 通过调整奖惩因子∂的参数权重比, 来观测其对准确率的影响。如图5所示。</p>
                </div>
                <div class="area_img" id="118">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201907043_118.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 奖惩因子变化所对应分类准确率" src="Detail/GetImg?filename=images/JYRJ201907043_118.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 奖惩因子变化所对应分类准确率  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201907043_118.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="119">由图5可以看出, 奖惩因子∂的数值变化对准确率也有着很大影响, 通过实验可以证明在验证图集的分类任务下, 以因子∂=5可让整个联合决策分类器效果达到最佳。</p>
                </div>
                <h4 class="anchor-tag" id="120" name="120"><b>3.3 基于不同分类器的分类对比实验</b></h4>
                <div class="p1">
                    <p id="121">为了验证本文算法相较单一决策分类器的准确率有明显差距, 选取SVM、GSRC<citation id="140" type="reference"><link href="31" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>、KNN、与本文方法在不同图集上进行验证比较, 结果如表3所示。</p>
                </div>
                <div class="area_img" id="122">
                    <p class="img_tit"><b>表3 本文方法与其他分类器方法在不同图集准确率</b> % <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="122" border="1"><tr><td><br />集</td><td>KNN</td><td>SVM</td><td>GSRC</td><td>本文方法</td></tr><tr><td><br />子集1</td><td>23.23</td><td>42.37</td><td>50.31</td><td>69.23</td></tr><tr><td><br />子集2</td><td>24.51</td><td>44.21</td><td>45.63</td><td>72.52</td></tr><tr><td><br />子集3</td><td>25.43</td><td>43.45</td><td>47.52</td><td>73.48</td></tr><tr><td><br />子集4</td><td>24.92</td><td>40.79</td><td>52.61</td><td>68.69</td></tr><tr><td><br />子集5</td><td>22.56</td><td>42.56</td><td>50.76</td><td>70.71</td></tr><tr><td><br />均值</td><td>24.13</td><td>42.67</td><td>49.36</td><td>70.92</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="123">由表3可看出, 仅提取单特征产生的稀疏表示分类器进行分类决策的实验与基于不同分类器在不同子集的实验作对比, 传统KNN分类器分类准确率仅为0.23至0.25之间, 本文方法相对SVM与GSRC分类器的分类效果与分类精度有着显著的提升。从以上基于单特征下与不同分类器下的2组对比实验, 可以发现, 通过使用多稀疏表示分类的融合决策模型进行图像分类, 相比单一特征的图像分类方法表现出了更好的识别效果。</p>
                </div>
                <h3 id="124" name="124" class="anchor-tag"><b>4 结 语</b></h3>
                <div class="p1">
                    <p id="125">本文提出了一种多稀疏表示分类器融合的图像分类方法, 通过构造3种特征所生成的稀疏表示分类器的融合决策模型, 并自适应调整每个稀疏表示分类之间的权重, 最后通过线性加权的方式对图像进行分类。实验数据表明, 本文方法较好地克服了单特征存在的特征局限性以及不同特征对不同样本分类情况下的影响权重比不同的问题。在接下来的研究当中, 主要还会对选择组成多稀疏表示分类器的特征这一方向从深度学习与字典学习等方向寻找图像分类更佳的解决方案。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJX201406001&amp;v=MDY5MDc0SDlYTXFZOUZaWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnRGeWpoVTc3T0x6N0Jkckc=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> 黄凯奇, 任伟强, 谭铁牛.图像物体分类与检测算法综述[J].计算机学报, 2014, 36 (6) :1-18.
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" >
                                    <b>[2]</b>
                                 Wright J, Yang A Y, Sastry S S, et al.Robust Face Recognition via Sparse Representation[J].IEEE Trans Pattern Anal Mach Intell, 2009, 31 (2) :210-227.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=-SVD: An Algorithm for Designing Overcomplete Dictionaries for Sparse Representation">

                                <b>[3]</b> Aharon M, Elad M, Bruckstein A.K-SVD:An Algorithm for Designing Overcomplete Dictionaries for Sparse Representation[J].IEEE Transactions on Signal Processing, 2006, 54 (11) :4311-4322.
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Label consistent KSVD:Learning a discriminative dictionary for recognition">

                                <b>[4]</b> Jiang Z, Lin Z, Davis L S.Label Consistent K-SVD:Learning a Discriminative Dictionary for Recognition[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2013, 35 (11) :2651-2664.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Survey of Sparse Representation:Algorithms and Applications">

                                <b>[5]</b> Zhang Z, Xu Y, Yang J, et al.A Survey of Sparse Representation:Algorithms and Applications[J].IEEE Access, 2015, 3:490-530.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201105006&amp;v=MjM0MTFNcW85RllvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVadEZ5amhVNzdPS0NMZlliRzRIOUQ=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b> 顾鑫, 王海涛, 汪凌峰, 等.基于不确定性度量的多特征融合跟踪[J].自动化学报, 2011, 37 (5) :550-559.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multi-Kernel Low-Rank Dictionary Pair Learning for Multiple Features Based Image Cclassification">

                                <b>[7]</b> Zhu X K, Jing X Y, Wu F, et al.Multi-Kernel Low-Rank Dictionary Pair Learning for Multiple Features Based Image Cclassification[C]// Thirty-First AAAI Conference on Artificial Intelligence, 2017.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Framing pictures: the role of knowledge in automatized encoding and memory for gist">

                                <b>[8]</b> Friedman A.Framing pictures:The role of knowledge in automatized encoding and memory for gist [J].Journal of Experimental Psychology General, 1979, 108 (3) :316-355.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011702289969&amp;v=MTc5NTVPZmJLN0h0RE5xSTlIWnVNR0JYb3dvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp0RmlubFVyeklJRm9VYXhRPU5pZg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b> Oliva A, Torralba A.Building the gist of a scene:the role of global image features in recognition[J].Progress in Brain Research:Visual Perception, 2006, 155:23-36.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_10" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002830901&amp;v=MjkwNTB2SUlWZz1OajdCYXJPNEh0SE9wNHhGYmVzT1kzazV6QmRoNGo5OVNYcVJyeG94Y01IN1I3cWRaK1p1Rml2bFU3&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[10]</b> Lowe D G.Distinctive image features from scale invariant key points[J].International Journal of Computer Vision, 2004, 60 (2) :91-110.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Scale-space for discrete signals">

                                <b>[11]</b> Lindeberg T.Scale-space for discrete signals[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 1990, 12 (3) :234-254.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Histograms of oriented gradients for humandetection">

                                <b>[12]</b> Dalal N, Triggs B.Histograms of Oriented Gradients for Human Detection[C]// IEEE Computer Society Conference on Computer Vision and Pattern Recognition.IEEE, 2005:886-893.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Compressive Sensing">

                                <b>[13]</b> Patel V M, Chellappa R.Compressive Sensing[M]// Sparse Representations and Compressive Sensing for Imaging and Vision.Springer New York, 2013:132-136.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A survey of multiple classifier systems as hybrid systems">

                                <b>[14]</b> Corchado E.A survey of multiple classifier systems as hybrid systems[M].Elsevier Science Publishers B.V.2014.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Gabor feature based sparse representation for facerecognition with gabor occlusion dictionary">

                                <b>[15]</b> Yang M, Zhang L.Gabor Feature Based Sparse Representation for Face Recognition with Gabor Occlusion Dictionary[C]// European Conference on Computer Vision.Springer, Berlin, Heidelberg, 2010.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JYRJ201907043" />
        <input id="dpi" type="hidden" value="600" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201907043&amp;v=MjQxOTlLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnRGeWpoVTc3T0x6VFpaTEc0SDlqTXFJOUJaNFE=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
