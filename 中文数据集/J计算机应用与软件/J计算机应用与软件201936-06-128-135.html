<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637135647368381250%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJYRJ201906027%26RESULT%3d1%26SIGN%3djYog8tVKOlTkSji4vy0pr0M7zBc%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JYRJ201906027&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JYRJ201906027&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201906027&amp;v=Mjc2Njk5ak1xWTlIWTRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVp0Rnl2bVY3ckpMelRaWkxHNEg=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#51" data-title="&lt;b&gt;0 引 言&lt;/b&gt; "><b>0 引 言</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#60" data-title="&lt;b&gt;1 相关工作&lt;/b&gt; "><b>1 相关工作</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#62" data-title="&lt;b&gt;2 屏幕区域边缘检测网络&lt;/b&gt; "><b>2 屏幕区域边缘检测网络</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#65" data-title="&lt;b&gt;2.1 网络结构&lt;/b&gt;"><b>2.1 网络结构</b></a></li>
                                                <li><a href="#73" data-title="&lt;b&gt;2.2 损失函数设计&lt;/b&gt;"><b>2.2 损失函数设计</b></a></li>
                                                <li><a href="#99" data-title="&lt;b&gt;2.3 训练数据&lt;/b&gt;"><b>2.3 训练数据</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#101" data-title="&lt;b&gt;3 屏幕区域边缘定位算法&lt;/b&gt; "><b>3 屏幕区域边缘定位算法</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#104" data-title=" (1) 屏幕区域边缘直线检测"> (1) 屏幕区域边缘直线检测</a></li>
                                                <li><a href="#116" data-title=" (2) 产生候选框"> (2) 产生候选框</a></li>
                                                <li><a href="#120" data-title=" (3) 候选框评分方法"> (3) 候选框评分方法</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#128" data-title="&lt;b&gt;4 实 验&lt;/b&gt; "><b>4 实 验</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#130" data-title="&lt;b&gt;4.1 不同的网络大小&lt;/b&gt;"><b>4.1 不同的网络大小</b></a></li>
                                                <li><a href="#137" data-title="&lt;b&gt;4.2 不同的loss 函数&lt;/b&gt;"><b>4.2 不同的loss 函数</b></a></li>
                                                <li><a href="#144" data-title="&lt;b&gt;4.3 相关方法对比实验&lt;/b&gt;"><b>4.3 相关方法对比实验</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#148" data-title="&lt;b&gt;5 结 语&lt;/b&gt; "><b>5 结 语</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#54" data-title="图1 输入图片 (左) 屏幕区域 (右) ">图1 输入图片 (左) 屏幕区域 (右) </a></li>
                                                <li><a href="#64" data-title="图2 屏幕区域边缘图像">图2 屏幕区域边缘图像</a></li>
                                                <li><a href="#68" data-title="图3 本文采用的网络结构">图3 本文采用的网络结构</a></li>
                                                <li><a href="#103" data-title="图4 边缘定位算法整体流程图">图4 边缘定位算法整体流程图</a></li>
                                                <li><a href="#118" data-title="图5 产生候选框集合的流程">图5 产生候选框集合的流程</a></li>
                                                <li><a href="#132" data-title="&lt;b&gt;表1 三种不同的网络大小&lt;/b&gt;"><b>表1 三种不同的网络大小</b></a></li>
                                                <li><a href="#134" data-title="&lt;b&gt;表2 三种网络的性能&lt;/b&gt;"><b>表2 三种网络的性能</b></a></li>
                                                <li><a href="#136" data-title="图6 三种不同网络的结果">图6 三种不同网络的结果</a></li>
                                                <li><a href="#142" data-title="图7 两种不同loss的结果">图7 两种不同loss的结果</a></li>
                                                <li><a href="#147" data-title="&lt;b&gt;表3 不同方法在屏幕检测上的性能对比&lt;/b&gt;"><b>表3 不同方法在屏幕检测上的性能对比</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="244">


                                    <a id="bibliography_1" title=" Kittler J.On the accuracy of the Sobel edge detector[J].Image &amp;amp; Vision Computing, 1983, 1 (1) :37-42." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=On the accuracy of the Sobel edge detector">
                                        <b>[1]</b>
                                         Kittler J.On the accuracy of the Sobel edge detector[J].Image &amp;amp; Vision Computing, 1983, 1 (1) :37-42.
                                    </a>
                                </li>
                                <li id="246">


                                    <a id="bibliography_2" title=" Canny J.A computational approach to edge detection[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 1986, 8 (6) :679-698." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A computational approach to edge detection">
                                        <b>[2]</b>
                                         Canny J.A computational approach to edge detection[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 1986, 8 (6) :679-698.
                                    </a>
                                </li>
                                <li id="248">


                                    <a id="bibliography_3" title=" Long J, Shelhamer E, Darrell T.Fully convolutional networks for semantic segmentation[C]//IEEE Conference on Computer Vision and Pattern Recognition, 2015." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fully convolutional netw orks for semantic segmentation">
                                        <b>[3]</b>
                                         Long J, Shelhamer E, Darrell T.Fully convolutional networks for semantic segmentation[C]//IEEE Conference on Computer Vision and Pattern Recognition, 2015.
                                    </a>
                                </li>
                                <li id="250">


                                    <a id="bibliography_4" title=" Arbelaez P, Maire M, Fowlkes C, et al.Contour Detection and Hierarchical Image Segmentation[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2011, 33 (5) :898-916." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Contour Detection and Hierarchical Image Segmentation">
                                        <b>[4]</b>
                                         Arbelaez P, Maire M, Fowlkes C, et al.Contour Detection and Hierarchical Image Segmentation[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2011, 33 (5) :898-916.
                                    </a>
                                </li>
                                <li id="252">


                                    <a id="bibliography_5" title=" Martin D R, Fowlkes C C, Malik J.Learning to detect natural image boundaries using local brightness, color, and texture cues[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2004, 26 (5) :530-549." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning to detect natural image boundaries using local brightness, color, and texture cues">
                                        <b>[5]</b>
                                         Martin D R, Fowlkes C C, Malik J.Learning to detect natural image boundaries using local brightness, color, and texture cues[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2004, 26 (5) :530-549.
                                    </a>
                                </li>
                                <li id="254">


                                    <a id="bibliography_6" title=" Dollar P, Tu Z, Belongie S.Supervised learning of edges and object boundaries[C]//IEEE Conference on Computer Vision and Pattern Recognition, 2006." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Supervised Learning of Edges and Object Boundaries">
                                        <b>[6]</b>
                                         Dollar P, Tu Z, Belongie S.Supervised learning of edges and object boundaries[C]//IEEE Conference on Computer Vision and Pattern Recognition, 2006.
                                    </a>
                                </li>
                                <li id="256">


                                    <a id="bibliography_7" title=" Ren X.Multi-scale improves boundary detection in natural images[C]//European Conference on Computer Vision, 2008." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multi-Scale Improves Boundary Detection in Natural Images">
                                        <b>[7]</b>
                                         Ren X.Multi-scale improves boundary detection in natural images[C]//European Conference on Computer Vision, 2008.
                                    </a>
                                </li>
                                <li id="258">


                                    <a id="bibliography_8" title=" Dollar P, Zitnick C L.Fast edge detection using struc-tured forests[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2015, 13 (7) :120-135." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fast edge detection using structured forests">
                                        <b>[8]</b>
                                         Dollar P, Zitnick C L.Fast edge detection using struc-tured forests[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2015, 13 (7) :120-135.
                                    </a>
                                </li>
                                <li id="260">


                                    <a id="bibliography_9" title=" Ganin Y, Lempitsky V.N&lt;sup&gt;4&lt;/sup&gt;-fields:Neural network nearest neighbor fields for image transforms[C]// Asian Conference on Computer Vision, 2014." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=N^ 4-Fields:Neural Network Nearest Neighbor Fields for Image Transforms">
                                        <b>[9]</b>
                                         Ganin Y, Lempitsky V.N&lt;sup&gt;4&lt;/sup&gt;-fields:Neural network nearest neighbor fields for image transforms[C]// Asian Conference on Computer Vision, 2014.
                                    </a>
                                </li>
                                <li id="262">


                                    <a id="bibliography_10" title=" Bertasius G, Shi J, Torresani L.DeepEdge:A Multi-Scale Bifurcated Deep Network for Top-Down Contour Detection[C]//IEEE Conference on Computer Vision and Pattern Recognition, 2015." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=DeepEdge:A multi-scale bifurcated deep network for top-down contour detection">
                                        <b>[10]</b>
                                         Bertasius G, Shi J, Torresani L.DeepEdge:A Multi-Scale Bifurcated Deep Network for Top-Down Contour Detection[C]//IEEE Conference on Computer Vision and Pattern Recognition, 2015.
                                    </a>
                                </li>
                                <li id="264">


                                    <a id="bibliography_11" title=" Hu H, Lan S, Jiang Y, et al.FastMask:Segment Multi-scale Object Candidates in One Shot[C]//IEEE Conference on Computer Vision and Pattern Recognition, 2017." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fast Mask segment multi-scale object candidates in one shot">
                                        <b>[11]</b>
                                         Hu H, Lan S, Jiang Y, et al.FastMask:Segment Multi-scale Object Candidates in One Shot[C]//IEEE Conference on Computer Vision and Pattern Recognition, 2017.
                                    </a>
                                </li>
                                <li id="266">


                                    <a id="bibliography_12" title=" Liu S, Jia J, Fidler S, et al.SGN:Sequential Grouping Networks for Instance Segmentation[C]// 2017 IEEE International Conference on Computer Vision (ICCV) .IEEE, 2017." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=SGN:Sequential Grouping Networks for Instance Segmentation">
                                        <b>[12]</b>
                                         Liu S, Jia J, Fidler S, et al.SGN:Sequential Grouping Networks for Instance Segmentation[C]// 2017 IEEE International Conference on Computer Vision (ICCV) .IEEE, 2017.
                                    </a>
                                </li>
                                <li id="268">


                                    <a id="bibliography_13" title=" Xie S, Tu Z.Holistically-Nested Edge Detection[C]//IEEE International Conference on Computer Vision, 2015." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Holistically-nested edge detection">
                                        <b>[13]</b>
                                         Xie S, Tu Z.Holistically-Nested Edge Detection[C]//IEEE International Conference on Computer Vision, 2015.
                                    </a>
                                </li>
                                <li id="270">


                                    <a id="bibliography_14" title=" Liu Y, Cheng M, Hu X, et al.Richer Convolutional Features for Edge Detection[C]//IEEE Conference on Computer Vision and Pattern Recognition, 2017." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Richer Convolutional Features for Edge Detection">
                                        <b>[14]</b>
                                         Liu Y, Cheng M, Hu X, et al.Richer Convolutional Features for Edge Detection[C]//IEEE Conference on Computer Vision and Pattern Recognition, 2017.
                                    </a>
                                </li>
                                <li id="272">


                                    <a id="bibliography_15" title=" Yu Z, Feng C, Liu M, et al.CASENet:Deep Category-Aware Semantic Edge Detection[C]//IEEE Conference on Computer Vision and Pattern Recognition, 2017." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=CASENet:Deep Category-Aware Semantic Edge Detection">
                                        <b>[15]</b>
                                         Yu Z, Feng C, Liu M, et al.CASENet:Deep Category-Aware Semantic Edge Detection[C]//IEEE Conference on Computer Vision and Pattern Recognition, 2017.
                                    </a>
                                </li>
                                <li id="274">


                                    <a id="bibliography_16" title=" Caelles S, Maninis K K, Pont-Tuset J, et al.One-Shot Video Object Segmentation[C]// 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) .IEEE, 2017." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=One-Shot Video Object Segmentation">
                                        <b>[16]</b>
                                         Caelles S, Maninis K K, Pont-Tuset J, et al.One-Shot Video Object Segmentation[C]// 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) .IEEE, 2017.
                                    </a>
                                </li>
                                <li id="276">


                                    <a id="bibliography_17" title=" Bertasius G, Shi J, Torresani L.Semantic segmentation with boundary neural fields[C]//IEEE Conference on Computer Vision and Pattern Recognition, 2016." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Semantic segmentation with boundary neural fields">
                                        <b>[17]</b>
                                         Bertasius G, Shi J, Torresani L.Semantic segmentation with boundary neural fields[C]//IEEE Conference on Computer Vision and Pattern Recognition, 2016.
                                    </a>
                                </li>
                                <li id="278">


                                    <a id="bibliography_18" title=" Cheng J, Liu S, Tsai Y, et al.Learning to Segment Instances in Videos with Spatial Propagation Network[C]//IEEE Conference on Computer Vision and Pattern Recognition, 2017." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning to Segment Instances in Videos with Spatial Propagation Network">
                                        <b>[18]</b>
                                         Cheng J, Liu S, Tsai Y, et al.Learning to Segment Instances in Videos with Spatial Propagation Network[C]//IEEE Conference on Computer Vision and Pattern Recognition, 2017.
                                    </a>
                                </li>
                                <li id="280">


                                    <a id="bibliography_19" title=" Harley A W, Derpanis K G, Kokkinos I.Segmentation-Aware Convolutional Networks Using Local Attention Masks [C]//IEEE International Conference on Computer Vision, 2017." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Segmentation-Aware Convolutional Networks Using Local Attention Masks">
                                        <b>[19]</b>
                                         Harley A W, Derpanis K G, Kokkinos I.Segmentation-Aware Convolutional Networks Using Local Attention Masks [C]//IEEE International Conference on Computer Vision, 2017.
                                    </a>
                                </li>
                                <li id="282">


                                    <a id="bibliography_20" title=" Li K, Wang J, Wang H, et al.Structuring Lecture Videos by Automatic Projection Screen Localization and Analysis[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2015, 37 (6) :1233-1246." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Structuring Lecture Videos by Automatic Projection Screen Localization and Analysis">
                                        <b>[20]</b>
                                         Li K, Wang J, Wang H, et al.Structuring Lecture Videos by Automatic Projection Screen Localization and Analysis[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2015, 37 (6) :1233-1246.
                                    </a>
                                </li>
                                <li id="284">


                                    <a id="bibliography_21" title=" He K, Gkioxari G, Dollar P, et al.Mask r-cnn[C]//IEEE International Conference on Computer Vision, 2017." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Mask R-CNN">
                                        <b>[21]</b>
                                         He K, Gkioxari G, Dollar P, et al.Mask r-cnn[C]//IEEE International Conference on Computer Vision, 2017.
                                    </a>
                                </li>
                                <li id="286">


                                    <a id="bibliography_22" title=" Bradski G R.Computer vision face tracking for use in a perceptual user interface[C]// IEEE Workshop Applications of Computer Vision, 1998:214-219." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Computer vision face tracking as a component of a perceptual user interface">
                                        <b>[22]</b>
                                         Bradski G R.Computer vision face tracking for use in a perceptual user interface[C]// IEEE Workshop Applications of Computer Vision, 1998:214-219.
                                    </a>
                                </li>
                                <li id="288">


                                    <a id="bibliography_23" title=" Grabner H, Grabner M, Bischof H.Real-time tracking via online boosting[C]//Proceedings of the British Machine Vision Conference, 2006." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Real-time tracking via online boosting">
                                        <b>[23]</b>
                                         Grabner H, Grabner M, Bischof H.Real-time tracking via online boosting[C]//Proceedings of the British Machine Vision Conference, 2006.
                                    </a>
                                </li>
                                <li id="290">


                                    <a id="bibliography_24" title=" Babenko B, Yang M, Belongie S.Visual tracking with online multiple instance learning[C]//IEEE Conference on Computer Vision and Pattern Recognition, 2009." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Visual tracking with online multiple instance learning">
                                        <b>[24]</b>
                                         Babenko B, Yang M, Belongie S.Visual tracking with online multiple instance learning[C]//IEEE Conference on Computer Vision and Pattern Recognition, 2009.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JYRJ" target="_blank">计算机应用与软件</a>
                2019,36(06),128-135 DOI:10.3969/j.issn.1000-386x.2019.06.026            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于全卷积神经网络的屏幕区域定位算法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E4%BB%98%E6%B3%BD%E4%BC%9F&amp;code=41979459&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">付泽伟</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%87%91%E5%9F%8E&amp;code=17270532&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">金城</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%A4%8D%E6%97%A6%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E6%8A%80%E6%9C%AF%E5%AD%A6%E9%99%A2&amp;code=0075855&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">复旦大学计算机科学技术学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>很多情况下, 人们需要记录屏幕、投影仪中出现的信息, 但是在拍摄到屏幕的同时不可避免地会拍摄到屏幕外的背景。为了解决这个问题, 提出一种在手机等便携设备上找到拍摄视频中出现的屏幕区域的算法。提取出视频中的每一帧;对每一帧用全卷积神经网络得到屏幕边缘图像和屏幕位置图像;在屏幕边缘图像上用直线检测算法检测直线;对屏幕位置图像进行分析, 从检测到的直线中找到四条直线作为屏幕区域的边缘。由于全卷积神经网络的加入, 该方法不需要设定复杂的参数, 而且便于扩展到名片、文档等检测上。实验结果表明, 该方法拥有很强的鲁棒性, 较好的识别速度和准确率。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%85%A8%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">全卷积神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%BE%B9%E7%BC%98%E6%A3%80%E6%B5%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">边缘检测;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%B1%8F%E5%B9%95%E5%AE%9A%E4%BD%8D&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">屏幕定位;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度学习;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    付泽伟, 硕士生, 主研领域:多媒体信息处理。;
                                </span>
                                <span>
                                    金城, 教授。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-10-30</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家重点研发计划项目 (2016YFC0801003);</span>
                                <span>上海市科技人才计划项目 (17XD1425000);</span>
                    </p>
            </div>
                    <h1><b>A SCREEN AREA LOCATION ALGORITHM BASED ON FULLY CONVOLUTIONAL NETWORK</b></h1>
                    <h2>
                    <span>Fu Zewei</span>
                    <span>Jin Cheng</span>
            </h2>
                    <h2>
                    <span>School of Computer Science, Fudan University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>In many cases, people need to record the information appearing in the screen and projector, but when they shoot the screen, they inevitably shoot the background outside the screen. In order to solve this problem, we proposed an algorithm to find the screen area in the shooting video on mobile phones and other portable devices. We extracted each frame in the video and used full convolution neural network to get the edge image and position image of the screen for each frame. Then the straight line detection algorithm was used to detect the line on the screen edge image, and we analyzed the position image of the screen. Four lines were found from the detected line as the edge of the screen area. Due to the addition of full convolution neural network, this method does not need to set complex parameters, and is easy to extend to business card, document and other detection. The experimental results show that the method has strong robustness, good recognition speed and accuracy.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Fully%20convolutional%20network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Fully convolutional network;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Edge%20detection&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Edge detection;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Screen%20location&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Screen location;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Deep%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Deep learning;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-10-30</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="51" name="51" class="anchor-tag"><b>0 引 言</b></h3>
                <div class="p1">
                    <p id="52">随着科技的进步手机等便携设备的计算能力不断增强, 拥有摄像头的移动设备也越来越普及, 应用这些设备可以很方便地进行拍照和摄像。人们经常需要利用手机等便携设备记录屏幕中播放的视频信息, 但是在拍摄到屏幕的同时不可避免地会拍摄到屏幕外的背景, 这些背景对后续的视频处理会带来很大的干扰。</p>
                </div>
                <div class="p1">
                    <p id="53">定位屏幕区域的目标是在图片或视频中, 找出屏幕边缘的四条直线, 如图1所示。</p>
                </div>
                <div class="area_img" id="54">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201906027_054.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 输入图片 (左) 屏幕区域 (右)" src="Detail/GetImg?filename=images/JYRJ201906027_054.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 输入图片 (左) 屏幕区域 (右)   <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201906027_054.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="55">实际上想要准确地找到屏幕边缘的四条直线很困难, 传统方法先用canny边缘检测算法得到视频中物体的边缘, 然后用HoughLine直线检测算法从边缘中检测直线, 但是传统方法只能在背景简单的情况下使用。真实的场景下, 屏幕外的背景复杂, 屏幕区域的边缘本来应该是由四条直线组成, 但是传统方法找到的边缘直线经常是多条长短不一的直线并且直线间也存在宽窄不等的间隙。此外, 传统方法也面临着屏幕区域外的背景和屏幕区域内的图像中的直线干扰等问题。因此, 本文对传统方法的两个关键步骤进行了改进。</p>
                </div>
                <div class="p1">
                    <p id="56"> (1) 改进了屏幕区域边缘检测方法。传统的canny边缘检测算法需要设置很多的阈值参数, 这些参数要根据实验效果人为设定, 一旦参数过多, 就很难根据经验设定, 另外这些参数只对特定的情形有很好的效果。而深度学习具有表征能力强, 不需要人为设定参数, 自动获得特征等优点。因此, 采用了基于全卷积神经网络的屏幕区域边缘检测方法, 获得了很好的检测效果。</p>
                </div>
                <div class="p1">
                    <p id="58"> (2) 提出一种新的屏幕区域边缘定位算法。传统的轮廓检测算法只能定位闭合轮廓的边缘, 适应性有限。此外由于有大量的干扰, 传统方法定位屏幕区域的边缘很困难。因此, 本文提出一种利用全卷积神经网络产生的屏幕区域边缘图像和屏幕区域位置图像来定位屏幕轮廓的算法, 这种算法具有效率高, 抗干扰能力强, 算法简单等特点。</p>
                </div>
                <h3 id="60" name="60" class="anchor-tag"><b>1 相关工作</b></h3>
                <div class="p1">
                    <p id="61">本方法实现的一个关键步骤就是检测到屏幕的边缘, 边缘检测算法的研究历史很悠久, 也是计算机视觉的基本研究课题之一。早期的方法是基于sobel算子的边缘检测方法<citation id="292" type="reference"><link href="244" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>, 以及被广泛采用的canny检测算法<citation id="293" type="reference"><link href="246" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>。基于信息论的算法有gPb分割算法<citation id="294" type="reference"><link href="250" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>、pb算法<citation id="295" type="reference"><link href="252" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>。有的算法依赖人类设计的特征, 比如, BEL算法<citation id="296" type="reference"><link href="254" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>、Multi-scale<citation id="297" type="reference"><link href="256" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>和StrucutredEdge<citation id="298" type="reference"><link href="258" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>。文献<citation id="299" type="reference">[<a class="sup">20</a>]</citation>依靠视频前后帧的关系定位屏幕范围, 取得了很好的效果。还有很多很多基于卷积神经网络CNN (Convolutional Neural Network) 的边缘检测算法, 比如, N4-Fields<citation id="300" type="reference"><link href="260" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>, DeepEdge<citation id="301" type="reference"><link href="262" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>, Fastmask<citation id="302" type="reference"><link href="264" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>, SGN<citation id="303" type="reference"><link href="266" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>。文献<citation id="308" type="reference">[<a class="sup">16</a>,<a class="sup">17</a>,<a class="sup">18</a>,<a class="sup">19</a>]</citation>进行图像的语音分割。有的采用全卷积神经网络, 例如, HED<citation id="304" type="reference"><link href="268" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>、RCF<citation id="305" type="reference"><link href="270" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>、CASENet<citation id="306" type="reference"><link href="272" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>、Mask r-cnn<citation id="307" type="reference"><link href="284" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>。这些方法把卷积层的输出接入一个side output 层, side output层由一个1×1的卷积层, 一个deconv层和一个softmax层组成, 每层side-output进行融合, 由于利用了多尺度的信息, 因此取得了很好的效果。</p>
                </div>
                <h3 id="62" name="62" class="anchor-tag"><b>2 屏幕区域边缘检测网络</b></h3>
                <div class="p1">
                    <p id="63">屏幕区域的边缘检测和基于深度学习的边缘检测有很多的不同, 最主要的区别是: (1) 基于深度学习的边缘检测目标是得到通用物体的边缘 (多种物体的边缘) , 而屏幕的边缘检测只要得到屏幕区域的边缘图像即可 (只有屏幕区域的边缘) , 因此需要神经网络排除其他物体的边缘。 (2) 虽然神经网络能够排除大部分其他物体的边缘, 但是还是存在部分干扰。如图2所示, fusion-output是神经网络输出的屏幕区域边缘图像, 其中存少量其他物体的边缘, 因此只用fusion-output无法得到正确结果, 我们需要神经网络输出屏幕区域的位置信息帮助定位 (也就是side-outout 5) 。</p>
                </div>
                <div class="area_img" id="64">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201906027_06400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 屏幕区域边缘图像" src="Detail/GetImg?filename=images/JYRJ201906027_06400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 屏幕区域边缘图像  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201906027_06400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="65" name="65"><b>2.1 网络结构</b></h4>
                <div class="p1">
                    <p id="66">网络通过修改VGG-16而来, VGG-16网络由13个卷积层, 和3个全连接层组成, VGG-16网络不但在分类问题中表现优异, 而且容易应用到其他场景<citation id="309" type="reference"><link href="272" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>, 比如物体检测和图像分类。本文的网络对VGG-16的修改主要有:</p>
                </div>
                <div class="p1">
                    <p id="67"> (1) 移动设备性能差, 而VGG-16是一个通用的神经网络框架, VGG-16模型对于移动设备来说太大了, 而屏幕边缘检测只需要网络能检测屏幕区域到边缘的直线即可, 因此减少VGG-16每一层卷积核的数量对屏幕区域边缘检测也不会造成太大影响。在实验部分也测试了不同卷积核的数量对最终结果的影响以及运行速度, 而本文最终采用的网络结构如图3所示。</p>
                </div>
                <div class="area_img" id="68">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201906027_068.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 本文采用的网络结构" src="Detail/GetImg?filename=images/JYRJ201906027_068.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 本文采用的网络结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201906027_068.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="69"> (2) 去掉了VGG-16的第五个max-pool层, 和后面的全连接层。主要原因是, 加入max-pool 5层会使输出的side-output大小缩小为原来的1/32, 经过32× deconv层后, 会使得精度大大降低, 对结果不利。</p>
                </div>
                <div class="p1">
                    <p id="70"> (3) 在Conv1-2、Conv2-2、Conv3-3、Conv4-3、Conv5-3五层后面接了一个1×1-1的卷积层进行降维, 这等于对所有卷积核输出的结果做加权求和, 同时权重由网络学习得到。之后接一个deconv层, 对feature map进行上采样, 使得feature map和原图一样大。</p>
                </div>
                <div class="p1">
                    <p id="71"> (4) 所有5个上采样得到的结果进行concat, 之后用一个1×1-1的卷积层进行特征图的融合, 最后使用一个sigmoid输出屏幕区域边缘图像 (图3的fusion-output所示) 。这里1×1-1的卷积的参数可以自动学习, 也可以人工指定, 本文中采用人工指定的方式, 具体值在2.2中说明。</p>
                </div>
                <div class="p1">
                    <p id="72"> (5) 在Conv5-3之后的deconv层连接一个sigmoid layer输出屏幕区域位置图像 (如图3的side-output 5) , side-output 5得到的屏幕区域边缘位置不够精细, 但是作为高层的输出比fusion-output少了很多干扰, 因此可以当作屏幕区域位置图像使用。</p>
                </div>
                <h4 class="anchor-tag" id="73" name="73"><b>2.2 损失函数设计</b></h4>
                <div class="p1">
                    <p id="74">假设我们的输入训练集是<i>S</i>={ (<i>X</i><sup> (<i>n</i>) </sup>, <i>Y</i><sup> (<i>n</i>) </sup>) |<i>n</i>=1, 2, …, <i>N</i>}, 其中<i>X</i><sup> (<i>n</i>) </sup>={<i>x</i><mathml id="75"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mrow><mo stretchy="false"> (</mo><mi>n</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>|<i>j</i>=1, 2, …, |<i>X</i><sup> (<i>n</i>) </sup>|}指的是输入的原始图片, 其中<i>x</i><mathml id="76"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mrow><mo stretchy="false"> (</mo><mi>n</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>是第<i>n</i>张图片第<i>j</i>个像素点的值, |<i>X</i><sup> (<i>n</i>) </sup>|是第<i>n</i>张图片中像素点的个数, <i>Y</i><sup> (<i>n</i>) </sup>={<i>y</i><mathml id="77"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mrow><mo stretchy="false"> (</mo><mi>n</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>|<i>j</i>=1, 2, …, |<i>Y</i><sup> (<i>n</i>) </sup>|}指的是对应的ground truth, 其中<i>y</i><mathml id="78"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mrow><mo stretchy="false"> (</mo><mi>n</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>∈{0, 1}是第<i>n</i>张图片的ground truth中第<i>j</i>个像素点的值, 0和1分别代表不是屏幕区域边缘的像素点和是屏幕区域边缘。下式用来计算fusion-output和side-output 5中每个像素点的loss。</p>
                </div>
                <div class="area_img" id="80">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JYRJ201906027_08000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="81">式中:</p>
                </div>
                <div class="p1">
                    <p id="82"><mathml id="83"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>r</mi><mo>=</mo><mfrac><mrow><mo stretchy="false">|</mo><mi>Y</mi><msup><mrow></mrow><mo>+</mo></msup><mo stretchy="false">|</mo></mrow><mrow><mo stretchy="false">|</mo><mi>Y</mi><mo stretchy="false">|</mo></mrow></mfrac><mtext> </mtext><mi>β</mi><mo>=</mo><mfrac><mrow><mo stretchy="false">|</mo><mi>Y</mi><msup><mrow></mrow><mo>-</mo></msup><mo stretchy="false">|</mo></mrow><mrow><mo stretchy="false">|</mo><mi>Y</mi><mo stretchy="false">|</mo></mrow></mfrac></mrow></math></mathml>      (2) </p>
                </div>
                <div class="p1">
                    <p id="84">Pr (<i>x</i><sub><i>j</i></sub>;<i>W</i>) =sigmoid (<i>a</i><sub><i>j</i></sub><sup> (<i>m</i>) </sup>) ∈[0, 1], <i>a</i><mathml id="85"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mrow><mo stretchy="false"> (</mo><mi>m</mi><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>是图片<i>m</i>中的像素点<i>x</i><sub><i>j</i></sub>在预测图中的激活值, |<i>Y</i><sup>+</sup>|和|<i>Y</i><sup>-</sup>|分别指的是ground truth中是屏幕区域边缘的集合和不是屏幕区域边缘的集合, <i>W</i>代表网络中所有需要训练的参数。</p>
                </div>
                <div class="p1">
                    <p id="86">下式表示各层第<i>j</i>个像素点融合为fusion-output的第<i>j</i>个像素点的值时各层的权重, 其中<i>w</i><sub>1</sub>=<i>w</i><sub>2</sub>=<i>w</i><sub>3</sub>=<i>w</i><sub>4</sub>=0.2, <i>w</i><sub>5</sub>=0.28。</p>
                </div>
                <div class="p1">
                    <p id="87"><mathml id="88"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>a</mi><msubsup><mrow></mrow><mi>j</mi><mrow><mo stretchy="false"> (</mo><mtext>f</mtext><mtext>u</mtext><mtext>s</mtext><mtext>i</mtext><mtext>o</mtext><mtext>n</mtext><mo stretchy="false">) </mo></mrow></msubsup><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>i</mi><mo>=</mo><mn>5</mn></mrow></munderover><mi>w</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mi>a</mi><msubsup><mrow></mrow><mi>j</mi><mrow><mo stretchy="false"> (</mo><mtext>s</mtext><mtext>i</mtext><mtext>d</mtext><mtext>e</mtext><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>      (3) </p>
                </div>
                <div class="p1">
                    <p id="89">下式表示Fusion-output的loss, 其中<i>Y</i><sup> (fusion-output) </sup>=<i>sigmoid</i> (<i>A</i><sup> (fusion) </sup>) , <i>A</i><sup> (fusion) </sup>={<i>a</i><mathml id="90"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mrow><mo stretchy="false"> (</mo><mtext>f</mtext><mtext>u</mtext><mtext>s</mtext><mtext>i</mtext><mtext>o</mtext><mtext>n</mtext><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>|<i>j</i>-1, 2, …, |<i>Y</i>|}, |<i>Y</i><sup> (</sup>fusion-output|是fusion-output输出值的集合。</p>
                </div>
                <div class="p1">
                    <p id="91"><i>L</i><sup> (fusion) </sup> (<i>X</i>;<mathml id="92"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>W</mi><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mrow><mo stretchy="false">|</mo><mi>Y</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mtext>f</mtext><mtext>u</mtext><mtext>s</mtext><mtext>i</mtext><mtext>o</mtext><mtext>n</mtext><mo>-</mo><mtext>o</mtext><mtext>u</mtext><mtext>t</mtext><mtext>p</mtext><mtext>u</mtext><mtext>t</mtext><mo stretchy="false">) </mo></mrow></msup><mo stretchy="false">|</mo></mrow></munderover><mi>l</mi></mstyle><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mtext>f</mtext><mtext>u</mtext><mtext>s</mtext><mtext>i</mtext><mtext>o</mtext><mtext>n</mtext><mo stretchy="false">) </mo></mrow></msup><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>j</mi></msub></mrow></math></mathml>;<i>W</i>)      (4) </p>
                </div>
                <div class="p1">
                    <p id="93">下式表示side-output 5的loss, <i>Y</i><sup> (side-output5) </sup>=<i>sigmoid</i> (<i>A</i><sup> (side5) </sup>, <i>A</i><sup> (side5) </sup>={<i>a</i><mathml id="94"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mrow><mo stretchy="false"> (</mo><mtext>s</mtext><mtext>i</mtext><mtext>d</mtext><mtext>e</mtext><mn>5</mn><mo stretchy="false">) </mo></mrow></msubsup></mrow></math></mathml>|<i>j</i>=1, 2, …, |<i>Y</i>|}, |<i>Y</i><sup> (side-output5) </sup>|是side-output 5输出值的集合。</p>
                </div>
                <div class="p1">
                    <p id="95"><i>L</i><sup> (side5) </sup> (<i>X</i>;<mathml id="96"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>W</mi><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mrow><mo stretchy="false">|</mo><mi>Y</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mtext>s</mtext><mtext>i</mtext><mtext>d</mtext><mtext>e</mtext><mo>-</mo><mtext>o</mtext><mtext>u</mtext><mtext>t</mtext><mtext>p</mtext><mtext>u</mtext><mtext>t</mtext><mn>5</mn><mo stretchy="false">) </mo></mrow></msup><mo stretchy="false">|</mo></mrow></munderover><mi>l</mi></mstyle><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mtext>s</mtext><mtext>i</mtext><mtext>d</mtext><mtext>e</mtext><mn>5</mn><mo stretchy="false">) </mo></mrow></msup><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>j</mi></msub></mrow></math></mathml>;<i>W</i>)      (5) </p>
                </div>
                <div class="p1">
                    <p id="97">式 (6) 是最终的损失函数, 该损失函数删去了HED<citation id="310" type="reference"><link href="268" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>中损失函数的低层部分, 保留了side-output 5。在实验部分解释了原因和实验结果。</p>
                </div>
                <div class="p1">
                    <p id="98"><i>L</i> (<i>W</i>) =<i>L</i><sup> (side 5) </sup> (<i>X</i><sup> (side 5) </sup>;<i>W</i>) +<i>L</i><sup> (fusion) </sup> (<i>X</i><sup> (fusion) </sup>;<i>W</i>)      (6) </p>
                </div>
                <h4 class="anchor-tag" id="99" name="99"><b>2.3 训练数据</b></h4>
                <div class="p1">
                    <p id="100">本文采用部分真实场景的图片和部分合成图片的方式自制数据集。合成的图片主要采用了不同的背景, 和不同的前景图片合成, 其中前景图片加上平移、旋转、透视变换。背景图片进行随机的裁剪, 合成的图片还进行了反转操作来扩充数据集, 这种自制数据总计有10万幅, 手工标注的真实场景数据总计2 000幅。另外有500幅真实场景的图片作为测试集。</p>
                </div>
                <h3 id="101" name="101" class="anchor-tag"><b>3 屏幕区域边缘定位算法</b></h3>
                <div class="p1">
                    <p id="102">主要利用fusion-output和side-output 5得到屏幕区域的边缘。整体流程如图4所示。</p>
                </div>
                <div class="area_img" id="103">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201906027_103.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 边缘定位算法整体流程图" src="Detail/GetImg?filename=images/JYRJ201906027_103.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 边缘定位算法整体流程图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201906027_103.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="104" name="104"> (1) 屏幕区域边缘直线检测</h4>
                <div class="p1">
                    <p id="105">先对Fusion-output生成的单通道图进行阈值化, 大于阈值<i>t</i>的点的像素值为255, 小于<i>t</i>的点的像素值为0, 其中的阈值<i>t</i>是Fusion-output中像素点的值大于30的点的均值, 设阈值化后的图像为<i>F</i><sub><i>t</i></sub>, 然后用Hough直线检测算法在<i>F</i><sub><i>t</i></sub>中检测直线, 但是同一条直线, 也会被检测为很多长短不一, 有重叠、有间隙的直线, 因此需要对检测到的直线进行合并。</p>
                </div>
                <div class="p1">
                    <p id="106">合并方法:</p>
                </div>
                <div class="p1">
                    <p id="107">① 对检测到的直线根据长度进行排序。</p>
                </div>
                <div class="p1">
                    <p id="108">② 找到最长的直线, 删除所有比这条直线短且和这条直线重合的直线 (重合判断方法在下文详述) 。</p>
                </div>
                <div class="p1">
                    <p id="109">③ 重复步骤①和②, 直到最后一条直线为止。</p>
                </div>
                <div class="p1">
                    <p id="110">④ 记录边缘直线集合<i>SI</i>。</p>
                </div>
                <div class="p1">
                    <p id="111">判断重合方法:</p>
                </div>
                <div class="p1">
                    <p id="112">① 计算较短直线两个端点到长直线的距离, 记为<i>L</i>1和<i>L</i>2。</p>
                </div>
                <div class="p1">
                    <p id="113">② 设定两个长度阈值<i>ps</i>和<i>pm</i> (<i>pm</i>&gt;<i>ps</i>, 在本文中<i>pm</i>=10, <i>ps</i>=5) 。</p>
                </div>
                <div class="p1">
                    <p id="114">③ 如果<i>L</i>1&gt;<i>pm</i>或<i>L</i>2&gt;<i>pm</i>, 判断为不重合;如果<i>L</i>1&lt;<i>ps</i>或<i>L</i>2&lt;<i>ps</i>, 判断为重合;如果<i>ps</i>&lt;<i>L</i>1&lt;<i>pm</i>并且<i>ps</i>&lt;<i>L</i>2&lt;<i>pm</i>, 判断为不重合。</p>
                </div>
                <div class="p1">
                    <p id="115">说明:只用一个阈值来区分两条直线是否为同一条直线会带来问题。因为fusion-output屏幕区域内外边缘之间有宽有窄, 如果长度阈值取得较大, 直线合并效果好, 但是在屏幕内外边缘距离较窄的情况下, 容易把内外边缘也错误合并;如果阈值取得较小, 直线合并效果差, 产生的干扰多。因此采用两个直线阈值, 可以获得很好的内外边缘直线。</p>
                </div>
                <h4 class="anchor-tag" id="116" name="116"> (2) 产生候选框</h4>
                <div class="p1">
                    <p id="117">图5显示了产生候选框集合的流程。在图片中选取一个点<i>t</i> (本文中设置<i>t</i>点为屏幕中心点) , 称之为目标点, <i>SI</i>集合中选两条直线<i>la</i>和<i>lb</i>, 直线<i>la</i>和<i>lb</i>的交点为<i>pi</i>, 选取<i>la</i>的一个端点为<i>pa</i>, 选取<i>lb</i>的一个端点为<i>pb</i>, 交点<i>pi</i>到<i>pa</i>的向量称为<b><i>pipa</i></b>, <i>pi</i>到<i>pb</i>的向量称为<b><i>pipb</i></b>, <i>pi</i>到目标点<i>t</i>称为向量<b><i>pit</i></b>, 计算满足公式<b><i>pit</i></b>=<i>a</i>·<b><i>pipa</i></b>+<i>b</i>·<b><i>pipb</i></b>, <i>a</i>&gt;0且<i>b</i>&gt;0的端点<i>pa</i>和<i>pb</i>。如果有多个端点满足要求, 则取到交点<i>pi</i>长度最大的端点。记录 <i>pi</i>、<i>pa</i>和<i>pb</i>, 作为一个角结构。依次对<i>SI</i>集合中剩余的直线进行处理。</p>
                </div>
                <div class="area_img" id="118">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201906027_118.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 产生候选框集合的流程" src="Detail/GetImg?filename=images/JYRJ201906027_118.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 产生候选框集合的流程  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201906027_118.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="119">从前面得到的角结构中, 取两个不含相同直线的角结构, 为<i>pi</i><sub>1</sub>、<i>pb</i><sub>1</sub>、<i>pa</i><sub>1</sub>和<i>pi</i><sub>2</sub>、<i>pb</i><sub>2</sub>、<i>pa</i><sub>2</sub>, 其中<i>pb</i><sub>1</sub>和<i>pb</i><sub>2</sub>位于点<i>pi</i><sub>1</sub>和<i>pi</i><sub>2</sub>组成的直线一边, <i>pa</i><sub>1</sub>和<i>pa</i><sub>2</sub>位于直线一边, 如图5所示。计算等式<b><i>pi</i></b><sub>1</sub><b><i>pi</i></b><sub>2</sub>=<i>a</i>1·<b><i>pi</i></b><sub>1</sub><b><i>pb</i></b><sub>1</sub>+<i>b</i><sub>1</sub>·<b><i>pi</i></b><sub>1</sub><b><i>pa</i></b><sub>1</sub>, <b><i>pi</i></b><sub>2</sub><b><i>pi</i></b><sub>1</sub>=<i>a</i><sub>2</sub>·<b><i>pi</i></b><sub>2</sub><b><i>pb</i></b><sub>2</sub>+<i>b</i><sub>2</sub>·<b><i>pi</i></b><sub>2</sub><b><i>pa</i></b><sub>2</sub>, 若<i>a</i><sub>1</sub>、<i>b</i><sub>1</sub>、<i>a</i><sub>2</sub>、<i>b</i><sub>2</sub>都大于0, 则这两个角结构可以组成一个候选框, 如图5最后一步所示。计算<b><i>pi</i></b><sub>1</sub><b><i>pb</i></b><sub>1</sub>和<b><i>pi</i></b><sub>2</sub><b><i>pb</i></b><sub>2</sub>的交点以及<b><i>pi</i></b><sub>1</sub><b><i>pa</i></b><sub>1</sub>和<b><i>pi</i></b><sub>2</sub><b><i>pa</i></b><sub>2</sub>的交点, 得到四边形的另外两个交点, 保存四个交点, 得到候选框, 重复对所有角结构进行计算。最终所有的候选框总算包围着目标点<i>t</i>。</p>
                </div>
                <h4 class="anchor-tag" id="120" name="120"> (3) 候选框评分方法</h4>
                <div class="p1">
                    <p id="121">我们主要用fusion-output和side-output 5对候选框打分, 式 (7) 计算四边形一条边的得分, 其中点 (<i>x</i><sub><i>i</i></sub>, <i>y</i><sub><i>i</i></sub>) 位于直线<i>Line</i><sub> (<i>i</i>) </sub>上, <i>p</i><sub><i>j</i></sub> (<i>x</i><sub><i>i</i></sub>, <i>y</i><sub><i>i</i></sub>) 是点 (<i>x</i><sub><i>i</i></sub>, <i>y</i><sub><i>i</i></sub>) 在图像<i>j</i>中的像素值 (本文中, <i>j</i>为fusion-output或side-output 5) , |<i>line</i><sub> (<i>i</i>) </sub>|是直线<i>Line</i><sub> (<i>i</i>) </sub>上点集的个数。式 (8) 计算一个候选框的得分, <i>Line</i><sub> (<i>i</i>) </sub>为候选框<i>Rect</i><sub> (<i>q</i>) </sub>的边缘线。</p>
                </div>
                <div class="p1">
                    <p id="122"><i>m</i> (<i>i</i>, <i>j</i>) =<mathml id="123"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>∈</mo><mi>L</mi><mi>i</mi><mi>n</mi><mi>e</mi><msub><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msub></mrow></munder><mi>p</mi></mstyle><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo></mrow><mrow><mo stretchy="false">|</mo><mi>l</mi><mi>i</mi><mi>n</mi><mi>e</mi><msub><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo></mrow></msub><mo stretchy="false">|</mo></mrow></mfrac></mrow></math></mathml>      (7) </p>
                </div>
                <div class="p1">
                    <p id="124"><i>M</i> (<i>q</i>, <i>j</i>) =<mathml id="125"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>l</mi><mi>i</mi><mi>n</mi><mi>e</mi><msub><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo stretchy="false">) </mo><mo>∈</mo><mi>R</mi><mi>e</mi><mi>c</mi><mi>t</mi><msub><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>q</mi><mo stretchy="false">) </mo></mrow></msub></mrow></msub></mrow></munder><mi>m</mi></mstyle><mo stretchy="false"> (</mo><mi>i</mi><mo>, </mo><mi>j</mi><mo stretchy="false">) </mo></mrow><mn>4</mn></mfrac></mrow></math></mathml>      (8) </p>
                </div>
                <div class="p1">
                    <p id="126">首先在side-output 5对候选框进行打分, 即对所有候选框计算<i>M</i> (<i>q</i>, <i>j</i>) <i>j</i>=side-output5, 最高得分乘0.9为阈值<i>a</i>, 排除得分小于阈值<i>a</i>的候选框, 之后对剩余的候选框在fusion-output中打分, 对剩余的候选框计算<i>M</i> (<i>q</i>, <i>j</i>) <i>j</i>=fusion-output, 得分最高者为最终结果。</p>
                </div>
                <div class="p1">
                    <p id="127">之所以采用这种方案是因为, fusion-output中屏幕边缘较为精细, 但是错误很多, side-output 5的边缘粗, 但是能准确反映边框的大概位置, 如果候选框只用fusion-output计算评分, 可能会出现画中画的侯选框得分最高。产生这种问题的原因是, fusion-output是由各层side-output融合而成, 低层错误的边缘可能会融合成像素值很大的错误边缘, 导致最终的评分错误。因此我们先用side-output 5得到一些位置准确, 但是可能包含一些由屏幕区域内边缘和外边缘组成候选框。之后用fusion-output评分, 因为内边缘和外边缘组成候选框会穿过屏幕边框的部分, 而屏幕的边框部分的像素值很低, 导致评分降低, 只有那些连成一个整体的全是由屏幕内边缘组成或全由屏幕外边缘组成的候选框, 才会在fusion-output中得到很高的评分, 之后选取评分最高的那个。因此最终结果要么是屏幕区域内边框组成的边缘, 或者是屏幕区域外边框组成的边缘。</p>
                </div>
                <h3 id="128" name="128" class="anchor-tag"><b>4 实 验</b></h3>
                <div class="p1">
                    <p id="129">我们利用了TensorFlow来实现网络, 先在网络最后的卷积层后加上Maxpool-5层、全连接层、soft-max层并用ImageNet数据集对网络进行物体检测训练, 然后用预训练好的参数初始化我们的网络, side-output1-5中的1×1conv采用期望为0、方差为0.01的高斯分布来初始化, bias=0。Fusion-output中的1×1convd用来融合各层的结果, 其中<i>a</i><sub><i>i</i></sub>为side-output <i>i</i> (<i>i</i>为1～5) 对应的参数, 其中<i>a</i><sub>1</sub>=<i>a</i><sub>2</sub>=<i>a</i><sub>3</sub>=<i>a</i><sub>4</sub>=0.2, <i>a</i><sub>5</sub>=0.28, 不参与训练。deconv-layer用 bilinear初始化。网络剩余的超参数为:minibatch size=10, learning rate=1e-6, 每训练10 k次除以10, momentum=0.9, weight decay=0.000 2, 所有参数的训练在1块NVIDIA TITAN X GPU上完成。</p>
                </div>
                <h4 class="anchor-tag" id="130" name="130"><b>4.1 不同的网络大小</b></h4>
                <div class="p1">
                    <p id="131">我们测试了不同的卷积核个数对fusion-output和side-output 5的影响, 表1显示了三种网络的结构, 每种网络都以VGG-16模型为基础, 只是改变了卷积核的个数。使用同样的屏幕区域边缘定位算法计算最终的结果。</p>
                </div>
                <div class="area_img" id="132">
                    <p class="img_tit"><b>表1 三种不同的网络大小</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="132" border="1"><tr><td><br />Model-A</td><td>Model-B</td><td>Model-C</td></tr><tr><td><br />13weight layers</td><td>13weight layers</td><td>13weight layers</td></tr><tr><td colspan="3"><br />Input (224x224 RGB image) </td></tr><tr><td><br />Conv3-16<br />Conv3-16</td><td>Conv3-32<br />Conv3-32</td><td>Conv3-64<br />Conv3-64</td></tr><tr><td colspan="3"><br />Maxpool-1</td></tr><tr><td><br />Conv3-32<br />Conv3-32</td><td>Conv3-64<br />Conv3-64</td><td>Conv3-128<br />Conv3-128</td></tr><tr><td colspan="3"><br />Maxpool-2</td></tr><tr><td><br />Conv3-64<br />Conv3-64<br />Conv3-64</td><td>Conv3-128<br />Conv3-128<br />Conv3-128</td><td>Conv3-256<br />Conv3-256<br />Conv3-256</td></tr><tr><td colspan="3"><br />Maxpool-3</td></tr><tr><td><br />Conv3-128<br />Conv3-128<br />Conv3-128</td><td>Conv3-256<br />Conv3-256<br />Conv3-256</td><td>Conv3-512<br />Conv3-512<br />Conv3-512</td></tr><tr><td colspan="3"><br />Maxpool-4</td></tr><tr><td><br />Conv3-128<br />Conv3-128<br />Conv3-128</td><td>Conv3-256<br />Conv3-256<br />Conv3-256</td><td>Conv3-512<br />Conv3-512<br />Conv3-512</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="133">经过测试表2显示了在一台iphone8手机上三种网络的计算性能。</p>
                </div>
                <div class="area_img" id="134">
                    <p class="img_tit"><b>表2 三种网络的性能</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="134" border="1"><tr><td><br />性能</td><td>A</td><td>B</td><td>C</td></tr><tr><td><br />Fps</td><td>10</td><td>2</td><td>0.7</td></tr><tr><td><br />模型大小/MB</td><td>6</td><td>14</td><td>56</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="135">图6显示了三幅不同的输入图在三种不同网络结构中得到的结果, 图片1在三种模型的最终结果中, 都把屏幕区域的左边缘标记为外边框, 右边缘标记为内边框, 我认为产生这种结果的原因是, 左-内边框和右-外边框都不明显, 导致fusion-output中对应的像素值较低, 最终候选框在fusion-output上得分较低, 因此我们的算法会优先选择边缘清晰的边框。图片2在三种模型的最终结果中下边缘都不对, 产生这种结果的原因是本文中目标点<i>t</i>设定为屏幕中间, 导致目标点<i>t</i>在下边缘的下面, 而边框的要求是必须包围目标点<i>t</i>, 因此导致错误。图片3是不含有屏幕区域的图像, Model-A不能准确地区分, 但是Model-B, Model-C的side-output 5、fusion-output都不会产生明显的响应, 可以区分这种不含屏幕的干扰。</p>
                </div>
                <div class="area_img" id="136">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201906027_136.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 三种不同网络的结果" src="Detail/GetImg?filename=images/JYRJ201906027_136.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 三种不同网络的结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201906027_136.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="137" name="137"><b>4.2 不同的loss 函数</b></h4>
                <div class="p1">
                    <p id="138">在HED<citation id="311" type="reference"><link href="268" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>边缘检测方法中, loss的计算公式如下:</p>
                </div>
                <div class="p1">
                    <p id="139"><mathml id="140"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><mo stretchy="false"> (</mo><mi>W</mi><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>s</mi></munderover><mi>L</mi></mstyle><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mtext>s</mtext><mtext>i</mtext><mtext>d</mtext><mtext>e</mtext><mi>i</mi><mo stretchy="false">) </mo></mrow></msup><mo stretchy="false"> (</mo><mi>X</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mtext>s</mtext><mtext>i</mtext><mtext>d</mtext><mtext>e</mtext><mi>i</mi><mo stretchy="false">) </mo></mrow></msup></mrow></math></mathml>;<i>W</i>) +<i>L</i><sup> (fusion) </sup> (<i>X</i><sup> (fusion) </sup>;<i>W</i>)      (9) </p>
                </div>
                <div class="p1">
                    <p id="141">我们测试了Model-B网络采用HED-loss和式 (6) 的输出, 如图7所示。</p>
                </div>
                <div class="area_img" id="142">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201906027_142.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 两种不同loss的结果" src="Detail/GetImg?filename=images/JYRJ201906027_142.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 两种不同loss的结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201906027_142.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="143">HED网络是一种通用的边缘检测方法, 通过让低层的side-output参与loss的计算, 可以让低层产生的更精细的结果参与最后的融合, 得到非常好的边缘检测效果。但是, 我们需要的只是检测到屏幕的边缘, 如果让低层也参与loss计算, 最后的结果会因为低层的影响而产生很多干扰。而且, 不同层检测到的边缘的粗细程度也不一样, 如果简单地加以融合, HED-loss产生的fusion-output屏幕边缘较粗, 很容易无法区分屏幕内外边框的边缘。而我们的loss边缘精细, 且干扰更少。</p>
                </div>
                <h4 class="anchor-tag" id="144" name="144"><b>4.3 相关方法对比实验</b></h4>
                <div class="p1">
                    <p id="145">为了证明算法的有效性, 我们将屏幕区域定位算法 (Model-B) 和三种基于特征点的目标跟踪算法进行对比。三种目标跟踪算法为:CamShift<citation id="312" type="reference"><link href="286" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>, Online Boosting (Boosting) <citation id="313" type="reference"><link href="288" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>, Multiple Instance Learning (MIL) <citation id="314" type="reference"><link href="290" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation><sup></sup></p>
                </div>
                <div class="p1">
                    <p id="146">具体来说, 我们手动标记视频帧的屏幕区域, 采用图像检索的评价标准, 正确率= (计算结果中是屏幕区域的像素个数) / (计算结果中总的像素个数) , 召回率= (计算结果中是屏幕区域的像素个数) / (屏幕区域中总的像素个数) 。我们将所有视频帧的平均结果作为最终结果记录于表3中。结果表明, 由于深度学习的应用, 本方法速度较其他三种慢, 但是准确率、召回率和F-score都有明显提高, 此外, 我们的方法相比其他三种不需要设定大量的参数, 使用简单。</p>
                </div>
                <div class="area_img" id="147">
                    <p class="img_tit"><b>表3 不同方法在屏幕检测上的性能对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="147" border="1"><tr><td><br />算法</td><td>准确率</td><td>召回率</td><td>F-score</td><td>FPS</td></tr><tr><td><br />CamShift<sup>[22]</sup></td><td>0.747</td><td>0.939</td><td>0.830</td><td>30</td></tr><tr><td><br />Boosting<sup>[23]</sup></td><td>0.878</td><td>0.812</td><td>0.843</td><td>20</td></tr><tr><td><br />MIL<sup>[24]</sup></td><td>0.926</td><td>0.856</td><td>0.889</td><td>25</td></tr><tr><td><br />本文算法</td><td>0.983</td><td>0.962</td><td>0.972</td><td>2</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h3 id="148" name="148" class="anchor-tag"><b>5 结 语</b></h3>
                <div class="p1">
                    <p id="149">本文提出了一种新的基于深度学习的定位屏幕轮廓的方法, 利用深度学习极强的表达能力, 产生好的边缘图像和位置图像, 并用屏幕轮廓定位算法定位屏幕轮廓。实验表明, 我们的方法在面对非屏幕的矩形物体的干扰下, 也能产生很好的效果, 即使图像背景复杂, 边缘干扰严重, 也能成功获得很好的结果。本方法计算速度快, 扩展性强, 可以用同样的框架检测其他形状的物体, 拥有广泛的应用前景。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="244">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=On the accuracy of the Sobel edge detector">

                                <b>[1]</b> Kittler J.On the accuracy of the Sobel edge detector[J].Image &amp; Vision Computing, 1983, 1 (1) :37-42.
                            </a>
                        </p>
                        <p id="246">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A computational approach to edge detection">

                                <b>[2]</b> Canny J.A computational approach to edge detection[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 1986, 8 (6) :679-698.
                            </a>
                        </p>
                        <p id="248">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fully convolutional netw orks for semantic segmentation">

                                <b>[3]</b> Long J, Shelhamer E, Darrell T.Fully convolutional networks for semantic segmentation[C]//IEEE Conference on Computer Vision and Pattern Recognition, 2015.
                            </a>
                        </p>
                        <p id="250">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Contour Detection and Hierarchical Image Segmentation">

                                <b>[4]</b> Arbelaez P, Maire M, Fowlkes C, et al.Contour Detection and Hierarchical Image Segmentation[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2011, 33 (5) :898-916.
                            </a>
                        </p>
                        <p id="252">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning to detect natural image boundaries using local brightness, color, and texture cues">

                                <b>[5]</b> Martin D R, Fowlkes C C, Malik J.Learning to detect natural image boundaries using local brightness, color, and texture cues[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2004, 26 (5) :530-549.
                            </a>
                        </p>
                        <p id="254">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Supervised Learning of Edges and Object Boundaries">

                                <b>[6]</b> Dollar P, Tu Z, Belongie S.Supervised learning of edges and object boundaries[C]//IEEE Conference on Computer Vision and Pattern Recognition, 2006.
                            </a>
                        </p>
                        <p id="256">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multi-Scale Improves Boundary Detection in Natural Images">

                                <b>[7]</b> Ren X.Multi-scale improves boundary detection in natural images[C]//European Conference on Computer Vision, 2008.
                            </a>
                        </p>
                        <p id="258">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fast edge detection using structured forests">

                                <b>[8]</b> Dollar P, Zitnick C L.Fast edge detection using struc-tured forests[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2015, 13 (7) :120-135.
                            </a>
                        </p>
                        <p id="260">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=N^ 4-Fields:Neural Network Nearest Neighbor Fields for Image Transforms">

                                <b>[9]</b> Ganin Y, Lempitsky V.N<sup>4</sup>-fields:Neural network nearest neighbor fields for image transforms[C]// Asian Conference on Computer Vision, 2014.
                            </a>
                        </p>
                        <p id="262">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=DeepEdge:A multi-scale bifurcated deep network for top-down contour detection">

                                <b>[10]</b> Bertasius G, Shi J, Torresani L.DeepEdge:A Multi-Scale Bifurcated Deep Network for Top-Down Contour Detection[C]//IEEE Conference on Computer Vision and Pattern Recognition, 2015.
                            </a>
                        </p>
                        <p id="264">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fast Mask segment multi-scale object candidates in one shot">

                                <b>[11]</b> Hu H, Lan S, Jiang Y, et al.FastMask:Segment Multi-scale Object Candidates in One Shot[C]//IEEE Conference on Computer Vision and Pattern Recognition, 2017.
                            </a>
                        </p>
                        <p id="266">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=SGN:Sequential Grouping Networks for Instance Segmentation">

                                <b>[12]</b> Liu S, Jia J, Fidler S, et al.SGN:Sequential Grouping Networks for Instance Segmentation[C]// 2017 IEEE International Conference on Computer Vision (ICCV) .IEEE, 2017.
                            </a>
                        </p>
                        <p id="268">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Holistically-nested edge detection">

                                <b>[13]</b> Xie S, Tu Z.Holistically-Nested Edge Detection[C]//IEEE International Conference on Computer Vision, 2015.
                            </a>
                        </p>
                        <p id="270">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Richer Convolutional Features for Edge Detection">

                                <b>[14]</b> Liu Y, Cheng M, Hu X, et al.Richer Convolutional Features for Edge Detection[C]//IEEE Conference on Computer Vision and Pattern Recognition, 2017.
                            </a>
                        </p>
                        <p id="272">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=CASENet:Deep Category-Aware Semantic Edge Detection">

                                <b>[15]</b> Yu Z, Feng C, Liu M, et al.CASENet:Deep Category-Aware Semantic Edge Detection[C]//IEEE Conference on Computer Vision and Pattern Recognition, 2017.
                            </a>
                        </p>
                        <p id="274">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=One-Shot Video Object Segmentation">

                                <b>[16]</b> Caelles S, Maninis K K, Pont-Tuset J, et al.One-Shot Video Object Segmentation[C]// 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) .IEEE, 2017.
                            </a>
                        </p>
                        <p id="276">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Semantic segmentation with boundary neural fields">

                                <b>[17]</b> Bertasius G, Shi J, Torresani L.Semantic segmentation with boundary neural fields[C]//IEEE Conference on Computer Vision and Pattern Recognition, 2016.
                            </a>
                        </p>
                        <p id="278">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning to Segment Instances in Videos with Spatial Propagation Network">

                                <b>[18]</b> Cheng J, Liu S, Tsai Y, et al.Learning to Segment Instances in Videos with Spatial Propagation Network[C]//IEEE Conference on Computer Vision and Pattern Recognition, 2017.
                            </a>
                        </p>
                        <p id="280">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Segmentation-Aware Convolutional Networks Using Local Attention Masks">

                                <b>[19]</b> Harley A W, Derpanis K G, Kokkinos I.Segmentation-Aware Convolutional Networks Using Local Attention Masks [C]//IEEE International Conference on Computer Vision, 2017.
                            </a>
                        </p>
                        <p id="282">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Structuring Lecture Videos by Automatic Projection Screen Localization and Analysis">

                                <b>[20]</b> Li K, Wang J, Wang H, et al.Structuring Lecture Videos by Automatic Projection Screen Localization and Analysis[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2015, 37 (6) :1233-1246.
                            </a>
                        </p>
                        <p id="284">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Mask R-CNN">

                                <b>[21]</b> He K, Gkioxari G, Dollar P, et al.Mask r-cnn[C]//IEEE International Conference on Computer Vision, 2017.
                            </a>
                        </p>
                        <p id="286">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Computer vision face tracking as a component of a perceptual user interface">

                                <b>[22]</b> Bradski G R.Computer vision face tracking for use in a perceptual user interface[C]// IEEE Workshop Applications of Computer Vision, 1998:214-219.
                            </a>
                        </p>
                        <p id="288">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Real-time tracking via online boosting">

                                <b>[23]</b> Grabner H, Grabner M, Bischof H.Real-time tracking via online boosting[C]//Proceedings of the British Machine Vision Conference, 2006.
                            </a>
                        </p>
                        <p id="290">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Visual tracking with online multiple instance learning">

                                <b>[24]</b> Babenko B, Yang M, Belongie S.Visual tracking with online multiple instance learning[C]//IEEE Conference on Computer Vision and Pattern Recognition, 2009.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JYRJ201906027" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201906027&amp;v=Mjc2Njk5ak1xWTlIWTRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVp0Rnl2bVY3ckpMelRaWkxHNEg=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="1" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
