<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637134081666193750%26DBCODE%3dCJFD%26TABLEName%3dCJFDTEMP%26FileName%3dJYRJ201911038%26RESULT%3d1%26SIGN%3dxSvKkEX14%252fIlWfAd%252fyCV9251dGM%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JYRJ201911038&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JYRJ201911038&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201911038&amp;v=MDAyNDF6VFpaTEc0SDlqTnJvOUdiSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnVGeS9oVkwvTEw=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#63" data-title="&lt;b&gt;0 引 言&lt;/b&gt; "><b>0 引 言</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#66" data-title="&lt;b&gt;1 相关工作&lt;/b&gt; "><b>1 相关工作</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#76" data-title="&lt;b&gt;2 方法设计&lt;/b&gt; "><b>2 方法设计</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#86" data-title="&lt;b&gt;2.1 分类任务&lt;/b&gt;"><b>2.1 分类任务</b></a></li>
                                                <li><a href="#90" data-title="&lt;b&gt;2.2 验证任务&lt;/b&gt;"><b>2.2 验证任务</b></a></li>
                                                <li><a href="#102" data-title="&lt;b&gt;2.3 哈希监督&lt;/b&gt;"><b>2.3 哈希监督</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#115" data-title="&lt;b&gt;3 实 验&lt;/b&gt; "><b>3 实 验</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#117" data-title="&lt;b&gt;3.1 数据集&lt;/b&gt;"><b>3.1 数据集</b></a></li>
                                                <li><a href="#122" data-title="&lt;b&gt;3.2 评价指标&lt;/b&gt;"><b>3.2 评价指标</b></a></li>
                                                <li><a href="#125" data-title="&lt;b&gt;3.3 对比实验及分析&lt;/b&gt;"><b>3.3 对比实验及分析</b></a></li>
                                                <li><a href="#141" data-title="&lt;b&gt;3.4 编码时间&lt;/b&gt;"><b>3.4 编码时间</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#145" data-title="&lt;b&gt;4 结 语&lt;/b&gt; "><b>4 结 语</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#79" data-title="图1 DMSH网络结构">图1 DMSH网络结构</a></li>
                                                <li><a href="#129" data-title="&lt;b&gt;表1 DMSH与其他方法在CIFAR-10和NUS-WIDE上的 检索MAP对比&lt;/b&gt;"><b>表1 DMSH与其他方法在CIFAR-10和NUS-WIDE上的 检索MAP对比</b></a></li>
                                                <li><a href="#134" data-title="&lt;b&gt;表2 多监督方法与非多监督方法的MAP的比较&lt;/b&gt;"><b>表2 多监督方法与非多监督方法的MAP的比较</b></a></li>
                                                <li><a href="#136" data-title="&lt;b&gt;表3 NUS-WIDE哈希效果对比&lt;/b&gt;"><b>表3 NUS-WIDE哈希效果对比</b></a></li>
                                                <li><a href="#143" data-title="图2 新图像的编码时间/μs">图2 新图像的编码时间/μs</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="177">


                                    <a id="bibliography_1" title=" Liu H,Wang R,Shan S,et al.Deep supervised hashing for fast image retrieval[C]//2016 IEEE Conference on Computer Vision and Pattern Recognition(CVPR).IEEE Computer Society,2016:2064-2072." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep Supervised Hashing for Fast Image Retrieval">
                                        <b>[1]</b>
                                         Liu H,Wang R,Shan S,et al.Deep supervised hashing for fast image retrieval[C]//2016 IEEE Conference on Computer Vision and Pattern Recognition(CVPR).IEEE Computer Society,2016:2064-2072.
                                    </a>
                                </li>
                                <li id="179">


                                    <a id="bibliography_2" title=" Lin K,Yang H F,Hsiao J H,et al.Deep learning of binary hash codes for fast image retrieval[C]//2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops(CVPRW).IEEE,2015:27-35." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep Learning of Binary Hash Codes for Fast Image Retrieval">
                                        <b>[2]</b>
                                         Lin K,Yang H F,Hsiao J H,et al.Deep learning of binary hash codes for fast image retrieval[C]//2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops(CVPRW).IEEE,2015:27-35.
                                    </a>
                                </li>
                                <li id="181">


                                    <a id="bibliography_3" title=" Wang J,Kumar S,Chang S F.Semi-supervised hashing for large-scale search[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2012,34(12):2393-2406." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Semi-Supervised Hashing for Large-Scale Search">
                                        <b>[3]</b>
                                         Wang J,Kumar S,Chang S F.Semi-supervised hashing for large-scale search[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2012,34(12):2393-2406.
                                    </a>
                                </li>
                                <li id="183">


                                    <a id="bibliography_4" title=" Jia Y,Shelhamer E,Donahue J,et al.Caffe:convolutional architecture for fast feature embedding[C]//Proceedings of the ACM International Conference on Multimedia,2014:675-678." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Caffe:convolutional architecture for fast feature embedding">
                                        <b>[4]</b>
                                         Jia Y,Shelhamer E,Donahue J,et al.Caffe:convolutional architecture for fast feature embedding[C]//Proceedings of the ACM International Conference on Multimedia,2014:675-678.
                                    </a>
                                </li>
                                <li id="185">


                                    <a id="bibliography_5" title=" Liu W,Wang J,Ji R,et al.Supervised hashing with kernels[C]//2012 IEEE Conference on Computer Vision and Pattern Recognition.IEEE,2012:2074-2081." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Supervised hashing with kernels">
                                        <b>[5]</b>
                                         Liu W,Wang J,Ji R,et al.Supervised hashing with kernels[C]//2012 IEEE Conference on Computer Vision and Pattern Recognition.IEEE,2012:2074-2081.
                                    </a>
                                </li>
                                <li id="187">


                                    <a id="bibliography_6" title=" Krizhevsky A,Sutskever I,Hinton G.ImageNet classification with deep convolutional neural networks[C]//Proceedings of the 25th International Conference on Neural Information Processing Systems-Volume 1.2012:1097-1105." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=ImageNet Classification with Deep Convolutional Neural Networks">
                                        <b>[6]</b>
                                         Krizhevsky A,Sutskever I,Hinton G.ImageNet classification with deep convolutional neural networks[C]//Proceedings of the 25th International Conference on Neural Information Processing Systems-Volume 1.2012:1097-1105.
                                    </a>
                                </li>
                                <li id="189">


                                    <a id="bibliography_7" title=" Szegedy C,Liu W,Jia Y,et al.Going deeper with convolutions[C]//2015 IEEE Conference on Computer Vision and Pattern Recognition(CVPR).IEEE,2015." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Going deeper with convolutions">
                                        <b>[7]</b>
                                         Szegedy C,Liu W,Jia Y,et al.Going deeper with convolutions[C]//2015 IEEE Conference on Computer Vision and Pattern Recognition(CVPR).IEEE,2015.
                                    </a>
                                </li>
                                <li id="191">


                                    <a id="bibliography_8" title=" He K,Zhang X,Ren S,et al.Delving deep into rectifiers:surpassing human-level performance on ImageNet classification[C]//2015 IEEE International Conference on Computer Vision(ICCV).IEEE,2015." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Delving Deep into Rectifiers:Surpassing Human-Level Performance on ImageNet Classification">
                                        <b>[8]</b>
                                         He K,Zhang X,Ren S,et al.Delving deep into rectifiers:surpassing human-level performance on ImageNet classification[C]//2015 IEEE International Conference on Computer Vision(ICCV).IEEE,2015.
                                    </a>
                                </li>
                                <li id="193">


                                    <a id="bibliography_9" title=" Szegedy C,Toshev A,Erhan D.Deep neural networks for object detection[C]//Advances in neural information processing systems,2013:2553-2561." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep Neural Networks for Object Detection">
                                        <b>[9]</b>
                                         Szegedy C,Toshev A,Erhan D.Deep neural networks for object detection[C]//Advances in neural information processing systems,2013:2553-2561.
                                    </a>
                                </li>
                                <li id="195">


                                    <a id="bibliography_10" title=" Wang M,Deng W H.Deep face recognition:a survey[C]//Computer Vision and Pattern Recognition(CVPR),2018." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep face recognition:a survey">
                                        <b>[10]</b>
                                         Wang M,Deng W H.Deep face recognition:a survey[C]//Computer Vision and Pattern Recognition(CVPR),2018.
                                    </a>
                                </li>
                                <li id="197">


                                    <a id="bibliography_11" title=" Long J,Shelhamer E,Darrell T.Fully convolutional networks for semantic segmentation[C]//2015 IEEE Conference on Computer Vision and Pattern Recognition(CVPR),2015:3431-3440." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fully convolutional netw orks for semantic segmentation">
                                        <b>[11]</b>
                                         Long J,Shelhamer E,Darrell T.Fully convolutional networks for semantic segmentation[C]//2015 IEEE Conference on Computer Vision and Pattern Recognition(CVPR),2015:3431-3440.
                                    </a>
                                </li>
                                <li id="199">


                                    <a id="bibliography_12" title=" Deng J,Ding N,Jia Y,et al.Large-scale object classification using label relation graphs[C]//ECCV 2014:European Conference on Computer Vision,2014:48-64." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Large-scale object classification using label relation graphs">
                                        <b>[12]</b>
                                         Deng J,Ding N,Jia Y,et al.Large-scale object classification using label relation graphs[C]//ECCV 2014:European Conference on Computer Vision,2014:48-64.
                                    </a>
                                </li>
                                <li id="201">


                                    <a id="bibliography_13" title=" Sun Y,Chen Y,Wang X,et al.Deep learning face representation by joint identification-verification[C]//Advances in Neural Information Processing Systems.2014:1988-1996." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep Learning Face Representation by Joint Identification-Verification">
                                        <b>[13]</b>
                                         Sun Y,Chen Y,Wang X,et al.Deep learning face representation by joint identification-verification[C]//Advances in Neural Information Processing Systems.2014:1988-1996.
                                    </a>
                                </li>
                                <li id="203">


                                    <a id="bibliography_14" title=" Mohedano E,McGuinness K,O&#39;Connor N E,et al.Bags of local convolutional features for scalable instance search[C]//Proceedings of the 2016 ACM on International Conference on Multimedia Retrieval.New York:ACM,2016:327-331." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Bags of Local Convolutional Features for Sca-lable Instance Search">
                                        <b>[14]</b>
                                         Mohedano E,McGuinness K,O&#39;Connor N E,et al.Bags of local convolutional features for scalable instance search[C]//Proceedings of the 2016 ACM on International Conference on Multimedia Retrieval.New York:ACM,2016:327-331.
                                    </a>
                                </li>
                                <li id="205">


                                    <a id="bibliography_15" title=" Weiss Y,Torralba A,Fergus R.Spectral hashing[C]//International Conference on Neural Information Processing Systems.Curran Associates Inc.2008:1753-1760." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Spectral Hashing">
                                        <b>[15]</b>
                                         Weiss Y,Torralba A,Fergus R.Spectral hashing[C]//International Conference on Neural Information Processing Systems.Curran Associates Inc.2008:1753-1760.
                                    </a>
                                </li>
                                <li id="207">


                                    <a id="bibliography_16" title=" Lai H,Pan Y,Liu Y,et al.Simultaneous feature learning and hash coding with deep neural networks[C]//2015 IEEE Conference on Computer Vision and Pattern Recognition(CVPR).IEEE,2015:3270-3278." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Simultaneous feature learning and hash coding with deep neural networks">
                                        <b>[16]</b>
                                         Lai H,Pan Y,Liu Y,et al.Simultaneous feature learning and hash coding with deep neural networks[C]//2015 IEEE Conference on Computer Vision and Pattern Recognition(CVPR).IEEE,2015:3270-3278.
                                    </a>
                                </li>
                                <li id="209">


                                    <a id="bibliography_17" title=" Yang H F,Lin K,Chen C S.Supervised learning of semantics-preserving hash via deep convolutional neural networks[J].IEEE Transactions on Pattern Analysis &amp;amp; Machine Intelligence,2018,40(2):437-451." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Supervised learning of semantics-preserving hash via deep convolutional neural networks">
                                        <b>[17]</b>
                                         Yang H F,Lin K,Chen C S.Supervised learning of semantics-preserving hash via deep convolutional neural networks[J].IEEE Transactions on Pattern Analysis &amp;amp; Machine Intelligence,2018,40(2):437-451.
                                    </a>
                                </li>
                                <li id="211">


                                    <a id="bibliography_18" title=" Datar M,Immorlica N,Indyk P,et al.Locality-sensitive hashing scheme based on p-stable distributions[C]//Proceedings of the twentieth annual symposium on Computational geometry.New York:ACM,2004:253-262." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Locality-sensitive hashing scheme based on p-stable distributions">
                                        <b>[18]</b>
                                         Datar M,Immorlica N,Indyk P,et al.Locality-sensitive hashing scheme based on p-stable distributions[C]//Proceedings of the twentieth annual symposium on Computational geometry.New York:ACM,2004:253-262.
                                    </a>
                                </li>
                                <li id="213">


                                    <a id="bibliography_19" title=" Gionis A,Indyk P,Motwani R.Similarity search in high dimensions via hashing[C]//Proceedings of the 25th International Conference on Very Large Data Bases.Morgan Kaufmann Publishers Inc.,1999:518-529." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Similarity search in highdimensions via hashing">
                                        <b>[19]</b>
                                         Gionis A,Indyk P,Motwani R.Similarity search in high dimensions via hashing[C]//Proceedings of the 25th International Conference on Very Large Data Bases.Morgan Kaufmann Publishers Inc.,1999:518-529.
                                    </a>
                                </li>
                                <li id="215">


                                    <a id="bibliography_20" title=" Lu J W,Liong V E,Zhou J.Deep hashing for scalable image search[J].IEEE Transactions on Image Processing,2017,26(5):2352-2367." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep hashing for scalable image search">
                                        <b>[20]</b>
                                         Lu J W,Liong V E,Zhou J.Deep hashing for scalable image search[J].IEEE Transactions on Image Processing,2017,26(5):2352-2367.
                                    </a>
                                </li>
                                <li id="217">


                                    <a id="bibliography_21" title=" Oliva A,Torralba A.Modeling the shape of the scene:a holistic representation of the spatial envelope[J].International Journal of Computer Vision,2001,42(3):145-175." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002830679&amp;v=MDU2NzhxZWJ1ZHRGU2psVkx2UElGMD1OajdCYXJPNEh0SE9wNHhGWXV3R1kzazV6QmRoNGo5OVNYcVJyeG94Y01IN1I3&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[21]</b>
                                         Oliva A,Torralba A.Modeling the shape of the scene:a holistic representation of the spatial envelope[J].International Journal of Computer Vision,2001,42(3):145-175.
                                    </a>
                                </li>
                                <li id="219">


                                    <a id="bibliography_22" title=" Kulis B,Darrell T.Learning to hash with binary reconstructive embeddings[C]//Proceedings of the 22nd International Conference on Neural Information Processing Systems.Curran Associates Inc.,2009:1042-1050." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning to Hash with Binary Reconstructive Embeddings">
                                        <b>[22]</b>
                                         Kulis B,Darrell T.Learning to hash with binary reconstructive embeddings[C]//Proceedings of the 22nd International Conference on Neural Information Processing Systems.Curran Associates Inc.,2009:1042-1050.
                                    </a>
                                </li>
                                <li id="221">


                                    <a id="bibliography_23" title=" Xia R,Pan Y,Lai H,et al.Supervised hashing for image retrieval via image representation learning[C]//Proceedings of the National Conference on Artificial Intelligence,2014:2156-2162." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Supervised hashing for image retrieval via image representation learning">
                                        <b>[23]</b>
                                         Xia R,Pan Y,Lai H,et al.Supervised hashing for image retrieval via image representation learning[C]//Proceedings of the National Conference on Artificial Intelligence,2014:2156-2162.
                                    </a>
                                </li>
                                <li id="223">


                                    <a id="bibliography_24" title=" Huang G,Liu Z,Maaten L V D,et al.Densely connected convolutional networks[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition,CVPR 2017,Honolulu,HI,USA,July 21-26,2017.IEEE Computer Society,2017." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Densely connected convolutional networks">
                                        <b>[24]</b>
                                         Huang G,Liu Z,Maaten L V D,et al.Densely connected convolutional networks[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition,CVPR 2017,Honolulu,HI,USA,July 21-26,2017.IEEE Computer Society,2017.
                                    </a>
                                </li>
                                <li id="225">


                                    <a id="bibliography_25" title=" Japkowicz N,Holte R.Learning from imbalanced data sets[J].AI Magazine,2001,22(1):131." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning from Imbalanced Data Sets">
                                        <b>[25]</b>
                                         Japkowicz N,Holte R.Learning from imbalanced data sets[J].AI Magazine,2001,22(1):131.
                                    </a>
                                </li>
                                <li id="227">


                                    <a id="bibliography_26" title=" Wang J,Zhang T,Song J,et al.A survey on learning to hash[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2018,40(4):769-790." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A survey on learning to hash">
                                        <b>[26]</b>
                                         Wang J,Zhang T,Song J,et al.A survey on learning to hash[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2018,40(4):769-790.
                                    </a>
                                </li>
                                <li id="229">


                                    <a id="bibliography_27" title=" Cao Y,Long M,Liu B,et al.Deep cauchy hashing for hamming space retrieval[C]//2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition(CVPR).IEEE,2018." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep cauchy hashing for hamming space retrieval">
                                        <b>[27]</b>
                                         Cao Y,Long M,Liu B,et al.Deep cauchy hashing for hamming space retrieval[C]//2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition(CVPR).IEEE,2018.
                                    </a>
                                </li>
                                <li id="231">


                                    <a id="bibliography_28" title=" Jiang Q Y,Cui X,Li W J,et al.Deep discrete supervised hashing[J].IEEE Transactions on Image Processing,2018,27(12):5996-6009." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep discrete supervised hashing">
                                        <b>[28]</b>
                                         Jiang Q Y,Cui X,Li W J,et al.Deep discrete supervised hashing[J].IEEE Transactions on Image Processing,2018,27(12):5996-6009.
                                    </a>
                                </li>
                                <li id="233">


                                    <a id="bibliography_29" title=" Jeong D J,Choo S K,Seo W,et al.Classification-Based Supervised Hashing with Complementary Networks for Image Search[C]//British Machine Vision Conference 2018,BMVC 2018,Northumbria University,Newcastle,UK,September 3-6,2018.BMVA Press,2018." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Classification-Based Supervised Hashing with Complementary Networks for Image Search">
                                        <b>[29]</b>
                                         Jeong D J,Choo S K,Seo W,et al.Classification-Based Supervised Hashing with Complementary Networks for Image Search[C]//British Machine Vision Conference 2018,BMVC 2018,Northumbria University,Newcastle,UK,September 3-6,2018.BMVA Press,2018.
                                    </a>
                                </li>
                                <li id="235">


                                    <a id="bibliography_30" title=" Wu J X,Luo J H.Learning Effective Binary Visual Representations with Deep Networks[EB].arXiv:1803.03004,2018." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning Effective Binary Visual Representations with Deep Networks[EB]">
                                        <b>[30]</b>
                                         Wu J X,Luo J H.Learning Effective Binary Visual Representations with Deep Networks[EB].arXiv:1803.03004,2018.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JYRJ" target="_blank">计算机应用与软件</a>
                2019,36(11),229-234 DOI:10.3969/j.issn.1000-386x.2019.11.037            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于深度多监督哈希的快速图像检索</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%83%81%E5%BB%B6%E7%8F%8D&amp;code=43235603&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">郁延珍</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%A4%8D%E6%97%A6%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E6%8A%80%E6%9C%AF%E5%AD%A6%E9%99%A2&amp;code=0075855&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">复旦大学计算机科学技术学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>由于较低的检索时间和空间复杂度,哈希方法被广泛应用于大规模图像检索领域。提出深度多监督哈希(Deep Multi-Supervised Hashing,DMSH)方法来学习具有高度判别能力和紧凑的哈希编码,并进行有效的图像检索。设计一个新的卷积神经网络结构来产生相似性保留的哈希编码,用一个识别信号来增加类间距离,用一个验证信号来降低类间距离。同时,通过正则化的方式降低网络输出和二进制哈希编码之间的损失并使二进制哈希值在每一维上均匀分布使网络输出更接近离散的哈希值。在两个数据集上的实验证明了该方法能够快速编码任意新的图像并取得先进的检索结果。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%9B%BE%E5%83%8F%E6%A3%80%E7%B4%A2&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">图像检索;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E5%A4%9A%E7%9B%91%E7%9D%A3%E5%93%88%E5%B8%8C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度多监督哈希;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">卷积神经网络;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    郁延珍，硕士，主研领域:多媒体信息处理与检索。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-02-20</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家重点研发计划项目(2016YFC0801003);</span>
                                <span>上海市科委科研计划项目(17511108504);</span>
                    </p>
            </div>
                    <h1><b>FAST IMAGE RETRIEVAL BASED ON DEEP MULTI-SUPERVISED HASHING</b></h1>
                    <h2>
                    <span>Yu Yanzhen</span>
            </h2>
                    <h2>
                    <span>School of Computer Science, Fudan University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Hashing methods have been widely applied in the field of large-scale image retrieval due to their low retrieval time and space complexity. This paper proposed deep multi-supervised hashing(DMSH) to learn highly discriminative and compact hash code for efficient image retrieval. We desinged a new CNN architecture to produce similarity-perserving hash code. A recognition signal was used to increase the inter-class distance and a verification signal was used to reduce the intra-class distance. We also applied regularization to reduce the loss between network output and binary hash code, and the binary hash value was evenly distributed in each dimension to make the network output closer to the discrete hash value. The experiments conducted on two datasets demonstrate that our proposed DMSH can fast encode any new-coming images and yield the state-of-the-art retrieval performance.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Image%20retrieval&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Image retrieval;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Deep%20multi-supervised%20hashing&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Deep multi-supervised hashing;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Convolutional%20neural%20network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Convolutional neural network;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2019-02-20</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="63" name="63" class="anchor-tag"><b>0 引 言</b></h3>
                <div class="p1">
                    <p id="64">随着大规模图像检索需求的日益增加,为了解决使用实值特征进行图像检索效率较低的问题,哈希方法开始被用来将高维的图像数据映射到紧凑的二进制编码空间,产生可以近似地保留图像原始空间信息的哈希码<citation id="237" type="reference"><link href="181" rel="bibliography" /><link href="183" rel="bibliography" /><link href="185" rel="bibliography" /><sup>[<a class="sup">3</a>,<a class="sup">4</a>,<a class="sup">5</a>]</sup></citation>。在基于哈希的图像检索中,图像由二进制码而不是实值特征表示,检索的时间和空间开销都大大降低,因而能够在数据库中更快地检索到用户所需要的图像信息。</p>
                </div>
                <div class="p1">
                    <p id="65">近年来,随着深度学习理论的飞速发展,以卷积神经网络(Convolutional Neutral Network,CNN)为代表的多种神经网络在诸如图像分类<citation id="240" type="reference"><link href="187" rel="bibliography" /><link href="189" rel="bibliography" /><link href="191" rel="bibliography" /><sup>[<a class="sup">6</a>,<a class="sup">7</a>,<a class="sup">8</a>]</sup></citation>、物体检测<citation id="238" type="reference"><link href="193" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>、人脸识别<citation id="239" type="reference"><link href="195" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>以及其他视觉任务<citation id="241" type="reference"><link href="197" rel="bibliography" /><link href="199" rel="bibliography" /><sup>[<a class="sup">11</a>,<a class="sup">12</a>]</sup></citation>领域不断取得突破。这些任务的成功表明从CNN上学得的特征可以很好地捕捉到图像的潜在语义结构信息。</p>
                </div>
                <h3 id="66" name="66" class="anchor-tag"><b>1 相关工作</b></h3>
                <div class="p1">
                    <p id="67">深度学习理论的飞速发展及其在众多领域的成功应用,向我们证明了CNN网络的强大学习能力。然而,主流的深度学习方法往往使用较高维度的特征,这极大地增加了图像检索的时间和空间开销。为了提高检索的效率,哈希方法,如LSH<citation id="242" type="reference"><link href="213" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>开始应用于图像检索领域,哈希特征的使用大大降低了图像检索在空间和时间上的开销。然而,哈希方法的检索准确率往往取决于它们所使用的特征,而手工编码的特征只能编码线性特征,无法捕捉图像的深层语义信息。</p>
                </div>
                <div class="p1">
                    <p id="68">最近,很多基于卷积神经网络的哈希方法被提出,如文献<citation id="243" type="reference">[<a class="sup">1</a>,<a class="sup">2</a>,<a class="sup">14</a>,<a class="sup">15</a>,<a class="sup">23</a>,<a class="sup">26</a>,<a class="sup">27</a>]</citation>,用来解决快速图像检索的问题。这些方法表明:深度卷积神经网络可以有效地编码非线性函数,图像特征及其对应的哈希函数都可以通过深度卷积神经网络学习得到。</p>
                </div>
                <div class="p1">
                    <p id="69">文献<citation id="244" type="reference">[<a class="sup">2</a>]</citation>通过挖掘深度卷积网络中的一个隐藏层来代表决定图像标签的隐藏信息,并通过哈希得到了可用于大规模图像检索的特征。文献<citation id="245" type="reference">[<a class="sup">23</a>]</citation>通过挖掘图像对之间的相似性矩阵来学习哈希编码。文献<citation id="246" type="reference">[<a class="sup">1</a>]</citation>利用图像的相似信息训练网络并提取特征,将求特征和哈希结合在同一个网络中,避免了二次优化的问题。文献<citation id="247" type="reference">[<a class="sup">14</a>]</citation>通过将CNN卷积层的特征进行组合,将图像的局部特征应用于尺度变化的图像实例检索并取得了较好的效果。在人脸识别领域中,DeepID2<citation id="248" type="reference"><link href="201" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>首次将人脸的分类信息和验证信息联系在一起,通过扩大类间的距离并减小类内距离大大提升了人脸识别算法的准确率,但是这些方法存在以下缺点,从而限制了检索准确率的提升:</p>
                </div>
                <div class="p1">
                    <p id="70">1) 只使用了分类信息和验证信息的一种;</p>
                </div>
                <div class="p1">
                    <p id="71">2) 没有考虑数据不平衡的问题。</p>
                </div>
                <div class="p1">
                    <p id="72">本文将图像表示和哈希方法结合起来,提出了深度多监督哈希DMSH,该方法从特征和哈希两个角度来入手,使用多监督信息来对网络进行调整,从而获得更优的哈希特征。实验表明,该方法比目前主流的方法检索效果更好。本文工作的贡献主要如下:</p>
                </div>
                <div class="p1">
                    <p id="73">1) 提出了一个深度多监督网络模型(DMSH)来学习哈希编码,使样本之间的非线性关系可以被有效编码。</p>
                </div>
                <div class="p1">
                    <p id="74">2) 采用多监督的方式,同时使用分类信息和验证信息来学习得到可区分的哈希编码。</p>
                </div>
                <div class="p1">
                    <p id="75">3) 针对正负样本不平衡问题,采用了合适的方式进行处理,解决了样本不平衡的问题。</p>
                </div>
                <h3 id="76" name="76" class="anchor-tag"><b>2 方法设计</b></h3>
                <div class="p1">
                    <p id="77">在图像检索领域,特征的鲁棒性、可区分性以及编码紧凑性对于提高图像检索的准确率和效率至关重要。为了能够同时满足这三个方面的要求,提高图像检索的表现,本文提出了一种深度多监督哈希(DMSH)方法。</p>
                </div>
                <div class="p1">
                    <p id="78">我们从DenseNet<citation id="249" type="reference"><link href="223" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>得到启发,使用Block作为网络的基础结构,在每个Block内部,层与层之间直接连接,保证了最大信息传输。DMSH网络包含38个Block结构,每个结构两两相连,后接两个全连接层和一个特征层。DMSH网络结构如图1所示。</p>
                </div>
                <div class="area_img" id="79">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201911038_079.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 DMSH网络结构" src="Detail/GetImg?filename=images/JYRJ201911038_079.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 DMSH网络结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201911038_079.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="80">目标函数通过可区分项(分类信息、验证信息)以及哈希项来优化网络,从而学习到具有可区分性的哈希特征。具体来说,网络模型通过顶层的三个约束任务学习得到:</p>
                </div>
                <div class="p1">
                    <p id="81">1) 通过分类任务,增加图像的类间距离;</p>
                </div>
                <div class="p1">
                    <p id="82">2) 利用验证任务,减小图像的类内距离;</p>
                </div>
                <div class="p1">
                    <p id="83">3) 通过哈希监督,获得有效的哈希编码。</p>
                </div>
                <div class="p1">
                    <p id="84">训练模型时,图像以组为单位进入网络,先经过一个卷积层,然后经过多个Block结构以及其后的卷积采样层,最后经过两个全连接层和一个特征层,分别进入分类、验证和哈希分支。</p>
                </div>
                <div class="p1">
                    <p id="85">在验证分支中,在网络内部组合成图像对,并根据标签信息来判断两图像是否相似。</p>
                </div>
                <h4 class="anchor-tag" id="86" name="86"><b>2.1 分类任务</b></h4>
                <div class="p1">
                    <p id="87">为了保证编码的可区分性,学习到的特征应该能很好地预测图像的标签信息。因此,分类任务使用softmax层将每幅图像分类到不同的<i>n</i>类中,softmax层输出图像在<i>n</i>类上的概率分布。输入图像<i>i</i>,损失函数表达式为:</p>
                </div>
                <div class="p1">
                    <p id="88"><mathml id="147"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><msub><mrow></mrow><mn>1</mn></msub><mspace width="0.25em" /><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo>=</mo><mo>-</mo><mi>p</mi><msub><mrow></mrow><mi>i</mi></msub><mrow><mi>log</mi></mrow><mo stretchy="false">(</mo><mover><mstyle mathsize="140%" displaystyle="true"><mrow><mi>p</mi><msub><mrow></mrow><mi>l</mi></msub></mrow></mstyle><mo>^</mo></mover><mo stretchy="false">)</mo></mrow></math></mathml>      (1)</p>
                </div>
                <div class="p1">
                    <p id="89">式中:<i>t</i>为目标类别,<mathml id="148"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mrow><mi>p</mi><msub><mrow></mrow><mi>l</mi></msub></mrow></mstyle><mo>^</mo></mover></mrow></math></mathml>为预测概率分布,<i>p</i><sub><i>i</i></sub>是目标概率分布,对任意<i>i</i>≠<i>t</i>来说,<i>p</i><sub><i>i</i></sub>=0,<i>p</i><sub><i>t</i></sub>=1。</p>
                </div>
                <h4 class="anchor-tag" id="90" name="90"><b>2.2 验证任务</b></h4>
                <div class="p1">
                    <p id="91">图像检索的准确率相似图像对的特征也尽量相似,非相似图像对的特征也尽量不相似。在衡量特征相似程度时,一般采用三种方法,分别为:<i>L</i><sub>1</sub>、<i>L</i><sub>2</sub>和余弦相似性。因为网络最终输出结果为哈希特征,而<i>L</i><sub>2</sub>在欧氏空间和汉明空间上一致,因此我们使用<i>L</i><sub>2</sub>距离衡量特征之间的相似性。</p>
                </div>
                <div class="p1">
                    <p id="92">验证任务的目的是使相似的图像映射成相似的特征,非相似图像映射成非相似的特征。因此,相似图像对的特征应该越近越好,而非相似的图像对特征之间的距离应该足够大。</p>
                </div>
                <div class="p1">
                    <p id="93">在图像配对时,非相似图像对数要远多于相似图像的对数,从而造成了正负样本不平衡问题。某些分类任务中,非平衡数据的存在,甚至造成了正样本准确率接近100%,而负样本的准确率却只有0～10%。因此我们需要对这种不平衡问题进行处理。应对数据不平衡问题时常用的方法包括采样、数据加成、加权等方法<citation id="250" type="reference"><link href="225" rel="bibliography" /><sup>[<a class="sup">25</a>]</sup></citation>。根据本文的情况,由于数据样本足够多且比例相差不是特别悬殊,因此本文采用加权的方法来解决数据不平衡问题,通过添加惩罚项,增加相似图像对的权重,降低非相似图像对的权重,使得网络能够取得较好的效果。</p>
                </div>
                <div class="p1">
                    <p id="94">基于以上考虑,我们设计了验证任务的损失函数:</p>
                </div>
                <div class="p1">
                    <p id="95"><i>L</i><sub>2</sub>(<i>a</i><sub>1</sub>,<i>a</i><sub>2</sub>)=<i>αδ</i>(<i>y</i><sub>1</sub>=<i>y</i><sub>2</sub>)<i>J</i>(<i>a</i><sub>1</sub>,<i>a</i><sub>2</sub>)+</p>
                </div>
                <div class="p1">
                    <p id="96"><i>βδ</i>(<i>y</i><sub>1</sub>≠<i>y</i><sub>2</sub>)<i>K</i>(<i>a</i><sub>1</sub>,<i>a</i><sub>2</sub>)      (2)</p>
                </div>
                <div class="p1">
                    <p id="97">式中:<i>a</i><sub>1</sub>、<i>a</i><sub>2</sub>是从两幅不同图像中提取出的特征,当两幅图像相似时,<i>δ</i>(<i>y</i><sub>1</sub>=<i>y</i><sub>2</sub>)=1;当两图像不相似时<i>δ</i>(<i>y</i><sub>1</sub>=<i>y</i><sub>2</sub>)=0,<i>δ</i>(<i>y</i><sub>1</sub>≠<i>y</i><sub>2</sub>)与<i>δ</i>(<i>y</i><sub>1</sub>=<i>y</i><sub>2</sub>)正好相反。<mathml id="149"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>J</mi><mo stretchy="false">(</mo><mi>a</mi><msub><mrow></mrow><mn>1</mn></msub><mo>,</mo><mrow><mi>a</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">)</mo><mo>=</mo></mrow><mrow><mo>|</mo><mrow><mi>a</mi><msub><mrow></mrow><mn>1</mn></msub><mo>-</mo><mi>a</mi><msub><mrow></mrow><mn>2</mn></msub></mrow><mo>|</mo></mrow><msub><mrow></mrow><mn>2</mn></msub></mrow></math></mathml>,用来衡量两个特征之间的距离,<i>K</i>(<i>a</i><sub>1</sub>,<i>a</i><sub>2</sub>)=max(<i>m</i>-<i>J</i>(<i>a</i><sub>1</sub>,<i>a</i><sub>2</sub>),0)。<i>α</i>、<i>β</i>为超参,用于调节正负样本不平衡的问题,本文中,<i>α</i>=2,<i>β</i>=0.4。</p>
                </div>
                <div class="p1">
                    <p id="98">对于所有的图像对,验证任务的损失函数为:</p>
                </div>
                <div class="p1">
                    <p id="99" class="code-formula">
                        <mathml id="99"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>L</mi><msub><mrow></mrow><mn>2</mn></msub><mo>=</mo><mi>α</mi><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mi>δ</mi></mstyle></mrow></mstyle><mo stretchy="false">(</mo><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mi>y</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo><mi>J</mi><mo stretchy="false">(</mo><mi>a</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mi>a</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo><mo>+</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mi>β</mi><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mi>δ</mi></mstyle></mrow></mstyle><mo stretchy="false">(</mo><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>≠</mo><mi>y</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo><mi>Κ</mi><mo stretchy="false">(</mo><mi>a</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mi>a</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>3</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="100">式中:<i>a</i><sub><i>i</i></sub>∈{+1,-1},<i>i</i>∈{1,2,…,<i>N</i>}。</p>
                </div>
                <div class="p1">
                    <p id="101">经过实验我们发现,处理数据不平衡后,网络性能会有2%～5%的提升。</p>
                </div>
                <h4 class="anchor-tag" id="102" name="102"><b>2.3 哈希监督</b></h4>
                <div class="p1">
                    <p id="103">为了降低图像检索的空间和时间开销,本文将网络最终的输出为二进制哈希码。然而,如果直接将输出限定为二进制码,反向传播将变得很困难。而如果忽视这个限制,欧式空间和汉明空间会有很大的差异,从而造成检索准确率的下降。针对这个问题,常用的方法是使用sigmoid函数或者tanh函数来近似表示阈值,但是,使用这些非线性的函数会使网络收敛速度变慢。因此,本文在网络输出上直接进行正则化来输出近似二值的特征。</p>
                </div>
                <div class="p1">
                    <p id="104">网络使用欧氏距离对输出做一个二进制的限制,使输出近似二值的特征。我们使用基于<i>L</i><sub>2</sub>的正则化。为了使输出接近-1或+1,通过增加限制项,使得输出的每一维数据的绝对值与1的方差最大和最小化,即:</p>
                </div>
                <div class="p1">
                    <p id="105"><mathml id="150"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><msub><mrow></mrow><mn>3</mn></msub><mo>=</mo><mrow><mo>|</mo><mrow><mo stretchy="false">|</mo><mi>a</mi><mo stretchy="false">|</mo><mo>-</mo><mn>1</mn></mrow><mo>|</mo></mrow><msub><mrow></mrow><mn>2</mn></msub></mrow></math></mathml>      (4)</p>
                </div>
                <div class="p1">
                    <p id="106">式中:<i>a</i>是一个<i>k</i>维的向量。</p>
                </div>
                <div class="p1">
                    <p id="107">有了这个限制,网络产生的输出可以满足近似二进制的要求。此外,我们也测试了<i>L</i><sub>1</sub>正则化的方法,最后证明<i>L</i><sub>2</sub>优于<i>L</i><sub>1</sub>。</p>
                </div>
                <div class="p1">
                    <p id="108">另外,对特征加入了平衡性的因素,假设网络中每个哈希码输出的+1和-1的个数尽可能相同,这个要求可以通过让每个训练样本输出的每个bit有50%的概率为+1,50%的概率为-1来达到,如下式所示:</p>
                </div>
                <div class="p1">
                    <p id="109"><mathml id="151"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><msub><mrow></mrow><mn>4</mn></msub><mo>=</mo><mrow><mo>|</mo><mrow><mi>m</mi><mi>e</mi><mi>a</mi><mi>n</mi><mo stretchy="false">(</mo><mo stretchy="false">|</mo><mi>a</mi><mo stretchy="false">|</mo><mo stretchy="false">)</mo><mo>-</mo><mn>0</mn></mrow><mo>|</mo></mrow><msub><mrow></mrow><mn>2</mn></msub></mrow></math></mathml>      (5)</p>
                </div>
                <div class="p1">
                    <p id="110">本文方法受到了文献<citation id="251" type="reference">[<a class="sup">15</a>,<a class="sup">17</a>]</citation>的启发,也使用类似的方法来达到平衡性的要求,提高了特征的表现。</p>
                </div>
                <div class="p1">
                    <p id="111">根据上文所述,总体的损失函数如下:</p>
                </div>
                <div class="p1">
                    <p id="112"><mathml id="152"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mn>4</mn></munderover><mi>w</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mi>L</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></math></mathml>      (6)</p>
                </div>
                <div class="p1">
                    <p id="113">式中:<i>w</i><sub><i>i</i></sub>为权重系数。</p>
                </div>
                <div class="p1">
                    <p id="114">对于输入的任意图像,都可以从特征层提取一个<i>k</i>维的哈希特征(<i>k</i>=12,24,36,48 bit)。哈希码可以通过使用sgn()函数获得,当<i>a</i><sub><i>i</i></sub>&gt;0时,sgn()=1; 反之sgn()=-1。</p>
                </div>
                <h3 id="115" name="115" class="anchor-tag"><b>3 实 验</b></h3>
                <div class="p1">
                    <p id="116">为了证明本文方法的有效性,我们在两个广泛使用的数据集(CIFAR-10、NUS-WIDE)上进行实验。</p>
                </div>
                <h4 class="anchor-tag" id="117" name="117"><b>3.1 数据集</b></h4>
                <div class="p1">
                    <p id="118">在快速图像检索领域,CIFAR-10和NUS-WIDE是两个常用的数据集,将本文所提到的方法与其他主流方法进行了充分的比较。</p>
                </div>
                <div class="p1">
                    <p id="119">CIFAR-10:该数据集由60 000幅32×32的图像组成,共分为10个互不相交的类,每类6 000幅图像。图像类别由手工标注。</p>
                </div>
                <div class="p1">
                    <p id="120">NUS-WIDE:该数据集由269 648幅从Flicker上得到的图像组成。每一幅图像被手工标注为81个类别中的某个或某几个用于模型评估。参考DSH的方法,使用最常用的21个类别,每个类别下至少包含5 000幅图像,共计195 834幅。</p>
                </div>
                <div class="p1">
                    <p id="121">在基于CNN的哈希方法中,直接使用图像作为网络的输入,对于传统方法,使用512维的GIST特征作为输入。</p>
                </div>
                <h4 class="anchor-tag" id="122" name="122"><b>3.2 评价指标</b></h4>
                <div class="p1">
                    <p id="123">在我们的实验中,拥有相同标签的图像被归于相似图像,而标签均不相同的图像被认为是非相似图像。对于CIFAR-10,来自相同类的图像被认为是相似图像,反之则认为是非相似图像。对于NUS-WIDE,如果两幅图像至少共享一个标签,则认为它们是相似的,反之认为它们是非相似的。</p>
                </div>
                <div class="p1">
                    <p id="124">参照文献<citation id="252" type="reference">[<a class="sup">8</a>,<a class="sup">23</a>,<a class="sup">28</a>]</citation>,我们使用Mean Average Precision (MAP) 作为评价指标。将数据集随机划分为两部分,分别作为训练集与验证集。给定一幅查询图像,如果查询图与被查询图共享一个标签,那么它们是相似的;否则,它们是非相似的。实验将数据集随机划分为两部分,分别作为训练集与验证集。在本文中,训练集与验证集的比例为5∶1。</p>
                </div>
                <h4 class="anchor-tag" id="125" name="125"><b>3.3 对比实验及分析</b></h4>
                <div class="p1">
                    <p id="126">我们通过实验结果的对比来验证本文所提到方法的有效性。</p>
                </div>
                <div class="p1">
                    <p id="127">将本文提出的DMSH模型的检索效果与其他哈希方法进行比较,包括四个传统网络LSH<citation id="253" type="reference"><link href="193" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>、MLH<citation id="254" type="reference"><link href="217" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>、BRE<citation id="255" type="reference"><link href="219" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>、KSH<citation id="256" type="reference"><link href="185" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>和四个深度学习网络CNNH<citation id="257" type="reference"><link href="221" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>、DNNH<citation id="258" type="reference"><link href="207" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>、DSH<citation id="259" type="reference"><link href="177" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>、SDH<citation id="260" type="reference"><link href="215" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>。实验使用特征层作为输出,特征维度分别为12 bit、24 bit、36 bit和48 bit。实验在两个广泛使用的图像检索数据集上进行:CIFAR-10和NUS-WIDE。</p>
                </div>
                <div class="p1">
                    <p id="128">表1给出了不同方法在CIFAR-10和NUS-WIDE上的MAP结果。哈希码的长度从12 bit到48 bit。数据显示,本文所提出的DMSH对比其他方法在检索准确率上有较大的提升。特别地,DMSH在两个数据集上的检索性能比DSH分别提升了18.26%和10.26%。在CIFAR-10数据集上,在不同的编码长度上的检索效果,MAP在24 bit时取得最好的效果,从而表明了特征不是越长越好,紧凑的编码由此产生。</p>
                </div>
                <div class="area_img" id="129">
                                            <p class="img_tit">
                                                <b>表1 DMSH与其他方法在CIFAR-10和NUS-WIDE上的 检索MAP对比</b>
                                                
                                                &nbsp;&nbsp;
                                                <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201911038_12900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a>
                                                <a class="table downimg" data-tablename="Detail/GetImg?filename=images/JYRJ201911038_12900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">下载原表</a>
                                            </p>
                                    <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201911038_12900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <img alt="表1 DMSH与其他方法在CIFAR-10和NUS-WIDE上的 检索MAP对比" src="Detail/GetImg?filename=images/JYRJ201911038_12900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                    </a>

                </div>
                <div class="p1">
                    <p id="131">由表1可知:(1) 本文提出的方法取得了最好的效果并且相比其他哈希方法有显著提升;(2) 通过将图像特征和哈希特征结合到同一个模块中,取得了较好的效果;(3) 深层哈希方法比浅层哈希方法效果好。</p>
                </div>
                <div class="p1">
                    <p id="132">DMSH的优势主要体现在下面三个方面:(1) DMSH同时使用了分类和验证信息,而DSH只使用了验证信息;(2) DMSH的网络结构可以更好地挖掘图像的深层信息,DSH使用图的网络略显简单;(3) DMSH增加了对数据不平衡问题的处理。</p>
                </div>
                <div class="p1">
                    <p id="133">表2给出了多监督方法与非多监督在CIFAR-10上的MAP结果,可以看出,使用单一验证网络比单一分类网络效果好,而二者结合起来能够取得更好的效果。</p>
                </div>
                <div class="area_img" id="134">
                    <p class="img_tit"><b>表2 多监督方法与非多监督方法的MAP的比较</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="134" border="1"><tr><td><br />CIFAR-10</td><td>图像特征</td><td>哈希特征</td></tr><tr><td><br />单一分类网络</td><td>0.796 3</td><td>0.813 9</td></tr><tr><td><br />单一验证网络</td><td>0.839 2</td><td>0.871 6</td></tr><tr><td><br />分类+验证网络</td><td><b>0.871 5</b></td><td><b>0.892 6</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="135">表3给出了真值特征、哈希特征及其之间的损失,数据表明,真值特征和哈希特征之间损失很小。</p>
                </div>
                <div class="area_img" id="136">
                    <p class="img_tit"><b>表3 NUS-WIDE哈希效果对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note">%</p>
                    <table id="136" border="1"><tr><td rowspan="2"><br />指标</td><td colspan="4"><br />DMSH</td></tr><tr><td><br />12 bit</td><td>24 bit</td><td>36 bit</td><td>48 bit</td></tr><tr><td><br />真值特征</td><td>65.74</td><td>66.01</td><td>65.96</td><td>65.32</td></tr><tr><td><br />哈希特征</td><td>62.31</td><td>62.36</td><td>62.72</td><td>61.54</td></tr><tr><td><br />差值</td><td><b>3.43</b></td><td><b>3.65</b></td><td><b>3.24</b></td><td><b>3.78</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="137">通过实验和分析,我们可以得出以下几个结论:</p>
                </div>
                <div class="p1">
                    <p id="138">1) 本文所使用的真值特征和哈希特征之间的损失很小;</p>
                </div>
                <div class="p1">
                    <p id="139">2) 同时使用分类信息和验证信息可以更好地保留图像信息,提高检索精度;</p>
                </div>
                <div class="p1">
                    <p id="140">3) 深度学习网络可以很好地编码哈希函数,获得优秀的哈希特征。</p>
                </div>
                <h4 class="anchor-tag" id="141" name="141"><b>3.4 编码时间</b></h4>
                <div class="p1">
                    <p id="142">在实际应用中,对于一个新的图像,应该能够快速地提取其哈希编码。为了比较DMSH与其他8个监督哈希方法的编码时间,在CIFAR-10数据集上分别对24 bit和48 bit的编码长度进行了实验。为了全面的比较,分别在CPU和GPU上对基于卷积神经网络的方法进行了实验,得到传统哈希方法的特征提取时间,实验结果如图2所示。</p>
                </div>
                <div class="area_img" id="143">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201911038_143.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 新图像的编码时间/μs" src="Detail/GetImg?filename=images/JYRJ201911038_143.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 新图像的编码时间/μs  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201911038_143.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="144">可以看出,基于卷积神经网络的哈希方法对新图像的编码时间几乎相同,将特征提取时间包括在内后,深度学习哈希方法的编码速度比传统哈希方法快10倍以上。</p>
                </div>
                <h3 id="145" name="145" class="anchor-tag"><b>4 结 语</b></h3>
                <div class="p1">
                    <p id="146">本文提出了一种基于深度多监督的快速图像检索方法,该方法通过端到端的方式同时得到图像的真值特征和哈希特征,在此基础上,使用多监督信息,达到同时降低类内距离,增加类间距离的目的,大大提升了图像检索网络的准确率和效率。我们将DMSH的检索效果归功于以下三个方面:1) 使用多监督方法,在减小类内距离的同时增加类间距离;2) 采用合适的方法解决了正负样本不平衡的问题;3) 采用端到端的方式得到图像的哈希特征,并使用正则化的方式降低了图像真值特征和哈希特征二者的差异。实验表明,该方法可以快速将任意图像编码成紧凑的二进制哈希编码,有效提升图像检索的准确率。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="177">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep Supervised Hashing for Fast Image Retrieval">

                                <b>[1]</b> Liu H,Wang R,Shan S,et al.Deep supervised hashing for fast image retrieval[C]//2016 IEEE Conference on Computer Vision and Pattern Recognition(CVPR).IEEE Computer Society,2016:2064-2072.
                            </a>
                        </p>
                        <p id="179">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep Learning of Binary Hash Codes for Fast Image Retrieval">

                                <b>[2]</b> Lin K,Yang H F,Hsiao J H,et al.Deep learning of binary hash codes for fast image retrieval[C]//2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops(CVPRW).IEEE,2015:27-35.
                            </a>
                        </p>
                        <p id="181">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Semi-Supervised Hashing for Large-Scale Search">

                                <b>[3]</b> Wang J,Kumar S,Chang S F.Semi-supervised hashing for large-scale search[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2012,34(12):2393-2406.
                            </a>
                        </p>
                        <p id="183">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Caffe:convolutional architecture for fast feature embedding">

                                <b>[4]</b> Jia Y,Shelhamer E,Donahue J,et al.Caffe:convolutional architecture for fast feature embedding[C]//Proceedings of the ACM International Conference on Multimedia,2014:675-678.
                            </a>
                        </p>
                        <p id="185">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Supervised hashing with kernels">

                                <b>[5]</b> Liu W,Wang J,Ji R,et al.Supervised hashing with kernels[C]//2012 IEEE Conference on Computer Vision and Pattern Recognition.IEEE,2012:2074-2081.
                            </a>
                        </p>
                        <p id="187">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=ImageNet Classification with Deep Convolutional Neural Networks">

                                <b>[6]</b> Krizhevsky A,Sutskever I,Hinton G.ImageNet classification with deep convolutional neural networks[C]//Proceedings of the 25th International Conference on Neural Information Processing Systems-Volume 1.2012:1097-1105.
                            </a>
                        </p>
                        <p id="189">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Going deeper with convolutions">

                                <b>[7]</b> Szegedy C,Liu W,Jia Y,et al.Going deeper with convolutions[C]//2015 IEEE Conference on Computer Vision and Pattern Recognition(CVPR).IEEE,2015.
                            </a>
                        </p>
                        <p id="191">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Delving Deep into Rectifiers:Surpassing Human-Level Performance on ImageNet Classification">

                                <b>[8]</b> He K,Zhang X,Ren S,et al.Delving deep into rectifiers:surpassing human-level performance on ImageNet classification[C]//2015 IEEE International Conference on Computer Vision(ICCV).IEEE,2015.
                            </a>
                        </p>
                        <p id="193">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep Neural Networks for Object Detection">

                                <b>[9]</b> Szegedy C,Toshev A,Erhan D.Deep neural networks for object detection[C]//Advances in neural information processing systems,2013:2553-2561.
                            </a>
                        </p>
                        <p id="195">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep face recognition:a survey">

                                <b>[10]</b> Wang M,Deng W H.Deep face recognition:a survey[C]//Computer Vision and Pattern Recognition(CVPR),2018.
                            </a>
                        </p>
                        <p id="197">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fully convolutional netw orks for semantic segmentation">

                                <b>[11]</b> Long J,Shelhamer E,Darrell T.Fully convolutional networks for semantic segmentation[C]//2015 IEEE Conference on Computer Vision and Pattern Recognition(CVPR),2015:3431-3440.
                            </a>
                        </p>
                        <p id="199">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Large-scale object classification using label relation graphs">

                                <b>[12]</b> Deng J,Ding N,Jia Y,et al.Large-scale object classification using label relation graphs[C]//ECCV 2014:European Conference on Computer Vision,2014:48-64.
                            </a>
                        </p>
                        <p id="201">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep Learning Face Representation by Joint Identification-Verification">

                                <b>[13]</b> Sun Y,Chen Y,Wang X,et al.Deep learning face representation by joint identification-verification[C]//Advances in Neural Information Processing Systems.2014:1988-1996.
                            </a>
                        </p>
                        <p id="203">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Bags of Local Convolutional Features for Sca-lable Instance Search">

                                <b>[14]</b> Mohedano E,McGuinness K,O'Connor N E,et al.Bags of local convolutional features for scalable instance search[C]//Proceedings of the 2016 ACM on International Conference on Multimedia Retrieval.New York:ACM,2016:327-331.
                            </a>
                        </p>
                        <p id="205">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Spectral Hashing">

                                <b>[15]</b> Weiss Y,Torralba A,Fergus R.Spectral hashing[C]//International Conference on Neural Information Processing Systems.Curran Associates Inc.2008:1753-1760.
                            </a>
                        </p>
                        <p id="207">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Simultaneous feature learning and hash coding with deep neural networks">

                                <b>[16]</b> Lai H,Pan Y,Liu Y,et al.Simultaneous feature learning and hash coding with deep neural networks[C]//2015 IEEE Conference on Computer Vision and Pattern Recognition(CVPR).IEEE,2015:3270-3278.
                            </a>
                        </p>
                        <p id="209">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Supervised learning of semantics-preserving hash via deep convolutional neural networks">

                                <b>[17]</b> Yang H F,Lin K,Chen C S.Supervised learning of semantics-preserving hash via deep convolutional neural networks[J].IEEE Transactions on Pattern Analysis &amp; Machine Intelligence,2018,40(2):437-451.
                            </a>
                        </p>
                        <p id="211">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Locality-sensitive hashing scheme based on p-stable distributions">

                                <b>[18]</b> Datar M,Immorlica N,Indyk P,et al.Locality-sensitive hashing scheme based on p-stable distributions[C]//Proceedings of the twentieth annual symposium on Computational geometry.New York:ACM,2004:253-262.
                            </a>
                        </p>
                        <p id="213">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Similarity search in highdimensions via hashing">

                                <b>[19]</b> Gionis A,Indyk P,Motwani R.Similarity search in high dimensions via hashing[C]//Proceedings of the 25th International Conference on Very Large Data Bases.Morgan Kaufmann Publishers Inc.,1999:518-529.
                            </a>
                        </p>
                        <p id="215">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep hashing for scalable image search">

                                <b>[20]</b> Lu J W,Liong V E,Zhou J.Deep hashing for scalable image search[J].IEEE Transactions on Image Processing,2017,26(5):2352-2367.
                            </a>
                        </p>
                        <p id="217">
                            <a id="bibliography_21" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002830679&amp;v=MjE3ODZkaDRqOTlTWHFScnhveGNNSDdSN3FlYnVkdEZTamxWTHZQSUYwPU5qN0Jhck80SHRIT3A0eEZZdXdHWTNrNXpC&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[21]</b> Oliva A,Torralba A.Modeling the shape of the scene:a holistic representation of the spatial envelope[J].International Journal of Computer Vision,2001,42(3):145-175.
                            </a>
                        </p>
                        <p id="219">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning to Hash with Binary Reconstructive Embeddings">

                                <b>[22]</b> Kulis B,Darrell T.Learning to hash with binary reconstructive embeddings[C]//Proceedings of the 22nd International Conference on Neural Information Processing Systems.Curran Associates Inc.,2009:1042-1050.
                            </a>
                        </p>
                        <p id="221">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Supervised hashing for image retrieval via image representation learning">

                                <b>[23]</b> Xia R,Pan Y,Lai H,et al.Supervised hashing for image retrieval via image representation learning[C]//Proceedings of the National Conference on Artificial Intelligence,2014:2156-2162.
                            </a>
                        </p>
                        <p id="223">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Densely connected convolutional networks">

                                <b>[24]</b> Huang G,Liu Z,Maaten L V D,et al.Densely connected convolutional networks[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition,CVPR 2017,Honolulu,HI,USA,July 21-26,2017.IEEE Computer Society,2017.
                            </a>
                        </p>
                        <p id="225">
                            <a id="bibliography_25" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning from Imbalanced Data Sets">

                                <b>[25]</b> Japkowicz N,Holte R.Learning from imbalanced data sets[J].AI Magazine,2001,22(1):131.
                            </a>
                        </p>
                        <p id="227">
                            <a id="bibliography_26" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A survey on learning to hash">

                                <b>[26]</b> Wang J,Zhang T,Song J,et al.A survey on learning to hash[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2018,40(4):769-790.
                            </a>
                        </p>
                        <p id="229">
                            <a id="bibliography_27" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep cauchy hashing for hamming space retrieval">

                                <b>[27]</b> Cao Y,Long M,Liu B,et al.Deep cauchy hashing for hamming space retrieval[C]//2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition(CVPR).IEEE,2018.
                            </a>
                        </p>
                        <p id="231">
                            <a id="bibliography_28" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep discrete supervised hashing">

                                <b>[28]</b> Jiang Q Y,Cui X,Li W J,et al.Deep discrete supervised hashing[J].IEEE Transactions on Image Processing,2018,27(12):5996-6009.
                            </a>
                        </p>
                        <p id="233">
                            <a id="bibliography_29" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Classification-Based Supervised Hashing with Complementary Networks for Image Search">

                                <b>[29]</b> Jeong D J,Choo S K,Seo W,et al.Classification-Based Supervised Hashing with Complementary Networks for Image Search[C]//British Machine Vision Conference 2018,BMVC 2018,Northumbria University,Newcastle,UK,September 3-6,2018.BMVA Press,2018.
                            </a>
                        </p>
                        <p id="235">
                            <a id="bibliography_30" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning Effective Binary Visual Representations with Deep Networks[EB]">

                                <b>[30]</b> Wu J X,Luo J H.Learning Effective Binary Visual Representations with Deep Networks[EB].arXiv:1803.03004,2018.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JYRJ201911038" />
        <input id="dpi" type="hidden" value="96" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201911038&amp;v=MDAyNDF6VFpaTEc0SDlqTnJvOUdiSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnVGeS9oVkwvTEw=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="0" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
