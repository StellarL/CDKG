<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637136397111221250%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJYRJ201903037%26RESULT%3d1%26SIGN%3dgcso3l%252bo4PNODOVi%252bHicjr4B570%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JYRJ201903037&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JYRJ201903037&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201903037&amp;v=MDMyNzVxcUJ0R0ZyQ1VSN3FmWnVac0ZpRGxWNzdQTHpUWlpMRzRIOWpNckk5R1k0UUtESDg0dlI0VDZqNTRPM3o=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#63" data-title="&lt;b&gt;0 引 言&lt;/b&gt; "><b>0 引 言</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#70" data-title="&lt;b&gt;1 人群异常行为检测流程与方法研究&lt;/b&gt; "><b>1 人群异常行为检测流程与方法研究</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#73" data-title="&lt;b&gt;1.1 检测和提取运动目标&lt;/b&gt;"><b>1.1 检测和提取运动目标</b></a></li>
                                                <li><a href="#108" data-title="&lt;b&gt;1.2 获取运动目标的特征信息&lt;/b&gt;"><b>1.2 获取运动目标的特征信息</b></a></li>
                                                <li><a href="#129" data-title="&lt;b&gt;1.3 构建卷积神经网络嵌套模型&lt;/b&gt;"><b>1.3 构建卷积神经网络嵌套模型</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#141" data-title="&lt;b&gt;2 实验与结果分析&lt;/b&gt; "><b>2 实验与结果分析</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#144" data-title="&lt;b&gt;2.1 在UCSD数据集上实验结果&lt;/b&gt;"><b>2.1 在UCSD数据集上实验结果</b></a></li>
                                                <li><a href="#150" data-title="&lt;b&gt;2.2 在UMN数据集上实验结果&lt;/b&gt;"><b>2.2 在UMN数据集上实验结果</b></a></li>
                                                <li><a href="#153" data-title="&lt;b&gt;2.3 时间复杂度分析对比&lt;/b&gt;"><b>2.3 时间复杂度分析对比</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#170" data-title="&lt;b&gt;3 结 语&lt;/b&gt; "><b>3 结 语</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#72" data-title="图1 人群异常行为检测算法流程图">图1 人群异常行为检测算法流程图</a></li>
                                                <li><a href="#75" data-title="图2 混合高斯模型的工作流程图">图2 混合高斯模型的工作流程图</a></li>
                                                <li><a href="#107" data-title="图3 与其他方法提取前景目标的结果对比">图3 与其他方法提取前景目标的结果对比</a></li>
                                                <li><a href="#140" data-title="图4 基于卷积神经网络的嵌套模型">图4 基于卷积神经网络的嵌套模型</a></li>
                                                <li><a href="#146" data-title="图5 UCSD数据集中非人类实体入侵和异常人类行为示例">图5 UCSD数据集中非人类实体入侵和异常人类行为示例</a></li>
                                                <li><a href="#149" data-title="&lt;b&gt;表1 AUC和EER在于Ped1 (UCSD数据集) 上的帧和像素级比较&lt;/b&gt;"><b>表1 AUC和EER在于Ped1 (UCSD数据集) 上的帧和像素级比较</b></a></li>
                                                <li><a href="#152" data-title="&lt;b&gt;表2 AUC和EER用于UMN数据集上的帧和像素级比较&lt;/b&gt;"><b>表2 AUC和EER用于UMN数据集上的帧和像素级比较</b></a></li>
                                                <li><a href="#169" data-title="&lt;b&gt;表3 本文算法与其他算法时间复杂度&lt;/b&gt;"><b>表3 本文算法与其他算法时间复杂度</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" title=" Shan Y H, Zhang Z, Huang K Q. Review, Current Situation and Prospect of Human Visual Behavior Recognition[J]. Journal of Computer Research and Development, 2016, 53 (1) : 93-112." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Review, Current Situation and Prospect of Human Visual Behavior Recognition">
                                        <b>[1]</b>
                                         Shan Y H, Zhang Z, Huang K Q. Review, Current Situation and Prospect of Human Visual Behavior Recognition[J]. Journal of Computer Research and Development, 2016, 53 (1) : 93-112.
                                    </a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" title=" Rabiee H, Haddadnia J, Mousavi H, et al. Novel dataset for fine-grained abnormal behavior understanding in crowd[C]//IEEE International Conference on Advanced Video and Signal Based Surveillance. IEEE, 2016:95-101." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Novel dataset for fine-grained abnormal behavior understanding in crowd">
                                        <b>[2]</b>
                                         Rabiee H, Haddadnia J, Mousavi H, et al. Novel dataset for fine-grained abnormal behavior understanding in crowd[C]//IEEE International Conference on Advanced Video and Signal Based Surveillance. IEEE, 2016:95-101.
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title=" Vishwakarma S. A survey on activity recognition and behavior understanding in video surveillance[J]. Visual Computer, 2013, 29 (10) :983-1009." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD13101000017144&amp;v=MDE4MDRyUmRHZXJxUVRNbndaZVp0RmlubFVyM0pLRjRRYXhVPU5qN0Jhcks3SDlITnI0OUZaT29JRFhnOW9CTVQ2VDRQUUgvaQ==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         Vishwakarma S. A survey on activity recognition and behavior understanding in video surveillance[J]. Visual Computer, 2013, 29 (10) :983-1009.
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" title=" Feng Y, Yuan Y, Lu X. Learning deep event models for crowd anomaly detection[J]. Neurocomputing, 2016, 219: 548-556." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning deep event models for crowd anomaly detection">
                                        <b>[4]</b>
                                         Feng Y, Yuan Y, Lu X. Learning deep event models for crowd anomaly detection[J]. Neurocomputing, 2016, 219: 548-556.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" title=" H&#228;m&#228;l&#228;inen W, Nyk&#228;nen M. Efficient discovery of statistically significant association rules[C]//Eighth IEEE International Conference on Data Mining. IEEE Computer Society, 2008:203-212." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Efficient discovery of statistically significant association rules">
                                        <b>[5]</b>
                                         H&#228;m&#228;l&#228;inen W, Nyk&#228;nen M. Efficient discovery of statistically significant association rules[C]//Eighth IEEE International Conference on Data Mining. IEEE Computer Society, 2008:203-212.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" title=" Kwon J, Lee K M. A unified framework for event summarization and rare event detection[C]//IEEE Conference on Computer Vision and Pattern Recognition. IEEE Computer Society, 2015:1266-1273." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A unified framework for event summarization and rare event detection">
                                        <b>[6]</b>
                                         Kwon J, Lee K M. A unified framework for event summarization and rare event detection[C]//IEEE Conference on Computer Vision and Pattern Recognition. IEEE Computer Society, 2015:1266-1273.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" title=" Yuan Y, Feng Y, Lu X. Statistical hypothesis detector for abnormal event detection in crowded scenes[J]. IEEE Transactions on Cybernetics, 2017, 47 (11) :3597-3608." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Statisticalhy pothesis detector for abnormal event detection in crowded scenes">
                                        <b>[7]</b>
                                         Yuan Y, Feng Y, Lu X. Statistical hypothesis detector for abnormal event detection in crowded scenes[J]. IEEE Transactions on Cybernetics, 2017, 47 (11) :3597-3608.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" title=" Fang J, Wang Q, Yuan Y. Part-based online tracking with geometry constraint and attention selection[J]. IEEE Transactions on Circuits &amp;amp; Systems for Video Technology, 2014, 24 (5) :854-864." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Part-based online tracking with geometry constraint and attention selection">
                                        <b>[8]</b>
                                         Fang J, Wang Q, Yuan Y. Part-based online tracking with geometry constraint and attention selection[J]. IEEE Transactions on Circuits &amp;amp; Systems for Video Technology, 2014, 24 (5) :854-864.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" title=" Yuan Y, Fang J, Wang Q. Online anomaly detection in crowd scenes via structure analysis[J]. IEEE Transactions on Cybernetics, 2015, 45 (3) :548-561." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Online anomaly detection in crowd scenes via structure analysis">
                                        <b>[9]</b>
                                         Yuan Y, Fang J, Wang Q. Online anomaly detection in crowd scenes via structure analysis[J]. IEEE Transactions on Cybernetics, 2015, 45 (3) :548-561.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_10" title=" Zhou S, Shen W, Zeng D, et al. Unusual event detection in crowded scenes by trajectory analysis[C]//IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE, 2015:1300-1304." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Unusual event detection in crowded scenes by trajectory analysis">
                                        <b>[10]</b>
                                         Zhou S, Shen W, Zeng D, et al. Unusual event detection in crowded scenes by trajectory analysis[C]//IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE, 2015:1300-1304.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_11" title=" Reddy V, Sanderson C, Lovell B C. Improved anomaly detection in crowded scenes via cell-based analysis of foreground speed, size and texture[C]//Computer Vision and Pattern Recognition Workshops. IEEE, 2011:55-61." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Improved anomaly detection in crowded scenes via cell-based analysis of foreground speed size and texture">
                                        <b>[11]</b>
                                         Reddy V, Sanderson C, Lovell B C. Improved anomaly detection in crowded scenes via cell-based analysis of foreground speed, size and texture[C]//Computer Vision and Pattern Recognition Workshops. IEEE, 2011:55-61.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_12" title=" Zhao B, Li F F, Xing E P. Online detection of unusual events in videos via dynamic sparse coding[C]//IEEE Conference on Computer Vision and Pattern Recognition. IEEE Computer Society, 2011:3313-3320." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Online detection of unusual events in videos via dynamic sparse coding">
                                        <b>[12]</b>
                                         Zhao B, Li F F, Xing E P. Online detection of unusual events in videos via dynamic sparse coding[C]//IEEE Conference on Computer Vision and Pattern Recognition. IEEE Computer Society, 2011:3313-3320.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_13" title=" Guo Y, Liu Y, Oerlemans A, et al. Deep learning for visual understanding: A review[J].Neurocomputing, 2016, 187 (C) :27-48." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES441041FD5D0BEA83528D7E392AC0EFDB&amp;v=MjMzMzltYUJ1SFlmT0dRbGZCckxVMDV0cGd6TGk3d2E4PU5pZk9mYmU4SDlISXJ2a3hZWjhQZmdsSXh4VVc2RGNKVHdyaHBSQkVDckxoTTg3dENPTnZGU2lXV3I3SklGcA==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[13]</b>
                                         Guo Y, Liu Y, Oerlemans A, et al. Deep learning for visual understanding: A review[J].Neurocomputing, 2016, 187 (C) :27-48.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_14" title=" Sabokrou M, Fayyaz M, Fathy M, et al. Deep-cascade: Cascading 3D deep neural networks for fast anomaly detection and localization in crowded scenes[J]. IEEE Transactions on Image Processing, 2017, 26 (4) :1992-2004." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep-cascade cascading 3d deep neural networks for fast anomaly detection and localization in crowded scenes">
                                        <b>[14]</b>
                                         Sabokrou M, Fayyaz M, Fathy M, et al. Deep-cascade: Cascading 3D deep neural networks for fast anomaly detection and localization in crowded scenes[J]. IEEE Transactions on Image Processing, 2017, 26 (4) :1992-2004.
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_15" title=" Ravanbakhsh M, Sangineto E, Nabi M, et al. Training adversarial discriminators for cross-channel abnormal event detection in crowds[EB]. arXiv:1706.07680, 2017." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Training adversarial discriminators for cross-channel abnormal event detection in crowds[EB]">
                                        <b>[15]</b>
                                         Ravanbakhsh M, Sangineto E, Nabi M, et al. Training adversarial discriminators for cross-channel abnormal event detection in crowds[EB]. arXiv:1706.07680, 2017.
                                    </a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_16" title=" Kiran B, Dilip T, Ranjith P. An overview of deep learning based methods for unsupervised and semi-supervised anomaly detection in videos[J]. Journal of Imaging, 2018, 4 (2) ." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=An overview of deep learning based methods for unsupervised and semi-supervised anomaly detection in videos">
                                        <b>[16]</b>
                                         Kiran B, Dilip T, Ranjith P. An overview of deep learning based methods for unsupervised and semi-supervised anomaly detection in videos[J]. Journal of Imaging, 2018, 4 (2) .
                                    </a>
                                </li>
                                <li id="35">


                                    <a id="bibliography_17" title=" Sajid H, Cheung S S. Universal multimode background subtraction[J]. IEEE Transactions on Image Processing A Publication of the IEEE Signal Processing Society, 2017, 26 (7) :3249-3260." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Universal Multimode Background Subtraction">
                                        <b>[17]</b>
                                         Sajid H, Cheung S S. Universal multimode background subtraction[J]. IEEE Transactions on Image Processing A Publication of the IEEE Signal Processing Society, 2017, 26 (7) :3249-3260.
                                    </a>
                                </li>
                                <li id="37">


                                    <a id="bibliography_18" title=" KaewTraKulPong P, Bowden R. An improved adaptive background mixture model for real-time tracking with shadow detection[M]//VIDEO BASED SURVEILLANCE SYSTEMS: Computer Vision and Distributed Processing. Kluwer Academic Publishers, 2002:135-144." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=An Improved Adaptive Background Mixture Model for Real-time Tracking with Shadow Detection">
                                        <b>[18]</b>
                                         KaewTraKulPong P, Bowden R. An improved adaptive background mixture model for real-time tracking with shadow detection[M]//VIDEO BASED SURVEILLANCE SYSTEMS: Computer Vision and Distributed Processing. Kluwer Academic Publishers, 2002:135-144.
                                    </a>
                                </li>
                                <li id="39">


                                    <a id="bibliography_19" title=" Liu J, Wang L. An improved algorithm for background method of mixed Gaussian model[J].Computer Engineering and Applications, 2010, 46 (13) :168-170." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=An improved algorithm for background method of mixed Gaussian model">
                                        <b>[19]</b>
                                         Liu J, Wang L. An improved algorithm for background method of mixed Gaussian model[J].Computer Engineering and Applications, 2010, 46 (13) :168-170.
                                    </a>
                                </li>
                                <li id="41">


                                    <a id="bibliography_20" title=" Xu Y C, Tan W A, Chen L T. A moving target detection algorithm based on improved mixed gaussian model[J].Control Engineering, 2018, 25 (4) :630-635." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=DKFX&amp;filename=JZDF201804015&amp;v=MjMwNjlqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0ZpRGxWNzdQTHpmUGFMRzRIOW5NcTQ5RVlZUUtESDg0dlI0VDY=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[20]</b>
                                         Xu Y C, Tan W A, Chen L T. A moving target detection algorithm based on improved mixed gaussian model[J].Control Engineering, 2018, 25 (4) :630-635.
                                    </a>
                                </li>
                                <li id="43">


                                    <a id="bibliography_21" title=" Huang X J, Zhou J M, Liu B Y. Moving target detection method based on adaptive hybrid Gaussian background model[J]. Journal of Computer Applications, 2010, 30 (1) : 71-74." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Moving target detection method based on adaptive hybrid Gaussian background model">
                                        <b>[21]</b>
                                         Huang X J, Zhou J M, Liu B Y. Moving target detection method based on adaptive hybrid Gaussian background model[J]. Journal of Computer Applications, 2010, 30 (1) : 71-74.
                                    </a>
                                </li>
                                <li id="45">


                                    <a id="bibliography_22" title=" Lin M, Chen Q, Yan S. Network in network[EB]. arXiv:1312.4400, 2013." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Network in network[EB]">
                                        <b>[22]</b>
                                         Lin M, Chen Q, Yan S. Network in network[EB]. arXiv:1312.4400, 2013.
                                    </a>
                                </li>
                                <li id="47">


                                    <a id="bibliography_23" title=" Ioffe S, Szegedy C. Batch normalization: accelerating deep network training by reducing internal covariate shift[C]//Proceedings of the 32nd International Conference on International Conference on Machine Learning—Volume 37. JMLR.org, 2015:448-456." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Batch Normalization:Accelerating Deep Network Training by Reducing Internal Covariate Shift">
                                        <b>[23]</b>
                                         Ioffe S, Szegedy C. Batch normalization: accelerating deep network training by reducing internal covariate shift[C]//Proceedings of the 32nd International Conference on International Conference on Machine Learning—Volume 37. JMLR.org, 2015:448-456.
                                    </a>
                                </li>
                                <li id="49">


                                    <a id="bibliography_24" title=" Ucsd anomaly dataset[OL]. http://www.svcl.ucsd.edu/projects/anomaly 25." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Ucsd anomaly dataset[OL]">
                                        <b>[24]</b>
                                         Ucsd anomaly dataset[OL]. http://www.svcl.ucsd.edu/projects/anomaly 25.
                                    </a>
                                </li>
                                <li id="51">


                                    <a id="bibliography_25" title=" Umn anomaly dataset[OL]. http://mha.cs.umn.edu/Movies/Crowd-Activity-All.avi." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Umn anomaly dataset[OL]">
                                        <b>[25]</b>
                                         Umn anomaly dataset[OL]. http://mha.cs.umn.edu/Movies/Crowd-Activity-All.avi.
                                    </a>
                                </li>
                                <li id="53">


                                    <a id="bibliography_26" title=" Ravanbakhsh M, Nabi M, Mousavi H, et al. Plug-and-Play CNN for Crowd Motion Analysis: An Application in Abnormal Event Detection[C]//2018 IEEE Winter Conference on Applications of Computer Vision (WACV) . IEEE Computer Society, 2018." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Plug-and-Play CNN for Crowd Motion Analysis: An Application in Abnormal Event Detection">
                                        <b>[26]</b>
                                         Ravanbakhsh M, Nabi M, Mousavi H, et al. Plug-and-Play CNN for Crowd Motion Analysis: An Application in Abnormal Event Detection[C]//2018 IEEE Winter Conference on Applications of Computer Vision (WACV) . IEEE Computer Society, 2018.
                                    </a>
                                </li>
                                <li id="55">


                                    <a id="bibliography_27" title=" Xu D X, Yan W Y, Ricci E, et al. Detecting anomalous events in videos by learning deep representations of appearance and motion[M]. Elsevier Science Inc. 2017." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Detecting anomalous events in videos by learning deep representations of appearance and motion">
                                        <b>[27]</b>
                                         Xu D X, Yan W Y, Ricci E, et al. Detecting anomalous events in videos by learning deep representations of appearance and motion[M]. Elsevier Science Inc. 2017.
                                    </a>
                                </li>
                                <li id="57">


                                    <a id="bibliography_28" title=" Chen T, Hou C, Wang Z, et al. Anomaly detection in crowded scenes using motion energy model[J]. Multimedia Tools &amp;amp; Applications, 2017 (3) :1-16." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Anomaly detection in crowded scenes using motion energy model">
                                        <b>[28]</b>
                                         Chen T, Hou C, Wang Z, et al. Anomaly detection in crowded scenes using motion energy model[J]. Multimedia Tools &amp;amp; Applications, 2017 (3) :1-16.
                                    </a>
                                </li>
                                <li id="59">


                                    <a id="bibliography_29" title=" Zhou S, Shen W, Zeng D, et al. Spatial-temporal convolutional neural networks for anomaly detection and localization in crowded scenes[J]. Signal Processing Image Communication, 2016, 47:358-368." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Spatial-temporal convolutional neural networks for anomaly detection and localization in crowded scenes">
                                        <b>[29]</b>
                                         Zhou S, Shen W, Zeng D, et al. Spatial-temporal convolutional neural networks for anomaly detection and localization in crowded scenes[J]. Signal Processing Image Communication, 2016, 47:358-368.
                                    </a>
                                </li>
                                <li id="61">


                                    <a id="bibliography_30" title=" Mousavi H, Nabi M, Kiani H, et al. Crowd motion monitoring using tracklet-based commotion measure[C]//IEEE International Conference on Image Processing. IEEE, 2015:2354-2358." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Crowd motion monitoring using tracklet-based commotion measure">
                                        <b>[30]</b>
                                         Mousavi H, Nabi M, Kiani H, et al. Crowd motion monitoring using tracklet-based commotion measure[C]//IEEE International Conference on Image Processing. IEEE, 2015:2354-2358.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JYRJ" target="_blank">计算机应用与软件</a>
                2019,36(03),196-201+276 DOI:10.3969/j.issn.1000-386x.2019.03.036            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于卷积神经网络嵌套模型的人群异常行为检测</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%AD%99%E6%9C%88%E9%A9%B0&amp;code=37519428&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">孙月驰</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%8E%E5%86%A0&amp;code=08240615&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">李冠</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%B1%B1%E4%B8%9C%E7%A7%91%E6%8A%80%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2%E5%B1%B1%E4%B8%9C%E7%9C%81%E6%99%BA%E6%85%A7%E7%9F%BF%E5%B1%B1%E4%BF%A1%E6%81%AF%E6%8A%80%E6%9C%AF%E9%87%8D%E7%82%B9%E5%AE%9E%E9%AA%8C%E5%AE%A4&amp;code=0049968&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">山东科技大学计算机科学与工程学院山东省智慧矿山信息技术重点实验室</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>为了提高对运动目标的精确提取, 减少冗余特征信息, 提升算法的泛化性能和非线性拟合能力, 提出基于卷积神经网络嵌套模型的人群异常行为检测方法。通过嵌套mlpconv层改进卷积神经网络结构, 利用混合高斯模型有效、精确地提取出视频中前景目标。嵌套多层的mlpconv层自动学习前景目标的深度层次特征, 生成的特征图经过向量化处理输入到与全连接层相连的Softmax分类器进行人群中异常行为检测。仿真实验结果表明, 该算法减少了对冗余信息的获取, 缩短了算法运算时间和学习时间, 改进的卷积神经网络在泛化性能和非线性拟合能力都有提高, 对人群异常行为检测取得较高准确率。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">卷积神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B7%E5%90%88%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">混合高斯模型;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%B5%8C%E5%A5%97mlpconv%E5%B1%82&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">嵌套mlpconv层;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%BC%82%E5%B8%B8%E8%A1%8C%E4%B8%BA%E6%A3%80%E6%B5%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">异常行为检测;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    孙月驰, 硕士生, 主研领域:智能信息处理。;
                                </span>
                                <span>
                                    李冠, 副教授。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-11-14</p>

                    <p>

                            <b>基金：</b>
                                                        <span>山东省研究生教育创新计划一般项目 (SDYC16022);</span>
                    </p>
            </div>
                    <h1><b>ABNORMAL BEHAVIOR DETECTION OF CROWDS BASED ON NESTED MODEL OF CONVOLUTIONAL NEURAL NETWORK</b></h1>
                    <h2>
                    <span>Sun Yuechi</span>
                    <span>Li Guan</span>
            </h2>
                    <h2>
                    <span>Shandong Key Laboratory of Intelligent Mine Information Technology, College of Computer Science and Engineering, Shandong University of Science and Technology</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>In order to improve the accurate extraction of moving objects, reduce redundant feature information extraction, improve the generalization performance and non-linear fitting ability of the algorithm, we proposed an abnormal behavior detection of crowds based on nested model of convolutional neural network. This method improved the structure of convolution neural network by nesting mlpconv layer, extracted foreground objects in video effectively and accurately by using mixed Gaussian model, and nested multi-layer mlpconv layer to automatically learn the depth features of foreground objects. The generated feature map was vectorized and input into the Softmax classifier connected with the full connection layer to detect abnormal behavior in the crowd. The simulation experimental results show that the algorithm reduces the acquisition of redundant information, shortens the computation time and learning time of the algorithm, improves the generalization performance and non-linear fitting ability of the improved convolutional neural network, and achieves a high accuracy for crowd abnormal behavior detection.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Convolutional%20neural%20network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Convolutional neural network;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Mixed%20Gaussian%20model&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Mixed Gaussian model;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Nested%20mlpconv%20layer&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Nested mlpconv layer;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Abnormal%20behavior%20detection&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Abnormal behavior detection;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-11-14</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="63" name="63" class="anchor-tag"><b>0 引 言</b></h3>
                <div class="p1">
                    <p id="64">基于机器视觉的智能视频监控系统能够实时检测公共区域中行人、车辆等目标的状态变化, 自动检测运动目标的异常行为<citation id="172" type="reference"><link href="3" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>。本文主要研究的人群异常行为分为人群异常行为 (聚集、混乱、拥挤等) 、非人体进入 (车辆驶入、骑车驶入等) 等<citation id="173" type="reference"><link href="5" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>。智能视频监控系统从视频序列中提取大量运动目标的特征信息, 进行异常行为的检测, 已成为预防公共突发事件的有效工具<citation id="174" type="reference"><link href="7" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>。异常事件检测<citation id="175" type="reference"><link href="9" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>旨在自动识别监控视频中的异常事件, 检测方法主要根据提取运动目标的各种特征信息, 将特征信息进行聚类、分类等处理, 进而判断运动目标的运动状态是否异常。随着计算机视觉技术的发展, 针对视频序列中异常事件检测算法得到广泛的应用, 根据提取运动目标特征信息的方式, 可以将人群中异常检测方法归纳成四类:</p>
                </div>
                <div class="p1">
                    <p id="65">第一类为基于行为关系的分析。该类方法运用统计分析关系事件或行为, 其中概率模型应用于描述不同行为之间的关系。例如, Hämäläinen等<citation id="176" type="reference"><link href="11" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>搭建统计显著关联规则模型 (SARM) 搜索统计显著的行为关联规则, 该算法在搜索统计非冗余关联规则上表现突出;Kwon等<citation id="177" type="reference"><link href="13" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>将异常事件检测转化为图模型编辑问题, 视频由图来表示, 图中每个节点代表在空间和时间上分割视频而获得的事件, 节点之间的权重描述了事件之间的关联关系;Yuan等<citation id="178" type="reference"><link href="15" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>将统计假设检验的想法引入到异常事件检测框架, 视频活动被视为一组事件模式的线性组合, 异常事件被认为是包含异常模式的事件, 同时具有异常检测器分数, 利用高斯混合来估算复杂的噪声分布更利于对视频事件建模, 提高了异常检测精度。该类方法检测效果较好, 然而, 需要大量的训练样本才能获得稳定可靠的关系模型。</p>
                </div>
                <div class="p1">
                    <p id="66">第二类为基于运动轨迹的分析。该类方法通过提取研究对象的轨迹特征信息, 检测跟踪对象的运动轨迹是否异常。例如, Fang等<citation id="179" type="reference"><link href="17" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>通过搭建自适应外观模型, 利用可靠的数据关联策略和合理的轨迹约束运动模型来提升对目标检测和追踪的效率;Yuan等<citation id="180" type="reference"><link href="19" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>首次提出信息结构背景描述符SCD (Structural Context Descriptor) 来描述人群中的个体, 通过计算SCD的变化, 设计出多目标追踪单元, 用以追踪不同帧图像中同一目标的轨迹, 通过线上时空分析SCD变化, 来判断人群异常情;Zhou等<citation id="181" type="reference"><link href="21" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>尝试构建一种新的统计框架来检测拥挤场景中异常轨迹的人群行为, 首先, 由特征跟踪器KLT (Kanade Lucas-Tomasi) 获取行人的轨迹特征, 形成代表性轨迹表示人群的潜在运动模式, 最后由马尔可夫模拟模型判断人群中异常的行为。该类算法在杂乱和拥挤的场景中不能准确地检测和定位视频事件, 在处理雾天、雨天等背景模糊的场景时识别精度不理想。</p>
                </div>
                <div class="p1">
                    <p id="67">第三类为基于底层特征的分析。该方法通过提取运动目标的低级特征进行分析运动状态, 根据研究对象的不同搭建各种各样行为模式的模型。例如, Reddy等<citation id="182" type="reference"><link href="23" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>针对检测拥挤场景中异常目标提出了一种具有相对较低复杂度的异常检测算法, 该方法基于每帧的前景掩码分析相关单元是否存在异常, 根据检测目标的运动状态、大小和纹理分别建模;Li等<citation id="183" type="reference"><link href="25" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>通过搭建表示复杂人群场景空间和时间的常态模型, 将空间或时间常态与外观或动态融合在一个常态模型中, 对异常事件进行检测。该类算法拥有一个共同特点需要人工辅助, 设计有效的描述模型需要耗费大量时间。</p>
                </div>
                <div class="p1">
                    <p id="68">第四类为基于深度学习<citation id="184" type="reference"><link href="27" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>的视觉特征分析。该类方法通过基于深度学习用来提取视频中运动目标的特征信息, 根据视频的特征信息进行人群异常事件的识别。例如, Sabokrou等<citation id="185" type="reference"><link href="29" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>提出一种基于立方体补丁的方法, 首先, 使用3D自动编码器进行识别正常立方体, 再使用更复杂、更深的3D卷积神经网络进行评估, 将深度自动编码器和CNN分成多个子级用作级联分类器, 最终实现对异常事件检测;Ravanbakhsh等<citation id="186" type="reference"><link href="31" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>为解决异常检测的问题引入生成对抗网络, 组成生成对抗网络的发生器 (G) 和鉴别器 (D) 分别进行深度网络学习, 发生器 (G) 仅学习生成正常模式, 鉴别器 (D) 学习区分正常和非正常模式;Kiran等<citation id="187" type="reference"><link href="33" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>提出深度卷积架构用于特征学习来进行视频中异常事件的检测。此类模型由多种组件构成, 产生较多参数增加了算法的复杂性。</p>
                </div>
                <div class="p1">
                    <p id="69">针对上述方法的不足, 本文提出一种改进的卷积神经网络人群异常行为识别方法, 该方法通过嵌套mlpconv层改进卷积神经网络结构, 利用混合高斯模型有效、精确地提取视频中前景目标, 嵌套多层的mlpconv层自动学习前景目标的深度层次特征, 生成的特征图经过向量化处理输入到与全连接层相连的Soft max分类器进行人群中异常行为检测。仿真实验结果表明, 该算法减少对冗余信息的获取, 缩短了算法运算时间和学习时间, 改进的卷积神经网络在泛化性能和非线性拟合能力都有提高, 对人群异常行为检测取得较高准确率。</p>
                </div>
                <h3 id="70" name="70" class="anchor-tag"><b>1 人群异常行为检测流程与方法研究</b></h3>
                <div class="p1">
                    <p id="71">本文提出基于卷积神经网络嵌套模型的人群异常行为检测方法的主要流程, 图1为人群异常行为检测算法流程图。该算法分为四个步骤:检测和提取运动目标、获取特征信息、构建嵌套网络模型、进行异常行为检测。</p>
                </div>
                <div class="area_img" id="72">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201903037_072.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 人群异常行为检测算法流程图" src="Detail/GetImg?filename=images/JYRJ201903037_072.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 人群异常行为检测算法流程图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201903037_072.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="73" name="73"><b>1.1 检测和提取运动目标</b></h4>
                <div class="p1">
                    <p id="74">目前, 视频中运动目标检测的方法主要有:光流法、帧间差分法和背景差分法三种<citation id="188" type="reference"><link href="35" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>。本文选取背景差分法中的混合高斯模型GMM<citation id="189" type="reference"><link href="37" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation> (Gaussian Mixture model) , 该模型与其他提取前景目标的方法比较, 不但能够成功检测到运动目标, 而且还能减少背景场景中微小重复运动的物体对前景目标检测的影响。视频中运动目标检测的工作流程如图2所示, 首先高斯分布对每个像素建立背景模型, 再进行背景模型参数自动更新, 最后实现对视频中运动目标的成功检测和提取。</p>
                </div>
                <div class="area_img" id="75">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201903037_075.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 混合高斯模型的工作流程图" src="Detail/GetImg?filename=images/JYRJ201903037_075.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 混合高斯模型的工作流程图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201903037_075.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="76" name="76"><b>1.1.1</b> 混合高斯背景建模</h4>
                <div class="p1">
                    <p id="77">对于任意像素点, 其历史像素序列可以下式描述<citation id="190" type="reference"><link href="39" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>:</p>
                </div>
                <div class="p1">
                    <p id="78">{<i>x</i><sub>1</sub>, <i>x</i><sub>2</sub>, …, <i>x</i><sub><i>t</i></sub>}={<i>I</i><sub><i>i</i></sub> (<i>x</i>, <i>y</i>) , 1≤<i>i</i>≤<i>t</i>}      (1) </p>
                </div>
                <div class="p1">
                    <p id="79">式中:<i>I</i><sub><i>i</i></sub> (<i>x</i>, <i>y</i>) 为第<i>i</i>个时刻的灰度值。</p>
                </div>
                <div class="p1">
                    <p id="80">在<i>t</i>时刻, 像素点 (<i>x</i>, <i>y</i>) 的概率函数计算公式为<citation id="191" type="reference"><link href="41" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>:</p>
                </div>
                <div class="p1">
                    <p id="81"><mathml id="82"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>p</mi><mo stretchy="false"> (</mo><mi>Ι</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><mi>W</mi></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mi>t</mi></mrow></msub><mo>×</mo><mi>η</mi><mrow><mo> (</mo><mrow><mi>Ι</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo><mo>, </mo><mi>μ</mi><msub><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mi>t</mi></mrow></msub><mo>, </mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>i</mi><mo>, </mo><mi>t</mi></mrow></munder><mrow></mrow></mstyle></mrow><mo>) </mo></mrow></mrow></math></mathml>      (2) </p>
                </div>
                <div class="p1">
                    <p id="83">式中:<i>W</i><sub><i>i</i>, <i>t</i></sub>为<i>t</i>时刻第<i>i</i>个模型权重值;<i>μ</i><sub><i>i</i>, <i>t</i></sub>为<i>t</i>时刻第<i>i</i>个高斯分布的均值;<mathml id="84"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>i</mi><mo>, </mo><mi>t</mi></mrow></munder><mrow></mrow></mstyle></mrow></math></mathml>为<i>t</i>时刻的协方差;<mathml id="85"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>η</mi><mrow><mo> (</mo><mrow><mi>Ι</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo><mo>, </mo><mi>μ</mi><msub><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mi>t</mi></mrow></msub><mo>, </mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>i</mi><mo>, </mo><mi>t</mi></mrow></munder><mrow></mrow></mstyle></mrow><mo>) </mo></mrow></mrow></math></mathml>为<i>t</i>时刻的概率密度函数, 计算公式为:</p>
                </div>
                <div class="p1">
                    <p id="86" class="code-formula">
                        <mathml id="86"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>η</mi><mrow><mo> (</mo><mrow><mi>Ι</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo><mo>, </mo><mi>μ</mi><msub><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mi>t</mi></mrow></msub><mo>, </mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>i</mi><mo>, </mo><mi>t</mi></mrow></munder><mrow></mrow></mstyle></mrow><mo>) </mo></mrow><mo>=</mo><mfrac><mn>1</mn><mrow><mo stretchy="false"> (</mo><mn>2</mn><mtext>π</mtext><mo stretchy="false">) </mo><msup><mrow></mrow><mfrac><mtext>π</mtext><mn>2</mn></mfrac></msup><mrow><mo>|</mo><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>i</mi><mo>, </mo><mi>t</mi></mrow></munder><mrow></mrow></mstyle></mrow><mo>|</mo></mrow><msup><mrow></mrow><mfrac><mn>1</mn><mn>2</mn></mfrac></msup></mrow></mfrac><mo>×</mo></mtd></mtr><mtr><mtd><mrow><mi>exp</mi></mrow><mrow><mo> (</mo><mrow><mo>-</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mo stretchy="false"> (</mo><mi>Ι</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo><mo>-</mo><mi>μ</mi><msub><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mi>t</mi></mrow></msub><mo stretchy="false">) </mo><msup><mrow></mrow><mtext>Τ</mtext></msup><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>, </mo><mi>t</mi></mrow><mrow><mo>-</mo><mn>1</mn></mrow></munderover><mo stretchy="false"> (</mo></mstyle><mi>Ι</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo><mo>-</mo><mi>μ</mi><msub><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mi>t</mi></mrow></msub><mo stretchy="false">) </mo></mrow><mo>) </mo></mrow><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <h4 class="anchor-tag" id="87" name="87"><b>1.1.2</b> 混合高斯模型参数更新</h4>
                <div class="p1">
                    <p id="88">按优先级大小将帧像素值<i>I</i><sub><i>i</i></sub> (<i>x</i>, <i>y</i>) 的高斯分布进行排序, 满足式 (4) , 说明帧像素值<i>I</i><sub><i>i</i></sub> (<i>x</i>, <i>y</i>) 与高斯分布匹配成功, 帧像素值<i>I</i><sub><i>i</i></sub> (<i>x</i>, <i>y</i>) 的高斯分布的参数依次按照式 (4) 至式 (8) 进行更新;不满足式 (4) , 帧像素值<i>I</i><sub><i>i</i></sub> (<i>x</i>, <i>y</i>) 与高斯分布匹配不成功, 高斯分布的参数不变, 权重值按式 (9) 进行更新。</p>
                </div>
                <div class="p1">
                    <p id="89"><mathml id="90"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>|</mo><mrow><mi>Ι</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo><mo>-</mo><mi>μ</mi><msub><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow><mo>|</mo></mrow><mo>&lt;</mo><mi>D</mi><mo>×</mo><mi>σ</mi><msub><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow></math></mathml>      (4) </p>
                </div>
                <div class="p1">
                    <p id="91"><i>W</i><sub><i>i</i>, <i>t</i></sub>= (1-<i>α</i>) <i>W</i><sub><i>i</i>, <i>t</i>-1</sub>+<i>α</i>      (5) </p>
                </div>
                <div class="p1">
                    <p id="92"><i>μ</i>= (1-<i>β</i>) <i>μ</i><sub><i>i</i>, <i>t</i>-1</sub>+<i>βI</i><sub><i>i</i></sub> (<i>x</i>, <i>y</i>)      (6) </p>
                </div>
                <div class="p1">
                    <p id="93"><i>δ</i><mathml id="94"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mi>t</mi></mrow><mn>2</mn></msubsup></mrow></math></mathml>= (1-<i>β</i>) <i>δ</i><mathml id="95"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mi>t</mi><mo>-</mo><mn>1</mn></mrow><mn>2</mn></msubsup></mrow></math></mathml>+<i>β</i> (<i>I</i><sub><i>t</i></sub> (<i>x</i>, <i>y</i>) -</p>
                </div>
                <div class="p1">
                    <p id="96"><i>μ</i><sub><i>i</i>, <i>t</i>-1</sub>) <sup>T</sup> (<i>I</i><sub><i>t</i></sub> (<i>x</i>, <i>y</i>) -<i>μ</i><sub><i>i</i>, <i>t</i>-1</sub>)      (7) </p>
                </div>
                <div class="p1">
                    <p id="97"><i>β</i>=<i>αη</i> (<i>I</i><sub><i>t</i></sub> (<i>x</i>, <i>y</i>) <i>μ</i>, <i>δ</i><sub><i>i</i>, <i>t</i></sub>)      (8) </p>
                </div>
                <div class="p1">
                    <p id="98"><i>W</i><sub><i>i</i>, <i>t</i></sub>= (1-<i>α</i>) <i>W</i><sub><i>i</i>, <i>t</i>-1</sub>+<i>α</i>      (9) </p>
                </div>
                <div class="p1">
                    <p id="99">式中:<i>α</i>、<i>β</i>分别表示混合高斯模型的学习率、更新率。</p>
                </div>
                <h4 class="anchor-tag" id="100" name="100"><b>1.1.3</b> 混合高斯模型对运动目标的提取</h4>
                <div class="p1">
                    <p id="101">在每个像素的混合高斯模型生成后, 按<mathml id="102"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mi>ω</mi><mi>μ</mi></mfrac></mrow></math></mathml>的值对高斯分布进行降序排列, 获取前<i>B</i>个高斯分布作为背景模型<citation id="192" type="reference"><link href="43" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>, 公式如下:</p>
                </div>
                <div class="p1">
                    <p id="103"><i>B</i>=argmin<mathml id="104"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow><mi>b</mi></munderover><mi>ω</mi></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mi>t</mi></mrow></msub><mo>&gt;</mo><mi>Τ</mi></mrow></math></mathml>      (10) </p>
                </div>
                <div class="p1">
                    <p id="105">式中, <i>T</i>为设置的阈值。</p>
                </div>
                <div class="p1">
                    <p id="106">获取了前<i>B</i>个高斯分布作为背景模型, 将当前像素值<i>I</i><sub><i>i</i></sub> (<i>x</i>, <i>y</i>) 与生成背景进行匹配, 如果当前像素值<i>I</i><sub><i>i</i></sub> (<i>x</i>, <i>y</i>) 与生成背景匹配不成功, 当前像素<i>I</i><sub><i>i</i></sub> (<i>x</i>, <i>y</i>) 点为运动目标;否则, 当前像素<i>I</i><sub><i>i</i></sub> (<i>x</i>, <i>y</i>) 点为背景点。经过上述过程之后, 混合高斯模型实现对视频中运动目标的检测及提取。与其他方法提取前景目标的结果对比图如图3所示。图3 (a) 是原始视频序列, 图3 (b) 是其他方法提取的前景目标, 图3 (c) 是混合高斯模型提取的前景目标。</p>
                </div>
                <div class="area_img" id="107">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201903037_10700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 与其他方法提取前景目标的结果对比" src="Detail/GetImg?filename=images/JYRJ201903037_10700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 与其他方法提取前景目标的结果对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201903037_10700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="108" name="108"><b>1.2 获取运动目标的特征信息</b></h4>
                <h4 class="anchor-tag" id="109" name="109"><b>1.2.1 mlpconv</b>层</h4>
                <div class="p1">
                    <p id="110">mlpconv层由一个线性卷积层和一个多层感知机MLP (Multy-Layer Perception) 构成, 局部感受视野中的输入映射对应的特征向量。mlpconv层使用由多个带非线性激活函数的全连接层构成的MLP来提取运动目标的特征信息, 将提取的特征信息转化为特征图, 再将特征图作为下一层的输入<citation id="193" type="reference"><link href="45" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="111">mlpconv层计算过程如下所示:</p>
                </div>
                <div class="p1">
                    <p id="112"><i>f</i><sup>1</sup><sub><i>i</i>, <i>j</i>, <i>k</i><sub>1</sub></sub>=max (<i>W</i><mathml id="113"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>k</mi><msub><mrow></mrow><mn>1</mn></msub></mrow><mrow><mn>1</mn><msup><mrow></mrow><mtext>Τ</mtext></msup></mrow></msubsup></mrow></math></mathml><i>x</i><sub><i>i</i>, <i>j</i></sub>+<i>b</i><sub><i>k</i><sub>1</sub></sub>, 0)      (11) </p>
                </div>
                <div class="p1">
                    <p id="114"><i>f</i><sup>2</sup><sub><i>i</i>, <i>j</i>, <i>k</i><sub>2</sub></sub>=max (<i>W</i><mathml id="115"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>k</mi><msub><mrow></mrow><mn>2</mn></msub></mrow><mrow><mn>2</mn><msup><mrow></mrow><mtext>Τ</mtext></msup></mrow></msubsup></mrow></math></mathml><i>f</i><sup>1</sup><sub><i>i</i>, <i>j</i></sub>+<i>b</i><sub><i>k</i><sub>2</sub></sub>, 0)      (12) </p>
                </div>
                <div class="p1">
                    <p id="116"><i>f</i><sup><i>n</i></sup><sub><i>i</i>, <i>j</i>, <i>k</i><sub><i>n</i></sub></sub>=max (<i>W</i><mathml id="117"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>k</mi><msub><mrow></mrow><mi>n</mi></msub></mrow><mrow><mi>n</mi><msup><mrow></mrow><mtext>Τ</mtext></msup></mrow></msubsup></mrow></math></mathml><i>f</i><sup><i>n</i>-1</sup><sub><i>i</i>, <i>j</i></sub>+<i>b</i><sub><i>k</i><sub><i>n</i></sub></sub>, 0)      (13) </p>
                </div>
                <div class="p1">
                    <p id="118">式中: (<i>i</i>, <i>j</i>) 是特征图中像素的位置, <i>x</i><sub><i>i</i>, <i>j</i></sub>是以像素点 (<i>i</i>, <i>j</i>) 为中心的输入块, <i>k</i><sub>1</sub>、<i>k</i><sub>2</sub>和<i>k</i><sub><i>n</i></sub>等是特征图中的各通道序号, <i>n</i>则是MLP的层数。</p>
                </div>
                <h4 class="anchor-tag" id="119" name="119"><b>2.2.2 Batch Normaliztion</b>技术</h4>
                <div class="p1">
                    <p id="120">在神经网络学习过程中, 随着各层参数的变化, 特别是算法的学习率和权值初始化会需要很长时间寻找合适值, 降低了神经网络的训练速度。在使用饱和非线性激活函数来训练神经网络模型时, 输入数据会误入激活函数的饱和区域, 使神经网络的收敛性速度降低。</p>
                </div>
                <div class="p1">
                    <p id="121">Ioffe等<citation id="194" type="reference"><link href="47" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>通过BN (Batch Normaliztion) 技术规范化每一层的输入来解决上述问题。BN技术使得输入数据具有零均值和单位方差:</p>
                </div>
                <div class="p1">
                    <p id="122"><mathml id="123"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>x</mi><mo>^</mo></mover><msub><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mi>j</mi><mo>, </mo><mi>n</mi></mrow></msub><mo>=</mo><mfrac><mrow><mover accent="true"><mi>x</mi><mo>^</mo></mover><msub><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mi>j</mi><mo>, </mo><mi>n</mi></mrow></msub><mo>-</mo><mi>E</mi><mo stretchy="false">[</mo><mi>x</mi><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false">]</mo></mrow><mrow><msqrt><mrow><mi>V</mi><mi>a</mi><mi>r</mi><mo stretchy="false">[</mo><mi>x</mi><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false">]</mo></mrow></msqrt></mrow></mfrac></mrow></math></mathml>      (14) </p>
                </div>
                <div class="p1">
                    <p id="124">规范化处理后, 参数需要相应的缩放和平移:</p>
                </div>
                <div class="p1">
                    <p id="125"><mathml id="126"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>f</mi><msub><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mi>j</mi><mo>, </mo><mi>n</mi></mrow></msub><mo>=</mo><mi>γ</mi><msub><mrow></mrow><mi>n</mi></msub><mover accent="true"><mi>x</mi><mo>^</mo></mover><msub><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mi>j</mi><mo>, </mo><mi>n</mi></mrow></msub><mo>+</mo><mi>β</mi><msub><mrow></mrow><mi>n</mi></msub></mrow></math></mathml>      (15) </p>
                </div>
                <div class="p1">
                    <p id="127">式中:<mathml id="128"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>x</mi><mo>^</mo></mover><msub><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mi>j</mi><mo>, </mo><mi>n</mi></mrow></msub></mrow></math></mathml>是输入数据在位置 (<i>i</i>, <i>j</i>) 处的值, <i>n</i>是特征图中通道序号, <i>γ</i><sub><i>n</i></sub>、<i>β</i><sub><i>n</i></sub>是网络训练中新引入的缩放和平移参数。</p>
                </div>
                <h4 class="anchor-tag" id="129" name="129"><b>1.3 构建卷积神经网络嵌套模型</b></h4>
                <div class="p1">
                    <p id="130">卷积神经网络嵌套模型的核心思想为:嵌套网络模型可以出色地自动学习到深度层次特征, 该模型获取的深层次特征主要是局部特征, 在获取运动目标特征信息时局部特征在背景中分离目标发挥重要作用, 此外, 嵌套网络模型在应对背景目标剧烈变化时也具有一定的鲁棒性。</p>
                </div>
                <div class="p1">
                    <p id="131">网络嵌套模型训练时, 首先, 对含有单个mlpconv层的卷积神经网络模型的权值初始化, 再进行卷积神经网络的训练, 整个训练过程结束, 得到单个mlpconv层权值的更新;然后, 接入第二个mlpconv层, 第二个mlpconv层的输入是第一个mlpconv层的输出, 进行第二个mlpconv层的权值初始化, 再进行卷积神经网络的训练, 整个训练过程结束, 得到第二个mlpconv层权值的更新。当有新的mlpconv层加入, 按照上述过程进行权值初始化、卷积神经网络的训练以及权值更新。</p>
                </div>
                <div class="p1">
                    <p id="132">此外, 在进行卷积计算之后使用BN技术, 也使得非线性单元能够产生比较稳定的分布, 达到去饱和的效果。在嵌套mlpconv层加入BN操作, 模型中特征图的计算方式如下:</p>
                </div>
                <div class="p1">
                    <p id="133"><i>f</i><sup>1</sup><sub><i>i</i>, <i>j</i>, <i>k</i><sub>1</sub></sub>=max (<i>BN</i> (<i>W</i><mathml id="134"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>k</mi><msub><mrow></mrow><mn>1</mn></msub></mrow><mrow><mn>1</mn><msup><mrow></mrow><mtext>Τ</mtext></msup></mrow></msubsup></mrow></math></mathml><i>x</i><sub><i>i</i>, <i>j</i></sub>+<i>b</i><sub><i>k</i><sub>1</sub></sub>) , 0)      (16) </p>
                </div>
                <div class="p1">
                    <p id="135"><i>f</i><sup>2</sup><sub><i>i</i>, <i>j</i>, <i>k</i><sub>1</sub></sub>=max (<i>BN</i> (<i>W</i><mathml id="136"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>k</mi><msub><mrow></mrow><mn>2</mn></msub></mrow><mrow><mn>2</mn><msup><mrow></mrow><mtext>Τ</mtext></msup></mrow></msubsup></mrow></math></mathml><i>f</i><sup>1</sup><sub><i>i</i>, <i>j</i></sub>+<i>b</i><sub><i>k</i><sub>2</sub></sub>) , 0)      (17) </p>
                </div>
                <div class="p1">
                    <p id="137"><i>f</i><sup><i>n</i></sup><sub><i>i</i>, <i>j</i>, <i>k</i><sub><i>n</i></sub></sub>=max (<i>BN</i> (<i>W</i><mathml id="138"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>k</mi><msub><mrow></mrow><mi>n</mi></msub></mrow><mrow><mi>n</mi><msup><mrow></mrow><mtext>Τ</mtext></msup></mrow></msubsup></mrow></math></mathml><i>f</i><sup><i>n</i>-1</sup><sub><i>i</i>, <i>j</i></sub>+<i>b</i><sub><i>k</i><sub><i>n</i></sub></sub>) , 0)      (18) </p>
                </div>
                <div class="p1">
                    <p id="139">式中:<i>BN</i> (<i>g</i>) 表示BN层, (<i>i</i>, <i>j</i>) 是特征图中像素的位置, <i>x</i><sub><i>i</i>, <i>j</i></sub>是以像素点 (<i>i</i>, <i>j</i>) 为中心的输入块, <i>k</i><sub>1</sub>、<i>k</i><sub>2</sub>和<i>k</i><sub><i>n</i></sub>等是特征图中的各通道序号, <i>n</i>则是MLP的层数。图4为基于卷积神经网络嵌套模型。</p>
                </div>
                <div class="area_img" id="140">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201903037_140.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 基于卷积神经网络的嵌套模型" src="Detail/GetImg?filename=images/JYRJ201903037_140.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 基于卷积神经网络的嵌套模型  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201903037_140.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h3 id="141" name="141" class="anchor-tag"><b>2 实验与结果分析</b></h3>
                <div class="p1">
                    <p id="142">在3.0 GHzCPU、64位Windows7操作系统, 采用MATLAB 2016a、Open CV作为开发工具进行仿真实验。为了验证提出算法的有效性, 本文选取常用于人群行为识别研究的基准数据集, 即UCSD<citation id="195" type="reference"><link href="49" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation> ( (University of California, San Diego) 数据集和UMN<citation id="196" type="reference"><link href="51" rel="bibliography" /><sup>[<a class="sup">25</a>]</sup></citation> (University of Minnesota) 数据集。这两个数据集涵盖了室内和室外、局部异常和全部异常等情况。仿真实验中, 定量评价采用AUC评价指标、等错误率 (EER) 以及运行时间 (Time) 等指标。</p>
                </div>
                <div class="p1">
                    <p id="143">输入数据是将每一帧都裁剪为80×60的灰度图像, 卷积神经网络模型的三个卷积层采用的卷积核分别是9×7、7×7、6×4的尺度, 两个下采样层采用的卷积核均为3×3尺度。输入的80×60×9的视频块最终被转化为128维的特征向量。</p>
                </div>
                <h4 class="anchor-tag" id="144" name="144"><b>2.1 在UCSD数据集上实验结果</b></h4>
                <div class="p1">
                    <p id="145">USCD<citation id="197" type="reference"><link href="49" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>数据集由加州大学圣地亚哥分校创建, 数据集是通过安装在一定高度、俯视人行道的摄像机, 采集自然状态下发生的异常行为, 主要针对人群中个体行为的识别研究。数据集由98个视频组成, 被分成ped1和ped2两个数据子集, 每个场景录制的视频录像被分成约200帧的片段, 像素分辨率分别为158 pixel×238 pixel和240 pixel×360 pixel。图5为UCSD数据集中的非人类实体入侵和异常人类行为场景的示例。</p>
                </div>
                <div class="area_img" id="146">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201903037_146.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 UCSD数据集中非人类实体入侵和异常人类行为示例" src="Detail/GetImg?filename=images/JYRJ201903037_146.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 UCSD数据集中非人类实体入侵和异常人类行为示例  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201903037_146.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="147">在UCSD ped1和UCSD ped2数据集验证该算法的有效性, 本文选取在上述数据库取得较好识别率的算法进行对比, 例如:TCP模型<citation id="198" type="reference"><link href="53" rel="bibliography" /><sup>[<a class="sup">26</a>]</sup></citation>、AMDN (double fusion) 模型<citation id="199" type="reference"><link href="55" rel="bibliography" /><sup>[<a class="sup">27</a>]</sup></citation>、运动能量 (Motion Energy) 模型<citation id="200" type="reference"><link href="57" rel="bibliography" /><sup>[<a class="sup">28</a>]</sup></citation>、时空卷积神经网络 (ST-CNN) 模型<citation id="201" type="reference"><link href="59" rel="bibliography" /><sup>[<a class="sup">29</a>]</sup></citation>及Commotion模型<citation id="202" type="reference"><link href="61" rel="bibliography" /><sup>[<a class="sup">30</a>]</sup></citation>等。</p>
                </div>
                <div class="p1">
                    <p id="148">由表1可知, 在UCSD ped1数据集上, 采用帧级度量时, 本文算法的EER较低, 显著提高了AUC评价指标;采用像素级度量时, 在EER指标、AUC评价指标上提高不是很明显, 但是两个指标均高于其他算法的指标。在UCSD ped2数据集上, 采用帧级度量测试, 本文算法在EER指标、AUC评价指标方面占优势, AUC评价指标提高0.11。</p>
                </div>
                <div class="area_img" id="149">
                    <p class="img_tit"><b>表1 AUC和EER在于Ped1 (UCSD数据集) 上的帧和像素级比较</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="149" border="1"><tr><td rowspan="2"><br />算法</td><td colspan="2"><br />Ped1 (frame) </td><td colspan="2">Ped1 (pixel) </td><td colspan="2">Ped2 (frame) </td></tr><tr><td>EER<br />/%</td><td>AUC</td><td>EER<br />/%</td><td>AUC</td><td>EER<br />/%</td><td>AUC</td></tr><tr><td>TCP</td><td>8</td><td>0.957</td><td>37</td><td>0.622</td><td>19</td><td>0.807</td></tr><tr><td><br />AMDN</td><td>16</td><td>0.921</td><td>40.1</td><td>0.672</td><td>17</td><td>0.908</td></tr><tr><td><br />Motion Energy</td><td>22</td><td>0.810</td><td>55</td><td>0.580</td><td>21</td><td>0.912</td></tr><tr><td><br />ST-CNN</td><td>24.4</td><td>0.860</td><td>38.5</td><td>0.880</td><td>25.5</td><td>0.835</td></tr><tr><td><br />Ours</td><td>7.5</td><td>0.960</td><td>35.2</td><td>0.676</td><td>16.2</td><td>0.923</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="150" name="150"><b>2.2 在UMN数据集上实验结果</b></h4>
                <div class="p1">
                    <p id="151">另外, 在UMN数据集也进行了实验验证, 该数据集前一半视频作为训练集, 后一半作为测试集, 在该数据集上采用帧级度量下的EER和AUC评价指标对算法的性能进行评估。验证结果如表2所示。在UMN数据集上采用帧级度量下的EER和AUC评价指标对算法的性能测试结果, 可以得出, 本文算法在AUC评价指标与已有算法的性能上相当, 在EER指标上优于其他算法, 该算法在耗时方面得到了提高。</p>
                </div>
                <div class="area_img" id="152">
                    <p class="img_tit"><b>表2 AUC和EER用于UMN数据集上的帧和像素级比较</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="152" border="1"><tr><td rowspan="2"><br />算法</td><td colspan="3"><br />Frame-Level</td></tr><tr><td><br />EER/%</td><td>AUC</td><td>Time/s</td></tr><tr><td><br />TCP</td><td>3.2</td><td>0.988</td><td>0.32</td></tr><tr><td><br />Motion Energy</td><td>4.1</td><td>0.989</td><td>0.09</td></tr><tr><td><br />ST-CNN</td><td>3.7</td><td>0.996</td><td>0.49</td></tr><tr><td><br />Commotion</td><td>3.1</td><td>0.988</td><td>0.25</td></tr><tr><td><br />Ours</td><td>3.1</td><td>0.992</td><td>0.12</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="153" name="153"><b>2.3 时间复杂度分析对比</b></h4>
                <div class="p1">
                    <p id="154">算法的时间复杂度是一个函数, 它定量描述了该算法的运行时间。</p>
                </div>
                <h4 class="anchor-tag" id="155" name="155"><b>2.3.1</b> 单个卷积层的时间复杂度</h4>
                <div class="p1">
                    <p id="156">时间复杂度是指执行算法所需要的计算工作量, 单层卷积层时间复杂度计算如下:</p>
                </div>
                <div class="p1">
                    <p id="157"><i>Time</i>～<i>O</i> (<i>M</i><sup>2</sup>·<i>K</i><sup>2</sup>·<i>C</i><sub>in</sub>·<i>C</i><sub>out</sub>)      (19) </p>
                </div>
                <div class="p1">
                    <p id="158">式中:<i>M</i>表示每个卷积核输出特征图 (Feature Map) 的边长;<i>K</i>表示每个卷积核 (Kernel) 的边长;<i>C</i><sub>in</sub>表示每个卷积核的通道数, 即输入通道数 (上一层的输出道数) ;<i>C</i><sub>out</sub>表示卷积层具有的卷积核个数, 即输出通道数。</p>
                </div>
                <div class="p1">
                    <p id="159">由式 (19) 可以得知, 卷积层的时间复杂度由输出特征图面积<i>M</i><sup>2</sup>、卷积核面积<i>K</i><sup>2</sup>、输入<i>C</i><sub>in</sub>和输出通道数<i>C</i><sub>out</sub>决定;输出特征图的尺寸又由输入矩阵尺寸<i>X</i>、卷积核尺寸<i>K</i>、<i>Padding</i>、<i>Stride</i>决定, 输出特征图的边长<i>M</i>表达式为:</p>
                </div>
                <div class="p1">
                    <p id="160"><mathml id="161"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Μ</mi><mo>=</mo><mfrac><mrow><mo stretchy="false"> (</mo><mi>X</mi><mo>-</mo><mi>Κ</mi><mo>+</mo><mn>2</mn><mo>×</mo><mi>Ρ</mi><mi>a</mi><mi>d</mi><mi>d</mi><mi>i</mi><mi>n</mi><mi>g</mi><mo stretchy="false">) </mo></mrow><mrow><mi>S</mi><mi>t</mi><mi>r</mi><mi>i</mi><mi>d</mi><mi>e</mi><mo>+</mo><mn>1</mn></mrow></mfrac></mrow></math></mathml>      (20) </p>
                </div>
                <h4 class="anchor-tag" id="162" name="162"><b>2.3.2</b> 卷积神经网络的整体的时间复杂度</h4>
                <div class="p1">
                    <p id="163">单层卷积神经网络的复杂度由式 (19) 计算, 卷积神经网络 (包含多层结构) 整体时间复杂度为每层时间复杂度的和, 计算公式如下:</p>
                </div>
                <div class="p1">
                    <p id="164"><mathml id="165"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Τ</mi><mi>i</mi><mi>m</mi><mi>e</mi><mo>~</mo><mi>Ο</mi><mrow><mo> (</mo><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>l</mi><mo>=</mo><mn>1</mn></mrow><mi>D</mi></munderover><mi>Μ</mi></mstyle><msubsup><mrow></mrow><mi>l</mi><mn>2</mn></msubsup><mo>⋅</mo><mi>Κ</mi><msubsup><mrow></mrow><mi>l</mi><mn>2</mn></msubsup><mo>⋅</mo><mi>C</mi><msub><mrow></mrow><mrow><mi>l</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>⋅</mo><mi>C</mi><msub><mrow></mrow><mi>l</mi></msub></mrow><mo>) </mo></mrow></mrow></math></mathml>      (21) </p>
                </div>
                <div class="p1">
                    <p id="166">式中:<i>D</i>表示神经网络所具有的卷积层数, 即网络深度;<i>l</i>表示神经网络第<i>l</i>个卷积层;<i>C</i><sub><i>l</i></sub>表示神经网络第<i>l</i>个卷积层的输出通道数<i>C</i><sub>out</sub>, 即该层的卷积核个数;第<i>l</i>个卷积层的输入通道数<i>C</i><sub>in</sub>是第 (<i>l</i>-1) 个卷积层输出通道数。</p>
                </div>
                <div class="p1">
                    <p id="167">在时间复杂度方面, 本文选取TCP模型、AMDN (double fusion) 模型、运动能量 (Motion Energy) 模型、时空卷积神经网络 (ST-CNN) 模型及Commotion模型与本文算法进行对比。由式 (13) 可以计算出各算法的时间复杂度, 因为各算法的具体参数数据不明确, 本文只计算算法时间复杂度属于哪个阶, 常见的时间复杂度关系为:<i>O</i> (1) &lt;<i>O</i> (log<i>n</i>) &lt;<i>O</i> (<i>n</i>) &lt;<i>O</i> (<i>n</i>log<i>n</i>) &lt;<i>O</i> (<i>n</i><sup>2</sup>) &lt;<i>O</i> (<i>n</i><sup>3</sup>) 。</p>
                </div>
                <div class="p1">
                    <p id="168">如表3所示, 本文算法与其他算法时间复杂度大部分在<i>O</i> (<i>n</i><sup>2</sup>) 阶上, 分析其他算法神经网络结构模型在输出特征图面积<i>M</i><sup>2</sup>、卷积核面积<i>K</i><sup>2</sup>、输入<i>C</i><sub>in</sub>和输出通道数<i>C</i><sub>out</sub>均比本文算法的复杂, 可以得出结论本文算法在时间复杂度上优于其他算法。</p>
                </div>
                <div class="area_img" id="169">
                    <p class="img_tit"><b>表3 本文算法与其他算法时间复杂度</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="169" border="1"><tr><td><br />算法</td><td>时间复杂度</td></tr><tr><td><br />TCP</td><td><i>O</i> (<i>n</i><sup>2</sup>) </td></tr><tr><td><br />AMDN</td><td><i>O</i> (<i>n</i><sup>3</sup>) </td></tr><tr><td><br />Motion Energy</td><td><i>O</i> (<i>n</i><sup>2</sup>) </td></tr><tr><td><br />ST-CNN</td><td><i>O</i> (<i>n</i><sup>2</sup>) </td></tr><tr><td><br />Commotion</td><td><i>O</i> (<i>n</i><sup>2</sup>) </td></tr><tr><td><br />Ours</td><td><i>O</i> (<i>n</i><sup>2</sup>) </td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h3 id="170" name="170" class="anchor-tag"><b>3 结 语</b></h3>
                <div class="p1">
                    <p id="171">本文提出基于改进的卷积神经网络对人群异常行为检测方法, 通过嵌套mlpconv层改进卷积神经网络结构, 利用混合高斯模型有效、精确地提取出视频序列中前景目标, 混合高斯模型在复杂场景背景中表现出鲁棒性, 不但能够成功检测到运动目标, 而且减少了背景场景中微小重复运动的物体对前景目标检查的影响。嵌套多层的mlpconv层自动学习已经提取到的前景运动目标的深度层次特征, 改进的卷积神经网络减少对冗余信息的获取。在UCSD和UMN数据集上实验结果表明, 在算法运算时间和学习时间上得到优化, 对人群中异常行为检测更迅速、准确, 改进的卷积神经网络在泛化性能和非线性拟合能力上的表现也很优秀。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Review, Current Situation and Prospect of Human Visual Behavior Recognition">

                                <b>[1]</b> Shan Y H, Zhang Z, Huang K Q. Review, Current Situation and Prospect of Human Visual Behavior Recognition[J]. Journal of Computer Research and Development, 2016, 53 (1) : 93-112.
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Novel dataset for fine-grained abnormal behavior understanding in crowd">

                                <b>[2]</b> Rabiee H, Haddadnia J, Mousavi H, et al. Novel dataset for fine-grained abnormal behavior understanding in crowd[C]//IEEE International Conference on Advanced Video and Signal Based Surveillance. IEEE, 2016:95-101.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD13101000017144&amp;v=MjQ0NTdNbndaZVp0RmlubFVyM0pLRjRRYXhVPU5qN0Jhcks3SDlITnI0OUZaT29JRFhnOW9CTVQ2VDRQUUgvaXJSZEdlcnFRVA==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> Vishwakarma S. A survey on activity recognition and behavior understanding in video surveillance[J]. Visual Computer, 2013, 29 (10) :983-1009.
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning deep event models for crowd anomaly detection">

                                <b>[4]</b> Feng Y, Yuan Y, Lu X. Learning deep event models for crowd anomaly detection[J]. Neurocomputing, 2016, 219: 548-556.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Efficient discovery of statistically significant association rules">

                                <b>[5]</b> Hämäläinen W, Nykänen M. Efficient discovery of statistically significant association rules[C]//Eighth IEEE International Conference on Data Mining. IEEE Computer Society, 2008:203-212.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A unified framework for event summarization and rare event detection">

                                <b>[6]</b> Kwon J, Lee K M. A unified framework for event summarization and rare event detection[C]//IEEE Conference on Computer Vision and Pattern Recognition. IEEE Computer Society, 2015:1266-1273.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Statisticalhy pothesis detector for abnormal event detection in crowded scenes">

                                <b>[7]</b> Yuan Y, Feng Y, Lu X. Statistical hypothesis detector for abnormal event detection in crowded scenes[J]. IEEE Transactions on Cybernetics, 2017, 47 (11) :3597-3608.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Part-based online tracking with geometry constraint and attention selection">

                                <b>[8]</b> Fang J, Wang Q, Yuan Y. Part-based online tracking with geometry constraint and attention selection[J]. IEEE Transactions on Circuits &amp; Systems for Video Technology, 2014, 24 (5) :854-864.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Online anomaly detection in crowd scenes via structure analysis">

                                <b>[9]</b> Yuan Y, Fang J, Wang Q. Online anomaly detection in crowd scenes via structure analysis[J]. IEEE Transactions on Cybernetics, 2015, 45 (3) :548-561.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Unusual event detection in crowded scenes by trajectory analysis">

                                <b>[10]</b> Zhou S, Shen W, Zeng D, et al. Unusual event detection in crowded scenes by trajectory analysis[C]//IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE, 2015:1300-1304.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Improved anomaly detection in crowded scenes via cell-based analysis of foreground speed size and texture">

                                <b>[11]</b> Reddy V, Sanderson C, Lovell B C. Improved anomaly detection in crowded scenes via cell-based analysis of foreground speed, size and texture[C]//Computer Vision and Pattern Recognition Workshops. IEEE, 2011:55-61.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Online detection of unusual events in videos via dynamic sparse coding">

                                <b>[12]</b> Zhao B, Li F F, Xing E P. Online detection of unusual events in videos via dynamic sparse coding[C]//IEEE Conference on Computer Vision and Pattern Recognition. IEEE Computer Society, 2011:3313-3320.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_13" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES441041FD5D0BEA83528D7E392AC0EFDB&amp;v=MTEyNDdJeHhVVzZEY0pUd3JocFJCRUNyTGhNODd0Q09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQnJMVTA1dHBnekxpN3dhOD1OaWZPZmJlOEg5SElydmt4WVo4UGZnbA==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[13]</b> Guo Y, Liu Y, Oerlemans A, et al. Deep learning for visual understanding: A review[J].Neurocomputing, 2016, 187 (C) :27-48.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep-cascade cascading 3d deep neural networks for fast anomaly detection and localization in crowded scenes">

                                <b>[14]</b> Sabokrou M, Fayyaz M, Fathy M, et al. Deep-cascade: Cascading 3D deep neural networks for fast anomaly detection and localization in crowded scenes[J]. IEEE Transactions on Image Processing, 2017, 26 (4) :1992-2004.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Training adversarial discriminators for cross-channel abnormal event detection in crowds[EB]">

                                <b>[15]</b> Ravanbakhsh M, Sangineto E, Nabi M, et al. Training adversarial discriminators for cross-channel abnormal event detection in crowds[EB]. arXiv:1706.07680, 2017.
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=An overview of deep learning based methods for unsupervised and semi-supervised anomaly detection in videos">

                                <b>[16]</b> Kiran B, Dilip T, Ranjith P. An overview of deep learning based methods for unsupervised and semi-supervised anomaly detection in videos[J]. Journal of Imaging, 2018, 4 (2) .
                            </a>
                        </p>
                        <p id="35">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Universal Multimode Background Subtraction">

                                <b>[17]</b> Sajid H, Cheung S S. Universal multimode background subtraction[J]. IEEE Transactions on Image Processing A Publication of the IEEE Signal Processing Society, 2017, 26 (7) :3249-3260.
                            </a>
                        </p>
                        <p id="37">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=An Improved Adaptive Background Mixture Model for Real-time Tracking with Shadow Detection">

                                <b>[18]</b> KaewTraKulPong P, Bowden R. An improved adaptive background mixture model for real-time tracking with shadow detection[M]//VIDEO BASED SURVEILLANCE SYSTEMS: Computer Vision and Distributed Processing. Kluwer Academic Publishers, 2002:135-144.
                            </a>
                        </p>
                        <p id="39">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=An improved algorithm for background method of mixed Gaussian model">

                                <b>[19]</b> Liu J, Wang L. An improved algorithm for background method of mixed Gaussian model[J].Computer Engineering and Applications, 2010, 46 (13) :168-170.
                            </a>
                        </p>
                        <p id="41">
                            <a id="bibliography_20" target="_blank" href="/kcms/detail/detail.aspx?dbcode=DKFX&amp;filename=JZDF201804015&amp;v=MTgwMzJPM3pxcUJ0R0ZyQ1VSN3FmWnVac0ZpRGxWNzdQTHpmUGFMRzRIOW5NcTQ5RVlZUUtESDg0dlI0VDZqNTQ=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[20]</b> Xu Y C, Tan W A, Chen L T. A moving target detection algorithm based on improved mixed gaussian model[J].Control Engineering, 2018, 25 (4) :630-635.
                            </a>
                        </p>
                        <p id="43">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Moving target detection method based on adaptive hybrid Gaussian background model">

                                <b>[21]</b> Huang X J, Zhou J M, Liu B Y. Moving target detection method based on adaptive hybrid Gaussian background model[J]. Journal of Computer Applications, 2010, 30 (1) : 71-74.
                            </a>
                        </p>
                        <p id="45">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Network in network[EB]">

                                <b>[22]</b> Lin M, Chen Q, Yan S. Network in network[EB]. arXiv:1312.4400, 2013.
                            </a>
                        </p>
                        <p id="47">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Batch Normalization:Accelerating Deep Network Training by Reducing Internal Covariate Shift">

                                <b>[23]</b> Ioffe S, Szegedy C. Batch normalization: accelerating deep network training by reducing internal covariate shift[C]//Proceedings of the 32nd International Conference on International Conference on Machine Learning—Volume 37. JMLR.org, 2015:448-456.
                            </a>
                        </p>
                        <p id="49">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Ucsd anomaly dataset[OL]">

                                <b>[24]</b> Ucsd anomaly dataset[OL]. http://www.svcl.ucsd.edu/projects/anomaly 25.
                            </a>
                        </p>
                        <p id="51">
                            <a id="bibliography_25" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Umn anomaly dataset[OL]">

                                <b>[25]</b> Umn anomaly dataset[OL]. http://mha.cs.umn.edu/Movies/Crowd-Activity-All.avi.
                            </a>
                        </p>
                        <p id="53">
                            <a id="bibliography_26" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Plug-and-Play CNN for Crowd Motion Analysis: An Application in Abnormal Event Detection">

                                <b>[26]</b> Ravanbakhsh M, Nabi M, Mousavi H, et al. Plug-and-Play CNN for Crowd Motion Analysis: An Application in Abnormal Event Detection[C]//2018 IEEE Winter Conference on Applications of Computer Vision (WACV) . IEEE Computer Society, 2018.
                            </a>
                        </p>
                        <p id="55">
                            <a id="bibliography_27" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Detecting anomalous events in videos by learning deep representations of appearance and motion">

                                <b>[27]</b> Xu D X, Yan W Y, Ricci E, et al. Detecting anomalous events in videos by learning deep representations of appearance and motion[M]. Elsevier Science Inc. 2017.
                            </a>
                        </p>
                        <p id="57">
                            <a id="bibliography_28" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Anomaly detection in crowded scenes using motion energy model">

                                <b>[28]</b> Chen T, Hou C, Wang Z, et al. Anomaly detection in crowded scenes using motion energy model[J]. Multimedia Tools &amp; Applications, 2017 (3) :1-16.
                            </a>
                        </p>
                        <p id="59">
                            <a id="bibliography_29" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Spatial-temporal convolutional neural networks for anomaly detection and localization in crowded scenes">

                                <b>[29]</b> Zhou S, Shen W, Zeng D, et al. Spatial-temporal convolutional neural networks for anomaly detection and localization in crowded scenes[J]. Signal Processing Image Communication, 2016, 47:358-368.
                            </a>
                        </p>
                        <p id="61">
                            <a id="bibliography_30" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Crowd motion monitoring using tracklet-based commotion measure">

                                <b>[30]</b> Mousavi H, Nabi M, Kiani H, et al. Crowd motion monitoring using tracklet-based commotion measure[C]//IEEE International Conference on Image Processing. IEEE, 2015:2354-2358.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JYRJ201903037" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201903037&amp;v=MDMyNzVxcUJ0R0ZyQ1VSN3FmWnVac0ZpRGxWNzdQTHpUWlpMRzRIOWpNckk5R1k0UUtESDg0dlI0VDZqNTRPM3o=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="1" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
