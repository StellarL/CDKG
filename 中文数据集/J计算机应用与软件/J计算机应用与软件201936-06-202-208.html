<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637135647843537500%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJYRJ201906039%26RESULT%3d1%26SIGN%3dTkLUWAFV4LzauXitxXBf4BTepQA%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JYRJ201906039&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JYRJ201906039&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201906039&amp;v=MTkwNzdMek9MelRaWkxHNEg5ak1xWTlHYllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVp0Rnl2bVY=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#25" data-title="&lt;b&gt;0 引 言&lt;/b&gt; "><b>0 引 言</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#29" data-title="&lt;b&gt;1 生成式对抗网络&lt;/b&gt; "><b>1 生成式对抗网络</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#32" data-title="&lt;b&gt;1.1 裂缝图像生成&lt;/b&gt;"><b>1.1 裂缝图像生成</b></a></li>
                                                <li><a href="#40" data-title="&lt;b&gt;1.2 裂缝图像修复&lt;/b&gt;"><b>1.2 裂缝图像修复</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#50" data-title="&lt;b&gt;2 生成式对抗网络的裂缝修复模型&lt;/b&gt; "><b>2 生成式对抗网络的裂缝修复模型</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#51" data-title="&lt;b&gt;2.1 距离加权修复掩膜&lt;/b&gt;"><b>2.1 距离加权修复掩膜</b></a></li>
                                                <li><a href="#64" data-title="&lt;b&gt;2.2 差异度量判别器&lt;/b&gt;"><b>2.2 差异度量判别器</b></a></li>
                                                <li><a href="#76" data-title="&lt;b&gt;2.3 生成式对抗网络的裂缝修复模型结构&lt;/b&gt;"><b>2.3 生成式对抗网络的裂缝修复模型结构</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#93" data-title="&lt;b&gt;3 实验结果与分析&lt;/b&gt; "><b>3 实验结果与分析</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#94" data-title="&lt;b&gt;3.1 数据集与裂缝修复生成式对抗网络的训练&lt;/b&gt;"><b>3.1 数据集与裂缝修复生成式对抗网络的训练</b></a></li>
                                                <li><a href="#97" data-title="&lt;b&gt;3.2 可视化比较&lt;/b&gt;"><b>3.2 可视化比较</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#119" data-title="&lt;b&gt;4 结 语&lt;/b&gt; "><b>4 结 语</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#31" data-title="图1 生成式对抗网络结构图">图1 生成式对抗网络结构图</a></li>
                                                <li><a href="#78" data-title="图2 CI-GAN结构图">图2 CI-GAN结构图</a></li>
                                                <li><a href="#89" data-title="图3 判别器结构">图3 判别器结构</a></li>
                                                <li><a href="#92" data-title="图4 生成网络结构对比图">图4 生成网络结构对比图</a></li>
                                                <li><a href="#100" data-title="图5 掩膜修复效果对比图">图5 掩膜修复效果对比图</a></li>
                                                <li><a href="#101" data-title="&lt;b&gt;表1 不同掩膜修复效果对比&lt;/b&gt;"><b>表1 不同掩膜修复效果对比</b></a></li>
                                                <li><a href="#103" data-title="图6 判别器修复效果对比图">图6 判别器修复效果对比图</a></li>
                                                <li><a href="#104" data-title="&lt;b&gt;表2 不同判别器的修复效果对比&lt;/b&gt;"><b>表2 不同判别器的修复效果对比</b></a></li>
                                                <li><a href="#106" data-title="图7 模型结构修复效果对比图">图7 模型结构修复效果对比图</a></li>
                                                <li><a href="#107" data-title="&lt;b&gt;表3 不同模型结构的修复效果对比&lt;/b&gt;"><b>表3 不同模型结构的修复效果对比</b></a></li>
                                                <li><a href="#110" data-title="图8 修复算法修复效果对比图">图8 修复算法修复效果对比图</a></li>
                                                <li><a href="#111" data-title="&lt;b&gt;表4 不同修复算法效果对比&lt;/b&gt;"><b>表4 不同修复算法效果对比</b></a></li>
                                                <li><a href="#114" data-title="图9 落叶修复效果图">图9 落叶修复效果图</a></li>
                                                <li><a href="#116" data-title="图10 石子修复效果图">图10 石子修复效果图</a></li>
                                                <li><a href="#118" data-title="图11 CI-GAN裂缝修复过程图">图11 CI-GAN裂缝修复过程图</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="130">


                                    <a id="bibliography_1" title="Bertalmio M, Sapiro G, Caselles V, et al.Image Inpainting[C]//Proceedings of the 27th annual conference on Computer graphics and interactive techniques.ACM, 2000:417-424." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image inpainting">
                                        <b>[1]</b>
                                        Bertalmio M, Sapiro G, Caselles V, et al.Image Inpainting[C]//Proceedings of the 27th annual conference on Computer graphics and interactive techniques.ACM, 2000:417-424.
                                    </a>
                                </li>
                                <li id="132">


                                    <a id="bibliography_2" title="Criminisi A, Perez P, Toyama K.Region filling and object removal by exemplar-based image inpainting[J].IEEETransactions on Image Processing, 2004, 13 (9) .1200-1212." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Region filling and object removal by exemplar-based image inpainting">
                                        <b>[2]</b>
                                        Criminisi A, Perez P, Toyama K.Region filling and object removal by exemplar-based image inpainting[J].IEEETransactions on Image Processing, 2004, 13 (9) .1200-1212.
                                    </a>
                                </li>
                                <li id="134">


                                    <a id="bibliography_3" title="Bertalmio M, Vese L, Sapiro G, et al.Simultaneous Structure and Texture Image Inpainting[J].IEEE Transactions on Image Processing, 2003, 12 (8) :882-889." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Simultaneous structure and texture image inpainting">
                                        <b>[3]</b>
                                        Bertalmio M, Vese L, Sapiro G, et al.Simultaneous Structure and Texture Image Inpainting[J].IEEE Transactions on Image Processing, 2003, 12 (8) :882-889.
                                    </a>
                                </li>
                                <li id="136">


                                    <a id="bibliography_4" title="Shen J, Chan T F.Mathematical Models for Local Nontexture Inpaintings[J].SIAM Journal on Applied Mathematics, 2002, 62:1019-1043." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJST&amp;filename=SJST14120403043018&amp;v=MDQ2Mzg5R1pPOE1ESDB4b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcnpJSTEwVGFSUT1OaWZZZXJLOEg5UE1xNA==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                        Shen J, Chan T F.Mathematical Models for Local Nontexture Inpaintings[J].SIAM Journal on Applied Mathematics, 2002, 62:1019-1043.
                                    </a>
                                </li>
                                <li id="138">


                                    <a id="bibliography_5" title="Chan T F, Shen J.Non-texture inpainting by curvature-driven diffusions (CDD) [J].Journal of Visual Communication and Image Representation, 2001, 12 (4) :436-449." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501264158&amp;v=MTI1OTJxbzlFWnUwTERYa3hvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp0RmlubFVyeklJMTBUYVJRPU5pZk9mYks3SHRETg==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                        Chan T F, Shen J.Non-texture inpainting by curvature-driven diffusions (CDD) [J].Journal of Visual Communication and Image Representation, 2001, 12 (4) :436-449.
                                    </a>
                                </li>
                                <li id="140">


                                    <a id="bibliography_6" title="Yeh R, Chen C, Lim T Y, et al.Semantic Image Inpainting with Perceptual and Contextual Losses[C]//The IEEE Computer Society Conference on Computer Vision and Pattern Recognition, Los Alamitos, July 7-26, 2016." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Semantic Image Inpainting with Perceptual and Contextual Losses">
                                        <b>[6]</b>
                                        Yeh R, Chen C, Lim T Y, et al.Semantic Image Inpainting with Perceptual and Contextual Losses[C]//The IEEE Computer Society Conference on Computer Vision and Pattern Recognition, Los Alamitos, July 7-26, 2016.
                                    </a>
                                </li>
                                <li id="142">


                                    <a id="bibliography_7" title="Pathak D, Krahenbuhl P, Donahue J, et al.Context Encoders:Feature Learning by Inpainting[C]//Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition.Los Alamitos.IEEE Computer Society Press, 2016:2536-2544." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Context Encoders:Feature Learning by Inpainting">
                                        <b>[7]</b>
                                        Pathak D, Krahenbuhl P, Donahue J, et al.Context Encoders:Feature Learning by Inpainting[C]//Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition.Los Alamitos.IEEE Computer Society Press, 2016:2536-2544.
                                    </a>
                                </li>
                                <li id="144">


                                    <a id="bibliography_8" title="Iizuka S, Simo-Serra E, Ishikawa H.Globally and locally consistent image completion[J].ACM Transactions on Graphics, 2017, 36 (4) :1-14." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM408B7C15F867A55233C65423FF57BC91&amp;v=MDk4MTVtYUJ1SFlmT0dRbGZCckxVMDV0dGh4N3U0dzY0PU5pZklZN2U0RnFQTDNJNUFFdU1KQ3cwOHloUVE2VXg3VFh2Z3IyUkRmTFhtTnJPZUNPTnZGU2lXV3I3SklGcA==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                        Iizuka S, Simo-Serra E, Ishikawa H.Globally and locally consistent image completion[J].ACM Transactions on Graphics, 2017, 36 (4) :1-14.
                                    </a>
                                </li>
                                <li id="146">


                                    <a id="bibliography_9" title="Li Y J, Liu S F.Generative Face Completion[C]//Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition.Los Alamitos.IEEE Computer Society Press, 2017:3911-3191." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Generative Face Completion">
                                        <b>[9]</b>
                                        Li Y J, Liu S F.Generative Face Completion[C]//Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition.Los Alamitos.IEEE Computer Society Press, 2017:3911-3191.
                                    </a>
                                </li>
                                <li id="148">


                                    <a id="bibliography_10" title="Yang C, Lu X, Lin Z, et al.High-Resolution Image Inpainting using Multi-Scale Neural Patch Synthesis[C]//Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition.Los Alamitos.IEEE Computer Society Press, 2017:6721-6729." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=High-Resolution Image Inpainting Using Multi-scale Neural Patch Synthesis">
                                        <b>[10]</b>
                                        Yang C, Lu X, Lin Z, et al.High-Resolution Image Inpainting using Multi-Scale Neural Patch Synthesis[C]//Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition.Los Alamitos.IEEE Computer Society Press, 2017:6721-6729.
                                    </a>
                                </li>
                                <li id="150">


                                    <a id="bibliography_11" title="Ren S Q, He K M, Girshick R, et al.Faster R-CNN:Towards Real-Time Object Detection with Region Proposal Networks[C]//Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition.Los Alamitos.IEEE Computer Society Press, 2015:1137-1149." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Faster R-CNN:Towards Real-Time Object Detection with Region Proposal Networks">
                                        <b>[11]</b>
                                        Ren S Q, He K M, Girshick R, et al.Faster R-CNN:Towards Real-Time Object Detection with Region Proposal Networks[C]//Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition.Los Alamitos.IEEE Computer Society Press, 2015:1137-1149.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JYRJ" target="_blank">计算机应用与软件</a>
                2019,36(06),202-208 DOI:10.3969/j.issn.1000-386x.2019.06.038            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于生成式对抗网络的裂缝图像修复方法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%83%A1%E6%95%8F&amp;code=27234072&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">胡敏</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%8E%E8%89%AF%E7%A6%8F&amp;code=10948989&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">李良福</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E9%99%95%E8%A5%BF%E5%B8%88%E8%8C%83%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E5%AD%A6%E9%99%A2&amp;code=0068380&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">陕西师范大学计算机科学学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>提出一种基于生成式对抗网络的裂缝图像修复方法。在修复过程中, 对障碍物所在位置进行信息擦除获得待修复图像。使用生成式对抗网络生成相应的裂缝图像, 为待修复图像和生成图像分别覆盖距离加权掩膜, 并计算获得修复块。对修复块与待修复图像的拼接图像进行优化获得最终修复结果。实验结果表明, 该方法可对裂缝图像进行了准确修复。与传统的修复方法相比, 使用该方法修复后的裂缝图像较之前方法峰值信噪比提升了0.6～0.9 dB, 实现了在有限的裂缝数据集条件下, 生成大量还原度较高的裂缝图像。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%B7%AF%E9%9D%A2%E8%A3%82%E7%BC%9D&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">路面裂缝;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%94%9F%E6%88%90%E5%BC%8F%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">生成式对抗网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%9B%BE%E5%83%8F%E4%BF%AE%E5%A4%8D&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">图像修复;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    胡敏, 硕士生, 主研领域:深度学习, 计算机视觉。;
                                </span>
                                <span>
                                    李良福, 副教授。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-12-24</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金项目 (61573232, 61401263);</span>
                    </p>
            </div>
                    <h1><b>THE CRACK IMAGE RESTORATION METHOD BASED ON GENERATIVE ADVERSARIAL NETWORK</b></h1>
                    <h2>
                    <span>Hu Min</span>
                    <span>Li Liangfu</span>
            </h2>
                    <h2>
                    <span>College of Computer and Science, Shaanxi Normal University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>We proposed a crack image restoration method based on generative adversarial network. In the restoration process, the obstacle location information was erased to obtain the defective image, and the corresponding crack image was generated by using the generated adversarial network. Distance weighting mask was covered for the defective image and the generated image respectively, and the repair block was achieved. Then, the joint image of the repair block and the defective image was optimized to obtain the final restoration result. The experimental results show that the proposed method can repair the crack image accurately. Compared with the traditional restoration method, the peak signal-to-noise ratio of restored image is increased by 0.6 dB to 0.9 dB, which generaes a large number of crack images with high reduction degree under the condition of limited crack data set.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Pavement%20crack&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Pavement crack;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Deep%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Deep learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Generative%20adversarial%20network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Generative adversarial network;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Image%20restoration&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Image restoration;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-12-24</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="25" name="25" class="anchor-tag"><b>0 引 言</b></h3>
                <div class="p1">
                    <p id="26">裂缝作为最常见的公路问题, 对其的检测必不可少。训练一个对裂缝信息敏感的检测器, 需要数量庞大且形态各异的裂缝图像数据集作为支撑, 然而现实生活中采集到的裂缝图像往往包含许多除了裂缝以外的障碍物, 如落叶、石子、电缆等, 这些障碍物可能会影响后期检测器对裂缝的检测, 造成误检、漏检等问题, 因此对采集到的裂缝图像进行修复至关重要。</p>
                </div>
                <div class="p1">
                    <p id="27">近年来, 随着计算机的发展, 一系列基于数字图像处理、深度学习的方法被运用于图像修复<citation id="157" type="reference"><link href="130" rel="bibliography" /><link href="132" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>]</sup></citation>领域。Bertalmio等<citation id="152" type="reference"><link href="134" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>提出采用偏微分方程的方法进行图像修复, 但是该算法缺少稳定性, 修复结果往往不佳。随后Chan等在此基础上提出基于能量最小化原则的统一修复模型<citation id="153" type="reference"><link href="136" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>, 但由于该模型受到修复区域大小的限制, 且不满足连续性原则, 进而又提出一种基于曲率扩散模型<citation id="154" type="reference"><link href="138" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>。只是以上算法均只适用于非纹理图像的修复, 当所需修复的图像是纹理图像时则没有办法完成修复任务。Raymond Yeh等<citation id="155" type="reference"><link href="140" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>在2016年提出使用深度卷积生成式对抗网络的方法进行图像修复, 通过训练有素的深度神经网络对破损图像的纹理特征进行修复, 降低了噪声对修复结果的影响, 带来了视觉多样性。但由于修复网络中的二进制掩码的不可变性使得所有的像素点在图像修复过程中对修复区域贡献相同, 往往导致修复结果不稳定;随后一系列基于文献<citation id="156" type="reference">[<a class="sup">6</a>]</citation>的改进<citation id="158" type="reference"><link href="142" rel="bibliography" /><link href="144" rel="bibliography" /><link href="146" rel="bibliography" /><link href="148" rel="bibliography" /><link href="150" rel="bibliography" /><sup>[<a class="sup">7</a>,<a class="sup">8</a>,<a class="sup">9</a>,<a class="sup">10</a>,<a class="sup">11</a>]</sup></citation>被提出, 虽然取得了相对较好的输出效果, 但此类方法固定了受损区域的大小以及位置, 灵活性较差, 并且增加了修复过程的复杂性。</p>
                </div>
                <div class="p1">
                    <p id="28">针对以上缺点, 本文提出了一种基于生成式对抗网络的裂缝图像修复方法CI-GAN (Crack Inpaint Generative Adversarial Network) , 首先擦除裂缝图像中的障碍物信息获得仅含有裂缝信息的待修复图像, 同时使用CI-GAN生成一系列候选向量送入CI-GAN的差异判别器, 计算候选向量的感知损失。然后分别为待修复图像和候选向量覆盖距离惩罚掩码, 计算待修复图像与覆盖距离惩罚掩码的候选向量之间的语义损失。选取感知损失与语义损失之和最小的候选向量为最优补全图, 并截取最优补全图中与待修复图像中信息缺失位置对应的像素块作为修复块, 将修复块拼接到待修复位置的残缺处即获得修复图。最后使用CI-GAN的全局判别器计算修复图的全局损失, 进一步对最优补全图进行优化得到最终的修复图。实验表明, 本文修复方法得到的裂缝图像对障碍物去除得更彻底, 且修复后的裂缝图像质量更高。</p>
                </div>
                <h3 id="29" name="29" class="anchor-tag"><b>1 生成式对抗网络</b></h3>
                <div class="p1">
                    <p id="30">生成式对抗网络类似于一个博弈结构, 其由生成器和判别器两个部分组成, 如图1所示。判别器以真实裂缝图像和生成器构造的虚假裂缝图像为输入, 通过神经网络模型对输入进行特征提取, 最终输出[0, 1]区间的概率值, 真实裂缝图像对应的标签为1, 生成器生成的裂缝图像对应的标签为0;生成器则以随机生成高斯白噪声为网络的输入, 经过生成器网络的解码过程, 最终输出一个与真实裂缝图像大小相同的向量, 然后通过判别器预测的置信值计算与真实类别对应标签之间的差距, 直接将此误差作为反向传播的误差来更新参数以及最初的输入向量。</p>
                </div>
                <div class="area_img" id="31">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201906039_031.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 生成式对抗网络结构图" src="Detail/GetImg?filename=images/JYRJ201906039_031.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 生成式对抗网络结构图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201906039_031.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="32" name="32"><b>1.1 裂缝图像生成</b></h4>
                <div class="p1">
                    <p id="33">生成式对抗网络采用交叉熵损失函数对参数进行优化, 损失函数如下:</p>
                </div>
                <div class="p1">
                    <p id="34" class="code-formula">
                        <mathml id="34"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mi>G</mi></munder><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mi>D</mi></munder><mi>V</mi><mo stretchy="false"> (</mo><mi>D</mi><mo>, </mo><mi>G</mi><mo stretchy="false">) </mo><mo>=</mo><mi>E</mi><msub><mrow></mrow><mrow><mi>x</mi><mo>∼</mo><mi>p</mi><msub><mrow></mrow><mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo></mrow></msub></mrow></msub><mo stretchy="false">[</mo><mrow><mi>log</mi></mrow><mo stretchy="false"> (</mo><mi>D</mi><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mo>+</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mi>E</mi><msub><mrow></mrow><mrow><mi>x</mi><mo>∼</mo><mi>p</mi><msub><mrow></mrow><mrow><mi>G</mi><mo stretchy="false"> (</mo><mi>z</mi><mo stretchy="false">) </mo></mrow></msub></mrow></msub><mo stretchy="false">[</mo><mrow><mi>log</mi></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>D</mi><mo stretchy="false"> (</mo><mi>G</mi><mo stretchy="false"> (</mo><mi>z</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="35">式中:<i>D</i>为判别器, <i>G</i>为生成器, <i>x</i>为真实裂缝图像, <i>z</i>为生成的随机噪声, <i>E</i><sub><i>x</i>～<i>p</i><sub><i>data</i> (<i>x</i>) </sub></sub>[log (<i>D</i> (<i>x</i>) ) ]表示真实裂缝图像的判别误差取对数的平均值, <i>E</i><sub><i>x</i>～<i>p</i><sub><i>G</i> (<i>z</i>) </sub></sub>[log (1-<i>D</i> (<i>G</i> (<i>z</i>) ) ) ]表示生成裂缝图像的判别误差取对数的平均值, <mathml id="36"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mi>G</mi></munder><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mi>D</mi></munder><mi>V</mi><mo stretchy="false"> (</mo><mi>D</mi><mo>, </mo><mi>G</mi><mo stretchy="false">) </mo></mrow></math></mathml>表示判别器最大化真实裂缝图像和生成裂缝图像之间的差异, 生成器最小化真实裂缝图像和生成裂缝图像之间的差异。首先对判别器进行优化:</p>
                </div>
                <div class="p1">
                    <p id="37" class="code-formula">
                        <mathml id="37"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mi>D</mi></munder><mi>V</mi><mo stretchy="false"> (</mo><mi>D</mi><mo>, </mo><mi>G</mi><mo stretchy="false">) </mo><mo>=</mo><mi>E</mi><msub><mrow></mrow><mrow><mi>x</mi><mo>∼</mo><mi>p</mi><msub><mrow></mrow><mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo></mrow></msub></mrow></msub><mo stretchy="false">[</mo><mrow><mi>log</mi></mrow><mo stretchy="false"> (</mo><mi>D</mi><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mo>+</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mi>E</mi><msub><mrow></mrow><mrow><mi>x</mi><mo>∼</mo><mi>p</mi><msub><mrow></mrow><mrow><mi>G</mi><mo stretchy="false"> (</mo><mi>z</mi><mo stretchy="false">) </mo></mrow></msub></mrow></msub><mo stretchy="false">[</mo><mrow><mi>log</mi></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>D</mi><mo stretchy="false"> (</mo><mi>G</mi><mo stretchy="false"> (</mo><mi>z</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="38">此时默认生成器是固定不变的, 判别器优化即为一个二分类判别模型, 输入的真实裂缝图片对应标签为1, 生成器生成的裂缝图片对应标签为0。对生成器进行优化时, 此时判别器仅充当判别输入裂缝图像真伪的分类器, 因此仅计算下式给出的损失即可:</p>
                </div>
                <div class="p1">
                    <p id="39" class="code-formula">
                        <mathml id="39"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mi>G</mi></munder><mi>V</mi><mo stretchy="false"> (</mo><mi>D</mi><mo>, </mo><mi>G</mi><mo stretchy="false">) </mo><mo>=</mo><mi>E</mi><msub><mrow></mrow><mrow><mi>x</mi><mo>∼</mo><mi>p</mi><msub><mrow></mrow><mrow><mi>G</mi><mo stretchy="false"> (</mo><mi>z</mi><mo stretchy="false">) </mo></mrow></msub></mrow></msub><mo stretchy="false">[</mo><mrow><mi>log</mi></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>D</mi><mo stretchy="false"> (</mo><mi>G</mi><mo stretchy="false"> (</mo><mi>z</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <h4 class="anchor-tag" id="40" name="40"><b>1.2 裂缝图像修复</b></h4>
                <div class="p1">
                    <p id="41">裂缝具有细长, 分布无规律的特性, 当裂缝图像中出现水渍、阴影、电线、石头等障碍物时, 裂缝误检或检测不出来的情况很容易发生。在对裂缝图像进行修复时, 首先将裂缝图像中障碍物所在位置的像素置零, 擦除障碍物所在位置的信息, 同时使用训练好的生成器生成一系列候选向量, 将生成的候选向量输入到已经训练好的判别器中, 得到感知损失:</p>
                </div>
                <div class="p1">
                    <p id="42"><i>L</i><sub><i>p</i></sub>=log (1-<i>D</i> (<i>G</i> (<i>z</i>) ) )      (4) </p>
                </div>
                <div class="p1">
                    <p id="43">式中:<i>L</i><sub><i>p</i></sub>为感知损失, 为了提取待修复的裂缝图像的缺失部分位置信息, 同时生成与用于待修复的裂缝图像补全的修复块, 文献<citation id="159" type="reference">[<a class="sup">6</a>]</citation>给出一种二进制掩膜:</p>
                </div>
                <div class="area_img" id="44">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JYRJ201906039_04400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="46">在该掩膜中, 待修复的裂缝图像的受损区域对应值为1, 未受损区域对应值为0, <i>M</i><sub><i>ij</i></sub>表示图像第<i>i</i>行第<i>j</i>列对应位置的掩码值, <i>σ</i>表示图像中的信息缺失区域。分别为待修复的裂缝图像和候选向量覆盖二进制掩膜, 并计算差异:</p>
                </div>
                <div class="p1">
                    <p id="47"><mathml id="48"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><msub><mrow></mrow><mi>s</mi></msub><mo>=</mo><mrow><mo>|</mo><mrow><mi>Μ</mi><mo>⊙</mo><mo stretchy="false"> (</mo><mi>G</mi><mo stretchy="false"> (</mo><mi>z</mi><mo stretchy="false">) </mo><mo>-</mo><mi>x</mi><mo stretchy="false">) </mo></mrow><mo>|</mo></mrow><msub><mrow></mrow><mn>1</mn></msub></mrow></math></mathml>      (6) </p>
                </div>
                <div class="p1">
                    <p id="49">式中:<i>L</i><sub><i>s</i></sub>表示语义损失, <i>M</i>⊙ (<i>G</i> (<i>z</i>) -<i>x</i>) 表示生成图像和待修复图像加入掩膜后的像素差异, 其中<i>L</i><sub><i>s</i></sub>越小, 表示生成的裂缝图像和待修复的裂缝图像在非受损区域像素分布越接近, 寻找语义损失和感知损失最小的候选向量作为最优裂缝补全图, 提取其与待修复的裂缝图像的受损区域对应位置的像素块作为修复块, 将修复块与待修复的裂缝图像拼接得到补全图。该方法在对裂缝图像进行修复时, 忽略了不同像素贡献的信息重要程度不同的问题, 此外修复后的裂缝图像没有进行语义上的再判断, 使补全图在视觉上看缺乏真实性, 无法直接在裂缝检测任务中使用。</p>
                </div>
                <h3 id="50" name="50" class="anchor-tag"><b>2 生成式对抗网络的裂缝修复模型</b></h3>
                <h4 class="anchor-tag" id="51" name="51"><b>2.1 距离加权修复掩膜</b></h4>
                <div class="p1">
                    <p id="52">传统的生成式对抗网络进行图像修复时使用的修复掩膜为二进制掩膜, 其默认在待修复区域内, 所有的像素点对修复时所作的贡献是相同的。这样做会导致生成器只注意到距离修复区域边缘远的像素信息, 而忽略临近修复区域边缘的像素信息, 从而造成修复边缘不连贯问题。</p>
                </div>
                <div class="p1">
                    <p id="53">为了改善上述问题, 本文提出了一种基于距离的加权修复掩膜<i>M</i><sub><i>D</i></sub>以及反向修复掩膜<i>M</i><mathml id="54"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>D</mi><mrow><mo>-</mo><mn>1</mn></mrow></msubsup></mrow></math></mathml>, 其公式如下:</p>
                </div>
                <div class="area_img" id="55">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JYRJ201906039_05500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="area_img" id="57">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JYRJ201906039_05700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="59">式中:<i>i</i>表示图像矩阵对应行, <i>j</i>表示图像矩阵对应列, 点<i>p</i>表示待修复区域的中心点位置, <i>x</i><sub><i>ij</i></sub>为当前像素点位置, <b><i>M</i></b>为标记矩阵, <i>M</i><sub><i>ij</i></sub>为标记矩阵内位置为 (<i>i</i>, <i>j</i>) 的对应点, <i>M</i><sub><i>D</i></sub>为距离修复掩膜, 此时的语义损失变为:</p>
                </div>
                <div class="p1">
                    <p id="60"><mathml id="61"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><msub><mrow></mrow><mi>s</mi></msub><mo>=</mo><mrow><mo>|</mo><mrow><mi>Μ</mi><msub><mrow></mrow><mi>D</mi></msub><mo>⊙</mo><mo stretchy="false"> (</mo><mi>G</mi><mo stretchy="false"> (</mo><mi>z</mi><mo stretchy="false">) </mo><mo>-</mo><mi>x</mi><mo stretchy="false">) </mo></mrow><mo>|</mo></mrow><msub><mrow></mrow><mn>1</mn></msub></mrow></math></mathml>      (9) </p>
                </div>
                <div class="p1">
                    <p id="62">在图像修复过程中, 待修复区域边缘的像素带是图像的完整区域到残缺区域的过渡区域, 对抗网络生成的补全图像的分布形状和结构与此部分的像素信息取值息息相关。因此, 此区域对应的像素分布差值应比其他位置的像素分布差值分配更大的权值。换句话说, 当生成器生成的补全图像与待修复图像的像素分布存在差异时, 差异位置越靠近残缺位置惩罚力度越大, 差异位置越远离残缺位置, 惩罚力度越小。当搜索到的补全图像与残缺图像的像素分布差值为同一值时, 待修复区域边缘的像素分布与最优向量<mathml id="63"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">z</mi><mo>^</mo></mover></math></mathml>的形态分布更加逼近受损图像, 从而在一定程度上改善了补全图像边缘不连续问题。</p>
                </div>
                <h4 class="anchor-tag" id="64" name="64"><b>2.2 差异度量判别器</b></h4>
                <div class="p1">
                    <p id="65">传统的生成式对抗网络中, 判别器用来鉴别输入的是真实裂缝图像还是生成器生成的裂缝图像。真实裂缝图像对应标签为1, 生成器生成的裂缝图像对应标签为0。在此过程中真实裂缝图像和生成器生成的裂缝图像之间互不干扰, 两者的潜在关系仅通过判别器逐个计算损失并把梯度信息回传给生成器, 生成器根据判别器传递的梯度信息调整模型的参数以及随机噪声的分布。</p>
                </div>
                <div class="p1">
                    <p id="66">为了更好地挖掘真实裂缝图像分布与生成的裂缝图像分布之间的潜在相关性, 引导生成网络生成的裂缝图像可以更完美地拟合真实裂缝图像, 本文对判别器结构作了如下改变。考虑到真实的裂缝图像与生成器生成的裂缝图像之间具有一定的相关性, 并且真实的裂缝图像对应的真实程度和生成器生成的裂缝图像对应的虚假程度都是相对于判别器学习到的两者间的相对差异而言的, 所以它们之间的差异可以表示为:</p>
                </div>
                <div class="p1">
                    <p id="67"><mathml id="68"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>d</mi><mi>i</mi><mi>s</mi><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>G</mi><mo stretchy="false"> (</mo><mi>z</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo>=</mo><mrow><mo>|</mo><mrow><mi>F</mi><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mo>-</mo><mi>E</mi><mo stretchy="false"> (</mo><mi>F</mi><mo stretchy="false"> (</mo><mi>G</mi><mo stretchy="false"> (</mo><mi>z</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo stretchy="false">) </mo></mrow><mo>|</mo></mrow><msub><mrow></mrow><mrow><mi>L</mi><msub><mrow></mrow><mn>1</mn></msub></mrow></msub></mrow></math></mathml>      (10) </p>
                </div>
                <div class="p1">
                    <p id="69">传统的判别器输出经激活函数<i>D</i>=<i>σ</i> (<i>F</i> (·) ) 映射到[0, 1]区间的概率值, 其中<i>F</i>表示判别网络特征提取过程, <i>σ</i>表示激活函数。因此, 式 (10) 中<i>F</i> (<i>x</i>) 表示真实高分辨率裂缝图像经过判别网络后提取到的特征, <i>F</i> (<i>G</i> (<i>z</i>) ) 表示生成的超分辨率裂缝图像经过判别网络后提取到的特征, <i>E</i> (<i>x</i>) 表示均值。对于真实的裂缝图像判别器输出为<i>D</i> (<i>dis</i> (<i>x</i>, <i>G</i> (<i>z</i>) ) ) , 对应生成的裂缝图像判别器输出为<i>D</i> (<i>dis</i> (<i>G</i> (<i>z</i>) , <i>x</i>) ) , 经过激活函数映射, 此时的判别器输出仍为[0, 1]的概率值, 与传统的判别器一样, 真实的裂缝图像相对生成裂缝的图像的真实程度对应标签为1, 生成器生成的裂缝图像相对真实的裂缝图像的虚假程度对应标签为0。此时生成式对抗网络的损失具有如下定义:</p>
                </div>
                <div class="p1">
                    <p id="70"><i>L</i><sub>adv</sub>=<i>E</i><sub><i>x</i>～<i>P</i><sub><i>data</i> (<i>x</i>) </sub></sub>[log (<i>D</i> (<i>dis</i> (<i>x</i>, <i>G</i> (<i>z</i>) ) ) ) ]+</p>
                </div>
                <div class="p1">
                    <p id="71"><i>E</i><sub><i>x</i>～<i>P</i><sub><i>G</i> (<i>z</i>) </sub></sub>[log (1-<i>D</i> (<i>dis</i> (<i>G</i> (<i>z</i>) , <i>x</i>) ) ) ]      (11) </p>
                </div>
                <div class="p1">
                    <p id="72">式中:<i>L</i><sub>adv</sub>表示对抗损失。在训练判别器时, 损失函数为:</p>
                </div>
                <div class="p1">
                    <p id="73" class="code-formula">
                        <mathml id="73"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mi>D</mi></munder><mi>V</mi><mo stretchy="false"> (</mo><mi>D</mi><mo>, </mo><mi>G</mi><mo stretchy="false">) </mo><mo>=</mo><mi>E</mi><msub><mrow></mrow><mrow><mi>x</mi><mo>∼</mo><mi>Ρ</mi><msub><mrow></mrow><mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo></mrow></msub></mrow></msub><mo stretchy="false">[</mo><mrow><mi>log</mi></mrow><mo stretchy="false"> (</mo><mi>D</mi><mo stretchy="false"> (</mo><mi>d</mi><mi>i</mi><mi>s</mi><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>G</mi><mo stretchy="false"> (</mo><mi>z</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mo>+</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mi>E</mi><msub><mrow></mrow><mrow><mi>x</mi><mo>∼</mo><mi>Ρ</mi><msub><mrow></mrow><mrow><mi>G</mi><mo stretchy="false"> (</mo><mi>z</mi><mo stretchy="false">) </mo></mrow></msub></mrow></msub><mo stretchy="false">[</mo><mrow><mi>log</mi></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>D</mi><mo stretchy="false"> (</mo><mi>d</mi><mi>i</mi><mi>s</mi><mo stretchy="false"> (</mo><mi>G</mi><mo stretchy="false"> (</mo><mi>z</mi><mo stretchy="false">) </mo><mo>, </mo><mi>x</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>2</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="74">训练生成器时, 损失函数为:</p>
                </div>
                <div class="p1">
                    <p id="75" class="code-formula">
                        <mathml id="75"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mi>G</mi></munder><mi>V</mi><mo stretchy="false"> (</mo><mi>D</mi><mo>, </mo><mi>G</mi><mo stretchy="false">) </mo><mo>=</mo><mi>E</mi><msub><mrow></mrow><mrow><mi>x</mi><mo>∼</mo><mi>p</mi><msub><mrow></mrow><mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo></mrow></msub></mrow></msub><mo stretchy="false">[</mo><mrow><mi>log</mi></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>D</mi><mo stretchy="false"> (</mo><mi>d</mi><mi>i</mi><mi>s</mi><mo stretchy="false"> (</mo><mi>G</mi><mo stretchy="false"> (</mo><mi>z</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo>, </mo><mi>x</mi><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <h4 class="anchor-tag" id="76" name="76"><b>2.3 生成式对抗网络的裂缝修复模型结构</b></h4>
                <div class="p1">
                    <p id="77">本方法不仅引入距离加权修复掩膜和差异度量判别器, 还从对抗网络整体结构、判别器结构、生成器结构三方面设计改进, 针对裂缝图像提出裂缝修复生成式对抗网络CI-GAN (Crack Inpaint Generative Adversarial Network) , 近一步提升裂缝图像的修复效果。模型的整体结构如图2所示, 本文判别器与传统判别器不同, 不但可以判别生成器生成的裂缝图像和真实的裂缝图像的差异, 同时还能够判别修复后的裂缝图像与真实的裂缝图像之间的差异。在CI-GAN模型的训练阶段, 模型的输入由真实的裂缝图像和生成器生成的裂缝图像两部分构成, 判别器执行生成判别器功能, 输出输入图像的真实程度或者虚假程度;在CI-GAN模型的修复阶段, 模型的输入为修复图像, 判别器执行全局判别功能, 输出输入图像整体的虚假程度。</p>
                </div>
                <div class="area_img" id="78">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201906039_078.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 CI-GAN结构图" src="Detail/GetImg?filename=images/JYRJ201906039_078.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 CI-GAN结构图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201906039_078.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="79" name="79"><b>2.3.1 判别器结构</b></h4>
                <div class="p1">
                    <p id="80">判别模型与生成裂缝图像的真实程度相关。本文判别器在不同的场景下, 实现的功能也不相同。当输入为生成的裂缝图像时, 执行生成判别器功能, 当输入为修复后的裂缝图像时执行全局判别器功能。其中生成判别器功能是鉴别输入图像是真实裂缝图像的真实程度还是生成裂缝图像的虚假程度, 全局判别器则是从图像语义分布方向鉴别修复图像整体的虚假程度, 并对应输出全局损失, 其公式如下:</p>
                </div>
                <div class="p1">
                    <p id="81"><i>L</i><sub><i>g</i></sub>=log (1-<i>D</i> (<i>M</i><mathml id="82"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>D</mi><mrow><mo>-</mo><mn>1</mn></mrow></msubsup></mrow></math></mathml>⊙<i>G</i> (<i>z</i>) +<i>M</i><sub><i>D</i></sub>⊙<i>x</i>) )      (14) </p>
                </div>
                <div class="p1">
                    <p id="83">式中:<i>L</i><sub><i>g</i></sub>为全局损失, <i>M</i><sub><i>D</i></sub>为距离加权掩膜, <i>M</i><mathml id="84"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>D</mi><mrow><mo>-</mo><mn>1</mn></mrow></msubsup></mrow></math></mathml>为反向修复掩膜, 模型最小化的<i>L</i><sub><i>g</i></sub>主要目的是改善因待修复图与修复块拼接而造成的拼接边缘不连续问题。CI-GAN模型在修复过程中的损失函数如下:</p>
                </div>
                <div class="p1">
                    <p id="85"><i>L</i>=<i>L</i><sub><i>s</i></sub>+<i>λL</i><sub><i>p</i></sub>+<i>μL</i><sub><i>g</i></sub>      (15) </p>
                </div>
                <div class="p1">
                    <p id="86">式中:<i>L</i>为联合损失函数, <i>λ</i>为感知参数, <i>μ</i>为全局参数, <i>L</i><sub><i>s</i></sub>为加权语义损失, <i>L</i><sub><i>p</i></sub>为感知损失, <i>L</i><sub><i>g</i></sub> 为全局损失。</p>
                </div>
                <div class="p1">
                    <p id="87">在设计判别模型结构时, 若判别模型过于简单, 导致网络对输入图像的特征学习不到位, 容易产生欠拟合问题, 使得生成的裂缝图像呈明显的网格化分布;若判别模型过于复杂, 网络训练难度大大增加, 会产生模型不收敛等问题。文献<citation id="160" type="reference">[<a class="sup">6</a>]</citation>中的模型的判别器由五个卷积块构成, 其中每个卷积块由卷积层 (Conv) 、激活层 (Leaky Relu) 、归一化层 (Batch Normalization) 按序排列构成。此结构的判别器使用在修复图像中虽然可以达到相对较好的效果, 但是由于模型层数浅, 特征学习不到位, 网络生成的裂缝图像含有较为明显的网格型噪声, 视觉效果差, 无法直接应用于裂缝检测。</p>
                </div>
                <div class="p1">
                    <p id="88">针对上述问题, 本文在保证不丢失特征、不改变输入输出尺寸的前提下, 对判别器模型进行了改进, 增加了卷积核大小为3×3, 步长为1的卷积层, 同时为了进一步筛选无效特征加入激活层, 并且为了加速模型收敛防止过拟合加入归一化层。CI-GAN判别器的具体结构如图3所示, 其中模型输入包含两个部分, G (z) 表示生成器生成的裂缝图像, X表示真实的裂缝图像, 模型的输出为[0, 1]区间的概率, 表示输入图像的真实程度或者虚假程度。判别器将输出的概率值回传给生成器, 为生成器的更新提供梯度信息。</p>
                </div>
                <div class="area_img" id="89">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201906039_089.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 判别器结构" src="Detail/GetImg?filename=images/JYRJ201906039_089.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 判别器结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201906039_089.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="90" name="90"><b>2.3.2 生成器结构</b></h4>
                <div class="p1">
                    <p id="91">生成器使用接收到的梯度信息更新模型参数和生成向量的分布。生成器作为生成裂缝图像的单元, 不仅决定了生成的裂缝图像的质量, 还影响着网络的修复性能。本方法生成器由反卷积层 (Deconv) 、激活层 (Relu) 、归一化层 (Batch Normalization) 构成。考虑到模型的学习能力随结构加深而增强, 本文加入了反卷积块 (Deconv Block) 结构, 其由四个小块顺序拼接组成。每个小块包含一层卷积核大小为3×3, 步长为2的反卷积层, 一层激活层和一层归一化层。为了增加模型的深度, 本文在第一小块和第三小块中增加了一层卷积核大小为3×3, 步长为1的卷积层。同时为了近一步过滤掉无用特征并且加速模型收敛, 在每个卷积层后面都加入了激活层和归一化层, CI-GAN中生成模型的具体结构如图4所示, 左边为生成模型的结构简图, 右边对应反卷积块的结构图。</p>
                </div>
                <div class="area_img" id="92">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201906039_092.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 生成网络结构对比图" src="Detail/GetImg?filename=images/JYRJ201906039_092.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 生成网络结构对比图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201906039_092.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h3 id="93" name="93" class="anchor-tag"><b>3 实验结果与分析</b></h3>
                <h4 class="anchor-tag" id="94" name="94"><b>3.1 数据集与裂缝修复生成式对抗网络的训练</b></h4>
                <div class="p1">
                    <p id="95">本文实拍的10 350张128×128大小的RGB彩色裂缝图像作为原始裂缝数据集。通过随机旋转、左右翻转对原始裂缝数据集进行增强, 最终得到31 050张裂缝图像, 其中31 000张作为CI-GAN的训练数据集。为测试本文算法的准确性和有效性, 另增加50张包含落叶遮挡的裂缝图像、50张包含石子覆盖的裂缝图像, 将增加含有障碍物的图像与剩余的50张裂缝图像整合, 共150张图像作为本实验测试集。</p>
                </div>
                <div class="p1">
                    <p id="96">本文使用adam梯度下降方式寻求最优解, 初始学习率为0.000 5, 学习率衰减为0.1, 衰减步长为50 000, 批尺寸为128, 动量项取值0.5, 迭代100 000次后将训练结果保存。</p>
                </div>
                <h4 class="anchor-tag" id="97" name="97"><b>3.2 可视化比较</b></h4>
                <h4 class="anchor-tag" id="98" name="98"><b>3.2.1 CI-GAN修复效果对比</b></h4>
                <div class="p1">
                    <p id="99">通过实验, 改进后模型对擦除障碍物的待修复裂缝图像的修复效果得到了明显的改善。改进效果对比如图5所示, 由左列至右列分别为原始图片、障碍物擦除后待修复图片、CI-GAN结合二进制掩膜修复图片以及CI-GAN结合距离加权掩膜修复图片。改进前后模型的修复效果使用模糊系数 (KBlur) 、峰值信噪比 (PSNR) 、结构相似度 (SSIM) 、均方误差 (MSE) 对修复结果进行评估, 对比如表1所示。可以看出本算法从实际观测效果和图片度量指标对比结果均优于改进前模型。</p>
                </div>
                <div class="area_img" id="100">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201906039_100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 掩膜修复效果对比图" src="Detail/GetImg?filename=images/JYRJ201906039_100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 掩膜修复效果对比图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201906039_100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="101">
                    <p class="img_tit"><b>表1 不同掩膜修复效果对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="101" border="1"><tr><td>算法名称</td><td>KBlur</td><td>PSNR</td><td>SSIM</td><td>MSE</td></tr><tr><td><br />本文网络</td><td>0.996</td><td>26.437</td><td>0.821</td><td>147.684</td></tr><tr><td><br />本文网络+距离掩膜</td><td>0.998</td><td>27.971</td><td>0.872</td><td>103.750</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="102">为了验证CI-GAN判别器较传统模型判别器在裂缝修复方面更具优势, 本实验通过对比CI-GAN使用普通判别器以及差异判别器对裂缝图像修复效果, 结果如图6所示, 由左列至右列分别为结构相似的裂缝图像原图、CI-GAN使用传统判别器的修复效果、CI-GAN使用差异判别器的修复效果。同时本文对两种判别器修复的裂缝图进行质量评估, 评估结果如表2所示。从表中能够得出, CI-GAN差异判别器在修复相似的裂缝图像时, 可以得到比传统判别器更稳定的修复效果。</p>
                </div>
                <div class="area_img" id="103">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201906039_103.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 判别器修复效果对比图" src="Detail/GetImg?filename=images/JYRJ201906039_103.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 判别器修复效果对比图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201906039_103.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="104">
                    <p class="img_tit"><b>表2 不同判别器的修复效果对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="104" border="1"><tr><td>算法名称</td><td>KBlur</td><td>PSNR</td><td>SSIM</td><td>MSE</td></tr><tr><td><br />本文网络+传统判别器</td><td>0.996</td><td>26.364</td><td>0.832</td><td>150.198</td></tr><tr><td><br />本文网络+差异判别器</td><td>0.998</td><td>27.971</td><td>0.872</td><td>103.750</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="105">为了验证CI-GAN结构较改进前DCGAN结构更适用于对裂缝图像的修复, 本实验又通过对比CI-GAN和DCGAN对裂缝图像修复效果, 结果如图7所示, 由左列至右列分别为裂缝原图像、DCGAN的修复图像、CI-GAN的修复图像。同时本文对两种判别器修复的裂缝图进行质量评估, 评估结果如表3所示。从表中能够得出, CI-GAN的修复效果更好。</p>
                </div>
                <div class="area_img" id="106">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201906039_106.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 模型结构修复效果对比图" src="Detail/GetImg?filename=images/JYRJ201906039_106.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 模型结构修复效果对比图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201906039_106.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="107">
                    <p class="img_tit"><b>表3 不同模型结构的修复效果对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="107" border="1"><tr><td>算法名称</td><td>KBlur</td><td>PSNR</td><td>SSIM</td><td>MSE</td></tr><tr><td><br />DCGAN</td><td>0.995</td><td>25.569</td><td>0.771</td><td>180.393</td></tr><tr><td><br />CI-GAN</td><td>0.998</td><td>27.971</td><td>0.872</td><td>103.750</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="108" name="108"><b>3.2.2 不同算法效果对比</b></h4>
                <div class="p1">
                    <p id="109">本算法与传统修复算法基于裂缝图像数据集进行对比试验, 本文提出的CI-GAN在修复残缺裂缝图像时表现更佳。图8分别对三张裂缝图像进行信息擦除, 并使用五种修复方法进行待修复图像的修复, 由左列至右列分别为实拍裂缝图像、信息擦除后的待修复图像、TV算法的修复结果图像、CDD算法的修复结果图像、Criminisi算法的修复结果图像、DCGAN的修复结果图像以及CI-GAN的修复结果图像。本文使用模糊系数、质量系数Q以及峰值信噪比对修复后的图片质量进行苹果, 评估结果如表4所示, 可以看出本文算法从肉眼观测效果和图片质量对比结果均优于其余四种算法。</p>
                </div>
                <div class="area_img" id="110">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201906039_110.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 修复算法修复效果对比图" src="Detail/GetImg?filename=images/JYRJ201906039_110.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图8 修复算法修复效果对比图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201906039_110.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="111">
                    <p class="img_tit"><b>表4 不同修复算法效果对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="111" border="1"><tr><td>修复方式</td><td>KBlur</td><td>质量系数Q</td><td>PSNR</td></tr><tr><td><br />TV</td><td>1.090</td><td>0.007</td><td>10.175</td></tr><tr><td><br />CDD</td><td>0.700</td><td>0.706</td><td>25.149</td></tr><tr><td><br />Criminisi</td><td>0.708</td><td>0.704</td><td>25.100</td></tr><tr><td><br />DCGAN</td><td>0.995</td><td>0.735</td><td>25.569</td></tr><tr><td><br />CI-GAN</td><td>0.998</td><td>0.744 3</td><td>27.971</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="112" name="112"><b>3.2.3 不同障碍物裂缝修复效果图</b></h4>
                <div class="p1">
                    <p id="113">针对含有不同类型障碍物的裂缝图像, 本文使用CI-GAN分别对其进行修复。其中CI-GAN对落叶修复效果如图9所示, 由左列至右列分别为实拍含落叶遮挡的裂缝图像、信息擦除后的待修复图像、迭代100次的修复效果图像、迭代1 000的修复效果图以及最终的修复效果图像。</p>
                </div>
                <div class="area_img" id="114">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201906039_114.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图9 落叶修复效果图" src="Detail/GetImg?filename=images/JYRJ201906039_114.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图9 落叶修复效果图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201906039_114.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="115">CI-GAN对石子修复效果如图10所示, 由左列至右列分别为实拍含石子遮挡的裂缝图像、信息擦除后的待修复图像、迭代100次的修复效果图像、迭代1 000的修复效果图以及最终的修复效果图像。</p>
                </div>
                <div class="area_img" id="116">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201906039_116.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图10 石子修复效果图" src="Detail/GetImg?filename=images/JYRJ201906039_116.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图10 石子修复效果图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201906039_116.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="117">CI-GAN对裂缝修复效果如图11所示, 由左列至右列分别为实拍裂缝图像、信息擦除后的待修复图像、迭代100次的修复效果图像、迭代1 000的修复效果图以及最终的修复效果图像。</p>
                </div>
                <div class="area_img" id="118">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201906039_118.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图11 CI-GAN裂缝修复过程图" src="Detail/GetImg?filename=images/JYRJ201906039_118.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图11 CI-GAN裂缝修复过程图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201906039_118.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h3 id="119" name="119" class="anchor-tag"><b>4 结 语</b></h3>
                <div class="p1">
                    <p id="120">本文将生成式对抗网络突破性地运用于裂缝图像修复领域, 由于裂缝图像在实际中含有较多噪声、干扰以及障碍物, 本文设计生成式对抗网络的裂缝修复模型 (CI-GAN) , 提出距离加权掩膜, 创新性地改进生成式对抗网络结构, 将传统判别器替换为差异程度判别器以更好应对含有杂乱遮挡的裂缝图像。通过实验, 本方法彻底去除各类障碍物并得到与多种图像修复方法相比更好的修复效果, 提高了裂缝图像修复的准确性与裂缝目标的完整性。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="130">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image inpainting">

                                <b>[1]</b>Bertalmio M, Sapiro G, Caselles V, et al.Image Inpainting[C]//Proceedings of the 27th annual conference on Computer graphics and interactive techniques.ACM, 2000:417-424.
                            </a>
                        </p>
                        <p id="132">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Region filling and object removal by exemplar-based image inpainting">

                                <b>[2]</b>Criminisi A, Perez P, Toyama K.Region filling and object removal by exemplar-based image inpainting[J].IEEETransactions on Image Processing, 2004, 13 (9) .1200-1212.
                            </a>
                        </p>
                        <p id="134">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Simultaneous structure and texture image inpainting">

                                <b>[3]</b>Bertalmio M, Vese L, Sapiro G, et al.Simultaneous Structure and Texture Image Inpainting[J].IEEE Transactions on Image Processing, 2003, 12 (8) :882-889.
                            </a>
                        </p>
                        <p id="136">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJST&amp;filename=SJST14120403043018&amp;v=Mjk2OTZaZVp0RmlubFVyeklJMTBUYVJRPU5pZlllcks4SDlQTXE0OUdaTzhNREgweG9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1udw==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b>Shen J, Chan T F.Mathematical Models for Local Nontexture Inpaintings[J].SIAM Journal on Applied Mathematics, 2002, 62:1019-1043.
                            </a>
                        </p>
                        <p id="138">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501264158&amp;v=MjIyOTlUYVJRPU5pZk9mYks3SHRETnFvOUVadTBMRFhreG9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaW5sVXJ6SUkxMA==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b>Chan T F, Shen J.Non-texture inpainting by curvature-driven diffusions (CDD) [J].Journal of Visual Communication and Image Representation, 2001, 12 (4) :436-449.
                            </a>
                        </p>
                        <p id="140">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Semantic Image Inpainting with Perceptual and Contextual Losses">

                                <b>[6]</b>Yeh R, Chen C, Lim T Y, et al.Semantic Image Inpainting with Perceptual and Contextual Losses[C]//The IEEE Computer Society Conference on Computer Vision and Pattern Recognition, Los Alamitos, July 7-26, 2016.
                            </a>
                        </p>
                        <p id="142">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Context Encoders:Feature Learning by Inpainting">

                                <b>[7]</b>Pathak D, Krahenbuhl P, Donahue J, et al.Context Encoders:Feature Learning by Inpainting[C]//Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition.Los Alamitos.IEEE Computer Society Press, 2016:2536-2544.
                            </a>
                        </p>
                        <p id="144">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM408B7C15F867A55233C65423FF57BC91&amp;v=MDM4OTZUWHZncjJSRGZMWG1Ock9lQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQnJMVTA1dHRoeDd1NHc2ND1OaWZJWTdlNEZxUEwzSTVBRXVNSkN3MDh5aFFRNlV4Nw==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b>Iizuka S, Simo-Serra E, Ishikawa H.Globally and locally consistent image completion[J].ACM Transactions on Graphics, 2017, 36 (4) :1-14.
                            </a>
                        </p>
                        <p id="146">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Generative Face Completion">

                                <b>[9]</b>Li Y J, Liu S F.Generative Face Completion[C]//Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition.Los Alamitos.IEEE Computer Society Press, 2017:3911-3191.
                            </a>
                        </p>
                        <p id="148">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=High-Resolution Image Inpainting Using Multi-scale Neural Patch Synthesis">

                                <b>[10]</b>Yang C, Lu X, Lin Z, et al.High-Resolution Image Inpainting using Multi-Scale Neural Patch Synthesis[C]//Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition.Los Alamitos.IEEE Computer Society Press, 2017:6721-6729.
                            </a>
                        </p>
                        <p id="150">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Faster R-CNN:Towards Real-Time Object Detection with Region Proposal Networks">

                                <b>[11]</b>Ren S Q, He K M, Girshick R, et al.Faster R-CNN:Towards Real-Time Object Detection with Region Proposal Networks[C]//Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition.Los Alamitos.IEEE Computer Society Press, 2015:1137-1149.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JYRJ201906039" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201906039&amp;v=MTkwNzdMek9MelRaWkxHNEg5ak1xWTlHYllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVp0Rnl2bVY=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ28xeDVteFZmK0p3enVSVkNoMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="1" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
