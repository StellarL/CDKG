<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637135622449221250%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJYRJ201907013%26RESULT%3d1%26SIGN%3d2%252f2n%252bLgvJaYWTRRti9bL2H7nUlY%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JYRJ201907013&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JYRJ201907013&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201907013&amp;v=MTQ5MzJGeWpnVmJ2S0x6VFpaTEc0SDlqTXFJOUVaNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnQ=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#37" data-title="&lt;b&gt;0 引 言&lt;/b&gt; "><b>0 引 言</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#40" data-title="&lt;b&gt;1 GFCC特征及PCA&lt;/b&gt; "><b>1 GFCC特征及PCA</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#41" data-title="&lt;b&gt;1.1 GFCC特征及提取方法&lt;/b&gt;"><b>1.1 GFCC特征及提取方法</b></a></li>
                                                <li><a href="#54" data-title="&lt;b&gt;1.2 PCA&lt;/b&gt;"><b>1.2 PCA</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#56" data-title="&lt;b&gt;2 法庭自动说话人识别系统&lt;/b&gt; "><b>2 法庭自动说话人识别系统</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#57" data-title="&lt;b&gt;2.1 基于似然比的法庭自动说话人识别系统&lt;/b&gt;"><b>2.1 基于似然比的法庭自动说话人识别系统</b></a></li>
                                                <li><a href="#65" data-title="&lt;b&gt;2.2 法庭说话人识别系统性能可靠性评估&lt;/b&gt;"><b>2.2 法庭说话人识别系统性能可靠性评估</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#70" data-title="&lt;b&gt;3 结果与讨论&lt;/b&gt; "><b>3 结果与讨论</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#71" data-title="&lt;b&gt;3.1 数据库设置&lt;/b&gt;"><b>3.1 数据库设置</b></a></li>
                                                <li><a href="#77" data-title="&lt;b&gt;3.2 纯净数据库测试结果&lt;/b&gt;"><b>3.2 纯净数据库测试结果</b></a></li>
                                                <li><a href="#82" data-title="&lt;b&gt;3.3 噪数据库测试结果&lt;/b&gt;"><b>3.3 噪数据库测试结果</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#86" data-title="&lt;b&gt;4 结 语&lt;/b&gt; "><b>4 结 语</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#62" data-title="图1 法庭自动说话人识别系统流程图">图1 法庭自动说话人识别系统流程图</a></li>
                                                <li><a href="#64" data-title="图2 说话人模型自适应流程图">图2 说话人模型自适应流程图</a></li>
                                                <li><a href="#74" data-title="图3 真实案件录音中噪声声谱图">图3 真实案件录音中噪声声谱图</a></li>
                                                <li><a href="#79" data-title="图4 参考数据库校正前Tippett图">图4 参考数据库校正前Tippett图</a></li>
                                                <li><a href="#80" data-title="图5 参考数据库校正后Tippett图">图5 参考数据库校正后Tippett图</a></li>
                                                <li><a href="#81" data-title="图6 测试数据库Tippett图">图6 测试数据库Tippett图</a></li>
                                                <li><a href="#84" data-title="&lt;b&gt;表1 不同信噪比条件下识别系统的Cllr值与错误识别率&lt;/b&gt;"><b>表1 不同信噪比条件下识别系统的Cllr值与错误识别率</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" title=" Enzinger E, Morrison G S, Ochoa F.A demonstration of the application of the new paradigm for the evaluation of forensic evidence under conditions reflecting those of a real forensic voice comparison case[J].Science &amp;amp; Justice, 2016, 56 (1) :42-57." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES234FB8BAF9352ADDC6921E5B74750D18&amp;v=MjgxMTJGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkJyTFUwNXR0aHhMMjV4SzA9TmlmT2ZiRzdHcWUrcC8wMEV1SU1DWDVJdTJKZzdEWi9TUXJuM2hVeGZyZVVNYnVYQ09Odg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         Enzinger E, Morrison G S, Ochoa F.A demonstration of the application of the new paradigm for the evaluation of forensic evidence under conditions reflecting those of a real forensic voice comparison case[J].Science &amp;amp; Justice, 2016, 56 (1) :42-57.
                                    </a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" title=" Rose P.Where the science ends and the law begins:likelihood ratio-based forensic voice comparison in a 150 million telephone fraud[J].International Journal of Speech Language and the Law, 2013, 20 (2) :277-324." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Where the science ends and the law begins:likelihood ratio-based forensic voice comparison in a$150 million telephone fraud">
                                        <b>[2]</b>
                                         Rose P.Where the science ends and the law begins:likelihood ratio-based forensic voice comparison in a 150 million telephone fraud[J].International Journal of Speech Language and the Law, 2013, 20 (2) :277-324.
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title=" Morrison G S.Distinguishing between forensic science and forensic pseudoscience:testing of validity and reliability, and approaches to forensic voice comparison[J].Science &amp;amp; Justice, 2014, 54 (3) :245-256." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14010600476630&amp;v=MzA2OTVUNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp0RmlubFVyeklJRnNTYmhjPU5pZk9mYks4SHRETXFZOUZZT3dKQ244NW9CTQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         Morrison G S.Distinguishing between forensic science and forensic pseudoscience:testing of validity and reliability, and approaches to forensic voice comparison[J].Science &amp;amp; Justice, 2014, 54 (3) :245-256.
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" title=" Rose P, Morrison G S.A response to the UK Position Statement on forensic speaker comparison[J].International Journal of Speech Language and the Law, 2009, 16 (1) :139-63." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A response to the UK position statement on forensic speaker comparison">
                                        <b>[4]</b>
                                         Rose P, Morrison G S.A response to the UK Position Statement on forensic speaker comparison[J].International Journal of Speech Language and the Law, 2009, 16 (1) :139-63.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" title=" 王华朋, 许锋.论法庭证据评估体系的发展[J].证据科学, 2014, 22 (1) :56-63." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=FLYZ201401006&amp;v=MTMxNjlqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVadEZ5amdWYnZOSXlIU2RMRzRIOVhNcm85RllvUUtESDg0dlI0VDY=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                         王华朋, 许锋.论法庭证据评估体系的发展[J].证据科学, 2014, 22 (1) :56-63.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" title=" Rose P.Likelihood ratio-based forensic voice comparison with higher level features:research and reality[J].Computer Speech &amp;amp; Language, 2017 (45) :475-502." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES1A67A45AEA0E88AE5BDA02CB53DA888E&amp;v=MjM4MzFPZmJMSkdOYTlxNG8wRVpvUGVYUXh2bU1XbUVzTVNIMlIzaGMyRGNPY1RiTHFDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZCckxVMDV0dGh4TDI1eEswPU5pZg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                         Rose P.Likelihood ratio-based forensic voice comparison with higher level features:research and reality[J].Computer Speech &amp;amp; Language, 2017 (45) :475-502.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" title=" Morrison G S.Special issue on measuring and reporting the precision of forensic likelihood ratios:Introduction to the debate[J].Science &amp;amp; Justice, 2016, 56 (5) :371-373." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Special issue on measuring and reporting the precision of forensic likelihood ratios:Introduction to the debate">
                                        <b>[7]</b>
                                         Morrison G S.Special issue on measuring and reporting the precision of forensic likelihood ratios:Introduction to the debate[J].Science &amp;amp; Justice, 2016, 56 (5) :371-373.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" title=" Lei Y, Scheffer N, Ferrer L, et al.A novel scheme for speaker recognition using a phonetically-aware deep neural network[C]//2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) .IEEE, 2014:1695-1699." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A novel scheme for speaker recognition using a phonetically-aware deep neural network">
                                        <b>[8]</b>
                                         Lei Y, Scheffer N, Ferrer L, et al.A novel scheme for speaker recognition using a phonetically-aware deep neural network[C]//2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) .IEEE, 2014:1695-1699.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" title=" Hasan T, Saeidi R, Hansen J H L, et al.Duration mismatch compensation for i-vector based speaker recognition systems[C]//2013 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) .IEEE, 2013:7663-7667." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Duration mismatch compensation for i-vector based speaker recognition systems">
                                        <b>[9]</b>
                                         Hasan T, Saeidi R, Hansen J H L, et al.Duration mismatch compensation for i-vector based speaker recognition systems[C]//2013 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) .IEEE, 2013:7663-7667.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_10" title=" Nidhyananthan S S, Kumari R S S, Selvi T S.Noise Robust Speaker Identification Using RASTA—MFCC Feature with Quadrilateral Filter Bank Structure[J].Wireless Personal Communications, 2016, 91 (3) :1321-1333." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Noise Robust Speaker Identification Using RASTA—MFCC Feature with Quadrilateral Filter Bank Structure">
                                        <b>[10]</b>
                                         Nidhyananthan S S, Kumari R S S, Selvi T S.Noise Robust Speaker Identification Using RASTA—MFCC Feature with Quadrilateral Filter Bank Structure[J].Wireless Personal Communications, 2016, 91 (3) :1321-1333.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_11" title=" 陈世雄, 宫琴, 金慧君.用Gammatone滤波器组仿真人耳基底膜的特性[J].清华大学学报 (自然科学版) , 2008, 48 (6) :1044-1048." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=QHXB200806035&amp;v=MjAwNDBOQ1hUYkxHNEh0bk1xWTlHWVlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVp0RnlqZ1Zidk4=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[11]</b>
                                         陈世雄, 宫琴, 金慧君.用Gammatone滤波器组仿真人耳基底膜的特性[J].清华大学学报 (自然科学版) , 2008, 48 (6) :1044-1048.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_12" title=" 王华朋.基于听觉模型的法庭语音证据特征量化[J].中国刑警学院学报, 2018 (1) :119-122." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XING201801023&amp;v=MjY0ODg2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnRGeWpnVmJ2TlBTVEZhYkc0SDluTXJvOUhaNFFLREg4NHZSNFQ=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[12]</b>
                                         王华朋.基于听觉模型的法庭语音证据特征量化[J].中国刑警学院学报, 2018 (1) :119-122.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_13" title=" 茅正冲, 王正创, 黄芳.基于GFCC与RLS的说话人识别抗噪系统研究[J].计算机工程与应用, 2015, 51 (10) :215-218." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSGG201510045&amp;v=Mjg2ODViRzRIOVROcjQ5QllZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVadEZ5amdWYnZOTHo3TWE=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[13]</b>
                                         茅正冲, 王正创, 黄芳.基于GFCC与RLS的说话人识别抗噪系统研究[J].计算机工程与应用, 2015, 51 (10) :215-218.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_14" title=" Garciaromero D, Mccree A.Supervised domain adaptation for I-vector based speaker recognition[C]//2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) .IEEE, 2014:4047-4051." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Supervised domain adaptation for I-vector based speaker recognition">
                                        <b>[14]</b>
                                         Garciaromero D, Mccree A.Supervised domain adaptation for I-vector based speaker recognition[C]//2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) .IEEE, 2014:4047-4051.
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_15" title=" 熊冰峰, 曾以成, 谢小娟.一种改进的听觉特征参数应用于说话人识别[J].计算机应用, 2016, 36 (S1) :82-85." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY2016S1021&amp;v=MTkwOTBldnJvOUhaWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnRGeWpnVmJ2Tkx6N0JkN0c0SDk=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[15]</b>
                                         熊冰峰, 曾以成, 谢小娟.一种改进的听觉特征参数应用于说话人识别[J].计算机应用, 2016, 36 (S1) :82-85.
                                    </a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_16" title=" 王晓兰, 王蓓, 顾为一.基于稀疏核主成分分析的语音情感识别研究[J].信息化研究, 2014, 40 (1) :36-39." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZGS201401009&amp;v=MTk0OTdidk5JVGZNZmJHNEg5WE1ybzlGYllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVp0RnlqZ1Y=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[16]</b>
                                         王晓兰, 王蓓, 顾为一.基于稀疏核主成分分析的语音情感识别研究[J].信息化研究, 2014, 40 (1) :36-39.
                                    </a>
                                </li>
                                <li id="35">


                                    <a id="bibliography_17" title=" Gauvain J L, Lee C H.Maximum a posteriori estimation for multivariate Gaussian mixture observations of Markov chains[J].IEEE Transactions on Speech and Audio Processing, 1994, 2 (2) :291-298." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Maximum a Posteriori Estimation for Multivariate Gaussian Mixture Observations of Markov Chains">
                                        <b>[17]</b>
                                         Gauvain J L, Lee C H.Maximum a posteriori estimation for multivariate Gaussian mixture observations of Markov chains[J].IEEE Transactions on Speech and Audio Processing, 1994, 2 (2) :291-298.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JYRJ" target="_blank">计算机应用与软件</a>
                2019,36(07),65-68+98 DOI:10.3969/j.issn.1000-386x.2019.07.012            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>噪声环境下法庭语音证据量化评价方法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E5%8D%8E%E6%9C%8B&amp;code=25271380&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王华朋</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%A7%9C%E5%9B%A1&amp;code=35884607&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">姜囡</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%99%81%E4%BA%9A%E4%B8%9C&amp;code=40580868&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">晁亚东</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%88%98%E6%81%A9&amp;code=40580867&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">刘恩</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E5%88%91%E4%BA%8B%E8%AD%A6%E5%AF%9F%E5%AD%A6%E9%99%A2%E5%A3%B0%E5%83%8F%E8%B5%84%E6%96%99%E6%A3%80%E9%AA%8C%E6%8A%80%E6%9C%AF%E7%B3%BB&amp;code=0135604&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国刑事警察学院声像资料检验技术系</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>针对说话人自动识别系统的性能与稳定性在高噪声环境下会严重下降, 人耳却能捕捉高噪声环境中的目标语音的问题。提出使用能模拟耳蜗听觉特性的GFCC (Gammatone Frequency Cepstral Coefficient) 特征与主成分分析 (Principal Component Analysis, PCA) 相结合的方法, 以提高识别系统的鲁棒性。在不同程度信噪比的真实语音案件噪声条件下, 对国际上最认可的基于似然比证据评估体系的法庭自动说话人识别系统的准确性和稳定性进行测试。实验结果显示:GFCC特征在多个程度的信噪比条件下, 甚至信噪比为-20 dB的条件下, 依然能保持较高的识别准确度和良好的稳定性, 并能够提供可量化、可重复的证据强度值。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=GFCC&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">GFCC;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%BC%BC%E7%84%B6%E6%AF%94&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">似然比;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%AF%81%E6%8D%AE%E5%BC%BA%E5%BA%A6&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">证据强度;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%A7%91%E5%AD%A6%E8%AF%81%E6%8D%AE&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">科学证据;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=PCA&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">PCA;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    王华朋, 副教授, 主研领域:法庭说话人识别, 证据强度量化。;
                                </span>
                                <span>
                                    姜囡, 副教授。;
                                </span>
                                <span>
                                    晁亚东, 硕士生。;
                                </span>
                                <span>
                                    刘恩, 硕士生。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-09-23</p>

                    <p>

                            <b>基金：</b>
                                                        <span>2016国家社会科学基金重点项目 (16AYY015);</span>
                                <span>辽宁省重点研发计划项目 (2017231006, 2017231004);</span>
                                <span>公安部公安理论及软科学项目 (2017LLYJXJXY040);</span>
                    </p>
            </div>
                    <h1><b>QUANTITATIVE EVALUATION METHOD OF COURT VOICE EVIDENCE IN NOISE ENVIRONMENT</b></h1>
                    <h2>
                    <span>Wang Huapeng</span>
                    <span>Jiang Nan</span>
                    <span>Chao Yadong</span>
                    <span>Liu En</span>
            </h2>
                    <h2>
                    <span>Department of Video and Audio Materior Examination, Criminal Investigation Police University of China</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>The performance and stability of speaker automatic recognition system are seriously degraded in high noise environment, but the human ear can capture target speech. In order to improve the robustness of the recognition system, we proposed a combination of gammatone frequency cepstral coefficients (GFCC) features and principal component analysis (PCA) , which could simulate the cochlear auditory characteristics. The accuracy and stability of the most internationally recognized court automatic speaker recognition system based on likelihood ratio evidences evaluation system were tested under the conditions of different SNR of real voice cases. The experimental results show that the GFCC features can maintain high recognition accuracy and good stability, and can provide quantifiable and repeatable evidence strength values under various SNR conditions, even under the condition of SNR-20 dB.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=GFCC&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">GFCC;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Likelihood%20ratio&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Likelihood ratio;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Evidence%20strength&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Evidence strength;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Scientific%20evidence&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Scientific evidence;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=PCA&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">PCA;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-09-23</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="37" name="37" class="anchor-tag"><b>0 引 言</b></h3>
                <div class="p1">
                    <p id="38">鉴定结论是指鉴定人对诉讼中涉及的专门性问题运用科学技术或专门知识进行鉴定和判断的基础上给出的综合结果。十届全国人大常委会第十四次会议通过的《关于司法鉴定管理问题的决定》中提出把“鉴定结论”改为“鉴定意见”, 更改的初衷是因为鉴定结果只是鉴定人基于个人认知能力的判断, 对于整个案件而言, 这些意见只是诸多证据中的一种证据, 所以用“鉴定意见”来描述更为恰当, 有利于正确说明这类证据在诉讼中的作用。这虽然是对证据效力思考后的一次巨大进步, 但是, 这并不意味着更改一下描述方式, 证据就具有了科学性, 越来越多的学者开始关注证据的科学性<citation id="89" type="reference"><link href="3" rel="bibliography" /><link href="5" rel="bibliography" /><link href="7" rel="bibliography" /><link href="9" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>,<a class="sup">3</a>,<a class="sup">4</a>]</sup></citation>。科学证据的基本要求包括:检验方法和检验过程都是科学的, 结果是客观的、可重复的、证据强度是可量化的<citation id="88" type="reference"><link href="11" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>。法庭说话人识别最主要的任务就是比对罪犯和嫌疑人的语音样本, 提取有效稳定语音特征, 利用这些特征加以识别或确认。随着语音证据出现率越来越高, 国内外对语音证据评估方法有了新的发展, 即采用基于似然比的证据评估方法<citation id="90" type="reference"><link href="3" rel="bibliography" /><link href="13" rel="bibliography" /><link href="15" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">6</a>,<a class="sup">7</a>]</sup></citation>。DNA检验率先使用该方法, 目前正被推广到其他的法庭证据领域。该方法可以量化评估证据对鉴定结论支持程度的大小, 是逻辑上和法律上都正确的法庭证据评估方法, 在国内外获得了广泛的认同。</p>
                </div>
                <div class="p1">
                    <p id="39">人类听觉系统是高度复杂并且高度精密的生理系统, 具有很强的声音识别能力和抗噪声能力<citation id="91" type="reference"><link href="17" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>, 能在复杂环境中捕捉目标语音。它的声音分析能力大幅度超过了自动说话人识别系统。在含有噪声的环境下, 人类听觉系统识别的准确率比任何自动识别系统更具有可靠性<citation id="92" type="reference"><link href="19" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>。基底膜是耳蜗中能够辨别分析声音最重要的部位<citation id="93" type="reference"><link href="21" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>, 频率不同的声音导致基底膜以不同的形式振动, 越接近基底膜顶部, 共振频率越低, 越接近基底膜底部, 共振频率越高。因此基底膜能够将不同频率的声音对应到基底膜对应的位置, 以基底膜振幅的幅度表示频率的强度<citation id="94" type="reference"><link href="23" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>, Gammatone滤波器能够模拟耳蜗基底膜的分析不同声音的特性<citation id="95" type="reference"><link href="25" rel="bibliography" /><link href="27" rel="bibliography" /><sup>[<a class="sup">12</a>,<a class="sup">13</a>]</sup></citation>。本文选择GFCC作为自动识别系统的输入参数, 在似然比证据强度评估体系下, 对语音中说话人身份的自动识别方法进行了研究, 为了提高计算效率和降低噪声的干扰, 使用主成分分析技术对GFCC特征进行了降维。</p>
                </div>
                <h3 id="40" name="40" class="anchor-tag"><b>1 GFCC特征及PCA</b></h3>
                <h4 class="anchor-tag" id="41" name="41"><b>1.1 GFCC特征及提取方法</b></h4>
                <div class="p1">
                    <p id="42">GFCC特征主要通过Gammatone滤波器来实现, 该滤波器能够模拟类似耳蜗的听觉模型其时域形式为:</p>
                </div>
                <div class="p1">
                    <p id="43"><i>g</i><sub><i>i</i></sub> (<i>f</i>, <i>t</i>) =<i>kt</i><sup><i>n</i>-1</sup>e<sup>-2π<i>b</i><sub><i>i</i></sub><i>t</i></sup>cos (2π<i>f</i><sub><i>i</i></sub>+ϕ) <i>t</i>≥0      (1) </p>
                </div>
                <div class="p1">
                    <p id="44">式中:ϕ为相位, 相位对人耳听觉影响较小, 常取ϕ=0, <i>f</i>为各子滤波器的中心频率, <i>n</i>表示滤波器阶数, 根据对耳蜗声学特性的测试结果, 4阶Gammatone滤波器与人耳听觉特性十分相似<citation id="96" type="reference"><link href="29" rel="bibliography" /><link href="31" rel="bibliography" /><sup>[<a class="sup">14</a>,<a class="sup">15</a>]</sup></citation>, 本文中<i>n</i>取值为4, <i>i</i>表示子滤波器编号, <i>b</i><sub><i>i</i></sub>、<i>f</i><sub><i>i</i></sub>为第<i>i</i>个子滤波器的带宽和中心频率, <i>k</i>为增益。</p>
                </div>
                <div class="p1">
                    <p id="45">4阶的Gammatone滤波器各频带宽度表示为:</p>
                </div>
                <div class="p1">
                    <p id="46"><mathml id="47"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>b</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mn>1</mn><mo>.</mo><mn>1</mn><mn>0</mn><mn>9</mn><mo>×</mo><mn>2</mn><mn>4</mn><mo>.</mo><mn>7</mn><mrow><mo> (</mo><mrow><mfrac><mrow><mn>4</mn><mo>.</mo><mn>3</mn><mn>7</mn><mi>f</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mrow><mn>1</mn><mspace width="0.25em" /><mn>0</mn><mn>0</mn><mn>0</mn></mrow></mfrac><mo>+</mo><mn>1</mn></mrow><mo>) </mo></mrow></mrow></math></mathml>      (2) </p>
                </div>
                <div class="p1">
                    <p id="48">Gammatone滤波器有<i>N</i>个 (一般取<i>N</i>=64) 单独滤波器组成, 整个滤波器的带宽一般从50 Hz到采样频率的二分之一。</p>
                </div>
                <div class="p1">
                    <p id="49">提取GFCC特征的步骤如下:</p>
                </div>
                <div class="p1">
                    <p id="50"> (1) 语音信号通过64通道Gammatone滤波器组。</p>
                </div>
                <div class="p1">
                    <p id="51"> (2) 对各通道的滤波响应取绝对值。</p>
                </div>
                <div class="p1">
                    <p id="52"> (3) 对上述滤波响应的绝对值取对数。</p>
                </div>
                <div class="p1">
                    <p id="53"> (4) 对上述对数结果进行离散余弦变换, 以减少各维特征之间的相关性。</p>
                </div>
                <h4 class="anchor-tag" id="54" name="54"><b>1.2 PCA</b></h4>
                <div class="p1">
                    <p id="55">主成分分析是数学上对数据分析主要成分的方法, 可降低特征的维度, 也能降低计算量、减弱特征间互相影响等问题, 还具有一定的降噪功能<citation id="97" type="reference"><link href="33" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>。它的基本思想是找到一个投影方向将高维的特征向量投影到低维空间中, 即从原始数据中分析出低维的主分量来代表原始数据。本文中根据累计贡献率大于95%的原则, 设置背景数据库50人中累计贡献率大于95%的需要的最低维度为主成分分析降维后的维度, 通过实验验证, 降维后的特征维度为14。</p>
                </div>
                <h3 id="56" name="56" class="anchor-tag"><b>2 法庭自动说话人识别系统</b></h3>
                <h4 class="anchor-tag" id="57" name="57"><b>2.1 基于似然比的法庭自动说话人识别系统</b></h4>
                <div class="p1">
                    <p id="58">似然比可以表示成在同源假设条件下出现嫌疑人语音特征的概率和在完全相反的非同源假设条件下出现相同语音特征概率的比值。因此, 似然比就是当前语音证据支持同源假设和支持非同源假设的相对强度, 似然比数值反映证据强度的大小。如果用<i>P</i>表示条件概率, <i>E</i>表示证据, <i>H</i><sub>0</sub>表示同源假设, <i>H</i><sub>1</sub>表示非同源假设, 那么似然比表示为:</p>
                </div>
                <div class="p1">
                    <p id="59"><mathml id="60"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><mi>R</mi><mo>=</mo><mfrac><mrow><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>E</mi><mo stretchy="false">|</mo><mi>Η</mi><msub><mrow></mrow><mn>0</mn></msub><mo stretchy="false">) </mo></mrow><mrow><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>E</mi><mo stretchy="false">|</mo><mi>Η</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">) </mo></mrow></mfrac></mrow></math></mathml>      (3) </p>
                </div>
                <div class="p1">
                    <p id="61">法庭自动说话人识别系统的流程图如图1所示, 其核心为基于LR的证据评估体系, 系统的输出结果是一个具有物理意义的概率比, 即犯罪证据在嫌疑人语音中出现的概率与该语音证据在除嫌疑人之外的其他人群中出现的概率比。该自动说话人识别系统的特征提取与模型训练部分和普通说话人识别相同, 只是在门限判决部分采用似然比, 而不是根据经验或先验知识得到的门限设置。似然比天然的识别阈限为1, 如果似然比结果大于1, 则说明证据支持同源假设;如果似然比结果小于1, 则说明证据支持非同源假设, 即罪犯和嫌疑人不是同一人;如果等于1, 对两个假设的支持力度相同, 说明是一个无效的证据。</p>
                </div>
                <div class="area_img" id="62">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201907013_062.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 法庭自动说话人识别系统流程图" src="Detail/GetImg?filename=images/JYRJ201907013_062.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 法庭自动说话人识别系统流程图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201907013_062.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="63">在图1虚线之上为法庭自动说话人识别系统的训练部分, 虚线之下为测试部分。由训练背景数据库训练出背景人群的高斯混合模型 (Universal Background Model, UBM) , 然后通过自适应高斯混合模型算法<citation id="98" type="reference"><link href="35" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>计算出每个说话人的高斯混合模型 (Gaussian Mixture Model, GMM) , 如图2所示。通过参考数据库中测试语音与GMM的对比, 获得输出得分, 然后根据相同说话人语音得分训练相同说话人正态分布模型, 根据不同说话人语音得分训练不同说话人正态分布模型, 并根据参考数据库中的数据使用逻辑回归算法对似然比结果进行校正, 获得校正加权值。测试时, 测试语音的得分在上述两个正态分布模型上分别计算出现概率, 其比值即为似然比, 通过测试部分获得的校正权值校正后, 即为最后的似然比得分, 是对证据强度的量化结果。</p>
                </div>
                <div class="area_img" id="64">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201907013_064.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 说话人模型自适应流程图" src="Detail/GetImg?filename=images/JYRJ201907013_064.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 说话人模型自适应流程图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201907013_064.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="65" name="65"><b>2.2 法庭说话人识别系统性能可靠性评估</b></h4>
                <div class="p1">
                    <p id="66">法庭自动说话人识别系统的识别性能由美国国家标准及技术署说话人识别技术评测中心使用的系统性能评估函数来度量, 它是与先验概率无关的对数似然比代价函数:</p>
                </div>
                <div class="p1">
                    <p id="67"><mathml id="68"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>C</mi><msub><mrow></mrow><mrow><mtext>l</mtext><mtext>l</mtext><mtext>r</mtext></mrow></msub><mo>=</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mrow><mo> (</mo><mrow><mfrac><mn>1</mn><mrow><mi>Ν</mi><msub><mrow></mrow><mrow><mi>s</mi><mi>s</mi></mrow></msub></mrow></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>Ν</mi><msub><mrow></mrow><mrow><mi>s</mi><mi>s</mi></mrow></msub></mrow></munderover><mtext>l</mtext></mstyle><mtext>o</mtext><mtext>g</mtext><msub><mrow></mrow><mn>2</mn></msub><mrow><mo> (</mo><mrow><mn>1</mn><mo>+</mo><mfrac><mn>1</mn><mrow><mi>L</mi><mi>R</mi><msub><mrow></mrow><mrow><mi>s</mi><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msub></mrow></mfrac></mrow><mo>) </mo></mrow><mo>+</mo><mfrac><mn>1</mn><mrow><mi>Ν</mi><msub><mrow></mrow><mrow><mi>d</mi><mi>s</mi></mrow></msub></mrow></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>Ν</mi><msub><mrow></mrow><mrow><mi>d</mi><mi>s</mi></mrow></msub></mrow></munderover><mtext>l</mtext></mstyle><mtext>o</mtext><mtext>g</mtext><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false"> (</mo><mn>1</mn><mo>+</mo><mi>L</mi><mi>R</mi><msub><mrow></mrow><mrow><mi>d</mi><mi>s</mi><msub><mrow></mrow><mi>j</mi></msub></mrow></msub><mo stretchy="false">) </mo></mrow><mo>) </mo></mrow></mrow></math></mathml>      (4) </p>
                </div>
                <div class="p1">
                    <p id="69">式中:<i>N</i><sub><i>ss</i></sub>和<i>N</i><sub><i>ds</i></sub>分别是相同说话人语音对和不同说话人语音对的个数, <i>LR</i><sub><i>ss</i></sub>和<i>LR</i><sub><i>ds</i></sub>分别是相同说话人比较对和不同说话人比较对产生的似然比数值。识别系统的结果越可靠, <i>C</i><sub>llr</sub>的值就会越低。</p>
                </div>
                <h3 id="70" name="70" class="anchor-tag"><b>3 结果与讨论</b></h3>
                <h4 class="anchor-tag" id="71" name="71"><b>3.1 数据库设置</b></h4>
                <div class="p1">
                    <p id="72">本文使用了3个男性说话人数据库, 分别为背景数据库 (50人) 、参考数据库 (20人) 和测试数据库 (60人) 。上述数据库中说话人年龄在19～24岁之间, 每个人录音2次, 采用中国刑事警察学院的内部固话系统录制, 保存为“Windows PCM, *.wav”格式, 采样率为8 000 Hz, 采样精度为16位。数据库中说话人被要求以普通话朗读固定的文本内容, 录制环境为普通的办公室环境。使用Audition CS6软件去除各条语音中的非语音段, 以排除讲话停顿时间的干扰。</p>
                </div>
                <div class="p1">
                    <p id="73">为模拟真实案件的情况, 本文提取了真实案件中办公室录制音频中的噪声, 其频谱特性如图3所示, 分别按不同程度信噪比加入到上述三个纯净数据库中, 形成了信噪比分别为20、10、0、-10和-20 dB的五个等级的数据库, 以测试GFCC特征抗噪声干扰能力。</p>
                </div>
                <div class="area_img" id="74">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201907013_074.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 真实案件录音中噪声声谱图" src="Detail/GetImg?filename=images/JYRJ201907013_074.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 真实案件录音中噪声声谱图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201907013_074.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="75">使用背景数据库中50人的两次录音 (各30秒) 训练背景人群数据库, 即UBM。它反映GFCC特征在人群中出现的情况, 由64维的高斯混合模型来表示。使用参考数据库中20名说话人组成的20人相同说话人比较对及190人不同说话人比较对, 提取每人两次录音的前10秒进行组合, 使用自适应高斯混合模型算法, 从UBM中生成表征每个人语音特性的GMM, 然后使用每人第一次录音的另外10秒语音进行测试。由20次相同说话人比较对的得分训练成正态分布模型H0, 表征说话人自身的变化性;由1 770次不同说话人比较对的得分训练成另外一个正态分布模型H1, 表征不同说话人之间的变化性。使用参考数据中每人第一次录音中没有用过的另外10秒数据进行测试, 测试输出得分在模型H0和H1上计算似然比数值。然后使用逻辑回归算法对结果进行校正, 在校正过程中产生校正权重, 用于对测试数据库的似然比输出结果进行校正。</p>
                </div>
                <div class="p1">
                    <p id="76">在每次测试中, 由测试数据库中的60人组成60个相同说话人比较语音对, 同时组成1 770个不同说话人比较语音对, 提取每人两次录音的前10秒进行组合, 使用自适应高斯混合模型算法, 从UBM中生成表征每个人语音特性的GMM, 然后使用每人第一次录音的其他10秒语音进行测试, 测试语音的长度为10秒。输出得分在H0和H1分别计算出现概率, 其比值即为测试数据库的似然比数值。</p>
                </div>
                <h4 class="anchor-tag" id="77" name="77"><b>3.2 纯净数据库测试结果</b></h4>
                <div class="p1">
                    <p id="78">在未加噪的纯净数据中, 本文采用了64维GFCC特征的前26维, 没有使用PCA 降维。图4是参考数据库中未使用过的10秒语音在本文FASR上获得的测试结果。结果使用Tippett图表示, 图中竖直的虚线为识别阈值, 似然比的识别阈值为1, 取对数后为0;左上较细的实线表示不同说话人似然比取对数10后大于等于x轴刻度的样本所占的比率, 小于阈值即为正确识别, 距离阈值越远, 证据强度越大;右上较粗的实线表示同一说话人似然比取对数10后小于等于x轴刻度的样本所占的比率, 大于阈值即为正确识别, 距离阈值越远, 证据强度越大。图5为使用逻辑回归校正算法校正后的似然比结果。对比图4和图5可以得到, 校正后, 代表阈值门限的竖直虚线右移, 更位于相同说话人和不同说话人的对数似然比得分的中间, 错误识别率进一步降低。图5中所有的不同说话人比较对和相同说话人比较对都得到了正确识别, 图4中校正前结果中存在一小部分不同说话人识别结果被错误认定, 说明校正算法是有效的。图6为测试数据库的似然比结果, 错误否定率 (False negative rate, FNR) 为0, 错误认定率 (False positive rate, FPR) 为0.17%, 值为0.006 4。</p>
                </div>
                <div class="area_img" id="79">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201907013_079.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 参考数据库校正前Tippett图" src="Detail/GetImg?filename=images/JYRJ201907013_079.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 参考数据库校正前Tippett图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201907013_079.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="80">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201907013_080.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 参考数据库校正后Tippett图" src="Detail/GetImg?filename=images/JYRJ201907013_080.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 参考数据库校正后Tippett图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201907013_080.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="81">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201907013_081.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 测试数据库Tippett图" src="Detail/GetImg?filename=images/JYRJ201907013_081.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 测试数据库Tippett图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201907013_081.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="82" name="82"><b>3.3 噪数据库测试结果</b></h4>
                <div class="p1">
                    <p id="83">在加噪声数据库中, 本文使用前文所述的PCA算法进行了降维, 把64维GFCC特征降为14维特征。在-20 dB的条件下, 对于相同说话人语音对, 其FNR为5%, 对于不同说话人语音对, 其错误认定率FPR为3.39%。为节省篇幅, 本文使用表1列出了不同信噪比条件下识别系统的值与错误识别率。从数据层面上来看, 本文方法优于文献<citation id="99" type="reference">[<a class="sup">13</a>]</citation>中的方法, 但是由于数据库选择、录制环境、加入噪声类型、测试语音长短不同等因素的影响, 很难进行客观的横向比较。</p>
                </div>
                <div class="area_img" id="84">
                    <p class="img_tit"><b>表1 不同信噪比条件下识别系统的Cllr值与错误识别率</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="84" border="1"><tr><td>SNR</td><td>参考库<br />校正前<i>C</i><sub>llr</sub></td><td>参考库<br />校正后<i>C</i><sub>llr</sub></td><td>测试库<br /><i>C</i><sub>llr</sub></td><td>FNR</td><td>FPR</td></tr><tr><td><br />纯净库</td><td>0.004 0</td><td>0.001 0</td><td>0.005 2</td><td>0</td><td>0.001 1</td></tr><tr><td><br />20 dB</td><td>0.034 2</td><td>0.004 8</td><td>0.596 6</td><td>0.016 7</td><td>0.025 4</td></tr><tr><td><br />10 dB</td><td>0.062 1</td><td>0.032 2</td><td>0.084 5</td><td>0</td><td>0.031 1</td></tr><tr><td><br />0 dB</td><td>0.233 4</td><td>0.097 9</td><td>0.097 9</td><td>0</td><td>0.039 0</td></tr><tr><td><br />-10 dB</td><td>0.535 8</td><td>0.500 1</td><td>0.392 4</td><td>0.050 0</td><td>0.106 2</td></tr><tr><td><br />-20 dB</td><td>0.098 8</td><td>0.069 6</td><td>0.199 0</td><td>0.050 0</td><td>0.033 9</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="85">从表1中测试结果可知, 参考库校正前后的值都有不同程度的下降, 说明本文使用的逻辑回归算法的有效性。FPR和FNR在信噪比逐渐降低的条件下, 保持相对缓慢的增长, 但是, 即使在高噪声条件下, 识别系统依然保持了良好的稳定性。即使识别错误, 其错误的程度并不大, 不会对鉴定结果产生较大的错误影响。</p>
                </div>
                <h3 id="86" name="86" class="anchor-tag"><b>4 结 语</b></h3>
                <div class="p1">
                    <p id="87">本文使用能模拟耳蜗听觉特性的GFCC特征与主成分分析相结合的方法, 在不同信噪比程度下, 对基于似然比证据评估体系的法庭自动说话人识别系统抗噪特性进行了研究。实验结果表明:该方法在信噪比不断降低的情况下, 法庭自动说话人识别系统依然能保持良好的稳定性和较高的识别率, 甚至在-20 dB的条件下, 具有5%的错误否定率、3.39%的错误认定率。测试采用来自真实案件的办公室录音噪声, 对法庭自动说话人识别系统的实际法庭应用具有重要意义, 下一步将对更多类型的案件噪声进行测试, 以拓宽应用范围。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES234FB8BAF9352ADDC6921E5B74750D18&amp;v=MDI1ODkvMDBFdUlNQ1g1SXUySmc3RFovU1FybjNoVXhmcmVVTWJ1WENPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkJyTFUwNXR0aHhMMjV4SzA9TmlmT2ZiRzdHcWUrcA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> Enzinger E, Morrison G S, Ochoa F.A demonstration of the application of the new paradigm for the evaluation of forensic evidence under conditions reflecting those of a real forensic voice comparison case[J].Science &amp; Justice, 2016, 56 (1) :42-57.
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Where the science ends and the law begins:likelihood ratio-based forensic voice comparison in a$150 million telephone fraud">

                                <b>[2]</b> Rose P.Where the science ends and the law begins:likelihood ratio-based forensic voice comparison in a 150 million telephone fraud[J].International Journal of Speech Language and the Law, 2013, 20 (2) :277-324.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14010600476630&amp;v=MjU5OTA5RllPd0pDbjg1b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcnpJSUZzU2JoYz1OaWZPZmJLOEh0RE1xWQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> Morrison G S.Distinguishing between forensic science and forensic pseudoscience:testing of validity and reliability, and approaches to forensic voice comparison[J].Science &amp; Justice, 2014, 54 (3) :245-256.
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A response to the UK position statement on forensic speaker comparison">

                                <b>[4]</b> Rose P, Morrison G S.A response to the UK Position Statement on forensic speaker comparison[J].International Journal of Speech Language and the Law, 2009, 16 (1) :139-63.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=FLYZ201401006&amp;v=MTUxNDVSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnRGeWpnVmJ2Tkl5SFNkTEc0SDlYTXJvOUZZb1FLREg4NHY=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b> 王华朋, 许锋.论法庭证据评估体系的发展[J].证据科学, 2014, 22 (1) :56-63.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES1A67A45AEA0E88AE5BDA02CB53DA888E&amp;v=MTkzNTNIMlIzaGMyRGNPY1RiTHFDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZCckxVMDV0dGh4TDI1eEswPU5pZk9mYkxKR05hOXE0bzBFWm9QZVhReHZtTVdtRXNNUw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b> Rose P.Likelihood ratio-based forensic voice comparison with higher level features:research and reality[J].Computer Speech &amp; Language, 2017 (45) :475-502.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Special issue on measuring and reporting the precision of forensic likelihood ratios:Introduction to the debate">

                                <b>[7]</b> Morrison G S.Special issue on measuring and reporting the precision of forensic likelihood ratios:Introduction to the debate[J].Science &amp; Justice, 2016, 56 (5) :371-373.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A novel scheme for speaker recognition using a phonetically-aware deep neural network">

                                <b>[8]</b> Lei Y, Scheffer N, Ferrer L, et al.A novel scheme for speaker recognition using a phonetically-aware deep neural network[C]//2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) .IEEE, 2014:1695-1699.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Duration mismatch compensation for i-vector based speaker recognition systems">

                                <b>[9]</b> Hasan T, Saeidi R, Hansen J H L, et al.Duration mismatch compensation for i-vector based speaker recognition systems[C]//2013 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) .IEEE, 2013:7663-7667.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Noise Robust Speaker Identification Using RASTA—MFCC Feature with Quadrilateral Filter Bank Structure">

                                <b>[10]</b> Nidhyananthan S S, Kumari R S S, Selvi T S.Noise Robust Speaker Identification Using RASTA—MFCC Feature with Quadrilateral Filter Bank Structure[J].Wireless Personal Communications, 2016, 91 (3) :1321-1333.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_11" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=QHXB200806035&amp;v=MTkzMTRqZ1Zidk5OQ1hUYkxHNEh0bk1xWTlHWVlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVp0Rnk=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[11]</b> 陈世雄, 宫琴, 金慧君.用Gammatone滤波器组仿真人耳基底膜的特性[J].清华大学学报 (自然科学版) , 2008, 48 (6) :1044-1048.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_12" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XING201801023&amp;v=MTg2NTF0RnlqZ1Zidk5QU1RGYWJHNEg5bk1ybzlIWjRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVo=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[12]</b> 王华朋.基于听觉模型的法庭语音证据特征量化[J].中国刑警学院学报, 2018 (1) :119-122.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_13" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSGG201510045&amp;v=MjY3NDBGckNVUjdxZlp1WnRGeWpnVmJ2Tkx6N01hYkc0SDlUTnI0OUJZWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEc=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[13]</b> 茅正冲, 王正创, 黄芳.基于GFCC与RLS的说话人识别抗噪系统研究[J].计算机工程与应用, 2015, 51 (10) :215-218.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Supervised domain adaptation for I-vector based speaker recognition">

                                <b>[14]</b> Garciaromero D, Mccree A.Supervised domain adaptation for I-vector based speaker recognition[C]//2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) .IEEE, 2014:4047-4051.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_15" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY2016S1021&amp;v=MjIwNDR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVp0RnlqZ1Zidk5MejdCZDdHNEg5ZXZybzlIWllRS0RIODQ=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[15]</b> 熊冰峰, 曾以成, 谢小娟.一种改进的听觉特征参数应用于说话人识别[J].计算机应用, 2016, 36 (S1) :82-85.
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_16" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZGS201401009&amp;v=MTEyNTh0R0ZyQ1VSN3FmWnVadEZ5amdWYnZOSVRmTWZiRzRIOVhNcm85RmJZUUtESDg0dlI0VDZqNTRPM3pxcUI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[16]</b> 王晓兰, 王蓓, 顾为一.基于稀疏核主成分分析的语音情感识别研究[J].信息化研究, 2014, 40 (1) :36-39.
                            </a>
                        </p>
                        <p id="35">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Maximum a Posteriori Estimation for Multivariate Gaussian Mixture Observations of Markov Chains">

                                <b>[17]</b> Gauvain J L, Lee C H.Maximum a posteriori estimation for multivariate Gaussian mixture observations of Markov chains[J].IEEE Transactions on Speech and Audio Processing, 1994, 2 (2) :291-298.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JYRJ201907013" />
        <input id="dpi" type="hidden" value="600" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201907013&amp;v=MTQ5MzJGeWpnVmJ2S0x6VFpaTEc0SDlqTXFJOUVaNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnQ=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="1" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
