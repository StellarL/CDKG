<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637135740850908750%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJYRJ201905028%26RESULT%3d1%26SIGN%3dZW5Zy0UuhD%252bfECC4tviqeBZDYuA%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JYRJ201905028&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JYRJ201905028&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201905028&amp;v=MjgxMjhadVp0Rnl6a1U3eklMelRaWkxHNEg5ak1xbzlIYklRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWY=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#41" data-title="&lt;b&gt;0 引 言&lt;/b&gt; "><b>0 引 言</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#46" data-title="&lt;b&gt;1 相关工作&lt;/b&gt; "><b>1 相关工作</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#47" data-title="&lt;b&gt;1.1 黑盒攻击&lt;/b&gt;"><b>1.1 黑盒攻击</b></a></li>
                                                <li><a href="#56" data-title="&lt;b&gt;1.2 生成对抗网络&lt;/b&gt;"><b>1.2 生成对抗网络</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#65" data-title="&lt;b&gt;2 基于GAN对抗样本生成策略&lt;/b&gt; "><b>2 基于GAN对抗样本生成策略</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#67" data-title="&lt;b&gt;2.1 威胁模型&lt;/b&gt;"><b>2.1 威胁模型</b></a></li>
                                                <li><a href="#70" data-title="&lt;b&gt;2.2 攻击框架&lt;/b&gt;"><b>2.2 攻击框架</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#105" data-title="&lt;b&gt;3 实 验&lt;/b&gt; "><b>3 实 验</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#107" data-title="&lt;b&gt;3.1 数据集采集&lt;/b&gt;"><b>3.1 数据集采集</b></a></li>
                                                <li><a href="#113" data-title="&lt;b&gt;3.2 实验设置与结果分析&lt;/b&gt;"><b>3.2 实验设置与结果分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#128" data-title="&lt;b&gt;4 结 语&lt;/b&gt; "><b>4 结 语</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#44" data-title="图1 对抗扰动">图1 对抗扰动</a></li>
                                                <li><a href="#45" data-title="图2 对抗贴片样本">图2 对抗贴片样本</a></li>
                                                <li><a href="#53" data-title="图3 黑盒攻击流程图">图3 黑盒攻击流程图</a></li>
                                                <li><a href="#58" data-title="图4 生成对抗网络框架图">图4 生成对抗网络框架图</a></li>
                                                <li><a href="#111" data-title="图5 眼镜生成轮廓">图5 眼镜生成轮廓</a></li>
                                                <li><a href="#112" data-title="图6 原始眼镜图像 (左) 与合成眼镜图像 (右) ">图6 原始眼镜图像 (左) 与合成眼镜图像 (右) </a></li>
                                                <li><a href="#117" data-title="图7 对抗眼镜图像">图7 对抗眼镜图像</a></li>
                                                <li><a href="#118" data-title="图8 原始人脸图像 (左) 与对抗人脸图像 (右) ">图8 原始人脸图像 (左) 与对抗人脸图像 (右) </a></li>
                                                <li><a href="#125" data-title="&lt;b&gt;表1 人脸识别模型性能&lt;/b&gt;"><b>表1 人脸识别模型性能</b></a></li>
                                                <li><a href="#127" data-title="&lt;b&gt;表2 黑盒攻击性能&lt;/b&gt;"><b>表2 黑盒攻击性能</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" title=" Papernot N, Mcdaniel P, Sinha A, et al.Towards the Science of Security and Privacy in Machine Learning[EB].arXiv:1611.03814, 2016." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Towards the Science of Security and Privacy in Machine Learning[EB]">
                                        <b>[1]</b>
                                         Papernot N, Mcdaniel P, Sinha A, et al.Towards the Science of Security and Privacy in Machine Learning[EB].arXiv:1611.03814, 2016.
                                    </a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" title=" Barreno M, Nelson B, Joseph A D, et al.The security of machine learning[J].Machine Learning, 2010, 81 (2) :121-148." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00003762447&amp;v=Mjk4NjRjTUg3UjdxZForWnVGaXZsVjc3SUkxND1OajdCYXJPNEh0SFBxSWxIWU84SVkzazV6QmRoNGo5OVNYcVJyeG94&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         Barreno M, Nelson B, Joseph A D, et al.The security of machine learning[J].Machine Learning, 2010, 81 (2) :121-148.
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title=" Papernot N, Mcdaniel P, Jha S, et al.The Limitations of Deep Learning in Adversarial Settings[C]//2016 IEEE European Symposium on Security and Privacy (EuroS&amp;amp;P) .IEEE, 2016:372-387." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The limitations of deep learning in adversarial settings">
                                        <b>[3]</b>
                                         Papernot N, Mcdaniel P, Jha S, et al.The Limitations of Deep Learning in Adversarial Settings[C]//2016 IEEE European Symposium on Security and Privacy (EuroS&amp;amp;P) .IEEE, 2016:372-387.
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" title=" Feinman R, Curtin R R, Shintre S, et al.Detecting Adversarial Samples from Artifacts[EB].arXiv:1703.00410, 2017." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Detecting Adversarial Samples from Artifacts[EB]">
                                        <b>[4]</b>
                                         Feinman R, Curtin R R, Shintre S, et al.Detecting Adversarial Samples from Artifacts[EB].arXiv:1703.00410, 2017.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" title=" Kurakin A, Goodfellow I, Bengio S.Adversarial examples in the physical world[EB].arXiv:1607.02533, 2016." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Adversarial examples in the physical world[EB]">
                                        <b>[5]</b>
                                         Kurakin A, Goodfellow I, Bengio S.Adversarial examples in the physical world[EB].arXiv:1607.02533, 2016.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" title=" Dong Y, Liao F, Pang T, et al.Boosting Adversarial Attacks with Momentum[EB].arXiv:1710.06081, 2017." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Boosting Adversarial Attacks with Momentum[EB]">
                                        <b>[6]</b>
                                         Dong Y, Liao F, Pang T, et al.Boosting Adversarial Attacks with Momentum[EB].arXiv:1710.06081, 2017.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" title=" Tram&#232;r F, Kurakin A, Papernot N, et al.Ensemble Adversarial Training:Attacks and Defenses[EB].arXiv:1705.07204, 2017." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Ensemble Adversarial Training:Attacks and Defenses[EB]">
                                        <b>[7]</b>
                                         Tram&#232;r F, Kurakin A, Papernot N, et al.Ensemble Adversarial Training:Attacks and Defenses[EB].arXiv:1705.07204, 2017.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" title=" Carlini N, Wagner D.Towards Evaluating the Robustness of Neural Networks[C]//2017 IEEE Symposium on Security and Privacy (SP) .IEEE, 2017:39-57." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Towards evaluating the robustness of neural networks">
                                        <b>[8]</b>
                                         Carlini N, Wagner D.Towards Evaluating the Robustness of Neural Networks[C]//2017 IEEE Symposium on Security and Privacy (SP) .IEEE, 2017:39-57.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" title=" Papernot N, Mcdaniel P, Goodfellow I.Transferability in Machine Learning:from Phenomena to Black-Box Attacks using Adversarial Samples[EB].arXiv:1605.07277, 2016." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Transferability in Machine Learning:from Phenomena to Black-Box Attacks using Adversarial Samples[EB]">
                                        <b>[9]</b>
                                         Papernot N, Mcdaniel P, Goodfellow I.Transferability in Machine Learning:from Phenomena to Black-Box Attacks using Adversarial Samples[EB].arXiv:1605.07277, 2016.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_10" title=" Szegedy C, Zaremba W, Sutskever I, et al.Intriguing properties of neural networks[EB].arXiv:1312.6199, 2013." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Intriguing properties of neural networks[EB]">
                                        <b>[10]</b>
                                         Szegedy C, Zaremba W, Sutskever I, et al.Intriguing properties of neural networks[EB].arXiv:1312.6199, 2013.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_11" title=" Tram&#232;r F, Papernot N, Goodfellow I, et al.The Space of Transferable Adversarial Examples[EB].arXiv:1704.03453, 2017." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The Space of Transferable Adversarial Examples[EB]">
                                        <b>[11]</b>
                                         Tram&#232;r F, Papernot N, Goodfellow I, et al.The Space of Transferable Adversarial Examples[EB].arXiv:1704.03453, 2017.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_12" title=" Liu Y, Chen X, Liu C, et al.Delving into Transferable Adversarial Examples and Black-box Attacks[EB].arXiv:1611.02770, 2016." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Delving into Transferable Adversarial Examples and Black-box Attacks[EB]">
                                        <b>[12]</b>
                                         Liu Y, Chen X, Liu C, et al.Delving into Transferable Adversarial Examples and Black-box Attacks[EB].arXiv:1611.02770, 2016.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_13" title=" Goodfellow I J, Shlens J, Szegedy C.Explaining and Harnessing Adversarial Examples[EB].arXiv:1412.6572, 2014." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Explaining and Harnessing Adversarial Examples[EB]">
                                        <b>[13]</b>
                                         Goodfellow I J, Shlens J, Szegedy C.Explaining and Harnessing Adversarial Examples[EB].arXiv:1412.6572, 2014.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_14" title=" Kurakin A, Goodfellow I, Bengio S.Adversarial Machine Learning at Scale[EB].arXiv:1611.01236, 2016." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Adversarial Machine Learning at Scale[EB]">
                                        <b>[14]</b>
                                         Kurakin A, Goodfellow I, Bengio S.Adversarial Machine Learning at Scale[EB].arXiv:1611.01236, 2016.
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_15" title=" Boer P T D, Kroese D P, Mannor S, et al.A Tutorial on the Cross-Entropy Method[J].Annals of Operations Research, 2005, 134 (1) :19-67." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00000425224&amp;v=MDcwNDI3QmFyTzRIdEhNcTQxQVp1a0xZM2s1ekJkaDRqOTlTWHFScnhveGNNSDdSN3FkWitadUZpdmxWNzdJSTE0PU5q&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[15]</b>
                                         Boer P T D, Kroese D P, Mannor S, et al.A Tutorial on the Cross-Entropy Method[J].Annals of Operations Research, 2005, 134 (1) :19-67.
                                    </a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_16" title=" Meng D, Chen H.Magnet:a two-pronged defense against adversarial examples[C]//Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security.ACM, 2017:135-147." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Magnet:a two-pronged defense against adversarial examples">
                                        <b>[16]</b>
                                         Meng D, Chen H.Magnet:a two-pronged defense against adversarial examples[C]//Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security.ACM, 2017:135-147.
                                    </a>
                                </li>
                                <li id="35">


                                    <a id="bibliography_17" title=" Parkhi O M, Vedaldi A, Zisserman A.Deep Face Recognition[C]//British Machine Vision Conference 2015.2015." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep Face Recognition">
                                        <b>[17]</b>
                                         Parkhi O M, Vedaldi A, Zisserman A.Deep Face Recognition[C]//British Machine Vision Conference 2015.2015.
                                    </a>
                                </li>
                                <li id="37">


                                    <a id="bibliography_18" title=" Huang G B, Ramesh M, Berg T, et al.Labeled faces in the wild:A database for studying face recognition in unconstrained environments[R].Technical Report 07-49, University of Massachusetts, Amherst, October 2007." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Labeled Faces in the Wild: A Database for Studying FaceRecognition in Unconstrained Environments">
                                        <b>[18]</b>
                                         Huang G B, Ramesh M, Berg T, et al.Labeled faces in the wild:A database for studying face recognition in unconstrained environments[R].Technical Report 07-49, University of Massachusetts, Amherst, October 2007.
                                    </a>
                                </li>
                                <li id="39">


                                    <a id="bibliography_19" title=" Amos B, Ludwiczuk B, Satyanarayanan M.OpenFace:A general-purpose face recognition library with mobile applications[R].Technical report, CMU-CS-16-118, CMU School of Computer Science, 2016." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Open Face:A general-purpose face recognition library with mobile applications">
                                        <b>[19]</b>
                                         Amos B, Ludwiczuk B, Satyanarayanan M.OpenFace:A general-purpose face recognition library with mobile applications[R].Technical report, CMU-CS-16-118, CMU School of Computer Science, 2016.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JYRJ" target="_blank">计算机应用与软件</a>
                2019,36(05),158-164 DOI:10.3969/j.issn.1000-386x.2019.05.027            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>对抗样本生成在人脸识别中的研究与应用</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BC%A0%E5%8A%A0%E8%83%9C&amp;code=40895469&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">张加胜</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%88%98%E5%BB%BA%E6%98%8E&amp;code=15599003&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">刘建明</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%9F%A9%E7%A3%8A&amp;code=41743680&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">韩磊</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%BA%AA%E9%A3%9E&amp;code=41743681&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">纪飞</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%88%98%E7%85%8C&amp;code=41743682&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">刘煌</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%A1%82%E6%9E%97%E7%94%B5%E5%AD%90%E7%A7%91%E6%8A%80%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%B8%8E%E4%BF%A1%E6%81%AF%E5%AE%89%E5%85%A8%E5%AD%A6%E9%99%A2&amp;code=0269119&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">桂林电子科技大学计算机与信息安全学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>随着深度学习模型在人脸识别、无人驾驶等安全敏感性任务中的广泛应用, 围绕深度学习模型展开的攻防逐渐成为机器学习和安全领域研究的热点。黑盒攻击作为典型的攻击类型, 在不知模型具体结构、参数、使用的数据集等情况下仍能进行有效攻击, 是真实背景下最常用的攻击方法。随着社会对人脸识别技术的依赖越来越强, 在安全性高的场合里部署神经网络, 往往容易忽略其脆弱性带来的安全威胁。充分分析深度学习模型存在的脆弱性并运用生成对抗网络, 设计一种新颖的光亮眼镜贴片样本, 能够成功欺骗基于卷积神经网络的人脸识别系统。实验结果表明, 基于生成对抗网络生成的对抗眼镜贴片样本能够成功攻击人脸识别系统, 性能优于传统的优化方法。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%BB%91%E7%9B%92%E6%94%BB%E5%87%BB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">黑盒攻击;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%84%86%E5%BC%B1%E6%80%A7&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">脆弱性;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">生成对抗网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%9C%BC%E9%95%9C%E8%B4%B4%E7%89%87&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">眼镜贴片;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    张加胜, 硕士生, 主研领域:机器学习, 模式识别。;
                                </span>
                                <span>
                                    刘建明, 教授。;
                                </span>
                                <span>
                                    韩磊, 硕士生。;
                                </span>
                                <span>
                                    纪飞, 硕士生。;
                                </span>
                                <span>
                                    刘煌, 硕士生。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-11-21</p>

            </div>
                    <h1><b>RESEARCH AND APPLICATION OF ADVERSARIAL SAMPLE GENERATION IN FACIAL RECOGNITION</b></h1>
                    <h2>
                    <span>Zhang Jiasheng</span>
                    <span>Liu Jianming</span>
                    <span>Han Lei</span>
                    <span>Ji Fei</span>
                    <span>Liu Huang</span>
            </h2>
                    <h2>
                    <span>School of Computer Science and Information Security, Guilin University of Electronic Technology</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Deep learning (DL) models have been widely applied into security-sensitivity tasks, such as facial recognition, automated driving, etc. Attacks and defenses associated with the DL have gradually become hot spots in the field of machine learning and security. The black box attack, as a typical attack type and the most common attack method in the real context, can still perform effective attacks without knowing the specific structure and parameters of the model, including data sets. With the increasing dependence on facial recognition technology, it is easy to ignore the security threats caused by its vulnerability when deploying neural networks in high security situations. This paper fully analyzed the vulnerability of the deep learning model and used the generated adversarial network (GAN) to design a novel bright glasses patch sample, which could successfully deceive the facial recognition system based on convolutional neural network. The experimental results show that the adversarial eyeglass patches generated by GAN can successfully attack the face recognition system, and the performance is better than the traditional optimization methods.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Deep%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Deep learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Black-Box%20attack&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Black-Box attack;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Vulnerability&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Vulnerability;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Generative%20adversarial%20network%20(GAN)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Generative adversarial network (GAN) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Eyeglass%20patches&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Eyeglass patches;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-11-21</p>
                            </div>


        <!--brief start-->
                        <h3 id="41" name="41" class="anchor-tag"><b>0 引 言</b></h3>
                <div class="p1">
                    <p id="42">随着深度学习技术的发展, 深度神经网络DNN (Deep Neural Network) 在人脸识别, 交通标识识别, 安防监控等安全敏感性任务中得到了广泛的应用<citation id="136" type="reference"><link href="3" rel="bibliography" /><link href="5" rel="bibliography" /><link href="7" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>,<a class="sup">3</a>]</sup></citation>。然而Szegedy等首次揭示了深度神经网络脆弱性的存在, 极易受到对抗性样本的影响, 即通过对原始输入样本进行不可察觉的微小的扰动, 可以使深度神经网络以较高的置信度错误分类<citation id="130" type="reference"><link href="21" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>。进而, Goodfellow解释了对抗样本存在的根本原因, 深度模型高度线性的本质<citation id="131" type="reference"><link href="29" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>, 并提出了相关对抗样本生成策略, 如FGSM<citation id="132" type="reference"><link href="27" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>、I-FGSM<citation id="133" type="reference"><link href="11" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>、RAND+FGSM<citation id="134" type="reference"><link href="15" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>等, 通过在熊猫的图片中加入微小的扰动向量, 在人眼不易察觉的情况下成功使神经网络以高置信度分类为长臂猿。然而这些攻击算法的设计都需要攻击者对目标系统的体系结构或训练参数有充分的了解 (白盒) 。由于在现实世界中很难获取到目标系统的内部信息, 对于攻击者来说, 目标系统完全就是一个黑盒。先前的研究表明, 不同学习模型之间存在着迁移性<citation id="137" type="reference"><link href="17" rel="bibliography" /><link href="19" rel="bibliography" /><link href="23" rel="bibliography" /><sup>[<a class="sup">8</a>,<a class="sup">9</a>,<a class="sup">11</a>]</sup></citation>, 也就是说, 采用不同攻击算法生成的攻击样本能够使多个模型同时错误分类。这一属性为攻击者在未知目标系统内部信息情景下能够实现黑盒攻击奠定了基础。那么, 现实世界中基于深度学习的黑盒系统的安全性受到了一定威胁<citation id="135" type="reference"><link href="13" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>, 例如, 在人脸识别系统中, 攻击者通过对人脸图像做精心设计的改动, 使系统误认为是攻击者想要的身份, 从而侵入系统导致安全威胁;对于无人驾驶系统而言, 稍微修改“STOP”标识图像使得深度神经网络错误识别为其他标识而不及时停车, 将造成安全事故;再比如在安防监控系统中, 攻击者往往通过化妆、装饰物 (包括眼镜、假发、首饰等) 进行伪装迷惑监控系统, 从而混入非法分子。所以, 研究对抗样本的生成过程, 分析基于深度学习的系统存在的安全漏洞, 这将有助于设计更加安全有效的防御机制。</p>
                </div>
                <div class="p1">
                    <p id="43">对于人脸识别系统, 攻击者往往利用合法用户的照片, 通过添加微小的、不可察觉的扰动试图入侵系统, 如图1所示。然而目前大部分人脸识别系统都针对此类攻击设计了相应的防御机制, 来抵御微小扰动的干扰, 例如通过对抗性训练新的网络, MagNet等检测器的设计<citation id="138" type="reference"><link href="9" rel="bibliography" /><link href="33" rel="bibliography" /><sup>[<a class="sup">4</a>,<a class="sup">16</a>]</sup></citation>。对于攻击者而言, 唯有增加扰动量却极易被系统检测出对抗样本的可能性。进而, 相关研究者提出了高亮贴片的概念<citation id="139" type="reference"><link href="25" rel="bibliography" /><link href="31" rel="bibliography" /><sup>[<a class="sup">12</a>,<a class="sup">15</a>]</sup></citation>, 这类贴片样本摆脱了以往的束缚, 即不要求生成的样本在人眼看来仍然是原来的图片, 贴片样本示例如图2所示。攻击者通过打印生成的对抗贴片样本贴在交通标识牌、眼镜框等显眼位置, 不仅不会引起警觉, 同样可以欺骗深度学习系统。基于该思想, 本文通过生成对抗网络原理设计出一种新颖的对抗眼镜贴片样本, 并且可以应用于任何真实场景中, 从而揭示了现实世界中深度学习模型的脆弱性依然存在。</p>
                </div>
                <div class="area_img" id="44">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201905028_044.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 对抗扰动" src="Detail/GetImg?filename=images/JYRJ201905028_044.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 对抗扰动  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201905028_044.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="45">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201905028_045.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 对抗贴片样本" src="Detail/GetImg?filename=images/JYRJ201905028_045.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 对抗贴片样本  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201905028_045.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h3 id="46" name="46" class="anchor-tag"><b>1 相关工作</b></h3>
                <h4 class="anchor-tag" id="47" name="47"><b>1.1 黑盒攻击</b></h4>
                <div class="p1">
                    <p id="48">在现实世界中, 由于攻击者很难获取到目标模型 (即攻击对象) 的内部结构信息, 包括训练数据、模型体系结构以及训练参数等。那么, 对于攻击者而言, 目标系统完全就是一个黑盒, 攻击者只能通过原始的输入获得对应的反馈结果。这种情况下, 攻击者就很难设计出具有很强攻击性的对抗样本。然而, 先前的研究表明, 不同的深度学习模型之间存在着迁移性, 这为攻击者间接地去实现同样的攻击效果提供了基础。比如, 攻击者可以根据已知任务去训练一个替代模型, 此时的替代模型对于攻击者而言即为白盒, 进而一系列的攻击策略可以设计去生成对抗样本, 然后利用模型迁移性的性质, 将生成的对抗样本应用于未知模型的黑盒系统。</p>
                </div>
                <div class="p1">
                    <p id="49">假设<i>X</i>∈<i>R</i><sup><i>D</i></sup>表示<i>D</i>维的特征空间, <i>Y</i>={<i>y</i><sup>1</sup>, <i>y</i><sup>2</sup>, …, <i>y</i><sup><i>c</i></sup>}表示<i>c</i>个不同标签的标签集合。给定合成数据集<i>data</i>={ (<i>x</i><sub><i>i</i></sub>, <i>y</i><sub><i>i</i></sub>) |1≤<i>i</i>≤<i>N</i>}, <i>x</i><sub><i>i</i></sub>∈<i>X</i>表示第<i>i</i>个训练样本, <i>y</i><sub><i>i</i></sub>∈<i>Y</i>表示第<i>i</i>个样本查询目标模型的反馈标记。黑盒攻击的过程是从<i>data</i>学习一个替代模型<i>F</i> (<i>x</i>) , 然后选用某种攻击算法为该模型生成攻击样本<i>x</i><sup>*</sup>, 即通过最优化以下目标损失函数使得神经网络模型以最大概率错误分类为目标标签<i>y</i><sub><i>t</i></sub>:</p>
                </div>
                <div class="p1">
                    <p id="50"><i>Loss</i>=max{<i>F</i> (<i>x</i><sup>*</sup>|<i>x</i>+<i>r</i>) <sub><i>y</i></sub>:<i>y</i>≠<i>y</i><sub><i>t</i></sub> }-<i>F</i> (<i>x</i><sup>*</sup>|<i>x</i>+<i>r</i>) <sub><i>y</i><sub><i>t</i></sub></sub>      (1) </p>
                </div>
                <div class="p1">
                    <p id="51">式中:<i>r</i>表示添加到原始输入样本的微小的扰动向量。</p>
                </div>
                <div class="p1">
                    <p id="52">然后将该样本迁移到目标模型<i>O</i> (<i>x</i>) , 使目标模型也错误分类, 即<i>O</i> (<i>x</i>) ≠<i>O</i> (<i>x</i><sup>*</sup>) 。详细攻击流程如图3所示。</p>
                </div>
                <div class="area_img" id="53">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201905028_053.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 黑盒攻击流程图" src="Detail/GetImg?filename=images/JYRJ201905028_053.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 黑盒攻击流程图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201905028_053.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="54">本文着重探讨深度学习模型的脆弱性, 那么替代模型也可选择深层模型, 即<i>n</i>个带参函数<i>f</i> (<i>θ</i>, <i>x</i>) 的分层组合, 来模拟高维输入<i>x</i>, 定义如下:</p>
                </div>
                <div class="p1">
                    <p id="55"><i>F</i> (<i>x</i>) =<i>f</i><sub><i>n</i></sub> (<i>θ</i><sub><i>θ</i></sub>, <i>f</i><sub><i>n</i>-1</sub> (<i>θ</i><sub><i>n</i>-1</sub>, …, <i>f</i><sub>2</sub> (<i>θ</i><sub>2</sub>, <i>f</i><sub>1</sub> (<i>θ</i><sub>1</sub>, <i>x</i>) ) ) )      (2) </p>
                </div>
                <h4 class="anchor-tag" id="56" name="56"><b>1.2 生成对抗网络</b></h4>
                <div class="p1">
                    <p id="57">生成对抗网络GAN是一种深度学习模型, 主要由生成器 (Generator) 和判别器 (Discriminator) 两个模块构成。其中生成器的作用是尽可能地学习真实的数据分布, 输入变量<i>z</i>, 则<i>G</i>尽可能地生成服从真实数据分布的样本<i>G</i> (<i>z</i>) 。判别器的作用是判别其输入数据是来自生成器<i>G</i>, 还是来自真实的数据<i>x</i>, 如果输入来自<i>G</i> (<i>z</i>) , 则标注为0, 并判别为伪, 否则标注为1, 并判别为真。这里生成器<i>G</i>的目标是使其生成的伪数据<i>G</i> (<i>z</i>) 在判别器<i>D</i>上的表现和真实数据<i>x</i>在<i>D</i>上的表现一致。<i>G</i>和<i>D</i>互相博弈学习并迭代优化的过程使得它们的性能不断提升, 随着<i>D</i>的判别能力提升, 并且无法判别其数据来源时, 就认为<i>G</i>已学到真实的数据分布, 如图4所示。在实际应用中一般均使用深度神经网络作为<i>G</i>和<i>D</i>, 一个较好的生成式对抗网络 (GAN) 应用需要有良好的训练方法, 否则可能由于神经网络模型的自由行而导致输出不理想。</p>
                </div>
                <div class="area_img" id="58">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201905028_058.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 生成对抗网络框架图" src="Detail/GetImg?filename=images/JYRJ201905028_058.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 生成对抗网络框架图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201905028_058.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="59">GAN的训练需迭代进行, 通过反向传播更新<i>G</i>和<i>D</i>的参数。<i>G</i>的训练通过最小化以下目标函数:</p>
                </div>
                <div class="p1">
                    <p id="60"><mathml id="61"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><mi>o</mi><mi>s</mi><mi>s</mi><msub><mrow></mrow><mi>G</mi></msub><mo stretchy="false"> (</mo><mi>Ζ</mi><mo>, </mo><mi>D</mi><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>z</mi><mo>∈</mo><mi>Ζ</mi></mrow></munder><mrow><mi>lg</mi></mrow></mstyle><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>D</mi><mo stretchy="false"> (</mo><mi>G</mi><mo stretchy="false"> (</mo><mi>z</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo stretchy="false">) </mo></mrow></math></mathml>      (3) </p>
                </div>
                <div class="p1">
                    <p id="62">当<i>G</i>误导<i>D</i> (即当<i>D</i> (<i>G</i> (<i>z</i>) ) 取值为1) , 该损失达到最小值。<i>D</i>的训练通过最大化以下目标函数:</p>
                </div>
                <div class="p1">
                    <p id="63" class="code-formula">
                        <mathml id="63"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>G</mi><mi>a</mi><mi>i</mi><mi>n</mi><msub><mrow></mrow><mi>D</mi></msub><mo stretchy="false"> (</mo><mi>G</mi><mo>, </mo><mi>Ζ</mi><mo>, </mo><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>x</mi><mo>∈</mo><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></munder><mrow><mi>lg</mi></mrow></mstyle><mo stretchy="false"> (</mo><mi>D</mi><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo>+</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>z</mi><mo>∈</mo><mi>Ζ</mi></mrow></munder><mrow><mi>lg</mi></mrow></mstyle><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>D</mi><mo stretchy="false"> (</mo><mi>G</mi><mo stretchy="false"> (</mo><mi>z</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="64">当<i>D</i>对真实样本发出1时, <i>Gain</i><sub><i>D</i></sub>最大化, 否则为0。目前已经提出了几种GAN体系结构和训练过程来训练GAN, 本文主要以Wasserstein GAN为主。</p>
                </div>
                <h3 id="65" name="65" class="anchor-tag"><b>2 基于GAN对抗样本生成策略</b></h3>
                <div class="p1">
                    <p id="66">本节主要描述提出的基于GAN的对抗样本生成策略以迷惑深度神经网络。第一部分明确了我们的威胁模型, 第二部分详细描述攻击框架的设计。</p>
                </div>
                <h4 class="anchor-tag" id="67" name="67"><b>2.1 威胁模型</b></h4>
                <div class="p1">
                    <p id="68">假设一个攻击者能够成功侵入一个已经训练好的人脸识别系统来发动攻击。由于攻击者不能通过注入错误标记的数据或修改训练数据来危害人脸识别系统的参数。因此, 只能通过改变输入以紊乱深度神经网络的分类。</p>
                </div>
                <div class="p1">
                    <p id="69">攻击者的目标是通过不易察觉地伪装自己并呈现在人脸识别系统前, 然后被错误地归类为除了自己以外的人。考虑这种攻击的两种变体, 在定向攻击中, 攻击者试图使人脸识别系统错误分类为特定的其他人;在非定向攻击时, 攻击者试图使人脸识别系统错误分类为任意其他人, 不用特定到某个人。通常假设攻击者在白盒场景下运行。也就是说, 攻击者知道特征空间 (RGB图像表示, 这在DNN图像分类中是常用的) 以及被攻击系统的内部 (体系结构和参数) 。在白盒假设条件下研究DNN的脆弱性是本文的重点。此外, 黑盒攻击可以使用本地替代模型进行白盒攻击, 然后将其转移到黑盒中。</p>
                </div>
                <h4 class="anchor-tag" id="70" name="70"><b>2.2 攻击框架</b></h4>
                <div class="p1">
                    <p id="71">除了少数几种攻击方法以外, 在传统深度神经网络逃避攻击中, 攻击者直接改变正常输入样本来最大化或最小化一个预先定义的函数与预期的错误分类相关的函数。与以往的攻击不同, 本文提出了通过训练神经网络来产生可用于达到预期目的的输出。也就是说, 与其反复调整正常样本输入, 使之成为对抗性样本, 不如尝试迭代更新深度神经网络的权值来调整将产生导致错误分类的输出。</p>
                </div>
                <div class="p1">
                    <p id="72">更具体地说, 本文通过训练生成对抗神经网络来生成眼镜的贴片并可以打印贴在眼镜镜框上, 当攻击者佩戴时, 不易被察觉, 却能使人脸识别系统紊乱, 产生定向或非定向的攻击效果。为了达到不显眼的攻击效果, 我们要求这些神经网络产生的眼镜图片与真实的眼镜设计相似。与传统的GAN训练类似, 不同于GAN的是, 还需要产生对抗性的输出 (即包含眼镜的人脸图像) , 能够误导用于人脸识别的神经网络模型。</p>
                </div>
                <div class="p1">
                    <p id="73">所以, 本文提出的方法需要训练三个深度神经网络:一个生成器<i>G</i>、一个判别器<i>D</i>和一个预训练的分类函数为<i>F</i> (·) 。当输入<i>x</i>到DNN时, 通过最小化<i>G</i>来生成不引人注目的对抗输出, 能够迷惑<i>F</i> (·) , 优化目标如下:</p>
                </div>
                <div class="p1">
                    <p id="74" class="code-formula">
                        <mathml id="74"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><mi>o</mi><mi>s</mi><mi>s</mi><msub><mrow></mrow><mi>G</mi></msub><mo stretchy="false"> (</mo><mi>Ζ</mi><mo>, </mo><mi>D</mi><mo stretchy="false">) </mo><mo>-</mo><mi>κ</mi><mo>⋅</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>z</mi><mo>∈</mo><mi>Ζ</mi></mrow></munder><mi>L</mi></mstyle><mi>o</mi><mi>s</mi><mi>s</mi><msub><mrow></mrow><mi>F</mi></msub><mo stretchy="false"> (</mo><mi>x</mi><mo>+</mo><mi>G</mi><mo stretchy="false"> (</mo><mi>z</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>5</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="75">其中损失函数<i>Loss</i><sub><i>G</i></sub>同式 (1) 定义, 通过最小化该损失的目的是希望生成器生成真实的不显眼的眼镜图像输出, 能够误导判别器<i>D</i>, 使判别器认为生成的眼镜图像即为真实图像。<i>Loss</i><sub><i>F</i></sub>是在DNN的分类中定义的损失函数, 在训练<i>G</i>过程中, 通过最大化该损失, 从而使得生成的眼镜图像付在人脸图像之上能够成功欺骗深度神经网络<i>F</i>。<i>Loss</i><sub><i>F</i></sub>的定义又分为两类, 分别针对定向和非定向而言。对于非定向攻击而言, <i>Loss</i><sub><i>F</i></sub>损失定义如下:</p>
                </div>
                <div class="p1">
                    <p id="76" class="code-formula">
                        <mathml id="76"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>L</mi><mi>o</mi><mi>s</mi><mi>s</mi><msub><mrow></mrow><mi>F</mi></msub><mo stretchy="false"> (</mo><mi>x</mi><mo>+</mo><mi>G</mi><mo stretchy="false"> (</mo><mi>z</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>i</mi><mo>≠</mo><mi>x</mi></mrow></munder><mi>F</mi></mstyle><msub><mrow></mrow><mrow><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msub><mo stretchy="false"> (</mo><mi>x</mi><mo>+</mo><mi>G</mi><mo stretchy="false"> (</mo><mi>z</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo>-</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mi>F</mi><msub><mrow></mrow><mrow><mi>y</mi><msub><mrow></mrow><mi>x</mi></msub></mrow></msub><mo stretchy="false"> (</mo><mi>x</mi><mo>+</mo><mi>G</mi><mo stretchy="false"> (</mo><mi>z</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>6</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="77">通过最大化<i>Loss</i><sub><i>F</i></sub>, 样本<i>x</i>被DNN识别成真实标签<i>y</i><sub><i>x</i></sub>的概率大大降低, 使识别成其他标签的概率增加, 从而实现非定向攻击的效果。对于定向攻击而言, <i>Loss</i><sub><i>F</i></sub>损失定义如下:</p>
                </div>
                <div class="p1">
                    <p id="78" class="code-formula">
                        <mathml id="78"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>L</mi><mi>o</mi><mi>s</mi><mi>s</mi><msub><mrow></mrow><mi>F</mi></msub><mo stretchy="false"> (</mo><mi>x</mi><mo>+</mo><mi>G</mi><mo stretchy="false"> (</mo><mi>z</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo>=</mo><mi>F</mi><msub><mrow></mrow><mrow><mi>y</mi><msub><mrow></mrow><mi>t</mi></msub></mrow></msub><mo stretchy="false"> (</mo><mi>x</mi><mo>+</mo><mi>G</mi><mo stretchy="false"> (</mo><mi>z</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo>-</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>i</mi><mo>≠</mo><mi>t</mi></mrow></munder><mi>F</mi></mstyle><msub><mrow></mrow><mrow><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msub><mo stretchy="false"> (</mo><mi>x</mi><mo>+</mo><mi>G</mi><mo stretchy="false"> (</mo><mi>z</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>7</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="79">通过最大化<i>Loss</i><sub><i>F</i></sub>, 样本<i>x</i>被DNN识别成攻击者指定的目标标签<i>y</i><sub><i>t</i></sub>的概率会增加。</p>
                </div>
                <div class="p1">
                    <p id="80">判别器<i>D</i>的训练作为训练过程的一部分, 目标使式 (4) 中定义的收益<i>Gain</i><sub><i>D</i></sub>最大化。通过调整判别器<i>D</i> 的权重进而激励生成器<i>G</i>生成更真实的实例, 两者相互博弈, 实现互利共赢。与<i>D</i>和<i>G</i>相反, <i>F</i> (·) 的权重在训练过程中是不需要改变 (因为希望在测试阶段, 生成的对抗输出能够成功欺骗相同的<i>DNN</i>) 。整个训练过程如算法1所示。</p>
                </div>
                <div class="p1">
                    <p id="81"><b>算法1</b> 基于GAN对抗眼镜贴片样本生成过程</p>
                </div>
                <div class="area_img" id="143">
                                <img alt="" src="Detail/GetImg?filename=images/JYRJ201905028_14300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="143">
                                <img alt="" src="Detail/GetImg?filename=images/JYRJ201905028_14301.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="103">该算法以一组正常样本<i>X</i>作为输入, 一个预初始化的生成器和鉴别器, 一个用于人脸识别的神经网络, 实际示例的数据集 (生成器的输出应该类似于这些数据集; 在我们的例子中这是一个眼镜的数据集) , 一个可以从<i>G</i>的潜在空间 (<i>Z</i>) 中采样的函数, 最大训练周期 (<i>N</i><sub><i>e</i></sub>) , mini-batches的大小<i>s</i><sub><i>b</i></sub>, 和<i>κ</i> (0和1之间的值) 。这个训练过程的结果是一个对抗的生成器, 它创建输出 (即眼镜图像) 以迷惑用于人脸识别的深度神经网络<i>F</i> (·) 。在每次训练迭代中, <i>D</i>或<i>G</i>都使用随机选择的数据子集进行更新。<i>D</i>的权值通过梯度上升来更新, 以增加增益。相反, <i>G</i>的权值通过梯度下降来更新, 使式 (3) 定义的损失值最小化。为了平衡生成器的两个目标, 分别将<i>Gain</i><sub><i>D</i></sub>和<i>Loss</i><sub><i>F</i></sub>的导数进行结合, 通过对这两个导数进行归一化得到这两个值的欧几里德范数 (算法中第16行) 。</p>
                </div>
                <div class="p1">
                    <p id="104">然后通过设置<i>κ</i>来控制这两个目标中的哪一个该获得更多的权重 (算法中的第17行) 。当<i>κ</i>接近零的时候, 会有更多的权重用以迷惑<i>F</i> (·) , 分配更少的权重使<i>G</i>的输出更真实。相反, 当<i>κ</i>接近一个1时, 会分配更大的权重使<i>G</i>的输出类似于真实的例子。当达到最大训练周期时, 训练结束, 或者当<i>F</i> (·) 被迷惑时, 即定向或非定向攻击已完成。</p>
                </div>
                <h3 id="105" name="105" class="anchor-tag"><b>3 实 验</b></h3>
                <div class="p1">
                    <p id="106">为了验证上述对抗样本生成策略的有效性, 在收集的眼镜数据集上进行了对抗眼镜样本的合成, 并在经典的人脸识别模型OpenFace, VGG上验证了提出的方法的攻击性能。需要如下准备: (1) 收集训练中使用的真实眼镜数据集; (2) 选择生成器和鉴别器的架构, 实例化它们的权值; (3) 训练可用于评估攻击的DNNs; (4) 设定攻击的参数。</p>
                </div>
                <h4 class="anchor-tag" id="107" name="107"><b>3.1 数据集采集</b></h4>
                <div class="p1">
                    <p id="108">一个真实的眼镜框架设计数据集是必要的, 以训练生成器创建真实的攻击。我们使用谷歌搜索的搜索“眼镜”及其同义词 (如“眼镜”、“护目镜”) , 有时使用形容词修饰, 我们使用的形容词主要包括颜色 (如“棕色”、“蓝色”) 、趋势 (如“极客”、“龟甲”) 和品牌 (如“拉夫·劳伦”、“普拉达”) 。总共做了430个独特的API查询, 收集了26 520张图片。</p>
                </div>
                <div class="p1">
                    <p id="109">收集的图像不仅仅是眼镜; 我们还发现了杯子、花瓶和眼镜品牌的标识阻碍了训练过程;此外, 这些图像还包括模特佩戴的眼镜和深色背景下的眼镜图像。我们发现这些图像很难用生成网络进行建模。因此, 我们训练了一个分类器来帮助我们检测和保存在白色背景下的眼镜图像, 从而不包括模特们佩戴时的图像。使用250张手工标记的图片, 训练了一个分类器, 并将其调整为100%的精确度和65%的召回率。在对数据集中的所有图像进行应用后, 仍有8 340幅眼镜图像, 手动检查这些图片并没有发现假阳性的一个子集。</p>
                </div>
                <div class="p1">
                    <p id="110">使用这个数据集的原始图像, 可以训练一个能发出不同图案、形状和方向的眼镜的生成器。不幸的是, 形状和方向的变化使得这种眼镜在运行算法1时难以有效和合理地对准人脸图像。因此对数据集中的图像进行预处理, 并将模式从它们的帧转移到一个固定的形状, 可以很容易地对齐到人脸图像。我们使用的形状的剪影如图5所示, 然后我们训练生成器生成这种形状的眼镜的图像, 但是它们有不同的颜色和质地。将眼镜的颜色和纹理转换成固定的形状, 对图像进行了识别, 以检测帧的区域。然后使用纹理合成技术将框架的纹理合成到固定形状上, 图6显示了纹理合成结果的示例。</p>
                </div>
                <div class="area_img" id="111">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201905028_111.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 眼镜生成轮廓" src="Detail/GetImg?filename=images/JYRJ201905028_111.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 眼镜生成轮廓  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201905028_111.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="112">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201905028_112.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 原始眼镜图像 (左) 与合成眼镜图像 (右)" src="Detail/GetImg?filename=images/JYRJ201905028_112.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 原始眼镜图像 (左) 与合成眼镜图像 (右)   <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201905028_112.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="113" name="113"><b>3.2 实验设置与结果分析</b></h4>
                <h4 class="anchor-tag" id="114" name="114"><b>3.2.1 预训练生成器与判别器</b></h4>
                <div class="p1">
                    <p id="115">当训练GANs时, 我们希望生成器构造出清晰的、真实的、多样的图像。只发射一小组图像表明生成器的功能不能很好地接近底层数据分布。为了实现这些目标, 也为了能够进行有效的训练, 我们选择了深度卷积生成对抗网络DCGAN (Deep Convolutional GAN) , 一个具有少量参数的极简主义架构。然后探索了发生器潜在空间的各种可能性, 输出维度, 以及<i>G</i>和<i>D</i>中的权重数 (通过调整过滤器的深度) 。最终发现一个潜在的空间[-1; 1]25, (即二十五维25-dimensional向量之间的实数-1和1) , 和输出的包含64×176像素的图像可产生最好看、最多样化的效果。</p>
                </div>
                <div class="p1">
                    <p id="116">为了确保攻击能够迅速融合, 将<i>G</i>和<i>D</i>初始化到一个状态, 在这个状态中, 生成器才能够有效生成真实的眼镜图像。为此选择了200次迭代进行预训练, 并存储它们以初始化以后的运行。此外, 本文还借鉴了Salimans提出的基于软标签来训练生成器。图7和图8展示了在训练结束时生成器器生成的眼镜图像。</p>
                </div>
                <div class="area_img" id="117">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201905028_117.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 对抗眼镜图像" src="Detail/GetImg?filename=images/JYRJ201905028_117.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 对抗眼镜图像  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201905028_117.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="118">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201905028_118.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 原始人脸图像 (左) 与对抗人脸图像 (右)" src="Detail/GetImg?filename=images/JYRJ201905028_118.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图8 原始人脸图像 (左) 与对抗人脸图像 (右)   <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201905028_118.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="119" name="119"><b>3.2.2 人脸识别模型</b></h4>
                <div class="p1">
                    <p id="120">本文评估了对两个体系结构中的DNNs的攻击。一个神经网络是建立在超分辨率测试序列 (VGG) 神经网络上<citation id="140" type="reference"><link href="35" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>。最初的VGG DNN在人面数据库 (LFW) 基准测试中, 在被标记的面孔上显示了最先进的结果, 以98.95%的准确率进行面部验证<citation id="141" type="reference"><link href="37" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>。另外一个DNN是在OpenFace神经网络上构建的, 它使用了谷歌FaceNet体系结构<citation id="142" type="reference"><link href="39" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>。OpenFace的主要设计考虑是提供高精度DNN, 低训练和预测时间, 使DNN可以部署在移动设备和物联网设备上。</p>
                </div>
                <div class="p1">
                    <p id="121">VGG网络训练: 原始的VGG网络需要一个224×224像素对齐的脸部图像用以输入, 并产生了一个具有高度鉴别性的4 096维的面部描述符 (即用矢量表示法展示脸部图像) 。在欧几里德空间中, 两个描述同一人的图像的描述符比两个描述不同人的图像的描述符更接近。使用描述符来训练两个简单的神经网络, 将图幅面积描述符映射到身份集合上的概率提高。如此, 原来的VGG网络就有效地充当了特征提取器的角色。</p>
                </div>
                <div class="p1">
                    <p id="122">OpenFace网络训练:原始的OpenFace网络需要一个96×96像素对齐的脸部图像作为输入和输出128维的描述符。与VGG网络相似, 同一个人的图像描述符在欧几里得空间中接近, 而不同人物形象的描述符却相差甚远。与VGG相反, OpenFace的描述符位于一个单位球面上。首先尝试训练神经网络, 使用与VGG DNNs相似的架构将OpenFace描述符映射到身份数据集, 找到了这些神经网络来得到有竞争力的精确度。然而, 与VGG DNNs类似, 它们也很容易受到闪避的攻击。与VGG DNNs不同, 简单的数据增加并不能提高DNNs的稳健性。我们认为这可能是由于使用线性分隔符对球面数据进行分类的局限性。</p>
                </div>
                <h4 class="anchor-tag" id="123" name="123"><b>3.2.3 结果分析</b></h4>
                <div class="p1">
                    <p id="124">为了评估提出的攻击策略的攻击性能, 随机为VGG和OpenFace选取10个攻击者。对于每个攻击者和DNN的组合, 使用攻击者的单个人脸图像来创建定向或非定向攻击。在非定向攻击中, 目标是随机选择的。为了避免评估成功率的不确定性, 本文使用攻击者的三个不同的图像重复每一次攻击。表1描述了模型的测试准确率以及被攻击时的平均成功率和标准误差。实验结果表示基于GAN生成的眼镜贴片成功攻击了用于人脸识别的VGG和OpenFace模型。对于非定向而言更具挑战性, 成功率有所降低。</p>
                </div>
                <div class="area_img" id="125">
                    <p class="img_tit"><b>表1 人脸识别模型性能</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="125" border="1"><tr><td>DNN模型</td><td>受试<br />者数</td><td>测试<br />准确率</td><td>定向攻击<br />成功率</td><td>非定向攻击<br />成功率</td></tr><tr><td><br />VGG</td><td>10</td><td>98.9%</td><td>97.5%±0.2%</td><td>100%±0.0%</td></tr><tr><td><br />OpenFace</td><td>10</td><td>99.2%</td><td>96.3%±0.3%</td><td>100%±0.1%</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="126">然后进一步探讨了生成的攻击样本的迁移性, 即在黑盒攻击场景中, 提出的基于GAN的对抗眼镜贴片的有效性, 实验结果如表2所示。实验结果表明提出的饿攻击策略在黑盒攻击领域仍获得了不错的攻击效果。另外, 还发现OpenFace体系结构的攻击仅在有限次的尝试中就成功地愚弄了VGG体系结构 (10%～12%) 。相比之下, 在63.33%的尝试中, 成功攻击VGG的同时也成功攻击了OpenFace。</p>
                </div>
                <div class="area_img" id="127">
                    <p class="img_tit"><b>表2 黑盒攻击性能</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="127" border="1"><tr><td><br />DNN模型</td><td>VGG</td><td>OpenFace</td></tr><tr><td><br />VGG</td><td>-</td><td>63.33%</td></tr><tr><td><br />OpenFace</td><td>10.00%</td><td>-</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h3 id="128" name="128" class="anchor-tag"><b>4 结 语</b></h3>
                <div class="p1">
                    <p id="129">本文旨在探索深度学习模型存在的脆弱性并运用生成对抗网络, 设计出一种新颖的光亮眼镜贴片样本, 能够成功欺骗基于卷积神经网络的人脸识别系统。通过在收集的眼镜数据集上进行合成实验, 并在OpenFace、VGG等常用的人脸识别模型上验证了所提想法的性能, 并证明了现实世界中深度学习模型的脆弱性依然不容忽视, 设计有效的防御机制成为未来研究的重点。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Towards the Science of Security and Privacy in Machine Learning[EB]">

                                <b>[1]</b> Papernot N, Mcdaniel P, Sinha A, et al.Towards the Science of Security and Privacy in Machine Learning[EB].arXiv:1611.03814, 2016.
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00003762447&amp;v=MTkwMjhZM2s1ekJkaDRqOTlTWHFScnhveGNNSDdSN3FkWitadUZpdmxWNzdJSTE0PU5qN0Jhck80SHRIUHFJbEhZTzhJ&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> Barreno M, Nelson B, Joseph A D, et al.The security of machine learning[J].Machine Learning, 2010, 81 (2) :121-148.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The limitations of deep learning in adversarial settings">

                                <b>[3]</b> Papernot N, Mcdaniel P, Jha S, et al.The Limitations of Deep Learning in Adversarial Settings[C]//2016 IEEE European Symposium on Security and Privacy (EuroS&amp;P) .IEEE, 2016:372-387.
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Detecting Adversarial Samples from Artifacts[EB]">

                                <b>[4]</b> Feinman R, Curtin R R, Shintre S, et al.Detecting Adversarial Samples from Artifacts[EB].arXiv:1703.00410, 2017.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Adversarial examples in the physical world[EB]">

                                <b>[5]</b> Kurakin A, Goodfellow I, Bengio S.Adversarial examples in the physical world[EB].arXiv:1607.02533, 2016.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Boosting Adversarial Attacks with Momentum[EB]">

                                <b>[6]</b> Dong Y, Liao F, Pang T, et al.Boosting Adversarial Attacks with Momentum[EB].arXiv:1710.06081, 2017.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Ensemble Adversarial Training:Attacks and Defenses[EB]">

                                <b>[7]</b> Tramèr F, Kurakin A, Papernot N, et al.Ensemble Adversarial Training:Attacks and Defenses[EB].arXiv:1705.07204, 2017.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Towards evaluating the robustness of neural networks">

                                <b>[8]</b> Carlini N, Wagner D.Towards Evaluating the Robustness of Neural Networks[C]//2017 IEEE Symposium on Security and Privacy (SP) .IEEE, 2017:39-57.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Transferability in Machine Learning:from Phenomena to Black-Box Attacks using Adversarial Samples[EB]">

                                <b>[9]</b> Papernot N, Mcdaniel P, Goodfellow I.Transferability in Machine Learning:from Phenomena to Black-Box Attacks using Adversarial Samples[EB].arXiv:1605.07277, 2016.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Intriguing properties of neural networks[EB]">

                                <b>[10]</b> Szegedy C, Zaremba W, Sutskever I, et al.Intriguing properties of neural networks[EB].arXiv:1312.6199, 2013.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The Space of Transferable Adversarial Examples[EB]">

                                <b>[11]</b> Tramèr F, Papernot N, Goodfellow I, et al.The Space of Transferable Adversarial Examples[EB].arXiv:1704.03453, 2017.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Delving into Transferable Adversarial Examples and Black-box Attacks[EB]">

                                <b>[12]</b> Liu Y, Chen X, Liu C, et al.Delving into Transferable Adversarial Examples and Black-box Attacks[EB].arXiv:1611.02770, 2016.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Explaining and Harnessing Adversarial Examples[EB]">

                                <b>[13]</b> Goodfellow I J, Shlens J, Szegedy C.Explaining and Harnessing Adversarial Examples[EB].arXiv:1412.6572, 2014.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Adversarial Machine Learning at Scale[EB]">

                                <b>[14]</b> Kurakin A, Goodfellow I, Bengio S.Adversarial Machine Learning at Scale[EB].arXiv:1611.01236, 2016.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_15" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00000425224&amp;v=MDUzNjErWnVGaXZsVjc3SUkxND1OajdCYXJPNEh0SE1xNDFBWnVrTFkzazV6QmRoNGo5OVNYcVJyeG94Y01IN1I3cWRa&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[15]</b> Boer P T D, Kroese D P, Mannor S, et al.A Tutorial on the Cross-Entropy Method[J].Annals of Operations Research, 2005, 134 (1) :19-67.
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Magnet:a two-pronged defense against adversarial examples">

                                <b>[16]</b> Meng D, Chen H.Magnet:a two-pronged defense against adversarial examples[C]//Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security.ACM, 2017:135-147.
                            </a>
                        </p>
                        <p id="35">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep Face Recognition">

                                <b>[17]</b> Parkhi O M, Vedaldi A, Zisserman A.Deep Face Recognition[C]//British Machine Vision Conference 2015.2015.
                            </a>
                        </p>
                        <p id="37">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Labeled Faces in the Wild: A Database for Studying FaceRecognition in Unconstrained Environments">

                                <b>[18]</b> Huang G B, Ramesh M, Berg T, et al.Labeled faces in the wild:A database for studying face recognition in unconstrained environments[R].Technical Report 07-49, University of Massachusetts, Amherst, October 2007.
                            </a>
                        </p>
                        <p id="39">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Open Face:A general-purpose face recognition library with mobile applications">

                                <b>[19]</b> Amos B, Ludwiczuk B, Satyanarayanan M.OpenFace:A general-purpose face recognition library with mobile applications[R].Technical report, CMU-CS-16-118, CMU School of Computer Science, 2016.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JYRJ201905028" />
        <input id="dpi" type="hidden" value="600" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201905028&amp;v=MjgxMjhadVp0Rnl6a1U3eklMelRaWkxHNEg5ak1xbzlIYklRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWY=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
