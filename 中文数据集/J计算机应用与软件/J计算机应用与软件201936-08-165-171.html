<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637135610735002500%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJYRJ201908030%26RESULT%3d1%26SIGN%3dDNmzTLxk0EdpaJW%252bIx%252fsX2wjN%252bY%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JYRJ201908030&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JYRJ201908030&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201908030&amp;v=MDMxNzdyM0tMelRaWkxHNEg5ak1wNDlHWklRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVp0RnlqbVY=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#35" data-title="&lt;b&gt;0 引 言&lt;/b&gt; "><b>0 引 言</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#39" data-title="&lt;b&gt;1 整体框架&lt;/b&gt; "><b>1 整体框架</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#43" data-title="&lt;b&gt;1.1 环境建模&lt;/b&gt;"><b>1.1 环境建模</b></a></li>
                                                <li><a href="#46" data-title="&lt;b&gt;1.2 任务分配&lt;/b&gt;"><b>1.2 任务分配</b></a></li>
                                                <li><a href="#48" data-title="&lt;b&gt;1.3 多Agent路径规划算法&lt;/b&gt;"><b>1.3 多Agent路径规划算法</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#50" data-title="&lt;b&gt;2 基于强化学习的多Agent路径规划&lt;/b&gt; "><b>2 基于强化学习的多Agent路径规划</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#51" data-title="&lt;b&gt;2.1 强化学习相关理论&lt;/b&gt;"><b>2.1 强化学习相关理论</b></a></li>
                                                <li><a href="#63" data-title="&lt;b&gt;2.2 多Agent路径规划&lt;/b&gt;"><b>2.2 多Agent路径规划</b></a></li>
                                                <li><a href="#67" data-title="&lt;b&gt;2.3 多Agent路径规划学习算法实现&lt;/b&gt;"><b>2.3 多Agent路径规划学习算法实现</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#113" data-title="&lt;b&gt;3 实验仿真与分析&lt;/b&gt; "><b>3 实验仿真与分析</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#114" data-title="&lt;b&gt;3.1 实验设置&lt;/b&gt;"><b>3.1 实验设置</b></a></li>
                                                <li><a href="#127" data-title="&lt;b&gt;3.2 实验结果与分析&lt;/b&gt;"><b>3.2 实验结果与分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#141" data-title="&lt;b&gt;4 结 语&lt;/b&gt; "><b>4 结 语</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#41" data-title="图1 整体框架图">图1 整体框架图</a></li>
                                                <li><a href="#45" data-title="图2 栅栏环境图">图2 栅栏环境图</a></li>
                                                <li><a href="#53" data-title="图3 强化学习基本模型">图3 强化学习基本模型</a></li>
                                                <li><a href="#65" data-title="图4 多Agent路径规划学习框架">图4 多Agent路径规划学习框架</a></li>
                                                <li><a href="#116" data-title="图5 实验环境">图5 实验环境</a></li>
                                                <li><a href="#125" data-title="&lt;b&gt;表1 更新函数的参数设置&lt;/b&gt;"><b>表1 更新函数的参数设置</b></a></li>
                                                <li><a href="#130" data-title="图6 Agent1实验结果图">图6 Agent1实验结果图</a></li>
                                                <li><a href="#132" data-title="图7 Agent2首次实验运动轨迹">图7 Agent2首次实验运动轨迹</a></li>
                                                <li><a href="#134" data-title="图8 最终路径规划结果">图8 最终路径规划结果</a></li>
                                                <li><a href="#137" data-title="图9 回合训练结果">图9 回合训练结果</a></li>
                                                <li><a href="#139" data-title="图10 总探索步数">图10 总探索步数</a></li>
                                                <li><a href="#140" data-title="图11 总路径平均步数">图11 总路径平均步数</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" title=" Hildebrandt A C, Klischat M, Wahrmann D, et al.Real-Time Path Planning in Unknown Environments for Bipedal Robots[J].IEEE Robotics and Automation Letters, 2017, 2 (4) :1856-1863." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Real-Time Path Planning in Unknown Environments for Bipedal Robots">
                                        <b>[1]</b>
                                         Hildebrandt A C, Klischat M, Wahrmann D, et al.Real-Time Path Planning in Unknown Environments for Bipedal Robots[J].IEEE Robotics and Automation Letters, 2017, 2 (4) :1856-1863.
                                    </a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" title=" Yu H, Meier K, Argyle M, et al.Cooperative Path Planning for Target Tracking in Urban Environments Using Unmanned Air and Ground Vehicles[J].IEEE/ASME Transactions on Mechatronics, 2015, 20 (2) :541-552." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Cooperative path planning for target tracking in urban environments using unmanned air and ground vehicles,&amp;quot;">
                                        <b>[2]</b>
                                         Yu H, Meier K, Argyle M, et al.Cooperative Path Planning for Target Tracking in Urban Environments Using Unmanned Air and Ground Vehicles[J].IEEE/ASME Transactions on Mechatronics, 2015, 20 (2) :541-552.
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title=" Yang P, Tang K, Lozano J A, et al.Path Planning for Single Unmanned Aerial Vehicle by Separately Evolving Waypoints[J].IEEE Transactions on Robotics, 2015, 31 (5) :1130-1146." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Path Planning for Single Unmanned Aerial Vehicle by Separately Evolving Waypoints">
                                        <b>[3]</b>
                                         Yang P, Tang K, Lozano J A, et al.Path Planning for Single Unmanned Aerial Vehicle by Separately Evolving Waypoints[J].IEEE Transactions on Robotics, 2015, 31 (5) :1130-1146.
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" title=" 熊超, 解武杰, 董文瀚.基于碰撞锥改进人工势场的无人机避障路径规划[J].计算机工程, 2018, 44 (9) :314-320." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201809051&amp;v=MDM0NjhadVp0RnlqbVZyM0tMejdCYmJHNEg5bk1wbzlBWllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWY=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                         熊超, 解武杰, 董文瀚.基于碰撞锥改进人工势场的无人机避障路径规划[J].计算机工程, 2018, 44 (9) :314-320.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" title=" 姜涛, 王建中, 施家栋.小型移动机器人自主返航路径规划方法[J].计算机工程, 2015, 41 (1) :164-168." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201501030&amp;v=MTgxMTF0RnlqbVZyM0tMejdCYmJHNEg5VE1ybzlHWklRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVo=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                         姜涛, 王建中, 施家栋.小型移动机器人自主返航路径规划方法[J].计算机工程, 2015, 41 (1) :164-168.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" title=" 刘洁, 赵海芳, 周德廉.一种改进量子行为粒子群优化算法的移动机器人路径规划[J].计算机科学, 2017, 44 (S2) :123-128." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJA2017S2026&amp;v=Mjc4NDU3RzRIOWF2clk5SFlvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVadEZ5am1WcjNLTHo3QmI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                         刘洁, 赵海芳, 周德廉.一种改进量子行为粒子群优化算法的移动机器人路径规划[J].计算机科学, 2017, 44 (S2) :123-128.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" title=" 赵晓, 王铮, 黄程侃, 等.基于改进A*算法的移动机器人路径规划[J].机器人, 2018, 40 (6) :903-910." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JQRR201806016&amp;v=MTIxNjU3cWZadVp0RnlqbVZyM0tMenpaZkxHNEg5bk1xWTlFWW9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                         赵晓, 王铮, 黄程侃, 等.基于改进A*算法的移动机器人路径规划[J].机器人, 2018, 40 (6) :903-910.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" title=" 游晓明, 刘升, 吕金秋.一种动态搜索策略的蚁群算法及其在机器人路径规划中的应用[J].控制与决策, 2017, 32 (3) :552-556." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=KZYC201703024&amp;v=MDMwMDJmU2JiRzRIOWJNckk5SFlJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVadEZ5am1WcjNLTGo=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                         游晓明, 刘升, 吕金秋.一种动态搜索策略的蚁群算法及其在机器人路径规划中的应用[J].控制与决策, 2017, 32 (3) :552-556.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" title=" Lamini C, Fathi Y, Benhlima S.Collaborative Q-learning path planning for autonomous robots based on holonic multi-agent system[C]//2015 10th International Conference on Intelligent Systems:Theories and Applications (SITA) .Rabat, 2015:1-6." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Collaborative Q-learning path planning for autonomous robots based on holonic multi-agent system">
                                        <b>[9]</b>
                                         Lamini C, Fathi Y, Benhlima S.Collaborative Q-learning path planning for autonomous robots based on holonic multi-agent system[C]//2015 10th International Conference on Intelligent Systems:Theories and Applications (SITA) .Rabat, 2015:1-6.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_10" title=" 冯涛.基于协同进化算法的多机器人路径规划研究[D].南京:南京邮电大学, 2015." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=1015730865.nh&amp;v=MDAyMzgzS1ZGMjZHN1M3SHRuS3FwRWJQSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnRGeWptVnI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[10]</b>
                                         冯涛.基于协同进化算法的多机器人路径规划研究[D].南京:南京邮电大学, 2015.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_11" title=" 李东正, 郝燕玲, 张振兴.基于主从结构的多水下机器人协同路径规划[J].计算机仿真, 2015, 32 (1) :382-387." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJZ201501084&amp;v=MjQxMzVOWUlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVp0RnlqbVZyM0tMejdCZExHNEg5VE1ybzk=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[11]</b>
                                         李东正, 郝燕玲, 张振兴.基于主从结构的多水下机器人协同路径规划[J].计算机仿真, 2015, 32 (1) :382-387.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_12" title=" 郑延斌, 李波, 安德宇, 等.基于分层强化学习及人工势场的多Agent路径规划方法[J].计算机应用, 2015, 35 (12) :3491-3496." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201512036&amp;v=MTY0MDBESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVadEZ5am1WcjNLTHo3QmQ3RzRIOVROclk5R1lvUUs=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[12]</b>
                                         郑延斌, 李波, 安德宇, 等.基于分层强化学习及人工势场的多Agent路径规划方法[J].计算机应用, 2015, 35 (12) :3491-3496.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_13" title=" 刘敬一.自动化仓储调度系统中多AGV路径规划的研究与实现[D].沈阳:中国科学院大学 (中国科学院沈阳计算技术研究所) , 2018." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=1018841663.nh&amp;v=MjMzNDBWRjI2RnJ1OEg5ZktySkViUElRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVp0RnlqbVZyM0s=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[13]</b>
                                         刘敬一.自动化仓储调度系统中多AGV路径规划的研究与实现[D].沈阳:中国科学院大学 (中国科学院沈阳计算技术研究所) , 2018.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_14" title=" Wang Z, Shi Z, Li Y, et al.The optimization of path planning for multi-robot system using Boltzmann Policy based Q-learning algorithm[C]//2013 IEEE International Conference on Robotics and Biomimetics (ROBIO) .Shenzhen, 2013:1199-1204." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The optimization of path planning for multi-robot system using Boltzmann Policy based Q-learning algorithm">
                                        <b>[14]</b>
                                         Wang Z, Shi Z, Li Y, et al.The optimization of path planning for multi-robot system using Boltzmann Policy based Q-learning algorithm[C]//2013 IEEE International Conference on Robotics and Biomimetics (ROBIO) .Shenzhen, 2013:1199-1204.
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_15" title=" Sutton R S, Barto A G.Reinforcement Learning:An introduction[M].Cambridge, Massachusetts:MIT Press, 1998." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Reinforcement learning: an introduction">
                                        <b>[15]</b>
                                         Sutton R S, Barto A G.Reinforcement Learning:An introduction[M].Cambridge, Massachusetts:MIT Press, 1998.
                                    </a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_16" title=" Kim J, Shin S, Wu J, et al.Obstacle Avoidance Path Planning for UAV Using Reinforcement Learning Under Simulated Environment[C]//IASER 3rd International Conference on Electronics, Electrical Engineering.Okinawa, 2017:34-36." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Obstacle Avoidance Path Planning for UAV Using Reinforcement Learning Under Simulated Environment">
                                        <b>[16]</b>
                                         Kim J, Shin S, Wu J, et al.Obstacle Avoidance Path Planning for UAV Using Reinforcement Learning Under Simulated Environment[C]//IASER 3rd International Conference on Electronics, Electrical Engineering.Okinawa, 2017:34-36.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JYRJ" target="_blank">计算机应用与软件</a>
                2019,36(08),165-171 DOI:10.3969/j.issn.1000-386x.2019.08.029            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于强化学习的多Agent路径规划方法研究</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E6%AF%85%E7%84%B6&amp;code=42373482&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王毅然</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%BB%8F%E5%B0%8F%E5%B7%9D&amp;code=36012078&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">经小川</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%94%B0%E6%B6%9B&amp;code=36012197&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">田涛</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%AD%99%E8%BF%90%E4%B9%BE&amp;code=36473087&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">孙运乾</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E4%BB%8E%E5%B8%85%E5%86%9B&amp;code=36012191&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">从帅军</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E8%88%AA%E5%A4%A9%E7%B3%BB%E7%BB%9F%E7%A7%91%E5%AD%A6%E4%B8%8E%E5%B7%A5%E7%A8%8B%E7%A0%94%E7%A9%B6%E9%99%A2&amp;code=1700437&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国航天系统科学与工程研究院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>以复杂任务下多个智能体路径规划问题为研究对象, 提出一种基于强化学习的多Agent路径规划方法。该方法采用无模型的在线Q学习算法, 多个Agent不断重复“探索-学习-利用”过程, 积累历史经验评估动作策略并优化决策, 完成未知环境下的多Agent的路径规划任务。仿真结果表明, 与基于强化学习的单Agent路径规划方法相比, 该方法在多Agent避免了相碰并成功躲避障碍物的前提下, 减少了17.4%的总探索步数, 形成了到达目标点的最短路径。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">多智能体;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">强化学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%B7%AF%E5%BE%84%E8%A7%84%E5%88%92&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">路径规划;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Q%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Q学习算法;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%9C%AA%E7%9F%A5%E7%8E%AF%E5%A2%83&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">未知环境;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    王毅然, 硕士生, 主研领域:人工智能, 计算机应用。;
                                </span>
                                <span>
                                    经小川, 研究员。;
                                </span>
                                <span>
                                    田涛, 博士。;
                                </span>
                                <span>
                                    孙运乾, 工程师。;
                                </span>
                                <span>
                                    从帅军, 工程师。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-01-10</p>

                    <p>

                            <b>基金：</b>
                                                        <span>广东省科技厅应用型研发基金专项 (2016B010127005);</span>
                    </p>
            </div>
                    <h1><b>MULTI-AGENT PATH PLANNING BASED ON REINFORCEMENT LEARNING</b></h1>
                    <h2>
                    <span>Wang Yiran</span>
                    <span>Jing Xiaochuan</span>
                    <span>Tian Tao</span>
                    <span>Sun Yunqian</span>
                    <span>Cong Shuaijun</span>
            </h2>
                    <h2>
                    <span>China Academy of Aerospace System Science and Engineering</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Taking multiple agents path planning problems under complex tasks as the research object, we proposed a multi-agent path planning method based on reinforcement learning. The method adopted a model-free online Q learning algorithm. In this method, a model-free online Q-learning algorithm was adopted. Many agents repeated the process of "exploration-learning-utilization", accumulated historical experience, evaluated action strategies and optimized decision-making, and completed the task of multi-agent path planning in unknown environment. The simulation results show that compared with the single agent path planning method based on reinforcement learning, this method reduces the total exploration steps by 17.4% and forms the shortest path to the target point on the premise that multi-agent avoids collision and successfully avoids obstacles.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Multi-agent&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Multi-agent;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Reinforcement%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Reinforcement learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Path%20planning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Path planning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Q%20learing%20algorithm&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Q learing algorithm;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Unknown%20environment&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Unknown environment;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2019-01-10</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="35" name="35" class="anchor-tag"><b>0 引 言</b></h3>
                <div class="p1">
                    <p id="36">随着科学技术的不断发展, 路径规划技术的研究成果已经广泛应用人类生产和生活的各个方面。如在地震救灾中, 无人机能够自主躲避障碍物, 规划一组较优的路径到达指定灾区, 完成灾情获取任务;在军事领域中, 无人机和机器人在完成情报侦察以及作战打击任务过程中, 要躲避敌方威胁和避免相撞, 规划一条较优路径完成任务<citation id="147" type="reference"><link href="3" rel="bibliography" /><link href="5" rel="bibliography" /><link href="7" rel="bibliography" /><link href="9" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>,<a class="sup">3</a>,<a class="sup">4</a>]</sup></citation>。随着工作任务变得越来越复杂, 往往需要多个智能体协同完成任务, 每个智能体均是环境中的一部分, 个体采取行动均会造成环境的改变, 此时在动态环境中, 单个智能体和其他智能体之间的协调与避障是多个智能体路径规划亟需解决的问题。路径规划的目标是寻找一条从给定的起始点到终止点的较优的运动路径。单智能体的路径规划在一个环境中的状态是有限的, 目前解决的方法主要有Dijkstra算法<citation id="143" type="reference"><link href="11" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>、粒子群算法<citation id="144" type="reference"><link href="13" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>、A*算法<citation id="145" type="reference"><link href="15" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>、遗传算法、模拟退火算法、蚁群算法<citation id="146" type="reference"><link href="17" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>等。多智能体系统与单个智能体相比, 往往能够完成复杂艰巨任务, 且通常能够付出更小的代价收获更大的整体效益, 因此多个智能体的路径规划研究具有十分重要的意义。</p>
                </div>
                <div class="p1">
                    <p id="37">多智能体系统是由具有一定自主性、能够在共同目标窗口内协作、竞争和通信的协作智能Agent组成的<citation id="148" type="reference"><link href="19" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>。单个Agent解决问题的能力是有限的, 复杂任务需要多个Agent协同合作, 共同完成整体或局部目标。如果在同一环境中存在多个Agent同时移动, 对其进行路径规划将会变得十分困难。目前解决多智能体的路径规划问题取得了一些进展, 文献<citation id="149" type="reference">[<a class="sup">10</a>]</citation>提出了免疫协同进化算法并仿真实现静态障碍物环境中多个机器人避障、避碰的最短路径;文献<citation id="150" type="reference">[<a class="sup">11</a>]</citation>提出了一种主从结构的并行多水下机器人协同路径规划算法, 子层结构应用粒子群并行算法, 生成各个机器人当前的最优路径, 同时主层结构应用微分进化算法实时给出当前考虑机器人与障碍物、机器人与机器人之间避碰情况下, 总系统运行时间最短的路径组合方案;文献<citation id="151" type="reference">[<a class="sup">12</a>]</citation>提出了一种基于分层强化学习及人工势场的多Agent路径规划算法, 首先将多Agent的运行环境虚拟为一个人工势能场, 根据先验知识确定每点的势能值, 它代表最优策略可获得的最大回报, 其次利用分层强化学习方法的无环境模型学习进行策略更新;文献<citation id="152" type="reference">[<a class="sup">13</a>]</citation>提出了首先利用A-Star算法启发式地得到多个智能体到达目标点的临时最短路径, 同时计算访问节点的时间, 通过动态地对时间窗进行精确计算和加锁来重置路线以避免冲突。</p>
                </div>
                <div class="p1">
                    <p id="38">为解决未知环境下多个Agent路径规划问题, 上述算法随着Agent的数量以及环境规模变大时, 算法的效率会变得很低。本文提出了一种基于强化学习的多Agent路径规划方法 (Multi-agent path planning based on reinforcement learning, MAPP-RL) , 该方法中的多个Agent不断地与环境交互, 当采取一个动作后, Agent会从环境中得到一个反馈, 用来评估该动作的好坏, 然后把评估结果作为历史经验, 不断地进行优化决策, 最后找到一个可以得到最大奖励的动作序列, 完成复杂未知环境下的多Agent路径规划任务。</p>
                </div>
                <h3 id="39" name="39" class="anchor-tag"><b>1 整体框架</b></h3>
                <div class="p1">
                    <p id="40">多智能体的路径规划整体框架主要包括四个层次:环境建模层、算法层、任务分配层、多Agent系统层, 如图1所示。</p>
                </div>
                <div class="area_img" id="41">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201908030_041.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 整体框架图" src="Detail/GetImg?filename=images/JYRJ201908030_041.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 整体框架图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201908030_041.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="42">在图1中, 首先对环境进行建模, 包括对环境中障碍、目标点等信息设置, 其次通过任务分配层主要根据实际任务划分多个子任务, 然后算法层接收环境信息以及多个Agent信息和任务分配情况, 并进行计算, 将结果返回给Agent。多Agent系统层与环境建模层、任务分配层、算法层进行交互, 每个Agent均能执行动作与环境交互, 同时也和任务分配模块的任务进行匹配, 通过执行算法层, 不断地更新策略, 最后得到一组较优策略完成多个Agent的路径规划任务。</p>
                </div>
                <h4 class="anchor-tag" id="43" name="43"><b>1.1 环境建模</b></h4>
                <div class="p1">
                    <p id="44">对环境地图的建模常用的方法主要有三种:栅栏地图建模、拓扑地图建模和可视地图建模。本文采用的是栅栏建模法, 如图2所示将环境分成<i>n</i><sup>2</sup>个面积相同的方格, 每个方格均携带不同0～3的参数信息, 当格子参数为0时表示该区域无障碍物, 当格子参数为1时表示该区域含有障碍物, 当格子参数为2时表示智能体的位置信息, 当格子参数为3时表示目标点的位置信息。通过构建栅栏地图, 能够很好地获取环境的信息。</p>
                </div>
                <div class="area_img" id="45">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201908030_045.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 栅栏环境图" src="Detail/GetImg?filename=images/JYRJ201908030_045.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 栅栏环境图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201908030_045.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="46" name="46"><b>1.2 任务分配</b></h4>
                <div class="p1">
                    <p id="47">任务分配是多智能体协同合作中的一个重要研究内容。多Agent的路径规划的任务分配问题为:现假设系统环境中存在<i>m</i>个目标点, 每个目标点至少一个Agent到达, 所有目标点都有Agent到达时任务完成。该任务分配的目标是将多个目标点分别分配给Agent, 以实现整体Agent到达目标点的路径总和最短。</p>
                </div>
                <h4 class="anchor-tag" id="48" name="48"><b>1.3 多Agent路径规划算法</b></h4>
                <div class="p1">
                    <p id="49">多Agent路径规划算法主要解决的问题是多个Agent的路径规划问题。本文采用的是基于强化学习的多Agent路径规划方法, 多个Agent在同一环境中, 不断与环境交互, 根据环境的反馈进一步优化动作, 完成整体的路径规划。对多个Agent进行路径规划主要有三个目标:一是对多个Agent进行路线规划时要考虑Agent间的路径冲突问题, 避免多个Agent相撞;二是多个Agent进行路线选择时要避开障碍物;三是多个Agent到达目标点的路径总和尽可能的短。</p>
                </div>
                <h3 id="50" name="50" class="anchor-tag"><b>2 基于强化学习的多Agent路径规划</b></h3>
                <h4 class="anchor-tag" id="51" name="51"><b>2.1 强化学习相关理论</b></h4>
                <div class="p1">
                    <p id="52">强化学习是一种无监督学习方法, Agent通过与动态环境的反复交互, 学会选择最优或近最优的行为以实现其长期目标<citation id="153" type="reference"><link href="29" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>。Sutton和Barto定义了强化学习方法的四个关键要素:策略、奖赏函数、价值函数、环境模型<citation id="154" type="reference"><link href="31" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>。强化学习的基本模型主要包括环境和智能体两部分, 如图3所示。</p>
                </div>
                <div class="area_img" id="53">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201908030_053.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 强化学习基本模型" src="Detail/GetImg?filename=images/JYRJ201908030_053.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 强化学习基本模型  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201908030_053.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="54">在图3中, Agent根据当前所处的环境状态, 执行一个动作与环境交互, 从环境中得到一个奖励, 同时到达新的状态, 进行学习更新策略, 接着再执行一个动作作用于环境, 不断重复此过程, 优化策略完成任务。</p>
                </div>
                <div class="p1">
                    <p id="55">很多强化学习问题可以形式化为马尔可夫决策过程 (Markov decision process, MDP) 。MDP是由〈<i>S</i>, <i>A</i>, <b><i>P</i></b>, <i>R</i>, <i>γ</i>〉构成的一个元组, 其中:</p>
                </div>
                <div class="p1">
                    <p id="56"><i>S</i>是一个有限状态集;</p>
                </div>
                <div class="p1">
                    <p id="57"><i>A</i> 是一个有限行为集;</p>
                </div>
                <div class="p1">
                    <p id="58"><b><i>P</i></b>是集合中基于行为的状态转移概率矩阵:</p>
                </div>
                <div class="p1">
                    <p id="59" class="code-formula">
                        <mathml id="59"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">Ρ</mi><msubsup><mrow></mrow><mrow><mi>s</mi><msup><mi>s</mi><mo>′</mo></msup></mrow><mi>a</mi></msubsup><mo>=</mo><mi>E</mi><mo stretchy="false">[</mo><mi>R</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy="false">|</mo><mi>S</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mi>s</mi><mo>, </mo><mi>A</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mi>a</mi><mo stretchy="false">]</mo><mo>;</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="60"><i>R</i>是基于状态和行为的奖励函数:</p>
                </div>
                <div class="p1">
                    <p id="61" class="code-formula">
                        <mathml id="61"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>R</mi><msubsup><mrow></mrow><mi>s</mi><mi>a</mi></msubsup><mo>=</mo><mi>E</mi><mo stretchy="false">[</mo><mi>R</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy="false">|</mo><mi>S</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mi>s</mi><mo>, </mo><mi>A</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mi>a</mi><mo stretchy="false">]</mo><mo>;</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="62"><i>γ</i> 是一个衰减因子:<i>γ</i>∈[0, 1]。</p>
                </div>
                <h4 class="anchor-tag" id="63" name="63"><b>2.2 多Agent路径规划</b></h4>
                <div class="p1">
                    <p id="64">在多Agent的强化学习过程中, 每个Agent获得的奖励不仅仅取决于Agent自身的动作, 同时还依赖于其他Agent的动作。因此本文将强化学习的MDP模型扩展为多马尔科夫决策过程 (MDPs) 。现假设有<i>n</i>个智能体, 每个Agent可以选择的动作<i>m</i>个 (即<i>a</i><sub><i>i</i></sub>, <i>i</i>=1, 2, …, <i>m</i>) , 每个Agent的状态个数为<i>k</i>个 (即<i>s</i><sub><i>j</i></sub>, <i>j</i>=1, 2, …, <i>m</i>) , 则多个Agent采取的联合动作可以表示为<i>A</i><sub><i>i</i></sub>, 多个Agent的联合状态可以表示为<i>S</i><sub><i>i</i></sub>。基于强化学习的基本模型, 结合本文的任务目标, 本文定义了多Agent的路径规划学习框架, 具体情况如图4所示。</p>
                </div>
                <div class="area_img" id="65">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201908030_065.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 多Agent路径规划学习框架" src="Detail/GetImg?filename=images/JYRJ201908030_065.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 多Agent路径规划学习框架  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201908030_065.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="66">在图4中, 为了提高Agent的学习速度, 本文中我们首先对多Agent所处的环境进行了预处理操作, 剔除了一些无关的环境状态, 同时将先验信息更新到知识库, 提高了多Agent的学习效率。在该模型中, 多个Agent基于当前所处的状态<i>S</i><sub><i>t</i></sub>, 每个Agent根据知识库的历史经验, 按照一定的策略规则采取动作集中的一个动作<i>a</i><sub><i>t</i></sub>, 所有的Agent的动作组合成一次联合动作<i>A</i><sub><i>t</i></sub>作用于环境。当联合动作<i>A</i><sub><i>t</i></sub>执行完毕后, 环境将转化为一个新的状态<i>S</i><sub><i>t</i>+1</sub>, 并且得到一个新的奖励值<i>R</i><sub><i>t</i>+1</sub>。然后进行学习, 更新历史经验, 进一步完善知识库。接着根据Agen所处的新状态<i>S</i><sub><i>t</i>+1</sub>和<i>R</i><sub><i>t</i>+1</sub>选择新的联合动作<i>A</i><sub><i>t</i>+1</sub>。多个Agent与环境进行周期性交互, 不断重复“探索-学习-决策-利用”过程, 从历史动作中进行学习更新自己的知识库, 作为历史经验指导下次动作选择。</p>
                </div>
                <h4 class="anchor-tag" id="67" name="67"><b>2.3 多Agent路径规划学习算法实现</b></h4>
                <h4 class="anchor-tag" id="68" name="68"><b>2.3.1 联合状态设定准则</b></h4>
                <div class="p1">
                    <p id="69">在多Agent的路径规划问题中, 多个Agent所做的决策是基于环境的当前状态, 因此对环境的状态定义十分重要。首先不考虑Agent的位置信息, 将环境状态进行预处理操作, 剔除一些如任务无关的环境状态, 得到处理后的环境状态集合<i>S</i>={<i>s</i><sub>1</sub>, <i>s</i><sub>2</sub>, …, <i>s</i><sub><i>n</i></sub>}。然而要解决多Agent的路径规划问题, 必须考虑多个Agent的位置信息, 因此本文提出联合状态集合<i>S</i>′。现假设存在<i>m</i>个Agent, 定义:<i>C</i><mathml id="70"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>s</mi><mn>1</mn></msubsup></mrow></math></mathml>代表从集合<i>S</i>中随机选择一个状态, 其中一个联合状态可以表示为<image id="71" type="formula" href="images/JYRJ201908030_07100.jpg" display="inline" placement="inline"><alt></alt></image>。<i>m</i>个Agent的联合状态个数为<i>n</i><sup><i>m</i></sup>个。</p>
                </div>
                <h4 class="anchor-tag" id="72" name="72"><b>2.3.2 联合动作</b></h4>
                <div class="p1">
                    <p id="73">动作空间表示了系统的解决方案空间。假设每个Agent的动作集合<i>A</i>={<i>a</i><sub>1</sub>, <i>a</i><sub>2</sub>, …, <i>a</i><sub><i>k</i></sub>}, 多个Agent在同一环境中要同时行动, 因此需要引入联合动作集合<i>A</i>′, 现假设该系统环境中存在<i>m</i>个Agent, 定义:<i>C</i><mathml id="74"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>A</mi><mn>1</mn></msubsup></mrow></math></mathml>代表从集合<i>A</i>中随机选择一个动作, 则其中一个联合动作可以表示为<image id="75" type="formula" href="images/JYRJ201908030_07500.jpg" display="inline" placement="inline"><alt></alt></image>。<i>m</i>个Agent的联合动作个数为<i>k</i><sup><i>m</i></sup>个。</p>
                </div>
                <h4 class="anchor-tag" id="76" name="76"><b>2.3.3 奖励函数</b></h4>
                <div class="p1">
                    <p id="77">奖励函数定义了Agent的学习目标, 并确定了Agent基于环境的感知状态即时行动的价值。由于Agent试图最大限度地获得总报酬, 因此奖励函数本质上是用来指导Agent实现其目标的。奖励函数的设置会决定强化学习算法的收敛速度和程度。常用的奖励函数设置方法有:稀疏奖励、形式化奖励、奖励系数变化奖励等。本文采用的是稀疏奖励的形式定义奖励函数, 设置情况如下式所示:</p>
                </div>
                <div class="area_img" id="78">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JYRJ201908030_07800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="80">式中:<i>a</i>, <i>b</i>, <i>c</i>&gt;0。</p>
                </div>
                <div class="p1">
                    <p id="81">如式 (1) 所示, 多Agent的路径规划目标是让多个Agent采取一组可以获得最大奖励的动作序列, 到达指定的目标点。当Agent完成目标时, 赋予一个正的奖励;当Agent碰到静态障碍物时, 赋予一个负的奖励;当有两个或以上的Agent相互碰撞时, 赋予一个负的奖励;其他情况的奖励值为0。</p>
                </div>
                <h4 class="anchor-tag" id="82" name="82"><b>2.3.4 价值更新函数</b></h4>
                <div class="p1">
                    <p id="83">多Agent的路径规划采用的是Q-learning算法, 在确定所有联合环境状态<i>S</i>和联合动作<i>A</i>后, 要生成一个<i>n</i><sup><i>m</i></sup>×<i>k</i><sup><i>m</i></sup>维的矩阵<b><i>Q</i></b>, 矩阵中的元素<i>Q</i> (<i>S</i>, <i>A</i>) 表示为多个Agent在环境状态<i>S</i><sub><i>t</i></sub>下选择动作<i>A</i><sub><i>t</i></sub>的价值。</p>
                </div>
                <div class="p1">
                    <p id="84">更新的过程:当多个Agent在环境状态<i>S</i><sub><i>t</i></sub>下, 按照既定的动作选择策略, 选择一个联合动作<i>A</i><sub><i>t</i></sub>, 执行完动作后Agent到达一个新的环境状态<i>S</i><sub><i>t</i>+1</sub>, 这时我们开始更新矩阵<b><i>Q</i></b>中的<i>Q</i> (<i>S</i>, <i>A</i>) 值。Agent在状态<i>S</i><sub><i>t</i>+1</sub>时选择<b><i>Q</i></b>矩阵对应<i>Q</i>值最大的<i>Q</i> (<i>S</i><sub><i>t</i>+1</sub>, <i>A</i><sub><i>t</i>+1</sub>) , 然后把<i>Q</i> (<i>S</i><sub><i>t</i>+1</sub>, <i>A</i><sub><i>t</i>+1</sub>) 乘上一个衰减值<i>γ</i>并加上到达<i>S</i><sub><i>t</i>+1</sub>时所获取的奖励<i>R</i>作为现实中<i>Q</i> (<i>S</i>, <i>A</i>) 的值, 然后减去之前的<i>Q</i> (<i>S</i>, <i>A</i>) , 接着乘以一个学习效率<i>α</i>累加上最初的<i>Q</i> (<i>S</i>, <i>A</i>) 的值则更新为新的<i>Q</i> (<i>S</i>, <i>A</i>) 。具体<i>Q</i> (<i>S</i>, <i>A</i>) 值的更新公式如下式所示:</p>
                </div>
                <div class="p1">
                    <p id="85"><i>Q</i> (<i>S</i><sub><i>t</i></sub>, <i>A</i><sub><i>t</i></sub>) ←<i>Q</i> (<i>S</i><sub><i>t</i></sub>, <i>A</i><sub><i>t</i></sub>) +<i>α</i>[<i>R</i>+</p>
                </div>
                <div class="p1">
                    <p id="86"><i>γ</i>max<sub><i>A</i><sub><i>t</i>+1</sub></sub><i>Q</i> (<i>S</i><sub><i>t</i>+1</sub>, <i>A</i><sub><i>t</i>+1</sub>) -<i>Q</i> (<i>S</i><sub><i>t</i></sub>, <i>A</i><sub><i>t</i></sub>) ]      (2) </p>
                </div>
                <h4 class="anchor-tag" id="87" name="87"><b>2.3.5 动作选择策略</b></h4>
                <div class="p1">
                    <p id="88">在强化学习问题中, 探索和利用是一对矛盾:探索意味着Agent必须尝试不同的行为继而收集更多的信息, 利用则是Agent做出当前信息下的最佳决定<citation id="155" type="reference"><link href="31" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>。探索可能会牺牲一些短期利益, 通过搜集更多信息而获得较为长期准确的利益估计;利用则侧重于根据已掌握的信息而做到短期利益最大化。探索不能无止境地进行, 否则就牺牲了太多的短期利益进而导致整体利益受损;同时也不能太看重短期利益而忽视一些未探索的可能会带来巨大利益的行为。</p>
                </div>
                <div class="p1">
                    <p id="89">目前, 常用的探索方法有:ε-贪婪探索、不确定优先探索以及利用信息价值进行探索等。本文采用的是ε-贪婪探索, 这里的<i>ε</i>是Agent随机选择的概率 (0≤<i>ε</i>≤1) , 在概率为1-<i>ε</i>的情况下, Agent使用贪婪的<i>Q</i>值方法选择<i>Q</i>值最大所对应的一个动作, 当存在多个<i>Q</i>值相同的动作时随机选择一个;在概率为ε的情况下, Agent从动作集合中随机选择动作。</p>
                </div>
                <h4 class="anchor-tag" id="90" name="90"><b>2.3.6 多Agent路径规划算法步骤</b></h4>
                <div class="p1">
                    <p id="91">在多Agent的路径规划中, 多个Agent根据当前所处的环境状态, 不断地与环境进行交互, 在学习过程中对学习结果进行更新修正, 用于指导Agent的动作选择, 最终通过不断的学习, 找到一组可以最大化奖励的动作序列, 完成多Agent路径规划任务。该方法的伪代码如算法1所示。</p>
                </div>
                <div class="p1">
                    <p id="92"><b>算法1</b> 多Agent路径规划算法</p>
                </div>
                <div class="area_img" id="166">
                                <img alt="" src="Detail/GetImg?filename=images/JYRJ201908030_16600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="105">该算法的具体学习过程的形式化描述如下:</p>
                </div>
                <div class="p1">
                    <p id="106"> (1) 初始化设置:地图生成, 设置Agent和目标点的数量及初始位置, 奖励函数设置, <i>Q</i>表初始化。</p>
                </div>
                <div class="p1">
                    <p id="107"> (2) 参数设置:终止学习周期<i>T</i><sub>max</sub>, 学习效率<i>α</i>、衰减度<i>γ</i>和探索度<i>ε</i>。</p>
                </div>
                <div class="p1">
                    <p id="108"> (3) 根据ε-贪婪策略选择动作<i>A</i><sub><i>t</i></sub>。</p>
                </div>
                <div class="p1">
                    <p id="109"> (4) 执行<i>A</i><sub><i>t</i></sub>, 返回奖励值<i>R</i>和下一个状态<i>S</i><sub><i>t</i>+1</sub>。</p>
                </div>
                <div class="p1">
                    <p id="110"> (5) 按式 (2) 更新<i>Q</i>值。</p>
                </div>
                <div class="p1">
                    <p id="111"> (6) 判断是否满足终止条件:若满足终止条件, 执行 (7) ;否则, 执行 (3) 。</p>
                </div>
                <div class="p1">
                    <p id="112"> (7) <i>T</i>:=<i>T</i>+1, 判断<i>T</i>&gt;<i>T</i><sub>max</sub>:若成立, 则学习结束;否则转 (3) 。</p>
                </div>
                <h3 id="113" name="113" class="anchor-tag"><b>3 实验仿真与分析</b></h3>
                <h4 class="anchor-tag" id="114" name="114"><b>3.1 实验设置</b></h4>
                <div class="p1">
                    <p id="115">为了验证该方法的有效性, 本文多个Agent的路径规划设置了一个虚拟的环境。与文献<citation id="156" type="reference">[<a class="sup">16</a>]</citation>一样, 本文创造了不同大小的栅栏地图环境, 其中障碍和目标点是随机生成的。如图5所示, 我们设置了包含7个障碍、两个智能体、两个目标点的7×7大小的原始环境地图。</p>
                </div>
                <div class="area_img" id="116">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201908030_116.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 实验环境" src="Detail/GetImg?filename=images/JYRJ201908030_116.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 实验环境  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201908030_116.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="117">针对同一任务目标, 将文献<citation id="157" type="reference">[<a class="sup">16</a>]</citation>的方法与本文方法进行实验对比。其中文献<citation id="158" type="reference">[<a class="sup">16</a>]</citation>智能体的动作集合为{<i>U</i>, <i>D</i>, <i>L</i>, <i>R</i>, <i>S</i>}, 其中<i>U</i>代表向上, <i>D</i>代表向下, <i>L</i>代表向左, <i>R</i>代表向右, <i>S</i>代表静止不动。本文方法的两个Agent的联合动作集为:</p>
                </div>
                <div class="p1">
                    <p id="118" class="code-formula">
                        <mathml id="118"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">A</mi><mo>=</mo><mrow><mo>[</mo><mrow><mtable><mtr><mtd><mi>U</mi><mi>U</mi></mtd><mtd><mi>U</mi><mi>D</mi></mtd><mtd><mi>U</mi><mi>L</mi></mtd><mtd><mi>U</mi><mi>R</mi></mtd></mtr><mtr><mtd><mi>D</mi><mi>U</mi></mtd><mtd><mi>D</mi><mi>D</mi></mtd><mtd><mi>D</mi><mi>L</mi></mtd><mtd><mi>D</mi><mi>R</mi></mtd></mtr><mtr><mtd><mi>L</mi><mi>U</mi></mtd><mtd><mi>L</mi><mi>D</mi></mtd><mtd><mi>L</mi><mi>L</mi></mtd><mtd><mi>L</mi><mi>R</mi></mtd></mtr><mtr><mtd><mi>R</mi><mi>U</mi></mtd><mtd><mi>R</mi><mi>D</mi></mtd><mtd><mi>R</mi><mi>L</mi></mtd><mtd><mi>R</mi><mi>R</mi></mtd></mtr><mtr><mtd><mi>S</mi><mi>U</mi></mtd><mtd><mi>S</mi><mi>D</mi></mtd><mtd><mi>S</mi><mi>L</mi></mtd><mtd><mi>S</mi><mi>R</mi></mtd></mtr><mtr><mtd><mi>U</mi><mi>S</mi></mtd><mtd><mi>D</mi><mi>S</mi></mtd><mtd><mi>L</mi><mi>S</mi></mtd><mtd><mi>R</mi><mi>S</mi></mtd></mtr></mtable></mrow><mo>]</mo></mrow></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="119">其中文献<citation id="159" type="reference">[<a class="sup">16</a>]</citation>的奖励函数<i>R</i>′设置如式 (3) 所示, 本文方法的奖励函数<i>R</i>具体设置如式 (4) 所示。</p>
                </div>
                <div class="area_img" id="120">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JYRJ201908030_12000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="area_img" id="122">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JYRJ201908030_12200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="124">文献<citation id="160" type="reference">[<a class="sup">16</a>]</citation>和本文方法采用同一的学习更新函数的参数设置, 如表1所示。</p>
                </div>
                <div class="area_img" id="125">
                    <p class="img_tit"><b>表1 更新函数的参数设置</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="125" border="1"><tr><td><br />参数名</td><td>学习效率 (<i>α</i>) </td><td>衰减度 (<i>γ</i>) </td><td>探索度 (<i>ε</i>) </td></tr><tr><td><br />值</td><td>0.01</td><td>0.9</td><td>0.1</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="126">本次实验假设两个智能体在环境中同时运动, 不会出现故障情况, 每次只能选择动作集合中的一个, 环境是有边界的, 当Agent选择超出边界的动作时, 强制Agent留在环境内。任务目标是第2行第2列的Agent1到达第5行第6列的目标点, 同时第2行第4列的Agent2到达第6行第4列的目标点, 在Agent移动期间要避免相撞和避开障碍物。</p>
                </div>
                <h4 class="anchor-tag" id="127" name="127"><b>3.2 实验结果与分析</b></h4>
                <div class="p1">
                    <p id="128">为了验证本文方法的有效性, 针对上述同一任务目标, 进行两组实验, 将本文方法与文献<citation id="161" type="reference">[<a class="sup">16</a>]</citation>方法进行对比, 两组实验均训练4 000次。</p>
                </div>
                <div class="p1">
                    <p id="129">本文运用文献<citation id="162" type="reference">[<a class="sup">16</a>]</citation>方法进行仿真实验, 该方法分为两个阶段, 首先分别对每个智能体进行路径规划, 其次对发生碰撞的Agent进行动态调整。实验环境在图5基础上, 分别进行单个智能体和目标点实验。首次实验时其中第2行第2列的Agent1运动轨迹如图6 (a) 所示。在图6 (a) 中, Agent1在第5个步长时与静态障碍物发生碰撞, Agent1的动作序列分别为:{<i>D</i>→<i>R</i>→<i>R</i>→<i>D</i>→<i>D</i>}, 这是由于首次实验, Agent并没有历史经验作为决策依据, 而是随机的选择动作, 不断“试错”。经过Agent不断与环境交互, 更新<i>Q</i>表, 进行动作选择, Agent的最终路径规划路线结果如图6 (b) 所示, Agent1到达目标点的总步长为7。</p>
                </div>
                <div class="area_img" id="130">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201908030_13000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 Agent1实验结果图" src="Detail/GetImg?filename=images/JYRJ201908030_13000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 Agent1实验结果图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201908030_13000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="131">类似地, 第2行第4列的Agent2运动轨迹如图7 (a) 所示, Agent2在第4个步长时与静态障碍物发生碰撞, Agent2的动作序列为{<i>L</i>→<i>U</i>→<i>R</i>→<i>R</i>}, 经过4 000次学习, 得到的最终路径规划结果如图7 (b) 所示, Agent2到达目标点的总步长为6。</p>
                </div>
                <div class="area_img" id="132">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201908030_13200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 Agent2首次实验运动轨迹" src="Detail/GetImg?filename=images/JYRJ201908030_13200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 Agent2首次实验运动轨迹  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201908030_13200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="133">从图6 (b) 和图7 (b) 可以看出, 当两个Agent在同一环境同时移动时, 会在第2行第3列的位置相撞, 运用动态规划思想对Agent的路径重新调整, 最终的路径规划如图8所示。在图8中两个Agent在同一环境中同时移动, 且能够躲避障碍物, 两个Agent不会发生相撞, 到达目标点路径最短。</p>
                </div>
                <div class="area_img" id="134">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201908030_134.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 最终路径规划结果" src="Detail/GetImg?filename=images/JYRJ201908030_134.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图8 最终路径规划结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201908030_134.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="135">运用本文的方法, 在图5所示的环境中进行实验。首次实验时, 两个Agent经过18个步长发生了相撞。这是由于本文的方法加入了先验信息, 有历史经验作为决策支持, 首次实验时避免了对障碍的学习, 使Agent进行试错时避开了障碍。经过499次回合训练后, 两个Agent第一次到达目标点, 完成任务的总步长为50。训练4 000次后最终的路径规划结果如图9所示, 总步长为14, 其中联合动作序列为:</p>
                </div>
                <div class="p1">
                    <p id="136">{<i>DL</i>→<i>RS</i>→<i>DD</i>→ <i>RD</i>→<i>RD</i>→<i>RD</i>→<i>DR</i>}</p>
                </div>
                <div class="area_img" id="137">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201908030_137.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图9 回合训练结果" src="Detail/GetImg?filename=images/JYRJ201908030_137.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图9 回合训练结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201908030_137.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="138">为了验证本文的有效性, 本文从总探索步数、完成任务的平均步数做了对比, 具体情况如图10、图11所示。在图10中, 文献<citation id="163" type="reference">[<a class="sup">16</a>]</citation>的总探索步数是65 810步, 本文方法的总探索步数是54 375步, 由于本文方法两个Agent采取动作时要考虑双方的位置信息, 引入联合动作, 避免了对单个Agent相撞后的路径重新规划, 减少了17.4%的总探索步数。从图11得出, 本文完成任务的平均步数与文献<citation id="164" type="reference">[<a class="sup">16</a>]</citation>相比减少了5步。</p>
                </div>
                <div class="area_img" id="139">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201908030_139.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图10 总探索步数" src="Detail/GetImg?filename=images/JYRJ201908030_139.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图10 总探索步数  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201908030_139.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="140">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201908030_140.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图11 总路径平均步数" src="Detail/GetImg?filename=images/JYRJ201908030_140.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图11 总路径平均步数  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201908030_140.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h3 id="141" name="141" class="anchor-tag"><b>4 结 语</b></h3>
                <div class="p1">
                    <p id="142">为解决复杂任务下多个Agent路径规划问题, 本文提出一种基于强化学习的多Agent路径规划方法。首先建立了多Agent路径强化学习模型, 并详细描述了各个基本要素, 以及多个Agent如何从历史数据中积累经验优化决策。通过仿真实验表明, 该方法是可行、有效的。为了提高该方法的学习效率, 本文提出了2种解决方案: (1) 环境预处理, 根据实际任务以及多Agent的信息, 剔除一些无关的环境状态; (2) 加入先验信息的Agent决策<i>Q</i>表, 基于先验信息更新<i>Q</i>表, 作为历史经验提供给Agent, 大大提高了Agent的学习效率, 与文献<citation id="165" type="reference">[<a class="sup">16</a>]</citation>方法相比, 减少了17.4%的总探索步数。下一步将研究多Agent动态目标的路径规划问题, 实现多Agent在复杂任务下的自主路径决策。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Real-Time Path Planning in Unknown Environments for Bipedal Robots">

                                <b>[1]</b> Hildebrandt A C, Klischat M, Wahrmann D, et al.Real-Time Path Planning in Unknown Environments for Bipedal Robots[J].IEEE Robotics and Automation Letters, 2017, 2 (4) :1856-1863.
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Cooperative path planning for target tracking in urban environments using unmanned air and ground vehicles,&amp;quot;">

                                <b>[2]</b> Yu H, Meier K, Argyle M, et al.Cooperative Path Planning for Target Tracking in Urban Environments Using Unmanned Air and Ground Vehicles[J].IEEE/ASME Transactions on Mechatronics, 2015, 20 (2) :541-552.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Path Planning for Single Unmanned Aerial Vehicle by Separately Evolving Waypoints">

                                <b>[3]</b> Yang P, Tang K, Lozano J A, et al.Path Planning for Single Unmanned Aerial Vehicle by Separately Evolving Waypoints[J].IEEE Transactions on Robotics, 2015, 31 (5) :1130-1146.
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201809051&amp;v=MzI0MTdZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVadEZ5am1WcjNLTHo3QmJiRzRIOW5NcG85QVo=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b> 熊超, 解武杰, 董文瀚.基于碰撞锥改进人工势场的无人机避障路径规划[J].计算机工程, 2018, 44 (9) :314-320.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201501030&amp;v=MjEzMzdCdEdGckNVUjdxZlp1WnRGeWptVnIzS0x6N0JiYkc0SDlUTXJvOUdaSVFLREg4NHZSNFQ2ajU0TzN6cXE=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b> 姜涛, 王建中, 施家栋.小型移动机器人自主返航路径规划方法[J].计算机工程, 2015, 41 (1) :164-168.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJA2017S2026&amp;v=MzI2NjlqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVadEZ5am1WcjNLTHo3QmI3RzRIOWF2clk5SFlvUUtESDg0dlI0VDY=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b> 刘洁, 赵海芳, 周德廉.一种改进量子行为粒子群优化算法的移动机器人路径规划[J].计算机科学, 2017, 44 (S2) :123-128.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JQRR201806016&amp;v=MDExNDRmTEc0SDluTXFZOUVZb1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnRGeWptVnIzS0x6elo=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b> 赵晓, 王铮, 黄程侃, 等.基于改进A*算法的移动机器人路径规划[J].机器人, 2018, 40 (6) :903-910.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=KZYC201703024&amp;v=MzIyODk5Yk1ySTlIWUlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVp0RnlqbVZyM0tMamZTYmJHNEg=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b> 游晓明, 刘升, 吕金秋.一种动态搜索策略的蚁群算法及其在机器人路径规划中的应用[J].控制与决策, 2017, 32 (3) :552-556.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Collaborative Q-learning path planning for autonomous robots based on holonic multi-agent system">

                                <b>[9]</b> Lamini C, Fathi Y, Benhlima S.Collaborative Q-learning path planning for autonomous robots based on holonic multi-agent system[C]//2015 10th International Conference on Intelligent Systems:Theories and Applications (SITA) .Rabat, 2015:1-6.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_10" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=1015730865.nh&amp;v=MTg2NjU3UzdIdG5LcXBFYlBJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVadEZ5am1WcjNLVkYyNkc=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[10]</b> 冯涛.基于协同进化算法的多机器人路径规划研究[D].南京:南京邮电大学, 2015.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_11" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJZ201501084&amp;v=MDQwODZHNEg5VE1ybzlOWUlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVp0RnlqbVZyM0tMejdCZEw=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[11]</b> 李东正, 郝燕玲, 张振兴.基于主从结构的多水下机器人协同路径规划[J].计算机仿真, 2015, 32 (1) :382-387.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_12" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201512036&amp;v=MDYyOTBadEZ5am1WcjNLTHo3QmQ3RzRIOVROclk5R1lvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnU=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[12]</b> 郑延斌, 李波, 安德宇, 等.基于分层强化学习及人工势场的多Agent路径规划方法[J].计算机应用, 2015, 35 (12) :3491-3496.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_13" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=1018841663.nh&amp;v=MTA2NjBGckNVUjdxZlp1WnRGeWptVnIzS1ZGMjZGcnU4SDlmS3JKRWJQSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEc=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[13]</b> 刘敬一.自动化仓储调度系统中多AGV路径规划的研究与实现[D].沈阳:中国科学院大学 (中国科学院沈阳计算技术研究所) , 2018.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The optimization of path planning for multi-robot system using Boltzmann Policy based Q-learning algorithm">

                                <b>[14]</b> Wang Z, Shi Z, Li Y, et al.The optimization of path planning for multi-robot system using Boltzmann Policy based Q-learning algorithm[C]//2013 IEEE International Conference on Robotics and Biomimetics (ROBIO) .Shenzhen, 2013:1199-1204.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Reinforcement learning: an introduction">

                                <b>[15]</b> Sutton R S, Barto A G.Reinforcement Learning:An introduction[M].Cambridge, Massachusetts:MIT Press, 1998.
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Obstacle Avoidance Path Planning for UAV Using Reinforcement Learning Under Simulated Environment">

                                <b>[16]</b> Kim J, Shin S, Wu J, et al.Obstacle Avoidance Path Planning for UAV Using Reinforcement Learning Under Simulated Environment[C]//IASER 3rd International Conference on Electronics, Electrical Engineering.Okinawa, 2017:34-36.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JYRJ201908030" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201908030&amp;v=MDMxNzdyM0tMelRaWkxHNEg5ak1wNDlHWklRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVp0RnlqbVY=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
