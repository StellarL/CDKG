<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637135596092502500%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJYRJ201909036%26RESULT%3d1%26SIGN%3dZnRmOig7fErOUkDKZ6KsrN1Yokw%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JYRJ201909036&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JYRJ201909036&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201909036&amp;v=MDY2MThRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVp0Rnlqa1VyN0FMelRaWkxHNEg5ak1wbzlHWW8=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#67" data-title="&lt;b&gt;0 引 言&lt;/b&gt; "><b>0 引 言</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#72" data-title="&lt;b&gt;1 自然场景下的文本检测算法&lt;/b&gt; "><b>1 自然场景下的文本检测算法</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#73" data-title="&lt;b&gt;1.1 研究现状&lt;/b&gt;"><b>1.1 研究现状</b></a></li>
                                                <li><a href="#77" data-title="&lt;b&gt;1.2 EAST算法&lt;/b&gt;"><b>1.2 EAST算法</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#85" data-title="&lt;b&gt;2 注意力机制&lt;/b&gt; "><b>2 注意力机制</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#96" data-title="&lt;b&gt;3 引入注意力机制的EAST算法&lt;/b&gt; "><b>3 引入注意力机制的EAST算法</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#97" data-title="&lt;b&gt;3.1 Attention-EAST算法结构&lt;/b&gt;"><b>3.1 Attention-EAST算法结构</b></a></li>
                                                <li><a href="#108" data-title="&lt;b&gt;3.2 损失函数设计&lt;/b&gt;"><b>3.2 损失函数设计</b></a></li>
                                                <li><a href="#133" data-title="&lt;b&gt;3.3 模型训练&lt;/b&gt;"><b>3.3 模型训练</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#135" data-title="&lt;b&gt;4 实验与分析&lt;/b&gt; "><b>4 实验与分析</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#136" data-title="&lt;b&gt;4.1 实验环境&lt;/b&gt;"><b>4.1 实验环境</b></a></li>
                                                <li><a href="#138" data-title="&lt;b&gt;4.2 实验结果&lt;/b&gt;"><b>4.2 实验结果</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#147" data-title="&lt;b&gt;5 结 语&lt;/b&gt; "><b>5 结 语</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#70" data-title="图1 目标检测算法基本流程图">图1 目标检测算法基本流程图</a></li>
                                                <li><a href="#79" data-title="图2 EAST算法结构图">图2 EAST算法结构图</a></li>
                                                <li><a href="#99" data-title="图3 Attention-EAST算法结构图">图3 Attention-EAST算法结构图</a></li>
                                                <li><a href="#142" data-title="图4 长文本检测对比图">图4 长文本检测对比图</a></li>
                                                <li><a href="#144" data-title="&lt;b&gt;表1 各文本检测算法实验结果对比数据&lt;/b&gt;"><b>表1 各文本检测算法实验结果对比数据</b></a></li>
                                                <li><a href="#146" data-title="&lt;b&gt;表2 两种算法文本检测效率对比数据(FPS&lt;/b&gt;)"><b>表2 两种算法文本检测效率对比数据(FPS</b>)</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="180">


                                    <a id="bibliography_1" title=" Girshick R,Donahue J,Darrell T,et al. Rich feature hierarchies for accurate object detection and semantic segmentation[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. Los Alamitos:IEEE Computer Society Press,2014:580-587." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Rich feature hierarchies for accurate object detection and semantic segmentation">
                                        <b>[1]</b>
                                         Girshick R,Donahue J,Darrell T,et al. Rich feature hierarchies for accurate object detection and semantic segmentation[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. Los Alamitos:IEEE Computer Society Press,2014:580-587.
                                    </a>
                                </li>
                                <li id="182">


                                    <a id="bibliography_2" title=" Krizhevsky A,Sutskever I,Hinton G. Image Net classification with deep convolutional neural networks[C]//Proceedings of the Advances in Neural Information Processing Systems. Lake Tahoe,USA,2012:1097-1105." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Imagenet classification with deep conv olutional neural networks">
                                        <b>[2]</b>
                                         Krizhevsky A,Sutskever I,Hinton G. Image Net classification with deep convolutional neural networks[C]//Proceedings of the Advances in Neural Information Processing Systems. Lake Tahoe,USA,2012:1097-1105.
                                    </a>
                                </li>
                                <li id="184">


                                    <a id="bibliography_3" title=" Girshick R. Fast R-CNN[C]//Proceedings of the IEEE International Conference on Computer Vision. Los Alamitos:IEEE Computer Society Press,2015:1440-1448." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fast R-CNN">
                                        <b>[3]</b>
                                         Girshick R. Fast R-CNN[C]//Proceedings of the IEEE International Conference on Computer Vision. Los Alamitos:IEEE Computer Society Press,2015:1440-1448.
                                    </a>
                                </li>
                                <li id="186">


                                    <a id="bibliography_4" title=" Ren S Q,He K M,Girshick R,et al. Faster R-CNN:towards real-time object detection with region proposal networks[C]//Proceedings of the 28th International Conference on Neural Information Processing Systems. Cambridge:MIT Press,2015,1:91-99." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Faster R-CNN:towards real-time object detection with region proposal networks">
                                        <b>[4]</b>
                                         Ren S Q,He K M,Girshick R,et al. Faster R-CNN:towards real-time object detection with region proposal networks[C]//Proceedings of the 28th International Conference on Neural Information Processing Systems. Cambridge:MIT Press,2015,1:91-99.
                                    </a>
                                </li>
                                <li id="188">


                                    <a id="bibliography_5" title=" Liu W,Anguelov D,Erhan D,et al. SSD:single shot multibox detector[C]//Proceedings of European Conference on Computer Vision. Aire-la-Ville:Eurographics Association Press,2016:21-37." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=SSD:Single Shot MultiBox Detector">
                                        <b>[5]</b>
                                         Liu W,Anguelov D,Erhan D,et al. SSD:single shot multibox detector[C]//Proceedings of European Conference on Computer Vision. Aire-la-Ville:Eurographics Association Press,2016:21-37.
                                    </a>
                                </li>
                                <li id="190">


                                    <a id="bibliography_6" title=" Redmon J,Divvala S,Girshick R,et al. You only look once:unified,real-time object detection[C]//Proceedings of the IEEE International Conference on Computer Vision.Los Alamitos:IEEE Computer Society Press,2016:779-788." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=You only look once:unified,real-time object detection">
                                        <b>[6]</b>
                                         Redmon J,Divvala S,Girshick R,et al. You only look once:unified,real-time object detection[C]//Proceedings of the IEEE International Conference on Computer Vision.Los Alamitos:IEEE Computer Society Press,2016:779-788.
                                    </a>
                                </li>
                                <li id="192">


                                    <a id="bibliography_7" title=" Jaderberg M,Simonyan K,Vedaldi A,et al. Reading text in the wild with convolutional neural networks[J]. International Journal of Computer Vision,2016,116(1):1-20." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Reading text in the wild with convolutional neural networks">
                                        <b>[7]</b>
                                         Jaderberg M,Simonyan K,Vedaldi A,et al. Reading text in the wild with convolutional neural networks[J]. International Journal of Computer Vision,2016,116(1):1-20.
                                    </a>
                                </li>
                                <li id="194">


                                    <a id="bibliography_8" title=" Bissacco A,Cummins M,Netzer Y,et al. Photo OCR:Reading text in uncontrolled conditions[C]//2013 IEEE International Conference on Computer Vision(ICCV). IEEE,2013." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=PhotoOCR:Reading text in uncontrolled conditions">
                                        <b>[8]</b>
                                         Bissacco A,Cummins M,Netzer Y,et al. Photo OCR:Reading text in uncontrolled conditions[C]//2013 IEEE International Conference on Computer Vision(ICCV). IEEE,2013.
                                    </a>
                                </li>
                                <li id="196">


                                    <a id="bibliography_9" title=" Neumann L,Matas J. Real-time scene text localization and recognition[C]//2012 IEEE Conference on Computer Vision and Pattern Recognition(CVPR). IEEE,2012." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Real-time scene text localization and recognition">
                                        <b>[9]</b>
                                         Neumann L,Matas J. Real-time scene text localization and recognition[C]//2012 IEEE Conference on Computer Vision and Pattern Recognition(CVPR). IEEE,2012.
                                    </a>
                                </li>
                                <li id="198">


                                    <a id="bibliography_10" title=" Gomez L,Karatzas D. Text Proposals:A text-specific selective search algorithm for word spotting in the wild[J]. Pattern Recognition,2017,70:60-74." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESE6954D5B835DB2AE95ED084AAD06D37E&amp;v=MjA1ODBhQnVIWWZPR1FsZkJyTFUwNXR0aHhMbSt3YUE9TmlmT2ZjYStGOVRJMjRvM2JPZ0tlQTQ3dm1NYTcwb0pTSGZtM1dOQmViVGdScjNxQ09OdkZTaVdXcjdKSUZwbQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[10]</b>
                                         Gomez L,Karatzas D. Text Proposals:A text-specific selective search algorithm for word spotting in the wild[J]. Pattern Recognition,2017,70:60-74.
                                    </a>
                                </li>
                                <li id="200">


                                    <a id="bibliography_11" title=" Long J,Shelhamer E,Darrell T. Fully convolutional networks for semantic segmentation[J]. IEEE Transactions on Pattern Analysis&amp;amp;Machine Intelligence,2014,39(4):640-651." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fully convolutional networks for semantic segmentation">
                                        <b>[11]</b>
                                         Long J,Shelhamer E,Darrell T. Fully convolutional networks for semantic segmentation[J]. IEEE Transactions on Pattern Analysis&amp;amp;Machine Intelligence,2014,39(4):640-651.
                                    </a>
                                </li>
                                <li id="202">


                                    <a id="bibliography_12" title=" Chucai Y,Yingli T. Localizing text in scene images by boundary clustering,stroke segmentation,and string fragment classification[J]. IEEE Transactions on Image Processing,2012,21(9):4256-4268." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Localizing Text in Scene Images by Boundary Clustering, Stroke Segmentation, and String Fragment Classification">
                                        <b>[12]</b>
                                         Chucai Y,Yingli T. Localizing text in scene images by boundary clustering,stroke segmentation,and string fragment classification[J]. IEEE Transactions on Image Processing,2012,21(9):4256-4268.
                                    </a>
                                </li>
                                <li id="204">


                                    <a id="bibliography_13" title=" Tian Z,Huang W,He T,et al. Detecting text in natural image with connectionist text proposal network[C]//Computer Vision—ECCV 2016:14th European Conference,Amsterdam,The Netherlands,October 11-14,2016,Proceedings,Part VIII. 2016:56-72." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Detecting Text in Natural Image with Connectionist Text Proposal Network">
                                        <b>[13]</b>
                                         Tian Z,Huang W,He T,et al. Detecting text in natural image with connectionist text proposal network[C]//Computer Vision—ECCV 2016:14th European Conference,Amsterdam,The Netherlands,October 11-14,2016,Proceedings,Part VIII. 2016:56-72.
                                    </a>
                                </li>
                                <li id="206">


                                    <a id="bibliography_14" title=" Cao X,Ren W,Zuo W,et al. Scene text deblurring using text-specific multiscale dictionaries[J]. IEEE Transactions on Image Processing,2015,24(4):1302-1314." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Scene text deblurring using text-specific multiscale dictionaries">
                                        <b>[14]</b>
                                         Cao X,Ren W,Zuo W,et al. Scene text deblurring using text-specific multiscale dictionaries[J]. IEEE Transactions on Image Processing,2015,24(4):1302-1314.
                                    </a>
                                </li>
                                <li id="208">


                                    <a id="bibliography_15" title=" Liao M,Shi B,Bai X,et al. Text Boxes:A fast text detector with a single deep neural network[C]//Thirty-First AAAI Conference on Artificial Intelligence,2016." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Text Boxes:A fast text detector with a single deep neural network">
                                        <b>[15]</b>
                                         Liao M,Shi B,Bai X,et al. Text Boxes:A fast text detector with a single deep neural network[C]//Thirty-First AAAI Conference on Artificial Intelligence,2016.
                                    </a>
                                </li>
                                <li id="210">


                                    <a id="bibliography_16" title=" Ma J,Shao W,Ye H,et al. Arbitrary-oriented scene text detection via rotation proposals[J]. IEEE Transactions on Multimedia,2017,20(11):3111-3122." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Arbitrary-oriented scene text detection via rotation proposals">
                                        <b>[16]</b>
                                         Ma J,Shao W,Ye H,et al. Arbitrary-oriented scene text detection via rotation proposals[J]. IEEE Transactions on Multimedia,2017,20(11):3111-3122.
                                    </a>
                                </li>
                                <li id="212">


                                    <a id="bibliography_17" title=" Yao C,Bai X,Sang N,et al. Scene text detection via holistic,multi-channel prediction[EB]. ar Xiv:1606. 09002,2016." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Scene text detection via holistic,multi-channel prediction[EB]">
                                        <b>[17]</b>
                                         Yao C,Bai X,Sang N,et al. Scene text detection via holistic,multi-channel prediction[EB]. ar Xiv:1606. 09002,2016.
                                    </a>
                                </li>
                                <li id="214">


                                    <a id="bibliography_18" title=" Liu Y,Jin L. Deep matching prior network:Toward tighter multi-oriented text detection[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition(CVPR),2017." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep matching prior network:toward tighter multi-oriented text detection">
                                        <b>[18]</b>
                                         Liu Y,Jin L. Deep matching prior network:Toward tighter multi-oriented text detection[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition(CVPR),2017.
                                    </a>
                                </li>
                                <li id="216">


                                    <a id="bibliography_19" title=" Zhou X,Yao C,Wen H,et al. EAST:An efficient and accurate scene text detector[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition(CVPR),2017.." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=EAST:An efficient and accurate scene text detector">
                                        <b>[19]</b>
                                         Zhou X,Yao C,Wen H,et al. EAST:An efficient and accurate scene text detector[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition(CVPR),2017..
                                    </a>
                                </li>
                                <li id="218">


                                    <a id="bibliography_20" title=" Huang L,Yang Y,Deng Y,et al. Dense Box:Unifying landmark localization with end to end object detection[EB].eprint ar Xiv:1509. 04874,2015." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Dense Box:Unifying landmark localization with end to end object detection[EB]">
                                        <b>[20]</b>
                                         Huang L,Yang Y,Deng Y,et al. Dense Box:Unifying landmark localization with end to end object detection[EB].eprint ar Xiv:1509. 04874,2015.
                                    </a>
                                </li>
                                <li id="220">


                                    <a id="bibliography_21" title=" Kingma D P,Ba J. Adam:A method for stochastic optimization[EB]. eprint ar Xiv:1412. 6980,2014." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Adam:A method for stochastic optimization[EB]">
                                        <b>[21]</b>
                                         Kingma D P,Ba J. Adam:A method for stochastic optimization[EB]. eprint ar Xiv:1412. 6980,2014.
                                    </a>
                                </li>
                                <li id="222">


                                    <a id="bibliography_22" title=" Xu K,Ba J,Kiros R,et al. Show,attend and tell:Neural image caption generation with visual attention[EB]. eprint ar Xiv:1502. 03044,2015." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Show,attend and tell:Neural image caption generation with visual attention[EB]">
                                        <b>[22]</b>
                                         Xu K,Ba J,Kiros R,et al. Show,attend and tell:Neural image caption generation with visual attention[EB]. eprint ar Xiv:1502. 03044,2015.
                                    </a>
                                </li>
                                <li id="224">


                                    <a id="bibliography_23" title=" Yao L,Torabi A,Cho K,et al. Describing videos by exploiting temporal structure[C]//2015 IEEE International Conference on Computer Vision(ICCV),2015." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Describing videos by exploiting temporal structure">
                                        <b>[23]</b>
                                         Yao L,Torabi A,Cho K,et al. Describing videos by exploiting temporal structure[C]//2015 IEEE International Conference on Computer Vision(ICCV),2015.
                                    </a>
                                </li>
                                <li id="226">


                                    <a id="bibliography_24" title=" Chen K,Wang J,Chen L C,et al. ABC-CNN:An attention based convolutional neural network for visual question answering[J]. eprint ar Xiv:1511. 05960,2015." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=ABC-CNN:An attention based convolutional neural network for visual question answering">
                                        <b>[24]</b>
                                         Chen K,Wang J,Chen L C,et al. ABC-CNN:An attention based convolutional neural network for visual question answering[J]. eprint ar Xiv:1511. 05960,2015.
                                    </a>
                                </li>
                                <li id="228">


                                    <a id="bibliography_25" title=" Yang Z,He X,Gao J,et al. Stacked attention networks for image question answering[C]//2016 IEEE Conference on Computer Vision and Pattern Recognition(CVPR),2015." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Stacked Attention Networks for Image Question Answering">
                                        <b>[25]</b>
                                         Yang Z,He X,Gao J,et al. Stacked attention networks for image question answering[C]//2016 IEEE Conference on Computer Vision and Pattern Recognition(CVPR),2015.
                                    </a>
                                </li>
                                <li id="230">


                                    <a id="bibliography_26" title=" Xu H,Saenko K. Ask,attend and answer:Exploring question-guided spatial attention for visual question answering[M]//Computer Vision—ECCV 2016. Springer International Publishing,2016." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Ask,attend and answer:Exploring question-guided spatial attention for visual question answering">
                                        <b>[26]</b>
                                         Xu H,Saenko K. Ask,attend and answer:Exploring question-guided spatial attention for visual question answering[M]//Computer Vision—ECCV 2016. Springer International Publishing,2016.
                                    </a>
                                </li>
                                <li id="232">


                                    <a id="bibliography_27" title=" Corbetta M,Shulman G L. Control of goal-directed and stimulus-driven attention in the brain[J]. Nature reviews Neuroscience,2002,3(3):215-229." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Control of goal-directed and stimulus-driven attention in the brain">
                                        <b>[27]</b>
                                         Corbetta M,Shulman G L. Control of goal-directed and stimulus-driven attention in the brain[J]. Nature reviews Neuroscience,2002,3(3):215-229.
                                    </a>
                                </li>
                                <li id="234">


                                    <a id="bibliography_28" title=" Li C,Hou Y,Wang P,et al. Joint distance maps based action recognition with convolutional neural networks[J].IEEE Signal Processing Letters,2017,24(5):624-628." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Joint distance maps based action recognition with convolutional neural networks">
                                        <b>[28]</b>
                                         Li C,Hou Y,Wang P,et al. Joint distance maps based action recognition with convolutional neural networks[J].IEEE Signal Processing Letters,2017,24(5):624-628.
                                    </a>
                                </li>
                                <li id="236">


                                    <a id="bibliography_29" title="孙萍,胡旭东,张永军.结合注意力机制的深度学习图像目标检测[J/OL].计算机工程与应用:1-7[2019-04-29]." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSGG201917028&amp;v=MTk3OTZVcjdBTHo3TWFiRzRIOWpOcUk5SGJJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVadEZ5ams=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[29]</b>
                                        孙萍,胡旭东,张永军.结合注意力机制的深度学习图像目标检测[J/OL].计算机工程与应用:1-7[2019-04-29].
                                    </a>
                                </li>
                                <li id="238">


                                    <a id="bibliography_30" title="孙亚圣,姜奇,胡洁,等.基于注意力机制的行人轨迹预测生成模型[J].计算机应用,2019,39(3):668-674." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201903010&amp;v=MTQ3OTh0R0ZyQ1VSN3FmWnVadEZ5amtVcjdBTHo3QmQ3RzRIOWpNckk5RVpJUUtESDg0dlI0VDZqNTRPM3pxcUI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[30]</b>
                                        孙亚圣,姜奇,胡洁,等.基于注意力机制的行人轨迹预测生成模型[J].计算机应用,2019,39(3):668-674.
                                    </a>
                                </li>
                                <li id="240">


                                    <a id="bibliography_31" title=" Chen L,Zhang H,Xiao J,et al. SCA-CNN:spatial and channel-wise attention in convolutional networks for image captioning[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition(CVPR),2017:6298-6306." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=SCA-CNN spatial and channel-wise attention in convolutional networks for image captioning">
                                        <b>[31]</b>
                                         Chen L,Zhang H,Xiao J,et al. SCA-CNN:spatial and channel-wise attention in convolutional networks for image captioning[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition(CVPR),2017:6298-6306.
                                    </a>
                                </li>
                                <li id="242">


                                    <a id="bibliography_32" title=" Xie S,Tu Z. Holistically-Nested Edge Detection[J]. International Journal of Computer Vision,2015,125(1/3):3-18." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD6BB00EB93AC104C12EB55DFC97247B75&amp;v=MTUyODFOSE0ydjFNWjVwOERYdzl2QmNSbjAxNFRRdVUzeHN5ZTdhVE43MmFDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZCckxVMDV0dGh4TG0rd2FBPU5qN0JhclhLYg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[32]</b>
                                         Xie S,Tu Z. Holistically-Nested Edge Detection[J]. International Journal of Computer Vision,2015,125(1/3):3-18.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JYRJ" target="_blank">计算机应用与软件</a>
                2019,36(09),198-203+269 DOI:10.3969/j.issn.1000-386x.2019.09.035            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>引入注意力机制的自然场景文本检测算法研究</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%89%9B%E4%BD%9C%E4%B8%9C&amp;code=41358504&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">牛作东</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%8E%E6%8D%8D%E4%B8%9C&amp;code=06944675&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">李捍东</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E8%B4%B5%E5%B7%9E%E5%A4%A7%E5%AD%A6%E7%94%B5%E6%B0%94%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0159277&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">贵州大学电气工程学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>随着深度学习、神经网络的兴起与发展,对于图像中的目标检测已经取得了巨大的进展。但是自然场景下的文本信息具有多样的形式和复杂的特点,通用的目标检测算法无法取得理想的效果,因此自然场景下的文本检测在计算机视觉以及机器学习领域仍然是一项具有挑战性的问题和未来的热点研究方向。根据当前学术界针对自然场景下的文本检测问题所提出的算法和思路,在EAST算法的主干网络PVANet的基础上通过引入注意力机制模块,使得提取文本目标特征时更加关注有用信息和抑制无用信息,从而有效改善原算法在预测长文本方向信息时视野不足的问题。实验结果显示,该方法在没有损失检测效率的同时提高了原算法的检测精度,并在一定程度上优于当前针对自然场景下的文本检测算法。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">注意力机制;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%96%87%E6%9C%AC%E6%A3%80%E6%B5%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">文本检测;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">特征提取;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    牛作东,硕士生,主研领域:计算机控制技术,模式识别。;
                                </span>
                                <span>
                                    李捍东,教授。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-04-29</p>

            </div>
                    <h1><b>NATURAL SCENE TEXT DETECTION ALGORITHM WITH ATTENTION MECHANISM</b></h1>
                    <h2>
                    <span>Niu Zuodong</span>
                    <span>Li Handong</span>
            </h2>
                    <h2>
                    <span>College of Electrical Engineering, Guizhou University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>With the rise and development of deep learning and neural network, object detection in images has made great progress. However, text information in natural scenes has various forms and complex characteristics, so universal object detection algorithms cannot achieve ideal results. Therefore, text detection in natural scenes is still a challenging problem and a hot research direction in the field of computer vision and machine learning. This paper studied the current academic algorithms and ideas for text detection in natural scenes. Based on the network-PVANet of EAST algorithm, we introduced the attention mechanism module to make the extraction of text target features pay more attention to useful information and suppress useless information, so as to effectively improve the problem of insufficient field of vision of EAST in predicting long text direction information. The experimental results show that compared with EAST, this method improves the detection accuracy without losing the detection efficiency, and to some extent it is superior to the current text detection algorithm for natural scenes.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Attention%20mechanism&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Attention mechanism;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Text%20detection&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Text detection;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Deep%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Deep learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Feature%20extraction&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Feature extraction;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2019-04-29</p>
                            </div>


        <!--brief start-->
                        <h3 id="67" name="67" class="anchor-tag"><b>0 引 言</b></h3>
                <div class="p1">
                    <p id="68">人工智能的兴起促进着当前社会生产活动中各项产业的革新与发展,以机器学习、深度学习和计算机视觉为代表的人工智能技术已经得到了很大程度的推广和应用。比如:无人驾驶中的高级驾驶辅助系统通过计算机视觉来实现对于动态物体的识别、侦测与追踪;以深度学习加强神经网络训练的阿尔法围棋已经被围棋界公认为超过了人类职业围棋顶尖水平。此外,深度学习在自然语言处理、语音识别、人脸识别、物体检测等诸多方面都有了广泛的应用,已经成为未来科技进步和发展的一个热点方向。</p>
                </div>
                <div class="p1">
                    <p id="69">自然场景下的物体检测问题吸引世界各地的专家学者的关注和研究,传统的目标检测算法主要是基于手工特征的选取和滑动窗口的方式来检测目标物体,其中最具代表性的算法主要包括VJ、HOG和DPM等。由于手工设计目标特征,因此带来了很多的缺点,比如在特定条件下存在着特征值不具有代表性和鲁棒性等问题,另外通过滑动窗口的方式来提取目标框进而进行判断的过程非复杂,算法量大,制约着系统的运行效率。DPM算法提出之后,取得前所未有的目标检测效果,同时也代表着传统的目标检测方法已经遇到了无法突破的瓶颈。之后Girshick等<citation id="244" type="reference"><link href="180" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>提出了R-CNN模型,开启了神经网络应用于目标检测的先河。随着2012年Krizhevsky等<citation id="245" type="reference"><link href="182" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>使用扩展了深度的CNN在ImageNet大规模视觉识别挑战竞赛(ImageNet Large Scale Visual Recognition Challenge, LSVRC)中取得了当时最佳成绩的分类效果,使得CNN越来越受研究者们的重视。随后,基于CNN模型的改进的算法Fast R-CNN<citation id="246" type="reference"><link href="184" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>、Faster R-CNN<citation id="247" type="reference"><link href="186" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>、SSD<citation id="248" type="reference"><link href="188" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>、YOLO<citation id="249" type="reference"><link href="190" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>、Retina-Net以及Pyramid Networks等又进一步推动了基于深度学习的神经网络模型在自然场景下目标检测领域的快速发展。相对于传统的目标检测方法,深度学习目标检测方法通过深度神经网络来学习到的目标特征更鲁棒,采用Proposal或者直接回归的方式来获得候选目标的过程更加高效。在完成目标分类的过程中同样使用深度网络实现了端到端的方式直接训练模型,从而使得深度学习目标检测方法准确度高实时性好。通用目标检测算法的基本流程如图1所示。</p>
                </div>
                <div class="area_img" id="70">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201909036_070.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 目标检测算法基本流程图" src="Detail/GetImg?filename=images/JYRJ201909036_070.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 目标检测算法基本流程图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201909036_070.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="71">基于深度学习的目标检测算法在用于通用物体检测方面取得了很好的效果,但是面对自然场景下的文本信息的检测任务时却存在着许多问题,主要原因有:(1) 文本长宽比不定并且相关较小,与常规物体检测存在着差异,这个差异使得在选取候选框的长宽比时要做相应的处理。(2) 自然场景下的文本普遍存在着水平、倾斜、弯曲等各种不定的形状和方向,这些差异性较丰富的文体状态导致了采用通用的目标检测算法很难沿着某个方向或者倾斜角度去完成文本检测的任务。(3) 场景下的文本很容易受到其中某些物体的局部图像影像影响其文本特征信息。(4) 自然场景下的文本内容存在许多艺术字、手写体等。(5) 自然场景下的文本容易受到自然环境的干扰,比如背景、光照影响等。针对上述问题,在设计文本检测算法的时候需要对常规物体检测方法进行改进的方向主要包括特征提取算法、区域建议网络(RPN)对候选区域进行推荐、多任务网络训练、损失函数Loss改进、非极大值抑制(NMS)、半监督学习等。因此,自然图像中的文本检测仍然是一项非常具有挑战性的研究工作,加上受到许多现实应用实例的驱动影响,比如地理位置和图像检索<citation id="250" type="reference"><link href="192" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>、照片OCR<citation id="251" type="reference"><link href="194" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>等,此方面的研究工作也具有现实意义。</p>
                </div>
                <h3 id="72" name="72" class="anchor-tag"><b>1 自然场景下的文本检测算法</b></h3>
                <h4 class="anchor-tag" id="73" name="73"><b>1.1 研究现状</b></h4>
                <div class="p1">
                    <p id="74">自然场景下的文本检测方法通常由两个主要部分组成,分别是文本检测和文本识别。文本检测主要以字边界框的形式定位图像中的文本。文本识别对裁剪后的单词或者文字图像进行编码,生成机器可解释的字符序列。本文的研究重点主要放在了文本的检测上。本文研究的目标检测算法中,可以将文本检测器分为基于原始检测目标的分类策略和基于目标边界框形状的分类策略,每个检测策略又有各自的特点和不同类别的算法。</p>
                </div>
                <div class="p1">
                    <p id="75">基于原始检测目标的分类策略主要有:(1) 基于角色的检测算法。其过程为:首先检测单个字符或文本的一部分;然后将其分组成一个单词,以Neumann<citation id="252" type="reference"><link href="196" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>为代表的研究提出通过对极值区域进行分类来定位字符;最后通过穷举搜索方法对检测到的字符进行分组。(2) 基于字的检测方法。它是与一般物体检测类似的方式直接提取文本,Gomez<citation id="253" type="reference"><link href="198" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>提出了一个基于R-CNN的框架,其中候选词首先由类别不可知的提议生成器生成,随后是随机森林分类器,再采用用于边界框回归的卷积神经网络来细化边界框。(3) 基于文本行的检测算法。该方法首先检测文本行,然后再分逐个分成单词,Long<citation id="254" type="reference"><link href="200" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>提出利用文本的对称特征来检测文本行,然后通过使用全卷积神经网络来定位文本行。</p>
                </div>
                <div class="p1">
                    <p id="76">基于目标边界框形状的分类策略的检测方法可以分为两类。第一类为水平或接近水平的检测方法,这类方法专注于检测图像中的水平或接近水平的文本。例如:Yi<citation id="255" type="reference"><link href="202" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>提出一个三阶段框架,它由边界聚类、笔划分割和字符串片段分类组成;Tian等<citation id="256" type="reference"><link href="204" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>提出的方法检测几乎水平的文本部分,然后将它们链接在一起开成单候选区域;Cao等<citation id="257" type="reference"><link href="206" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>尝试使用去模糊技术来获得更强大的检测结果。第二类为多方向的检测方法。与水平或接近水平检测方法相比,多方向的文本检测更加稳健,因为自然场景下的文本可以在图像中处于任意方向。这类的主要研究方法有利用检测多方向文本的旋转不变特征,首先在特征计算之前估计检测目标的中心、比例、方向信息,然后根据尺寸变化、颜色自相似性和结构自相似性进行链级特征。此外还有基于纹理的纹理分类器用于区分文本和非文本候选区<citation id="258" type="reference"><link href="212" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>。文献<citation id="259" type="reference">[<a class="sup">18</a>]</citation>提出了四边形滑动窗口、蒙特卡罗方法和平滑L<sub>n</sub>损失来检测定向文本,这在检测十分复杂的场景时是有效的。</p>
                </div>
                <h4 class="anchor-tag" id="77" name="77"><b>1.2 EAST算法</b></h4>
                <div class="p1">
                    <p id="78">本文重点研究了Zhou等<citation id="260" type="reference"><link href="216" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>提出的EAST(An Efɦcient and Accurate Scene Text Detector)算法,经验证,该算法在准确性和效率总体方面明显优于其他的方法。在EAST算法中提出了一种快速、准确的场景文本检测流水线,该流水线只有两个阶段。管道采用完全卷积网络(FCN)模型,直接生成字或文本行级别的预测,不包括冗余和缓慢的中间步骤。生成的文本预测,可以是旋转的矩形或四边形,发送到非最大抑制以产生最终结果,算法总体结构如图2所示。</p>
                </div>
                <div class="area_img" id="79">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201909036_079.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 EAST算法结构图" src="Detail/GetImg?filename=images/JYRJ201909036_079.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 EAST算法结构图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201909036_079.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="80" name="80">(1) EAST算法管道和网络设计</h4>
                <div class="p1">
                    <p id="81">从EAST算法结构图可以看出该算法遵循Densebox<citation id="261" type="reference"><link href="218" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>的总体设计,将图像输入到FCN中,生成多个像素级文本分数图和几何图形通道。其中一个预测通道是一个分数图,其像素值在[0,1]范围内。其余通道表示从每个像素的视图中包含单词的几何图形。分数代表在同一位置预测的几何图形的可信度。</p>
                </div>
                <div class="p1">
                    <p id="82">在进行网络设计时,为处理文本字体大小以及各个形态的特征差异,该算法采用了U形<citation id="262" type="reference"><link href="220" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>的思想,在保持上采样分支较小的同时,逐渐合并特征图,最终形成了一个既能利用不同层次特性又能保持较小计算成本的网络。</p>
                </div>
                <h4 class="anchor-tag" id="83" name="83">(2) EAST算法几何图形的生成</h4>
                <div class="p1">
                    <p id="84">为不失一般性,该算法只考虑了四边形的情况,最终生成的几何图是RBox或者Quad之一。对于文本区域标注为四元样式的数据集,首先生成一个旋转矩形,该矩形覆盖区域最小。然后,对于每个具有正分数的像素,计算其到文本框的4个边界的距离,并将它们放到RBOX地面真值的4个通道中。对于四边形地真值,8通道几何图中每个带正分数的像素的值是其从四边形的4个顶点的坐标偏移,进而确定文本区域。</p>
                </div>
                <h3 id="85" name="85" class="anchor-tag"><b>2 注意力机制</b></h3>
                <div class="p1">
                    <p id="86">视觉注意已经在各种结构的神经网络对于目标检测和预测任务中有所应用,比如图像或视频字幕<citation id="264" type="reference"><link href="218" rel="bibliography" /><link href="220" rel="bibliography" /><sup>[<a class="sup">20</a>,<a class="sup">21</a>]</sup></citation>和视觉问题回答<citation id="265" type="reference"><link href="226" rel="bibliography" /><link href="228" rel="bibliography" /><link href="230" rel="bibliography" /><sup>[<a class="sup">24</a>,<a class="sup">25</a>,<a class="sup">26</a>]</sup></citation>。它的可行性主要归功于合理的假设,即人类视觉不会立即整体处理整个图像。相反,人们只在需要的时间和地点专注于整个视觉空间的选择性部分<citation id="263" type="reference"><link href="232" rel="bibliography" /><sup>[<a class="sup">27</a>]</sup></citation>。具体而言,注意力不是将图像编码成静态矢量,而是允许图像特征从手边的句子上下文演化,从而导致对杂乱图像的更丰富和更长的描述。通过这种方式,视觉注意力可以被视为一种动态特征提取机制,它随着时间的推移结合了上下文定位。</p>
                </div>
                <div class="p1">
                    <p id="87">当在描述图像中检测目标的特征和信息的图像处理任务中加入注意力机制,注意力模块需要处理的特征信息包含明确的序列项<i>a</i>={<b><i>a</i></b><sub>1</sub>,<b><i>a</i></b><sub>2</sub>,<b><i>a</i></b><sub>3</sub>,…,<b><i>a</i></b><sub><i>L</i></sub>},<b><i>a</i></b><sub><i>i</i></sub>∈<b>R</b><sup><i>D</i></sup>,其中<i>L</i>代表特征向量的个数,<i>D</i>代表的是空间维度。因此所采用的注意力机制需要计算出当前时刻<i>t</i>每个特征向量<b><i>a</i></b><sub><i>i</i></sub>的权重<i>α</i><sub><i>t</i>,<i>i</i></sub>,公式如下:</p>
                </div>
                <div class="p1">
                    <p id="88"><i>e</i><sub><i>ti</i></sub>=<i>f</i><sub>att</sub>(<b><i>a</i></b><sub><i>i</i></sub>,<i>h</i><sub><i>t</i>-1</sub>)      (1)</p>
                </div>
                <div class="p1">
                    <p id="89"><mathml id="90"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>α</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>,</mo><mi>i</mi></mrow></msub><mo>=</mo><mfrac><mrow><mi>exp</mi><mo stretchy="false">(</mo><mi>e</mi><msub><mrow></mrow><mrow><mi>t</mi><mi>i</mi></mrow></msub><mo stretchy="false">)</mo></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></munderover><mtext>e</mtext></mstyle><mtext>x</mtext><mtext>p</mtext><mo stretchy="false">(</mo><mi>e</mi><msub><mrow></mrow><mrow><mi>t</mi><mi>k</mi></mrow></msub><mo stretchy="false">)</mo></mrow></mfrac></mrow></math></mathml>      (2)</p>
                </div>
                <div class="p1">
                    <p id="91">式中:<i>f</i><sub>att</sub>()代表多层感知机;<i>e</i><sub><i>ti</i></sub>代表中间变量;<i>h</i><sub><i>t</i>-1</sub>代表的是上个时刻的隐含状态;<i>k</i>代表特征向量的下标。计算出权重后,模型就可以对输入的序列<i>a</i>进行筛选,得到筛选后的序列项为:</p>
                </div>
                <div class="p1">
                    <p id="92"><mathml id="93"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>Ζ</mi><mo>⌢</mo></mover><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mi>μ</mi><mo stretchy="false">(</mo><mo stretchy="false">{</mo><mi>α</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>,</mo><mi>i</mi></mrow></msub><mo stretchy="false">}</mo><mo>,</mo><mo stretchy="false">{</mo><mi mathvariant="bold-italic">a</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">}</mo><mo stretchy="false">)</mo></mrow></math></mathml>      (3)</p>
                </div>
                <div class="p1">
                    <p id="94">最终由函数<i>μ</i>来决定该注意机制是硬注意力还是软注意力<citation id="266" type="reference"><link href="234" rel="bibliography" /><sup>[<a class="sup">28</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="95">在图像检测和目标识别的深度学习领域,文献<citation id="267" type="reference">[<a class="sup">30</a>]</citation>在目标检测框架中引入与子区域特征和宽高比特性相关的注意力特征库,并生成注意力特征图对原始的ROI池化特征进行优化,模型的检测精度和检测速度有明显提升。文献<citation id="268" type="reference">[<a class="sup">28</a>]</citation>设计了一种的引入注意力机制的AttenGAN模型,该模型包括一个生成器和一个判别器,生成器根据行人过去的轨迹概率性地对未来进行多种可能性预测。文献<citation id="269" type="reference">[<a class="sup">31</a>]</citation>介绍了一种新的卷积神经网络,称SCA-CNN,它在CNN中结合了空间注意力和通道方向注意力机制,并取得了良好的效果。</p>
                </div>
                <h3 id="96" name="96" class="anchor-tag"><b>3 引入注意力机制的EAST算法</b></h3>
                <h4 class="anchor-tag" id="97" name="97"><b>3.1 Attention-EAST算法结构</b></h4>
                <div class="p1">
                    <p id="98">针对EAST算法文本检测器可处理的最大文本实例大小与网络的接收字段成比例,其限制了网络预测更长文本区域的能力。为了使文本检测器PVANet网络保证在提取精确的文本特征和位置特征的基础上,扩大目标检测视野,本文在EAST算法的基础上引入了注意力机制,构成Attention-EAST算法,该算法的核心框架如图3所示。</p>
                </div>
                <div class="area_img" id="99">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201909036_099.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 Attention-EAST算法结构图" src="Detail/GetImg?filename=images/JYRJ201909036_099.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 Attention-EAST算法结构图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201909036_099.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="100">在利用PVANet网络进行下采样的过程中,通过中间的文本特征信息的空间关系生成空间注意力模块,其主要功能是用来捕捉二维空间中对于目标区判定的重要性信息,每次卷积生成的特征信息为<b><i>I</i>∈<i>R</i></b><sup>1×<i>H</i>×<i>W</i></sup>,并经过sigmod函数激活,其表达式为:</p>
                </div>
                <div class="p1">
                    <p id="101"><b><i>W</i></b><sub><i>S</i></sub>(<b><i>I</i></b>)=<i>σf</i><sup>7×7</sup><i>Pool</i>(<b><i>I</i></b>)      (4)</p>
                </div>
                <div class="p1">
                    <p id="102">式中:<i>f</i><sup>7×7</sup>为卷积操作,卷积核为7×7的卷积层。在上采样的过程中通过unpool池化的方式提取特征用于对目标位特征的逼近生成通道注意力模块,然后经过共享网络MLP进行调整,其表达式为:</p>
                </div>
                <div class="p1">
                    <p id="103"><b><i>W</i></b><sub><i>C</i></sub>(<b><i>I</i></b>′)=<i>σMLP</i>(<i>unpool</i>(<b><i>I</i></b>))=<i>σ</i><b><i>W</i></b><sub>1</sub><b><i>W</i></b><sub>0</sub><b><i>I</i></b>′      (5)</p>
                </div>
                <div class="p1">
                    <p id="104">式中:<i>σ</i>为sigmod激活函数;<b><i>W</i></b><sub>0</sub>∈<b>R</b><sup><i>C</i>/<i>r</i>×<i>C</i></sup>和<b><i>W</i></b><sub>1</sub>∈<b>R</b><sup><i>C</i>×<i>C</i>/<i>r</i></sup>分别为MLP的权重。最后在特征融合的过程中,将通道注意力权重和空间注意力权重构成整个软件分支注意力模型,其过程可表示为:</p>
                </div>
                <div class="p1">
                    <p id="105"><b><i>I</i></b>′=(<i>W</i><sub><i>S</i></sub>(<b><i>I</i></b>)+1)⊙<b><i>I</i></b>      (6)</p>
                </div>
                <div class="p1">
                    <p id="106"><b><i>I</i></b>″=(<i>W</i><sub><i>C</i></sub>(<b><i>I</i></b>′)+1)⊙<b><i>I</i></b>′      (7)</p>
                </div>
                <div class="p1">
                    <p id="107">式中:⊙为对应矩阵元素相乘,由于每个模块最后都需要使用sigmod函数来激活,使注意力通道每个元素值在[0,1]之间,达到注意力模块强化有用图像信息和抑制无用信息的效果。</p>
                </div>
                <h4 class="anchor-tag" id="108" name="108"><b>3.2 损失函数设计</b></h4>
                <div class="p1">
                    <p id="109">损失函数可表示为:</p>
                </div>
                <div class="p1">
                    <p id="110"><i>L</i>=<i>L</i><sub>s</sub>+<i>λ</i><sub>g</sub><i>L</i><sub>g</sub>      (8)</p>
                </div>
                <div class="p1">
                    <p id="111">式中:<i>L</i><sub>s</sub>和<i>L</i><sub>g</sub>分别表示分数图和几何图形的损失,而<i>λ</i><sub>g</sub>表示两个损失之间的重要性。本文将<i>λ</i><sub>g</sub>设为1。为了简化训练过程,本文算法借鉴了文献<citation id="270" type="reference">[<a class="sup">32</a>]</citation>中引入的类平衡交叉熵:</p>
                </div>
                <div class="p1">
                    <p id="112" class="code-formula">
                        <mathml id="112"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>L</mi><msub><mrow></mrow><mtext>s</mtext></msub><mo>=</mo><mi>b</mi><mi>a</mi><mi>l</mi><mi>a</mi><mi>n</mi><mi>c</mi><mi>e</mi><mi>d</mi><mo>-</mo><mi>x</mi><mi>e</mi><mi>n</mi><mi>t</mi><mo stretchy="false">(</mo><mover accent="true"><mi>Y</mi><mo>^</mo></mover><mo>,</mo><mi>Y</mi><msup><mrow></mrow><mo>*</mo></msup><mo stretchy="false">)</mo><mo>=</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mo>-</mo><mi>β</mi><mi>Y</mi><msup><mrow></mrow><mo>*</mo></msup><mrow><mi>log</mi></mrow><mo stretchy="false">(</mo><mover accent="true"><mi>Y</mi><mo>^</mo></mover><mo stretchy="false">)</mo><mo>-</mo><mo stretchy="false">(</mo><mn>1</mn><mo>-</mo><mi>β</mi><mo stretchy="false">)</mo><mo stretchy="false">(</mo><mn>1</mn><mo>-</mo><mi>Y</mi><msup><mrow></mrow><mo>*</mo></msup><mo stretchy="false">)</mo><mrow><mi>log</mi></mrow><mo stretchy="false">(</mo><mn>1</mn><mo>-</mo><mover accent="true"><mi>Y</mi><mo>^</mo></mover><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>9</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="113">式中:<mathml id="114"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>Y</mi><mo>^</mo></mover><mo>=</mo><mi>F</mi><msub><mrow></mrow><mtext>s</mtext></msub></mrow></math></mathml>是分数图的预测值;<i>Y</i><sup>*</sup>是基本的真实值。参数<i>β</i>是正负样本之间的平衡因子:</p>
                </div>
                <div class="p1">
                    <p id="115"><mathml id="116"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>β</mi><mo>=</mo><mn>1</mn><mo>-</mo><mfrac><mrow><mstyle displaystyle="true"><msub><mo>∑</mo><mrow><mi>y</mi><msup><mrow></mrow><mo>*</mo></msup><mo>∈</mo><mi>Y</mi><msup><mrow></mrow><mo>*</mo></msup></mrow></msub><mi>y</mi></mstyle><msup><mrow></mrow><mo>*</mo></msup></mrow><mrow><mrow><mo>|</mo><mrow><mi>Y</mi><msup><mrow></mrow><mo>*</mo></msup></mrow><mo>|</mo></mrow></mrow></mfrac></mrow></math></mathml>      (10)</p>
                </div>
                <div class="p1">
                    <p id="117">为了使大文本区域和小文本区域生成精确的文本几何预测,保持回归损失尺度不变,旋转矩形框RBox回归部分采用IoU损失函数,因为它对不同尺度的对象是固定,其表达式为:</p>
                </div>
                <div class="p1">
                    <p id="118" class="code-formula">
                        <mathml id="118"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><msub><mrow></mrow><mi>R</mi></msub><mo>=</mo><mo>-</mo><mrow><mi>log</mi></mrow><mo stretchy="false">(</mo><mi>Ι</mi><mi>o</mi><mi>U</mi><mo stretchy="false">(</mo><mover accent="true"><mi>R</mi><mo>^</mo></mover><mo>,</mo><mi>R</mi><msup><mrow></mrow><mo>*</mo></msup><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>=</mo><mo>-</mo><mrow><mi>log</mi></mrow><mfrac><mrow><mrow><mo>|</mo><mrow><mover accent="true"><mi>R</mi><mo>^</mo></mover><mstyle displaystyle="true"><mo>∩</mo><mi>R</mi></mstyle><msup><mrow></mrow><mo>*</mo></msup></mrow><mo>|</mo></mrow></mrow><mrow><mrow><mo>|</mo><mrow><mover accent="true"><mi>R</mi><mo>^</mo></mover><mstyle displaystyle="true"><mo>∪</mo><mi>R</mi></mstyle><msup><mrow></mrow><mo>*</mo></msup></mrow><mo>|</mo></mrow></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>1</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="119">式中:<mathml id="120"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>R</mi><mo>^</mo></mover></math></mathml>表示为预测的几何形状;<i>R</i><sup>*</sup>是其对应的真实形状;相交矩形<mathml id="121"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>|</mo><mrow><mover accent="true"><mi>R</mi><mo>^</mo></mover><mstyle displaystyle="true"><mo>∩</mo><mi>R</mi></mstyle><msup><mrow></mrow><mo>*</mo></msup></mrow><mo>|</mo></mrow></mrow></math></mathml>的宽度和高度分别为:</p>
                </div>
                <div class="p1">
                    <p id="122" class="code-formula">
                        <mathml id="122"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtable columnalign="left"><mtr><mtd><mi>ω</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mi>min</mi><mo stretchy="false">(</mo><mover accent="true"><mi>d</mi><mo>^</mo></mover><msub><mrow></mrow><mn>2</mn></msub><mo>,</mo><mi>d</mi><msubsup><mrow></mrow><mn>2</mn><mo>*</mo></msubsup><mo stretchy="false">)</mo><mo>+</mo><mi>min</mi><mo stretchy="false">(</mo><mover accent="true"><mi>d</mi><mo>^</mo></mover><msub><mrow></mrow><mn>4</mn></msub><mo>,</mo><mi>d</mi><msubsup><mrow></mrow><mn>4</mn><mo>*</mo></msubsup><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mi>h</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mi>min</mi><mo stretchy="false">(</mo><mover accent="true"><mi>d</mi><mo>^</mo></mover><msub><mrow></mrow><mn>1</mn></msub><mo>,</mo><mi>d</mi><msubsup><mrow></mrow><mn>1</mn><mo>*</mo></msubsup><mo stretchy="false">)</mo><mo>+</mo><mi>min</mi><mo stretchy="false">(</mo><mover accent="true"><mi>d</mi><mo>^</mo></mover><msub><mrow></mrow><mn>3</mn></msub><mo>,</mo><mi>d</mi><msubsup><mrow></mrow><mn>3</mn><mo>*</mo></msubsup><mo stretchy="false">)</mo></mtd></mtr></mtable><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>2</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="123">式中:<i>d</i><sub>1</sub>、<i>d</i><sub>2</sub>、<i>d</i><sub>3</sub>和<i>d</i><sub>4</sub>分别表示像素到其对应矩形的上、右、下和左边界的距离。联合区由以下公式给出:</p>
                </div>
                <div class="p1">
                    <p id="124"><mathml id="125"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>|</mo><mrow><mover accent="true"><mi>R</mi><mo>^</mo></mover><mstyle displaystyle="true"><mo>∪</mo><mi>R</mi></mstyle><msup><mrow></mrow><mo>*</mo></msup></mrow><mo>|</mo></mrow><mo>=</mo><mrow><mo>|</mo><mover accent="true"><mi>R</mi><mo>^</mo></mover><mo>|</mo></mrow><mo>+</mo><mrow><mo>|</mo><mrow><mi>R</mi><msup><mrow></mrow><mo>*</mo></msup></mrow><mo>|</mo></mrow><mo>-</mo><mrow><mo>|</mo><mrow><mover accent="true"><mi>R</mi><mo>^</mo></mover><mstyle displaystyle="true"><mo>∩</mo><mi>R</mi></mstyle><msup><mrow></mrow><mo>*</mo></msup></mrow><mo>|</mo></mrow></mrow></math></mathml>      (13)</p>
                </div>
                <div class="p1">
                    <p id="126">由此可以轻松计算交叉或者联合区域。接下来,旋转角损失计算如下:</p>
                </div>
                <div class="p1">
                    <p id="127"><mathml id="128"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><msub><mrow></mrow><mi>θ</mi></msub><mo stretchy="false">(</mo><mover accent="true"><mi>θ</mi><mo>^</mo></mover><mo>,</mo><mi>θ</mi><msup><mrow></mrow><mo>*</mo></msup><mo stretchy="false">)</mo><mo>=</mo><mn>1</mn><mo>-</mo><mrow><mi>cos</mi></mrow><mo stretchy="false">(</mo><mover accent="true"><mi>θ</mi><mo>^</mo></mover><mo>-</mo><mi>θ</mi><msup><mrow></mrow><mo>*</mo></msup><mo stretchy="false">)</mo></mrow></math></mathml>      (14)</p>
                </div>
                <div class="p1">
                    <p id="129">式中:<mathml id="130"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>θ</mi><mo>^</mo></mover></math></mathml>是对旋转角度的预测;<i>θ</i><sup>*</sup>表示实际值。最后可计算出总的几何损失为:</p>
                </div>
                <div class="p1">
                    <p id="131"><i>L</i><sub>g</sub>=<i>L</i><sub><i>R</i></sub>+<i>λ</i><sub><i>θ</i></sub><i>L</i><sub><i>θ</i></sub>      (15)</p>
                </div>
                <div class="p1">
                    <p id="132">在实验过程中本文将<i>λ</i><sub><i>θ</i></sub>设置成10。由此整个算法的损失函数搭建完毕。</p>
                </div>
                <h4 class="anchor-tag" id="133" name="133"><b>3.3 模型训练</b></h4>
                <div class="p1">
                    <p id="134">对于本文提出的模型按照EAST算法的训练方式采用Adam<citation id="271" type="reference"><link href="220" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>优化器对网络进行端到端的训练。为了加快学习速度,将原始图像512×512的训练样本每次统一打包成24个进行批处理。Adam的学习率从1e<sup>-3</sup>开始,每27 300个小批量下降到十分之一,停在1e<sup>-5</sup>,对网络进行训练,直到性能改善趋于平稳。</p>
                </div>
                <h3 id="135" name="135" class="anchor-tag"><b>4 实验与分析</b></h3>
                <h4 class="anchor-tag" id="136" name="136"><b>4.1 实验环境</b></h4>
                <div class="p1">
                    <p id="137">本次实验是在Ubuntu18.04 LTS操作系统上进行,开发语言为Python 3.6,集成开发环境为Pycharm,深度学习框架是GPU版本的TensorFlow。硬件配置CPU为四核八线程的 i7-6700k,其主频4 GHz,内存为32 GB,GPU 为 NVIDIA GTX 1080T,显存11 GB。</p>
                </div>
                <h4 class="anchor-tag" id="138" name="138"><b>4.2 实验结果</b></h4>
                <div class="p1">
                    <p id="139">本次实验采用的数据集为ICDAR挑战赛所用的数据集,该数据集也是当文本目标检测算法中比较流行的数据集,共有1 500张图片,其中1 000张图片用于模型训练,其余图片用于测试集。其文本区域由四边形的四个顶点进行注释,对应于目标文本中的四边几何图形。这些图片均由手机或相机随机拍摄,因此,场景中的文本信息是任意方向的,而且可能受到自然环境的影响,这些特征有利于对文本检测算法的估计检验。</p>
                </div>
                <div class="p1">
                    <p id="140">本文引入注意力机制的Attention-EAST算法与EAST算法在处理自然场景下的长文本的检测结果对比如图4所示。</p>
                </div>
                <div class="area_img" id="142">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201909036_14200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 长文本检测对比图" src="Detail/GetImg?filename=images/JYRJ201909036_14200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 长文本检测对比图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201909036_14200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="143">可以看出,通过加入注意力机制对于提取文本和方位的特征信息的增强,提高了文本检测视野,有效改善了对于长文本的检测效果。同时,本文使用召回率(Recall)、准确率(Precision)和加权调和平均值F-measured三个指标来评价本文算法在ICDAR数据集上的训练效果。并选择了当前文本检测领域比较有代表性的算法进行对比,包括以R-CNN为框架的文献<citation id="272" type="reference">[<a class="sup">10</a>]</citation>,以SSD为框架的文献<citation id="273" type="reference">[<a class="sup">15</a>]</citation>和以Faster R-CNN为框架的文献<citation id="274" type="reference">[<a class="sup">16</a>]</citation>,实验结果如表1所示。实验结果表明,本文提出的引入注意力机制的方法相比原EAST算法在文本检测性能指标均有所提升,并且相对优于其他主流的文本检测算法。</p>
                </div>
                <div class="area_img" id="144">
                    <p class="img_tit"><b>表1 各文本检测算法实验结果对比数据</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="144" border="1"><tr><td>算法</td><td>Recall</td><td>Precision</td><td>F-measure</td></tr><tr><td><br />Attention-EAST</td><td>0.790 2</td><td>0.840 1</td><td>0.814 4</td></tr><tr><td><br />EAST</td><td>0.783 1</td><td>0.822 4</td><td>0.802 2</td></tr><tr><td><br />文献[10]</td><td>0.677 5</td><td>0.798 5</td><td>0.733 0</td></tr><tr><td><br />文献[15]</td><td>0.586 6</td><td>0.721 4</td><td>0.647 1</td></tr><tr><td><br />文献[16]</td><td>0.523 9</td><td>0.732 1</td><td>0.610 7</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="145">为分析引入注意力模块后在检测效率上对于原EAST算法的影响,在本文的实验环境下采用每秒帧率(Frame Per Second,FPS)这一指标来评价本文算法和原EAST算法的检测效率,表示每秒处理的图片数量,将测试集500张检测图片随机分成5份分别进行测试。实验结果如表2所示,可以看出加注入注意力模块后,并没有损失原算法的检测效率。</p>
                </div>
                <div class="area_img" id="146">
                    <p class="img_tit"><b>表2 两种算法文本检测效率对比数据(FPS</b>) <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="146" border="1"><tr><td><br />算法</td><td>1st</td><td>2nd</td><td>3rd</td><td>4th</td><td>5th</td><td>Average</td></tr><tr><td><br />EAST</td><td>4.83</td><td>4.86</td><td>4.90</td><td>4.82</td><td>4.86</td><td>4.854</td></tr><tr><td><br />Attention-EAST</td><td>4.86</td><td>4.84</td><td>4.82</td><td>4.87</td><td>4.90</td><td>4.858</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h3 id="147" name="147" class="anchor-tag"><b>5 结 语</b></h3>
                <div class="p1">
                    <p id="148">本文研究了当前计算机视觉领域中文本检测的算法,其中针对EAST算法在对于文本方向特征提取时视野受限的问题。通过在主干网络PVANet中引入注意力机制,提出一种Attention-EAST算法,使得训练模型在提取文本目标特征时更加关注有用信息和抑制无用信息。实验证明,该方法有效提升了EAST算法检测长文本的能力,在没有损失检测效率的同时提升了检测精度。本文研究通过引入注意力机制的方法可为自然环境下的文本检测研究领域提供一定的参考。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="180">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Rich feature hierarchies for accurate object detection and semantic segmentation">

                                <b>[1]</b> Girshick R,Donahue J,Darrell T,et al. Rich feature hierarchies for accurate object detection and semantic segmentation[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. Los Alamitos:IEEE Computer Society Press,2014:580-587.
                            </a>
                        </p>
                        <p id="182">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Imagenet classification with deep conv olutional neural networks">

                                <b>[2]</b> Krizhevsky A,Sutskever I,Hinton G. Image Net classification with deep convolutional neural networks[C]//Proceedings of the Advances in Neural Information Processing Systems. Lake Tahoe,USA,2012:1097-1105.
                            </a>
                        </p>
                        <p id="184">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fast R-CNN">

                                <b>[3]</b> Girshick R. Fast R-CNN[C]//Proceedings of the IEEE International Conference on Computer Vision. Los Alamitos:IEEE Computer Society Press,2015:1440-1448.
                            </a>
                        </p>
                        <p id="186">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Faster R-CNN:towards real-time object detection with region proposal networks">

                                <b>[4]</b> Ren S Q,He K M,Girshick R,et al. Faster R-CNN:towards real-time object detection with region proposal networks[C]//Proceedings of the 28th International Conference on Neural Information Processing Systems. Cambridge:MIT Press,2015,1:91-99.
                            </a>
                        </p>
                        <p id="188">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=SSD:Single Shot MultiBox Detector">

                                <b>[5]</b> Liu W,Anguelov D,Erhan D,et al. SSD:single shot multibox detector[C]//Proceedings of European Conference on Computer Vision. Aire-la-Ville:Eurographics Association Press,2016:21-37.
                            </a>
                        </p>
                        <p id="190">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=You only look once:unified,real-time object detection">

                                <b>[6]</b> Redmon J,Divvala S,Girshick R,et al. You only look once:unified,real-time object detection[C]//Proceedings of the IEEE International Conference on Computer Vision.Los Alamitos:IEEE Computer Society Press,2016:779-788.
                            </a>
                        </p>
                        <p id="192">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Reading text in the wild with convolutional neural networks">

                                <b>[7]</b> Jaderberg M,Simonyan K,Vedaldi A,et al. Reading text in the wild with convolutional neural networks[J]. International Journal of Computer Vision,2016,116(1):1-20.
                            </a>
                        </p>
                        <p id="194">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=PhotoOCR:Reading text in uncontrolled conditions">

                                <b>[8]</b> Bissacco A,Cummins M,Netzer Y,et al. Photo OCR:Reading text in uncontrolled conditions[C]//2013 IEEE International Conference on Computer Vision(ICCV). IEEE,2013.
                            </a>
                        </p>
                        <p id="196">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Real-time scene text localization and recognition">

                                <b>[9]</b> Neumann L,Matas J. Real-time scene text localization and recognition[C]//2012 IEEE Conference on Computer Vision and Pattern Recognition(CVPR). IEEE,2012.
                            </a>
                        </p>
                        <p id="198">
                            <a id="bibliography_10" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESE6954D5B835DB2AE95ED084AAD06D37E&amp;v=MTk1ODlMbSt3YUE9TmlmT2ZjYStGOVRJMjRvM2JPZ0tlQTQ3dm1NYTcwb0pTSGZtM1dOQmViVGdScjNxQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQnJMVTA1dHRoeA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[10]</b> Gomez L,Karatzas D. Text Proposals:A text-specific selective search algorithm for word spotting in the wild[J]. Pattern Recognition,2017,70:60-74.
                            </a>
                        </p>
                        <p id="200">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fully convolutional networks for semantic segmentation">

                                <b>[11]</b> Long J,Shelhamer E,Darrell T. Fully convolutional networks for semantic segmentation[J]. IEEE Transactions on Pattern Analysis&amp;Machine Intelligence,2014,39(4):640-651.
                            </a>
                        </p>
                        <p id="202">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Localizing Text in Scene Images by Boundary Clustering, Stroke Segmentation, and String Fragment Classification">

                                <b>[12]</b> Chucai Y,Yingli T. Localizing text in scene images by boundary clustering,stroke segmentation,and string fragment classification[J]. IEEE Transactions on Image Processing,2012,21(9):4256-4268.
                            </a>
                        </p>
                        <p id="204">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Detecting Text in Natural Image with Connectionist Text Proposal Network">

                                <b>[13]</b> Tian Z,Huang W,He T,et al. Detecting text in natural image with connectionist text proposal network[C]//Computer Vision—ECCV 2016:14th European Conference,Amsterdam,The Netherlands,October 11-14,2016,Proceedings,Part VIII. 2016:56-72.
                            </a>
                        </p>
                        <p id="206">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Scene text deblurring using text-specific multiscale dictionaries">

                                <b>[14]</b> Cao X,Ren W,Zuo W,et al. Scene text deblurring using text-specific multiscale dictionaries[J]. IEEE Transactions on Image Processing,2015,24(4):1302-1314.
                            </a>
                        </p>
                        <p id="208">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Text Boxes:A fast text detector with a single deep neural network">

                                <b>[15]</b> Liao M,Shi B,Bai X,et al. Text Boxes:A fast text detector with a single deep neural network[C]//Thirty-First AAAI Conference on Artificial Intelligence,2016.
                            </a>
                        </p>
                        <p id="210">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Arbitrary-oriented scene text detection via rotation proposals">

                                <b>[16]</b> Ma J,Shao W,Ye H,et al. Arbitrary-oriented scene text detection via rotation proposals[J]. IEEE Transactions on Multimedia,2017,20(11):3111-3122.
                            </a>
                        </p>
                        <p id="212">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Scene text detection via holistic,multi-channel prediction[EB]">

                                <b>[17]</b> Yao C,Bai X,Sang N,et al. Scene text detection via holistic,multi-channel prediction[EB]. ar Xiv:1606. 09002,2016.
                            </a>
                        </p>
                        <p id="214">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep matching prior network:toward tighter multi-oriented text detection">

                                <b>[18]</b> Liu Y,Jin L. Deep matching prior network:Toward tighter multi-oriented text detection[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition(CVPR),2017.
                            </a>
                        </p>
                        <p id="216">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=EAST:An efficient and accurate scene text detector">

                                <b>[19]</b> Zhou X,Yao C,Wen H,et al. EAST:An efficient and accurate scene text detector[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition(CVPR),2017..
                            </a>
                        </p>
                        <p id="218">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Dense Box:Unifying landmark localization with end to end object detection[EB]">

                                <b>[20]</b> Huang L,Yang Y,Deng Y,et al. Dense Box:Unifying landmark localization with end to end object detection[EB].eprint ar Xiv:1509. 04874,2015.
                            </a>
                        </p>
                        <p id="220">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Adam:A method for stochastic optimization[EB]">

                                <b>[21]</b> Kingma D P,Ba J. Adam:A method for stochastic optimization[EB]. eprint ar Xiv:1412. 6980,2014.
                            </a>
                        </p>
                        <p id="222">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Show,attend and tell:Neural image caption generation with visual attention[EB]">

                                <b>[22]</b> Xu K,Ba J,Kiros R,et al. Show,attend and tell:Neural image caption generation with visual attention[EB]. eprint ar Xiv:1502. 03044,2015.
                            </a>
                        </p>
                        <p id="224">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Describing videos by exploiting temporal structure">

                                <b>[23]</b> Yao L,Torabi A,Cho K,et al. Describing videos by exploiting temporal structure[C]//2015 IEEE International Conference on Computer Vision(ICCV),2015.
                            </a>
                        </p>
                        <p id="226">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=ABC-CNN:An attention based convolutional neural network for visual question answering">

                                <b>[24]</b> Chen K,Wang J,Chen L C,et al. ABC-CNN:An attention based convolutional neural network for visual question answering[J]. eprint ar Xiv:1511. 05960,2015.
                            </a>
                        </p>
                        <p id="228">
                            <a id="bibliography_25" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Stacked Attention Networks for Image Question Answering">

                                <b>[25]</b> Yang Z,He X,Gao J,et al. Stacked attention networks for image question answering[C]//2016 IEEE Conference on Computer Vision and Pattern Recognition(CVPR),2015.
                            </a>
                        </p>
                        <p id="230">
                            <a id="bibliography_26" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Ask,attend and answer:Exploring question-guided spatial attention for visual question answering">

                                <b>[26]</b> Xu H,Saenko K. Ask,attend and answer:Exploring question-guided spatial attention for visual question answering[M]//Computer Vision—ECCV 2016. Springer International Publishing,2016.
                            </a>
                        </p>
                        <p id="232">
                            <a id="bibliography_27" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Control of goal-directed and stimulus-driven attention in the brain">

                                <b>[27]</b> Corbetta M,Shulman G L. Control of goal-directed and stimulus-driven attention in the brain[J]. Nature reviews Neuroscience,2002,3(3):215-229.
                            </a>
                        </p>
                        <p id="234">
                            <a id="bibliography_28" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Joint distance maps based action recognition with convolutional neural networks">

                                <b>[28]</b> Li C,Hou Y,Wang P,et al. Joint distance maps based action recognition with convolutional neural networks[J].IEEE Signal Processing Letters,2017,24(5):624-628.
                            </a>
                        </p>
                        <p id="236">
                            <a id="bibliography_29" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSGG201917028&amp;v=MjA3OTh0R0ZyQ1VSN3FmWnVadEZ5amtVcjdBTHo3TWFiRzRIOWpOcUk5SGJJUUtESDg0dlI0VDZqNTRPM3pxcUI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[29]</b>孙萍,胡旭东,张永军.结合注意力机制的深度学习图像目标检测[J/OL].计算机工程与应用:1-7[2019-04-29].
                            </a>
                        </p>
                        <p id="238">
                            <a id="bibliography_30" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201903010&amp;v=MjgwMDl1WnRGeWprVXI3QUx6N0JkN0c0SDlqTXJJOUVaSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlo=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[30]</b>孙亚圣,姜奇,胡洁,等.基于注意力机制的行人轨迹预测生成模型[J].计算机应用,2019,39(3):668-674.
                            </a>
                        </p>
                        <p id="240">
                            <a id="bibliography_31" target="_blank" href="http://scholar.cnki.net/result.aspx?q=SCA-CNN spatial and channel-wise attention in convolutional networks for image captioning">

                                <b>[31]</b> Chen L,Zhang H,Xiao J,et al. SCA-CNN:spatial and channel-wise attention in convolutional networks for image captioning[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition(CVPR),2017:6298-6306.
                            </a>
                        </p>
                        <p id="242">
                            <a id="bibliography_32" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD6BB00EB93AC104C12EB55DFC97247B75&amp;v=MjcxNTc1cDhEWHc5dkJjUm4wMTRUUXVVM3hzeWU3YVRONzJhQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQnJMVTA1dHRoeExtK3dhQT1OajdCYXJYS2JOSE0ydjFNWg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[32]</b> Xie S,Tu Z. Holistically-Nested Edge Detection[J]. International Journal of Computer Vision,2015,125(1/3):3-18.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JYRJ201909036" />
        <input id="dpi" type="hidden" value="600" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201909036&amp;v=MDY2MThRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVp0Rnlqa1VyN0FMelRaWkxHNEg5ak1wbzlHWW8=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
