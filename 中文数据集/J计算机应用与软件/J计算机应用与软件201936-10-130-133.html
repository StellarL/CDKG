<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637135556152971250%26DBCODE%3dCJFD%26TABLEName%3dCJFDTEMP%26FileName%3dJYRJ201910024%26RESULT%3d1%26SIGN%3dHrTmYRrZmW0PYvjn894NmUWh8nY%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JYRJ201910024&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JYRJ201910024&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201910024&amp;v=MDUwMzF0RmlEaFViM01MelRaWkxHNEg5ak5yNDlIWUlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVo=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#33" data-title="&lt;b&gt;0 引 言&lt;/b&gt; "><b>0 引 言</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#37" data-title="&lt;b&gt;1 方 法&lt;/b&gt; "><b>1 方 法</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#38" data-title="&lt;b&gt;1.1 Mask RCNN简介&lt;/b&gt;"><b>1.1 Mask RCNN简介</b></a></li>
                                                <li><a href="#41" data-title="&lt;b&gt;1.2 自下而上特征融合&lt;/b&gt;"><b>1.2 自下而上特征融合</b></a></li>
                                                <li><a href="#47" data-title="&lt;b&gt;1.3 空洞卷积神经网络&lt;/b&gt;"><b>1.3 空洞卷积神经网络</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#51" data-title="&lt;b&gt;2 实 验&lt;/b&gt; "><b>2 实 验</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#52" data-title="&lt;b&gt;2.1 实验环境与数据集&lt;/b&gt;"><b>2.1 实验环境与数据集</b></a></li>
                                                <li><a href="#54" data-title="&lt;b&gt;2.2 实验结果分析&lt;/b&gt;"><b>2.2 实验结果分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#64" data-title="&lt;b&gt;3 结 语&lt;/b&gt; "><b>3 结 语</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#40" data-title="图1 Mask RCNN框架结构">图1 Mask RCNN框架结构</a></li>
                                                <li><a href="#44" data-title="图2 特征融合路径">图2 特征融合路径</a></li>
                                                <li><a href="#45" data-title="图3 自下而上特征融合操作">图3 自下而上特征融合操作</a></li>
                                                <li><a href="#49" data-title="图4 空洞卷积示例">图4 空洞卷积示例</a></li>
                                                <li><a href="#59" data-title="&lt;b&gt;表1 三种分割方法测试结果对比&lt;/b&gt;"><b>表1 三种分割方法测试结果对比</b></a></li>
                                                <li><a href="#61" data-title="图5 不同方法分割结果示例">图5 不同方法分割结果示例</a></li>
                                                <li><a href="#85" data-title="图6 改善效果细节图">图6 改善效果细节图</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" title=" Pinheiro P O,Collobert R,Doll&#225;r P.Learning to segment object candidates[C]//Proceedings of the 28th International Conference on Neural Information Processing Systems:MIT Press,2015:1990-1998." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning to segment object candidates">
                                        <b>[1]</b>
                                         Pinheiro P O,Collobert R,Doll&#225;r P.Learning to segment object candidates[C]//Proceedings of the 28th International Conference on Neural Information Processing Systems:MIT Press,2015:1990-1998.
                                    </a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" title=" Dai J F,He K M,Sun J.Instance-aware semantic segmentation via multi-task network cascades[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition:IEEE Press,2015:3150-3158." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Instance-Aware Semantic Segmentation via Multi-task Network Cascades">
                                        <b>[2]</b>
                                         Dai J F,He K M,Sun J.Instance-aware semantic segmentation via multi-task network cascades[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition:IEEE Press,2015:3150-3158.
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title=" Chen L C,Papandreou G,Kokkinos I,et al.DeepLab:semantic image segmentation with deep convolutional nets,atrous convolution,and fully connected CRFs[J].IEEE Trans on Pattern Analysis &amp;amp; Machine Intelligence,2016,40(4):834-848." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep Lab:semantic image segmentation with deep convolutional nets,atrous convolution,and fully connected CRFs">
                                        <b>[3]</b>
                                         Chen L C,Papandreou G,Kokkinos I,et al.DeepLab:semantic image segmentation with deep convolutional nets,atrous convolution,and fully connected CRFs[J].IEEE Trans on Pattern Analysis &amp;amp; Machine Intelligence,2016,40(4):834-848.
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" title=" Li Yi,Qi Haozhi,Dai Jifeng,et al.Fully convolutional instance-aware semantic segmentation[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition:IEEE Press,2017:4438-4446." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fully convolutional instance-aware semantic segmentation">
                                        <b>[4]</b>
                                         Li Yi,Qi Haozhi,Dai Jifeng,et al.Fully convolutional instance-aware semantic segmentation[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition:IEEE Press,2017:4438-4446.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" title=" Dai J F,He K M,Li Y,et al.Instance-sensitive fully convolutional networks[C]//European Conference on Computer Vision,2016:534-549." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Instance-Sensitive Fully Convolutional Networks">
                                        <b>[5]</b>
                                         Dai J F,He K M,Li Y,et al.Instance-sensitive fully convolutional networks[C]//European Conference on Computer Vision,2016:534-549.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" title=" He K M,Gkioxari G,Doll&#225;r P,et al.Mask R-CNN[C]//2017 IEEE International Conference on Computer Vision (ICCV).IEEE,2017." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Mask R-CNN">
                                        <b>[6]</b>
                                         He K M,Gkioxari G,Doll&#225;r P,et al.Mask R-CNN[C]//2017 IEEE International Conference on Computer Vision (ICCV).IEEE,2017.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" title=" Ren S,He K,Girshick R,et al.Faster R-CNN:towards real-time object detection with region proposal networks[J].IEEE Transactions on Pattern Analysis &amp;amp; Machine Intelligence,2015,39(6):1137-1149." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Faster R-CNN:towards real-time object detection with region proposal networks">
                                        <b>[7]</b>
                                         Ren S,He K,Girshick R,et al.Faster R-CNN:towards real-time object detection with region proposal networks[J].IEEE Transactions on Pattern Analysis &amp;amp; Machine Intelligence,2015,39(6):1137-1149.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" title=" Long J,Shelhamer E,Darrell T.Fully convolutional networks for semantic segmentation[J].IEEE Trans on Pattern Analysis &amp;amp; Machine Intelligence,2015,39(4):640-651." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fully convolutional networks for semantic segmentation">
                                        <b>[8]</b>
                                         Long J,Shelhamer E,Darrell T.Fully convolutional networks for semantic segmentation[J].IEEE Trans on Pattern Analysis &amp;amp; Machine Intelligence,2015,39(4):640-651.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" title=" Lin T Y,Doll&#225;r P,Girshick R,et al.Feature pyramid networks for object detection[C]//Conference on Computer Vision and Pattern Recognition,2016:936-944." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Feature Pyramid Networks for Object Detection">
                                        <b>[9]</b>
                                         Lin T Y,Doll&#225;r P,Girshick R,et al.Feature pyramid networks for object detection[C]//Conference on Computer Vision and Pattern Recognition,2016:936-944.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_10" title=" Chen L C,Zhu Y K,Papandreou G,et al.Encoder-decoder with atrous separable convolution for semantic image segmentation[C]//The European Conference on Computer Vision,2018." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Encoder-decoder with atrous separable convolution for semantic image segmentation">
                                        <b>[10]</b>
                                         Chen L C,Zhu Y K,Papandreou G,et al.Encoder-decoder with atrous separable convolution for semantic image segmentation[C]//The European Conference on Computer Vision,2018.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_11" title=" Luc P,Couprie C,Lecun Y,et al.Predicting future instance segmentation by forecasting convolutional features[C]//The European Conference on Computer Vision,2018." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Predicting future instance segmentation by forecasting convolutional features">
                                        <b>[11]</b>
                                         Luc P,Couprie C,Lecun Y,et al.Predicting future instance segmentation by forecasting convolutional features[C]//The European Conference on Computer Vision,2018.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_12" title=" Zhang H,Dana K,Shi J,et al.Context encoding for semantic segmentation[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.IEEE Press,2018." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Context encoding for semantic segmentation">
                                        <b>[12]</b>
                                         Zhang H,Dana K,Shi J,et al.Context encoding for semantic segmentation[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.IEEE Press,2018.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_13" title=" Zhang Z L,Zhang X Y,Peng C,et al.ExFuse:enhancing feature fusion for semantic segmentation[C]//The European Conference on Computer Vision,2018." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=ExFuse enhancing feature fusion for semantic segmentation">
                                        <b>[13]</b>
                                         Zhang Z L,Zhang X Y,Peng C,et al.ExFuse:enhancing feature fusion for semantic segmentation[C]//The European Conference on Computer Vision,2018.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_14" title=" Abadi M,Barham P,Chen J,et al.TensorFlow:A system for large-scale machine learning[C]//Proceedings of the 12th USENIX conference on Operating Systems Design and Implementation,2016:265-283." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=TensorFlow:a system for large-scale machine learning">
                                        <b>[14]</b>
                                         Abadi M,Barham P,Chen J,et al.TensorFlow:A system for large-scale machine learning[C]//Proceedings of the 12th USENIX conference on Operating Systems Design and Implementation,2016:265-283.
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_15" title=" Lin T Y,Maire M,Belongie S,et al.Microsoft COCO:common objects in context[C]//European Conference on Computer Vision,2014:740-755." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Microsoft COCO:Common objects in context">
                                        <b>[15]</b>
                                         Lin T Y,Maire M,Belongie S,et al.Microsoft COCO:common objects in context[C]//European Conference on Computer Vision,2014:740-755.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JYRJ" target="_blank">计算机应用与软件</a>
                2019,36(10),130-133 DOI:10.3969/j.issn.1000-386x.2019.10.023            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>一种改进的Mask RCNN特征融合实例分割方法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%B8%A9%E5%B0%A7%E4%B9%90&amp;code=39694700&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">温尧乐</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%8E%E6%9E%97%E7%87%95&amp;code=39694699&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">李林燕</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%B0%9A%E6%AC%A3%E8%8C%B9&amp;code=42911870&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">尚欣茹</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%83%A1%E4%BC%8F%E5%8E%9F&amp;code=31281244&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">胡伏原</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E8%8B%8F%E5%B7%9E%E7%A7%91%E6%8A%80%E5%A4%A7%E5%AD%A6%E7%94%B5%E5%AD%90%E4%B8%8E%E4%BF%A1%E6%81%AF%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0228583&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">苏州科技大学电子与信息工程学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E8%8B%8F%E5%B7%9E%E7%A7%91%E6%8A%80%E5%A4%A7%E5%AD%A6%E8%8B%8F%E5%B7%9E%E5%B8%82%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B8%8E%E4%BF%A1%E6%81%AF%E6%9C%8D%E5%8A%A1%E9%87%8D%E7%82%B9%E5%AE%9E%E9%AA%8C%E5%AE%A4&amp;code=0262405&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">苏州科技大学苏州市大数据与信息服务重点实验室</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E8%8B%8F%E5%B7%9E%E7%BB%8F%E8%B4%B8%E8%81%8C%E4%B8%9A%E6%8A%80%E6%9C%AF%E5%AD%A6%E9%99%A2&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">苏州经贸职业技术学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E8%8B%8F%E5%B7%9E%E7%A7%91%E6%8A%80%E5%A4%A7%E5%AD%A6%E8%8B%8F%E5%B7%9E%E5%B8%82%E8%99%9A%E6%8B%9F%E7%8E%B0%E5%AE%9E%E6%99%BA%E8%83%BD%E4%BA%A4%E4%BA%92%E5%8F%8A%E5%BA%94%E7%94%A8%E6%8A%80%E6%9C%AF%E9%87%8D%E7%82%B9%E5%AE%9E%E9%AA%8C%E5%AE%A4&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">苏州科技大学苏州市虚拟现实智能交互及应用技术重点实验室</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>实例分割需要兼顾像素级的分类准确性和目标实例级的高级语义特性,非常具有挑战性。由于特征金字塔网络低层特征到高层特征的融合路径太长,导致低层特征在整个特征层次中的作用较弱。在特征金字塔网络的基础上,引入一条自下而上的路径来增强整个特征层次,缩短较低层特征与顶部特征之间的融合路径,增强低层特征在整个特征层次中的作用;在卷积神经网络中引入空洞卷积算法扩大卷积感受域,进一步提升掩膜预测准确度。在Microsoft COCO数据集测试结果表明,该方法有效提高了实例分割的准确度。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%AE%9E%E4%BE%8B%E5%88%86%E5%89%B2&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">实例分割;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%89%B9%E5%BE%81%E8%9E%8D%E5%90%88&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">特征融合;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">卷积神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%A9%BA%E6%B4%9E%E5%8D%B7%E7%A7%AF&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">空洞卷积;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    温尧乐，硕士生，主研领域:计算机视觉。;
                                </span>
                                <span>
                                    李林燕，高工。;
                                </span>
                                <span>
                                    尚欣茹，硕士生。;
                                </span>
                                <span>
                                    胡伏原，教授。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-02-25</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金项目(61876121,61472267,61672371);</span>
                                <span>江苏省重点研发计划项目(BE2017663);</span>
                    </p>
            </div>
                    <h1><b>AN INSTANCE SEGMENTATION METHOD BASED ON IMPROVED MASK RCNN FEATURE FUSION</b></h1>
                    <h2>
                    <span>Wen Yaole</span>
                    <span>Li Linyan</span>
                    <span>Shang Xinru</span>
                    <span>Hu Fuyuan</span>
            </h2>
                    <h2>
                    <span>School of Electronic and Information Engineering,Suzhou University of Science and Technology</span>
                    <span>Suzhou Key Laboratory for Big Data and Information Service,Suzhou University of Science and Technology</span>
                    <span>Suzhou Institute of Trade and Commerce</span>
                    <span>Virtual Reality Key Laboratory of Intelligent Interaction and Application Technology of Suzhou,Suzhou University of Scienceand Technology</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Instance segmentation needs to focus on both the classification accuracy at the pixel level and the high-level semantic features at the target instance level, which is very challenging. Due to the long fusion path from low-level features to high-level features of feature pyramid networks, the role of low-level features in the whole feature hierarchy is weakened. Based on the feature pyramid networks, this paper introduced a bottom-to-top path to enhance the entire feature hierarchy, hence shortening the fusion path between low-level features and top features. It also enhanced the function of lower features in the entire feature hierarchy. The atrous convolution was applied into the convolutional neural network to expand the convolution receptive domain and further improve the mask prediction accuracy. The test results in the Microsoft COCO dataset show that the method effectively improves the accuracy of the segmentation results.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Instance%20segmentation&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Instance segmentation;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Feature%20fusion&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Feature fusion;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Convolutional%20neural%20network(CNN)&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Convolutional neural network(CNN);</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Atrous%20convolution&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Atrous convolution;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2019-02-25</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="33" name="33" class="anchor-tag"><b>0 引 言</b></h3>
                <div class="p1">
                    <p id="34">实例分割是计算机视觉的主要研究内容之一,它是将图像中不同类别的个体精准地分割出来,对图像中的每一个像素都进行类别的标注,并对同一类别的不同个体进行区分。随着深度学习的快速发展,精确且高效的实例分割技术的需求越来越大,如在自动驾驶、智慧农业、视频监控等领域,该技术得到了越来越多研究人员的关注。</p>
                </div>
                <div class="p1">
                    <p id="35">目前实例分割的大多数方法是基于候选区域的。Pinheiro等<citation id="69" type="reference"><link href="3" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>提出的DeepMask通过输入图像中出现的实例来输出预测候选掩膜,以此分割出每个实例对象,但是对边界分割的准确度较低;Dai等<citation id="70" type="reference"><link href="5" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>使用共享特征图将提议的实例对象从边界框中分割出来,大大提高了计算速度;Chen等<citation id="71" type="reference"><link href="7" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>提出了Deeplab网络结构,该网络中使用空洞卷积核,从而使得在增加卷积感受域的同时保持特征图的尺寸不变,避免了最后需要对特征图进行上采样来扩大尺寸造成的分割结果不精细的弊端;Li等<citation id="72" type="reference"><link href="9" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>提出的全卷积实例分割(FCIS)是首个端到端的实例分割框架,通过对位置敏感的得分图<citation id="73" type="reference"><link href="11" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>进行改进,FCIS同时预测边界框和实例掩膜,但FCIS对处理重叠对象实例时,在其重叠区域分割效果相对薄弱,仅粗略地检测各个实例对象的边界;He等<citation id="74" type="reference"><link href="13" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>提出的Mask RCNN框架有较为精细的实例分割结果。该框架基于Faster RCNN<citation id="75" type="reference"><link href="15" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>用于检测目标和分类,利用全卷积网络(FCN)<citation id="76" type="reference"><link href="17" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>用于掩模预测,使用特征金字塔网络(FPN)<citation id="77" type="reference"><link href="19" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>提取网络中的特征层次,区域建议网络(RPN)根据提取的特征生成边界框,提出的兴趣区域对齐算法(RoIAlign)来获得精确的感兴趣区域并产生实例级结果。但是对基于候选区域的实例分割方法而言,其分割准确度非常依赖于区域建议网络(RPN),而对生成特征的利用效率极大的影响RPN的性能,这是目前此类方法亟待解决的问题。</p>
                </div>
                <div class="p1">
                    <p id="36">由于低层次特征信息的传播效率对于增强整个特征层次有较大影响,而Mask RCNN中低层次特征与高层次特征的融合路径太长,导致了低层特征的位置信息没有被很好的利用。为了增强低层特征在全局特征中的作用,本文利用低层特征中具有的目标位置信息,引入一条自下而上的特征融合路径,缩短了低层特征与高层特征的融合路径,提高了特征金字塔网络融合特征的能力。另外,由于传统卷积神经网络中卷积核的大小与其感受域是一对矛盾,导致在上采样过程中图像信息损失严重。故在卷积神经网络中借助空洞卷积核来扩大感受域,减小提取高层特征图像信息的损失<citation id="78" type="reference"><link href="21" rel="bibliography" /><link href="25" rel="bibliography" /><sup>[<a class="sup">10</a>,<a class="sup">12</a>]</sup></citation>,提高了掩膜预测准确度。</p>
                </div>
                <h3 id="37" name="37" class="anchor-tag"><b>1 方 法</b></h3>
                <h4 class="anchor-tag" id="38" name="38"><b>1.1 Mask RCNN简介</b></h4>
                <div class="p1">
                    <p id="39">Mask RCNN实例分割包括:定位目标、目标类别分类、分割掩膜预测三个部分。如图1所示。首先输入一幅图片后,利用特征金字塔网络(FPN)经过一系列的卷积、池化操作提取出图像的特征图(feature map);其次RPN<citation id="79" type="reference"><link href="15" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>网络在特征图上选定出候选目标,使用softmax分类器来判别候选目标属于背景还是前景,同时利用范围框回归器修正候选目标的位置,生成候选目标区域。最后利用全卷积网络(FCN)预测相应的目标分割掩膜。分类网络利用特征图和RPN网络生成的候选区域实现目标类别的检测,FCN利用特征图实现目标的像素级精确分割。由于FPN低层特征与高层特征的融合路径较长,导致低层特征的作用没有被充分利用起来。</p>
                </div>
                <div class="area_img" id="40">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201910024_040.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 Mask RCNN框架结构" src="Detail/GetImg?filename=images/JYRJ201910024_040.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 Mask RCNN框架结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201910024_040.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="41" name="41"><b>1.2 自下而上特征融合</b></h4>
                <div class="p1">
                    <p id="42">为提升基于侯选区域的实例分割框架内的各层次特征的信息传播与融合<citation id="80" type="reference"><link href="23" rel="bibliography" /><link href="27" rel="bibliography" /><sup>[<a class="sup">11</a>,<a class="sup">13</a>]</sup></citation>,在特征金字塔结构(FPN)的基础上引入一条自下向上的路径来增强低层特征中的位置信息,建立低层特征和高层特征之间的特征信息融合路径。</p>
                </div>
                <div class="p1">
                    <p id="43">本文以ResNet作为基础网络结构,使用FPN生成的特征网络阶段P2、P3、P4<citation id="81" type="reference"><link href="19" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>,FPN高层特征(P4)向下与低层特征(P2)的融合路径如图2(a)所示。每个阶段的最后一层特征作为该阶段的输出特征。在自下而上的融合路径中,L2和P2完全相同。首先,对L2特征进行步幅为2的3×3卷积核来缩小空间尺寸。其次,通过横向连接与较高层特征(P3)逐元素相加(经过1×1卷积核,保持通道数目相同)生成融合后的特征阶段(L3)。最后将融合后的特征阶段经过多层卷积生成该阶段的特征图,并将最后一层的特征图输出。这是一个逐层迭代的过程,到达顶层(P4)后终止,如图2(b)所示。至此从低层到高层的特征融合路径能够达到5层到10层之间(图2长虚线),相比之下,FPN中由低层到高层的特征融合路径达到了100层以上(图2短虚线)。其中,横向连接特征融合如图3所示。</p>
                </div>
                <div class="area_img" id="44">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201910024_04400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 特征融合路径" src="Detail/GetImg?filename=images/JYRJ201910024_04400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 特征融合路径  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201910024_04400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="45">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201910024_045.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 自下而上特征融合操作" src="Detail/GetImg?filename=images/JYRJ201910024_045.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 自下而上特征融合操作  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201910024_045.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="46">经融合后生成的L2、L3、L4共同组成新的特征金字塔结构,新的特征金字塔结构组成的各层特征供后续网络使用。为了能使每层输出的特征在后续的网络中共享一个分类层,每层的特征图始终保持256的通道数的输出。</p>
                </div>
                <h4 class="anchor-tag" id="47" name="47"><b>1.3 空洞卷积神经网络</b></h4>
                <div class="p1">
                    <p id="48">局部感受域是卷积神经网络(CNN)中非常重要的一个概念,但是CNN在做实例分割任务时,会导致最后的特征图尺寸远远小于输入图像的尺寸。最终预测得到的分割掩膜(Mask)会由于过度上采样而比较粗糙。由于空洞卷积算法能够控制卷积核的<i>rate</i>的大小,从而能够得到不同大小的卷积感受域,因此该算法解决了CNN中对于提高感受域和保持特征图尺寸之间的矛盾的问题<citation id="82" type="reference"><link href="7" rel="bibliography" /><link href="21" rel="bibliography" /><link href="25" rel="bibliography" /><sup>[<a class="sup">3</a>,<a class="sup">10</a>,<a class="sup">12</a>]</sup></citation>。图4(a)中显示的是传统的3×3大小的卷积核作用的感受域,与<i>rate</i>=1的3×3大小的空洞卷积核感受域相同,每次覆盖的区域是3×3大小的。图4(b)中对应的是<i>rate</i>=2的3×3大小的空洞卷积核作用的感受域,该卷积核仍然是3×3大小的,但是该卷积核的感受域增大到了7×7。</p>
                </div>
                <div class="area_img" id="49">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201910024_04900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 空洞卷积示例" src="Detail/GetImg?filename=images/JYRJ201910024_04900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 空洞卷积示例  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201910024_04900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="50">本文在特征金字塔结构中加入3种不同<i>rate</i>的空洞卷积核来提取特征,对每个金字塔阶段的最后一层输出特征进行空洞卷积操作,在空间尺寸很小的高层特征中,比较好地保留了高层特征信息;这些全局的共享特征输出到FCN网络中,在卷积层中仍然使用3种不同<i>rate</i>的卷积核,在上采样过程中保留了更多的图像信息,最终在像素级的类别预测阶段,能够有效提高掩膜预测的准确度。</p>
                </div>
                <h3 id="51" name="51" class="anchor-tag"><b>2 实 验</b></h3>
                <h4 class="anchor-tag" id="52" name="52"><b>2.1 实验环境与数据集</b></h4>
                <div class="p1">
                    <p id="53">本文算法的具体实现使用的是深度学习框架Tensorflow<citation id="83" type="reference"><link href="29" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>,实验环境为Ubantu14.04操作系统,使用4块NVIDIA 1080Ti图形处理器(GPU)加速运算。以ResNeXt-101-FPN网络作为基准网络,在Microsoft COCO<citation id="84" type="reference"><link href="31" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>数据集上训练所有模型。该数据集由30多万幅图片、80个类别构成。本文在trainval35k子集共80 000万幅图片来进行训练,在minival子集共5 000幅图片来进行测试和验证。</p>
                </div>
                <h4 class="anchor-tag" id="54" name="54"><b>2.2 实验结果分析</b></h4>
                <div class="p1">
                    <p id="55">实验使用随机梯度下降法进行训练,设置衰减系数为0.000 1,动量系数为0.9,初始学习速率设置为0.002。使用的评价指标为预测的类别像素点和正确的类别像素点的准确度(intersection over union,IoU),以及平均IoU(Mean intersection over union,mIoU),其定义如下:</p>
                </div>
                <div class="p1">
                    <p id="56"><mathml id="67"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ι</mi><mi>o</mi><mi>U</mi><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo><mo>=</mo><mi>n</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>i</mi></mrow></msub><mo>/</mo><mrow><mo>(</mo><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mi>j</mi></munder><mo stretchy="false">(</mo></mstyle><mi>n</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>+</mo><mi>n</mi><msub><mrow></mrow><mrow><mi>j</mi><mi>i</mi></mrow></msub><mo stretchy="false">)</mo><mo>-</mo><mi>n</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>i</mi></mrow></msub></mrow><mo>)</mo></mrow></mrow></math></mathml>      (1)</p>
                </div>
                <div class="p1">
                    <p id="57"><mathml id="68"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>m</mi><mi>Ι</mi><mi>o</mi><mi>U</mi><mo>=</mo><mfrac><mn>1</mn><mi>Ν</mi></mfrac><mstyle displaystyle="true"><munder><mo>∑</mo><mi>i</mi></munder><mi>Ι</mi></mstyle><mi>o</mi><mi>U</mi><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></math></mathml>      (2)</p>
                </div>
                <div class="p1">
                    <p id="58">式中:<i>n</i><sub><i>ii</i></sub>代表类别为AP<sup>50</sup>的像素点数目被预测为类别<i>i</i>的像素点数目(即正确分割的像素点数目);<i>n</i><sub><i>ij</i></sub>代表类别为<i>i</i>的像素点数目被预测为类别<i>j</i>的像素点数目;<i>n</i><sub><i>ji</i></sub>代表类别为<i>j</i>的像素点数目被预测为类别<i>i</i>的像素点数目;<i>N</i>表示类别数。另外,定义IoU阈值超过0.5和0.75时的指标AP<sup>50</sup>和AP<sup>75</sup>,如表1所示。结果表明,通过引入自下而上的特征融合路径,增强了低层特征的作用后,对物体的分割平均准确度提高了3.7%。</p>
                </div>
                <div class="area_img" id="59">
                    <p class="img_tit"><b>表1 三种分割方法测试结果对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="59" border="1"><tr><td>算法</td><td>基准</td><td>mIoU</td><td><i>AP</i><sup>50</sup></td><td><i>AP</i><sup>75</sup></td></tr><tr><td><br />FCIS<sup>[4]</sup></td><td>ResNet-101-C5-dilated</td><td>29.2</td><td>49.5</td><td>-</td></tr><tr><td><br />Mask R-CNN<sup>[6]</sup></td><td>ResNeXt-101-FPN</td><td>37.1</td><td>60.0</td><td>39.4</td></tr><tr><td><br />本文算法</td><td>ResNeXt-101-FPN</td><td>40.8</td><td>62.9</td><td>42.6</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="60">图像实例分割结果如图5所示,本文方法与Mask RCNN相比,对于大目标的分割更加精细,这是由于低层特征位置信息与高层特征融合后的作用;对于小目标而言,由于空洞卷积核的使用,上采样过程中图像细节损失没有之前严重,使得一些小目标被检测和识别,改善效果细节如图6所示。</p>
                </div>
                <div class="area_img" id="61">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201910024_06100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 不同方法分割结果示例" src="Detail/GetImg?filename=images/JYRJ201910024_06100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 不同方法分割结果示例  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201910024_06100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="85">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201910024_08500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 改善效果细节图" src="Detail/GetImg?filename=images/JYRJ201910024_08500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 改善效果细节图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201910024_08500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h3 id="64" name="64" class="anchor-tag"><b>3 结 语</b></h3>
                <div class="p1">
                    <p id="65">本文提出一种改进的Mask RCNN特征融合实例分割方法。通过在特征金字塔网络的基础上引入一条自下而上的特征融合路径来改善全局特征,有效利用了低层次特征的优点。在COCO数据集下的实验结果表明,所提方法有效提高了实例分割的准确度。</p>
                </div>
                <div class="p1">
                    <p id="66">由于掩膜的预测是基于单个网络阶段的输出,这对于目标尺寸差别较大的时候并没有最佳匹配到相应的特征层级,未来考虑将融合后的多层次特征用来自适应目标尺寸大小,使各个特征层次的优势得到有效利用。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning to segment object candidates">

                                <b>[1]</b> Pinheiro P O,Collobert R,Dollár P.Learning to segment object candidates[C]//Proceedings of the 28th International Conference on Neural Information Processing Systems:MIT Press,2015:1990-1998.
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Instance-Aware Semantic Segmentation via Multi-task Network Cascades">

                                <b>[2]</b> Dai J F,He K M,Sun J.Instance-aware semantic segmentation via multi-task network cascades[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition:IEEE Press,2015:3150-3158.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep Lab:semantic image segmentation with deep convolutional nets,atrous convolution,and fully connected CRFs">

                                <b>[3]</b> Chen L C,Papandreou G,Kokkinos I,et al.DeepLab:semantic image segmentation with deep convolutional nets,atrous convolution,and fully connected CRFs[J].IEEE Trans on Pattern Analysis &amp; Machine Intelligence,2016,40(4):834-848.
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fully convolutional instance-aware semantic segmentation">

                                <b>[4]</b> Li Yi,Qi Haozhi,Dai Jifeng,et al.Fully convolutional instance-aware semantic segmentation[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition:IEEE Press,2017:4438-4446.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Instance-Sensitive Fully Convolutional Networks">

                                <b>[5]</b> Dai J F,He K M,Li Y,et al.Instance-sensitive fully convolutional networks[C]//European Conference on Computer Vision,2016:534-549.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Mask R-CNN">

                                <b>[6]</b> He K M,Gkioxari G,Dollár P,et al.Mask R-CNN[C]//2017 IEEE International Conference on Computer Vision (ICCV).IEEE,2017.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Faster R-CNN:towards real-time object detection with region proposal networks">

                                <b>[7]</b> Ren S,He K,Girshick R,et al.Faster R-CNN:towards real-time object detection with region proposal networks[J].IEEE Transactions on Pattern Analysis &amp; Machine Intelligence,2015,39(6):1137-1149.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fully convolutional networks for semantic segmentation">

                                <b>[8]</b> Long J,Shelhamer E,Darrell T.Fully convolutional networks for semantic segmentation[J].IEEE Trans on Pattern Analysis &amp; Machine Intelligence,2015,39(4):640-651.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Feature Pyramid Networks for Object Detection">

                                <b>[9]</b> Lin T Y,Dollár P,Girshick R,et al.Feature pyramid networks for object detection[C]//Conference on Computer Vision and Pattern Recognition,2016:936-944.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Encoder-decoder with atrous separable convolution for semantic image segmentation">

                                <b>[10]</b> Chen L C,Zhu Y K,Papandreou G,et al.Encoder-decoder with atrous separable convolution for semantic image segmentation[C]//The European Conference on Computer Vision,2018.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Predicting future instance segmentation by forecasting convolutional features">

                                <b>[11]</b> Luc P,Couprie C,Lecun Y,et al.Predicting future instance segmentation by forecasting convolutional features[C]//The European Conference on Computer Vision,2018.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Context encoding for semantic segmentation">

                                <b>[12]</b> Zhang H,Dana K,Shi J,et al.Context encoding for semantic segmentation[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.IEEE Press,2018.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=ExFuse enhancing feature fusion for semantic segmentation">

                                <b>[13]</b> Zhang Z L,Zhang X Y,Peng C,et al.ExFuse:enhancing feature fusion for semantic segmentation[C]//The European Conference on Computer Vision,2018.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=TensorFlow:a system for large-scale machine learning">

                                <b>[14]</b> Abadi M,Barham P,Chen J,et al.TensorFlow:A system for large-scale machine learning[C]//Proceedings of the 12th USENIX conference on Operating Systems Design and Implementation,2016:265-283.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Microsoft COCO:Common objects in context">

                                <b>[15]</b> Lin T Y,Maire M,Belongie S,et al.Microsoft COCO:common objects in context[C]//European Conference on Computer Vision,2014:740-755.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JYRJ201910024" />
        <input id="dpi" type="hidden" value="600" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201910024&amp;v=MDUwMzF0RmlEaFViM01MelRaWkxHNEg5ak5yNDlIWUlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVo=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="0" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
