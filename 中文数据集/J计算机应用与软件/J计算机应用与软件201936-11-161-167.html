<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637134079567131250%26DBCODE%3dCJFD%26TABLEName%3dCJFDTEMP%26FileName%3dJYRJ201911028%26RESULT%3d1%26SIGN%3dRo8j9sF7OfDvN44cSrEs%252bHDjzo8%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JYRJ201911028&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JYRJ201911028&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201911028&amp;v=MDU4MzZiSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnVGeS9oVUxySkx6VFpaTEc0SDlqTnJvOUg=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#69" data-title="&lt;b&gt;0 引 言&lt;/b&gt; "><b>0 引 言</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#75" data-title="&lt;b&gt;1 SIFT-CNN人脸表情识别算法&lt;/b&gt; "><b>1 SIFT-CNN人脸表情识别算法</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#78" data-title="&lt;b&gt;1.1 图像预处理&lt;/b&gt;"><b>1.1 图像预处理</b></a></li>
                                                <li><a href="#81" data-title="&lt;b&gt;1.2 全局特征提取&lt;/b&gt;"><b>1.2 全局特征提取</b></a></li>
                                                <li><a href="#93" data-title="&lt;b&gt;1.3 局部特征提取&lt;/b&gt;"><b>1.3 局部特征提取</b></a></li>
                                                <li><a href="#121" data-title="&lt;b&gt;1.4 特征融合&lt;/b&gt;"><b>1.4 特征融合</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#125" data-title="&lt;b&gt;2 实 验&lt;/b&gt; "><b>2 实 验</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#126" data-title="&lt;b&gt;2.1 数据集简介&lt;/b&gt;"><b>2.1 数据集简介</b></a></li>
                                                <li><a href="#130" data-title="&lt;b&gt;2.2 训练方法&lt;/b&gt;"><b>2.2 训练方法</b></a></li>
                                                <li><a href="#135" data-title="&lt;b&gt;2.3 实验结果比较&lt;/b&gt;"><b>2.3 实验结果比较</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#143" data-title="&lt;b&gt;3 结 语&lt;/b&gt; "><b>3 结 语</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#77" data-title="图1 SIFT-CNN表情识别算法流程图">图1 SIFT-CNN表情识别算法流程图</a></li>
                                                <li><a href="#80" data-title="图2 CK+数据集部分预处理图像">图2 CK+数据集部分预处理图像</a></li>
                                                <li><a href="#85" data-title="图3 CNN结构图">图3 CNN结构图</a></li>
                                                <li><a href="#91" data-title="图4 2&#215;2的最大值下采样">图4 2×2的最大值下采样</a></li>
                                                <li><a href="#114" data-title="图5 SIFT 128维特征向量">图5 SIFT 128维特征向量</a></li>
                                                <li><a href="#116" data-title="图6 人脸图像的SIFT特征向量">图6 人脸图像的SIFT特征向量</a></li>
                                                <li><a href="#120" data-title="图7 视觉词袋模型步骤">图7 视觉词袋模型步骤</a></li>
                                                <li><a href="#137" data-title="&lt;b&gt;表1 不同方法在CK+数据集上识别结果比较&lt;/b&gt;"><b>表1 不同方法在CK+数据集上识别结果比较</b></a></li>
                                                <li><a href="#138" data-title="&lt;b&gt;表2 不同方法在FER2013数据集上识别结果比较&lt;/b&gt;"><b>表2 不同方法在FER2013数据集上识别结果比较</b></a></li>
                                                <li><a href="#140" data-title="&lt;b&gt;表3 CK+数据集的混淆矩阵&lt;/b&gt;"><b>表3 CK+数据集的混淆矩阵</b></a></li>
                                                <li><a href="#141" data-title="&lt;b&gt;表4 FER2013数据集的混淆矩阵&lt;/b&gt;"><b>表4 FER2013数据集的混淆矩阵</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="172">


                                    <a id="bibliography_1" title=" Zavaschi T H H,Britto A S,Oliveira L E S,et al.Fusion of feature sets and classifiers for facial expression recognition[J].Expert Systems with Applications,2013,40(2):646-655." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14010600397696&amp;v=Mjk0MjFNbndaZVp1SHlqbVViL0lKMW9YYnhNPU5pZk9mYks4SHRETXFZOUZaK0lJQ25VL29CTVQ2VDRQUUgvaXJSZEdlcnFRVA==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         Zavaschi T H H,Britto A S,Oliveira L E S,et al.Fusion of feature sets and classifiers for facial expression recognition[J].Expert Systems with Applications,2013,40(2):646-655.
                                    </a>
                                </li>
                                <li id="174">


                                    <a id="bibliography_2" title=" Hu B,Wang J.3D facial expression recognition method based on bimodal and semantic knowledge[J].Chinese Journal of Scientific Instrument,2013,34(4):873-880." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=YQXB201304024&amp;v=MDU4NTMzenFxQnRHRnJDVVJMT2VaZVZ1RnkvaFVMckpQRHpUYkxHNEg5TE1xNDlIWUlRS0RIODR2UjRUNmo1NE8=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         Hu B,Wang J.3D facial expression recognition method based on bimodal and semantic knowledge[J].Chinese Journal of Scientific Instrument,2013,34(4):873-880.
                                    </a>
                                </li>
                                <li id="176">


                                    <a id="bibliography_3" title=" Sandbach G,Zafeiriou S,Pantic M,et al.Static and dynamic 3D facial expression recognition:A comprehensive survey [J].Image &amp;amp; Vision Computing,2012,30(10):683-697." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012201348695&amp;v=MjA5OTZIeWptVWIvSUoxb1hieE09TmlmT2ZiSzdIdERPclk5RVorOEhDblU4b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadQ==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         Sandbach G,Zafeiriou S,Pantic M,et al.Static and dynamic 3D facial expression recognition:A comprehensive survey [J].Image &amp;amp; Vision Computing,2012,30(10):683-697.
                                    </a>
                                </li>
                                <li id="178">


                                    <a id="bibliography_4" title=" Zhou J,Zhang S,Mei H,et al.A method of facial expression recognition based on Gabor and NMF[J].Pattern Recognition and Image Analysis,2016,26(1):119-124." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Method of Facial Expression Recognition Based on Gabor and NMF">
                                        <b>[4]</b>
                                         Zhou J,Zhang S,Mei H,et al.A method of facial expression recognition based on Gabor and NMF[J].Pattern Recognition and Image Analysis,2016,26(1):119-124.
                                    </a>
                                </li>
                                <li id="180">


                                    <a id="bibliography_5" title=" Gu W,Xiang C,Venkatesh Y V,et al.Facial expression recognition using radial encoding of local Gabor features and classifier synthesis[J].Pattern Recognition,2012,45(1):80-91." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011600738060&amp;v=MzE3MDhESG81b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VYi9JSjFvWGJ4TT1OaWZPZmJLN0h0RE5xWTlGWStnSA==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                         Gu W,Xiang C,Venkatesh Y V,et al.Facial expression recognition using radial encoding of local Gabor features and classifier synthesis[J].Pattern Recognition,2012,45(1):80-91.
                                    </a>
                                </li>
                                <li id="182">


                                    <a id="bibliography_6" title=" Jia Q,Gao X,Guo H,et al.Multi-layer sparse representation for weighted LBP-patches based facial expression recognition[J].Sensors,2015,15(3):6719." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multi-Layer Sparse Representation for Weighted LBP-Patches Based Facial Expression Recognition">
                                        <b>[6]</b>
                                         Jia Q,Gao X,Guo H,et al.Multi-layer sparse representation for weighted LBP-patches based facial expression recognition[J].Sensors,2015,15(3):6719.
                                    </a>
                                </li>
                                <li id="184">


                                    <a id="bibliography_7" title=" Wang L,Li R F,Wang K,et al.Feature representation for facial expression recognition based on FACS and LBP[J].International Journal of Automation and Computing,2014,11(5):459-468." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD14101300049680&amp;v=MTUzMjM1b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VYi9JSjFvWGJ4TT1OajdCYXJLOEg5SE5ySTlGWk84R0NuUQ==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                         Wang L,Li R F,Wang K,et al.Feature representation for facial expression recognition based on FACS and LBP[J].International Journal of Automation and Computing,2014,11(5):459-468.
                                    </a>
                                </li>
                                <li id="186">


                                    <a id="bibliography_8" title=" Shan C,Gong S,Mcowan P W.Facial expression recognition based on Local Binary Patterns:A comprehensive study[J].Image and Vision Computing,2009,27(6):803-816." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012201349092&amp;v=MDUzNjRKMW9YYnhNPU5pZk9mYks3SHRET3JZOUVaKzhHREhVN29CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVWIvSQ==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                         Shan C,Gong S,Mcowan P W.Facial expression recognition based on Local Binary Patterns:A comprehensive study[J].Image and Vision Computing,2009,27(6):803-816.
                                    </a>
                                </li>
                                <li id="188">


                                    <a id="bibliography_9" title=" Wang X,Jin C,Liu W,et al.Feature Fusion of HOG and WLD for Facial Expression Recognition[C]//IEEE/SICE International Symposium on System Integration.IEEE,2014." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Feature fusion of HOG and WLD for facial expression recognition">
                                        <b>[9]</b>
                                         Wang X,Jin C,Liu W,et al.Feature Fusion of HOG and WLD for Facial Expression Recognition[C]//IEEE/SICE International Symposium on System Integration.IEEE,2014.
                                    </a>
                                </li>
                                <li id="190">


                                    <a id="bibliography_10" title=" 刘洋,韩广良,史春蕾.基于SIFT算法的多表情人脸识别[J].液晶与显示,2016(12):1156-1160." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=YJYS201612010&amp;v=MDIzOTF1RnkvaFVMckpQQ2ZTZmJHNEg5Zk5yWTlFWklRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVY=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[10]</b>
                                         刘洋,韩广良,史春蕾.基于SIFT算法的多表情人脸识别[J].液晶与显示,2016(12):1156-1160.
                                    </a>
                                </li>
                                <li id="192">


                                    <a id="bibliography_11" title=" 黄忠,胡敏,刘娟.基于AAM-SIFT特征描述的两级SVM人脸表情识别[J].计算机工程与应用,2016,52(3):178-183." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSGG201603036&amp;v=MDMxNTdCdEdGckNVUkxPZVplVnVGeS9oVUxySkx6N01hYkc0SDlmTXJJOUdZb1FLREg4NHZSNFQ2ajU0TzN6cXE=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[11]</b>
                                         黄忠,胡敏,刘娟.基于AAM-SIFT特征描述的两级SVM人脸表情识别[J].计算机工程与应用,2016,52(3):178-183.
                                    </a>
                                </li>
                                <li id="194">


                                    <a id="bibliography_12" title=" Zhang T,Zheng W,Cui Z,et al.A deep neural network-driven feature learning method for multi-view facial expression recognition[J].IEEE Transactions on Multimedia,2016,18(12):2528-2536." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Deep Neural Network Driven Feature Learning Method for Multi-view Facial Expression Recognition">
                                        <b>[12]</b>
                                         Zhang T,Zheng W,Cui Z,et al.A deep neural network-driven feature learning method for multi-view facial expression recognition[J].IEEE Transactions on Multimedia,2016,18(12):2528-2536.
                                    </a>
                                </li>
                                <li id="196">


                                    <a id="bibliography_13" title=" Ren F,Huang Z.Facial expression recognition based on AAM-SIFT and adaptive regional weighting[J].IEEJ Transactions on Electrical and Electronic Engineering,2015,10(6):713-722." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJWD&amp;filename=SJWD15101200000384&amp;v=MDEzNzBITnJZOUZaT3NQRDNROW9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVWIvSUoxb1hieE09TmlmY2FySzlIOQ==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[13]</b>
                                         Ren F,Huang Z.Facial expression recognition based on AAM-SIFT and adaptive regional weighting[J].IEEJ Transactions on Electrical and Electronic Engineering,2015,10(6):713-722.
                                    </a>
                                </li>
                                <li id="198">


                                    <a id="bibliography_14" title=" 林子澄,黄元亮,刘一民.基于位移特征与随机森林的表情识别方法研究[J].光学技术,2018,44(1):25-29." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=DKFX&amp;filename=GXJS201801005&amp;v=Mjc5MDNCZmJHNEg5bk1ybzlGWVlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ1RnkvaFVMckpJalg=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[14]</b>
                                         林子澄,黄元亮,刘一民.基于位移特征与随机森林的表情识别方法研究[J].光学技术,2018,44(1):25-29.
                                    </a>
                                </li>
                                <li id="200">


                                    <a id="bibliography_15" title=" Li T,Mei T,Kweon I S,et al.Contextual bag-of-words for visual categorization[J].IEEE Transactions on Circuits &amp;amp; Systems for Video Technology,2011,21(4):381-392." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Contextual Bag-of-Words for Visual Categorization">
                                        <b>[15]</b>
                                         Li T,Mei T,Kweon I S,et al.Contextual bag-of-words for visual categorization[J].IEEE Transactions on Circuits &amp;amp; Systems for Video Technology,2011,21(4):381-392.
                                    </a>
                                </li>
                                <li id="202">


                                    <a id="bibliography_16" title=" Krizhevsky A,Sutskever I,Hinton G E.ImageNet classification with deep convolutional neural networks[C]//International Conference on Neural Information Processing Systems.Curran Associates Inc.2012:1097-1105." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=ImageNet Classification with Deep Convolutional Neural Networks">
                                        <b>[16]</b>
                                         Krizhevsky A,Sutskever I,Hinton G E.ImageNet classification with deep convolutional neural networks[C]//International Conference on Neural Information Processing Systems.Curran Associates Inc.2012:1097-1105.
                                    </a>
                                </li>
                                <li id="204">


                                    <a id="bibliography_17" title=" Zhao X,Zhang S,Shi X.Facial expression recognition via deep learning[J].IETE Technical Review,2015,32(5):347-355." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=STJD&amp;filename=STJD15100800002564&amp;v=Mjk2NjZzTkNYbzlvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVViL0lKMW9YYnhNPU5qbkJhcks5SDlITXA0OUZaTw==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[17]</b>
                                         Zhao X,Zhang S,Shi X.Facial expression recognition via deep learning[J].IETE Technical Review,2015,32(5):347-355.
                                    </a>
                                </li>
                                <li id="206">


                                    <a id="bibliography_18" title=" Lv Y,Feng Z,Xu C.Facial expression recognition via deep learning[C]//2014 International Conference on Smart Computing (SMARTCOMP).IEEE Computer Society,2014." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Facial expression recognition via deep learning">
                                        <b>[18]</b>
                                         Lv Y,Feng Z,Xu C.Facial expression recognition via deep learning[C]//2014 International Conference on Smart Computing (SMARTCOMP).IEEE Computer Society,2014.
                                    </a>
                                </li>
                                <li id="208">


                                    <a id="bibliography_19" title=" Li W,Tsangouri C,Abtahi F,et al.A recursive framework for expression recognition:from web images to deep models to game dataset[J].Machine Vision &amp;amp; Applications,2018,29(3):489-502." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A recursive framework for expression recognition:from web images to deep models to game dataset">
                                        <b>[19]</b>
                                         Li W,Tsangouri C,Abtahi F,et al.A recursive framework for expression recognition:from web images to deep models to game dataset[J].Machine Vision &amp;amp; Applications,2018,29(3):489-502.
                                    </a>
                                </li>
                                <li id="210">


                                    <a id="bibliography_20" title=" Sun W,Zhao H,Jin Z.A visual attention based ROI detection method for facial expression recognition[J].Neurocomputing,2018,296:12-22." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES3B5597359A788BA038ABE410D3248BB4&amp;v=MTczOTVLRzlURnFJeEFiWm9JQkhSTHZoWVE0azRQUFh2anJHWTJlN2FjTjhpYkNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkNwYlEzNWRoaHc3eTh4YWs9TmlmT2ZiRA==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[20]</b>
                                         Sun W,Zhao H,Jin Z.A visual attention based ROI detection method for facial expression recognition[J].Neurocomputing,2018,296:12-22.
                                    </a>
                                </li>
                                <li id="212">


                                    <a id="bibliography_21" title=" Lopes A T,Aguiar E D,Souza A F D,et al.Facial expression recognition with convolutional neural networks:coping with few data and the training sample order[J].Pattern Recognition,2016,61:610-628." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Facial Expression Recognition with Convolutional Neural Networks:Coping with Few Data and the Training Sample Order">
                                        <b>[21]</b>
                                         Lopes A T,Aguiar E D,Souza A F D,et al.Facial expression recognition with convolutional neural networks:coping with few data and the training sample order[J].Pattern Recognition,2016,61:610-628.
                                    </a>
                                </li>
                                <li id="214">


                                    <a id="bibliography_22" title=" Rikhtegar A,Pooyan M,Manzuri-Shalmani M T.Genetic algorithm-optimised structure of convolutional neural network for face recognition applications[J].IET Computer Vision,2016,10(6):559-566." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Genetic algorithm-optimised structure of convolutional neural network for face recognition applications">
                                        <b>[22]</b>
                                         Rikhtegar A,Pooyan M,Manzuri-Shalmani M T.Genetic algorithm-optimised structure of convolutional neural network for face recognition applications[J].IET Computer Vision,2016,10(6):559-566.
                                    </a>
                                </li>
                                <li id="216">


                                    <a id="bibliography_23" title=" Zhang C,Wang P,Chen K.Identity-aware convolutional neural networks for facial expression recognition [J].Journal of Systems Engineering and Electronics,2017,28(4):784-792." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XTGJ201704018&amp;v=MDI4MTdMckpQVG5NWkxHNEg5Yk1xNDlFYklRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ1RnkvaFU=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[23]</b>
                                         Zhang C,Wang P,Chen K.Identity-aware convolutional neural networks for facial expression recognition [J].Journal of Systems Engineering and Electronics,2017,28(4):784-792.
                                    </a>
                                </li>
                                <li id="218">


                                    <a id="bibliography_24" title=" Kim D H,Baddar W,Jang J,et al.Multi-objective based spatio-temporal feature representation learning robust to expression intensity variations for facial expression recognition[J].IEEE Transactions on Affective Computing,2019,10(2):223-236." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multi-objective based spatio-temporal feature representation learning robust to expression intensity variations for facial expression recognition">
                                        <b>[24]</b>
                                         Kim D H,Baddar W,Jang J,et al.Multi-objective based spatio-temporal feature representation learning robust to expression intensity variations for facial expression recognition[J].IEEE Transactions on Affective Computing,2019,10(2):223-236.
                                    </a>
                                </li>
                                <li id="220">


                                    <a id="bibliography_25" title=" 于明,安梦涛,刘依.基于多特征与卷积神经网络的人脸表情识别[J].科学技术与工程,2018,18(13):104-110." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=KXJS201813017&amp;v=MDgxMzR6cXFCdEdGckNVUkxPZVplVnVGeS9oVUxySkxqWEJmYkc0SDluTnJJOUVZNFFLREg4NHZSNFQ2ajU0TzM=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[25]</b>
                                         于明,安梦涛,刘依.基于多特征与卷积神经网络的人脸表情识别[J].科学技术与工程,2018,18(13):104-110.
                                    </a>
                                </li>
                                <li id="222">


                                    <a id="bibliography_26" >
                                        <b>[26]</b>
                                     Lecun Y L,Bottou L,Bengio Y,et al.Gradient-based learning applied to document recognition[J].Proceedings of the IEEE,1998,86(11):2278-2324.</a>
                                </li>
                                <li id="224">


                                    <a id="bibliography_27" title=" Lowe D G.Distinctive image features from scale-invariant keypoints[J].International Journal of Computer Vision,2004,60(2):91-110." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002830901&amp;v=MTM0NjZkaDRqOTlTWHFScnhveGNNSDdSN3FlYnVkdEZTamxWTHZMSlY4PU5qN0Jhck80SHRIT3A0eEZiZXNPWTNrNXpC&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[27]</b>
                                         Lowe D G.Distinctive image features from scale-invariant keypoints[J].International Journal of Computer Vision,2004,60(2):91-110.
                                    </a>
                                </li>
                                <li id="226">


                                    <a id="bibliography_28" title=" 侯小红,郭敏.一种基于Harris-SIFT特征点检测的LBP人脸表情识别算法[J].西北大学学报,2017,47(2):209-214." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XBDZ201702010&amp;v=MDI1MzZaSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnVGeS9oVUxySlBTL1BkTEc0SDliTXJZOUU=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[28]</b>
                                         侯小红,郭敏.一种基于Harris-SIFT特征点检测的LBP人脸表情识别算法[J].西北大学学报,2017,47(2):209-214.
                                    </a>
                                </li>
                                <li id="228">


                                    <a id="bibliography_29" title=" Jung H,Lee S,Yim J,et al.Joint fine-tuning in deep neural networks for facial expression recognition[C]//2015 IEEE International Conference on Computer Vision (ICCV).IEEE Computer Society,2015." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Joint fine-tuning in deep neural networks for facial expression recognition">
                                        <b>[29]</b>
                                         Jung H,Lee S,Yim J,et al.Joint fine-tuning in deep neural networks for facial expression recognition[C]//2015 IEEE International Conference on Computer Vision (ICCV).IEEE Computer Society,2015.
                                    </a>
                                </li>
                                <li id="230">


                                    <a id="bibliography_30" title=" Ptucha R W,Tsagkatakis G,Savakis A E.Manifold based Sparse Representation for robust expression recognition without neutral subtraction[C]//IEEE International Conference on Computer Vision Workshops,ICCV 2011 Workshops,Barcelona,Spain,November 6-13,2011.IEEE,2011." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Manifold based Sparse Representation for robust expression recognition without neutral subtraction">
                                        <b>[30]</b>
                                         Ptucha R W,Tsagkatakis G,Savakis A E.Manifold based Sparse Representation for robust expression recognition without neutral subtraction[C]//IEEE International Conference on Computer Vision Workshops,ICCV 2011 Workshops,Barcelona,Spain,November 6-13,2011.IEEE,2011.
                                    </a>
                                </li>
                                <li id="232">


                                    <a id="bibliography_31" title=" Liu M,Li S,Shan S,et al.Deeply learning deformable facial action parts model for dynamic expression analysis[C]//Asian Conference on Computer Vision.Springer International Publishing,2014." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deeply learning deformable facial action parts model for dynamic expression analysis">
                                        <b>[31]</b>
                                         Liu M,Li S,Shan S,et al.Deeply learning deformable facial action parts model for dynamic expression analysis[C]//Asian Conference on Computer Vision.Springer International Publishing,2014.
                                    </a>
                                </li>
                                <li id="234">


                                    <a id="bibliography_32" title=" Yu Z.Image based static facial expression recognition with multiple deep network learning[C]//Acm on International Conference on Multimodal Interaction.ACM,2015." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image based Static Facial Expression Recognition with Multiple Deep Network Learning">
                                        <b>[32]</b>
                                         Yu Z.Image based static facial expression recognition with multiple deep network learning[C]//Acm on International Conference on Multimodal Interaction.ACM,2015.
                                    </a>
                                </li>
                                <li id="236">


                                    <a id="bibliography_33" title=" Mollahosseini A,Chan D,Mahoor M H.Going deeper in facial expression recognition using deep neural networks[C]//2016 IEEE Winter Conference on Applications of Computer Vision (WACV).IEEE Computer Society,2016." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Going deeper in facial expression recognition using deep neural networks">
                                        <b>[33]</b>
                                         Mollahosseini A,Chan D,Mahoor M H.Going deeper in facial expression recognition using deep neural networks[C]//2016 IEEE Winter Conference on Applications of Computer Vision (WACV).IEEE Computer Society,2016.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JYRJ" target="_blank">计算机应用与软件</a>
                2019,36(11),161-167 DOI:10.3969/j.issn.1000-386x.2019.11.027            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于卷积神经网络融合SIFT特征的人脸表情识别</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BC%A0%E4%BF%9E%E6%99%B4&amp;code=40479421&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">张俞晴</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E4%BD%95%E5%AE%81&amp;code=06352223&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">何宁</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%AD%8F%E6%B6%A6%E8%BE%B0&amp;code=43235600&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">魏润辰</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%8C%97%E4%BA%AC%E8%81%94%E5%90%88%E5%A4%A7%E5%AD%A6%E5%8C%97%E4%BA%AC%E5%B8%82%E4%BF%A1%E6%81%AF%E6%9C%8D%E5%8A%A1%E5%B7%A5%E7%A8%8B%E9%87%8D%E7%82%B9%E5%AE%9E%E9%AA%8C%E5%AE%A4&amp;code=0133172&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">北京联合大学北京市信息服务工程重点实验室</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%8C%97%E4%BA%AC%E8%81%94%E5%90%88%E5%A4%A7%E5%AD%A6%E6%99%BA%E6%85%A7%E5%9F%8E%E5%B8%82%E5%AD%A6%E9%99%A2&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">北京联合大学智慧城市学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>表情识别技术是计算机从静态表情图像或动态表情图像中识别出特定的表情,是实现人机交互的基础。提出一种融合卷积神经网络(CNN)与SIFT特征的人脸表情识别方法。通过图像预处理得到规范化的表情图像;采用视觉词袋模型将图像提取的SIFT特征作进一步处理,将得到的图像特征向量作为局部特征,CNN提取的特征作为全局特征,全局特征用以描述表情的整体差异,局部特征用以描述表情的局部差异;将提取出的两组特征融合后采用Softmax分类。与流形稀疏表示(Manifold Sparse Representation,MSR)及3DCNN等方法在CK+及FER2013数据集上的实验表明,该方法是一种有效的表情识别方法。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">卷积神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=SIFT%E7%89%B9%E5%BE%81&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">SIFT特征;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%A7%86%E8%A7%89%E8%AF%8D%E8%A2%8B%E6%A8%A1%E5%9E%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">视觉词袋模型;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%89%B9%E5%BE%81%E8%9E%8D%E5%90%88&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">特征融合;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%A1%A8%E6%83%85%E8%AF%86%E5%88%AB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">表情识别;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    张俞晴，硕士生，主研领域:数字图像处理。;
                                </span>
                                <span>
                                    何宁，教授。;
                                </span>
                                <span>
                                    魏润辰，硕士生。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-02-01</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金项目(61572077,61370138,61872042);</span>
                    </p>
            </div>
                    <h1><b>FACE EXPRESSION RECOGNITION BASED ON CONVOLUTIONAL NEURAL NETWORK FUSING SIFT FEATURES</b></h1>
                    <h2>
                    <span>Zhang Yuqing</span>
                    <span>He Ning</span>
                    <span>Wei Runchen</span>
            </h2>
                    <h2>
                    <span>Beijing Key Laboratory of Information Service Engineering, Beijing Union University</span>
                    <span>Smart City College, Beijing Union University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>The expression recognition technology is that a computer recognizes a specific expression from a static expression image or a dynamic expression image, and is the basis for realizing human-computer interaction. This paper proposed a face expression recognition method that combined convolutional neural network(CNN) and SIFT features. We performed image preprocessing to obtain a normalized expression image. Then the visual bag of words model was used to further process the SIFT features extracted by images, we used image feature vector as a local feature and CNN features as global features. Global features were used to describe the overall difference in expressions. Local features were used to describe local difference in expressions. After the fusion of the two features, Softmax classification was adopted to classify them. Experiments on CK+ dataset and FER2013 dataset with manifold sparse representation(MSR) and 3 D CNN show that this method is an effective expression recognition method.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Convolutional%20neural%20network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Convolutional neural network;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=SIFT%20features&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">SIFT features;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Visual%20bag%20of%20words%20model&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Visual bag of words model;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Feature%20fusion&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Feature fusion;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Expression%20recognition&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Expression recognition;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2019-02-01</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="69" name="69" class="anchor-tag"><b>0 引 言</b></h3>
                <div class="p1">
                    <p id="70">表情中包含着丰富的感情信息,是人类交流的非语言形式。人脸表情识别技术是实现人工智能与人机交互的基础,也是情感计算的关键技术,其涉及机器视觉、图像处理、模式识别等多个领域,在虚拟现实、身份认证等领域有着广泛的应用前景。</p>
                </div>
                <div class="p1">
                    <p id="71">人脸表情识别技术由人脸检测、特征提取及表情分类三个部分组成。其中特征提取是最重要、最困难的部分,特征的有效性将直接影响表情识别的准确率和性能。表情识别的关键问题是如何提取出有利于识别的特征<citation id="245" type="reference"><link href="172" rel="bibliography" /><link href="174" rel="bibliography" /><link href="176" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>,<a class="sup">3</a>]</sup></citation>。表情识别的研究一般分为基于静态的表情识别和基于动态的表情识别两类。基于静态的表情识别方法仅提取输入图像的空间特征,不包含图像的时序特征;而基于动态的表情识别方法则从序列图像中提取表情的时间特征用以表情识别。常用的空间特征提取方法有Gabor小波变换<citation id="246" type="reference"><link href="178" rel="bibliography" /><link href="180" rel="bibliography" /><sup>[<a class="sup">4</a>,<a class="sup">5</a>]</sup></citation>、局部二值模式(local binary patterns,LBP)<citation id="247" type="reference"><link href="182" rel="bibliography" /><link href="184" rel="bibliography" /><link href="186" rel="bibliography" /><sup>[<a class="sup">6</a>,<a class="sup">7</a>,<a class="sup">8</a>]</sup></citation>、尺度不变特征变换(Scale Invariant Feature Transform, SIFT)以及方向梯度直方图(Histograms of Oriented Gradients, HOG)<citation id="238" type="reference"><link href="188" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>。SIFT算法在表情识别上取得了不错的成绩。刘洋等<citation id="239" type="reference"><link href="190" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>采用传统的SIFT特征提取方式,对每幅表情图像的特征进行匹配,文中所采用的阈值为0.3时的匹配效果最佳。SIFT特征有效地提取出面部表情的独特特征,克服人脸间的相似性,该方法在JAFFE数据集上的识别率为95%。文献<citation id="240" type="reference">[<a class="sup">11</a>]</citation>先定位出48个与表情相关的特征点,并提取每个特征点的SIFT特征,每个特征点选用相邻的梯度直方图作为SIFT特征的描述符,最终融合局部和整体类别的两级SVM进行分类,该方法在JAFFE数据集上的识别率达到94.2%。文献<citation id="241" type="reference">[<a class="sup">12</a>]</citation>提出了一种解决非正面人脸表情识别的方法,将人脸图像提取出的SIFT特征送到深度神经网络中学习出一组最优的辨别特征向量,利用这些特征向量对表情进行分类。活动外观模型<citation id="242" type="reference"><link href="196" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>、光流法<citation id="243" type="reference"><link href="198" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>等也常用于提取动态图片的特征。视觉词袋模型对提取的图像特征作相应处理获得视觉词典已广泛地应用于图像分类与图像检索等领域,文献<citation id="244" type="reference">[<a class="sup">15</a>]</citation>将词袋模型结合上下文语义环境进行图像分类。</p>
                </div>
                <div class="p1">
                    <p id="72">基于深度学习的相关方法在近几年已经成功用于人脸相关的领域中。基于深度学习的算法与传统的浅层学习算法的区别在于能够自主地从样本数据中学习特征。卷积神经网络(Convolutional Neural Networks, CNN)、堆叠式自动编码器(Stacked Auto-encoder, SAE)、深度置信网络(Deep Belief Networks, DBNs)等是深度学习模型中比较经典的网络。深度卷积神经网络<citation id="248" type="reference"><link href="202" rel="bibliography" /><link href="204" rel="bibliography" /><link href="206" rel="bibliography" /><sup>[<a class="sup">16</a>,<a class="sup">17</a>,<a class="sup">18</a>]</sup></citation>的端到端学习方法被认为在分类和模式识别中能够自动发现最佳图像特征。CNN的局部权值共享特殊结构降低了网络的复杂性,同时提高了特征提取和分类的性能,使得CNN在语音识别和图像处理方面占据着一定的优势。CNN可以直接输入图像的像素值,自主学习训练样本,同时完成特征提取和分类的任务,并且直接通过网络给出识别结果。</p>
                </div>
                <div class="p1">
                    <p id="73">近几年,CNN已大量用于人脸表情识别,Li等<citation id="249" type="reference"><link href="208" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>提出了一个递归框架来识别真实场景中图像的面部表情。该框架通过微调AlexNet,不改变网络的前四层卷积层,采用CIFE数据集更新第五层卷积层及第一层全连接层,将第二层的全连接数改为2 048,第三层全连接层的数为7。同时采用VGG网络中的第二到四层卷积层代替AlexNet中的每个卷积层,微调后的网络对CIFE及GaMo两个数据集的识别率分别为85%及83%。这个递归网络可以帮助构建更好的面部表情模型来处理真实场景面部表情任务。Sun等<citation id="250" type="reference"><link href="210" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>设计的网络结构在池化层与全连接层加入了嵌入式注意力模型。该网络通过卷积层提取面部的局部卷积特征,嵌入式注意力模型将根据这些局部特征自动确定感兴趣的区域,利用这些区域的特征进行识别,该方法在RaFD-FRONT数据集上的识别率有95%左右。文献<citation id="253" type="reference">[<a class="sup">21</a>,<a class="sup">22</a>]</citation>将卷积神经网络分别与特定的图像预处理步骤和遗传算法(GA)相结合来进行表情识别。Zhang等<citation id="251" type="reference"><link href="216" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>将局部人脸补丁和多尺度全局图像的框架用于表情识别,明显提高了识别的性能。结合表情序列的识别方法能够增大表情的识别率,虽然CNN能够有效地提取表情的空间特征,但提取图像序列的时间特征效果欠佳。为了提高表情的识别率,常将CNN提取的空间特征与其他方法提取的时间特征进行融合。Kim等<citation id="252" type="reference"><link href="218" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>通过CNN学习代表性表达状态帧的空间图像特征,再将空间特征表示的时间特征通过长时间的面部表情记忆来学习,提高识别率,该网络在MMI数据集上的识别率有78.61%。</p>
                </div>
                <div class="p1">
                    <p id="74">本文提出的融合CNN和SIFT特征的表情识别方法,主要用于静态表情图像的识别,采用CNN提取图像的全局特征,图像的局部特征则是采用词袋模型对图像提取的SIFT特征作进一步处理得到。在卷积神经网络中的全连接层之后对两种特征进行融合,从而提高对表情的识别率。</p>
                </div>
                <h3 id="75" name="75" class="anchor-tag"><b>1 SIFT-CNN人脸表情识别算法</b></h3>
                <div class="p1">
                    <p id="76">本文首先对数据集中的图像进行预处理,去除与表情不相关的背景等信息,得到有利于识别的规范表情图像;然后提取CNN全连接层的特征作为全局特征以及采用视觉词袋模型对图像的SIFT特征作进一步处理得到的特征向量作为局部特征;最终将提取出的两组特征进行融合,在Softmax层将融合后的特征进行分类。本文方法的算法流程如图1所示。</p>
                </div>
                <div class="area_img" id="77">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201911028_077.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 SIFT-CNN表情识别算法流程图" src="Detail/GetImg?filename=images/JYRJ201911028_077.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 SIFT-CNN表情识别算法流程图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201911028_077.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="78" name="78"><b>1.1 图像预处理</b></h4>
                <div class="p1">
                    <p id="79">人脸五官附近聚集了大量的表情信息,但一般表情图像还中包括了背景、头发、配饰等与表情无关的信息,这些信息会降低分类的准确率<citation id="254" type="reference"><link href="220" rel="bibliography" /><sup>[<a class="sup">25</a>]</sup></citation>。本文根据人脸的特征对表情图像进行手动裁剪,额头与下巴也包含了大量的表情信息,因此我们将此部分保留,去除与表情无关的信息,再对图像进行尺寸归一化处理,归一化后的图片大小为48×48。图2展示了部分图像预处理前后的图片。</p>
                </div>
                <div class="area_img" id="80">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201911028_080.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 CK+数据集部分预处理图像" src="Detail/GetImg?filename=images/JYRJ201911028_080.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 CK+数据集部分预处理图像  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201911028_080.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="81" name="81"><b>1.2 全局特征提取</b></h4>
                <div class="p1">
                    <p id="82">卷积神经网络(CNN)在1998年由Lecun等<citation id="255" type="reference"><link href="222" rel="bibliography" /><sup>[<a class="sup">26</a>]</sup></citation>提出,其在人脸识别、人脸检测、行人检测、目标分类等应用中非常有效。</p>
                </div>
                <div class="p1">
                    <p id="83">CNN的基本网络结构由输入层以及若干个卷积层、池化层、全连接层组成。在卷积层与池化层中提取图像的特征。浅层卷积层常用于提取图像边缘、线条等低级特征,更深层次的图像特征则在深层卷积层中提取。池化层采用下采样的方法对卷积层提取出的特征进行压缩得到新的、维度较小的特征。CNN为了减少网络训练的参数个数,常采用局部感受野和权值共享这两种方法,卷积层与池化层中的每个神经元的输入与前一层的局部接受域相连,特征提取层中每个神经元只对特征面的局部区域进行感知,提取图像的局部特征。全连接层则把前两层提取出的所有局部特征综合起来得到全局特征,CNN是一种有效提取全局特征的方法。</p>
                </div>
                <div class="p1">
                    <p id="84">本文的CNN结构如图3所示,包含3个卷积层,每个卷基层之后接一个池化层、 1个全连接层和1个Softmax层。网络采用48×48的灰度图像作为输入图像,卷积层采用大小为5×5的卷积核,池化层采用的是2×2最大值采样。第一层卷积层采用10个卷积核与输入图像进行卷积运算,第二层卷积层采用20个卷积核对第一层池化层的输出进行卷积运算,第三层卷积层采用40个卷积核与第二层池化层的输出进行卷积运算,全连接层中神经元的个数为100,表情分类在Softmax层中进行。</p>
                </div>
                <div class="area_img" id="85">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201911028_085.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 CNN结构图" src="Detail/GetImg?filename=images/JYRJ201911028_085.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 CNN结构图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201911028_085.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="86">一般地,卷积层的计算公式为:</p>
                </div>
                <div class="p1">
                    <p id="87"><mathml id="145"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>X</mi><msubsup><mrow></mrow><mi>j</mi><mi>l</mi></msubsup><mo>=</mo><mi>f</mi><mo stretchy="false">(</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>i</mi><mo>∈</mo><mi>Μ</mi><msub><mrow></mrow><mi>j</mi></msub></mrow></munder><mi>X</mi></mstyle><msubsup><mrow></mrow><mi>i</mi><mrow><mi>l</mi><mo>-</mo><mn>1</mn></mrow></msubsup><mo>×</mo><mi>k</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mi>l</mi></msubsup><mo>+</mo><mi>b</mi><msubsup><mrow></mrow><mi>j</mi><mi>l</mi></msubsup><mo stretchy="false">)</mo></mrow></math></mathml>      (1)</p>
                </div>
                <div class="p1">
                    <p id="88">式中:<i>l</i>为当前计算层;<i>l</i>-1为前一层;<i>X</i><mathml id="146"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mi>l</mi></msubsup></mrow></math></mathml>表示当前计算层的第<i>j</i>个特征区域;<i>X</i><mathml id="147"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mrow><mi>l</mi><mo>-</mo><mn>1</mn></mrow></msubsup></mrow></math></mathml>表示前一层第<i>i</i>个特征区域;<i>k</i><mathml id="148"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mi>l</mi></msubsup></mrow></math></mathml>表示前一层第<i>i</i>个特征区域与当前计算层第<i>j</i>个特征区域的卷积核;<i>b</i><mathml id="149"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mi>l</mi></msubsup></mrow></math></mathml>为当前计算层第<i>j</i>个特征区域的偏置;<i>M</i><sub><i>j</i></sub>为当前计算层特征区域的数量;<i>f</i>(·)为激活函数。本文采用ReLU函数作为激活函数,其定义如下:</p>
                </div>
                <div class="p1">
                    <p id="89"><i>f</i>(<i>x</i>)=max(0,<i>x</i>)      (2)</p>
                </div>
                <div class="p1">
                    <p id="90">本文采用的池化方法是2×2最大值下采样(Max-Pooling),池化层通过对上一层特征图的局部区域取最大值来压缩特征图,对特征图中主要特征进行提取的同时降低网络计算复杂度,图4是2×2的Max-Pooling。</p>
                </div>
                <div class="area_img" id="91">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201911028_091.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 2&#215;2的最大值下采样" src="Detail/GetImg?filename=images/JYRJ201911028_091.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 2×2的最大值下采样  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201911028_091.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="92">传统的随机梯度下降算法与本文采用的Adam算法相比,前者保持单一的学习率更新所有的权重,学习率在整个训练过程中始终保持不变;后者则利用梯度的一阶矩估计和二阶矩估计为不同的参数设计相应独立的自适应性学习率。</p>
                </div>
                <h4 class="anchor-tag" id="93" name="93"><b>1.3 局部特征提取</b></h4>
                <div class="p1">
                    <p id="94">SIFT特征是Lowe<citation id="256" type="reference"><link href="224" rel="bibliography" /><sup>[<a class="sup">27</a>]</sup></citation>在2004年提出的,SIFT特征对视角、仿射变化及噪声保持着一定的稳定性,同时在旋转、尺度缩放、亮度变化方面具有良好的不变性,SIFT的这些特性让其成为一种十分稳定的局部特征。本文提取表情图像的SIFT特征作为局部特征,SIFT特征提取的步骤一般包括:1) 尺度空间极值检测;2) 关键点精确定位;3) 关键点方向确定;4) 关键点描述。</p>
                </div>
                <h4 class="anchor-tag" id="95" name="95"><b>1.3.1 尺度空间极值检测</b></h4>
                <div class="p1">
                    <p id="96">在高斯尺度空间中进行极值点的初步检测,高斯尺度空间则通过高斯函数与原图像进行卷积得到:</p>
                </div>
                <div class="p1">
                    <p id="97"><i>L</i>(<i>x</i>,<i>y</i>,<i>σ</i>)=<i>G</i>(<i>x</i>,<i>y</i>,<i>σ</i>)×<i>I</i>(<i>x</i>,<i>y</i>)      (3)</p>
                </div>
                <div class="p1">
                    <p id="98">式中: <i>σ</i>为尺度空间因子;<i>G</i>(<i>x</i>,<i>y</i>,<i>σ</i>)为高斯核函数。<i>σ</i>的值越大图像越模糊,对应的尺度也越大。</p>
                </div>
                <div class="p1">
                    <p id="99">对图像进行不同尺度的高斯模糊后进行隔点采样构建高斯金字塔,金字塔中每组相邻两层图像相减便可以得到高斯差分(Difference of Gaussian, DoG)金字塔。使用DoG金字塔来检测尺度空间中的极值点,DoG定义为:</p>
                </div>
                <div class="p1">
                    <p id="100"><i>D</i>(<i>x</i>,<i>y</i>,<i>σ</i>)=[<i>G</i>(<i>x</i>,<i>y</i>,<i>kσ</i>)-<i>G</i>(<i>x</i>,<i>y</i>,<i>σ</i>)]×<i>I</i>(<i>x</i>,<i>y</i>)=</p>
                </div>
                <div class="p1">
                    <p id="101"><i>L</i>(<i>x</i>,<i>y</i>,<i>kσ</i>)-<i>L</i>(<i>x</i>,<i>y</i>,<i>σ</i>)      (4)</p>
                </div>
                <div class="p1">
                    <p id="102">式中:<i>k</i>为两个相邻尺度空间的比例因子。</p>
                </div>
                <div class="p1">
                    <p id="103">在DoG金字塔同一组内,像素点大于(或者小于)相邻两层图像之间共26个像素点,该点就是初步得到的极值点。</p>
                </div>
                <h4 class="anchor-tag" id="104" name="104"><b>1.3.2 关键点精确定位</b></h4>
                <div class="p1">
                    <p id="105">在尺度空间中初步检测到的极值点是离散的,更精确的关键点的位置和尺度通过拟合三维二次函数得到,在尺度空间中高斯差分函数的泰勒展开式为:</p>
                </div>
                <div class="p1">
                    <p id="106"><mathml id="150"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>D</mi><mo stretchy="false">(</mo><mi>X</mi><mo stretchy="false">)</mo><mo>=</mo><mi>D</mi><mo>+</mo><mfrac><mrow><mo>∂</mo><mi>D</mi><msup><mrow></mrow><mtext>Τ</mtext></msup></mrow><mrow><mo>∂</mo><mi>X</mi></mrow></mfrac><mi>X</mi><mo>+</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mi>X</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mfrac><mrow><mo>∂</mo><msup><mrow></mrow><mn>2</mn></msup><mi>D</mi></mrow><mrow><mo>∂</mo><mi>X</mi><msup><mrow></mrow><mn>2</mn></msup></mrow></mfrac><mi>X</mi></mrow></math></mathml>      (5)</p>
                </div>
                <div class="p1">
                    <p id="107">对式(5)求导确定极值点,舍弃绝对值小于0.03的极值点。为了增强关键点的稳定性及抗噪性能,采用二维Hessian去除对比度较低的关键点以及不稳定的边缘影响点。</p>
                </div>
                <h4 class="anchor-tag" id="108" name="108"><b>1.3.3 关键点方向确定</b></h4>
                <div class="p1">
                    <p id="109">通过计算关键点领域像素的梯度的模值和方向为每个关键点分配一个基准方向,使描述符具有旋转不变性,梯度的模值和方向如下:</p>
                </div>
                <div class="p1">
                    <p id="110" class="code-formula">
                        <mathml id="110"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>m</mi><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>=</mo></mtd></mtr><mtr><mtd><msqrt><mrow><mo stretchy="false">(</mo><mi>L</mi><mo stretchy="false">(</mo><mi>x</mi><mo>+</mo><mn>1</mn><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>-</mo><mi>L</mi><mo stretchy="false">(</mo><mi>x</mi><mo>-</mo><mn>1</mn><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mo stretchy="false">(</mo><mi>L</mi><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo>+</mo><mn>1</mn><mo stretchy="false">)</mo><mo>-</mo><mi>L</mi><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo>-</mo><mn>1</mn><mo stretchy="false">)</mo><mo stretchy="false">)</mo><msup><mrow></mrow><mn>2</mn></msup></mrow></msqrt><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>6</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="111"><mathml id="151"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>θ</mi><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>=</mo><mrow><mi>tan</mi></mrow><msup><mrow></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mrow><mo>(</mo><mrow><mfrac><mrow><mi>L</mi><mo stretchy="false">(</mo><mi>x</mi><mo>+</mo><mn>1</mn><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>-</mo><mi>L</mi><mo stretchy="false">(</mo><mi>x</mi><mo>-</mo><mn>1</mn><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo></mrow><mrow><mi>L</mi><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo>+</mo><mn>1</mn><mo stretchy="false">)</mo><mo>-</mo><mi>L</mi><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo>-</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></mfrac></mrow><mo>)</mo></mrow></mrow></math></mathml>      (7)</p>
                </div>
                <h4 class="anchor-tag" id="112" name="112"><b>1.3.4 关键点描述</b></h4>
                <div class="p1">
                    <p id="113">每个关键点用一组向量来描述,描述符包含位置、尺度和方向三个信息,在关键点尺度空间4×4的区域内,区域内的每个字块中计算8个方向的梯度信息,用最终形成的128维向量来表征描述子,如图5所示。</p>
                </div>
                <div class="area_img" id="114">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201911028_114.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 SIFT 128维特征向量" src="Detail/GetImg?filename=images/JYRJ201911028_114.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 SIFT 128维特征向量  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201911028_114.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="115">不同表情提取出来的SIFT特征的关键点的方向不一样,有助于表情的分类。图6是从人脸图像中提取出的SIFT特征向量,箭头代表SIFT特征点的方向,长度代表SIFT特征点的大小。可以看出,SIFT特征大量集中在眼睛、鼻子及嘴巴附近,这些特征点将有利于表情的识别。</p>
                </div>
                <div class="area_img" id="116">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201911028_116.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 人脸图像的SIFT特征向量" src="Detail/GetImg?filename=images/JYRJ201911028_116.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 人脸图像的SIFT特征向量  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201911028_116.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="117" name="117"><b>1.3.5 视觉词袋模型</b></h4>
                <div class="p1">
                    <p id="118">每幅图像提取出的SIFT特征数不一致,将影响表情的匹配,从而影响表情的识别率。因此本文将采用视觉词袋模型对提取出的SIFT特征作进一步处理。将每一幅图像的SIFT特征统计为视觉单词频数直方图,每幅图像所提取出的特征直方图包含相同的视觉单词,不同的表情的单词词频不同,将有利于表情的识别。</p>
                </div>
                <div class="p1">
                    <p id="119">本文采用视觉词袋模型主要包括以下几个步骤:1) 对提取出的所有图像的SIFT特征通过K-means聚类的方法聚出<i>k</i>类(即<i>k</i>个单词),将这些类作为字典;2) 对每一幅图像统计字典中每个单词在图像中出现的次数;3) 用一个<i>k</i>维数值向量表示相应的图像。本文采用的视觉词袋模型步骤如图7所示。</p>
                </div>
                <div class="area_img" id="120">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201911028_120.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 视觉词袋模型步骤" src="Detail/GetImg?filename=images/JYRJ201911028_120.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 视觉词袋模型步骤  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201911028_120.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="121" name="121"><b>1.4 特征融合</b></h4>
                <div class="p1">
                    <p id="122">图像的SIFT特征表述的是图像的底层特征,底层特征缺乏空间集合信息及对图像高层语义的描述。CNN全连接层提取的特征对图像局部细节的描述欠佳,同时SIFT特征的几何不变性优于CNN提取的特征,SIFT与CNN的特征可以改进相互之间的缺点。本文将CNN全连接层的输出特征<i><b>f</b></i><sub>c</sub>与视觉词袋模型得到的特征向量<i><b>f</b></i><sub>s</sub>进行归一化处理,两组特征归一化之后串联连接在一起得到融合后的特征向量,表示为:</p>
                </div>
                <div class="p1">
                    <p id="123"><i><b>f</b></i><b>=(</b><i><b>f</b></i><sub>c</sub>,<i><b>f</b></i><sub>s</sub>)      (8)</p>
                </div>
                <div class="p1">
                    <p id="124">融合SIFT特征与CNN可以有效提高描述符的几何不变性,增强特征的区分力与抗噪性能,提高表情识别率。</p>
                </div>
                <h3 id="125" name="125" class="anchor-tag"><b>2 实 验</b></h3>
                <h4 class="anchor-tag" id="126" name="126"><b>2.1 数据集简介</b></h4>
                <div class="p1">
                    <p id="127">本文对公开的<i>The Extended Cohn</i>-<i>Kanade</i>(<i>CK</i>+)数据集以及<i>FER</i>2013数据集进行人脸识别验证。</p>
                </div>
                <div class="p1">
                    <p id="128"><i>CK</i>+数据集由123个人拍摄而成,一共包含593个图像序列,其中有327个序列带有表情标签,此数据集是表情识别中常用的一个数据集。本文选用了共计1 356幅表情图像,包含愤怒(179幅)、厌恶(275幅)、恐惧(96幅)、高兴(352幅)、悲伤(106幅)、惊讶(348幅)。本文实验中的训练集选用其中的1 256幅峰值图像,测试集则选用其中的100幅表情图像。</p>
                </div>
                <div class="p1">
                    <p id="129"><i>FER</i>2013数据集包含中性、愤怒、厌恶、恐惧、高兴、委屈、惊讶7种表情共35 000多幅表情图像,本文对此数据集中除去中性剩余的6种表情进行测验,本文选用28 000幅图像作为训练集,3 000幅图像作为测试集。</p>
                </div>
                <h4 class="anchor-tag" id="130" name="130"><b>2.2 训练方法</b></h4>
                <div class="p1">
                    <p id="131">本文采用识别率作为表情识别性能的定量评价指标,定义为:</p>
                </div>
                <div class="area_img" id="171">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JYRJ201911028_17100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="134">训练过程中,输入图像的大小为48×48,采用步长为1、大小为5×5的卷积核进行卷积计算,不仅减小了参数数量,还可以保留图像或特征图的尺寸空间,提升了整个网络的准确率。本文采用dropout策略避免模型出现过拟合,dropout是很简单的正则化技术,以一定的概率将隐层神经元的输入、输出设置为0,被选中的神经元不参加前向传播及误差的反向传播,训练时,设置dropout为0.5。Batch_size是CNN训练时的重要参数,适当范围内的Batch_size较大使得下降方向越准确,震荡越小。Batch_size过大,会出现局部最优的情况;Batch_size过小,产生的随机性更大,导致收敛困难。故将Batch_size设置为30。</p>
                </div>
                <h4 class="anchor-tag" id="135" name="135"><b>2.3 实验结果比较</b></h4>
                <div class="p1">
                    <p id="136">本文方法在<i>CK</i>+和<i>FER</i>2013两个数据集上进行了测验,为了验证本文方法在表情识别上的有效性,在<i>CK</i>+数据集上的实验与<i>SIFT</i>、<i>CNN</i>、<i>MSR</i>等的表情识别算法进行了比较,在<i>FER</i>2013数据集上的实验与<i>CNN</i>和<i>DNN</i>的表情识别算法进行了比较。表1、表2分别列出了在<i>CK</i>+和<i>FER</i>2013两个数据集上不同方法的识别率比较。本文方法在两个数据集上的识别率高于其他几种方法,验证了本文方法在识别表情上的可行性。</p>
                </div>
                <div class="area_img" id="137">
                    <p class="img_tit"><b>表1 不同方法在CK+数据集上识别结果比较</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="137" border="1"><tr><td><br />参考文献</td><td>方法</td><td>识别率/%</td></tr><tr><td><br />文献[28]</td><td>SIFT</td><td>92.7</td></tr><tr><td><br />文献[29]</td><td>CNN</td><td>91.44</td></tr><tr><td><br />文献[30]</td><td>MSR</td><td>91.4</td></tr><tr><td><br />文献[31]</td><td>3DCNN_DAP</td><td>92.4</td></tr><tr><td><br /><b>本文方法</b></td><td><b>SIFT CNN</b></td><td><b>95.4</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="138">
                    <p class="img_tit"><b>表2 不同方法在FER2013数据集上识别结果比较</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="138" border="1"><tr><td><br />参考文献</td><td>方法</td><td>识别率/%</td></tr><tr><td><br />文献[32]</td><td>CNN</td><td>67.1</td></tr><tr><td><br />文献[33]</td><td>DNN</td><td>66.5</td></tr><tr><td><br /><b>本文方法</b></td><td><b>SIFT CNN</b></td><td><b>68.1</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="139">本文方法在两个数据集上的混淆矩阵如表3、表4所示。从表3中可以看出,本文的方法在CK+数据集上对愤怒、开心、惊讶这三种表情的识别结果很好,在厌恶、恐惧、沮丧这三种表情的识别结果比较欠缺,原因是这三种表情具有一定的相似性,对区分不同表情的特征在提取时增加了一定的难度。</p>
                </div>
                <div class="area_img" id="140">
                    <p class="img_tit"><b>表3 CK+数据集的混淆矩阵</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="140" border="1"><tr><td><br /></td><td>An</td><td>Di</td><td>Fe</td><td>Ha</td><td>Sa</td><td>Su</td></tr><tr><td><br />An</td><td>100</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td><br />Di</td><td>0</td><td>84.62</td><td>0</td><td>3.85</td><td>0</td><td>11.53</td></tr><tr><td><br />Fe</td><td>12.5</td><td>0</td><td>87.5</td><td>0</td><td>0</td><td>0</td></tr><tr><td><br />Ha</td><td>0</td><td>0</td><td>0</td><td>100</td><td>0</td><td>0</td></tr><tr><td><br />Sa</td><td>0</td><td>0</td><td>7.7</td><td>0</td><td>92.3</td><td>0</td></tr><tr><td><br />Su</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>100</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="141">
                    <p class="img_tit"><b>表4 FER2013数据集的混淆矩阵</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="141" border="1"><tr><td><br /></td><td>An</td><td>Di</td><td>Fe</td><td>Ha</td><td>Sa</td><td>Su</td></tr><tr><td><br />An</td><td>74.52</td><td>12.85</td><td>11.56</td><td>0</td><td>1.07</td><td>0</td></tr><tr><td><br />Di</td><td>7.14</td><td>53.57</td><td>14.29</td><td>0</td><td>3.57</td><td>21.43</td></tr><tr><td><br />Fe</td><td>6.45</td><td>3.63</td><td>63.31</td><td>12.1</td><td>4.43</td><td>10.08</td></tr><tr><td><br />Ha</td><td>0</td><td>0</td><td>2.79</td><td>89.39</td><td>3.35</td><td>4.47</td></tr><tr><td><br />Sa</td><td>9.65</td><td>6.12</td><td>19.91</td><td>0</td><td>53.6</td><td>10.72</td></tr><tr><td><br />Su</td><td>4.82</td><td>0</td><td>9.64</td><td>15.66</td><td>9.64</td><td>60.24</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="142">从表4中可以看出,本文的方法在FER2013数据集上对愤怒和开心这两种表情的识别结果很好,在厌恶、恐惧、沮丧和惊讶这三种表情的识别结果比较欠缺,原因是FER2013数据集中数据样本质量不如CK+数据集,对区分不同表情的特征在提取时增加了一定的难度。</p>
                </div>
                <h3 id="143" name="143" class="anchor-tag"><b>3 结 语</b></h3>
                <div class="p1">
                    <p id="144">本文针对静态表情图像提出了一种融合SIFT特征与CNN的方法,与其他几种方法相比,本文在特征提取方面具有重要改进。采用CNN全连接层的特征作为全局特征用以描述表情的整体差异,同时采用视觉词袋模型对图像提取出的SIFT特征作进一步处理。将得到的特征向量作为局部特征来描述表情的细节差异,通过融合两组特征来增加表情特征的稳定性,以此提高表情的识别率。实验结果表明,本文方法在FER2013和 CK+两个数据集上的识别率有所提高。未来将结合表情序列图像的时间特征,进一步提高表情的识别率。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="172">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14010600397696&amp;v=MjYzODBieE09TmlmT2ZiSzhIdERNcVk5RlorSUlDblUvb0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VYi9JSjFvWA==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> Zavaschi T H H,Britto A S,Oliveira L E S,et al.Fusion of feature sets and classifiers for facial expression recognition[J].Expert Systems with Applications,2013,40(2):646-655.
                            </a>
                        </p>
                        <p id="174">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=YQXB201304024&amp;v=MTIwNzJxNDlIWUlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ1RnkvaFVMckpQRHpUYkxHNEg5TE0=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> Hu B,Wang J.3D facial expression recognition method based on bimodal and semantic knowledge[J].Chinese Journal of Scientific Instrument,2013,34(4):873-880.
                            </a>
                        </p>
                        <p id="176">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012201348695&amp;v=MDUxNjhOaWZPZmJLN0h0RE9yWTlFWis4SENuVThvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVViL0lKMW9YYnhNPQ==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> Sandbach G,Zafeiriou S,Pantic M,et al.Static and dynamic 3D facial expression recognition:A comprehensive survey [J].Image &amp; Vision Computing,2012,30(10):683-697.
                            </a>
                        </p>
                        <p id="178">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Method of Facial Expression Recognition Based on Gabor and NMF">

                                <b>[4]</b> Zhou J,Zhang S,Mei H,et al.A method of facial expression recognition based on Gabor and NMF[J].Pattern Recognition and Image Analysis,2016,26(1):119-124.
                            </a>
                        </p>
                        <p id="180">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011600738060&amp;v=MjI4MzF1SHlqbVViL0lKMW9YYnhNPU5pZk9mYks3SHRETnFZOUZZK2dIREhvNW9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWg==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b> Gu W,Xiang C,Venkatesh Y V,et al.Facial expression recognition using radial encoding of local Gabor features and classifier synthesis[J].Pattern Recognition,2012,45(1):80-91.
                            </a>
                        </p>
                        <p id="182">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multi-Layer Sparse Representation for Weighted LBP-Patches Based Facial Expression Recognition">

                                <b>[6]</b> Jia Q,Gao X,Guo H,et al.Multi-layer sparse representation for weighted LBP-patches based facial expression recognition[J].Sensors,2015,15(3):6719.
                            </a>
                        </p>
                        <p id="184">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD14101300049680&amp;v=MjY5NzFRVE1ud1plWnVIeWptVWIvSUoxb1hieE09Tmo3QmFySzhIOUhOckk5RlpPOEdDblE1b0JNVDZUNFBRSC9pclJkR2VycQ==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b> Wang L,Li R F,Wang K,et al.Feature representation for facial expression recognition based on FACS and LBP[J].International Journal of Automation and Computing,2014,11(5):459-468.
                            </a>
                        </p>
                        <p id="186">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012201349092&amp;v=MTA1MTFHZXJxUVRNbndaZVp1SHlqbVViL0lKMW9YYnhNPU5pZk9mYks3SHRET3JZOUVaKzhHREhVN29CTVQ2VDRQUUgvaXJSZA==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b> Shan C,Gong S,Mcowan P W.Facial expression recognition based on Local Binary Patterns:A comprehensive study[J].Image and Vision Computing,2009,27(6):803-816.
                            </a>
                        </p>
                        <p id="188">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Feature fusion of HOG and WLD for facial expression recognition">

                                <b>[9]</b> Wang X,Jin C,Liu W,et al.Feature Fusion of HOG and WLD for Facial Expression Recognition[C]//IEEE/SICE International Symposium on System Integration.IEEE,2014.
                            </a>
                        </p>
                        <p id="190">
                            <a id="bibliography_10" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=YJYS201612010&amp;v=MTIzNDVMT2VaZVZ1RnkvaFVMckpQQ2ZTZmJHNEg5Zk5yWTlFWklRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[10]</b> 刘洋,韩广良,史春蕾.基于SIFT算法的多表情人脸识别[J].液晶与显示,2016(12):1156-1160.
                            </a>
                        </p>
                        <p id="192">
                            <a id="bibliography_11" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSGG201603036&amp;v=MTcxODFIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ1RnkvaFVMckpMejdNYWJHNEg5Zk1ySTlHWW9RS0Q=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[11]</b> 黄忠,胡敏,刘娟.基于AAM-SIFT特征描述的两级SVM人脸表情识别[J].计算机工程与应用,2016,52(3):178-183.
                            </a>
                        </p>
                        <p id="194">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Deep Neural Network Driven Feature Learning Method for Multi-view Facial Expression Recognition">

                                <b>[12]</b> Zhang T,Zheng W,Cui Z,et al.A deep neural network-driven feature learning method for multi-view facial expression recognition[J].IEEE Transactions on Multimedia,2016,18(12):2528-2536.
                            </a>
                        </p>
                        <p id="196">
                            <a id="bibliography_13" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJWD&amp;filename=SJWD15101200000384&amp;v=MjQ1MTA0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVViL0lKMW9YYnhNPU5pZmNhcks5SDlITnJZOUZaT3NQRDNROW9CTVQ2VA==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[13]</b> Ren F,Huang Z.Facial expression recognition based on AAM-SIFT and adaptive regional weighting[J].IEEJ Transactions on Electrical and Electronic Engineering,2015,10(6):713-722.
                            </a>
                        </p>
                        <p id="198">
                            <a id="bibliography_14" target="_blank" href="/kcms/detail/detail.aspx?dbcode=DKFX&amp;filename=GXJS201801005&amp;v=MjQyMjdlWmVWdUZ5L2hVTHJKSWpYQmZiRzRIOW5Ncm85RllZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE8=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[14]</b> 林子澄,黄元亮,刘一民.基于位移特征与随机森林的表情识别方法研究[J].光学技术,2018,44(1):25-29.
                            </a>
                        </p>
                        <p id="200">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Contextual Bag-of-Words for Visual Categorization">

                                <b>[15]</b> Li T,Mei T,Kweon I S,et al.Contextual bag-of-words for visual categorization[J].IEEE Transactions on Circuits &amp; Systems for Video Technology,2011,21(4):381-392.
                            </a>
                        </p>
                        <p id="202">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=ImageNet Classification with Deep Convolutional Neural Networks">

                                <b>[16]</b> Krizhevsky A,Sutskever I,Hinton G E.ImageNet classification with deep convolutional neural networks[C]//International Conference on Neural Information Processing Systems.Curran Associates Inc.2012:1097-1105.
                            </a>
                        </p>
                        <p id="204">
                            <a id="bibliography_17" target="_blank" href="/kcms/detail/detail.aspx?dbcode=STJD&amp;filename=STJD15100800002564&amp;v=MDk2MTE5b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VYi9JSjFvWGJ4TT1Oam5CYXJLOUg5SE1wNDlGWk9zTkNYbw==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[17]</b> Zhao X,Zhang S,Shi X.Facial expression recognition via deep learning[J].IETE Technical Review,2015,32(5):347-355.
                            </a>
                        </p>
                        <p id="206">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Facial expression recognition via deep learning">

                                <b>[18]</b> Lv Y,Feng Z,Xu C.Facial expression recognition via deep learning[C]//2014 International Conference on Smart Computing (SMARTCOMP).IEEE Computer Society,2014.
                            </a>
                        </p>
                        <p id="208">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A recursive framework for expression recognition:from web images to deep models to game dataset">

                                <b>[19]</b> Li W,Tsangouri C,Abtahi F,et al.A recursive framework for expression recognition:from web images to deep models to game dataset[J].Machine Vision &amp; Applications,2018,29(3):489-502.
                            </a>
                        </p>
                        <p id="210">
                            <a id="bibliography_20" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES3B5597359A788BA038ABE410D3248BB4&amp;v=MDAwNTdyN0pJRnBtYUJ1SFlmT0dRbGZDcGJRMzVkaGh3N3k4eGFrPU5pZk9mYkRLRzlURnFJeEFiWm9JQkhSTHZoWVE0azRQUFh2anJHWTJlN2FjTjhpYkNPTnZGU2lXVw==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[20]</b> Sun W,Zhao H,Jin Z.A visual attention based ROI detection method for facial expression recognition[J].Neurocomputing,2018,296:12-22.
                            </a>
                        </p>
                        <p id="212">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Facial Expression Recognition with Convolutional Neural Networks:Coping with Few Data and the Training Sample Order">

                                <b>[21]</b> Lopes A T,Aguiar E D,Souza A F D,et al.Facial expression recognition with convolutional neural networks:coping with few data and the training sample order[J].Pattern Recognition,2016,61:610-628.
                            </a>
                        </p>
                        <p id="214">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Genetic algorithm-optimised structure of convolutional neural network for face recognition applications">

                                <b>[22]</b> Rikhtegar A,Pooyan M,Manzuri-Shalmani M T.Genetic algorithm-optimised structure of convolutional neural network for face recognition applications[J].IET Computer Vision,2016,10(6):559-566.
                            </a>
                        </p>
                        <p id="216">
                            <a id="bibliography_23" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XTGJ201704018&amp;v=MTMxNjdUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ1RnkvaFVMckpQVG5NWkxHNEg5Yk1xNDlFYklRS0RIODR2UjQ=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[23]</b> Zhang C,Wang P,Chen K.Identity-aware convolutional neural networks for facial expression recognition [J].Journal of Systems Engineering and Electronics,2017,28(4):784-792.
                            </a>
                        </p>
                        <p id="218">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multi-objective based spatio-temporal feature representation learning robust to expression intensity variations for facial expression recognition">

                                <b>[24]</b> Kim D H,Baddar W,Jang J,et al.Multi-objective based spatio-temporal feature representation learning robust to expression intensity variations for facial expression recognition[J].IEEE Transactions on Affective Computing,2019,10(2):223-236.
                            </a>
                        </p>
                        <p id="220">
                            <a id="bibliography_25" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=KXJS201813017&amp;v=MDcxMDI4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnVGeS9oVUxySkxqWEJmYkc0SDluTnJJOUVZNFFLREg=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[25]</b> 于明,安梦涛,刘依.基于多特征与卷积神经网络的人脸表情识别[J].科学技术与工程,2018,18(13):104-110.
                            </a>
                        </p>
                        <p id="222">
                            <a id="bibliography_26" >
                                    <b>[26]</b>
                                 Lecun Y L,Bottou L,Bengio Y,et al.Gradient-based learning applied to document recognition[J].Proceedings of the IEEE,1998,86(11):2278-2324.
                            </a>
                        </p>
                        <p id="224">
                            <a id="bibliography_27" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002830901&amp;v=MTQ4MjZIN1I3cWVidWR0RlNqbFZMdkxKVjg9Tmo3QmFyTzRIdEhPcDR4RmJlc09ZM2s1ekJkaDRqOTlTWHFScnhveGNN&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[27]</b> Lowe D G.Distinctive image features from scale-invariant keypoints[J].International Journal of Computer Vision,2004,60(2):91-110.
                            </a>
                        </p>
                        <p id="226">
                            <a id="bibliography_28" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XBDZ201702010&amp;v=MDA4NDVMRzRIOWJNclk5RVpJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdUZ5L2hVTHJKUFMvUGQ=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[28]</b> 侯小红,郭敏.一种基于Harris-SIFT特征点检测的LBP人脸表情识别算法[J].西北大学学报,2017,47(2):209-214.
                            </a>
                        </p>
                        <p id="228">
                            <a id="bibliography_29" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Joint fine-tuning in deep neural networks for facial expression recognition">

                                <b>[29]</b> Jung H,Lee S,Yim J,et al.Joint fine-tuning in deep neural networks for facial expression recognition[C]//2015 IEEE International Conference on Computer Vision (ICCV).IEEE Computer Society,2015.
                            </a>
                        </p>
                        <p id="230">
                            <a id="bibliography_30" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Manifold based Sparse Representation for robust expression recognition without neutral subtraction">

                                <b>[30]</b> Ptucha R W,Tsagkatakis G,Savakis A E.Manifold based Sparse Representation for robust expression recognition without neutral subtraction[C]//IEEE International Conference on Computer Vision Workshops,ICCV 2011 Workshops,Barcelona,Spain,November 6-13,2011.IEEE,2011.
                            </a>
                        </p>
                        <p id="232">
                            <a id="bibliography_31" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deeply learning deformable facial action parts model for dynamic expression analysis">

                                <b>[31]</b> Liu M,Li S,Shan S,et al.Deeply learning deformable facial action parts model for dynamic expression analysis[C]//Asian Conference on Computer Vision.Springer International Publishing,2014.
                            </a>
                        </p>
                        <p id="234">
                            <a id="bibliography_32" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image based Static Facial Expression Recognition with Multiple Deep Network Learning">

                                <b>[32]</b> Yu Z.Image based static facial expression recognition with multiple deep network learning[C]//Acm on International Conference on Multimodal Interaction.ACM,2015.
                            </a>
                        </p>
                        <p id="236">
                            <a id="bibliography_33" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Going deeper in facial expression recognition using deep neural networks">

                                <b>[33]</b> Mollahosseini A,Chan D,Mahoor M H.Going deeper in facial expression recognition using deep neural networks[C]//2016 IEEE Winter Conference on Applications of Computer Vision (WACV).IEEE Computer Society,2016.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JYRJ201911028" />
        <input id="dpi" type="hidden" value="96" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201911028&amp;v=MDU4MzZiSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnVGeS9oVUxySkx6VFpaTEc0SDlqTnJvOUg=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="0" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
