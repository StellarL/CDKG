<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637135753824502500%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJYRJ201904028%26RESULT%3d1%26SIGN%3dryfm%252fOvHvycE0DPf%252bcsFtwTtSoY%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JYRJ201904028&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JYRJ201904028&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201904028&amp;v=MTY2NDY0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVadEZ5em1VTHZCTHpUWlpMRzRIOWpNcTQ5SGJJUUtESDg0dlI=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#27" data-title="&lt;b&gt;0 引 言&lt;/b&gt; "><b>0 引 言</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#31" data-title="&lt;b&gt;1 JULE模型简介&lt;/b&gt; "><b>1 JULE模型简介</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#36" data-title="&lt;b&gt;2 算法设计及应用&lt;/b&gt; "><b>2 算法设计及应用</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#39" data-title="&lt;b&gt;2.1 点击数据及点击特征向量&lt;/b&gt;"><b>2.1 点击数据及点击特征向量</b></a></li>
                                                <li><a href="#42" data-title="&lt;b&gt;2.2 基于弱监督深度学习的文本聚类算法&lt;/b&gt;"><b>2.2 基于弱监督深度学习的文本聚类算法</b></a></li>
                                                <li><a href="#96" data-title="&lt;b&gt;2.3 基于点击数据的图像识别&lt;/b&gt;"><b>2.3 基于点击数据的图像识别</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#114" data-title="&lt;b&gt;3 实 验&lt;/b&gt; "><b>3 实 验</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#116" data-title="&lt;b&gt;3.1 实验设置&lt;/b&gt;"><b>3.1 实验设置</b></a></li>
                                                <li><a href="#120" data-title="&lt;b&gt;3.2 点击特征图实验&lt;/b&gt;"><b>3.2 点击特征图实验</b></a></li>
                                                <li><a href="#135" data-title="&lt;b&gt;3.3 弱监督深度聚类模型&lt;/b&gt;"><b>3.3 弱监督深度聚类模型</b></a></li>
                                                <li><a href="#141" data-title="&lt;b&gt;3.4 与相关方法的对比&lt;/b&gt;"><b>3.4 与相关方法的对比</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#156" data-title="&lt;b&gt;4 结 语&lt;/b&gt; "><b>4 结 语</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#38" data-title="图1 基于弱监督深度学习的文本聚类与图像识别框架">图1 基于弱监督深度学习的文本聚类与图像识别框架</a></li>
                                                <li><a href="#47" data-title="图2 点击特征图构建流程">图2 点击特征图构建流程</a></li>
                                                <li><a href="#93" data-title="图3 基于弱监督深度学习的文本聚类框架">图3 基于弱监督深度学习的文本聚类框架</a></li>
                                                <li><a href="#95" data-title="&lt;b&gt;表1 网络结构细节&lt;/b&gt;"><b>表1 网络结构细节</b></a></li>
                                                <li><a href="#119" data-title="&lt;b&gt;表2 数据集详细信息&lt;/b&gt;"><b>表2 数据集详细信息</b></a></li>
                                                <li><a href="#125" data-title="&lt;b&gt;表3 聚类个数对精度的影响&lt;/b&gt;"><b>表3 聚类个数对精度的影响</b></a></li>
                                                <li><a href="#128" data-title="图4 不同参数构建的点击特征图效果对比">图4 不同参数构建的点击特征图效果对比</a></li>
                                                <li><a href="#134" data-title="&lt;b&gt;表4 点击特征图构造过程结果对比&lt;/b&gt;"><b>表4 点击特征图构造过程结果对比</b></a></li>
                                                <li><a href="#137" data-title="&lt;b&gt;表5 卷积核大小对精度影响对比&lt;/b&gt;"><b>表5 卷积核大小对精度影响对比</b></a></li>
                                                <li><a href="#138" data-title="&lt;b&gt;表6 网络层数对精度影响对比&lt;/b&gt;"><b>表6 网络层数对精度影响对比</b></a></li>
                                                <li><a href="#140" data-title="图5 弱监督中参数不同值的效果对比">图5 弱监督中参数不同值的效果对比</a></li>
                                                <li><a href="#146" data-title="&lt;b&gt;表7 在Clickture-Dog上的不同深度模型对比&lt;/b&gt;"><b>表7 在Clickture-Dog上的不同深度模型对比</b></a></li>
                                                <li><a href="#147" data-title="&lt;b&gt;表8 在Clickture-Bird上的不同深度模型对比&lt;/b&gt;"><b>表8 在Clickture-Bird上的不同深度模型对比</b></a></li>
                                                <li><a href="#153" data-title="图6 C-JULE聚类效果">图6 C-JULE聚类效果</a></li>
                                                <li><a href="#154" data-title="图7 C-JWLE聚类效果">图7 C-JWLE聚类效果</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="178">


                                    <a id="bibliography_1" title=" Zheng G, Tan M, Yu J, et al. Fine-grained image recognition via weakly supervised click data guided bilinear CNN model[C]//IEEE International Conference on Multimedia and Expo. IEEE, 2017:661-666." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fine-grained image recognition via weakly supervised click data guided bilinear CNN model">
                                        <b>[1]</b>
                                         Zheng G, Tan M, Yu J, et al. Fine-grained image recognition via weakly supervised click data guided bilinear CNN model[C]//IEEE International Conference on Multimedia and Expo. IEEE, 2017:661-666.
                                    </a>
                                </li>
                                <li id="180">


                                    <a id="bibliography_2" title=" Tan M, Yu J, Zheng G, et al. Deep Neural Network Boosted Large Scale Image Recognition Using User Click Data[C]//International Conference on Internet Multimedia Computing and Service. ACM, 2016:118-121." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep neural network boosted large scale image recognition using user click data">
                                        <b>[2]</b>
                                         Tan M, Yu J, Zheng G, et al. Deep Neural Network Boosted Large Scale Image Recognition Using User Click Data[C]//International Conference on Internet Multimedia Computing and Service. ACM, 2016:118-121.
                                    </a>
                                </li>
                                <li id="182">


                                    <a id="bibliography_3" title=" Wu W, Tan M, Zheng G, et al. Query Modeling for Click Data Based Image Recognition Using Graph Based Propagation and Sparse Coding[C]//International Conference on Internet Multimedia Computing and Service. Springer, Singapore, 2017:191-199." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Query modeling for click data based image recognition using graph based propagation and sparse coding">
                                        <b>[3]</b>
                                         Wu W, Tan M, Zheng G, et al. Query Modeling for Click Data Based Image Recognition Using Graph Based Propagation and Sparse Coding[C]//International Conference on Internet Multimedia Computing and Service. Springer, Singapore, 2017:191-199.
                                    </a>
                                </li>
                                <li id="184">


                                    <a id="bibliography_4" title=" Yu J, Rui Y, Chen B. Exploiting Click Constraints and Multi-view Features for Image Re-ranking[J]. IEEE Transactions on Multimedia, 2013, 16 (1) :159-168." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Exploiting Click Constraints and Multi-view Features for Image Re-ranking">
                                        <b>[4]</b>
                                         Yu J, Rui Y, Chen B. Exploiting Click Constraints and Multi-view Features for Image Re-ranking[J]. IEEE Transactions on Multimedia, 2013, 16 (1) :159-168.
                                    </a>
                                </li>
                                <li id="186">


                                    <a id="bibliography_5" title=" Yu J, Rui Y, Tao D. Click prediction for web image reranking using multimodal sparse coding[J]. Image Processing IEEE Transactions on, 2014, 23 (5) :2019-2032." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Click prediction for web image reranking using multimodal sparse coding">
                                        <b>[5]</b>
                                         Yu J, Rui Y, Tao D. Click prediction for web image reranking using multimodal sparse coding[J]. Image Processing IEEE Transactions on, 2014, 23 (5) :2019-2032.
                                    </a>
                                </li>
                                <li id="188">


                                    <a id="bibliography_6" title=" Yang J, Parikh D, Batra D. Joint Unsupervised Learning of Deep Representations and Image Clusters[C]//2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) .2016:5147-5156." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Joint unsupervised learning of deep repre-sentations and image clusters">
                                        <b>[6]</b>
                                         Yang J, Parikh D, Batra D. Joint Unsupervised Learning of Deep Representations and Image Clusters[C]//2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) .2016:5147-5156.
                                    </a>
                                </li>
                                <li id="190">


                                    <a id="bibliography_7" title=" Dizaji K G, Herandi A, Deng C, et al. Deep Clustering via Joint Convolutional Autoencoder Embedding and Relative Entropy Minimization[EB]. arXiv:1704.06327, 2017." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep Clustering via Joint Convolutional Autoencoder Embedding and Relative Entropy Minimization[EB]">
                                        <b>[7]</b>
                                         Dizaji K G, Herandi A, Deng C, et al. Deep Clustering via Joint Convolutional Autoencoder Embedding and Relative Entropy Minimization[EB]. arXiv:1704.06327, 2017.
                                    </a>
                                </li>
                                <li id="192">


                                    <a id="bibliography_8" title=" Tian F, Gao B, Cui Q, et al. Learning Deep Representations for Graph Clustering[C]//Twenty-eighth Aaai Conference on Artificial Intelligence. AAAI Press, 2014." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning Deep Representations for Graph Clustering">
                                        <b>[8]</b>
                                         Tian F, Gao B, Cui Q, et al. Learning Deep Representations for Graph Clustering[C]//Twenty-eighth Aaai Conference on Artificial Intelligence. AAAI Press, 2014.
                                    </a>
                                </li>
                                <li id="194">


                                    <a id="bibliography_9" title=" Tan M, Yu J, Yu Z, et al. User-Click-Data-Based Fine-Grained Image Recognition via Weakly Supervised Metric Learning[J]. ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM) , 2018, 14 (3) :70." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM2788D3A1CE7F1EB030FD0F1CE3465F3D&amp;v=MTYxODA1dHRod0x1OHhLRT1OaWZJWTdHL0Z0bTRyUDVFRjU0SWVuMU12UllRNmtrSlNBbmozMmMyZmJTUk03bnJDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZCckxVMA==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                         Tan M, Yu J, Yu Z, et al. User-Click-Data-Based Fine-Grained Image Recognition via Weakly Supervised Metric Learning[J]. ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM) , 2018, 14 (3) :70.
                                    </a>
                                </li>
                                <li id="196">


                                    <a id="bibliography_10" title=" Feng W, Liu D. Fine-Grained Image Recognition from Click-Through Logs Using Deep Siamese Network[C]//International Conference on Multimedia Modeling. Springer, Cham, 2017:127-138." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fine-Grained Image Recognition from Click-Through Logs Using Deep Siamese Network">
                                        <b>[10]</b>
                                         Feng W, Liu D. Fine-Grained Image Recognition from Click-Through Logs Using Deep Siamese Network[C]//International Conference on Multimedia Modeling. Springer, Cham, 2017:127-138.
                                    </a>
                                </li>
                                <li id="198">


                                    <a id="bibliography_11" title=" Hua X S, Yang L, Wang J, et al. Clickage: Towards Bridging Semantic and Intent Gaps via Mining Click Logs of Search Engines[C]//Acm International Conference on Multimedia. ACM, 2013:243-252." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Clickage:Towards bridging semantic and intent gaps via mining click logs of search engines">
                                        <b>[11]</b>
                                         Hua X S, Yang L, Wang J, et al. Clickage: Towards Bridging Semantic and Intent Gaps via Mining Click Logs of Search Engines[C]//Acm International Conference on Multimedia. ACM, 2013:243-252.
                                    </a>
                                </li>
                                <li id="200">


                                    <a id="bibliography_12" title=" Huttenlocher D P, Klanderman G, Rucklidge W J. Comparing images using the Hausdorff distance[J]. IEEE Transactions on Pattern Analysis &amp;amp; Machine Intelligence, 1993, 15 (9) :850-863." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Comparing images using the Hausdorff distance">
                                        <b>[12]</b>
                                         Huttenlocher D P, Klanderman G, Rucklidge W J. Comparing images using the Hausdorff distance[J]. IEEE Transactions on Pattern Analysis &amp;amp; Machine Intelligence, 1993, 15 (9) :850-863.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JYRJ" target="_blank">计算机应用与软件</a>
                2019,36(04),171-177 DOI:10.3969/j.issn.1000-386x.2019.04.027            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于弱监督深度学习的文本聚类算法及应用</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%B0%AD%E6%95%8F&amp;code=37641605&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">谭敏</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BC%A0%E5%AE%8F%E6%BA%90&amp;code=38126800&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">张宏源</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BC%A0%E6%B5%B7%E8%B6%85&amp;code=38126801&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">张海超</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%9D%AD%E5%B7%9E%E7%94%B5%E5%AD%90%E7%A7%91%E6%8A%80%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%AD%A6%E9%99%A2&amp;code=0073968&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">杭州电子科技大学计算机学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>围绕基于用户点击数据的文本聚类展开研究。利用点击数据将查询文本表征为图像点击特征图, 并在此上训练深度点击模型。为了应对文本噪声, 引入可刻画文本可靠性的权重, 提出基于弱监督深度学习的文本聚类算法来迭代更新文本权重和深度模型。将该算法应用于基于点击特征的图像识别中, 通过合并相似文本, 为图像构建紧凑的文本集点击特征向量, 实现高效的图像识别。在Clickture-Dog和Clickture-Bird两个公开点击数据集上进行验证, 结果表明:用图像点击特征图来表征查询文本可有效解决原始点击特征向量的稀疏和不连续性, 帮助获得优秀识别率;弱监督深度聚类模型不仅帮助学习强大的文本表征, 还能有效选择高质量文本数据训练模型, 进一步提高性能。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">图像识别;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E8%81%9A%E7%B1%BB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度聚类;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%94%A8%E6%88%B7%E7%82%B9%E5%87%BB%E6%95%B0%E6%8D%AE&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">用户点击数据;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%9F%A5%E8%AF%A2%E5%90%88%E5%B9%B6&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">查询合并;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%BC%B1%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">弱监督学习;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    谭敏, 讲师, 主研领域:深度学习。;
                                </span>
                                <span>
                                    张宏源, 硕士生。;
                                </span>
                                <span>
                                    张海超, 硕士生。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-11-28</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金青年基金项目 (61602136);</span>
                    </p>
            </div>
                    <h1><b>TEXT CLUSTERING ALGORITHM AND ITS APPLICATION BASED ON WEAKLY-SUPERVISED DEEP LEARNING</b></h1>
                    <h2>
                    <span>Tan Min</span>
                    <span>Zhang Hongyuan</span>
                    <span>Zhang Haichao</span>
            </h2>
                    <h2>
                    <span>School of Computer Science and Technology, Hangzhou Dianzi University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>The research is based on the text clustering from user-click data. With click data, a query-text was represented as a smooth image-click-graph, and a deep click model was trained. In order to deal with heavy noise in the clicked query-text set, a weight vector that could measure the reliability of the query-text was introduced, and a text clustering algorithm based on weakly-supervised training method was proposed to iteratively update the weight vector and deep model. The text clustering algorithm was applied to click-feature-based image recognition. After combining similar query-text, a compact click-frequency-vector for images was constructed to achieve accurate image recognition. The proposed method was verified on public Clickture-Dog and Clickture-Bird datasets. The experimental results show that representing each query as an image-click-graph can deal with the non-smoothness and sparseness in the original click vectors, which helps to improve image recognition accuracy. Weakly-supervised deep learning not only helps to learn powerful representations, but also can effectively select queries of high quality, which further improved the recognition performance.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Image%20recognition&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Image recognition;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Deep%20clustering&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Deep clustering;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=User-click%20data&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">User-click data;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Query%20clustering&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Query clustering;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Weakly-supervised%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Weakly-supervised learning;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-11-28</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="27" name="27" class="anchor-tag"><b>0 引 言</b></h3>
                <div class="p1">
                    <p id="28">图像识别一直是计算机视觉领域中最受关注的问题之一。尽管近年来在相关技术方面有了较大的突破和进展, 但是如何克服“语义鸿沟”依然是一个巨大的挑战。为了解决这个问题, 近年来一些学者开始使用用户点击数据来代替视觉特征表示图像<citation id="203" type="reference"><link href="178" rel="bibliography" /><link href="180" rel="bibliography" /><link href="182" rel="bibliography" /><link href="184" rel="bibliography" /><link href="186" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>,<a class="sup">3</a>,<a class="sup">4</a>,<a class="sup">5</a>]</sup></citation>。利用点击数据, 一张图片可以被表示为一个文本点击频率向量, 即文本点击特征<citation id="202" type="reference"><link href="180" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>。由于点击数据是从商业搜索引擎中爬取的用户反馈数据, 与传统的视觉特征相比, 文本点击特征有更丰富的语义信息, 在许多计算机视觉任务上表现更为出色<citation id="204" type="reference"><link href="178" rel="bibliography" /><link href="180" rel="bibliography" /><link href="182" rel="bibliography" /><link href="184" rel="bibliography" /><link href="186" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>,<a class="sup">3</a>,<a class="sup">4</a>,<a class="sup">5</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="29">尽管点击特征有诸多优势, 但直接将这种点击特征用于图像识别仍然面临很多的挑战。由于查询文本集的规模庞大, 噪声较多, 原始的点击特征非常稀疏和冗余。针对此问题, 许多学者提出了利用点击特征进行文本合并的方法<citation id="205" type="reference"><link href="182" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>来应对传统自然语言处理方法中的“语义鸿沟”问题。然而这些工作都是利用图像点击次数向量来表征文本。这种特征尽管简单, 但无法刻画文本的层次化的深度语义特征。为此, 我们提出利用深度网络学习文本的深度点击特征表达, 并基于深度点击特征表达合并相近语义的查询文本。</p>
                </div>
                <div class="p1">
                    <p id="30">随着深度模型在视觉分类领域的广泛应用, 近年来, 学者们也开始研究基于深度学习的图像聚类模型<citation id="207" type="reference"><link href="188" rel="bibliography" /><link href="190" rel="bibliography" /><sup>[<a class="sup">6</a>,<a class="sup">7</a>]</sup></citation>。基于此类模型, 本文提出了面向点击特征的深度文本聚类框架来合并语义相似的查询文本, 其中深度特征和查询类别通过网络自主迭代学习。为了克服点击特征向量的稀疏性, 本文提出构建平滑的结构化的点击特征图来表征查询文本, 并以此作为深度网络的输入来学习查询文本的深度点击特征。本文将杨等提出的无监督深度聚类框架JULE<citation id="206" type="reference"><link href="188" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation> (Joint Unsupervised LEarning of deep representations and image clusters) 扩展到点击数据上, 并融合弱监督学习策略对文本进行加权, 利用迭代优化交替地学习文本权重和深度点击特征, 从而实现在噪声文本数据中的自动样本选择。</p>
                </div>
                <h3 id="31" name="31" class="anchor-tag"><b>1 JULE模型简介</b></h3>
                <div class="p1">
                    <p id="32">JULE是一个端到端的深度图像聚类模型, 它通过迭代更新深度图像特征和类别标号实现无监督的图像聚类。与传统的深度图像识别模型相比, 该模型不需要精确的图像类别信息, 只需要为模型初始化粗糙的类标号。鉴于这些优势, 我们将此模型扩展到基于点击数据的文本聚类上, 以应对原始查询文本缺乏类别标号的特点。该模型的特点是在训练过程中联合更新图像的聚类结果和深度特征实现完全自主学习。</p>
                </div>
                <div class="p1">
                    <p id="33">该模型通过一个三元加权的损失函数组进行训练。实验证明, 该模型在许多图像识别数据集中都具有优秀的特征学习能力和图像聚类效果, 如MNIST、USPS、COIL、UMist、FRGC、CMU-PIE、YTF等。</p>
                </div>
                <div class="p1">
                    <p id="34">除了JULE外, 关于如何将深度学习应用到聚类任务中也有许多其他的研究。如Dizaji等提出了DEPICT模型, 它通过将数据映射到一个具有差异性的子空间来获得更好的聚类效果<citation id="208" type="reference"><link href="190" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>;Tian等提出了一种简单的深度学习方法来进行图片聚类, 该方法首先通过堆叠自动编码器得到图片的视觉特征, 然后用K-means算法对这些特征进行聚类<citation id="209" type="reference"><link href="192" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="35">尽管近些年深度聚类的研究工作越来越多, 但已有模型都是针对图像数据设计的, 而本文研究的查询文本与图像本质上具有较大差距。为此, 本文基于光滑性假设, 为查询文本构建了点击特征图, 从而将JULE扩展到文本聚类任务上。此外, 本文结合弱监督学习策略提出了可对抗文本噪声的深度聚类网络。</p>
                </div>
                <h3 id="36" name="36" class="anchor-tag"><b>2 算法设计及应用</b></h3>
                <div class="p1">
                    <p id="37">本文提出了一种基于弱监督深度学习的文本聚类方法来进行查询文本合并, 并利用合并后的文本集为图像构建紧凑的点击特征, 从而实现高效的图像识别。本文所提出的图像识别算法流程如图1所示。在本节中, 首先将简介点击数据及对应的图像 (文本) 点击特征, 接着详细介绍基于弱监督深度学习的文本聚类框架, 最后介绍算法在图像识别中的具体应用。</p>
                </div>
                <div class="area_img" id="38">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201904028_038.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 基于弱监督深度学习的文本聚类与图像识别框架" src="Detail/GetImg?filename=images/JYRJ201904028_038.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 基于弱监督深度学习的文本聚类与图像识别框架  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201904028_038.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="39" name="39"><b>2.1 点击数据及点击特征向量</b></h4>
                <div class="p1">
                    <p id="40">假设包含<i>n</i>张图片的训练图片集为{<i>x</i><sub><i>i</i></sub>1, 2, …, <i>n</i>}, 图片所对应的类别标签为{<i>y</i><sub><i>i</i></sub><i>i</i>=1, 2, …, <i>n</i>}。该图像集在一个包含<i>m</i>条查询的文本集{<i>q</i><sub><i>j</i></sub><i>j</i>=1, 2, …, <i>m</i>}上有非零的用户点击次数, 且相应点击矩阵为<b><i>C</i></b>∈<b><i>R</i></b><sup><i>n</i>×<i>m</i></sup> (其中<i>c</i><sub><i>i</i>, <i>j</i></sub>表示第<i>i</i>张图片在查询<i>j</i>下的点击次数) , 每张图片可以用查询文本下的用户点击频率向量来表示。</p>
                </div>
                <div class="p1">
                    <p id="41">具体而言, 利用点击数据, 任意图片可表示为<b><i>u</i></b><sub><i>i</i></sub>= (<i>c</i><sub><i>i</i>, 1</sub>, <i>c</i><sub><i>i</i>, 2</sub>, …, <i>c</i><sub><i>i</i>, <i>m</i></sub>) 。类似地, 查询文本可表示为<b><i>v</i></b><sub><i>j</i></sub>= (<i>c</i><sub>1, <i>j</i></sub>, <i>c</i><sub>2, <i>j</i></sub>, …, <i>c</i><sub><i>n</i>, <i>j</i></sub>) 。注意到原始的点击向量<b><i>u</i></b><sub><i>i</i></sub>和<b><i>v</i></b><sub><i>j</i></sub>的特征维度分别由点击数据涉及的图像和查询文本集大小决定, 而高维的点击数据容易导致维度灾难。</p>
                </div>
                <h4 class="anchor-tag" id="42" name="42"><b>2.2 基于弱监督深度学习的文本聚类算法</b></h4>
                <div class="p1">
                    <p id="43">本文将查询文本表征为图像点击特征, 并在此上学习它的深度点击特征。</p>
                </div>
                <h4 class="anchor-tag" id="44" name="44"><b>2.2.1</b> 点击特征图的构建</h4>
                <div class="p1">
                    <p id="45">如前文所述, 本文将利用深度学习网络学习查询文本的深度点击特征。与文献<citation id="210" type="reference">[<a class="sup">1</a>,<a class="sup">3</a>,<a class="sup">9</a>]</citation>中类似, 利用用户点击数据, 输入的查询文本可表示为图像点击向量。然而, 由于互联网图像集庞大, 原始的图像点击特征往往过于稀疏。为了解决该特征的不平滑性和稀疏性, 本文利用原始图像点击向量, 每个查询文本构建了点击特征图<i>G</i>。</p>
                </div>
                <div class="p1">
                    <p id="46">点击特征图的构建流程如图2所示。首先将查询文本的原始点击特征转化为图像类点击特征矩阵, 再利用视觉相似性将此矩阵转化为平滑的点击特征图。如下将展开介绍这两个过程。</p>
                </div>
                <div class="area_img" id="47">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201904028_047.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 点击特征图构建流程" src="Detail/GetImg?filename=images/JYRJ201904028_047.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 点击特征图构建流程  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201904028_047.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="48" name="48">1) 图像类点击特征矩阵。</h4>
                <div class="p1">
                    <p id="49">构建图像类点击特征矩阵要利用到上文所述的点击向量<b><i>v</i></b><sub><i>j</i></sub>及真实标签<i>y</i><sub><i>i</i></sub>。利用类别的真实标签对<b><i>v</i></b>进行重排列得到矩阵 (<b><i>M</i></b><sub><i>j</i></sub>) <sub><i>i</i></sub>, 使得 (<b><i>M</i></b><sub><i>j</i></sub>) <sub><i>i</i></sub>的每一行对应同一类图像下的点击特征向量。由于Clickture-Dog和Clickture-Bird数据集类内不平衡, 有些种类的图片过少。为了平衡数据, 本文首先利用图像扩增算法对图片数量少的类别进行扩充操作。对于每一张图片<i>x</i><sub><i>i</i></sub>, 它的扩充图像<i>L</i><sub><i>i</i></sub>定义如下:</p>
                </div>
                <div class="p1">
                    <p id="50"><i>L</i><sub><i>i</i></sub>={<i>τ</i> (<i>x</i><sub><i>i</i></sub>) <i>τ</i> (·) ∈<i>Γ</i> (·) }      (1) </p>
                </div>
                <div class="p1">
                    <p id="51">式中:<i>τ</i> (·) 是一种图像变换, 包括遮挡、加噪、改变颜色及其混合。<i>L</i>是增强后的数据集, 变换后的图片与原始图片共享点击特征。</p>
                </div>
                <div class="p1">
                    <p id="52">得到增强过的数据后, 本文将每个种类的图片集聚类到<i>N</i><sub><i>I</i></sub>个子类, 这样文本在同一类图像下的点击向量就可以转化为一个维度<i>N</i><sub><i>I</i></sub>的类点击向量。具体来说, 对于第<i>j</i>类图片集, 实现基于深度视觉特征的聚类, 得到对应的子类图像集索引{<i>A</i><sub><i>j</i>, 1</sub>, <i>A</i><sub><i>j</i>, 2</sub>, …, <i>A</i><sub><i>j</i>, <i>N</i><sub><i>I</i></sub></sub>}。</p>
                </div>
                <div class="p1">
                    <p id="53">聚类完成后, 更新后的点击特征矩阵定义如下:</p>
                </div>
                <div class="p1">
                    <p id="54"><mathml id="55"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false"> (</mo><mi>Μ</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mrow><mo> (</mo><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo>, </mo><mi>t</mi><mo stretchy="false">) </mo><mo>∈</mo><mi>Α</mi><msubsup><mrow></mrow><mi>i</mi><mn>1</mn></msubsup></mrow></munder><mi>c</mi></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mi>j</mi></mrow></msub><mo>, </mo><mo>⋯</mo><mo>, </mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mo stretchy="false"> (</mo><mi>i</mi><mo>, </mo><mi>t</mi><mo stretchy="false">) </mo><mo>∈</mo><mi>Α</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mi>Ν</mi><msub><mrow></mrow><mi>Ι</mi></msub></mrow></msubsup></mrow></munder><mi>c</mi></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mi>j</mi></mrow></msub></mrow><mo>) </mo></mrow></mrow></math></mathml>      (2) </p>
                </div>
                <div class="p1">
                    <p id="56">式中: (<i>i</i>, <i>t</i>) 和下文的<i>I</i><mathml id="57"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>t</mi></msubsup></mrow></math></mathml>都表示第<i>i</i>张图片用了第<i>t</i>种图像变换得到的图片。本文将更新后得到的点击矩阵称为图像类点击特征矩阵。</p>
                </div>
                <div class="p1">
                    <p id="58">相比于利用原始点击特征构建的点击特征矩阵, 经过图像扩增后聚类操作后得到的结构化的类点击特征矩阵有效克服了数据集中的类别不平衡。</p>
                </div>
                <h4 class="anchor-tag" id="59" name="59">2) 点击特征图。</h4>
                <div class="p1">
                    <p id="60">为了改善图像类点击特征矩阵稀疏不连续的缺点, 本文利用排序和传播算法将图像类点击特征矩阵转化为平滑的点击特征图。受到文献<citation id="211" type="reference">[<a class="sup">3</a>]</citation>启发, 本文提出了2-D的重排序和2-D点击传播算法。该方法将点击量在各图像类和同类不同图像中传播, 有效改善了点击矩阵不连续性和稀疏性的问题。</p>
                </div>
                <div class="p1">
                    <p id="61">以上两种算法都是基于图像相似度图进行的。相似度图分为类间相似度图和类内相似度图。类间相似度图<i>S</i>用来衡量不同图像类间的距离, 它定义两个图像集的深度视觉特征的Hausdorff距离<citation id="212" type="reference"><link href="200" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>。类内相似度矩阵<mathml id="62"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi mathvariant="bold-italic">S</mi></mstyle><mo>∼</mo></mover><msub><mrow></mrow><mi>i</mi></msub></mrow></math></mathml>用同一类别下不同子类的聚类中心视觉特征来衡量。基于相似度图, 进行如下的排序和传播算法:</p>
                </div>
                <div class="p1">
                    <p id="63"> (1) 点击重排序 重排序算法是为了增加点击特征图的平滑性, 使得相似的大类或者小类在点击矩阵坐标空间中更接近。重排序包括基于<i>S</i>类的类间排序和基于<i>S</i><sub><i>i</i></sub>的类内排序。具体而言, 选择任意图像类/图像子类为参考类, 根据<i>S</i>/<i>S</i><sub><i>i</i></sub>进行降序排列, 从而得到了排列后的图像类点击特征矩阵<mathml id="64"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false"> (</mo><mover accent="true"><mi mathvariant="bold-italic">Μ</mi><mo>¯</mo></mover><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><msub><mrow></mrow><mi>i</mi></msub></mrow></math></mathml>, 该矩阵在维度空间上依照视觉相似性排列, 因而具有类似于图像的局部相似性特性。</p>
                </div>
                <div class="p1">
                    <p id="65"> (2) 点击传播 传播算法主要是为了解决点击特征稀疏的问题。通过在相似样本间分享点击量, 使得点击特征更加平滑均匀。与重排序过程类似, 传播分为类间传播和类内传播两过程。类间传播是指一个图像类的点击量和按照比例分享给其他相似类。</p>
                </div>
                <div class="p1">
                    <p id="66">由上文可知, 查询<i>j</i>的重排后点击特征为<mathml id="67"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi mathvariant="bold-italic">Μ</mi><mo>¯</mo></mover><msub><mrow></mrow><mi>j</mi></msub></mrow></math></mathml>, 将图像类别的点击总量特征记为<i>μ</i>, 则<i>μ</i>定义如下:</p>
                </div>
                <div class="p1">
                    <p id="68"><mathml id="69"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>μ</mi><mo>=</mo><mo stretchy="false"> (</mo><mrow><mo>|</mo><mrow><mo stretchy="false"> (</mo><mover accent="true"><mi mathvariant="bold-italic">Μ</mi><mo>¯</mo></mover><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><msub><mrow></mrow><mn>1</mn></msub></mrow><mo>|</mo></mrow><mo>, </mo><mrow><mo>|</mo><mrow><mo stretchy="false"> (</mo><mover accent="true"><mi mathvariant="bold-italic">Μ</mi><mo>¯</mo></mover><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><msub><mrow></mrow><mn>2</mn></msub></mrow><mo>|</mo></mrow><mo>, </mo><mo>⋯</mo><mo>, </mo><mrow><mo>|</mo><mrow><mo stretchy="false"> (</mo><mover accent="true"><mi mathvariant="bold-italic">Μ</mi><mo>¯</mo></mover><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><msub><mrow></mrow><mi>Ν</mi></msub></mrow><mo>|</mo></mrow><mo stretchy="false">) </mo></mrow></math></mathml>      (3) </p>
                </div>
                <div class="p1">
                    <p id="70">类间传播的公式如下:</p>
                </div>
                <div class="p1">
                    <p id="71" class="code-formula">
                        <mathml id="71"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtable columnalign="left"><mtr><mtd><mover accent="true"><mi>μ</mi><mo>¯</mo></mover><mo>=</mo><mi>μ</mi><mo stretchy="false"> (</mo><mi>ρ</mi><mi>S</mi><mo>+</mo><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>ρ</mi><mo stretchy="false">) </mo><mi mathvariant="bold-italic">E</mi><mo>-</mo><mi>ρ</mi><mi>Λ</mi><mo stretchy="false"> (</mo><mi>S</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi>Λ</mi><mo stretchy="false"> (</mo><mi>S</mi><mo stretchy="false">) </mo><mo>=</mo><mi>d</mi><mi>i</mi><mi>a</mi><mi>g</mi><mo stretchy="false"> (</mo><mi>s</mi><msub><mrow></mrow><mrow><mn>1</mn><mo>, </mo><mn>1</mn></mrow></msub><mo>, </mo><mi>s</mi><msub><mrow></mrow><mrow><mn>2</mn><mo>, </mo><mn>2</mn></mrow></msub><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>s</mi><msub><mrow></mrow><mrow><msup><mi>Ν</mi><mo>′</mo></msup><mo>, </mo><msup><mi>Ν</mi><mo>′</mo></msup></mrow></msub><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mo stretchy="false"> (</mo><mover><mstyle mathsize="140%" displaystyle="true"><mi mathvariant="bold-italic">Μ</mi></mstyle><mo>∼</mo></mover><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mfrac><mrow><mover accent="true"><mi>μ</mi><mo>¯</mo></mover><msub><mrow></mrow><mi>i</mi></msub></mrow><mrow><mi>μ</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></mfrac><mo stretchy="false"> (</mo><mover accent="true"><mi mathvariant="bold-italic">Μ</mi><mo>¯</mo></mover><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo></mtd></mtr></mtable><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="72">式中:<i>ρ</i>为传播率, <b><i>E</i></b>是单位矩阵。</p>
                </div>
                <div class="p1">
                    <p id="73">类内传播与类间传播相似, 是指将同一个类图像里各子类的点击量根据类内相似度相互分享。本文采用K-近邻传播方法分享类内点击量, 此过程基于式 (4) 得到的<mathml id="74"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false"> (</mo><mover><mstyle mathsize="140%" displaystyle="true"><mi mathvariant="bold-italic">Μ</mi></mstyle><mo>∼</mo></mover><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><msub><mrow></mrow><mi>i</mi></msub></mrow></math></mathml>矩阵进行, 其公式如下:</p>
                </div>
                <div class="p1">
                    <p id="75"><mathml id="76"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>G</mi><mo>=</mo><mo stretchy="false"> (</mo><mover accent="true"><mi mathvariant="bold-italic">Μ</mi><mo>^</mo></mover><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mo stretchy="false"> (</mo><mover><mstyle mathsize="140%" displaystyle="true"><mi mathvariant="bold-italic">Μ</mi></mstyle><mo>∼</mo></mover><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false"> (</mo><mi>ρ</mi><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi mathvariant="bold-italic">S</mi></mstyle><mo>∼</mo></mover></mrow><msup><mrow></mrow><mi>i</mi></msup><mo>+</mo><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>ρ</mi><mo stretchy="false">) </mo><mi mathvariant="bold-italic">E</mi><mo>-</mo><mi>ρ</mi><mi mathvariant="bold-italic">Λ</mi><mo stretchy="false"> (</mo><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi mathvariant="bold-italic">S</mi></mstyle><mo>∼</mo></mover></mrow><msup><mrow></mrow><mi>i</mi></msup><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow></mrow></math></mathml>      (5) </p>
                </div>
                <div class="p1">
                    <p id="77">式中:<b><i>E</i></b>和<i>Λ</i> (·) 同式 (4) 一样分别代表单位矩阵和对角化矩阵。</p>
                </div>
                <div class="p1">
                    <p id="78">通过2-D重排和2-D传播算法, 得到最终的点击矩阵<mathml id="79"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false"> (</mo><mover accent="true"><mi mathvariant="bold-italic">Μ</mi><mo>^</mo></mover><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><msub><mrow></mrow><mi>i</mi></msub></mrow></math></mathml>, 本文称其为点击特征图, 因为经过处理后的矩阵有类似图的平滑性, 可以作为理想的深度网络的输入。通过近邻的传播点击量, 最大程度地保留了各子类点击数据的独特性。</p>
                </div>
                <h4 class="anchor-tag" id="80" name="80"><b>2.2.2</b> 弱监督深度文本聚类框架</h4>
                <div class="p1">
                    <p id="81">弱监督深度学习的文本聚类框架旨在学习文本的深度点击特征。受到文献<citation id="213" type="reference">[<a class="sup">6</a>]</citation>中图像深度聚类网络“JULE”的启发, 我们构建了面向点击特征图的深度聚类模型。</p>
                </div>
                <div class="p1">
                    <p id="82">除了构建点击特征图作为输入外, 本文还将弱监督学习引入到训练过程中, 使得深度网络在训练的过程中能自动选择可靠性较高的文本进行训练。具体地, 我们引入了权重向量<i>ω</i>来衡量查询文本的可靠性, 并使用弱监督学习方法使得网络在训练过程中自动更新权重<i>ω</i>。设网络的参数为<i>θ</i>, 则整个模型可形式化为求解如下问题:</p>
                </div>
                <div class="p1">
                    <p id="83"><mathml id="84"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false"> (</mo><mi>θ</mi><msup><mrow></mrow><mo>*</mo></msup><mo>, </mo><mi mathvariant="bold-italic">w</mi><msup><mrow></mrow><mo>*</mo></msup><mo stretchy="false">) </mo><mo>=</mo><mrow><mtext>a</mtext><mtext>r</mtext><mtext>g</mtext><mtext>m</mtext><mtext>i</mtext><mtext>n</mtext></mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mrow><mo>|</mo><mi>θ</mi><mo>|</mo></mrow><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup><mo>+</mo></mrow></math></mathml><mathml id="85"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mi>c</mi><mi>n</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mi>w</mi></mstyle><msub><mrow></mrow><mi>j</mi></msub><mi>l</mi><mo stretchy="false"> (</mo><mi>y</mi><msub><mrow></mrow><mi>j</mi></msub><mo>, </mo><mi>o</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo></mrow></math></mathml>+</p>
                </div>
                <div class="p1">
                    <p id="86"><i>βP</i> (<b><i>w</i></b>) +<i>γS</i> (<i>Z</i>, <b><i>w</i></b>)      (6) </p>
                </div>
                <div class="p1">
                    <p id="87" class="code-formula">
                        <mathml id="87"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtext>s</mtext><mo>.</mo><mtext>t</mtext><mo>.</mo><mspace width="0.25em" /><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mi>w</mi></mstyle><msub><mrow></mrow><mi>j</mi></msub><mo>=</mo><mi>m</mi></mtd></mtr><mtr><mtd><mi>l</mi><mo stretchy="false"> (</mo><mi>y</mi><msub><mrow></mrow><mi>j</mi></msub><mo>, </mo><mi>o</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mo>-</mo><mi>log</mi><mrow><mo> (</mo><mrow><mfrac><mrow><mtext>e</mtext><msup><mrow></mrow><mrow><mi>o</mi><msub><mrow></mrow><mrow><mi>y</mi><msub><mrow></mrow><mi>j</mi></msub></mrow></msub></mrow></msup></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mtext>e</mtext></mstyle><msup><mrow></mrow><mrow><mi>o</mi><msub><mrow></mrow><mi>k</mi></msub></mrow></msup></mrow></mfrac></mrow><mo>) </mo></mrow></mtd></mtr><mtr><mtd><mi>w</mi><msub><mrow></mrow><mi>j</mi></msub><mo>&gt;</mo><mn>0</mn><mtext> </mtext><mo>∀</mo><mi>j</mi></mtd></mtr></mtable></mrow></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="88">式中:<i>y</i><sub><i>j</i></sub>是查询文本的类别, 它被初始为k-means算法得到的类别标号, 并随着网络迭代逐步更新类标号, <i>o</i><sub><i>j</i></sub>为网络输出结果。<i>l</i> (<i>o</i>, <i>y</i>) 是样本分类损失项, <i>P</i> (<b><i>w</i></b>) 是权重先验项, 依据文献<citation id="214" type="reference">[<a class="sup">6</a>]</citation>, 本文用文本被点击的次数总和来估计相应的权重, 即:</p>
                </div>
                <div class="p1">
                    <p id="89"><mathml id="90"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">w</mi><mo stretchy="false">) </mo><mo>=</mo><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">w</mi><mo>-</mo><mi mathvariant="bold-italic">w</mi><msup><mrow></mrow><mi>C</mi></msup></mrow><mo>|</mo></mrow><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></math></mathml>      (7) </p>
                </div>
                <div class="p1">
                    <p id="91">式中:<b><i>w</i></b><sup><i>C</i></sup>是每个查询文本点击次数和构成的向量。式 (6) 中<i>S</i> (<i>Z</i>, <b><i>w</i></b>) 是平滑项, 与文献<citation id="215" type="reference">[<a class="sup">6</a>]</citation>中类似, 它是根据特征一致性假设构建的。由于式 (6) 是个过于复杂的非凸优化问题, 因此本文仿照文献<citation id="216" type="reference">[<a class="sup">6</a>]</citation>, 分两步来训练整个网络。首先固定权重向量<i>ω</i>更新网络参数<i>θ</i>, 之后利用新的网络所提取出的特征和产生的新聚类结果来更新权重<i>ω</i>。</p>
                </div>
                <div class="p1">
                    <p id="92">整个网络的构造如图3所示。</p>
                </div>
                <div class="area_img" id="93">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201904028_093.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 基于弱监督深度学习的文本聚类框架" src="Detail/GetImg?filename=images/JYRJ201904028_093.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 基于弱监督深度学习的文本聚类框架  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201904028_093.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="94">与文献<citation id="217" type="reference">[<a class="sup">6</a>]</citation>中“JULE”网络的结构不同, 本文特别为点击输入构建了文本深度网络结构。由于点击的稀疏性, 该框架采用相对较少的卷积层。表1列出文本深度聚类网络的结构。</p>
                </div>
                <div class="area_img" id="95">
                    <p class="img_tit"><b>表1 网络结构细节</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="95" border="1"><tr><td>模块</td><td>类型</td><td>卷积核, 步长, 填充</td><td>输出</td></tr><tr><td><br />输入</td><td>-</td><td>-</td><td><i>n</i>×<i>m</i></td></tr><tr><td rowspan="2"><br />卷积<br />模块</td><td><br />Conv</td><td><i>p</i>×<i>q</i>, 1, <math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mi>p</mi><mn>2</mn></mfrac></mrow></math>×<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mi>q</mi><mn>2</mn></mfrac></mrow></math></td><td><i>n</i>×<i>m</i>×32</td></tr><tr><td><br />Maxpooling</td><td>2×2, 2, 0</td><td><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mi>n</mi><mn>2</mn></mfrac></mrow></math>×<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mi>m</mi><mn>2</mn></mfrac></mrow></math>×32</td></tr><tr><td rowspan="2"><br />卷积<br />模块</td><td><br />Conv</td><td><i>p</i>×<i>q</i>, 1, <math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mi>p</mi><mn>2</mn></mfrac></mrow></math>×<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mi>q</mi><mn>2</mn></mfrac></mrow></math></td><td><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mi>n</mi><mn>2</mn></mfrac></mrow></math>×<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mi>m</mi><mn>2</mn></mfrac></mrow></math>×64</td></tr><tr><td><br />Maxpooling</td><td>2×2, 2, 0</td><td><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mi>n</mi><mn>4</mn></mfrac></mrow></math>×<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mi>m</mi><mn>4</mn></mfrac></mrow></math>×64</td></tr><tr><td rowspan="2"><br />卷积<br />模块</td><td><br />Conv</td><td><i>p</i>×<i>q</i>, 1, <math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mi>p</mi><mn>2</mn></mfrac></mrow></math>×<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mi>q</mi><mn>2</mn></mfrac></mrow></math></td><td><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mi>n</mi><mn>4</mn></mfrac></mrow></math>×<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mi>m</mi><mn>4</mn></mfrac></mrow></math>×128</td></tr><tr><td><br />Maxpooling</td><td>2×2, 2, 0</td><td><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mi>n</mi><mn>8</mn></mfrac></mrow></math>×<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mi>m</mi><mn>8</mn></mfrac></mrow></math>×128</td></tr><tr><td>全连接<br />模块</td><td>FC</td><td>1×160</td><td>1×160</td></tr><tr><td><br />损失<br />函数</td><td>Softmax</td><td>-</td><td>1×160</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="96" name="96"><b>2.3 基于点击数据的图像识别</b></h4>
                <div class="p1">
                    <p id="97">如上文所述, 本文对原始查询文本进行聚类来合并相似查询, 并用合并后的查询文本来表示每张图片。利用合并后的查询文本, 原始的图片表征<i>u</i><sub><i>i</i></sub>则可更新为<mathml id="98"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>u</mi><mo>^</mo></mover><msub><mrow></mrow><mi>i</mi></msub></mrow></math></mathml>, 其定义如下:</p>
                </div>
                <div class="p1">
                    <p id="99"><mathml id="100"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>u</mi><mo>^</mo></mover><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mrow><mo> (</mo><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>j</mi><mo>∈</mo><mtext>ϑ</mtext><msub><mrow></mrow><mn>1</mn></msub></mrow></munder><mi>c</mi></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mi>j</mi></mrow></msub><mo>, </mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>j</mi><mo>∈</mo><mtext>ϑ</mtext><msub><mrow></mrow><mn>2</mn></msub></mrow></munder><mi>c</mi></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mi>j</mi></mrow></msub><mo>, </mo><mo>⋯</mo><mo>, </mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>j</mi><mo>∈</mo><mtext>ϑ</mtext><msub><mrow></mrow><mi>Κ</mi></msub></mrow></munder><mi>c</mi></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mi>j</mi></mrow></msub></mrow><mo>) </mo></mrow></mrow></math></mathml>      (8) </p>
                </div>
                <div class="p1">
                    <p id="101">式中:<i>K</i>是查询文本的聚类个数, ϑ={{<i>i</i><mathml id="102"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>l</mi><mo>^</mo></mover><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mn>1</mn><mo stretchy="false">}</mo><mo>, </mo><mo stretchy="false">{</mo><mo stretchy="false">{</mo><mi>i</mi></mrow></math></mathml><mathml id="103"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>l</mi><mo>^</mo></mover><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mn>2</mn><mo stretchy="false">}</mo><mo>, </mo><mo stretchy="false">{</mo><mo stretchy="false">{</mo><mi>i</mi></mrow></math></mathml><mathml id="104"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>l</mi><mo>^</mo></mover><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mi>Κ</mi><mo stretchy="false">}</mo><mo stretchy="false">}</mo></mrow></math></mathml>是每个查询聚类的查询索引集。</p>
                </div>
                <div class="p1">
                    <p id="105">当图像均被表征为文本点击特征向量后, 利用如下最近邻算法得到预测测试图像集标签<mathml id="106"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>y</mi><mo>^</mo></mover><msub><mrow></mrow><mi>i</mi></msub></mrow></math></mathml>:</p>
                </div>
                <div class="p1">
                    <p id="107"><mathml id="108"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>y</mi><mo>^</mo></mover><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mi>y</mi><msub><mrow></mrow><mrow><mi>i</mi><msup><mrow></mrow><mo>*</mo></msup></mrow></msub><mtext> </mtext><mtext> </mtext><mi>i</mi><msup><mrow></mrow><mo>*</mo></msup><mo>=</mo><mtext>a</mtext><mtext>r</mtext><mtext>g</mtext><mtext>m</mtext><mtext>i</mtext><mtext>n</mtext></mrow></math></mathml><mathml id="109"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>|</mo><mrow><mover accent="true"><mi>u</mi><mo>^</mo></mover><mo>-</mo><mover accent="true"><mi>u</mi><mo>^</mo></mover><msub><mrow></mrow><mi>i</mi></msub></mrow><mo>|</mo></mrow><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup></mrow></math></mathml>      (9) </p>
                </div>
                <div class="p1">
                    <p id="110">值得注意的是, 训练和测试集中的查询文本往往区别很大, 即在训练图像上点击过的查询有可能在测试集上点击次数为零。为了解决这个问题, 本文通过寻求查询文本在训练-测试集中映射关系, 并利用此关系将测试图像也表征为训练文本集上的点击特征。</p>
                </div>
                <div class="p1">
                    <p id="111">在构建文本映射时, 需要衡量两个查询之间的距离, 本文利用文本点击的图像视觉特征相似度来度量文本间距离。训练集与测试集中的查询文本对 (q<sub>i</sub>, q<sub>j</sub>) 之间的距离公式如下:</p>
                </div>
                <div class="p1">
                    <p id="112" class="code-formula">
                        <mathml id="112"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>d</mi><mo stretchy="false"> (</mo><mi>q</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>q</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mrow><mo>|</mo><mrow><mi>f</mi><mo stretchy="false"> (</mo><mtext>ϕ</mtext><msub><mrow></mrow><mi>t</mi></msub><mo>, </mo><mi mathvariant="bold-italic">v</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>-</mo><mi>f</mi><mo stretchy="false"> (</mo><mtext>ϕ</mtext><msub><mrow></mrow><mi>s</mi></msub><mo>, </mo><mi mathvariant="bold-italic">v</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo></mrow><mo>|</mo></mrow></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mi>f</mi><mo stretchy="false"> (</mo><mtext>ϕ</mtext><mo>, </mo><mi mathvariant="bold-italic">v</mi><mo stretchy="false">) </mo><mo>=</mo><mtext>ϕ</mtext><mo>⋅</mo><mi mathvariant="bold-italic">v</mi><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>0</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="113">式中:<b><i>v</i></b><sub><i>i</i></sub>、<b><i>v</i></b><sub><i>j</i></sub>是<i>q</i><sub><i>i</i></sub>、<i>q</i><sub><i>j</i></sub>的图像类点击特征向量, ϕ<sub><i>t</i></sub>、ϕ<sub><i>s</i></sub>是训练 (测试) 图像集的深度视觉特征矩阵。</p>
                </div>
                <h3 id="114" name="114" class="anchor-tag"><b>3 实 验</b></h3>
                <div class="p1">
                    <p id="115">和文献<citation id="218" type="reference">[<a class="sup">9</a>]</citation>一样, 本文在Clickture-Dog和Clickture-Bird两个公开的点击数据集上进行了实验。Clickture数据集是从商业图像搜索引擎必应的一年点击日志中抽取的, 该数据集包含了一系列 (图像、查询文本、点击次数) 三元组, 是目前最为主流和完善的点击数据集。在本节中, 将首先介绍实验的相关设置;之后通过图像识别精度展现点击特征图及深度聚类网络的优势;最后将本文方法与一些经典算法进行对比验证。本文利用基于文本类点击特征的图像识别精度来度量文本聚类算法的效果, 所列出的实验结果为多次实验后的平均结果。</p>
                </div>
                <h4 class="anchor-tag" id="116" name="116"><b>3.1 实验设置</b></h4>
                <div class="p1">
                    <p id="117">和文献<citation id="219" type="reference">[<a class="sup">10</a>]</citation>一样, 本文首先对Clickture-Dog和Clickture-Bird数据集进行了预处理。并用与文献<citation id="220" type="reference">[<a class="sup">11</a>]</citation>同样的方式划分数据集。</p>
                </div>
                <div class="p1">
                    <p id="118">在表2中, 我们详细列出了实验数据的相关信息, 包括在上文中提到的图像扩增操作。下文中, 如无特别说明, 所列数据是在Clickture-Dog上的结果。</p>
                </div>
                <div class="area_img" id="119">
                    <p class="img_tit"><b>表2 数据集详细信息</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="119" border="1"><tr><td>图像</td><td>类别</td><td colspan="3">图像</td><td colspan="2">文本</td></tr><tr><td>Clickture</td><td>-</td><td>训练集</td><td>扩增后</td><td>测试集</td><td>训练集</td><td>测试集</td></tr><tr><td><br />Dog</td><td>129</td><td>13 812</td><td>27 493</td><td>4 922</td><td>12 728</td><td>56 614</td></tr><tr><td><br />Bird</td><td>75</td><td>7 498</td><td>20 821</td><td>2 461</td><td>16 335</td><td>6 524</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="120" name="120"><b>3.2 点击特征图实验</b></h4>
                <div class="p1">
                    <p id="121">首先实验研究各参数对于点击特征图构建的影响, 然后对比原始点击特征向量和点击特征图的识别率, 以此验证点击特征图的有效性。</p>
                </div>
                <h4 class="anchor-tag" id="122" name="122"><b>3.2.1</b> 参数实验</h4>
                <h4 class="anchor-tag" id="123" name="123">1) 聚类个数</h4>
                <div class="p1">
                    <p id="124">本文对聚类个数做了大量实验, 结果如表3所示。对比不同取值的聚类个数<i>N</i><sub><i>I</i></sub>后, 可发现: (1) 图像识别精度与聚类个数<i>N</i><sub><i>I</i></sub>间呈负相关关系。这种现象表明把某一图像类细分为太多的子类会打破这类样本集间的相关性, 从而消除相邻元素间的本征联系。 (2) 子类个数太少则使点击特征矩阵维度过低, 而子类个数过多又会丧失点击数据的特点。因此, 本文选择了一个适中的聚类个数, 即<i>N</i><sub><i>I</i></sub>为30。</p>
                </div>
                <div class="area_img" id="125">
                    <p class="img_tit"><b>表3 聚类个数对精度的影响</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="125" border="1"><tr><td><br /><i>N</i><sub><i>I</i></sub></td><td>10</td><td>20</td><td>30</td><td>40</td><td>60</td></tr><tr><td><br />精度/%</td><td>73.36<br />±0.54</td><td>71.53<br />±0.32</td><td>69.90<br />±0.14</td><td>67.48<br />±0.66</td><td>63.25<br />±0.80</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="126" name="126">2) 近邻传播参数</h4>
                <div class="p1">
                    <p id="127">本文测试了不同的近邻传播参数为<i>K</i>′与传播率为<i>ρ</i>对构建点击特征图的影响, 如图4所示。</p>
                </div>
                <div class="area_img" id="128">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201904028_128.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 不同参数构建的点击特征图效果对比" src="Detail/GetImg?filename=images/JYRJ201904028_128.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 不同参数构建的点击特征图效果对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201904028_128.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="129">由图4可知: (1) 除传播率<i>ρ</i>=1以外, 识别精度与传播率<i>ρ</i>间呈正相关关系。可能有两方面原因:一是将自身点击量全部传播出去将打破原始点击信息的有效性, 降低图像识别精度;二是适当的传播操作可以改善点击数据的稀疏性, 令点击数据更加平滑、图像识别精度更高。 (2) 当传播率<i>ρ</i>&lt;0.5时, 识别精度随<i>K</i>′的增加而增加;当传播率<i>ρ</i>&gt;0.5时, <i>K</i>′=10或<i>K</i>′=15条件下的识别精度较优。</p>
                </div>
                <div class="p1">
                    <p id="130">经过以上实验, 我们选择<i>N</i><sub><i>I</i></sub>=30、<i>ρ</i>=0.8、<i>K</i>′=15。</p>
                </div>
                <h4 class="anchor-tag" id="131" name="131"><b>3.2.2</b> 点击特征图有效性</h4>
                <div class="p1">
                    <p id="132">本文通过不同点击特征形式的精度验证构建点击特征图的有效性。</p>
                </div>
                <div class="p1">
                    <p id="133">表4中的“V”、“VP”、“M”、“G”分别表示点击特征向量、传播的点击特征向量 (<i>ρ</i>=0.8) 、点击特征矩阵 (传播前) 、点击特征图 (传播后) , 对比结果可知: (1) “VP”远优于“V”的结果, 证实了K近邻传播操作能有效地解决点击数据过于稀疏的问题; (2) “VP”与“M”的识别精度相当, 说明图像聚类操作对文本聚类结果的影响并不明显; (3) M的效果好于“V”也说明了增强图片和聚类表达点击特征具有一定的效果; (4) 综合对比“V”、“VP”、“M”、“G”下的识别精度, 可以发现使用点击特征图“G”的图像识别效果明显优于聚类其他类型的点击特征的识别结果。</p>
                </div>
                <div class="area_img" id="134">
                    <p class="img_tit"><b>表4 点击特征图构造过程结果对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="134" border="1"><tr><td><br />特征类型</td><td>V</td><td>VP</td><td>M</td><td>G</td></tr><tr><td><br />精度/%</td><td>42.80±1.87</td><td>65.49±0.28</td><td>65.61±1.64</td><td>72.95±0.95</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="135" name="135"><b>3.3 弱监督深度聚类模型</b></h4>
                <div class="p1">
                    <p id="136">如上文所述, 本文的输入为点击特征图, 而传统的深度网络的输入为图像。为了寻找最适合于点击数据的深度模型, 本文充分研究了几个主要网络结构参数的影响, 即卷积核大小和网络层数, 结果如表5和表6所示。根据实验数据, 最终确定卷积核大小为7×7, 网络结构为3个卷积层加1个全连接层。</p>
                </div>
                <div class="area_img" id="137">
                    <p class="img_tit"><b>表5 卷积核大小对精度影响对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="137" border="1"><tr><td><br />卷积核大小</td><td>3×3</td><td>3×5</td><td>5×3</td><td>5×5</td><td>7×7</td></tr><tr><td><br />精度/%</td><td>73.06<br />±0.82</td><td>73.11<br />±0.34</td><td>73.15<br />±0.72</td><td>73.80<br />±0.72</td><td>73.87<br />±0.33</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="138">
                    <p class="img_tit"><b>表6 网络层数对精度影响对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="138" border="1"><tr><td><br />网络层数</td><td>2</td><td>3</td><td>4</td><td>5</td><td>6</td></tr><tr><td><br />精度/%</td><td>72.93<br />±0.95</td><td>73.87<br />±0.33</td><td>73.42<br />±0.76</td><td>73.06<br />±1.00</td><td>72.49<br />±0.56</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="139">对于弱监督参数<i>β</i>、<i>γ</i>和权重更新次数<i>T</i>, 本文进行了如图5所示的对比实验。由图可知, <i>T</i>也对结果有很大影响, 权重更新次数越多, 学习到的特征表征能力越强。在最优性能下我们设定<i>β</i>=0.1、<i>γ</i>=0.001。</p>
                </div>
                <div class="area_img" id="140">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201904028_140.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 弱监督中参数不同值的效果对比" src="Detail/GetImg?filename=images/JYRJ201904028_140.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 弱监督中参数不同值的效果对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201904028_140.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="141" name="141"><b>3.4 与相关方法的对比</b></h4>
                <div class="p1">
                    <p id="142">本小节将本文提出的方法和其他常用的深度特征模型进行对比, 利用不同模型获得文本的深度点击特征, 再利用K-means进行文本聚类。本文最终设定查询聚类的聚类个数<i>K</i>=500。</p>
                </div>
                <div class="p1">
                    <p id="143">本文采用VGG、JULE和DEPICT<citation id="221" type="reference"><link href="190" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>作为对比网络。VGG是经典的卷积神经网络, 而JULE和DEPICT是深度聚类网络。由于本文的输入是点击特征图, 因此我们对JULE和DEPICT进行了调整, 将点击特征图作为输入。调整后的模型我们称为C-JULE和C-DEPICT。本文提出的方法使用点击特征图作为输入, 并融合了弱监督的训练方法, 因此将本文的方法称为C-JWLE (Click-data guided Joint Weakly-supervised LEarning of deep representations) 。</p>
                </div>
                <h4 class="anchor-tag" id="144" name="144"><b>3.4.1</b> 识别精度</h4>
                <div class="p1">
                    <p id="145">我们在Clickture-Dog和Clickture-Bird上进行对比实验, 结果如表7和表8所示。</p>
                </div>
                <div class="area_img" id="146">
                    <p class="img_tit"><b>表7 在Clickture-Dog上的不同深度模型对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="146" border="1"><tr><td>方法</td><td>Graph</td><td>VGG</td><td>JULE</td><td>C-DEPICT</td><td>C-JULE</td><td>C-JWLE</td></tr><tr><td><br />精度<br />/%</td><td>72.95<br />±0.95</td><td>71.22<br />±1.39</td><td>72.83<br />±0.90</td><td>73.87<br />±0.33</td><td>72.49<br />±0.56</td><td>74.16<br />±0.42</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="147">
                    <p class="img_tit"><b>表8 在Clickture-Bird上的不同深度模型对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="147" border="1"><tr><td>方法</td><td>Graph</td><td>VGG</td><td>JULE</td><td>C-JULE</td><td>C-JWLE</td></tr><tr><td><br />精度/%</td><td>36.68<br />±0.37</td><td>34.19<br />±1.08</td><td>36.85<br />±0.50</td><td>37.95<br />±0.85</td><td>38.42<br />±0.42</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="148">从上述结果可知:</p>
                </div>
                <div class="p1">
                    <p id="149"> (1) C-DEPICT/C-JULE优于VGG/JULE的性能, 说明传统的图像深度模型 (VGG和JULE) 是依据图像的视觉特点搭建的, 并不适用于点击数据。与之相比, C-DEPICT/C-JULE是专门针对点击数据设计的浅层深度模型。同时, C-JULE明显优于JULE方法的识别精度, 也证明了基于点击数据设计专属模型的必要性。</p>
                </div>
                <div class="p1">
                    <p id="150"> (2) 与C-JULE相比, C-JWLE由于融合了弱监督学习策略, 取得了更好的效果。说明弱监督的学习策略可以更好地消除点击数据中的噪声, 进而提升模型的整体性能。</p>
                </div>
                <h4 class="anchor-tag" id="151" name="151"><b>3.4.2</b> 聚类可视化分析</h4>
                <div class="p1">
                    <p id="152">进一步地, 为了更加直观地分析弱监督算法的效果, 本文对基于C-JULE和C-JWLE的聚类结果进行了可视化对比, 图6和图7分别展示了C-JULE和C-JWLE产生的若干个文本聚类结果, 图中每个查询类cluster中一行表示一条查询文本。</p>
                </div>
                <div class="area_img" id="153">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201904028_153.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 C-JULE聚类效果" src="Detail/GetImg?filename=images/JYRJ201904028_153.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 C-JULE聚类效果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201904028_153.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="154">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201904028_154.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 C-JWLE聚类效果" src="Detail/GetImg?filename=images/JYRJ201904028_154.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 C-JWLE聚类效果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201904028_154.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="155">由图可知, 基于C-JWLE得到的每个聚类中, 更多的查询文本拥有相同的主题词根 (黑色划线) , 而C-JULE更容易将含有不同意义词根 (黑色加粗) 的文本聚成一类。这种现象说明C-JWLE由于能更好地应对文本噪声, 从而产生优于C-JULE的文本聚类效果。</p>
                </div>
                <h3 id="156" name="156" class="anchor-tag"><b>4 结 语</b></h3>
                <div class="p1">
                    <p id="157">本文利用点击数据将图像表征为文本点击特征向量进而实现鲁棒的图像识别。针对查询文本集的规模庞大、冗余的问题, 本文提出面向点击特征的深度文本聚类框架来合并语义相似的查询文本。特别地, 本文提出了一种新颖的2-D重排和2-D点击传播方法来构建一个平滑的结构化的点击特征图来表示查询文本。此外, 本文将深度学框架扩展到点击数据上, 学习查询文本的深度表征。本文还结合弱监督学习策略自动学习查询文本权重, 利用迭代优化的方法交替更新文本权重和深度点击特征。本文在公共数据集Clickture-Dog和Clickture-Bird上进行了实验。结果表明: (1) 点击特征图的构建有效地解决了查询文本的稀疏性和不平滑性问题; (2) 通过引入弱监督学习策略, 有效地克服了查询文本中的噪声问题。今后, 将继续对该算法进行改进, 以获得更好的聚类效果。同时, 也在考虑利用迁移学习的思想, 将点击数据应用到其他公共数据集中, 辅助完成其他计算机视觉任务。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="178">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fine-grained image recognition via weakly supervised click data guided bilinear CNN model">

                                <b>[1]</b> Zheng G, Tan M, Yu J, et al. Fine-grained image recognition via weakly supervised click data guided bilinear CNN model[C]//IEEE International Conference on Multimedia and Expo. IEEE, 2017:661-666.
                            </a>
                        </p>
                        <p id="180">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep neural network boosted large scale image recognition using user click data">

                                <b>[2]</b> Tan M, Yu J, Zheng G, et al. Deep Neural Network Boosted Large Scale Image Recognition Using User Click Data[C]//International Conference on Internet Multimedia Computing and Service. ACM, 2016:118-121.
                            </a>
                        </p>
                        <p id="182">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Query modeling for click data based image recognition using graph based propagation and sparse coding">

                                <b>[3]</b> Wu W, Tan M, Zheng G, et al. Query Modeling for Click Data Based Image Recognition Using Graph Based Propagation and Sparse Coding[C]//International Conference on Internet Multimedia Computing and Service. Springer, Singapore, 2017:191-199.
                            </a>
                        </p>
                        <p id="184">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Exploiting Click Constraints and Multi-view Features for Image Re-ranking">

                                <b>[4]</b> Yu J, Rui Y, Chen B. Exploiting Click Constraints and Multi-view Features for Image Re-ranking[J]. IEEE Transactions on Multimedia, 2013, 16 (1) :159-168.
                            </a>
                        </p>
                        <p id="186">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Click prediction for web image reranking using multimodal sparse coding">

                                <b>[5]</b> Yu J, Rui Y, Tao D. Click prediction for web image reranking using multimodal sparse coding[J]. Image Processing IEEE Transactions on, 2014, 23 (5) :2019-2032.
                            </a>
                        </p>
                        <p id="188">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Joint unsupervised learning of deep repre-sentations and image clusters">

                                <b>[6]</b> Yang J, Parikh D, Batra D. Joint Unsupervised Learning of Deep Representations and Image Clusters[C]//2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) .2016:5147-5156.
                            </a>
                        </p>
                        <p id="190">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep Clustering via Joint Convolutional Autoencoder Embedding and Relative Entropy Minimization[EB]">

                                <b>[7]</b> Dizaji K G, Herandi A, Deng C, et al. Deep Clustering via Joint Convolutional Autoencoder Embedding and Relative Entropy Minimization[EB]. arXiv:1704.06327, 2017.
                            </a>
                        </p>
                        <p id="192">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning Deep Representations for Graph Clustering">

                                <b>[8]</b> Tian F, Gao B, Cui Q, et al. Learning Deep Representations for Graph Clustering[C]//Twenty-eighth Aaai Conference on Artificial Intelligence. AAAI Press, 2014.
                            </a>
                        </p>
                        <p id="194">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM2788D3A1CE7F1EB030FD0F1CE3465F3D&amp;v=MTA1MzM3bnJDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZCckxVMDV0dGh3THU4eEtFPU5pZklZN0cvRnRtNHJQNUVGNTRJZW4xTXZSWVE2a2tKU0FuajMyYzJmYlNSTQ==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b> Tan M, Yu J, Yu Z, et al. User-Click-Data-Based Fine-Grained Image Recognition via Weakly Supervised Metric Learning[J]. ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM) , 2018, 14 (3) :70.
                            </a>
                        </p>
                        <p id="196">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fine-Grained Image Recognition from Click-Through Logs Using Deep Siamese Network">

                                <b>[10]</b> Feng W, Liu D. Fine-Grained Image Recognition from Click-Through Logs Using Deep Siamese Network[C]//International Conference on Multimedia Modeling. Springer, Cham, 2017:127-138.
                            </a>
                        </p>
                        <p id="198">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Clickage:Towards bridging semantic and intent gaps via mining click logs of search engines">

                                <b>[11]</b> Hua X S, Yang L, Wang J, et al. Clickage: Towards Bridging Semantic and Intent Gaps via Mining Click Logs of Search Engines[C]//Acm International Conference on Multimedia. ACM, 2013:243-252.
                            </a>
                        </p>
                        <p id="200">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Comparing images using the Hausdorff distance">

                                <b>[12]</b> Huttenlocher D P, Klanderman G, Rucklidge W J. Comparing images using the Hausdorff distance[J]. IEEE Transactions on Pattern Analysis &amp; Machine Intelligence, 1993, 15 (9) :850-863.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JYRJ201904028" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201904028&amp;v=MTY2NDY0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVadEZ5em1VTHZCTHpUWlpMRzRIOWpNcTQ5SGJJUUtESDg0dlI=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
