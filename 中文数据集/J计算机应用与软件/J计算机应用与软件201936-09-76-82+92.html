<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637135592351408750%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJYRJ201909015%26RESULT%3d1%26SIGN%3dlpbpjblB5jPkj3lVfv63osA6uzA%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JYRJ201909015&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JYRJ201909015&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201909015&amp;v=MjI1NzlLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnRGeW5oVnI3Qkx6VFpaTEc0SDlqTXBvOUVZWVE=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#27" data-title="&lt;b&gt;0 引 言&lt;/b&gt; "><b>0 引 言</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#33" data-title="&lt;b&gt;1 基础理论&lt;/b&gt; "><b>1 基础理论</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#34" data-title="&lt;b&gt;1.1 LSTM和 B-LSTM结构&lt;/b&gt;"><b>1.1 LSTM和 B-LSTM结构</b></a></li>
                                                <li><a href="#64" data-title="&lt;b&gt;1.2 小批量梯度下降算法&lt;/b&gt;"><b>1.2 小批量梯度下降算法</b></a></li>
                                                <li><a href="#71" data-title="&lt;b&gt;1.3 Dropout正则化&lt;/b&gt;"><b>1.3 Dropout正则化</b></a></li>
                                                <li><a href="#74" data-title="&lt;b&gt;1.4 评价指标&lt;/b&gt;"><b>1.4 评价指标</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#83" data-title="&lt;b&gt;2 语义角色标注模型&lt;/b&gt; "><b>2 语义角色标注模型</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#84" data-title="&lt;b&gt;2.1 基于LSTM的词性特性模型&lt;/b&gt;"><b>2.1 基于LSTM的词性特性模型</b></a></li>
                                                <li><a href="#92" data-title="&lt;b&gt;2.2 语义密度聚类&lt;/b&gt;"><b>2.2 语义密度聚类</b></a></li>
                                                <li><a href="#102" data-title="&lt;b&gt;2.3 词向量模糊机制&lt;/b&gt;"><b>2.3 词向量模糊机制</b></a></li>
                                                <li><a href="#111" data-title="&lt;b&gt;2.4 基于模糊机制和语义密度聚类的语义角色标注模型&lt;/b&gt;"><b>2.4 基于模糊机制和语义密度聚类的语义角色标注模型</b></a></li>
                                                <li><a href="#130" data-title="&lt;b&gt;2.5 CRF(条件随机场&lt;/b&gt;)"><b>2.5 CRF(条件随机场</b>)</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#153" data-title="&lt;b&gt;3 实 验&lt;/b&gt; "><b>3 实 验</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#154" data-title="&lt;b&gt;3.1 语料与标注模式&lt;/b&gt;"><b>3.1 语料与标注模式</b></a></li>
                                                <li><a href="#157" data-title="&lt;b&gt;3.2 实验结果与分析&lt;/b&gt;"><b>3.2 实验结果与分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#184" data-title="&lt;b&gt;4 结 语&lt;/b&gt; "><b>4 结 语</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#37" data-title="图1 LSTM单元示意图">图1 LSTM单元示意图</a></li>
                                                <li><a href="#58" data-title="图2 Bi-LSTM网络示意图">图2 Bi-LSTM网络示意图</a></li>
                                                <li><a href="#128" data-title="图3 基于B-LSTM的汉语自动语义角色标注模型">图3 基于B-LSTM的汉语自动语义角色标注模型</a></li>
                                                <li><a href="#162" data-title="&lt;b&gt;表1 Dropout概率参数对比实验&lt;/b&gt;"><b>表1 Dropout概率参数对比实验</b></a></li>
                                                <li><a href="#166" data-title="&lt;b&gt;表2 学习率对比实验&lt;/b&gt;"><b>表2 学习率对比实验</b></a></li>
                                                <li><a href="#170" data-title="&lt;b&gt;表3 词向量维度对比实验&lt;/b&gt;"><b>表3 词向量维度对比实验</b></a></li>
                                                <li><a href="#174" data-title="&lt;b&gt;表4 语义密度聚类与模糊机制对比实验&lt;/b&gt;"><b>表4 语义密度聚类与模糊机制对比实验</b></a></li>
                                                <li><a href="#178" data-title="图4 不同模型的Loss变化曲线图">图4 不同模型的Loss变化曲线图</a></li>
                                                <li><a href="#182" data-title="&lt;b&gt;表5 与已有标注模型的结果对比&lt;/b&gt;"><b>表5 与已有标注模型的结果对比</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" title=" Sun H,Jurafsky D.Shallow Semantc Parsing of Chinese[C]//Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics,HLT-NAACL 2004,Boston,Massachusetts,USA,May 2-7,2004:249-256." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Shallow Semantc Parsing of Chinese">
                                        <b>[1]</b>
                                         Sun H,Jurafsky D.Shallow Semantc Parsing of Chinese[C]//Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics,HLT-NAACL 2004,Boston,Massachusetts,USA,May 2-7,2004:249-256.
                                    </a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" title=" Xue N W,Palmer M.Automatic Semantic Role Labeling for Chinese Verbs[C]//Proceedings of the 19th international joint conference on Artificial intelligence.Morgan Kaufmann Publishers Inc.,2005:1160-1165." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Automatic semanticrole labeling for Chinese verbs">
                                        <b>[2]</b>
                                         Xue N W,Palmer M.Automatic Semantic Role Labeling for Chinese Verbs[C]//Proceedings of the 19th international joint conference on Artificial intelligence.Morgan Kaufmann Publishers Inc.,2005:1160-1165.
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title=" Xue N W,Palmer M.Annotating the propositions in the Penn Chinese Treebank[C]//Proceedings of the second SIGHAN workshop on Chinese language processing-Volume 17.Association for Computational Linguistics,2003:47-54." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Annotating the Propositions in the Penn Chinese Treebank">
                                        <b>[3]</b>
                                         Xue N W,Palmer M.Annotating the propositions in the Penn Chinese Treebank[C]//Proceedings of the second SIGHAN workshop on Chinese language processing-Volume 17.Association for Computational Linguistics,2003:47-54.
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" title=" Wang Z,Jiang T S,Chang B B,et al.Chinese Semantic RoleLabeling with Bidirectional Recurrent Neural Networks[C]//Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,2015:1626-1631." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Chinese Semantic Role Labeling with Bidirectional Recurrent Neural Networks">
                                        <b>[4]</b>
                                         Wang Z,Jiang T S,Chang B B,et al.Chinese Semantic RoleLabeling with Bidirectional Recurrent Neural Networks[C]//Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,2015:1626-1631.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" title=" Roth M,Lapata M.Neural semantic role labeling with dependency path embeddings[C]//Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics,2016." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Neural Semantic Role Labeling with Dependency Path Embeddings">
                                        <b>[5]</b>
                                         Roth M,Lapata M.Neural semantic role labeling with dependency path embeddings[C]//Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics,2016.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" title=" Sha L,Li S,Chang B,et al.Capturing argument relationship for Chinese semantic role labeling[C]//Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing.2016:2011-2016." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Capturing Argument Relationship for Chinese Semantic Role Labeling">
                                        <b>[6]</b>
                                         Sha L,Li S,Chang B,et al.Capturing argument relationship for Chinese semantic role labeling[C]//Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing.2016:2011-2016.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" title=" Guo J,Che W,Wang H,et al.A unified architecture for semantic role labeling and relation classification[C]//Proceedings of COLING 2016,the 26th International Conference on Computational Linguistics:Technical Papers.2016:1264-1274." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Unified Architecture for Semantic Role Labeling and Relation Classification">
                                        <b>[7]</b>
                                         Guo J,Che W,Wang H,et al.A unified architecture for semantic role labeling and relation classification[C]//Proceedings of COLING 2016,the 26th International Conference on Computational Linguistics:Technical Papers.2016:1264-1274.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" title=" Marcheggiani D,Frolov A,Titov I.A simple and accurate syntax-agnostic neural model for dependency-based semantic role labeling[C]//Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017),2017." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A simple and accurate syntax-agnostic neural model for dependency-based semantic role labeling">
                                        <b>[8]</b>
                                         Marcheggiani D,Frolov A,Titov I.A simple and accurate syntax-agnostic neural model for dependency-based semantic role labeling[C]//Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017),2017.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" title=" 王瑞波,李济洪,李国臣,等.基于Dropout正则化的汉语框架语义角色识别[J].中文信息学报,2017,31(1):147-154." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MESS201701019&amp;v=MTk4MjM0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVadEZ5bmhWcjdBS0NqWWZiRzRIOWJNcm85RWJZUUtESDg=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                         王瑞波,李济洪,李国臣,等.基于Dropout正则化的汉语框架语义角色识别[J].中文信息学报,2017,31(1):147-154.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_10" title=" 袁里驰.利用配价信息的语义角色标注[J].电子学报,2017,45(10):2533-2539." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZXU201710031&amp;v=MzIyNzN5bmhWcjdBSVRmVGU3RzRIOWJOcjQ5R1pZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVadEY=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[10]</b>
                                         袁里驰.利用配价信息的语义角色标注[J].电子学报,2017,45(10):2533-2539.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_11" title=" 张苗苗,张玉洁,刘明童,等.基于Gate机制与Bi-LSTM-CRF的汉语语义角色标注[J].计算机与现代化,2018(4):1-6,31." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYXH201804002&amp;v=MTYyODBESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVadEZ5bmhWcjdBTHpUVFpyRzRIOW5NcTQ5RlpvUUs=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[11]</b>
                                         张苗苗,张玉洁,刘明童,等.基于Gate机制与Bi-LSTM-CRF的汉语语义角色标注[J].计算机与现代化,2018(4):1-6,31.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_12" title=" Srivastava N,Hinton G,Krizhevsky A,et al.Dropout:A simple way to prevent neural networks from overfitting[J].The Journal of Machine Learning Research,2014,15(1):1929-1958." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Dropout:A simple way to prevent neural networks from overfitting">
                                        <b>[12]</b>
                                         Srivastava N,Hinton G,Krizhevsky A,et al.Dropout:A simple way to prevent neural networks from overfitting[J].The Journal of Machine Learning Research,2014,15(1):1929-1958.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JYRJ" target="_blank">计算机应用与软件</a>
                2019,36(09),76-82+92 DOI:10.3969/j.issn.1000-386x.2019.09.014            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于模糊机制和语义密度聚类的汉语自动语义角色标注研究</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E6%97%AD%E9%98%B3&amp;code=07919390&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王旭阳</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9C%B1%E9%B9%8F%E9%A3%9E&amp;code=42341959&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">朱鹏飞</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%85%B0%E5%B7%9E%E7%90%86%E5%B7%A5%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%B8%8E%E9%80%9A%E4%BF%A1%E5%AD%A6%E9%99%A2&amp;code=0167938&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">兰州理工大学计算机与通信学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>基于CPB (Chinese Proposition Bank)提出一种基于LSTM-Bi-LSTM的汉语自动语义角色标注方法,并提出语义密度聚类进行数据预处理以及“模糊”机制利用于词向量转换过程。语义密度聚类通过密度的概念对谓词进行全局统一的聚类,将稀疏谓词替换为其所属聚类集合中的常见谓词;利用语义距离概念,将“模糊”机制引入词向量的转换过程,能适当地减少词向量的语义性,并提升与谓词词向量的相关性。利用Bi-LSTM网络自动学习特征表达,然后利用CRF和IOBES标注策略转化为词序列标注问题,引进一种词性学习方法;利用LSTM网络学习生成的词性特征向量与“模糊化”后的词向量融合后一同作为模型的输入向量;训练过程中采用了小批量梯度下降算法和Dropout正则化,这既加快了训练速度,又易于得到全局最优解,还防止了参数过拟合情况的出现。多组对比实验表明,该方法标注结果的F值最高达到了81.24%。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=SRL&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">SRL;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%A8%A1%E7%B3%8A%E6%9C%BA%E5%88%B6&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">模糊机制;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%AF%AD%E4%B9%89%E5%AF%86%E5%BA%A6%E8%81%9A%E7%B1%BB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">语义密度聚类;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%AF%8D%E5%90%91%E9%87%8F&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">词向量;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=CRF&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">CRF;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Dropout&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Dropout;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    王旭阳,教授,主研领域:数据库理论和应用,数据挖掘,知识工程。;
                                </span>
                                <span>
                                    朱鹏飞,硕士生。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-01-08</p>

            </div>
                    <h1><b>CHINESE AUTOMATIC SEMANTIC ROLE LABELING BASED ON FUZZY MECHANISM AND SEMANTIC DENSITY CLUSTERING</b></h1>
                    <h2>
                    <span>Wang Xuyang</span>
                    <span>Zhu Pengfei</span>
            </h2>
                    <h2>
                    <span>School of Computer and Communication, Lanzhou University of Technology</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>On the basis of Chinese Proposition Bank(CPB), this paper proposed a Chinese automatic semantic role labeling method based on LSTM-Bi-LSTM. And the semantic density clustering was proposed for data preprocessing, and the fuzzy mechanism was applied to the word vector transformation process. Semantic density clustering used the concept of density to cluster the predicates globally, and then replaced the sparse predicates with the common predicates in the clustering set to which they belonged. By using the concept of semantic distance, the fuzzy mechanism was introduced into the transformation process of the word vector, which could appropriately reduce the natural semantic of the word vector and improve the correlation with the predicate word vector. Bi-LSTM network was used to automatically learn feature expression, then CRF and IOBES labeling strategies were used to transform into a word sequence annotation problem, and a part of speech learning method was introduced. The part of speech feature vectors generated by LSTM network learning and the fuzzified part of speech vectors were used as input vectors of the model. In the training process, we adopted the low-batch gradient descent algorithm and Dropout regularization. It not only speeded up the training, but also made it easy to get the global optimal solution, and prevented the occurrence of over-fitting of parameters. Multi-group comparison experiments show that the F value of the labeling results of this method reaches 81.24%.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=SRL&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">SRL;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Fuzzy%20mechanism&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Fuzzy mechanism;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Semantic%20density%20clustering&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Semantic density clustering;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Neural%20network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Neural network;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Word%20embedding&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Word embedding;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=CRF&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">CRF;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Dropout&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Dropout;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2019-01-08</p>
                            </div>


        <!--brief start-->
                        <h3 id="27" name="27" class="anchor-tag"><b>0 引 言</b></h3>
                <div class="p1">
                    <p id="28">语义角色标注是浅层语义分析的常见实现方式,已广泛应用于机器翻译等领域。语义角色标注是分析句子中每个词与对应谓词的语义关系,并作相应的语义标记,如施事、受事、工具或附加语等。传统的语义角色标注任务使用基于统计和基于规则的方法,文献<citation id="194" type="reference">[<a class="sup">1</a>,<a class="sup">2</a>,<a class="sup">3</a>]</citation>研究了基于分词、词性和短语结构的自动语义标注;Wang<citation id="186" type="reference"><link href="9" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>提出一种基于Bi-RNN的汉语语义角色标注方法,规避了传统方法的缺点(如工作量大),同时利用序列中长距离的信息在CPB语料上进行了实验,得到77.09%的F值;Roth等<citation id="187" type="reference"><link href="11" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>提出了一种利用神经序列建模技术进行语义角色标注的新模型;Sha<citation id="188" type="reference"><link href="13" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>在论元关系识别过程中引入依存信息,在CPB上取得了77.69%的F值;Guo等<citation id="189" type="reference"><link href="15" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>在充分利用了句法路径信息的基础上使用Bi-LSTM进行建模;Marcheggiani<citation id="190" type="reference"><link href="17" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>等利用双向LSTM在标准的外域测试集上实现了最好的标注结果;王瑞波等<citation id="191" type="reference"><link href="19" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>将汉语词语、词性等特征进行有效融合后利用神经网络构建了语义角色标注模型,并使用了Dropout技术改善模型的训练过程,使得模型的F值有了接近9%的提升;袁里驰等<citation id="192" type="reference"><link href="21" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>利用配价结构体现语义与句法结构的关系,并将谓词的配价信息融入语义角色标注模型,在动词性谓词标注模型和名词性谓词标注模型中分别取得了93.69%和79.23%的F值;张苗苗等<citation id="193" type="reference"><link href="23" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>采用基于句法特征的统计机器学习算法,引入了Gate机制对词向量进行调整,最后F值达到了79.53%。传统的语义角色标注任务使用基于统计和基于规则的方法,具有以下缺点:(1) 过于依赖句法分析的结果;(2) 特征的提取和选择十分困难,并且投入较大;(3) 领域适应性差。</p>
                </div>
                <div class="p1">
                    <p id="29">总结上述内容,本文提出一种基于模糊机制和语义密度聚类的汉语自动语义角色标注系统,主要工作如下:</p>
                </div>
                <div class="p1">
                    <p id="30">(1) 引入语义密度聚类算法,利用密度的概念对语料中的谓词进行全局的聚类,将稀疏谓词替换为其所属聚类集合中的常见谓词。</p>
                </div>
                <div class="p1">
                    <p id="31">(2) 引入模糊机制,利用距离的概念减小原始词向量的语义性并提升与谓词词向量的相关性。</p>
                </div>
                <div class="p1">
                    <p id="32">(3) 利用Bi-LSTM网络实现端到端的SRL模型,对文本的历史信息和未来信息能得到充分的利用。在训练阶段引入了Dropout正则化,避免神经网络过拟合问题的出现。最后使用CRF对标签概率进行全局的归一化处理后,完成最优的序列标注。</p>
                </div>
                <h3 id="33" name="33" class="anchor-tag"><b>1 基础理论</b></h3>
                <h4 class="anchor-tag" id="34" name="34"><b>1.1 LSTM和 B-LSTM结构</b></h4>
                <div class="p1">
                    <p id="35">BRNN解决了传统RNN网络无法利用句子未来信息的缺陷,但是两者都无法对长距离信息进行很好的建模,而且很容易出现梯度消失以及梯度爆炸的问题,随着LSTM单元的引入能很好地解决这些问题。</p>
                </div>
                <div class="p1">
                    <p id="36">所有的RNN网络都有隐藏状态,也就是“记忆模块”,用于结合当前输入与前一个状态,除此以外,LSTM还添加了一个单元状态(cell state),用于记录随时间传递的信息。在传递过程中,通过当前输入、上一时刻隐藏层状态、上一时刻细胞状态以及门结构来增加或删除细胞状态中的信息。门结构用来控制增加或删除信息的程度。LSTM单元的原理图如图1所示。</p>
                </div>
                <div class="area_img" id="37">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201909015_037.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 LSTM单元示意图" src="Detail/GetImg?filename=images/JYRJ201909015_037.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 LSTM单元示意图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201909015_037.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="38">LSTM利用三个门结构来用于信息的更新和利用,即输入门、遗忘门和输出门。设<i>h</i>为LSTM单元输出,<i>c</i>为LSTM记忆单元的值,<i>x</i>为输入数据。所有门的计算除了受当前输入数据<i>x</i><sub><i>t</i></sub>和前一时刻的LSTM单元输出<i>h</i><sub><i>t</i>-1</sub>的影响外,还受前一时刻单元值<i>c</i><sub><i>t</i>-1</sub>的影响。</p>
                </div>
                <div class="p1">
                    <p id="39">(1) 输入门:当前时刻的输入数据<i>x</i><sub><i>t</i></sub>与前一时刻LSTM的输出<i>h</i><sub><i>t</i>-1</sub>构成了当前时刻的候选记忆单元值<mathml id="41"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi>c</mi></mstyle><mo>∼</mo></mover><msub><mrow></mrow><mi>t</mi></msub></mrow></math></mathml>;输入门的<i>i</i><sub><i>t</i></sub>值控制<mathml id="42"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi>c</mi></mstyle><mo>∼</mo></mover><msub><mrow></mrow><mi>t</mi></msub></mrow></math></mathml>中有多少信息量能传递到细胞状态<i>c</i><sub><i>t</i></sub>中。</p>
                </div>
                <div class="p1">
                    <p id="43"><mathml id="44"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi>c</mi></mstyle><mo>∼</mo></mover><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mrow><mi>tanh</mi></mrow><mo stretchy="false">(</mo><mi>w</mi><msub><mrow></mrow><mrow><mi>x</mi><mi>c</mi></mrow></msub><mo>×</mo><mi>x</mi><msub><mrow></mrow><mi>t</mi></msub><mo>+</mo><mi>w</mi><msub><mrow></mrow><mrow><mi>h</mi><mi>c</mi></mrow></msub><mo>×</mo><mi>h</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>+</mo><mi>b</mi><msub><mrow></mrow><mi>c</mi></msub><mo stretchy="false">)</mo></mrow></math></mathml>      (1)</p>
                </div>
                <div class="p1">
                    <p id="45">i<sub>t</sub>=σ(w<sub>xi</sub>×x<sub>t</sub>+w<sub>hi</sub>×h<sub>t-1</sub>+w<sub>ci</sub>×c<sub>t-1</sub>+b<sub>i</sub>)      (2)</p>
                </div>
                <div class="p1">
                    <p id="46">(2) 遗忘门:利用值域为(0,1)的<i>f</i><sub><i>t</i></sub>函数控制前一时刻<i>c</i><sub><i>t</i>-1</sub>的细胞状态传递到当前时刻<i>c</i><sub><i>t</i></sub>中的信息量。</p>
                </div>
                <div class="p1">
                    <p id="48"><i>f</i><sub><i>t</i></sub>=<i>σ</i>(<i>w</i><sub><i>xf</i></sub>×<i>x</i><sub><i>t</i></sub>+<i>w</i><sub><i>hf</i></sub>×<i>h</i><sub><i>t</i>-1</sub>+<i>w</i><sub><i>cf</i></sub>×<i>c</i><sub><i>t</i>-1</sub>+<i>b</i><sub><i>f</i></sub>)      (3)</p>
                </div>
                <div class="p1">
                    <p id="49">式中:<i>w</i>代表权重。</p>
                </div>
                <div class="p1">
                    <p id="50">由上可得当前时刻细胞状态值:</p>
                </div>
                <div class="p1">
                    <p id="51"><mathml id="52"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>c</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mi>f</mi><msub><mrow></mrow><mi>t</mi></msub><mo>⊙</mo><mi>c</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>+</mo><mi>i</mi><msub><mrow></mrow><mi>t</mi></msub><mo>⊙</mo><mover><mstyle mathsize="140%" displaystyle="true"><mi>c</mi></mstyle><mo>∼</mo></mover><msub><mrow></mrow><mi>t</mi></msub></mrow></math></mathml>      (4)</p>
                </div>
                <div class="p1">
                    <p id="53">(3) 输出门:用于控制LSTM记忆单元状态值的输出。</p>
                </div>
                <div class="p1">
                    <p id="54"><i>o</i><sub><i>t</i></sub>=<i>σ</i>(<i>w</i><sub><i>xo</i></sub>×<i>x</i><sub><i>t</i></sub>+<i>w</i><sub><i>ho</i></sub>×<i>h</i><sub><i>t</i>-1</sub>+<i>w</i><sub><i>co</i></sub>×<i>c</i><sub><i>t</i>-1</sub>+<i>b</i><sub><i>o</i></sub>)      (5)</p>
                </div>
                <div class="p1">
                    <p id="55">最后可以得出LSTM单元的输出:</p>
                </div>
                <div class="p1">
                    <p id="56"><i>h</i><sub><i>t</i></sub>=<i>o</i><sub><i>t</i></sub>⊙tanh(<i>c</i><sub><i>t</i></sub>)      (6)</p>
                </div>
                <div class="p1">
                    <p id="57">RNN-BLSTM使用LSTM单元在Bi-RNN的基础上进行了改进,在同一层使用两套连接权重矩阵分别对正向和反向的信息进行建模。Bi-LSTM结构如图2所示。</p>
                </div>
                <div class="area_img" id="58">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201909015_058.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 Bi-LSTM网络示意图" src="Detail/GetImg?filename=images/JYRJ201909015_058.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 Bi-LSTM网络示意图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201909015_058.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="59">图中的<i>x</i><sub><i>t</i></sub>表示网络在<i>t</i>时刻的输入;方框中的LSTM为标准LSTM模型;<mathml id="60"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mrow><mi>y</mi><msub><mrow></mrow><mi>t</mi></msub></mrow><mo stretchy="true">→</mo></mover></mrow></math></mathml>为前向LSTM在<i>t</i>时刻的输出,<mathml id="61"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mrow><mi>y</mi><msub><mrow></mrow><mi>t</mi></msub></mrow><mo stretchy="true">←</mo></mover></mrow></math></mathml>为反向LSTM在<i>t</i>时刻的输出,也就是说Bi-LSTM在<i>t</i>时刻的输出表示定义为<mathml id="62"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>y</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mo stretchy="false">[</mo><mover accent="true"><mrow><mi>y</mi><msub><mrow></mrow><mi>t</mi></msub></mrow><mo stretchy="true">→</mo></mover></mrow></math></mathml>:<mathml id="63"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mrow><mi>y</mi><msub><mrow></mrow><mi>t</mi></msub></mrow><mo stretchy="true">←</mo></mover><mo stretchy="false">]</mo></mrow></math></mathml>,即<i>t</i>时刻的输出由前向输出与反向输出直接拼接而成。</p>
                </div>
                <h4 class="anchor-tag" id="64" name="64"><b>1.2 小批量梯度下降算法</b></h4>
                <div class="p1">
                    <p id="65">梯度下降算法是最常用的神经网络模型训练优化算法。梯度下降算法的原理:目标函数<i>J</i>(<i>θ</i>)关于参数<i>θ</i>的梯度就是目标函数上升最快的方向。对于最小优化问题,只需在梯度相反的方向上将参数前进一个步长,就能够实现目标函数的下降,其中步长又称为学习率。参数<i>θ</i>的更新公式如下:</p>
                </div>
                <div class="p1">
                    <p id="66"><i>θ</i>←<i>θ</i>-<i>η</i>·ᐁ<sub><i>θ</i></sub><i>J</i>(<i>θ</i>)      (7)</p>
                </div>
                <div class="p1">
                    <p id="67">式中:ᐁ<sub><i>θ</i></sub><i>J</i>(<i>θ</i>)是参数的梯度。</p>
                </div>
                <div class="p1">
                    <p id="68">小批量梯度下降算法的核心思想是每次更新的时候从训练集中随机选择<i>n</i>个样本进行学习,更新公式如下:</p>
                </div>
                <div class="p1">
                    <p id="69"><i>θ</i>=<i>θ</i>-<i>η</i>·ᐁ<sub><i>θ</i></sub><i>J</i>(<i>θ</i>;<i>x</i><sub><i>i</i>:<i>i</i>+<i>m</i></sub>;<i>y</i><sub><i>i</i>:<i>i</i>+<i>m</i></sub>)      (8)</p>
                </div>
                <div class="p1">
                    <p id="70">小批量梯度下降算法具有随机梯度下降算法和批量梯度下降算法的优点,实现了更新速度和更新次数之间的平衡。与随机梯度下降算法相比,小批量梯度下降算法减小了训练过程中参数的变化幅度,能够更稳定地收敛;相对于批量梯度下降算法,小批量梯度下降算法利用高度优化提高了梯度计算的效率,加快了每次学习的速度,而且规避了内存不足的现象。</p>
                </div>
                <h4 class="anchor-tag" id="71" name="71"><b>1.3 Dropout正则化</b></h4>
                <div class="p1">
                    <p id="72">Dropout正则化是Hinton<citation id="195" type="reference"><link href="25" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>在2014年提出的一种防止神经网络过拟合的正则化约束技术。通过在反向传播误差更新权值的时候随机选择一部分权值不更新,相当于随机删除一部分的Dropout隐藏节点,随着节点数目的减少,就能防止神经网络的过度训练,也就是防止参数过拟合问题的出现。但这种删除实质上只是暂时的忽略这些节点,而不是真正意义上的完全抛弃。</p>
                </div>
                <div class="p1">
                    <p id="73">在训练过程中,被“丢弃”的隐藏节点是随机的,也就是说在每一次的训练过程中,使用的网络都是不一样的。由于每次用于训练的隐藏节点是随机的,所以并不是每一个节点都能同时出现在每一次的训练过程中,这样就可以保证权值的更新不依赖于有固定关系隐藏节点的共同作用,很大程度上保证了特征的有效性和随机性;在模型用于预测的时候,又会用到所有的隐藏节点,相当于将所有的训练的模型进行了有效的组合,得到了一个更完美的模型。</p>
                </div>
                <h4 class="anchor-tag" id="74" name="74"><b>1.4 评价指标</b></h4>
                <div class="p1">
                    <p id="75">我们采用信息检索中常用的精确度、召回率和 F 值来评估模型的性能。</p>
                </div>
                <h4 class="anchor-tag" id="76" name="76">(1) 精确度:反映了模型标记的正确率。公式如下:</h4>
                <div class="p1">
                    <p id="77"><i>P</i>=<i>f</i><sub><i>n</i></sub>×100%/<i>f</i><sub><i>a</i></sub>      (9)</p>
                </div>
                <h4 class="anchor-tag" id="78" name="78">(2) 召回率:衡量了模型标记的正确覆盖率。公式如下:</h4>
                <div class="p1">
                    <p id="79"><i>R</i>=<i>f</i><sub><i>n</i></sub>×100%/<i>n</i>      (10)</p>
                </div>
                <h4 class="anchor-tag" id="80" name="80">(3) F值:对精确度和召回率进行调和平均。公式如下:</h4>
                <div class="p1">
                    <p id="81"><i>F</i>=2×<i>P</i>×<i>R</i>/(<i>P</i>+<i>R</i>)      (11)</p>
                </div>
                <div class="p1">
                    <p id="82">式中: <i>f</i><sub><i>n</i></sub>表示模型能够正确标记的语义角色个数,<i>f</i><sub><i>a</i></sub>表示模型标记出的语义角色个数,<i>n</i>表示测试集中包含的所有语义角色的个数。</p>
                </div>
                <h3 id="83" name="83" class="anchor-tag"><b>2 语义角色标注模型</b></h3>
                <h4 class="anchor-tag" id="84" name="84"><b>2.1 基于LSTM的词性特性模型</b></h4>
                <div class="p1">
                    <p id="85">本文构建并训练了一个LSTM网络层,以获得词性特征的表达,并利用词性标签对数据进行训练。</p>
                </div>
                <div class="p1">
                    <p id="86">在模型的构建阶段,我们使用式(12)初始化参数,然后输入词向量,通过神经网络的计算,获得网络层最后时刻的输出值<i>x</i><mathml id="87"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>p</mi><mrow><mi>Ρ</mi><mi>o</mi><mi>s</mi></mrow></msubsup></mrow></math></mathml>,我们认为该向量能体现整个序列的词性信息;在网络的训练阶段,我们首先将输出层的输出值<i>x</i><mathml id="88"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>t</mi><mrow><mi>Ρ</mi><mi>o</mi><mi>s</mi></mrow></msubsup></mrow></math></mathml>发送到Softmax分类器以获得词性标签,然后将该标签与训练文本的词性标签进行比较,计算损失函数值,接着利用BP算法将误差值按照从后向前的顺序逐层传递,依次更新每层的权值直到模型循环结束,最后得到的就是输入数据对应的词性特征向量。</p>
                </div>
                <div class="p1">
                    <p id="89"><i>sigmoid</i>:<mathml id="90"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>W</mi><mo>∼</mo><mi>U</mi><mi>n</mi><mi>i</mi><mi>f</mi><mi>o</mi><mi>r</mi><mi>m</mi><mrow><mo>(</mo><mrow><mo>-</mo><mn>4</mn><mo>×</mo><mfrac><mrow><msqrt><mn>6</mn></msqrt></mrow><mrow><msqrt><mrow><mi>n</mi><msub><mrow></mrow><mrow><mtext>i</mtext><mtext>n</mtext></mrow></msub><mo>+</mo><mi>n</mi><msub><mrow></mrow><mrow><mtext>o</mtext><mtext>u</mtext><mtext>t</mtext></mrow></msub></mrow></msqrt></mrow></mfrac><mo>,</mo><mn>4</mn><mo>×</mo><mfrac><mrow><msqrt><mn>6</mn></msqrt></mrow><mrow><msqrt><mrow><mi>n</mi><msub><mrow></mrow><mrow><mtext>i</mtext><mtext>n</mtext></mrow></msub><mo>+</mo><mi>n</mi><msub><mrow></mrow><mrow><mtext>o</mtext><mtext>u</mtext><mtext>t</mtext></mrow></msub></mrow></msqrt></mrow></mfrac></mrow><mo>)</mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>2</mn><mo stretchy="false">)</mo></mrow></math></mathml></p>
                </div>
                <div class="p1">
                    <p id="91">式中:<i>n</i><sub>in</sub>表示网络层输入节点数, <i>n</i><sub>out</sub>表示网络层输出节点数。</p>
                </div>
                <h4 class="anchor-tag" id="92" name="92"><b>2.2 语义密度聚类</b></h4>
                <div class="p1">
                    <p id="93">在中文的语义角色标注中,存在明显的谓词标注不均匀问题,常见谓词和稀疏谓词的标注实例呈现两种极端的状况,稀疏谓词由于标注的次数很少,所以在模型训练过程中缺乏足够的样本实例,导致模型很难学习到该类谓词的有效参数,使得训练后的模型对稀疏谓词不能进行很好的语义分析。本文将出现15次以下的谓词表示为稀疏谓词,超过15次的表示为常见谓语,并引入了语义密度聚类算法对稀疏谓词进行聚类以后替换为语义最相近的常见谓词,提升语义角色标注模型对于稀疏谓词标注的适用性。</p>
                </div>
                <div class="p1">
                    <p id="94">我们采用wordvec提供的Skip-Gram模型训练维基百科中文数据得到的词向量对语料中谓词进行初始化,然后使用语义密度聚类算法进行聚类,其步骤描述如下:</p>
                </div>
                <div class="p1">
                    <p id="95">(1) 计算每个谓词词向量与其他谓词词向量的欧式几何距离。</p>
                </div>
                <div class="p1">
                    <p id="96">(2) 计算每个谓词词向量的<i>K</i>距离,升序排列后得到<i>E</i>′。</p>
                </div>
                <div class="p1">
                    <p id="97">(3) 根据<i>E</i>′绘制曲线,急剧变化位置所对应的<i>K</i>距离作为<i>Eps</i>。</p>
                </div>
                <div class="p1">
                    <p id="98">(4) 根据<i>Eps</i>计算得到所有核心向量,建立核心向量到<i>Eps</i>范围内的映射。</p>
                </div>
                <div class="p1">
                    <p id="99">(5) 根据核心点集合以及<i>Eps</i>计算可以连通的核心向量,得到噪声向量。</p>
                </div>
                <div class="p1">
                    <p id="100">(6) 将能连通的每一组核心向量以及核心距离小于<i>Eps</i>的向量归为一个聚类集合。</p>
                </div>
                <div class="p1">
                    <p id="101">完成语义密度聚类以后,我们可以得到多个基于密度的词向量集合。在数据输入阶段,当一个谓词输入进入语义角色标注系统的时候,会先判断其是否为稀疏谓词,如果是稀疏谓词,我们将其替换为其所在聚类集合中的常见谓词,如果聚类集合中存在多个核心谓词,则选择距离最近的常见谓词;如果是噪声向量即没有所属聚类集合的谓词,以及常见谓词,我们不做任何替换。</p>
                </div>
                <h4 class="anchor-tag" id="102" name="102"><b>2.3 词向量模糊机制</b></h4>
                <div class="p1">
                    <p id="103">语义角色标注任务的目的是获取文本中词语与谓词的语义关联,因此语义角色标注模型实际上学习的是每个词语与谓词之间的关系,每个词语具有独立的语义。在训练之前,词向量之间不能体现相互间的关系,所以在模型训练可以说是从零开始的。为此我们引入了词向量“模糊”机制,利用距离的概念减小原始词向量的语义表示性并提升与谓词词向量的相关性。</p>
                </div>
                <div class="p1">
                    <p id="104">本文在数据输入阶段,对输入句子的所有词语进行词向量初始化,然后利用下列公式分别计算非谓词与谓词之间的距离<i>d</i><sub><i>i</i></sub>和距离总和<i>d</i>。</p>
                </div>
                <div class="p1">
                    <p id="105"><mathml id="106"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>d</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><msqrt><mrow><mo stretchy="false">(</mo><mi>w</mi><msub><mrow></mrow><mi>p</mi></msub><mo>-</mo><mi>w</mi><msub><mrow></mrow><mrow><mi>d</mi><mi>i</mi></mrow></msub><mo stretchy="false">)</mo><mo stretchy="false">(</mo><mi>w</mi><msub><mrow></mrow><mi>p</mi></msub><mo>-</mo><mi>w</mi><msub><mrow></mrow><mrow><mi>d</mi><mi>i</mi></mrow></msub><mo stretchy="false">)</mo><msup><mrow></mrow><mtext>Τ</mtext></msup></mrow></msqrt></mrow></math></mathml>      (13)</p>
                </div>
                <div class="p1">
                    <p id="107"><i>d</i>=∑<i>d</i><sub><i>i</i></sub>      (14)</p>
                </div>
                <div class="p1">
                    <p id="108">词向量模糊机制由非谓词词向量以及谓词词向量和乘积相加操作组成,如式(15)所示,经过这样处理的词向量在一定程度上减小了其原始语义信息的表示程度并提升了与谓词词向量的相关性。<i>w</i><sub><i>d</i></sub>表示非谓词词向量,<i>w</i><sub><i>p</i></sub>表示谓词词向量。</p>
                </div>
                <div class="p1">
                    <p id="109"><mathml id="110"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msup><mi>w</mi><mo>′</mo></msup><msub><mrow></mrow><mi>d</mi></msub><mo>=</mo><mfrac><mrow><mi>d</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mi>d</mi></mfrac><mo>⋅</mo><mi>w</mi><msub><mrow></mrow><mi>d</mi></msub><mo>+</mo><mrow><mo>(</mo><mrow><mn>1</mn><mo>-</mo><mfrac><mrow><mi>d</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mi>d</mi></mfrac></mrow><mo>)</mo></mrow><mo>⋅</mo><mi>w</mi><msub><mrow></mrow><mi>p</mi></msub></mrow></math></mathml>      (15)</p>
                </div>
                <h4 class="anchor-tag" id="111" name="111"><b>2.4 基于模糊机制和语义密度聚类的语义角色标注模型</b></h4>
                <div class="p1">
                    <p id="112">基于Bi-LSTM神经网络模型,提出了一个基于模糊机制和语义密度聚类的汉语自动语义角色标注模型。</p>
                </div>
                <div class="p1">
                    <p id="113">作为SRL模型的一部分输入,我们将“模糊化”后的词向量与词性向量融合为一种的新的特征向量组合,所以在模型构建阶段,SRL模型网络层的输入向量可以定义为:</p>
                </div>
                <div class="p1">
                    <p id="114"><i>X</i><mathml id="115"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>t</mi><mrow><mi>s</mi><mi>r</mi><mi>l</mi></mrow></msubsup></mrow></math></mathml>={<i>x</i>′<sub><i>t</i></sub>,<i>x</i><mathml id="116"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>t</mi><mrow><mi>Ρ</mi><mi>o</mi><mi>s</mi></mrow></msubsup></mrow></math></mathml>}</p>
                </div>
                <div class="p1">
                    <p id="117">式中:<i>x</i>′<sub><i>t</i></sub>表示“模糊化”后的词向量,<i>x</i><mathml id="118"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>t</mi><mrow><mi>Ρ</mi><mi>o</mi><mi>s</mi></mrow></msubsup></mrow></math></mathml>表示词性向量,<i>t</i>表示输入时刻。</p>
                </div>
                <div class="p1">
                    <p id="119">在模型的训练阶段,对于整个模型的网络层来说,输入应该是融合向量与输入数据对应的语义角色标签,所以我们定义模型的输入为:</p>
                </div>
                <div class="p1">
                    <p id="120"><i>X</i>={(<i>x</i><mathml id="121"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>1</mn><mrow><mi>s</mi><mi>r</mi><mi>l</mi></mrow></msubsup></mrow></math></mathml>,<i>srl</i><sub>1</sub>),(<i>x</i><mathml id="122"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>2</mn><mrow><mi>s</mi><mi>r</mi><mi>l</mi></mrow></msubsup></mrow></math></mathml>,<i>srl</i><sub>2</sub>),…,(<i>x</i><mathml id="123"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow><mrow><mi>s</mi><mi>r</mi><mi>l</mi></mrow></msubsup></mrow></math></mathml>,<i>srl</i><sub><i>t</i>-1</sub>),(<i>x</i><mathml id="124"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>t</mi><mrow><mi>s</mi><mi>r</mi><mi>l</mi></mrow></msubsup></mrow></math></mathml>,<i>srl</i><sub><i>t</i></sub>)}</p>
                </div>
                <div class="p1">
                    <p id="125">式中:<i>x</i><mathml id="126"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>t</mi><mrow><mi>s</mi><mi>r</mi><mi>l</mi></mrow></msubsup></mrow></math></mathml>表示<i>t</i>时刻输入的融合向量,<i>srl</i><sub><i>t</i></sub>表示<i>t</i>时刻输入数据对应的语义角色标签。</p>
                </div>
                <div class="p1">
                    <p id="127">数据完成预处理以后,就发送到网络层开始训练。随着神经网络深度的增加,容易出现过拟合现象,并且泛化能力会有所下降,所以本文构建了4层B-LSTM网络来用于我们的语义角色标注任务,图3是一个只有两层网络的模型整体框架。</p>
                </div>
                <div class="area_img" id="128">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201909015_128.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 基于B-LSTM的汉语自动语义角色标注模型" src="Detail/GetImg?filename=images/JYRJ201909015_128.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 基于B-LSTM的汉语自动语义角色标注模型  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201909015_128.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="129">在模型训练阶段,我们使用小批量梯度下降法(MBGD)作为整体的训练技术,实现目标函数的优化。输入特征向量通过网络层的特征学习后,将网络层输出送入CRF层使用维特比算法进行序列标记任务。</p>
                </div>
                <h4 class="anchor-tag" id="130" name="130"><b>2.5 CRF(条件随机场</b>)</h4>
                <div class="p1">
                    <p id="131">在本文的方法中,我们将语义角色标注模型转换为单词序列标注问题,因此在我们计算出神经网络中输入序列的每个单词的标签概率之后,将标签概率送入CRF层,并且对所有表现序列归一化处理以完成最优的序列标注。我们使用了维特比算法来进行最优标签序列的推断。维特比伪算法如算法1所示。</p>
                </div>
                <div class="p1">
                    <p id="132"><b>算法1</b> 维特比伪算法</p>
                </div>
                <div class="p1">
                    <p id="133">输入:标签概率<i>x</i>=(<i>x</i><sub>1</sub>,<i>x</i><sub>2</sub>,…,<i>x</i><sub><i>n</i></sub>)、特征向量<i>F</i>(<i>y</i>,<i>x</i>)和权值向量<i>w</i>。</p>
                </div>
                <div class="p1">
                    <p id="134">输出:标签序列<i>y</i><sup>*</sup>=(<i>y</i><mathml id="135"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>1</mn><mo>*</mo></msubsup></mrow></math></mathml>,<i>y</i><mathml id="136"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>2</mn><mo>*</mo></msubsup></mrow></math></mathml>,…,<i>y</i><mathml id="137"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>n</mi><mo>*</mo></msubsup></mrow></math></mathml>)。</p>
                </div>
                <div class="p1">
                    <p id="138">(1) for  <i>j</i>= 1  to <i>m</i></p>
                </div>
                <div class="p1">
                    <p id="139">(2)  <i>δ</i><sub>1</sub>(<i>j</i>)=<i>w</i>·<i>F</i><sub>1</sub>(<i>y</i><sub>0</sub>=<i>start</i>,<i>y</i><sub>1</sub>=<i>j</i>,<i>x</i>);</p>
                </div>
                <div class="p1">
                    <p id="140">(3) end  for;</p>
                </div>
                <div class="p1">
                    <p id="141">(4) for  <i>i</i>=2  to <i>n</i></p>
                </div>
                <div class="p1">
                    <p id="142">(5)  for <i>l</i>=1  to <i>m</i></p>
                </div>
                <div class="p1">
                    <p id="143">(6) <i>δ</i><sub><i>i</i></sub>(<i>j</i>)=max{<i>δ</i><sub><i>i</i>-1</sub>(<i>j</i>)+<i>w</i>·<i>F</i><sub><i>i</i></sub>(<i>y</i><sub><i>i</i>-1</sub>=<i>j</i>,<i>y</i><sub><i>i</i></sub>=<i>l</i>,<i>x</i>)};</p>
                </div>
                <div class="p1">
                    <p id="144">(7) <i>ψ</i><sub><i>i</i></sub>(<i>l</i>)=argmax{<i>δ</i><sub><i>i</i>-1</sub>(<i>j</i>)+<i>w</i>·<i>F</i><sub><i>i</i></sub>(<i>y</i><sub><i>i</i>-1</sub>=<i>j</i>,<i>y</i><sub><i>i</i></sub>=<i>l</i>,<i>x</i>)};</p>
                </div>
                <div class="p1">
                    <p id="145">(8)  end for;</p>
                </div>
                <div class="p1">
                    <p id="146">(9) max(<i>w</i>·<i>F</i>(<i>y</i>,<i>x</i>))=max<i>δ</i><sub><i>n</i></sub>(<i>j</i>);</p>
                </div>
                <div class="p1">
                    <p id="147">(10)<mathml id="148"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>y</mi><msubsup><mrow></mrow><mi>n</mi><mo>*</mo></msubsup><mo>=</mo><mrow><mi>arg</mi></mrow><mrow><mi>max</mi></mrow><mi>δ</mi><msub><mrow></mrow><munderover><mstyle mathsize="140%" displaystyle="true"><mi>n</mi></mstyle><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover></msub><mo stretchy="false">(</mo><mi>j</mi><mo stretchy="false">)</mo><mo>;</mo><mspace width="0.25em" /></mrow></math></mathml></p>
                </div>
                <div class="p1">
                    <p id="149">(11) end for;</p>
                </div>
                <div class="p1">
                    <p id="150">(12) <i>y</i><mathml id="151"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mo>*</mo></msubsup></mrow></math></mathml>=<i>ψ</i><sub><i>i</i>+1</sub>(<i>y</i><mathml id="152"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow><mo>*</mo></msubsup></mrow></math></mathml>) <i>i</i>=(<i>n</i>-1,<i>n</i>-2,…,1);</p>
                </div>
                <h3 id="153" name="153" class="anchor-tag"><b>3 实 验</b></h3>
                <h4 class="anchor-tag" id="154" name="154"><b>3.1 语料与标注模式</b></h4>
                <div class="p1">
                    <p id="155">本文利用Google开源工具wordvec提供的Skip-Gram模型来训练维基百科中文数据得到词向量,并用于对实验语料进行词向量初始化。实验语料选取的是Chinese Proposition Bank(CPB),它是宾州大学人工标注语料库,标注标注方法和English PropBank类似。CPB仅标注核心动词及其相关语义角色,共定义了谓语动词、6类核心语义角色和13类附加语义角色,如Pred、Arg0-Arg5等。在本文的实验中,我们选取CPB第81分块至第364分块(chtb_081.fid～ chtb_364.fid)的文本数据作为训练数据,选取第1分块至第17分块(chtb_001.fid～ chtb_017.fid)作为测试集,第41分块至第59分块(chtb_041.fid～ chtb_059.fid)作为开发集。</p>
                </div>
                <div class="p1">
                    <p id="156">我们将语义角色的识别和分类任务同时进行,目的是为一个词给出相应的Arg0-Arg5或者ArgM的标签。在实际标注过程中,一个语义角色不仅包含一个词,也有可能由几个词组成,简单地使用像Arg0这样的标签并不能真正识别语义角色的边界或者非语义角色的词。所以我们采用了IOBES标注策略,使用I-ArgX表示语义角色的中间词,B-ArgX表示语义角色的开始词,E-ArgX表示语义角色的结束词,S-ArgX表示单一词构成的语义角色,增加标记O为非语义角色的标签,共计形成77个标签。</p>
                </div>
                <h4 class="anchor-tag" id="157" name="157"><b>3.2 实验结果与分析</b></h4>
                <div class="p1">
                    <p id="158">本文将实验模型分为四类,分别为:(1) 原始词向量融合词性特征向量作为输入的模型;(2) 对非谓语词向量进行“模糊”化并融合词性特征向量作为输入的模型;(3) 对谓语进行语义密度聚类并将原始词向量融合词性特征向量作为输入的模型;(4) 对谓语进行语义密度聚类并将“模糊”化后的非谓语词向量融合词性特征向量作为输入的模型。为了方便描述,本文将这四类模型分别依次标记为模型一、模型二、模型三和模型四。首先我们语义角色标注模型的角度即dropout概率参数、学习率、向量维度三个影响因子对本文提出的模型四进行性能评估。</p>
                </div>
                <div class="p1">
                    <p id="159">默认超参数设置:词性向量维度为32,训练次数为150, 隐藏节点数为128。</p>
                </div>
                <h4 class="anchor-tag" id="160" name="160"><b>3.2.1 Dropout概率参数实验对比</b></h4>
                <div class="p1">
                    <p id="161">我们在表1中对比了不同Dropout概率参数下模型的性能,该实验的超参数设置:学习率为0.003,词向量维度为200。具体实验结果如表1所示。</p>
                </div>
                <div class="area_img" id="162">
                    <p class="img_tit"><b>表1 Dropout概率参数对比实验</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="162" border="1"><tr><td>Dropout概率参数</td><td>精确度P/%</td><td>召回率R/%</td><td>F值/%</td></tr><tr><td><br />0(不加入Dropout)</td><td>74.78</td><td>73.23</td><td>73.84</td></tr><tr><td><br />0.4</td><td>76.60</td><td>72.98</td><td>74.75</td></tr><tr><td><br />0.5</td><td>78.13</td><td>75.39</td><td>76.74</td></tr><tr><td><br />0.6</td><td>80.62</td><td>77.38</td><td>78.96</td></tr><tr><td><br />0.7</td><td>77.26</td><td>74.07</td><td>75.63</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="163">从表1中可以看出,不加入Dropout正则化技术的时候模型的F值仅为73.84%,随着连接的丢弃概率的逐渐升高,在概率为0.6时模型的F值较概率为0时提高了约5%,达到了78.96%;但是概率参数达到0.7时,模型的F值会下降。从精确度和召回率角度分析,Dropout正则化技术的引入大幅度地提高了精确度,然而,对召回率的影响很小,这说明,Dropout正则化有效地解决了模型过度拟合的问题。</p>
                </div>
                <h4 class="anchor-tag" id="164" name="164"><b>3.2.2 学习率实验对比</b></h4>
                <div class="p1">
                    <p id="165">我们在表2中对比了不同学习率下模型的性能,该实验的超参数设置:Dropout概率参数为0.6,词向量维度为200。具体实验结果如表2所示。</p>
                </div>
                <div class="area_img" id="166">
                    <p class="img_tit"><b>表2 学习率对比实验</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="166" border="1"><tr><td><br />学习率</td><td>精确度P/%</td><td>召回率R/%</td><td>F值/%</td></tr><tr><td><br />0.1</td><td>71.67</td><td>68.11</td><td>69.84</td></tr><tr><td><br />0.01</td><td>73.88</td><td>71.06</td><td>72.44</td></tr><tr><td><br />0.03</td><td>76.76</td><td>73.32</td><td>75.00</td></tr><tr><td><br />0.003</td><td>80.62</td><td>77.38</td><td>78.96</td></tr><tr><td><br />0.001</td><td>78.98</td><td>74.11</td><td>76.46</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="167">从表2可以看出,学习率从0.1减小到0.003,模型的F值有了约8%的提升。说明随着学习率的减小,可以使梯度下降法得到更好的性能,从而提高模型的训练效率。但是由于本文使用到的数据量不是十分的巨大,所以当学习率降到0.001时,模型的性能反而比不过学习率为0.003时的模型性能。</p>
                </div>
                <h4 class="anchor-tag" id="168" name="168"><b>3.2.3 词向量维度实验对比</b></h4>
                <div class="p1">
                    <p id="169">我们在表3中对比了不同词向量维度下模型的性能,该实验的超参数设置:Dropout概率参数为0.6,学习率为0.003。具体实验结果如表3所示。</p>
                </div>
                <div class="area_img" id="170">
                    <p class="img_tit"><b>表3 词向量维度对比实验</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="170" border="1"><tr><td><br />词向量维度</td><td>精确度P/%</td><td>召回率R/%</td><td>F值/%</td></tr><tr><td><br />OS-100</td><td>78.96</td><td>76.16</td><td>77.53</td></tr><tr><td><br />OS-200</td><td>80.62</td><td>77.38</td><td>78.96</td></tr><tr><td><br />OS-300</td><td>83.14</td><td>79.43</td><td>81.24</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="171">从表3可以看出,随着词向量维度的增加,模型的各个评价指标值都有所增加,说明词向量维度的增加可以提高特征的区别度,提高模型对特征的捕捉能力;从另一角度来看,三个评价指标的增长幅度都不大,说明word2eVec可以将字词转化为稠密向量用于表征词,能很好地获取词义信息。</p>
                </div>
                <h4 class="anchor-tag" id="172" name="172"><b>3.2.4 语义密度聚类与模糊机制实验对比</b></h4>
                <div class="p1">
                    <p id="173">通过上述三个实验可以得出结论:当Dropout概率参数为0.6,学习率为0.003,词向量维度为300,本文模型的性能达到最佳。利用该参数下的模型,我们在表4中对比了加入语义密度聚类和模糊机制模型的性能,具体实验结果如表4所示。</p>
                </div>
                <div class="area_img" id="174">
                    <p class="img_tit"><b>表4 语义密度聚类与模糊机制对比实验</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="174" border="1"><tr><td>采用方法</td><td>精确度P/%</td><td>召回率R/%</td><td>F值/%</td></tr><tr><td><br />模型一</td><td>80.68</td><td>76.40</td><td>78.48</td></tr><tr><td><br />模型二</td><td>81.26</td><td>78.03</td><td>79.61</td></tr><tr><td><br />模型三</td><td>82.97</td><td>78.82</td><td>80.84</td></tr><tr><td><br />模型四</td><td>83.14</td><td>79.43</td><td>81.24</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="175">从表4中我们可以看出,模糊机制与语义密度聚类对于提升模型的各项指标都有较为明显的促进作用,说明模糊机制能一定程度上提升非谓词词向量与谓词词向量的相关性并减小其原始的语义表达能力;语义密度聚类能减小模型对稀疏谓词训练不足的缺陷,基本上能让每个“谓词-论元”组合都有足够数量的训练样本用于模型的训练,较大程度地提升模型的性能。</p>
                </div>
                <h4 class="anchor-tag" id="176" name="176"><b>3.2.5 不同方法组合的Loss图</b></h4>
                <div class="p1">
                    <p id="177">在对四个模型训练的过程中,我们统计了各个模型的Loss变化曲线,并整合为如图4所示的曲线图,以便于观察比较。</p>
                </div>
                <div class="area_img" id="178">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201909015_178.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 不同模型的Loss变化曲线图" src="Detail/GetImg?filename=images/JYRJ201909015_178.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 不同模型的Loss变化曲线图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201909015_178.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="179">从图4可以看出,在训练了大概1 400个epoch之后,模型一开始趋于稳定;在训练了大概1 700个epoch之后,模型2开始趋于稳定;在训练了大概1 200个epoch之后,模型三开始趋于稳定;在训练了大概1 300个epoch之后,模型四开始趋于稳定。在将 “模糊”机制应用于SRL模型以后,模型需要更多的数据训练才能趋于稳定,说明“模糊”机制使得词向量变得更无规律性,减小了词向量的原始语义表达能力。但是随着训练的进行,训练后期的Loss值变化相较于其他模型显得平滑许多,说明“模糊”机制使得非谓语词向量增大了与谓语词向量的相关性,在两者差异性缩小以后,模型的适用性会得到提高。将语义密度聚类应用于SRL模型以后,训练模型趋于平稳所需要的数据量有了一定程度的缩小,说明语义密度聚类减小了稀疏谓语对于模型训练不均衡的影响,使得“谓语-论元”的数据变得更加具有代表性。</p>
                </div>
                <h4 class="anchor-tag" id="180" name="180"><b>3.2.6 与其他模型实验对比</b></h4>
                <div class="p1">
                    <p id="181">通过上述三个实验可以得出结论:当Dropout概率参数为0.6,学习率为0.003,词向量维度为300,本文模型的性能达到最佳。我们将用该参数下的模型与其他模型进行性能对比。具体对比结果如表5所示。</p>
                </div>
                <div class="area_img" id="182">
                    <p class="img_tit"><b>表5 与已有标注模型的结果对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="182" border="1"><tr><td><br />方法</td><td>F值/%</td></tr><tr><td><br />Yang and Zong (2014)</td><td>75.31</td></tr><tr><td><br />Wang(2015)</td><td>77.09</td></tr><tr><td><br />Sha(2016)</td><td>77.69</td></tr><tr><td><br />袁里驰(2017)</td><td>79.23</td></tr><tr><td><br />张苗苗(2018)</td><td>79.53</td></tr><tr><td><br />本文方法</td><td>81.24</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="183">从表5可以看出,本文提出的方法相比较于依赖句法分析和人工特征的传统方法,F值有了很大幅度的提升,说明基于深度学习的语义角色标注模型能更好地学习字词特征之间细微差别,对于特征的学习效率更高。本文引入了“模糊”机制和语义密度聚类算法,能提升模型对于数据训练的密集程度,减小稀疏谓语对于模型训练不均衡的影响,并提升非谓语词向量与谓语词向量的相关性,提升模型的训练准确度。</p>
                </div>
                <h3 id="184" name="184" class="anchor-tag"><b>4 结 语</b></h3>
                <div class="p1">
                    <p id="185">为了弥补传统语义角色标注方法和现有基于神经网络模型方法的局限性,本文提出了一种基于“模糊”机制和语义密度聚类的汉语语义角色标注模型,并在CPB上进行验证。实验表明:本文提出的方法较传统语义角色标注方法性能有了很明显的改进,较已有的基于深度学习的语义角色标注模型也有了小幅度的进步。本文对不同的实验超参数以及不同模型的组合进行了对比实验,并且在模型训练过程中使用了Dropout正则化方法来缓解模型过拟合的现象,最终本文提出的模型最好的F值达到了81.24%。虽然本文提出的方法相较于以前的大部分方法,性能上有了一定的改进,但还有很多的不足。例如在语义密度聚类的时候,没有考虑特征的选择,而是直接利用距离来判断谓词之间语义的相关性进行直接的替换,而且在模型的调优方面没有做到尽善尽美。后续工作将从词向量的语义表达形式以及如何将领域信息融入SRL模型进行深入的研究。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Shallow Semantc Parsing of Chinese">

                                <b>[1]</b> Sun H,Jurafsky D.Shallow Semantc Parsing of Chinese[C]//Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics,HLT-NAACL 2004,Boston,Massachusetts,USA,May 2-7,2004:249-256.
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Automatic semanticrole labeling for Chinese verbs">

                                <b>[2]</b> Xue N W,Palmer M.Automatic Semantic Role Labeling for Chinese Verbs[C]//Proceedings of the 19th international joint conference on Artificial intelligence.Morgan Kaufmann Publishers Inc.,2005:1160-1165.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Annotating the Propositions in the Penn Chinese Treebank">

                                <b>[3]</b> Xue N W,Palmer M.Annotating the propositions in the Penn Chinese Treebank[C]//Proceedings of the second SIGHAN workshop on Chinese language processing-Volume 17.Association for Computational Linguistics,2003:47-54.
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Chinese Semantic Role Labeling with Bidirectional Recurrent Neural Networks">

                                <b>[4]</b> Wang Z,Jiang T S,Chang B B,et al.Chinese Semantic RoleLabeling with Bidirectional Recurrent Neural Networks[C]//Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,2015:1626-1631.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Neural Semantic Role Labeling with Dependency Path Embeddings">

                                <b>[5]</b> Roth M,Lapata M.Neural semantic role labeling with dependency path embeddings[C]//Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics,2016.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Capturing Argument Relationship for Chinese Semantic Role Labeling">

                                <b>[6]</b> Sha L,Li S,Chang B,et al.Capturing argument relationship for Chinese semantic role labeling[C]//Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing.2016:2011-2016.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Unified Architecture for Semantic Role Labeling and Relation Classification">

                                <b>[7]</b> Guo J,Che W,Wang H,et al.A unified architecture for semantic role labeling and relation classification[C]//Proceedings of COLING 2016,the 26th International Conference on Computational Linguistics:Technical Papers.2016:1264-1274.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A simple and accurate syntax-agnostic neural model for dependency-based semantic role labeling">

                                <b>[8]</b> Marcheggiani D,Frolov A,Titov I.A simple and accurate syntax-agnostic neural model for dependency-based semantic role labeling[C]//Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017),2017.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MESS201701019&amp;v=MTMxNjY0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVadEZ5bmhWcjdBS0NqWWZiRzRIOWJNcm85RWJZUUtESDg0dlI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b> 王瑞波,李济洪,李国臣,等.基于Dropout正则化的汉语框架语义角色识别[J].中文信息学报,2017,31(1):147-154.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_10" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZXU201710031&amp;v=MjIxNzVxcUJ0R0ZyQ1VSN3FmWnVadEZ5bmhWcjdBSVRmVGU3RzRIOWJOcjQ5R1pZUUtESDg0dlI0VDZqNTRPM3o=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[10]</b> 袁里驰.利用配价信息的语义角色标注[J].电子学报,2017,45(10):2533-2539.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_11" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYXH201804002&amp;v=MTQ2MjU3cWZadVp0RnluaFZyN0FMelRUWnJHNEg5bk1xNDlGWm9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[11]</b> 张苗苗,张玉洁,刘明童,等.基于Gate机制与Bi-LSTM-CRF的汉语语义角色标注[J].计算机与现代化,2018(4):1-6,31.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Dropout:A simple way to prevent neural networks from overfitting">

                                <b>[12]</b> Srivastava N,Hinton G,Krizhevsky A,et al.Dropout:A simple way to prevent neural networks from overfitting[J].The Journal of Machine Learning Research,2014,15(1):1929-1958.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JYRJ201909015" />
        <input id="dpi" type="hidden" value="600" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201909015&amp;v=MjI1NzlLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnRGeW5oVnI3Qkx6VFpaTEc0SDlqTXBvOUVZWVE=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
