<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637135740745752500%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJYRJ201905025%26RESULT%3d1%26SIGN%3dWZOpIiatoeX4wHjpdTLiVoX9VBY%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JYRJ201905025&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JYRJ201905025&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201905025&amp;v=MTE4MzJxbzlIWVlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVp0Rnl6a1U3L0pMelRaWkxHNEg5ak0=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#35" data-title="&lt;b&gt;0 引 言&lt;/b&gt; "><b>0 引 言</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#40" data-title="&lt;b&gt;1 国内外发展现状&lt;/b&gt; "><b>1 国内外发展现状</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#41" data-title="&lt;b&gt;1.1 物体检测&lt;/b&gt;"><b>1.1 物体检测</b></a></li>
                                                <li><a href="#44" data-title="&lt;b&gt;1.2 文字检测&lt;/b&gt;"><b>1.2 文字检测</b></a></li>
                                                <li><a href="#46" data-title="&lt;b&gt;1.3 域不变性&lt;/b&gt;"><b>1.3 域不变性</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#49" data-title="&lt;b&gt;2 方法原理&lt;/b&gt; "><b>2 方法原理</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#51" data-title="&lt;b&gt;2.1 网络结构&lt;/b&gt;"><b>2.1 网络结构</b></a></li>
                                                <li><a href="#77" data-title="&lt;b&gt;2.2 特征分布&lt;/b&gt;"><b>2.2 特征分布</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#89" data-title="&lt;b&gt;3 实验设置&lt;/b&gt; "><b>3 实验设置</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#91" data-title="&lt;b&gt;3.1 数据集&lt;/b&gt;"><b>3.1 数据集</b></a></li>
                                                <li><a href="#93" data-title="&lt;b&gt;3.2 评价标准&lt;/b&gt;"><b>3.2 评价标准</b></a></li>
                                                <li><a href="#102" data-title="&lt;b&gt;3.3 实验细节&lt;/b&gt;"><b>3.3 实验细节</b></a></li>
                                                <li><a href="#104" data-title="&lt;b&gt;3.4 实验结果&lt;/b&gt;"><b>3.4 实验结果</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#114" data-title="&lt;b&gt;4 结 语&lt;/b&gt; "><b>4 结 语</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#53" data-title="图1 网络结构">图1 网络结构</a></li>
                                                <li><a href="#74" data-title="图2 IBDW模块">图2 IBDW模块</a></li>
                                                <li><a href="#87" data-title="图3 特征分布">图3 特征分布</a></li>
                                                <li><a href="#106" data-title="&lt;b&gt;表1 IN在不同基础网络的性能对比&lt;/b&gt;"><b>表1 IN在不同基础网络的性能对比</b></a></li>
                                                <li><a href="#108" data-title="&lt;b&gt;表2 IN在ICDAR2013数据集训练, ICDAR2013和2015测试集测试的性能对比&lt;/b&gt;"><b>表2 IN在ICDAR2013数据集训练, ICDAR2013和2015测试集测试的性能对比</b></a></li>
                                                <li><a href="#109" data-title="&lt;b&gt;表3 IN在ICDAR2015数据集训练, ICDAR2013和2015测试集测试的性能对比&lt;/b&gt;"><b>表3 IN在ICDAR2015数据集训练, ICDAR2013和2015测试集测试的性能对比</b></a></li>
                                                <li><a href="#112" data-title="图4 可视化效果图">图4 可视化效果图</a></li>
                                                <li><a href="#112" data-title="图4 可视化效果图">图4 可视化效果图</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" title=" 朱利娟, 云中华, 边巴旺堆.基于极坐标变换的脱机手写藏文字符特征提取方法[J].计算机应用与软件, 2018, 35 (3) :162-166." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201803032&amp;v=MjI5ODZHNEg5bk1ySTlHWm9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVp0Rnl6a1U3L0lMelRaWkw=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         朱利娟, 云中华, 边巴旺堆.基于极坐标变换的脱机手写藏文字符特征提取方法[J].计算机应用与软件, 2018, 35 (3) :162-166.
                                    </a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" title=" Girshick R, Donahue J, Darrell T, et al.Rich feature hierarchies for accurate object detection and semantic segmentation[C]// IEEE Conference on Computer Vision and Pattern Recognition, 2014:580-587." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Rich feature hierarchies for accurate object detection and semantic segmentation">
                                        <b>[2]</b>
                                         Girshick R, Donahue J, Darrell T, et al.Rich feature hierarchies for accurate object detection and semantic segmentation[C]// IEEE Conference on Computer Vision and Pattern Recognition, 2014:580-587.
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title=" Girshick R.Fast R-CNN[C]// Proceedings of the 2015 IEEE International Conference on Computer Vision (ICCV) .IEEE, 2015:1440-1448." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=R-CNN">
                                        <b>[3]</b>
                                         Girshick R.Fast R-CNN[C]// Proceedings of the 2015 IEEE International Conference on Computer Vision (ICCV) .IEEE, 2015:1440-1448.
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" title=" Ren S, He K, Girshick R, et al.Faster R-CNN:Towards Real-Time Object Detection with Region Proposal Networks[J].IEEE Transactions on Pattern Analysis &amp;amp; Machine Intelligence, 2017, 39 (6) :1137-1149." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Faster R-CNN:towards real-time object detection with region proposal networks">
                                        <b>[4]</b>
                                         Ren S, He K, Girshick R, et al.Faster R-CNN:Towards Real-Time Object Detection with Region Proposal Networks[J].IEEE Transactions on Pattern Analysis &amp;amp; Machine Intelligence, 2017, 39 (6) :1137-1149.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" title=" Liu W, Anguelov D, Erhan D, et al.SSD:Single Shot MultiBox Detector[C]// European Conference on Computer Vision, 2016:21-37." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=SSD:Single Shot MultiBox Detector">
                                        <b>[5]</b>
                                         Liu W, Anguelov D, Erhan D, et al.SSD:Single Shot MultiBox Detector[C]// European Conference on Computer Vision, 2016:21-37.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" title=" Redmon J, Divvala S, Girshick R, et al.You Only Look Once:Unified, Real-Time Object Detection[C]// IEEE Conference on Computer Vision and Pattern Recognition, 2015:779-788." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=You only look once:Unified,real-time object detection">
                                        <b>[6]</b>
                                         Redmon J, Divvala S, Girshick R, et al.You Only Look Once:Unified, Real-Time Object Detection[C]// IEEE Conference on Computer Vision and Pattern Recognition, 2015:779-788.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" title=" Kim K H, Hong S, Roh B, et al.PVANET:Deep but Lightweight Neural Networks for Real-time Object Detection[C]// Conference and Workshop on Neural Information Processing Systems, 2016." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=PVANET:Deep but Lightweight Neural Networks for Real-time Object Detection">
                                        <b>[7]</b>
                                         Kim K H, Hong S, Roh B, et al.PVANET:Deep but Lightweight Neural Networks for Real-time Object Detection[C]// Conference and Workshop on Neural Information Processing Systems, 2016.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" title=" Zeming Li, Chao Peng, Gang Yu, et al.DetNet:A Backbone network for Object Detection[C]// European Conference on Computer Vision, 2018." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=DetNet:A Backbone network for Object Detection">
                                        <b>[8]</b>
                                         Zeming Li, Chao Peng, Gang Yu, et al.DetNet:A Backbone network for Object Detection[C]// European Conference on Computer Vision, 2018.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" title=" Matas J, Chum O, Urban M, et al.Robust wide-baseline stereo from maximally stable extremal regions[J].Image &amp;amp; Vision Computing, 2004, 22 (10) :761-767." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012201349782&amp;v=MjUyOTdpZk9mYks3SHRET3JZOUVaKzhHQzNRN29CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaW5sVXJ6SUpGOFVhaEk9Tg==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                         Matas J, Chum O, Urban M, et al.Robust wide-baseline stereo from maximally stable extremal regions[J].Image &amp;amp; Vision Computing, 2004, 22 (10) :761-767.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_10" title=" Epshtein B, Ofek E, Wexler Y.Detecting text in natural scenes with stroke width transform[C]// IEEE Conference on Computer Vision and Pattern Recognition, 2010:2963-2970." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Detecting text in natural scenes with stroke width transform">
                                        <b>[10]</b>
                                         Epshtein B, Ofek E, Wexler Y.Detecting text in natural scenes with stroke width transform[C]// IEEE Conference on Computer Vision and Pattern Recognition, 2010:2963-2970.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_11" title=" Liao M, Shi B, Bai X.TextBoxes++:A Single-Shot Oriented Scene Text Detector[J].IEEE Transactions on Image Processing, 2018, 27 (8) :3676-3690." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=TextBoxes十十:A single-shot oriented scene text detector">
                                        <b>[11]</b>
                                         Liao M, Shi B, Bai X.TextBoxes++:A Single-Shot Oriented Scene Text Detector[J].IEEE Transactions on Image Processing, 2018, 27 (8) :3676-3690.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_12" title=" Jaderberg M, Simonyan K, Zisserman A, et al.Spatial Transformer Networks[C]// Advances in Neural Information Processing Systems, 2015:2017-2025." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Spatial Transformer Networks">
                                        <b>[12]</b>
                                         Jaderberg M, Simonyan K, Zisserman A, et al.Spatial Transformer Networks[C]// Advances in Neural Information Processing Systems, 2015:2017-2025.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_13" title=" Inoue N, Furuta R, Yamasaki T, et al.Cross-Domain Weakly-Supervised Object Detection through Progressive Domain Adaptation[C]// IEEE Conference on Computer Vision and Pattern Recognition, 2018." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Cross-Domain Weakly-Supervised Object Detection through Progressive Domain Adaptation">
                                        <b>[13]</b>
                                         Inoue N, Furuta R, Yamasaki T, et al.Cross-Domain Weakly-Supervised Object Detection through Progressive Domain Adaptation[C]// IEEE Conference on Computer Vision and Pattern Recognition, 2018.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_14" title=" Volpi R, Morerio P, Savarese S, et al.Adversarial Feature Augmentation for Unsupervised Domain Adaptation[C]// IEEE Conference on Computer Vision and Pattern Recognition, 2018." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Adversarial Feature Augmentation for Unsupervised Domain Adaptation">
                                        <b>[14]</b>
                                         Volpi R, Morerio P, Savarese S, et al.Adversarial Feature Augmentation for Unsupervised Domain Adaptation[C]// IEEE Conference on Computer Vision and Pattern Recognition, 2018.
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_15" title=" Johnson-Roberson M, Barto C, Mehta R, et al.Driving in the Matrix:Can virtual worlds replace human-generated annotations for real world tasks?[C]// IEEE International Conference on Robotics and Automation.IEEE, 2017:746-753." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Driving in the Matrix: Can virtual worlds replace human-generated annotations for real world tasks?">
                                        <b>[15]</b>
                                         Johnson-Roberson M, Barto C, Mehta R, et al.Driving in the Matrix:Can virtual worlds replace human-generated annotations for real world tasks?[C]// IEEE International Conference on Robotics and Automation.IEEE, 2017:746-753.
                                    </a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_16" title=" Ulyanov D, Vedaldi A, Lempitsky V.Instance Normalization:The Missing Ingredient for Fast Stylization[C]// IEEE Conference on Computer Vision and Pattern Recognition, 2016." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Instance Normalization:The Missing Ingredient for Fast Stylization">
                                        <b>[16]</b>
                                         Ulyanov D, Vedaldi A, Lempitsky V.Instance Normalization:The Missing Ingredient for Fast Stylization[C]// IEEE Conference on Computer Vision and Pattern Recognition, 2016.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JYRJ" target="_blank">计算机应用与软件</a>
                2019,36(05),133-137+198 DOI:10.3969/j.issn.1000-386x.2019.05.024            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>跨域标记牌文字检测算法研究</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E4%BA%8E%E4%B9%8B%E9%9D%96&amp;code=24426349&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">于之靖</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E5%98%89%E4%BC%9F&amp;code=37348659&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王嘉伟</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%83%91%E5%BB%BA%E6%96%87&amp;code=41002969&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">郑建文</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%99%B6%E6%B0%B8%E5%A5%8E&amp;code=41002968&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">陶永奎</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%AF%B8%E8%91%9B%E6%99%B6%E6%98%8C&amp;code=30183830&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">诸葛晶昌</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E6%B0%91%E8%88%AA%E5%A4%A7%E5%AD%A6%E7%94%B5%E5%AD%90%E4%BF%A1%E6%81%AF%E4%B8%8E%E8%87%AA%E5%8A%A8%E5%8C%96%E5%AD%A6%E9%99%A2&amp;code=0029470&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国民航大学电子信息与自动化学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E6%B0%91%E8%88%AA%E5%A4%A7%E5%AD%A6%E8%88%AA%E7%A9%BA%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国民航大学航空工程学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>由于受到光照、遮挡、倾斜等诸多因素的影响, 基于深度学习的文字检测方法的训练集和测试集分布存在差异, 导致模型在不同真实场景下的鲁棒性不足。为了提升现有模型的泛化能力, 改善模型在真实场景下的抗干扰能力, 尝试从网络结构角度出发, 为机场标记牌文字检测任务设计具有域不变性的网络结构, 在不增加计算量的前提下提升算法鲁棒性。结果表明, 该结构可以有效提升模型在不同真实场景下的性能表现。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%96%87%E5%AD%97%E6%A3%80%E6%B5%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">文字检测;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%9F%9F%E4%B8%8D%E5%8F%98%E6%80%A7&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">域不变性;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%9C%BA%E5%9C%BA%E6%A0%87%E8%AE%B0%E7%89%8C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">机场标记牌;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">网络结构;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    于之靖, 教授, 主研领域:计算机视觉, 图像处理。;
                                </span>
                                <span>
                                    王嘉伟, 硕士生。;
                                </span>
                                <span>
                                    郑建文, 硕士生。;
                                </span>
                                <span>
                                    陶永奎, 硕士生。;
                                </span>
                                <span>
                                    诸葛晶昌, 博士。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-11-25</p>

                    <p>

                            <b>基金：</b>
                                                        <span>中央高校基本科研业务费项目 (3122017005, ZYGX2018039);</span>
                    </p>
            </div>
                    <h1><b>CROSS-DOMAIN TAG BOARD TEXT DETECTION ALGORITHM</b></h1>
                    <h2>
                    <span>Yu Zhijing</span>
                    <span>Wang Jiawei</span>
                    <span>Zheng Jianwen</span>
                    <span>Tao Yongkui</span>
                    <span>Zhuge Jingchang</span>
            </h2>
                    <h2>
                    <span>College of Electronic Information and Automation, Civil Aviation University of China</span>
                    <span>College of Aeronautical Engineering, Civil Aviation University of China</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Due to the influence of illumination, occlusion, tilt and other factors, the distribution of training set and test set of text detection method based on deep learning is different, which leads to the insufficient robustness of the model in different real scenes. In order to improve the generalization ability of the existing models and the anti-jamming ability of the models in real scenes, we designed a domain invariant network structure for the text detection task of the airport tag board from the perspective of network structure to enhance the robustness of the algorithm without increasing the computation. The results show that the structure can effectively improve the performance of the model in different real scenes.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Text%20detection&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Text detection;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Domain%20invariance&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Domain invariance;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Airport%20tag%20board&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Airport tag board;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Network%20structure&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Network structure;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-11-25</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="35" name="35" class="anchor-tag"><b>0 引 言</b></h3>
                <div class="p1">
                    <p id="36">近年来, 旅客出行次数日益增多, 民航业的发展较为迅速, 这对民航安全问题带来了新的挑战。机场标记牌作为引导飞机起降滑行的信息指示牌, 对于维持机场秩序、保障飞机起降安全具有重要意义。传统检查标记牌合格性的方法是人工巡检, 此方法不仅效率低而且会造成误检漏检。而人工智能作为近年来国家大力发展的新兴技术产业之一, 已经在城市、医疗、交通、安全等多个领域产生了积极的影响。如何让AI为民航赋能, 建设安全、友好、高效的“智慧机场”还是一个亟待解决的问题。</p>
                </div>
                <div class="p1">
                    <p id="37">自然场景下的文字是一种非常常见的视觉对象, 在路标、牌照以及产品包装等地方经常出现。正确识别自然场景下的文字不仅可以帮助人们更好地感知周围环境, 而且可以辅助人们在面对突发情况时做出正确的决策。</p>
                </div>
                <div class="p1">
                    <p id="38">不同于发展较为成熟的OCR<citation id="116" type="reference"><link href="3" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>, 自然场景下的文字检测由于面对着光照、遮挡、倾斜等诸多因素的影响, 算法的鲁棒性还面临着诸多的挑战。 而近些年, 基于深度学习的物体检测算法发展较为迅速, 现在的卷积神经网络已经具有很好的表达能力, 但网络迁移泛化能力不强。对于在训练中出现过的样本, 网络可以较好地学习, 但针对真实场景中未曾出现的样本, 网络性能就会大幅下降。而真实场景下的文字通常会受光照、遮挡、角度的影响, 如何处理训练集和测试集之间数据分布的差异具有重要意义。</p>
                </div>
                <div class="p1">
                    <p id="39">鉴于此, 尝试从模型结构角度出发, 通过引入域不变性的IN/BN, 来让网络学习到更本质的特征, 并对特征做更精细的定位, 从而提升模型的性能。</p>
                </div>
                <h3 id="40" name="40" class="anchor-tag"><b>1 国内外发展现状</b></h3>
                <h4 class="anchor-tag" id="41" name="41"><b>1.1 物体检测</b></h4>
                <div class="p1">
                    <p id="42">文字检测问题是物体检测问题的一个重要分支, 近些年很多研究人员提出的方法从物体检测发展而来。针对于通用物体检测, 其检测方法大致可以分为两类:一种是多阶段的检测方法, 如R-CNN<citation id="122" type="reference"><link href="5" rel="bibliography" /><link href="7" rel="bibliography" /><link href="9" rel="bibliography" /><sup>[<a class="sup">2</a>,<a class="sup">3</a>,<a class="sup">4</a>]</sup></citation>系列。R-CNN<citation id="117" type="reference"><link href="5" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>把检测问题当成分类问题来看待, 先通过选择性搜索算法得到大量的候选框, 然后通过CNN提取每个候选框的特征, 对于每个框的特征再用SVM进行分类。Fast RCNN<citation id="118" type="reference"><link href="7" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>共享整个卷及网络并通过ROI Pooling输出特定维度特征。而Faster RCNN<citation id="119" type="reference"><link href="9" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>提出了RPN网络来代替选择性搜索算法, 从而极大地减少了测试时间。通常来说, 这样的多阶段的检测方法召回率更高、性能更好, 但是参数量和计算量也更大。另一种是单阶段的检测方法, 如YOLO<citation id="120" type="reference"><link href="13" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>、SSD<citation id="121" type="reference"><link href="11" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>等。最初的YOLO直接在整张图像的不同地方上输出回归框来代替RPN, 这极大地减少了测试时间。而SSD在不同尺度的特征上输出不同长宽比的default box, 这使得SSD在不增加测试时间的同时提升了模型性能。这样的单阶段的检测方法速度更快、实时性更高, 但是精度比多阶段模型略差。特别是对于密集、尺度变化大的文字来说往往效果更加不好。</p>
                </div>
                <div class="p1">
                    <p id="43">此外, PVANet<citation id="123" type="reference"><link href="15" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>利用更好的基础网络结构提取到更加精细的特征, 从而提升了模型性能。而DetNet<citation id="124" type="reference"><link href="17" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>从网络结构的角度出发, 通过设计一个更适合检测任务的基础网络, 提取到了更加合理的特征。</p>
                </div>
                <h4 class="anchor-tag" id="44" name="44"><b>1.2 文字检测</b></h4>
                <div class="p1">
                    <p id="45">文字检测算法主要分为经典算法和深度学习算法两类。MSER<citation id="125" type="reference"><link href="19" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation> (Maximally Stable Extremal Region) 是区域检测中影响最大的算法, 该算法通过不断调整二值化阈值检测稳定极值区域;SWT<citation id="126" type="reference"><link href="21" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation> (Stroke Width Transform) 利用Canny算子对图像进行边缘检测, 得到的每个文字边缘像素点的梯度。如果两个像素点梯度方向相反并且欧氏距离小于一定阈值则被认定为文字边缘;Textbox++<citation id="127" type="reference"><link href="23" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>改变了SSD的default box比例以及卷积核的尺寸, 以提取更加合适的文字特征;STN<citation id="128" type="reference"><link href="25" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation> (Spatial Transformer Networks) 设计了一个空间变换模块解决了扭曲文字定位与识别问题。</p>
                </div>
                <h4 class="anchor-tag" id="46" name="46"><b>1.3 域不变性</b></h4>
                <div class="p1">
                    <p id="47">域不变性是指在某个场景下训练得到的模型在不同场景下依旧具有良好的鲁棒性。目前, 针对文字检测任务的研究较少, 研究者们更多关注图像分类任务。其算法主要分为三类:半监督、迁移学习、GAN。Naoto Inoue等<citation id="129" type="reference"><link href="27" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>提出了一种基于半监督的域不变性算法, 通过利用GAN生成的数据和伪标注数据做多次迁移学习, 使得模型具有更好的域不变性;R Volpi等<citation id="130" type="reference"><link href="29" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>以对抗学习的方式让模型学习到更加鲁棒的特征;C Barto等<citation id="131" type="reference"><link href="31" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>通过迁移学习的方式探讨了用虚拟数据训练模型的可能性。</p>
                </div>
                <div class="p1">
                    <p id="48">以上的这些方法更多地从优化方法的角度提升模型的域不变性, 却较少关注模型结构本身。因此, 针对跨模态标记牌文字检测问题, 研究尝试设计具有域不变性的网络结构和更优质的卷积特征, 在不增加计算量的前提下提升算法的鲁棒性。</p>
                </div>
                <h3 id="49" name="49" class="anchor-tag"><b>2 方法原理</b></h3>
                <div class="p1">
                    <p id="50">本文提出的模型是一个端对端可训练的文字检测器, 通过重新调整DW卷积, 为基础结构引入域不变性的IN, 让网络学习到域不变性的特征, 从而提升基础网络的泛化能力。</p>
                </div>
                <h4 class="anchor-tag" id="51" name="51"><b>2.1 网络结构</b></h4>
                <div class="p1">
                    <p id="52">整个文字检测网络如图1所示。网络结构由三个部分组成:基础网络IBDW, 区域提出网络RPN, Fast RCNN边框回归器。</p>
                </div>
                <div class="area_img" id="53">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201905025_053.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 网络结构" src="Detail/GetImg?filename=images/JYRJ201905025_053.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 网络结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201905025_053.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="54">Instance Normalization:该模块最初应用于风格迁移, ulyanov等<citation id="132" type="reference"><link href="33" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>提出用Instance Normalization (IN) 替换Batch Normalization (BN) , 这样在通过GAN进行风格迁移后时可以阻止实例特定的均值和协方差简化学习的过程, 最大程度保留每张图片独特的纹理细节。其中BN数学公式如下:</p>
                </div>
                <div class="p1">
                    <p id="55"><mathml id="56"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>y</mi><msub><mrow></mrow><mrow><mi>t</mi><mi>i</mi><mi>j</mi><mi>k</mi></mrow></msub><mo>=</mo><mfrac><mrow><mi>x</mi><msub><mrow></mrow><mrow><mi>t</mi><mi>i</mi><mi>j</mi><mi>k</mi></mrow></msub><mo>-</mo><mi>μ</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mrow><msqrt><mrow><mi>σ</mi><msubsup><mrow></mrow><mi>i</mi><mn>2</mn></msubsup></mrow></msqrt><mo>+</mo><mi>ε</mi></mrow></mfrac></mrow></math></mathml>      (1) </p>
                </div>
                <div class="p1">
                    <p id="57"><mathml id="58"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>μ</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mfrac><mn>1</mn><mrow><mi>Η</mi><mi>W</mi><mi>Τ</mi></mrow></mfrac></mrow></math></mathml><mathml id="59"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>Τ</mi></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>l</mi><mo>=</mo><mn>1</mn></mrow><mi>W</mi></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>m</mi><mo>=</mo><mn>1</mn></mrow><mi>Η</mi></munderover><mi>X</mi></mstyle></mrow></mstyle></mrow></mstyle><msub><mrow></mrow><mrow><mi>t</mi><mi>i</mi><mi>l</mi><mi>m</mi></mrow></msub></mrow></math></mathml>      (2) </p>
                </div>
                <div class="p1">
                    <p id="60"><mathml id="61"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>σ</mi><msubsup><mrow></mrow><mi>i</mi><mn>2</mn></msubsup><mo>=</mo><mfrac><mn>1</mn><mrow><mi>Η</mi><mi>W</mi><mi>Τ</mi></mrow></mfrac></mrow></math></mathml><mathml id="62"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>Τ</mi></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>l</mi><mo>=</mo><mn>1</mn></mrow><mi>W</mi></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>m</mi><mo>=</mo><mn>1</mn></mrow><mi>Η</mi></munderover><mo stretchy="false"> (</mo></mstyle></mrow></mstyle></mrow></mstyle><mi>X</mi><msub><mrow></mrow><mrow><mi>t</mi><mi>i</mi><mi>l</mi><mi>m</mi></mrow></msub><mo>-</mo><mi>μ</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><msup><mrow></mrow><mn>2</mn></msup></mrow></math></mathml>      (3) </p>
                </div>
                <div class="p1">
                    <p id="63">相较于BN, IN单独计算每个样本所有像素点的均值和方差, 并做归一化。其数学公式如下:</p>
                </div>
                <div class="p1">
                    <p id="64"><mathml id="65"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>y</mi><msub><mrow></mrow><mrow><mi>t</mi><mi>i</mi><mi>j</mi><mi>k</mi></mrow></msub><mo>=</mo><mfrac><mrow><mi>x</mi><msub><mrow></mrow><mrow><mi>t</mi><mi>i</mi><mi>j</mi><mi>k</mi></mrow></msub><mo>-</mo><mi>μ</mi><msub><mrow></mrow><mrow><mi>t</mi><mi>i</mi></mrow></msub></mrow><mrow><msqrt><mrow><mi>σ</mi><msubsup><mrow></mrow><mrow><mi>t</mi><mi>i</mi></mrow><mn>2</mn></msubsup><mo>+</mo><mi>ε</mi></mrow></msqrt></mrow></mfrac></mrow></math></mathml>      (4) </p>
                </div>
                <div class="p1">
                    <p id="66"><mathml id="67"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>μ</mi><msub><mrow></mrow><mrow><mi>t</mi><mi>i</mi></mrow></msub><mo>=</mo><mfrac><mn>1</mn><mrow><mi>Η</mi><mi>W</mi></mrow></mfrac></mrow></math></mathml><mathml id="68"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>l</mi><mo>=</mo><mn>1</mn></mrow><mi>W</mi></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>m</mi><mo>=</mo><mn>1</mn></mrow><mi>Η</mi></munderover><mi>x</mi></mstyle></mrow></mstyle><msub><mrow></mrow><mrow><mi>t</mi><mi>i</mi><mi>l</mi><mi>m</mi></mrow></msub></mrow></math></mathml>      (5) </p>
                </div>
                <div class="p1">
                    <p id="69"><mathml id="70"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>σ</mi><msubsup><mrow></mrow><mrow><mi>t</mi><mi>i</mi></mrow><mn>2</mn></msubsup><mo>=</mo><mfrac><mn>1</mn><mrow><mi>Η</mi><mi>W</mi></mrow></mfrac></mrow></math></mathml><mathml id="71"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>l</mi><mo>=</mo><mn>1</mn></mrow><mi>W</mi></munderover><mrow></mrow></mstyle><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>m</mi><mo>=</mo><mn>1</mn></mrow><mi>Η</mi></munderover><mrow></mrow></mstyle></mrow></math></mathml> (<i>x</i><sub><i>tilm</i></sub>-<i>μ</i><sub><i>ti</i></sub>) <sup>2</sup>      (6) </p>
                </div>
                <div class="p1">
                    <p id="72">研究发现在神经网络训练优化过程中, 特别是训练样本类内距分布较大时, 网络总会隐式地学习一种风格迁移能力。如果从模型结构角度出发, 用IN替换掉部分BN, 则可以直接赋予模型泛化能力, 保留更多底层纹理细节, 提升模型在真实场景下的表现。</p>
                </div>
                <div class="p1">
                    <p id="73">基础网络:研究提出的基础结构IBDW如图2所示。这样的设计一方面为模型引入域不变性能力。另一方面, 适度的增加IN可以更好地保存成分信息。通过引入IBDW模块, 让网络学习到更加本质特征, 在不增加模型参数的前提下, 提升模型性能。</p>
                </div>
                <div class="area_img" id="74">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201905025_074.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 IBDW模块" src="Detail/GetImg?filename=images/JYRJ201905025_074.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 IBDW模块  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201905025_074.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="75">区域提出网络:区域提出网络RPN在IBDW提取到的特征上为Fast RCNN生成文本建议框。本文设置不同大小、不同长宽比的先验框。通过这样的方式, RPN可以处理不同大小、不同长宽比的文字。ROI Align用来提取区域特征。相比于ROI Pooling, ROI Align可以得到更加精细的位置信息。</p>
                </div>
                <div class="p1">
                    <p id="76">边框回归器:Fast R-CNN包括一个分类任务和一个回归任务。这一部分的主要功能是为检测提供更精确的回归框。RPN网络输出的文本建议框经过ROI Align得到7×7的特征图, 该特征图作为输入经过Fast R-CNN最终得到精细的回归框。</p>
                </div>
                <h4 class="anchor-tag" id="77" name="77"><b>2.2 特征分布</b></h4>
                <div class="p1">
                    <p id="78">为了说明IN模块如何提升模型泛化能力, 这里通过计算不同域特征不同通道的特征分布进行分析。假设特征单通道服从高斯分布, 均值为<i>μ</i>, 方差为<i>σ</i><sup>2</sup>, 则不同域<i>A</i>、<i>B</i>间单通道的KL散度为:</p>
                </div>
                <div class="p1">
                    <p id="79"><i>D</i> (<i>F</i><sub><i>A</i></sub>‖<i>F</i><sub><i>B</i></sub>) =<i>KL</i> (<i>F</i><sub><i>A</i></sub>‖<i>F</i><sub><i>B</i></sub>) -<i>KL</i> (<i>F</i><sub><i>B</i></sub>‖<i>F</i><sub><i>A</i></sub>)      (7) </p>
                </div>
                <div class="p1">
                    <p id="80"><mathml id="81"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Κ</mi><mi>L</mi><mo stretchy="false"> (</mo><mi>F</mi><msub><mrow></mrow><mi>A</mi></msub><mo stretchy="false">∥</mo><mi>F</mi><msub><mrow></mrow><mi>B</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mrow><mi>log</mi></mrow><mfrac><mrow><mi>σ</mi><msub><mrow></mrow><mi>A</mi></msub></mrow><mrow><mi>σ</mi><msub><mrow></mrow><mi>B</mi></msub></mrow></mfrac><mo>+</mo><mfrac><mrow><mi>σ</mi><msubsup><mrow></mrow><mi>A</mi><mn>2</mn></msubsup><mo>+</mo><mo stretchy="false"> (</mo><mi>μ</mi><msub><mrow></mrow><mi>A</mi></msub><mo>-</mo><mi>μ</mi><msub><mrow></mrow><mi>B</mi></msub><mo stretchy="false">) </mo><msup><mrow></mrow><mn>2</mn></msup></mrow><mrow><mn>2</mn><mi>μ</mi><msubsup><mrow></mrow><mi>B</mi><mn>2</mn></msubsup></mrow></mfrac><mo>-</mo><mfrac><mn>1</mn><mn>2</mn></mfrac></mrow></math></mathml>      (8) </p>
                </div>
                <div class="p1">
                    <p id="82">若<i>D</i> (<i>F</i><sub><i>iA</i></sub>‖<i>F</i><sub><i>iB</i></sub>) 表示第<i>i</i>通道的KL散度, 则每层神经网络的平均KL散度表示为:</p>
                </div>
                <div class="p1">
                    <p id="83"><mathml id="84"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>D</mi><mo stretchy="false"> (</mo><mi>L</mi><msub><mrow></mrow><mi>A</mi></msub><mo stretchy="false">∥</mo><mi>L</mi><msub><mrow></mrow><mi>B</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mfrac><mn>1</mn><mi>C</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>C</mi></munderover><mi>D</mi></mstyle><mo stretchy="false"> (</mo><mi>F</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>A</mi></mrow></msub><mo stretchy="false">∥</mo><mi>F</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>B</mi></mrow></msub><mo stretchy="false">) </mo></mrow></math></mathml>      (9) </p>
                </div>
                <div class="p1">
                    <p id="85">式中:<i>C</i>是该层的通道数。式 (9) 提供了不同域之间特征分布的距离度量方法。</p>
                </div>
                <div class="p1">
                    <p id="86">由于MNIST和SVHN数据集均是由不同分布的数字构成, 因此研究采用SVHN-MNIST构建不同域特征。研究分别抽取两个数据集部分相同字符, 然后统计了11个ReLU层的特征散度。统计结果如图3所示。</p>
                </div>
                <div class="area_img" id="87">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201905025_087.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 特征分布" src="Detail/GetImg?filename=images/JYRJ201905025_087.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 特征分布  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201905025_087.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="88">从图3可以看出, 在IN-MobileNet中由外观差异引起的特征散度明显减少, 这种现象一直持续到未添加IN的深层。这也说明在深层特征中外观差异对特征提取的影响较小。</p>
                </div>
                <h3 id="89" name="89" class="anchor-tag"><b>3 实验设置</b></h3>
                <div class="p1">
                    <p id="90">在这一部分, 实验在公开数据集上对比了不同方法的实验性能, 并用标准的评价指标进行评测。</p>
                </div>
                <h4 class="anchor-tag" id="91" name="91"><b>3.1 数据集</b></h4>
                <div class="p1">
                    <p id="92">为了验证算法在不同场景下的鲁棒性, 实验分别在ICDAR2013数据集和ICDAR2015数据集的训练集和测试集上做模型训练和性能测试, 对比模型在不同数据集上的性能表现, 从而验证提出的IN模块在数据分布不一致时对于模型性能的影响。另外实验通过CCD相机采集了100张助航灯光标记牌图像, 以观察模型在目标场景下的表现。</p>
                </div>
                <h4 class="anchor-tag" id="93" name="93"><b>3.2 评价标准</b></h4>
                <div class="p1">
                    <p id="94">研究采用了标准的文字检测评价标准:准确率 (<i>P</i>) , 召回率 (<i>R</i>) 和<i>F</i>值 (<i>F</i>) 。它们的数学表达如下:</p>
                </div>
                <div class="p1">
                    <p id="95"><mathml id="96"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><mo>=</mo><mfrac><mrow><mi>Τ</mi><mi>Ρ</mi></mrow><mrow><mi>Τ</mi><mi>Ρ</mi><mo>+</mo><mi>F</mi><mi>Ρ</mi></mrow></mfrac></mrow></math></mathml>      (10) </p>
                </div>
                <div class="p1">
                    <p id="97"><mathml id="98"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>R</mi><mo>=</mo><mfrac><mrow><mi>Τ</mi><mi>Ρ</mi></mrow><mrow><mi>Τ</mi><mi>Ρ</mi><mo>+</mo><mi>F</mi><mi>Ν</mi></mrow></mfrac></mrow></math></mathml>      (11) </p>
                </div>
                <div class="p1">
                    <p id="99"><mathml id="100"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>F</mi><mo>=</mo><mfrac><mrow><mn>2</mn><mo>×</mo><mi>Ρ</mi><mo>×</mo><mi>R</mi></mrow><mrow><mi>Ρ</mi><mo>+</mo><mi>R</mi></mrow></mfrac></mrow></math></mathml>      (12) </p>
                </div>
                <div class="p1">
                    <p id="101">式中:<i>TP</i>代表真正率, <i>FP</i>代表假正率, <i>FN</i>代表假负率。对于文字检测, 如果检测框与ground truth交并比大于阈值 (一般设置0.5) 并且得分也大于相应阈值则被视为正例。F值是准确率和召回率的调和平均值。</p>
                </div>
                <h4 class="anchor-tag" id="102" name="102"><b>3.3 实验细节</b></h4>
                <div class="p1">
                    <p id="103">为了加速网络收敛, 实验预先在SynthText预训练并在真实数据集上做迁移学习。优化方法采用随机梯度下降, 动量设置为0.9, Batch Size为16。训练前重新调整图像大小, 设置最长边800, 长宽比与原图保持一致, 并在调整后的图像上做了随机裁剪 (裁剪比率0.7～1.0) 和随机偏转 (-15°～15°) 。在预训练阶段, 采用10<sup>-3</sup>学习率训练迭代10万次, 然后分别用10<sup>-4</sup>和10<sup>-5</sup>的学习率训练迭代5万次。</p>
                </div>
                <h4 class="anchor-tag" id="104" name="104"><b>3.4 实验结果</b></h4>
                <div class="p1">
                    <p id="105">为了验证IN对于不同基础网络不产生模型偏好, 实验一将IN引入不同模型, 并在ICDAR2013数据集上训练、测试, 实验结果见表1。</p>
                </div>
                <div class="area_img" id="106">
                    <p class="img_tit"><b>表1 IN在不同基础网络的性能对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="106" border="1"><tr><td> 模型</td><td>正确率<br />/%</td><td>召回率<br />/%</td><td>F值<br />/%</td></tr><tr><td><br />VGG</td><td>71.4</td><td>83.2</td><td>76.8</td></tr><tr><td><br />ResNet50</td><td>72.6</td><td>83.0</td><td>77.5</td></tr><tr><td><br />MobileNet_V1</td><td>75.8</td><td>86.2</td><td>80.7</td></tr><tr><td><br />VGG_MS_Align</td><td>72.2</td><td>84.1</td><td>77.9</td></tr><tr><td><br />ResNet50_ MS_Align</td><td>74.3</td><td>84.2</td><td>79.0</td></tr><tr><td><br />MobileNet_ V1_ MS_Align</td><td>78.0</td><td>88.1</td><td>82.8</td></tr><tr><td><br />VGG_MS_Align_IN (Ours) </td><td>74.1</td><td>85.2</td><td>79.2</td></tr><tr><td><br />ResNet50_MS_Align_IN (Ours) </td><td>75.5</td><td>87.7</td><td>81.1</td></tr><tr><td><br />MobileNet_V1_ MS_Align_ IN (Ours) </td><td>78.8</td><td>89.9</td><td>84.0</td></tr><tr><td colspan="4"><br /> 注:MS代表多尺度输入, IN代表用IN替换部分BN, Align代表用RoI_Align代替RoI_Pooling</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="107">为了研究IN对于模型泛化性能影响, 实验二、三分别在ICDAR2013和ICDAR2015数据集上训练和测试, 实验结果见表2、表3。</p>
                </div>
                <div class="area_img" id="108">
                    <p class="img_tit"><b>表2 IN在ICDAR2013数据集训练, ICDAR2013和2015测试集测试的性能对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="108" border="1"><tr><td rowspan="2"><br />模型</td><td colspan="3"><br />ICDAR2013/ICDAR2015</td></tr><tr><td><br />正确率<br />/%</td><td>召回率<br />/%</td><td>F值<br />/%</td></tr><tr><td><br />MobileNet_V1_MS_Align</td><td>78.0/<br />74.3</td><td>88.1/<br />84.3</td><td>82.8/<br />79.0</td></tr><tr><td><br />MobileNet_V1_MS_Align_IN (Ours) </td><td>78.8/<br />77.3</td><td>89.9/<br />88.5</td><td>84.0/<br />82.5</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="109">
                    <p class="img_tit"><b>表3 IN在ICDAR2015数据集训练, ICDAR2013和2015测试集测试的性能对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="109" border="1"><tr><td rowspan="2"><br />模型</td><td colspan="3"><br />ICDAR2013/ICDAR2015</td></tr><tr><td><br />正确率<br />/%</td><td>召回率<br />/%</td><td>F值<br />/%</td></tr><tr><td><br />MobileNet_V1_MS_Align</td><td>74.3/<br />77.9</td><td>85.8/<br />88.2</td><td>79.6/<br />82.7</td></tr><tr><td><br />MobileNet_V1_MS_Align_IN (Ours) </td><td>78.1/<br />79.6</td><td>88.0/<br />89.5</td><td>82.8/<br />84.2</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="110">在目标场景下可视化效果如图4所示。</p>
                </div>
                <div class="area_img" id="112">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201905025_11200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 可视化效果图" src="Detail/GetImg?filename=images/JYRJ201905025_11200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 可视化效果图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201905025_11200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="112">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201905025_11201.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 可视化效果图" src="Detail/GetImg?filename=images/JYRJ201905025_11201.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 可视化效果图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201905025_11201.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="113">实验结果表明, 针对于不同场景、不同时刻、不同光照强度的测试环境, 重新设计的网络结构可以有效得使得模型更关注于目标本身, 降低由外观差异而引起的特征散度, 提升模型在不同测试环境下的鲁棒性和泛化能力。与其他方法不同, 研究通过重新设计网络结构直接赋予网络域不变性的能力, 使得模型可以在较强的外界干扰条件下学习到更加鲁棒的特征, 从而提升算法性能。</p>
                </div>
                <h3 id="114" name="114" class="anchor-tag"><b>4 结 语</b></h3>
                <div class="p1">
                    <p id="115">研究提出了一个具有域不变性的文字检测模型, 通过引入IN使得模型可以提取到更鲁棒的特征。实验结果表明, 重新设计的网络结构降低了由外界干扰引起的特征散度, 提升了算法在不同域间的泛化能力, 针对于不同场景下标记牌文字表现出更稳定的检测性能, 为机场秩序维护、飞机起降安全等提供了保障。但同样需要注意的是, 提出的结构只能在一定程度减弱数据分布差异带来的影响。如何让网络在提取到更鲁棒特征的同时让特征分布更贴近于目标场景、在有限的数据集上提升模型的泛化能力是下一步的工作重点。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201803032&amp;v=MDUyOTdvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVadEZ5emtVNy9JTHpUWlpMRzRIOW5Nckk5R1o=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> 朱利娟, 云中华, 边巴旺堆.基于极坐标变换的脱机手写藏文字符特征提取方法[J].计算机应用与软件, 2018, 35 (3) :162-166.
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Rich feature hierarchies for accurate object detection and semantic segmentation">

                                <b>[2]</b> Girshick R, Donahue J, Darrell T, et al.Rich feature hierarchies for accurate object detection and semantic segmentation[C]// IEEE Conference on Computer Vision and Pattern Recognition, 2014:580-587.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=R-CNN">

                                <b>[3]</b> Girshick R.Fast R-CNN[C]// Proceedings of the 2015 IEEE International Conference on Computer Vision (ICCV) .IEEE, 2015:1440-1448.
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Faster R-CNN:towards real-time object detection with region proposal networks">

                                <b>[4]</b> Ren S, He K, Girshick R, et al.Faster R-CNN:Towards Real-Time Object Detection with Region Proposal Networks[J].IEEE Transactions on Pattern Analysis &amp; Machine Intelligence, 2017, 39 (6) :1137-1149.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=SSD:Single Shot MultiBox Detector">

                                <b>[5]</b> Liu W, Anguelov D, Erhan D, et al.SSD:Single Shot MultiBox Detector[C]// European Conference on Computer Vision, 2016:21-37.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=You only look once:Unified,real-time object detection">

                                <b>[6]</b> Redmon J, Divvala S, Girshick R, et al.You Only Look Once:Unified, Real-Time Object Detection[C]// IEEE Conference on Computer Vision and Pattern Recognition, 2015:779-788.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=PVANET:Deep but Lightweight Neural Networks for Real-time Object Detection">

                                <b>[7]</b> Kim K H, Hong S, Roh B, et al.PVANET:Deep but Lightweight Neural Networks for Real-time Object Detection[C]// Conference and Workshop on Neural Information Processing Systems, 2016.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=DetNet:A Backbone network for Object Detection">

                                <b>[8]</b> Zeming Li, Chao Peng, Gang Yu, et al.DetNet:A Backbone network for Object Detection[C]// European Conference on Computer Vision, 2018.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012201349782&amp;v=MDE5NTV0RmlubFVyeklKRjhVYWhJPU5pZk9mYks3SHRET3JZOUVaKzhHQzNRN29CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWg==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b> Matas J, Chum O, Urban M, et al.Robust wide-baseline stereo from maximally stable extremal regions[J].Image &amp; Vision Computing, 2004, 22 (10) :761-767.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Detecting text in natural scenes with stroke width transform">

                                <b>[10]</b> Epshtein B, Ofek E, Wexler Y.Detecting text in natural scenes with stroke width transform[C]// IEEE Conference on Computer Vision and Pattern Recognition, 2010:2963-2970.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=TextBoxes十十:A single-shot oriented scene text detector">

                                <b>[11]</b> Liao M, Shi B, Bai X.TextBoxes++:A Single-Shot Oriented Scene Text Detector[J].IEEE Transactions on Image Processing, 2018, 27 (8) :3676-3690.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Spatial Transformer Networks">

                                <b>[12]</b> Jaderberg M, Simonyan K, Zisserman A, et al.Spatial Transformer Networks[C]// Advances in Neural Information Processing Systems, 2015:2017-2025.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Cross-Domain Weakly-Supervised Object Detection through Progressive Domain Adaptation">

                                <b>[13]</b> Inoue N, Furuta R, Yamasaki T, et al.Cross-Domain Weakly-Supervised Object Detection through Progressive Domain Adaptation[C]// IEEE Conference on Computer Vision and Pattern Recognition, 2018.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Adversarial Feature Augmentation for Unsupervised Domain Adaptation">

                                <b>[14]</b> Volpi R, Morerio P, Savarese S, et al.Adversarial Feature Augmentation for Unsupervised Domain Adaptation[C]// IEEE Conference on Computer Vision and Pattern Recognition, 2018.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Driving in the Matrix: Can virtual worlds replace human-generated annotations for real world tasks?">

                                <b>[15]</b> Johnson-Roberson M, Barto C, Mehta R, et al.Driving in the Matrix:Can virtual worlds replace human-generated annotations for real world tasks?[C]// IEEE International Conference on Robotics and Automation.IEEE, 2017:746-753.
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Instance Normalization:The Missing Ingredient for Fast Stylization">

                                <b>[16]</b> Ulyanov D, Vedaldi A, Lempitsky V.Instance Normalization:The Missing Ingredient for Fast Stylization[C]// IEEE Conference on Computer Vision and Pattern Recognition, 2016.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JYRJ201905025" />
        <input id="dpi" type="hidden" value="600" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201905025&amp;v=MTE4MzJxbzlIWVlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVp0Rnl6a1U3L0pMelRaWkxHNEg5ak0=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
