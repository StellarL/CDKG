<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637135754377315000%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJYRJ201904036%26RESULT%3d1%26SIGN%3dQVQkanwX3QDse%252fGmprpncRltUxM%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JYRJ201904036&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JYRJ201904036&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201904036&amp;v=MTM2MTZWcjdOTHpUWlpMRzRIOWpNcTQ5R1lvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVadEZ5em0=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#51" data-title="&lt;b&gt;0 引 言&lt;/b&gt; "><b>0 引 言</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#62" data-title="&lt;b&gt;1 相关工作&lt;/b&gt; "><b>1 相关工作</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#63" data-title="&lt;b&gt;1.1 潜在子空间的学习&lt;/b&gt;"><b>1.1 潜在子空间的学习</b></a></li>
                                                <li><a href="#65" data-title="&lt;b&gt;1.2 哈希模型&lt;/b&gt;"><b>1.2 哈希模型</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#68" data-title="&lt;b&gt;2 算法设计&lt;/b&gt; "><b>2 算法设计</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#74" data-title="&lt;b&gt;2.1 学习有鉴别的哈希码&lt;/b&gt;"><b>2.1 学习有鉴别的哈希码</b></a></li>
                                                <li><a href="#90" data-title="&lt;b&gt;2.2 拉普拉斯项&lt;/b&gt;"><b>2.2 拉普拉斯项</b></a></li>
                                                <li><a href="#104" data-title="&lt;b&gt;2.3 目标函数&lt;/b&gt;"><b>2.3 目标函数</b></a></li>
                                                <li><a href="#109" data-title="&lt;b&gt;2.4 迭代优化&lt;/b&gt;"><b>2.4 迭代优化</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#148" data-title="&lt;b&gt;3 实 验&lt;/b&gt; "><b>3 实 验</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#150" data-title="&lt;b&gt;3.1 实验设置&lt;/b&gt;"><b>3.1 实验设置</b></a></li>
                                                <li><a href="#164" data-title="&lt;b&gt;3.2 结果及分析&lt;/b&gt;"><b>3.2 结果及分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#176" data-title="&lt;b&gt;4 结 语&lt;/b&gt; "><b>4 结 语</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#57" data-title="图1 算法流程">图1 算法流程</a></li>
                                                <li><a href="#166" data-title="&lt;b&gt;表1 Wiki数据集上的mAP值&lt;/b&gt;"><b>表1 Wiki数据集上的mAP值</b></a></li>
                                                <li><a href="#167" data-title="&lt;b&gt;表2 NUS_WIDE数据集上mAP值&lt;/b&gt;"><b>表2 NUS_WIDE数据集上mAP值</b></a></li>
                                                <li><a href="#174" data-title="图2 Wiki数据集在不同哈希码长度上的精度-召回曲线 (Img to Txt) ">图2 Wiki数据集在不同哈希码长度上的精度-召回曲线 (Img to Txt) </a></li>
                                                <li><a href="#175" data-title="图3 Wiki数据集在不同哈希码长度上的 精度-召回曲线 (Txt to img) ">图3 Wiki数据集在不同哈希码长度上的 精度-召回曲线 (Txt to img) </a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" title=" Liu L, Yu M, Shao L. Multiview alignment hashing for efficient image search[J]. IEEE Transactions on Image Processing, 2015, 24 (3) : 956-966." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multiview alignment hashing for efficient image search">
                                        <b>[1]</b>
                                         Liu L, Yu M, Shao L. Multiview alignment hashing for efficient image search[J]. IEEE Transactions on Image Processing, 2015, 24 (3) : 956-966.
                                    </a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" title=" Zhou J, Ding G, Guo Y. Latent semantic sparse hashing for cross-modal similarity search[C]//Proceedings of the 37th International ACM SIGIR Conference on Research &amp;amp; Development in Information Retrieval. ACM, 2014: 415-424." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Latent semantic sparse hashing for cross-modal similarity search">
                                        <b>[2]</b>
                                         Zhou J, Ding G, Guo Y. Latent semantic sparse hashing for cross-modal similarity search[C]//Proceedings of the 37th International ACM SIGIR Conference on Research &amp;amp; Development in Information Retrieval. ACM, 2014: 415-424.
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title=" Ding G, Guo Y, Zhou J. Collective matrix factorization hashing for multimodal data[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. IEEE, 2014: 2075-2082." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Collective matrix factorization hashing for multimodal data">
                                        <b>[3]</b>
                                         Ding G, Guo Y, Zhou J. Collective matrix factorization hashing for multimodal data[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. IEEE, 2014: 2075-2082.
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" title=" Bronstein M M, Bronstein A M, Michel F, et al. Data fusion through cross-modality metric learning using similarity-sensitive hashing[C]//Computer Vision and Pattern Recognition, 2010 IEEE Conference. IEEE, 2010: 3594-3601." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Data fusion through cross-modality metric learning using similarity-sensitive hashing">
                                        <b>[4]</b>
                                         Bronstein M M, Bronstein A M, Michel F, et al. Data fusion through cross-modality metric learning using similarity-sensitive hashing[C]//Computer Vision and Pattern Recognition, 2010 IEEE Conference. IEEE, 2010: 3594-3601.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" title=" Zhang D, Li W J. Large-Scale Supervised Multimodal Hashing with Semantic Correlation Maximization[C]//The Association for the Advancement of Artificial Intelligence. AAAI, 2014, 1 (2) : 7-18." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Large-Scale supervised multimodal hashing with semantic correlation maximization">
                                        <b>[5]</b>
                                         Zhang D, Li W J. Large-Scale Supervised Multimodal Hashing with Semantic Correlation Maximization[C]//The Association for the Advancement of Artificial Intelligence. AAAI, 2014, 1 (2) : 7-18.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" title=" Zhen Y, Yeung D Y. Co-regularized hashing for multimodal data[C]//Advances in neural information processing systems. NIPS, 2012: 1376-1384." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Co-regularized hashing for multimodal data">
                                        <b>[6]</b>
                                         Zhen Y, Yeung D Y. Co-regularized hashing for multimodal data[C]//Advances in neural information processing systems. NIPS, 2012: 1376-1384.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" title=" Irie G, Arai H, Taniguchi Y. Alternating co-quantization for cross-modal hashing[C]//Proceedings of the IEEE International Conference on Computer Vision. IEEE, 2015: 1886-1894." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Alternating Co-Quantization for Cross-Modal Hashing">
                                        <b>[7]</b>
                                         Irie G, Arai H, Taniguchi Y. Alternating co-quantization for cross-modal hashing[C]//Proceedings of the IEEE International Conference on Computer Vision. IEEE, 2015: 1886-1894.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" title=" Xu X, Shen F, Yang Y, et al. Learning discriminative binary codes for large-scale cross-modal retrieval[J]. IEEE Transactions on Image Processing, 2017, 26 (5) : 2494-2507." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning discriminative binary codes for large-scale cross-modal retrieval">
                                        <b>[8]</b>
                                         Xu X, Shen F, Yang Y, et al. Learning discriminative binary codes for large-scale cross-modal retrieval[J]. IEEE Transactions on Image Processing, 2017, 26 (5) : 2494-2507.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" title=" Rasiwasia N, Costa Pereira J, Coviello E, et al. A new approach to cross-modal multimedia retrieval[C]//Proceedings of the 18th ACM international conference on Multimedia. ACM, 2010: 251-260." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A New Approach to Cross-modal Multimedia Retrieval">
                                        <b>[9]</b>
                                         Rasiwasia N, Costa Pereira J, Coviello E, et al. A new approach to cross-modal multimedia retrieval[C]//Proceedings of the 18th ACM international conference on Multimedia. ACM, 2010: 251-260.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_10" title=" Luo Y, Liu T, Tao D, Xu C. Multiview matrix completion for multilabel image classification[J]. IEEE Transactions on Image Processing, 2015, 24 (8) : 2355-2368." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multiview matrix completion for multilabel image classification">
                                        <b>[10]</b>
                                         Luo Y, Liu T, Tao D, Xu C. Multiview matrix completion for multilabel image classification[J]. IEEE Transactions on Image Processing, 2015, 24 (8) : 2355-2368.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_11" title=" Mandal D, Biswas S. Generalized coupled dictionary learning approach with applications to cross-modal matching[J]. IEEE Transactions on Image Processing, 2016, 25 (8) : 3826-3837." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Generalized coupled dictionary learning approach with applications to cross-modal matching">
                                        <b>[11]</b>
                                         Mandal D, Biswas S. Generalized coupled dictionary learning approach with applications to cross-modal matching[J]. IEEE Transactions on Image Processing, 2016, 25 (8) : 3826-3837.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_12" title=" Sharma A, Kumar A, Daume H, et al. Generalized multiview analysis: A discriminative latent space[C]//2012 IEEE Conference on Computer Vision and Pattern Recognition. IEEE, 2012: 2160-2167." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Generalized Multiview analysis:A discriminative latent space">
                                        <b>[12]</b>
                                         Sharma A, Kumar A, Daume H, et al. Generalized multiview analysis: A discriminative latent space[C]//2012 IEEE Conference on Computer Vision and Pattern Recognition. IEEE, 2012: 2160-2167.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_13" title=" Gong Y, Ke Q, Isard M, et al. A multi-view embedding space for modeling internet images, tags and their semantics[J].International journal of computer vision, 2014, 106 (2) : 210-233." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD14030300028340&amp;v=MjAzOTJKRjBSYXhZPU5qN0Jhcks4SHRMTXJJOUZaT2tIRDNnNW9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaW5sVXJ6SQ==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[13]</b>
                                         Gong Y, Ke Q, Isard M, et al. A multi-view embedding space for modeling internet images, tags and their semantics[J].International journal of computer vision, 2014, 106 (2) : 210-233.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_14" title=" Andrew G, Arora R, Bilmes J, et al. Deep canonical correlation analysis[C]//International Conference on Machine Learning. ICML, 2013: 1247-1255." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep canonical cor-relation analysis">
                                        <b>[14]</b>
                                         Andrew G, Arora R, Bilmes J, et al. Deep canonical correlation analysis[C]//International Conference on Machine Learning. ICML, 2013: 1247-1255.
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_15" title=" Ngiam J, Khosla A, Kim M, et al. Multimodal deep learning[C]//Proceedings of the 28th international conference on machine learning. ICML, 2011: 689-696." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multimodal Deep Learning">
                                        <b>[15]</b>
                                         Ngiam J, Khosla A, Kim M, et al. Multimodal deep learning[C]//Proceedings of the 28th international conference on machine learning. ICML, 2011: 689-696.
                                    </a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_16" title=" Srivastava N, Salakhutdinov R R. Multimodal learning with deep boltzmann machines[C]//Advances in neural information processing systems. NIPS, 2012: 2222-2230." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multimodal learning with deep boltzmann machines">
                                        <b>[16]</b>
                                         Srivastava N, Salakhutdinov R R. Multimodal learning with deep boltzmann machines[C]//Advances in neural information processing systems. NIPS, 2012: 2222-2230.
                                    </a>
                                </li>
                                <li id="35">


                                    <a id="bibliography_17" title=" Liu T, Tao D. Classification with noisy labels by importance reweighting[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2016, 38 (3) : 447-461." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Classification with Noisy Labels by Importance Reweighting">
                                        <b>[17]</b>
                                         Liu T, Tao D. Classification with noisy labels by importance reweighting[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2016, 38 (3) : 447-461.
                                    </a>
                                </li>
                                <li id="37">


                                    <a id="bibliography_18" title=" Jiang R, Qiao H, Zhang B. Speeding up graph regularized sparse coding by dual gradient ascent[J]. IEEE signal processing letters, 2015, 22 (3) : 313-317." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Speeding Up Graph Regularized Sparse Coding by Dual Gradient Ascent">
                                        <b>[18]</b>
                                         Jiang R, Qiao H, Zhang B. Speeding up graph regularized sparse coding by dual gradient ascent[J]. IEEE signal processing letters, 2015, 22 (3) : 313-317.
                                    </a>
                                </li>
                                <li id="39">


                                    <a id="bibliography_19" title=" Xu C, Liu T, Tao D. Local rademacher complexity for multi-label learning[J]. IEEE Transactions on Image Processing, 2016, 25 (3) : 1495-1507." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Local rademacher complexity for multi-label learning">
                                        <b>[19]</b>
                                         Xu C, Liu T, Tao D. Local rademacher complexity for multi-label learning[J]. IEEE Transactions on Image Processing, 2016, 25 (3) : 1495-1507.
                                    </a>
                                </li>
                                <li id="41">


                                    <a id="bibliography_20" title=" Wang J, Kumar, Chang S F. Semi-supervised hashing for scalable image retrieval[C]//IEEE Conference on Computer Vision &amp;amp; Pattern Recognition. DBLP, 2010:1654-1661." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Semi-supervised hashing for scalable image retrieval">
                                        <b>[20]</b>
                                         Wang J, Kumar, Chang S F. Semi-supervised hashing for scalable image retrieval[C]//IEEE Conference on Computer Vision &amp;amp; Pattern Recognition. DBLP, 2010:1654-1661.
                                    </a>
                                </li>
                                <li id="43">


                                    <a id="bibliography_21" title=" Kumar S, Udupa R. Learning Hash Functions for Cross-View Similarity Search[C]//IJCAI 2011, Proceedings of the 22nd International Joint Conference on Artificial Intelligence, Barcelona, Catalonia, Spain, July 16-22, 2011. AAAI Press, 2011." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning hash functions for cross-view similarity search">
                                        <b>[21]</b>
                                         Kumar S, Udupa R. Learning Hash Functions for Cross-View Similarity Search[C]//IJCAI 2011, Proceedings of the 22nd International Joint Conference on Artificial Intelligence, Barcelona, Catalonia, Spain, July 16-22, 2011. AAAI Press, 2011.
                                    </a>
                                </li>
                                <li id="45">


                                    <a id="bibliography_22" title=" Zhen Y, Yeung D Y. A probabilistic model for multimodal hash function learning[C]//Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2012: 940-948." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A probabilistic model for multimodal hash function learning">
                                        <b>[22]</b>
                                         Zhen Y, Yeung D Y. A probabilistic model for multimodal hash function learning[C]//Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2012: 940-948.
                                    </a>
                                </li>
                                <li id="47">


                                    <a id="bibliography_23" title=" Gong Y, Lazebnik S. Iterative quantization: A procrustean approach to learning binary codes[J]. IEEE Transactions on Multimedia, 2016, 16 (2) : 427-439." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Iterative quantization A procrustean approach to learning binary codes">
                                        <b>[23]</b>
                                         Gong Y, Lazebnik S. Iterative quantization: A procrustean approach to learning binary codes[J]. IEEE Transactions on Multimedia, 2016, 16 (2) : 427-439.
                                    </a>
                                </li>
                                <li id="49">


                                    <a id="bibliography_24" title=" Tang J, Wang K, Shao L. Supervised matrix factorization hashing for cross-modal retrieval[J]. IEEE Transactions on Image Processing, 2016, 25 (7) : 3157-3166." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Supervised matrix factorization hashing for cross-modal retrieval">
                                        <b>[24]</b>
                                         Tang J, Wang K, Shao L. Supervised matrix factorization hashing for cross-modal retrieval[J]. IEEE Transactions on Image Processing, 2016, 25 (7) : 3157-3166.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JYRJ" target="_blank">计算机应用与软件</a>
                2019,36(04),223-229 DOI:10.3969/j.issn.1000-386x.2019.04.035            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>有监督鉴别哈希跨模态检索</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9C%B1%E6%B2%BB%E5%85%B0&amp;code=41483480&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">朱治兰</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%8D%86%E6%99%93%E8%BF%9C&amp;code=11342648&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">荆晓远</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%91%A3%E8%A5%BF%E4%BC%9F&amp;code=33949425&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">董西伟</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%90%B4%E9%A3%9E&amp;code=31225403&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">吴飞</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%8D%97%E4%BA%AC%E9%82%AE%E7%94%B5%E5%A4%A7%E5%AD%A6%E8%87%AA%E5%8A%A8%E5%8C%96%E5%AD%A6%E9%99%A2&amp;code=0101257&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">南京邮电大学自动化学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B9%9D%E6%B1%9F%E5%AD%A6%E9%99%A2%E4%BF%A1%E6%81%AF%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF%E5%AD%A6%E9%99%A2&amp;code=0149573&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">九江学院信息科学与技术学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>随着大数据时代的到来, 利用哈希方法实现对异质多模态数据的快速跨模态检索受到越来越多的关注。为了获取更好的跨模态检索性能, 提出有监督鉴别跨模态哈希算法。利用对象的标签信息对所要生成的哈希码进行约束。算法中的线性分类项和图拉普拉斯算子项分别用于提升哈希码鉴别能力和保留模态间相似性。对算法的目标函数利用迭代法进行求解。该算法在两个基准数据集的实验结果展现出优于目前最前沿的跨模态哈希检索方法。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%B7%A8%E6%A8%A1%E6%80%81%E6%A3%80%E7%B4%A2&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">跨模态检索;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%93%88%E5%B8%8C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">哈希;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%A0%87%E7%AD%BE%E4%BF%A1%E6%81%AF&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">标签信息;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB%E9%A1%B9&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">线性分类项;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%9B%BE%E6%8B%89%E6%99%AE%E6%8B%89%E6%96%AF%E9%A1%B9&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">图拉普拉斯项;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    朱治兰, 硕士生, 主研领域:模式识别, 跨模态检索。;
                                </span>
                                <span>
                                    荆晓远, 教授。;
                                </span>
                                <span>
                                    董西伟, 讲师。;
                                </span>
                                <span>
                                    吴飞, 讲师。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-10-29</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金项目 (61702280);</span>
                                <span>江苏省研究生创新工程项目 (CXLX11_0418);</span>
                    </p>
            </div>
                    <h1><b>SUPERVISED DISCRIMINATIVE CROSS-MODAL HASHING</b></h1>
                    <h2>
                    <span>Zhu Zhilan</span>
                    <span>Jing Xiaoyuan</span>
                    <span>Dong Xiwei</span>
                    <span>Wu Fei</span>
            </h2>
                    <h2>
                    <span>College of Automation, Nanjing University of Posts and Telecommunications</span>
                    <span>School of Information Science and Technology, Jiujiang University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>With the advent of the era of big data, applying Hash methods for heterogeneous multimodal data to achieve fast cross-modal retrieval is receiving more and more attention. In order to obtain better cross-modal retrieval performance, we proposed supervised discriminative cross-modal hashing (SDCH) . We used label information of objects to constrain the hash code to be generated. The linear classification term and the graph Laplacian term in the algorithm were used to improve the discriminating ability of hash codes and preserve inter-modal similarity, respectively. The objective function of the algorithm was solved in an iterative method. Related experiments have been conducted in two benchmark databases. Experimental results show that the algorithm is superior to state-of-the-art cross-modal Hash methods.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Cross-modal%20retrieval&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Cross-modal retrieval;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Hash&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Hash;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Label%20information&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Label information;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Linear%20classification&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Linear classification;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Graph%20Laplacian%20term&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Graph Laplacian term;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-10-29</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="51" name="51" class="anchor-tag"><b>0 引 言</b></h3>
                <div class="p1">
                    <p id="52">近几十年来, 互联网多媒体数据的爆炸性增长, 使得跨媒体数据检索需求增长, 并且促进了复杂多模态检索技术的发展。现如今, 多媒体数据往往来自不同的互联网多媒体平台以及不同的数据资源。这些数据经常共同出现且被用来描述同一物体和事件, 因此跨模态检索在实际应用中已经成为必要。例如, 人们经常利用图片来检索相关的文本文献, 或者用文本来检索相关的图片内容。但是, 由于多模态数据属于不同的特征空间, 这种异质的特征被认为是跨模态检索最大的挑战。</p>
                </div>
                <div class="p1">
                    <p id="53">为了消除不同模态特征之间的异质性, 最近有很多研究把关注点放在潜在子空间的学习上。研究的关键点在于如何通过学习得到一个共同的语义子空间, 使得不同模态数据之间的异质性得到消除, 进而使得这些特征在这个学习得到的语义子空间中能被直接相互匹配。然而, 由于忽视了特征维度的可伸缩性, 在解决大规模数据的多模态检索时, 这些方法受到了限制。此时出现了大量的哈希方法。之前大多数的哈希方法目的在于生成多模态数据的哈希码, 生成哈希码的方法可以分为两大类, 一类是依赖训练数据的, 另一类是独立于训练数据的。其中有一种不依赖训练数据的著名算法是局部敏感哈希算法<citation id="179" type="reference"><link href="3" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>, 它随机地选取线性投影矩阵作为哈希函数。相比独立于训练数据的哈希算法, 依赖训练数据的哈希算法提供了一种更加可靠的投影机制, 从而能够得到更加简洁以及有鉴别力度的哈希码。</p>
                </div>
                <div class="p1">
                    <p id="54">从有无利用标签信息的角度来看, 现有的跨模态哈希算法又被划分成有监督的哈希算法<citation id="180" type="reference"><link href="5" rel="bibliography" /><link href="7" rel="bibliography" /><sup>[<a class="sup">2</a>,<a class="sup">3</a>]</sup></citation>和无监督的哈希算法<citation id="181" type="reference"><link href="9" rel="bibliography" /><link href="11" rel="bibliography" /><link href="13" rel="bibliography" /><sup>[<a class="sup">4</a>,<a class="sup">5</a>,<a class="sup">6</a>]</sup></citation>。无监督的方法一般是利用训练数据之间的相关性学习一种能够将多模态特征转化为哈希码的投影机制, 而有监督的方法则是将训练数据的标签信息作为一种语义约束, 从而得到更加合适的哈希码。前人的实验表明有监督的哈希算法在跨模态检索中取得的效果通常比无监督的哈希算法取得的效果要好。</p>
                </div>
                <div class="p1">
                    <p id="55">对有监督跨模态哈希学习算法, 一些研究者对模态间和模态内的相似性保留问题进行了研究。而且, 大部分的有监督跨模态哈希具有相同的特性, 即通过标签信息保留模态间和模态内的相似性来学习哈希码。然而, 这一特性的缺点是削弱了学习到的哈希码的鉴别力度。第一, 保留模态间和模态内的相似性不能保证学习到的哈希码在语义上的鉴别力。第二, 计算给定标签下的成对相似性不可避免地会造成额外的存储和计算损耗。</p>
                </div>
                <div class="p1">
                    <p id="56">为了解决上述问题, 本文提出了一种新的跨模态哈希算法, 即:有监督鉴别跨模态哈希算法SDCH (Supervised Discriminative Cross-modal Hashing) 。与现有的跨模态哈希方法<citation id="182" type="reference"><link href="13" rel="bibliography" /><link href="15" rel="bibliography" /><sup>[<a class="sup">6</a>,<a class="sup">7</a>]</sup></citation>相比, SDCH不仅可以节省存储空间还可以减少在线检索时间。图1描述了整个工作流程。SDCH利用模态间的标签信息作为主要的约束条件, 来保留模态间数据的相似性, 结合线性分类器来学习得到多模态数据的统一的有鉴别力的哈希码。</p>
                </div>
                <div class="area_img" id="57">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201904036_057.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 算法流程" src="Detail/GetImg?filename=images/JYRJ201904036_057.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 算法流程  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201904036_057.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="58">本文的主要贡献总结为以下几个方面:</p>
                </div>
                <div class="p1">
                    <p id="59">1) 将模态间相似性的保留融合到分类器框架中, 从而学习到既能保证相似性又能体现鉴别力的统一哈希码。</p>
                </div>
                <div class="p1">
                    <p id="60">2) 仅使用模态间的标签信息作为主要的约束条件, 与传统的相似性保留方法相比减少了计算时间, 节省了存储消耗, 同时还能保留模间同标签数据之间的相似性。</p>
                </div>
                <div class="p1">
                    <p id="61">3) 本文在两个数据集上做了实验来评估SDCH方法, 实验结果显示SDCH较前沿方法具有一定的竞争性。</p>
                </div>
                <h3 id="62" name="62" class="anchor-tag"><b>1 相关工作</b></h3>
                <h4 class="anchor-tag" id="63" name="63"><b>1.1 潜在子空间的学习</b></h4>
                <div class="p1">
                    <p id="64">对于跨模态检索问题, 最近许多研究都把关注点放在潜在子空间学习上。其中, 典型相关性分析CCA (Canonical Correlation Analysis) 是目前最流行跨模态检索方法之一, 它主要学习一对能够使得两种模态数据投影到共同的潜在空间上的投影变换, 而投影后得到的数据能够直接匹配, 且能够最大程度地反映出两种模态数据之间的关系。CCA被广泛地研究并延伸出许多相关算法。例如, Rasiwasia等<citation id="183" type="reference"><link href="19" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>提出的在跨模态检索方法能够学习多模态数据的最大相关子空间。其他经典的算法还有双线性分离样式模型<citation id="184" type="reference"><link href="21" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>和广义耦合字典学习<citation id="185" type="reference"><link href="23" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>, 它们和CCA非常相似都是学习共同的潜在子空间来进行跨模态检索工作。除了以上经典的方法以外, 还有一些子空间学习方法利用分类标签信息来改善检索性能, 例如, Sharma等<citation id="186" type="reference"><link href="25" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>提出的一种普通的多模态分析算法, 利用标签信息学习有鉴别的潜在子空间;Gong等<citation id="187" type="reference"><link href="27" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>提出的多模态CCA框架, 基于标签的语义信息将图像和文本两种模态的数据联系在一起进行跨模态检索;再有深度跨模态模型, 比如深度CCA<citation id="188" type="reference"><link href="29" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>、多模态自编码<citation id="189" type="reference"><link href="31" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>以及多模态限制玻尔兹曼机<citation id="190" type="reference"><link href="33" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>等都是用来解决模态检索问题而提出的算法。这些算法的目的都是希望学习到能够更好地保留原始数据特征的子空间。</p>
                </div>
                <h4 class="anchor-tag" id="65" name="65"><b>1.2 哈希模型</b></h4>
                <div class="p1">
                    <p id="66">随着高维度跨模态数据的爆炸性增长, 最近邻搜索花费的代价越来越高。为了解决这个问题, 基于哈希的一系列算法被提出, 比如文献<citation id="195" type="reference">[<a class="sup">17</a>,<a class="sup">18</a>,<a class="sup">19</a>,<a class="sup">20</a>]</citation>, 在大规模多模态数据检索领域上得到了广泛的关注。文献<citation id="191" type="reference">[<a class="sup">4</a>]</citation>中提出的算法第一次将哈希思想运用到多模态检索问题上。不过这个算法的缺陷在于它只考虑到了模态内数据的相关性而忽略了模态间数据的相关性。为了解决这一问题, 文献<citation id="192" type="reference">[<a class="sup">21</a>]</citation>中通过最小化相似数据哈希码之间的的距离并最大化不相关数据哈希码之间的距离来生成跨模态检索的哈希码。更近一步地, Wu等<citation id="193" type="reference"><link href="45" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>提出了稀疏多模态哈希算法, 这一算法通过联合的多模态字典学习获取不同模态数据的稀疏哈希码从而解决跨模态检索问题。为了更好地利用多模态数据的标签信息, Tang等<citation id="194" type="reference"><link href="47" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>提出了有监督的矩阵分解哈希SMFH (Supervised Matrix Factorization Hashing for Cross-Modal Retrieval) , 该算法利用集体矩阵分解技术得到统一的哈希码, 并且结合不同模态数据的标签一致性以及模态内数据的局部几何一致性使得得到的哈希码具有更好的鉴别力。</p>
                </div>
                <div class="p1">
                    <p id="67">然而, 这种通过标签信息来保留哈希码相似性的有监督学习算法没能通过给定的标签信息挖掘分类信息, 从而使得学习得到的哈希码缺乏鉴别力。而且, 这种相似性的保留, 会占用更大的存储空间以及消耗更多的检索时间。为此, 本文提出了一种新的算法SDCH, 该算法利用相同对象不同模态间的数据具有相同标签信息这一特性来保留成对哈希码的相似性, 同时又利用标签信息构造能使哈希码所表示的数据被正确分类的线性分类器来保证哈希码的鉴别力。</p>
                </div>
                <h3 id="68" name="68" class="anchor-tag"><b>2 算法设计</b></h3>
                <div class="p1">
                    <p id="69">本节将描述所提算法SDCH的细节。为了简洁地阐述该算法, 以两种模态 (图像和文本) 哈希码的学习为例。不失一般性, 它可以延伸到多模态数据的学习上。</p>
                </div>
                <div class="p1">
                    <p id="70">首先, 假设本文有两种模态的训练数据<b><i>V</i></b>={<b><i>v</i></b><sub>1</sub>, <b><i>v</i></b><sub>2</sub>, …, <b><i>v</i></b><sub><i>n</i></sub>}和<b><i>T</i></b>={<b><i>t</i></b><sub>1</sub>, <b><i>t</i></b><sub>2</sub>, …, <b><i>t</i></b><sub><i>n</i></sub>}, 它们分别是同一对象的两种表示模态, 这里的<i>n</i>为训练样本的个数。每个对象都由对应的图像和文本共同组成。对于第<i>i</i>个对象, <b><i>v</i></b><sub><i>i</i></sub>∈<b><i>R</i></b><sup><i>d</i><sub>1</sub></sup>是<i>d</i><sub>1</sub>维度的图像特征向量, <b><i>t</i></b><sub><i>i</i></sub>∈<b><i>R</i></b><sup><i>d</i><sub>2</sub></sup>是<i>d</i><sub>2</sub>维度的文本特征向量 (在大部分情况下<i>d</i><sub>1</sub>≠<i>d</i><sub>2</sub>) 。不失一般性, 本文对跨模态数据进行中心化处理, 即令<mathml id="71"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mi>i</mi><mi>n</mi></munderover><mi mathvariant="bold-italic">v</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mn>0</mn></mrow></math></mathml>以及<mathml id="72"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mi>i</mi><mi>n</mi></munderover><mi mathvariant="bold-italic">t</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mn>0</mn></mrow></math></mathml>。</p>
                </div>
                <div class="p1">
                    <p id="73">算法SDCH的目的是分别为图像和文本找到能够使得特征向量转化为统一的哈希码的哈希函数, 即<i>f</i> (<i>v</i>) :<i>R</i><sup><i>d</i><sub>1</sub></sup>→{-1, 1}<sup><i>k</i></sup>以及<i>g</i> (<i>t</i>) :<i>R</i><sup><i>d</i><sub>2</sub></sup>→{-1, 1}<sup><i>k</i></sup>, 这里的<i>k</i>指代的是哈希码的长度。为了得到有意义的哈希码, 本文借鉴文献<citation id="196" type="reference">[<a class="sup">3</a>]</citation>中使用的方法将异质数据投影到同一汉明空间中, 同时假设模态间同标签数据具有相同哈希码。这样图像数据和文本数据就能在被投影到汉明空间的同时保留原始数据的语义信息。</p>
                </div>
                <h4 class="anchor-tag" id="74" name="74"><b>2.1 学习有鉴别的哈希码</b></h4>
                <div class="p1">
                    <p id="75">本文考虑来自两种模态的特征<b><i>v</i></b><sub><i>i</i></sub>和<b><i>t</i></b><sub><i>i</i></sub>具有相同的哈希码表示<i>s</i><sub><i>i</i></sub>。本文希望这里的<i>s</i><sub><i>i</i></sub>能够消除来自两种模态数据原始特征的语义鸿沟。正如图1所描述的, 原始特征被投影到一个共同的汉明空间中。</p>
                </div>
                <div class="p1">
                    <p id="76">因此对于跨模态数据, 本文分别通过两种线性变换投影原始图像和文本特征到汉明空间:</p>
                </div>
                <div class="p1">
                    <p id="77"><b><i>S</i></b><sub><i>V</i></sub>=<b><i>P</i></b><sub><i>V</i></sub><b><i>V</i></b>      (1) </p>
                </div>
                <div class="p1">
                    <p id="78"><b><i>S</i></b><sub><i>T</i></sub>=<b><i>P</i></b><sub><i>T</i></sub><b><i>T</i></b>      (2) </p>
                </div>
                <div class="p1">
                    <p id="79">式中:<b><i>P</i></b><sub><i>V</i></sub>∈<b><i>R</i></b><sup><i>k</i>×<i>d</i><sub>1</sub></sup>, <b><i>P</i></b><sub><i>T</i></sub>∈<b><i>R</i></b><sup><i>k</i>×<i>d</i><sub>2</sub></sup>。</p>
                </div>
                <div class="p1">
                    <p id="80">基于同对象不同模态的数据具有相同语义表示这一假设, 本文通过最小化以下函数来求解两个线性变换矩阵:</p>
                </div>
                <div class="p1">
                    <p id="81" class="code-formula">
                        <mathml id="81"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>Ο</mi><msub><mrow></mrow><mrow><mi>l</mi><mi>p</mi></mrow></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">Ρ</mi><msub><mrow></mrow><mi>V</mi></msub><mo>, </mo><mi mathvariant="bold-italic">Ρ</mi><msub><mrow></mrow><mi>Τ</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">S</mi><mo>-</mo><mi mathvariant="bold-italic">S</mi><msub><mrow></mrow><mi>V</mi></msub></mrow><mo>|</mo></mrow><msubsup><mrow></mrow><mi>F</mi><mn>2</mn></msubsup><mo>+</mo><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">S</mi><mo>-</mo><mi mathvariant="bold-italic">S</mi><msub><mrow></mrow><mi>Τ</mi></msub></mrow><mo>|</mo></mrow><msubsup><mrow></mrow><mi>F</mi><mn>2</mn></msubsup><mo>=</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">S</mi><mo>-</mo><mi mathvariant="bold-italic">Ρ</mi><msub><mrow></mrow><mi>V</mi></msub><mi mathvariant="bold-italic">V</mi></mrow><mo>|</mo></mrow><msubsup><mrow></mrow><mi>F</mi><mn>2</mn></msubsup><mo>+</mo><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">S</mi><mo>-</mo><mi mathvariant="bold-italic">Ρ</mi><msub><mrow></mrow><mi>Τ</mi></msub><mi mathvariant="bold-italic">Τ</mi></mrow><mo>|</mo></mrow><msubsup><mrow></mrow><mi>F</mi><mn>2</mn></msubsup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="82">式中:<mathml id="83"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>|</mo><mo>⋅</mo><mo>|</mo></mrow><msub><mrow></mrow><mi>F</mi></msub></mrow></math></mathml>表示矩阵的Frobenius范数。</p>
                </div>
                <div class="p1">
                    <p id="84">此外, 原始多模态数据特征还可以反映分类信息, 为了让本文得到的哈希码也能够反映这一特性, 那么得到的哈希码就能够通过它们的原始标签被分类<citation id="197" type="reference"><link href="17" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>。因此假设给定第<i>i</i>个目标的标签向量<b><i>y</i></b><sub><i>i</i></sub>, 本文就可以用线性分类器<b><i>W</i>∈<i>R</i></b><sup><i>k</i>×<i>c</i></sup>来预测哈希码的标签向量, 即:</p>
                </div>
                <div class="p1">
                    <p id="85"><b><i>Y</i>=<i>W</i></b><sup>T</sup><i>S</i>      (4) </p>
                </div>
                <div class="p1">
                    <p id="86">线性分类器<b><i>W</i></b>可以通过最小化以下函数来求解:</p>
                </div>
                <div class="p1">
                    <p id="87"><mathml id="88"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ο</mi><msub><mrow></mrow><mrow><mi>m</mi><mi>f</mi></mrow></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">W</mi><mo>, </mo><mi mathvariant="bold-italic">S</mi><mo stretchy="false">) </mo><mo>=</mo><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">Y</mi><mo>-</mo><mi mathvariant="bold-italic">W</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">S</mi></mrow><mo>|</mo></mrow><msubsup><mrow></mrow><mi>F</mi><mn>2</mn></msubsup></mrow></math></mathml>      (5) </p>
                </div>
                <div class="p1">
                    <p id="89">式中:<b><i>Y</i>∈<i>R</i></b><sup><i>c</i>×<i>n</i></sup>表示<i>n</i>个标签向量组成的矩阵。</p>
                </div>
                <h4 class="anchor-tag" id="90" name="90"><b>2.2 拉普拉斯项</b></h4>
                <div class="p1">
                    <p id="91">为了利用标签信息, 本文为双模态数据之间的标签一致性建模, 并且将图像和文本两种模态数据之间的语义类同度量为:</p>
                </div>
                <div class="area_img" id="92">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JYRJ201904036_09200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="94">为了保留两种模态数据在汉明空间中的标签一致性, 本文可以最小化以下函数:</p>
                </div>
                <div class="p1">
                    <p id="95"><mathml id="96"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ο</mi><msub><mrow></mrow><mi>c</mi></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">S</mi><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>, </mo><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>c</mi></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mrow><mo>|</mo><mrow><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mi>s</mi><msub><mrow></mrow><mi>j</mi></msub></mrow><mo>|</mo></mrow><msup><mrow></mrow><mn>2</mn></msup></mrow></math></mathml>      (7) </p>
                </div>
                <div class="p1">
                    <p id="97">式中:<mathml id="98"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>|</mo><mo>⋅</mo><mo>|</mo></mrow></mrow></math></mathml>为二范数。</p>
                </div>
                <div class="p1">
                    <p id="99">通过一系列的代数运算, 可以将式 (7) 重新生成为:</p>
                </div>
                <div class="p1">
                    <p id="100" class="code-formula">
                        <mathml id="100"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ο</mi><msub><mrow></mrow><mi>c</mi></msub><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">S</mi><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>, </mo><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>c</mi></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mrow><mo>|</mo><mrow><mi>s</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mi>s</mi><msub><mrow></mrow><mi>j</mi></msub></mrow><mo>|</mo></mrow><msup><mrow></mrow><mn>2</mn></msup><mo>=</mo><mn>2</mn><mi>t</mi><mi>r</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">S</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">D</mi><mo>-</mo><mi mathvariant="bold-italic">C</mi><mo stretchy="false">) </mo><mi mathvariant="bold-italic">S</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mo stretchy="false">) </mo><mo>=</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="101">2<i>tr</i> (<b><i>SLS</i></b><sup>T</sup>)      (8) </p>
                </div>
                <div class="p1">
                    <p id="102">式中:<b><i>C</i>∈<i>R</i></b><sup><i>n</i>×<i>n</i></sup>为相似性矩阵;<b><i>D</i></b>∈<i>R</i><sup><i>n</i>×<i>n</i></sup>是对角矩阵, 其中对角线元素为<b><i>C</i></b>矩阵列之和, 即:<mathml id="103"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>D</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>i</mi></mrow></msub><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>j</mi></munder><mi>C</mi></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow></math></mathml>。因此<b><i>L</i>=<i>D</i>-<i>C</i></b>为拉普拉斯矩阵的形式。</p>
                </div>
                <h4 class="anchor-tag" id="104" name="104"><b>2.3 目标函数</b></h4>
                <div class="p1">
                    <p id="105">完整的目标函数由以下几个部分共同组成, 分别是式 (5) 有鉴别项<i>O</i><sub><i>mf</i></sub>、式 (3) 线性变换项<i>O</i><sub><i>lp</i></sub>、式 (8) 拉普拉斯项<i>O</i><sub><i>c</i></sub>以及Frobenius范数项, 具体如下式所示:</p>
                </div>
                <div class="p1">
                    <p id="106" class="code-formula">
                        <mathml id="106"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mrow><mi mathvariant="bold-italic">W</mi><mo>, </mo><mi mathvariant="bold-italic">Ρ</mi><msub><mrow></mrow><mi>V</mi></msub><mo>, </mo><mi mathvariant="bold-italic">Ρ</mi><msub><mrow></mrow><mi>Τ</mi></msub><mo>, </mo><mi mathvariant="bold-italic">S</mi></mrow></munder><mi>Ο</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">W</mi><mo>, </mo><mi mathvariant="bold-italic">Ρ</mi><msub><mrow></mrow><mi>V</mi></msub><mo>, </mo><mi mathvariant="bold-italic">Ρ</mi><msub><mrow></mrow><mi>Τ</mi></msub><mo>, </mo><mi mathvariant="bold-italic">S</mi><mo stretchy="false">) </mo><mo>=</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mi>Ο</mi><msub><mrow></mrow><mrow><mi>m</mi><mi>f</mi></mrow></msub><mo>+</mo><mi>Ο</mi><msub><mrow></mrow><mrow><mi>l</mi><mi>p</mi></mrow></msub><mo>+</mo><mi>Ο</mi><msub><mrow></mrow><mi>c</mi></msub><mo>+</mo><mi>λ</mi><mrow><mo>|</mo><mi mathvariant="bold-italic">W</mi><mo>|</mo></mrow><msubsup><mrow></mrow><mi>F</mi><mn>2</mn></msubsup><mo>=</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">Y</mi><mo>-</mo><mi mathvariant="bold-italic">W</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">S</mi></mrow><mo>|</mo></mrow><msubsup><mrow></mrow><mi>F</mi><mn>2</mn></msubsup><mo>+</mo><mi>μ</mi><msub><mrow></mrow><mi>V</mi></msub><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">S</mi><mo>-</mo><mi mathvariant="bold-italic">Ρ</mi><msub><mrow></mrow><mi>V</mi></msub><mi mathvariant="bold-italic">V</mi></mrow><mo>|</mo></mrow><msubsup><mrow></mrow><mi>F</mi><mn>2</mn></msubsup><mo>+</mo><mi>μ</mi><msub><mrow></mrow><mi>Τ</mi></msub><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">S</mi><mo>-</mo><mi mathvariant="bold-italic">Ρ</mi><msub><mrow></mrow><mtext>Τ</mtext></msub><mi mathvariant="bold-italic">Τ</mi></mrow><mo>|</mo></mrow><msubsup><mrow></mrow><mi>F</mi><mn>2</mn></msubsup><mo>+</mo></mtd></mtr><mtr><mtd><mi>γ</mi><mi>t</mi><mi>r</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">S</mi><mi mathvariant="bold-italic">L</mi><mi mathvariant="bold-italic">S</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mo stretchy="false">) </mo><mo>+</mo><mi>λ</mi><mrow><mo>|</mo><mi mathvariant="bold-italic">W</mi><mo>|</mo></mrow><msubsup><mrow></mrow><mi>F</mi><mn>2</mn></msubsup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>9</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="107">这里的正则项<mathml id="108"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>|</mo><mo>⋅</mo><mo>|</mo></mrow><msubsup><mrow></mrow><mi>F</mi><mn>2</mn></msubsup></mrow></math></mathml>起到防止过拟合的作用。<i>λ</i>=0.01为正则系数。<i>μ</i><sub><i>V</i></sub>、<i>μ</i><sub><i>T</i></sub>为惩罚因子, 其值设置为10<sup>-5</sup>。<i>γ</i>=1为折衷参数。</p>
                </div>
                <h4 class="anchor-tag" id="109" name="109"><b>2.4 迭代优化</b></h4>
                <div class="p1">
                    <p id="110">式 (9) 所示的最优化问题是拥有四个矩阵型变量的非凸函数, 所以解决他是具有一定困难的。不过, 当固定其他三个变量只求解其中一个变量的情况下, 它又是凸的。因此, 关于这一最优化问题的求解就可以被总结成如下三步迭代算法:</p>
                </div>
                <div class="p1">
                    <p id="111">第一步:固定<b><i>S</i></b>和<b><i>W</i></b>求<b><i>P</i></b><sub><i>V</i></sub>和<b><i>P</i></b><sub><i>T</i></sub>。当固定<b><i>S</i></b>和<b><i>W</i></b>后, 式 (9) 转化为如下的优化问题:</p>
                </div>
                <div class="p1">
                    <p id="112" class="code-formula">
                        <mathml id="112"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mrow><mi mathvariant="bold-italic">Ρ</mi><msub><mrow></mrow><mi>V</mi></msub><mo>, </mo><mi mathvariant="bold-italic">Ρ</mi><msub><mrow></mrow><mi>Τ</mi></msub></mrow></munder><mi>Ο</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">Ρ</mi><msub><mrow></mrow><mi>V</mi></msub><mo>, </mo><mi mathvariant="bold-italic">Ρ</mi><msub><mrow></mrow><mi>Τ</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mi>μ</mi><msub><mrow></mrow><mi>V</mi></msub><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">S</mi><mo>-</mo><mi mathvariant="bold-italic">Ρ</mi><msub><mrow></mrow><mi>V</mi></msub><mi mathvariant="bold-italic">V</mi></mrow><mo>|</mo></mrow><msubsup><mrow></mrow><mi>F</mi><mn>2</mn></msubsup><mo>+</mo><mi>μ</mi><msub><mrow></mrow><mi>Τ</mi></msub><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">S</mi><mo>-</mo><mi mathvariant="bold-italic">Ρ</mi><msub><mrow></mrow><mi>Τ</mi></msub><mi mathvariant="bold-italic">Τ</mi></mrow><mo>|</mo></mrow><msubsup><mrow></mrow><mi>F</mi><mn>2</mn></msubsup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>0</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="113">令<mathml id="114"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mo>∂</mo><mi>Ο</mi></mrow><mrow><mo>∂</mo><mi mathvariant="bold-italic">Ρ</mi><msub><mrow></mrow><mi>V</mi></msub></mrow></mfrac><mo>=</mo><mn>0</mn><mo>, </mo><mfrac><mrow><mo>∂</mo><mi>Ο</mi></mrow><mrow><mo>∂</mo><mi mathvariant="bold-italic">Ρ</mi><msub><mrow></mrow><mi>Τ</mi></msub></mrow></mfrac><mo>=</mo><mn>0</mn></mrow></math></mathml>, 进一步推导可得:</p>
                </div>
                <div class="p1">
                    <p id="115"><mathml id="116"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">Ρ</mi><msub><mrow></mrow><mi>V</mi></msub><mo>=</mo><mi mathvariant="bold-italic">S</mi><mi mathvariant="bold-italic">V</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mrow><mo> (</mo><mrow><mi mathvariant="bold-italic">V</mi><mi mathvariant="bold-italic">V</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mo>+</mo><mfrac><mi mathvariant="bold-italic">Ι</mi><mrow><mi>μ</mi><msub><mrow></mrow><mi>V</mi></msub></mrow></mfrac></mrow><mo>) </mo></mrow><msup><mrow></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup></mrow></math></mathml>      (11) </p>
                </div>
                <div class="p1">
                    <p id="117"><mathml id="118"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">Ρ</mi><msub><mrow></mrow><mi>Τ</mi></msub><mo>=</mo><mi mathvariant="bold-italic">S</mi><mi mathvariant="bold-italic">Τ</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mrow><mo> (</mo><mrow><mi mathvariant="bold-italic">Τ</mi><mi mathvariant="bold-italic">Τ</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mo>+</mo><mfrac><mi mathvariant="bold-italic">Ι</mi><mrow><mi>μ</mi><msub><mrow></mrow><mi>Τ</mi></msub></mrow></mfrac></mrow><mo>) </mo></mrow><msup><mrow></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup></mrow></math></mathml>      (12) </p>
                </div>
                <div class="p1">
                    <p id="119">式中:<b><i>I</i></b>表示单位矩阵, (·) <sup>-1</sup>表示所求矩阵的逆运算。</p>
                </div>
                <div class="p1">
                    <p id="120">第二步:固定<b><i>P</i></b><sub><i>V</i></sub>和<b><i>P</i></b><sub><i>T</i></sub>以及<b><i>S</i></b>求<b><i>W</i></b>。当固定<b><i>P</i></b><sub><i>V</i></sub>和<b><i>P</i></b><sub><i>T</i></sub>以及<b><i>S</i></b>后, 式 (9) 转化为如下的优化问题:</p>
                </div>
                <div class="p1">
                    <p id="121"><mathml id="122"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mi mathvariant="bold-italic">W</mi></munder><mspace width="0.25em" /><mi>μ</mi><msub><mrow></mrow><mi>V</mi></msub><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">Y</mi><mo>-</mo><mi mathvariant="bold-italic">W</mi><msup><mrow></mrow><mtext>Τ</mtext></msup></mrow><mo>|</mo></mrow><msubsup><mrow></mrow><mi>F</mi><mn>2</mn></msubsup><mo>+</mo><mi>λ</mi><mrow><mo>|</mo><mi mathvariant="bold-italic">W</mi><mo>|</mo></mrow><msubsup><mrow></mrow><mi>F</mi><mn>2</mn></msubsup></mrow></math></mathml>      (13) </p>
                </div>
                <div class="p1">
                    <p id="123">令<mathml id="124"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mo>∂</mo><mi>Ο</mi></mrow><mrow><mo>∂</mo><mi mathvariant="bold-italic">W</mi></mrow></mfrac><mo>=</mo><mn>0</mn></mrow></math></mathml>, 进一步推导可得:</p>
                </div>
                <div class="p1">
                    <p id="125"><b><i>W</i>= (<i>SS</i></b><sup>T</sup>+<i>λ</i><b><i>I</i></b>) <sup>-1</sup><b><i>SY</i></b><sup>T</sup>      (14) </p>
                </div>
                <div class="p1">
                    <p id="126">第三步:固定<b><i>P</i></b><sub><i>V</i></sub>和<b><i>P</i></b><sub><i>T</i></sub>以及<b><i>W</i></b>求<b><i>S</i></b>。当固定<b><i>P</i></b><sub><i>V</i></sub>和<b><i>P</i></b><sub><i>T</i></sub>以及<b><i>W</i></b>后, 式 (9) 转化为如下的优化问题:</p>
                </div>
                <div class="p1">
                    <p id="127" class="code-formula">
                        <mathml id="127"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mi mathvariant="bold-italic">S</mi></munder><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">Y</mi><mo>-</mo><mi mathvariant="bold-italic">W</mi><msup><mrow></mrow><mtext>Τ</mtext></msup></mrow><mo>|</mo></mrow><msubsup><mrow></mrow><mi>F</mi><mn>2</mn></msubsup><mo>+</mo><mi>μ</mi><msub><mrow></mrow><mi>V</mi></msub><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">S</mi><mo>-</mo><mi mathvariant="bold-italic">Ρ</mi><msub><mrow></mrow><mi>V</mi></msub></mrow><mo>|</mo></mrow><msubsup><mrow></mrow><mi>F</mi><mn>2</mn></msubsup><mo>+</mo><mi>μ</mi><msub><mrow></mrow><mi>Τ</mi></msub><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">S</mi><mo>-</mo><mi mathvariant="bold-italic">Ρ</mi><msub><mrow></mrow><mi>Τ</mi></msub></mrow><mo>|</mo></mrow><msubsup><mrow></mrow><mi>F</mi><mn>2</mn></msubsup><mo>+</mo><mi>γ</mi><mi>t</mi><mi>r</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">S</mi><mi mathvariant="bold-italic">L</mi><mi mathvariant="bold-italic">S</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>5</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="128">令<mathml id="129"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mo>∂</mo><mi>Ο</mi></mrow><mrow><mo>∂</mo><mi mathvariant="bold-italic">S</mi></mrow></mfrac><mo>=</mo><mn>0</mn></mrow></math></mathml>整理得:</p>
                </div>
                <div class="p1">
                    <p id="130"><b><i>AS</i>+<i>SB</i>+<i>E</i></b>=0      (16) </p>
                </div>
                <div class="p1">
                    <p id="131">式中:</p>
                </div>
                <div class="p1">
                    <p id="132"><b><i>A</i></b>=2 (<b><i>WW</i></b><sup>T</sup>+ (<i>μ</i><sub><i>V</i></sub>+<i>μ</i><sub>T</sub>) <b><i>I</i></b>) </p>
                </div>
                <div class="p1">
                    <p id="133"><b><i>B</i></b>=<b><i>L</i>+<i>L</i></b><sup>T</sup></p>
                </div>
                <div class="p1">
                    <p id="134"><b><i>E</i></b>=-2 (<b><i>WT</i></b>+<i>μ</i><sub><i>V</i></sub><b><i>P</i></b><sub><i>V</i></sub><b><i>V</i></b>+<i>μ</i><sub><i>T</i></sub><b><i>P</i></b><sub><i>T</i></sub><b><i>T</i></b>) </p>
                </div>
                <div class="p1">
                    <p id="135">值得注意的是式 (16) 是希尔维斯特方程, 可以用MATLAB中的李雅普诺夫函数求解。</p>
                </div>
                <div class="p1">
                    <p id="136">SDCH算法可以被概括成算法1。当一个新的检索条目<i>x</i><sub><i>q</i></sub>被输入, SDCH首先会利用式 (1) 或者式 (2) 生成语义表示<i>s</i><sub><i>q</i></sub>然后本文会根据符号函数<i>H</i><sup><i>x</i><sub><i>q</i></sub></sup>=<i>sign</i> (<i>s</i><sub><i>q</i></sub>) 生成想要的哈希码。</p>
                </div>
                <div class="p1">
                    <p id="137"><b>算法1</b> 有监督鉴别哈希跨模态检索</p>
                </div>
                <div class="p1">
                    <p id="138">输入:图片特征矩阵<b><i>V</i></b>, 文本特征矩阵<b><i>T</i></b>, 两种模态的语义标签<i>Y</i>, 系数<i>λ</i>, <i>μ</i><sub><i>V</i></sub>, <i>μ</i><sub><i>T</i></sub>, <i>γ</i>, 以及哈希码长度<i>k</i>。</p>
                </div>
                <div class="p1">
                    <p id="139">输出:哈希码<i>H</i>, 投影矩阵<b><i>P</i></b><sub><i>V</i></sub>, <b><i>P</i></b><sub><i>T</i></sub>。</p>
                </div>
                <div class="p1">
                    <p id="140">1:中心化<b><i>V</i>, <i>T</i></b>, 根据式 (7) 和式 (8) 构造拉氏矩阵</p>
                </div>
                <div class="p1">
                    <p id="141">2:随机初始化<b><i>P</i></b><sub><i>V</i></sub>, <b><i>P</i></b><sub><i>T</i></sub>, <b><i>S</i>, <i>W</i></b>。</p>
                </div>
                <div class="p1">
                    <p id="142">3:重复步骤4-6</p>
                </div>
                <div class="p1">
                    <p id="143">4:固定<b><i>S</i>、<i>W</i></b>, 根据式 (11) 和式 (12) 更新<b><i>P</i></b><sub><i>V</i></sub>、<b><i>P</i></b><sub><i>T</i></sub>。</p>
                </div>
                <div class="p1">
                    <p id="144">5:固定<b><i>P</i></b><sub><i>V</i></sub>、<b><i>P</i></b><sub><i>T</i></sub>、<b><i>S</i></b>, 根据式 (14) 更新<b><i>W</i></b>。</p>
                </div>
                <div class="p1">
                    <p id="145">6:固定<b><i>P</i></b><sub><i>V</i></sub>、<b><i>P</i></b><sub><i>T</i></sub>、<b><i>W</i></b>, 根据式 (16) 更新<b><i>S</i></b>。</p>
                </div>
                <div class="p1">
                    <p id="146">7:直到收敛</p>
                </div>
                <div class="p1">
                    <p id="147">8:<i>H</i>=<i>sign</i> (<b><i>S</i></b>) </p>
                </div>
                <h3 id="148" name="148" class="anchor-tag"><b>3 实 验</b></h3>
                <div class="p1">
                    <p id="149">本文将在Wiki, NUS_WIDE两个数据库上进行实验, 而后将实验结果和最前沿方法生成的结果进行对比。</p>
                </div>
                <h4 class="anchor-tag" id="150" name="150"><b>3.1 实验设置</b></h4>
                <h4 class="anchor-tag" id="151" name="151"><b>3.1.1</b> 数据集</h4>
                <div class="p1">
                    <p id="152">Wiki:它是从维基百科中搜集来的2 866个图像-文本对的集合。其中图像是由128维度的SIFT特征表示的, 而文本则是由10维度的主题向量特征构成的。本文所用的Wiki数据集包含了10个语义分类, 并且随机抽取其中的2 173个数据对作为训练集, 将剩余的693个数据对作为测试集。</p>
                </div>
                <div class="p1">
                    <p id="153">NUS_WIDE:本文实验中所用到的NUS_WIDE数据集是从Flickr中下载到的网页图像集合。原始数据集是包含了81个主题的且都有标注信息的269 648幅图像数据的集合。这样每幅图像和它对应的标注信息就构成了一个图像-文本对。本文从中挑选包含186 577幅前十类的图片作为实验数据。其中, 图像数据是由500维度的视觉词袋SIFT直方图表示的, 基于top-1000标注信息生成文本的词袋特征向量表示。对于所挑选的数据集, 本文从中随机地挑选5 000图像文本对作为训练集, 然后在剩余数据中再随机挑选1 866图像文本对作为测试集。</p>
                </div>
                <h4 class="anchor-tag" id="154" name="154"><b>3.1.2</b> 对比算法</h4>
                <div class="p1">
                    <p id="155">本文将SDCH算法与五个最前沿的算法典型相关性分析CCA (Canonical Correlation Analysis) <citation id="198" type="reference"><link href="47" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>、集体矩阵分解哈希CMFH (Collective Matrix Factorization Hashing) <citation id="199" type="reference"><link href="7" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>、交叉视图哈希模型CVH (Cross-View Hashing Model) <citation id="200" type="reference"><link href="43" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>、潜在语义稀疏哈希LSSH (Latent Semantic Sparse Hashing) <citation id="201" type="reference"><link href="5" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>和有监督的矩阵分解哈希SMFH (Supervised Matrix Factorization Hashing) <citation id="202" type="reference"><link href="49" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>作对比。</p>
                </div>
                <div class="p1">
                    <p id="156">其中, CCA算法将双模态数据投影到一个能使数据之间的相关性最大化的同一空间中;CMFH通过集体矩阵分解发现不同模态数据的潜在因子模型, 从而学习统一的哈希码;CVH通过解决广义特征值问题最小化数据对的加权平均多视图的l2范式距离;LSSH假设同一对象不同模态数据之间具有完全相同的哈希码, 并首次将稀释编码和矩阵分解结合到一起来分别学习文本和图像的语义特征, 为了缩小语义鸿沟继而将其投影到抽象空间中;SMFH利用矩阵分解技术的同时, 考虑不同模态数据之间的标签一致性以及同模态数据之间的局部几何结构的一致性。</p>
                </div>
                <h4 class="anchor-tag" id="157" name="157"><b>3.1.3</b> 评估度量</h4>
                <div class="p1">
                    <p id="158">本文将进行两种跨模态检索任务, 一种是“Img to Txt”, 即利用图像来搜索相关的文本。另一种是“Txt to Img”, 即利用文本来搜索相关的图片。为了评估跨模态检索的性能, 本文引入平均精度均值mAP (mean Average Precision) 这一度量标准:</p>
                </div>
                <div class="p1">
                    <p id="159" class="code-formula">
                        <mathml id="159"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>m</mi><mi>A</mi><mi>Ρ</mi><mo>=</mo><mfrac><mn>1</mn><mi>Ν</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mtext>A</mtext></mstyle><mtext>Ρ</mtext><mo stretchy="false"> (</mo><mi>q</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="160">式中:<i>q</i><sub><i>i</i></sub>是一条检索输入且<i>N</i>是检索条目输入总数。平均精度AP (Average Precision) 的计算公式如下:</p>
                </div>
                <div class="p1">
                    <p id="161" class="code-formula">
                        <mathml id="161"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>A</mi><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>q</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mn>1</mn><mi>Τ</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>r</mi><mo>=</mo><mn>1</mn></mrow><mi>R</mi></munderover><mi>Ρ</mi></mstyle><msub><mrow></mrow><mi>q</mi></msub><mo stretchy="false"> (</mo><mi>r</mi><mo stretchy="false">) </mo><mi>ξ</mi><mo stretchy="false"> (</mo><mi>r</mi><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="162">式中:<i>T</i>是检索集中所有的相关实体的个数, <i>P</i><sub><i>q</i></sub> (<i>r</i>) 是按照相关度排名后的前<i>r</i>个实体的精度, <i>ξ</i> (<i>r</i>) 是一个指示函数, 当第<i>r</i>个被检索到的实体与检索内容相关则其值为1否则为0。</p>
                </div>
                <div class="p1">
                    <p id="163">本文还学习了Wiki数据集上的表现曲线, 即精度-召回曲线 (PR-Curves) 。精度-召回曲线是精度值关于召回值的函数, 它被广泛地运用到跨模态检索性能的评估上。因为评估数据是随机选取的, 所以本文取10次实验的平均值作为最后的结果。</p>
                </div>
                <h4 class="anchor-tag" id="164" name="164"><b>3.2 结果及分析</b></h4>
                <div class="p1">
                    <p id="165">表1和表2展示了本文提出的SDCH算法和五个对比算法的mAP值。通过观察表1和表2可以看出, 与对比算法相比, 本文所提出的SDCH算法在不同哈希码长度下都具有较好的mAP值。这说明本文提出的SDCH算法能够挖掘到更多的鉴别信息来提升跨模态检索性能, 这得益于标签信息的利用保留了跨模态数据之间的相似性也得益于线性分类器思想的应用提高了哈希码的鉴别力。通过观察表1和表2还可以看到, 在哈希码比较短的16位时, SDCH算法相较于SMFH算法在mAP值上也有很大的优势, 这进一步表明了SDCH算法在实质上作了改善。此外, 结果还表明, 随着哈希码长度的增加, SDCH的性能就越好。对比SDCH算法的“Img to Txt”和“Txt to Img”两个任务还发现“Txt to Img”任务的检索效果总是优于“Img to Txt”任务的检索效果, 且对于数据尺度较大的NUS-WIDE数据集而言“Txt to Img”任务的检索效果的得到了显著的提升。</p>
                </div>
                <div class="area_img" id="166">
                    <p class="img_tit"><b>表1 Wiki数据集上的mAP值</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="166" border="1"><tr><td rowspan="2"><br />任务</td><td rowspan="2">方法</td><td colspan="4"><br />哈希码长度</td></tr><tr><td><br />16位</td><td>32位</td><td>64位</td><td>128位</td></tr><tr><td rowspan="6"><br />Img to Txt</td><td><br />CCA</td><td>0.171 4</td><td>0.154 7</td><td>0.146 5</td><td>0.128 6</td></tr><tr><td><br />CMFH</td><td>0.211 6</td><td>0.226 0</td><td>0.239 4</td><td>0.241 4</td></tr><tr><td><br />CVH</td><td>0.171 8</td><td>0.157 3</td><td>0.150 8</td><td>0.135 1</td></tr><tr><td><br />LSSH</td><td>0.216 2</td><td>0.224 8</td><td>0.225 5</td><td>0.219 0</td></tr><tr><td><br />SMFH</td><td>0.269 9</td><td>0.283 5</td><td>0.292 0</td><td>0.298 1</td></tr><tr><td><br />SDCH</td><td>0.298 8</td><td>0.324 1</td><td>0.350 9</td><td>0.359 9</td></tr><tr><td rowspan="6"><br />Txt to Img</td><td><br />CCA</td><td>0.159 0</td><td>0.141 5</td><td>0.130 0</td><td>0.115 1</td></tr><tr><td><br />CMFH</td><td>0.483 0</td><td>0.523 5</td><td>0.536 1</td><td>0.531 8</td></tr><tr><td><br />CVH</td><td>0.150 5</td><td>0.134 7</td><td>0.121 4</td><td>0.113 6</td></tr><tr><td><br />LSSH</td><td>0.501 0</td><td>0.524 6</td><td>0.532 4</td><td>0.538 7</td></tr><tr><td><br />SMFH</td><td>0.605 1</td><td>0.625 7</td><td>0.635 7</td><td>0.642 8</td></tr><tr><td><br />SDCH</td><td>0.696 4</td><td>0.715 5</td><td>0.721 2</td><td>0.728 8</td></tr><tr><td colspan="6"><br /></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="167">
                    <p class="img_tit"><b>表2 NUS_WIDE数据集上mAP值</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="167" border="1"><tr><td rowspan="2"><br />任务</td><td rowspan="2">方法</td><td colspan="4"><br />哈希码长度</td></tr><tr><td><br />16位</td><td>32位</td><td>64位</td><td>128位</td></tr><tr><td rowspan="6"><br />Img to Txt</td><td><br />CCA</td><td>0.360 6</td><td>0.353 8</td><td>0.349 9</td><td>0.347 2</td></tr><tr><td><br />CMFH</td><td>0.372 8</td><td>0.379 3</td><td>0.378 8</td><td>0.377 1</td></tr><tr><td><br />CVH</td><td>0.373 8</td><td>0.362 7</td><td>0.355 5</td><td>0.350 6</td></tr><tr><td><br />LSSH</td><td>0.383 8</td><td>0.383 7</td><td>0.390 3</td><td>0.386 4</td></tr><tr><td><br />SMFH</td><td>0.455 3</td><td>0.462 3</td><td>0.465 8</td><td>0.468 0</td></tr><tr><td><br />SDCH</td><td>0.538 9</td><td>0.575 4</td><td>0.575 3</td><td>0.578 9</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="168"><b>续表2</b></p>
                </div>
                <div class="area_img" id="169">
                    <p class="img_tit"> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="169" border="1"><tr><td rowspan="2"><br />任务</td><td rowspan="2">方法</td><td colspan="4"><br />哈希码长度</td></tr><tr><td><br />16位</td><td>32位</td><td>64位</td><td>128位</td></tr><tr><td rowspan="6"><br />Txt to Img</td><td><br />CCA</td><td>0.359 8</td><td>0.355 3</td><td>0.350 2</td><td>0.348 2</td></tr><tr><td><br />CMFH</td><td>0.373 4</td><td>0.378 5</td><td>0.382 3</td><td>0.381 9</td></tr><tr><td><br />CVH</td><td>0.375 7</td><td>0.363 6</td><td>0.356 7</td><td>0.356 2</td></tr><tr><td><br />LSSH</td><td>0.415 0</td><td>0.411 4</td><td>0.416 5</td><td>0.413 2</td></tr><tr><td><br />SMFH</td><td>0.503 3</td><td>0.505 6</td><td>0.506 5</td><td>0.507 9</td></tr><tr><td><br />SDCH</td><td>0.700 6</td><td>0.708 7</td><td>0.724 3</td><td>0.691 4</td></tr><tr><td colspan="6"><br /></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="170">图2和图3显示了SDCH算法和五个对比算法的精度-召回曲线。通过观察发现SDCH算法表现优于其他的对比算法, 这与用mAP对算法性能进行评价的结果一致。通过观察还发现, 对于CCA和CVH算法而言, 精度-召回值更像是随机出现的, 所以对于这两个算法分析它的精度-召回曲线几乎是没有意义的。</p>
                </div>
                <div class="p1">
                    <p id="171">最后, 通过观察还可以看到, SDCH在16位哈希码的条件下进行检索时的效果甚至超过了对比算法在更长的哈希码下检索的效果, 这也就充分体现出了本文所提算法在性能上的优越性。</p>
                </div>
                <div class="area_img" id="174">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201904036_17400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 Wiki数据集在不同哈希码长度上的精度-召回曲线 (Img to Txt)" src="Detail/GetImg?filename=images/JYRJ201904036_17400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 Wiki数据集在不同哈希码长度上的精度-召回曲线 (Img to Txt)   <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201904036_17400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="175">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201904036_175.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 Wiki数据集在不同哈希码长度上的 精度-召回曲线 (Txt to img)" src="Detail/GetImg?filename=images/JYRJ201904036_175.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 Wiki数据集在不同哈希码长度上的 精度-召回曲线 (Txt to img)   <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201904036_175.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h3 id="176" name="176" class="anchor-tag"><b>4 结 语</b></h3>
                <div class="p1">
                    <p id="177">本文提出了一种新的跨模态检索方法, 即有监督鉴别跨模态哈希算法。为了得到有鉴别力的哈希码, 本文将哈希码的学习嵌入到线性分类器的框架中, 其中线性分类器部分公式的形成利用了标签信息的监督原理。此外为了不损坏模态间数据的相似性, 本文利用模态间数据的标签一致性作为相似性的约束。</p>
                </div>
                <div class="p1">
                    <p id="178">本文在两个常用的数据集Wiki和NUS_WIDE上进行了实验来验证本文所提算法的有效性。本文将SDCH方法的实验结果和几种最前沿跨模态哈希检索算法的实验结果进行了对比和分析评估, 结果显示所提算法SDCH能够取得更好的跨模态检索性能。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multiview alignment hashing for efficient image search">

                                <b>[1]</b> Liu L, Yu M, Shao L. Multiview alignment hashing for efficient image search[J]. IEEE Transactions on Image Processing, 2015, 24 (3) : 956-966.
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Latent semantic sparse hashing for cross-modal similarity search">

                                <b>[2]</b> Zhou J, Ding G, Guo Y. Latent semantic sparse hashing for cross-modal similarity search[C]//Proceedings of the 37th International ACM SIGIR Conference on Research &amp; Development in Information Retrieval. ACM, 2014: 415-424.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Collective matrix factorization hashing for multimodal data">

                                <b>[3]</b> Ding G, Guo Y, Zhou J. Collective matrix factorization hashing for multimodal data[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. IEEE, 2014: 2075-2082.
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Data fusion through cross-modality metric learning using similarity-sensitive hashing">

                                <b>[4]</b> Bronstein M M, Bronstein A M, Michel F, et al. Data fusion through cross-modality metric learning using similarity-sensitive hashing[C]//Computer Vision and Pattern Recognition, 2010 IEEE Conference. IEEE, 2010: 3594-3601.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Large-Scale supervised multimodal hashing with semantic correlation maximization">

                                <b>[5]</b> Zhang D, Li W J. Large-Scale Supervised Multimodal Hashing with Semantic Correlation Maximization[C]//The Association for the Advancement of Artificial Intelligence. AAAI, 2014, 1 (2) : 7-18.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Co-regularized hashing for multimodal data">

                                <b>[6]</b> Zhen Y, Yeung D Y. Co-regularized hashing for multimodal data[C]//Advances in neural information processing systems. NIPS, 2012: 1376-1384.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Alternating Co-Quantization for Cross-Modal Hashing">

                                <b>[7]</b> Irie G, Arai H, Taniguchi Y. Alternating co-quantization for cross-modal hashing[C]//Proceedings of the IEEE International Conference on Computer Vision. IEEE, 2015: 1886-1894.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning discriminative binary codes for large-scale cross-modal retrieval">

                                <b>[8]</b> Xu X, Shen F, Yang Y, et al. Learning discriminative binary codes for large-scale cross-modal retrieval[J]. IEEE Transactions on Image Processing, 2017, 26 (5) : 2494-2507.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A New Approach to Cross-modal Multimedia Retrieval">

                                <b>[9]</b> Rasiwasia N, Costa Pereira J, Coviello E, et al. A new approach to cross-modal multimedia retrieval[C]//Proceedings of the 18th ACM international conference on Multimedia. ACM, 2010: 251-260.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multiview matrix completion for multilabel image classification">

                                <b>[10]</b> Luo Y, Liu T, Tao D, Xu C. Multiview matrix completion for multilabel image classification[J]. IEEE Transactions on Image Processing, 2015, 24 (8) : 2355-2368.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Generalized coupled dictionary learning approach with applications to cross-modal matching">

                                <b>[11]</b> Mandal D, Biswas S. Generalized coupled dictionary learning approach with applications to cross-modal matching[J]. IEEE Transactions on Image Processing, 2016, 25 (8) : 3826-3837.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Generalized Multiview analysis:A discriminative latent space">

                                <b>[12]</b> Sharma A, Kumar A, Daume H, et al. Generalized multiview analysis: A discriminative latent space[C]//2012 IEEE Conference on Computer Vision and Pattern Recognition. IEEE, 2012: 2160-2167.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_13" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD14030300028340&amp;v=MTk1NzZKRjBSYXhZPU5qN0Jhcks4SHRMTXJJOUZaT2tIRDNnNW9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaW5sVXJ6SQ==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[13]</b> Gong Y, Ke Q, Isard M, et al. A multi-view embedding space for modeling internet images, tags and their semantics[J].International journal of computer vision, 2014, 106 (2) : 210-233.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep canonical cor-relation analysis">

                                <b>[14]</b> Andrew G, Arora R, Bilmes J, et al. Deep canonical correlation analysis[C]//International Conference on Machine Learning. ICML, 2013: 1247-1255.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multimodal Deep Learning">

                                <b>[15]</b> Ngiam J, Khosla A, Kim M, et al. Multimodal deep learning[C]//Proceedings of the 28th international conference on machine learning. ICML, 2011: 689-696.
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multimodal learning with deep boltzmann machines">

                                <b>[16]</b> Srivastava N, Salakhutdinov R R. Multimodal learning with deep boltzmann machines[C]//Advances in neural information processing systems. NIPS, 2012: 2222-2230.
                            </a>
                        </p>
                        <p id="35">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Classification with Noisy Labels by Importance Reweighting">

                                <b>[17]</b> Liu T, Tao D. Classification with noisy labels by importance reweighting[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2016, 38 (3) : 447-461.
                            </a>
                        </p>
                        <p id="37">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Speeding Up Graph Regularized Sparse Coding by Dual Gradient Ascent">

                                <b>[18]</b> Jiang R, Qiao H, Zhang B. Speeding up graph regularized sparse coding by dual gradient ascent[J]. IEEE signal processing letters, 2015, 22 (3) : 313-317.
                            </a>
                        </p>
                        <p id="39">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Local rademacher complexity for multi-label learning">

                                <b>[19]</b> Xu C, Liu T, Tao D. Local rademacher complexity for multi-label learning[J]. IEEE Transactions on Image Processing, 2016, 25 (3) : 1495-1507.
                            </a>
                        </p>
                        <p id="41">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Semi-supervised hashing for scalable image retrieval">

                                <b>[20]</b> Wang J, Kumar, Chang S F. Semi-supervised hashing for scalable image retrieval[C]//IEEE Conference on Computer Vision &amp; Pattern Recognition. DBLP, 2010:1654-1661.
                            </a>
                        </p>
                        <p id="43">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning hash functions for cross-view similarity search">

                                <b>[21]</b> Kumar S, Udupa R. Learning Hash Functions for Cross-View Similarity Search[C]//IJCAI 2011, Proceedings of the 22nd International Joint Conference on Artificial Intelligence, Barcelona, Catalonia, Spain, July 16-22, 2011. AAAI Press, 2011.
                            </a>
                        </p>
                        <p id="45">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A probabilistic model for multimodal hash function learning">

                                <b>[22]</b> Zhen Y, Yeung D Y. A probabilistic model for multimodal hash function learning[C]//Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2012: 940-948.
                            </a>
                        </p>
                        <p id="47">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Iterative quantization A procrustean approach to learning binary codes">

                                <b>[23]</b> Gong Y, Lazebnik S. Iterative quantization: A procrustean approach to learning binary codes[J]. IEEE Transactions on Multimedia, 2016, 16 (2) : 427-439.
                            </a>
                        </p>
                        <p id="49">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Supervised matrix factorization hashing for cross-modal retrieval">

                                <b>[24]</b> Tang J, Wang K, Shao L. Supervised matrix factorization hashing for cross-modal retrieval[J]. IEEE Transactions on Image Processing, 2016, 25 (7) : 3157-3166.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JYRJ201904036" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201904036&amp;v=MTM2MTZWcjdOTHpUWlpMRzRIOWpNcTQ5R1lvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVadEZ5em0=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
