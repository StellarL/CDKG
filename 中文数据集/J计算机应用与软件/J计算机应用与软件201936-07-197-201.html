<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637135626261565000%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJYRJ201907034%26RESULT%3d1%26SIGN%3dcUK6YThIaCMaLNWpBAPc5cDkVig%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JYRJ201907034&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JYRJ201907034&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201907034&amp;v=MDUwMDlqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVadEZ5amhVcnpQTHpUWlpMRzRIOWpNcUk5R1lJUUtESDg0dlI0VDY=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#25" data-title="&lt;b&gt;0 引 言&lt;/b&gt; "><b>0 引 言</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#29" data-title="&lt;b&gt;1 卷积神经网络&lt;/b&gt; "><b>1 卷积神经网络</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#58" data-title="&lt;b&gt;2 多特征融合密集残差CNN&lt;/b&gt; "><b>2 多特征融合密集残差CNN</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#73" data-title="&lt;b&gt;3 实 验&lt;/b&gt; "><b>3 实 验</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#75" data-title="&lt;b&gt;3.1 数据集&lt;/b&gt;"><b>3.1 数据集</b></a></li>
                                                <li><a href="#81" data-title="&lt;b&gt;3.2 实验结果分析&lt;/b&gt;"><b>3.2 实验结果分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#89" data-title="&lt;b&gt;4 结 语&lt;/b&gt; "><b>4 结 语</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#63" data-title="图1 CNN模型图">图1 CNN模型图</a></li>
                                                <li><a href="#64" data-title="图2 DenseBlock (&lt;i&gt;i&lt;/i&gt;) (&lt;i&gt;i&lt;/i&gt;=1, 2, 3) ">图2 DenseBlock (<i>i</i>) (<i>i</i>=1, 2, 3) </a></li>
                                                <li><a href="#77" data-title="图3 Ck+表情库7种表情实例">图3 Ck+表情库7种表情实例</a></li>
                                                <li><a href="#80" data-title="图4 FER2013表情库7种表情实例">图4 FER2013表情库7种表情实例</a></li>
                                                <li><a href="#84" data-title="&lt;b&gt;表1 CK+不同种类表情的分类正确率&lt;/b&gt; %"><b>表1 CK+不同种类表情的分类正确率</b> %</a></li>
                                                <li><a href="#85" data-title="图5 卷积神经网络预测Fer2013混淆矩阵">图5 卷积神经网络预测Fer2013混淆矩阵</a></li>
                                                <li><a href="#86" data-title="图6 训练过程中的损失函数">图6 训练过程中的损失函数</a></li>
                                                <li><a href="#88" data-title="&lt;b&gt;表2 不同算法的识别率对比&lt;/b&gt;"><b>表2 不同算法的识别率对比</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" title=" Mollahosseini A, Chan D, Mahoor M H.Going deeper in facial expression recognition using deep neural networks[C]//2016 IEEE Winter Conference on Applications of Computer Vision (WACV) .Lake Placid, NY, USA:IEEE, 2016:1-10." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Going deeper in facial expression recognition using deep neural networks.&amp;quot;">
                                        <b>[1]</b>
                                         Mollahosseini A, Chan D, Mahoor M H.Going deeper in facial expression recognition using deep neural networks[C]//2016 IEEE Winter Conference on Applications of Computer Vision (WACV) .Lake Placid, NY, USA:IEEE, 2016:1-10.
                                    </a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" title=" Xie S, Hu H.Facial expression recognition with FRR-CNN[J].Electronics Letters, 2017, 53 (4) :235-237." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Facial Expression Recognition with FRR-CNN">
                                        <b>[2]</b>
                                         Xie S, Hu H.Facial expression recognition with FRR-CNN[J].Electronics Letters, 2017, 53 (4) :235-237.
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title=" Pramerdorfer C, Kampel M.Facial expression recognition using convolutional neural networks:state of the art[EB].arXiv preprint arXiv, 1612.02903, 2016." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Facial expression recognition using convolutional neural networks:state of the art[EB]">
                                        <b>[3]</b>
                                         Pramerdorfer C, Kampel M.Facial expression recognition using convolutional neural networks:state of the art[EB].arXiv preprint arXiv, 1612.02903, 2016.
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" title=" Lu P, Li B, Shama S, et al.Regularizing the loss layer of CNNs for facial expression recognition using crowd sourced labels[C]//2017 21st Asia Pacific Symposium on Intelligent and Evolutionary Systems (IES) .Hanoi, Vietnam:IEEE, 2017:31-36." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Regularizing the loss layer of CNNs for facial expression recognition using crowd sourced labels">
                                        <b>[4]</b>
                                         Lu P, Li B, Shama S, et al.Regularizing the loss layer of CNNs for facial expression recognition using crowd sourced labels[C]//2017 21st Asia Pacific Symposium on Intelligent and Evolutionary Systems (IES) .Hanoi, Vietnam:IEEE, 2017:31-36.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" title=" 李勇, 林小竹, 蒋梦莹.基于跨连接 LeNet-5网络的面部表情识别[J].自动化学报, 2018, 44 (1) :176-182." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201801015&amp;v=MDg1MzQ5RVlZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVadEZ5amhVcnpQS0NMZlliRzRIOW5Ncm8=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                         李勇, 林小竹, 蒋梦莹.基于跨连接 LeNet-5网络的面部表情识别[J].自动化学报, 2018, 44 (1) :176-182.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" title=" 何志超, 赵龙章, 陈闯.用于人脸表情识别的多分辨率特征融合CNN[J].激光与光电子学进展, 2018, 55 (7) :071503." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JGDJ201807047&amp;v=MDAwMjdUNmo1NE8zenFxQnRHRnJDVVI3cWZadVp0RnlqaFVyelBMeXJQWkxHNEg5bk1xSTlCWTRRS0RIODR2UjQ=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                         何志超, 赵龙章, 陈闯.用于人脸表情识别的多分辨率特征融合CNN[J].激光与光电子学进展, 2018, 55 (7) :071503.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" title=" Ioffe S, Szegedy C.Batch Normalization:Accelerating Deep Network Training by Reducing Internal Covariate Shift[C]//32nd International Conference on Machine Learning, ICML 2015Lile, France:ICML, 2015:448-456." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Batch normalization:Accelerating deep network training by reducing internal covariate shift">
                                        <b>[7]</b>
                                         Ioffe S, Szegedy C.Batch Normalization:Accelerating Deep Network Training by Reducing Internal Covariate Shift[C]//32nd International Conference on Machine Learning, ICML 2015Lile, France:ICML, 2015:448-456.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" title=" He K, Zhang X, Ren S, et al.Deep residual learning for image recognition[C]//Proceedings of the IEEE conference on computer vision and pattern recognition.Las Vegas, Nevada:IEEE, 2016:770-778." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep Residual Learning for Image Recognition">
                                        <b>[8]</b>
                                         He K, Zhang X, Ren S, et al.Deep residual learning for image recognition[C]//Proceedings of the IEEE conference on computer vision and pattern recognition.Las Vegas, Nevada:IEEE, 2016:770-778.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" title=" Huang G, Liu Z, Weinberger K Q, et al.Densely connected convolutional networks[C]//Proceedings of the IEEE conference on computer vision and pattern recognition.Honolulu, Hawaii:IEEE, 2017." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Densely connected convolutional networks">
                                        <b>[9]</b>
                                         Huang G, Liu Z, Weinberger K Q, et al.Densely connected convolutional networks[C]//Proceedings of the IEEE conference on computer vision and pattern recognition.Honolulu, Hawaii:IEEE, 2017.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_10" title=" 张婷, 李玉鑑, 胡海鹤, 等.基于跨连卷积神经网络的性别分类模型[J].自动化学报, 2016, 42 (6) :858-865." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201606006&amp;v=MDQzNTR6cXFCdEdGckNVUjdxZlp1WnRGeWpoVXJ6UEtDTGZZYkc0SDlmTXFZOUZZb1FLREg4NHZSNFQ2ajU0TzM=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[10]</b>
                                         张婷, 李玉鑑, 胡海鹤, 等.基于跨连卷积神经网络的性别分类模型[J].自动化学报, 2016, 42 (6) :858-865.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_11" title=" Lucey P, Cohn J F, Kanade T, et al.The extended cohn-kanade dataset (ck+) :A complete dataset for action unit and emotion-specified expression[C]//Computer Vision and Pattern Recognition Workshops (CVPRW) , 2010 IEEE Computer Society Conference on.IEEE, 2010:94-101." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The extended CohnKanade dataset (CK+) A complete dataset for action unit and emotion-specified expression">
                                        <b>[11]</b>
                                         Lucey P, Cohn J F, Kanade T, et al.The extended cohn-kanade dataset (ck+) :A complete dataset for action unit and emotion-specified expression[C]//Computer Vision and Pattern Recognition Workshops (CVPRW) , 2010 IEEE Computer Society Conference on.IEEE, 2010:94-101.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JYRJ" target="_blank">计算机应用与软件</a>
                2019,36(07),197-201 DOI:10.3969/j.issn.1000-386x.2019.07.033            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于多特征融合密集残差CNN的人脸表情识别</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%A9%AC%E4%B8%AD%E5%90%AF&amp;code=42231770&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">马中启</a>
                                <a href="javascript:;">朱好生</a>
                                <a href="javascript:;">杨海仕</a>
                                <a href="javascript:;">王琪</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%83%A1%E7%87%95%E6%B5%B7&amp;code=08785905&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">胡燕海</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%AE%81%E6%B3%A2%E5%A4%A7%E5%AD%A6%E6%9C%BA%E6%A2%B0%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%8A%9B%E5%AD%A6%E5%AD%A6%E9%99%A2&amp;code=0160135&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">宁波大学机械工程与力学学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%AE%81%E6%B3%A2%E6%88%B4%E7%BB%B4%E5%8C%BB%E7%96%97%E5%99%A8%E6%A2%B0%E8%82%A1%E4%BB%BD%E6%9C%89%E9%99%90%E5%85%AC%E5%8F%B8&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">宁波戴维医疗器械股份有限公司</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>传统人脸表情识别主要基于人工提取特征, 其存在算法鲁棒性较差、易受人脸身份信息干扰等问题, 以及传统卷积神经网络不能充分提取人脸表情特征的现状。对此提出一种基于多特征融合密集残差卷积神经网络的人脸表情识别。该方法能够充分利用神经网络中每层的特征, 在密集块中, 对于每一个卷积层, 其前面所有卷积层的输出都将作为本卷积层的输入。然后将每个密集块的输出送入到全连接层中进行特征融合, 经过Softmax分类器分类。在CK+和FER2013数据集上进行多次实验, 与传统的机器学习方法相比, 该方法具有较高的准确率与较强的鲁棒性。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%A1%A8%E6%83%85%E8%AF%86%E5%88%AB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">表情识别;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%AF%86%E9%9B%86%E5%9E%8B%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">密集型卷积神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%89%B9%E5%BE%81%E8%9E%8D%E5%90%88&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">特征融合;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度学习;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    马中启, 硕士生, 主研领域:计算机视觉, 深度学习。;
                                </span>
                                <span>
                                    朱好生, 工程师。;
                                </span>
                                <span>
                                    杨海仕, 工程师。;
                                </span>
                                <span>
                                    王琪, 助工。;
                                </span>
                                <span>
                                    胡燕海, 教授。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-10-17</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金项目 (51705263);</span>
                                <span>宁波市重大科技专项 (2017C110030);</span>
                    </p>
            </div>
                    <h1><b>FACIAL EXPRESSION RECOGNITION BASED ON MULTI-FEATURE FUSION DENSE RESIDUAL CNN</b></h1>
                    <h2>
                    <span>Ma Zhongqi</span>
                    <span>Zhu Haosheng</span>
                    <span>Yang Haishi</span>
                    <span>Wang Qi</span>
                    <span>Hu Yanhai</span>
            </h2>
                    <h2>
                    <span>College of Mechanical Engineering and Mechanics, Ningbo University</span>
                    <span>Ningbo David Medical Device Co., Ltd.</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Because the traditional facial expression recognition is mainly based on the artificial extraction of features, the robustness of the algorithm is poor, and it is easy to be interfered by the face identity information. Traditional CNN cannot adequately extract facial expression features. We proposed a facial expression recognition based on multi-feature fusion dense residual CNN. This method could make full use of the characteristics of each layer in the neural network. For each convolution layer in the dense block, the output of all convolution layers in front of it would be the input of this convolution layer. Then the output of each dense block was fed into the full connection layer for feature fusion, and classified by Softmax classifier. Experiments were performed on CK+ and FER2013 data sets. Compared with traditional machine learning, our method has higher accuracy and stronger robustness.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Expression%20recognition&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Expression recognition;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Dense%20residual%20CNN&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Dense residual CNN;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Feature%20fusion&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Feature fusion;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Deep%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Deep learning;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-10-17</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="25" name="25" class="anchor-tag"><b>0 引 言</b></h3>
                <div class="p1">
                    <p id="26">随着计算机技术的不断发展, 人机交互相关产品正逐渐走入大众视野。在计算机视觉领域中, 人脸识别技术日益成熟, 人脸表情识别成为了热点研究课题。相比较人脸识别, 人脸表情识别不仅需要考虑光照、遮挡和姿态等问题, 同时还要考虑人脸身份特征以及人脸表情变化非刚性的特点。在传统机器学习中, 人脸表情研究主要由图像预处理、特征提取和分类三个步骤完成。传统特征提取过程中表情特征点为手工标注, 使得人脸表情特征提取很大程度上依赖于人为干预, 算法的鲁棒性较差。近年来, 随着深度学习技术的不断发展, 卷积神经网络在图像分类与识别方面取得巨大优势, 特征提取这一过程不再需要人为干预, 神经网络依据所提供样本特征自动学习表情特征, 然后通过分类器进行分类, 这种方法很大程度上减少了人为干预的影响, 提高了算法的鲁棒性以及准确率。</p>
                </div>
                <div class="p1">
                    <p id="27">卷积神经网络在面部表情识别应用领域属于起步阶段, Mollahosseini等<citation id="92" type="reference"><link href="3" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>使用了GoogleNet的网络结构, 结合多个数据集, 并对数据进行归一化处理, 训练结果接近人类能够识别的水准。实验结果表明深度神经网络能够有效提取表情特征, 但是该方法并未充分利用不同特征图之间的关系, 未使得表情特征信息最大流动。Xie等<citation id="93" type="reference"><link href="5" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>使用了FRR-CNN的结构对CK+数据集进行分类, 通过多个输入进行训练, 然后进行特征融合, 使用TI-pooling方法进行特征池化, 最后进行分类, 多输入的集成算法能够提升算法的鲁棒性。但文中仅使用了两层的卷积网络, 不能很好提取表情的特征。Pramerdorfer等<citation id="94" type="reference"><link href="7" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>通过对比目前各种数据集的start of art 结果, 提出了集成深度神经网络解决现在表情识别的瓶颈问题, 说明了使用深度神经网络集成能够很好地进行特征提取与分类。Lu等<citation id="95" type="reference"><link href="9" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>使用了一种利用众包标签的正则化CNN损失函数的人脸表情分类, 通过优化数据标签, 提高了算法的准确性。李勇等<citation id="96" type="reference"><link href="11" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>使用了一种基于跨连接的LeNet-5模型结构对人脸面部表情进行识别, 该方法有效地将低层次特征与高层次特征结合, 实验结果的准确率比仅使用高层次特征有明显提升。文中只是提取了不同层次的特征, 没有对层与层之间的特征进行更深层次的连接, 从而使得特征提取不够充分。何志超等<citation id="97" type="reference"><link href="13" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>提出了一种多分辨率特征融合的卷积神经网络, 将图片经过两个相互独立且深度不同的通道进行特征提取, 然后融合不同分辨率特征进行分类。</p>
                </div>
                <div class="p1">
                    <p id="28">本文提出一种多特征密集残差卷积神经网络, 该网络实现对每层卷积特征的重复利用与融合。首先在残差网络的基础上, 为了增加层与层之间的连接, 将每层网络提取出的特征分别传递给后面若干层网络。在特征传递的过程中, 设计一个密集模块, 在每个密集块中共有3个3×3卷积层和一个1×1卷积的Bottle layer层, 后一卷积的输入均为前面所有层卷积输出特征图之和。随着密集块个数的增加, 卷积层输出特征图数也随之以2的倍数增加。这样可以最大程度地利用每层特征图之间的关系以及特征信息的最大流动。其次将不同层的特征提取出来, 使其在全连接时与网络的输出层进行融合, 最后将融合后的特征添加Island loss以增大类间距离, 缩小类内距离, 随后送入softmax分类器进行分类。实验表明, 该模型不仅能有效提高准确率, 而且可以充分提取表情特征, 防止梯度消失。同时, 由于密集块的使用, 使得每层网络中的卷积核数量均有所降低, 进而减少了该模型的参数量, 在一定程度上能够降低过拟合。</p>
                </div>
                <h3 id="29" name="29" class="anchor-tag"><b>1 卷积神经网络</b></h3>
                <div class="p1">
                    <p id="30">卷积神经网络一般包括卷积层、降采样层、激活函数、全连接层和分类输出五部分。</p>
                </div>
                <div class="p1">
                    <p id="31">通常外部输入的图片直接与卷积层相连, 根据图片尺寸不同, 卷积层的卷积核大小也不尽相同。卷积层一般可以表示为:</p>
                </div>
                <div class="p1">
                    <p id="32"><mathml id="33"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>x</mi><msubsup><mrow></mrow><mi>j</mi><mi>l</mi></msubsup><mo>=</mo><mi>f</mi><mrow><mo>{</mo><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mi>i</mi></munder><mi>x</mi></mstyle><msubsup><mrow></mrow><mi>i</mi><mrow><mi>l</mi><mo>-</mo><mn>1</mn></mrow></msubsup><mo>*</mo><mi>Κ</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mi>j</mi><mtext> </mtext><mi>i</mi><mo>, </mo><mi>j</mi></mrow><mrow><mi>l</mi><mo>-</mo><mn>1</mn></mrow></msubsup><mo>+</mo><mi>b</mi><msubsup><mrow></mrow><mi>j</mi><mrow><mi>l</mi><mo>-</mo><mn>1</mn></mrow></msubsup></mrow><mo>}</mo></mrow></mrow></math></mathml>      (1) </p>
                </div>
                <div class="p1">
                    <p id="34"><i>f</i> (<i>x</i>) =max (0, <i>x</i>)      (2) </p>
                </div>
                <div class="p1">
                    <p id="35">式中:<i>x</i><mathml id="36"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mi>l</mi></msubsup></mrow></math></mathml>表示第<i>l</i>层的第<i>j</i>个特征图, <i>x</i><mathml id="37"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mrow><mi>l</mi><mo>-</mo><mn>1</mn></mrow></msubsup></mrow></math></mathml>表示的是第<i>l</i>-1层的第<i>i</i>个特征图, <i>K</i><mathml id="38"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mi>j</mi></mrow><mrow><mi>l</mi><mo>-</mo><mn>1</mn></mrow></msubsup></mrow></math></mathml>表示从<i>x</i><mathml id="39"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mrow><mi>l</mi><mo>-</mo><mn>1</mn></mrow></msubsup></mrow></math></mathml>到<i>x</i><mathml id="40"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mi>l</mi></msubsup></mrow></math></mathml>的卷积核, *为卷积运算, <i>b</i><mathml id="41"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mi>l</mi></msubsup></mrow></math></mathml>为其对应的偏置。<i>f</i> (·) 为激活函数。</p>
                </div>
                <div class="p1">
                    <p id="42">降采样层 (池化层) 一般可以表示为:</p>
                </div>
                <div class="p1">
                    <p id="43"><i>x</i><mathml id="44"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>l</mi></msubsup></mrow></math></mathml>=<i>down</i> (<i>x</i><mathml id="45"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>l</mi></msubsup></mrow></math></mathml>)      (3) </p>
                </div>
                <div class="p1">
                    <p id="46">式中:<i>x</i><mathml id="47"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>l</mi></msubsup></mrow></math></mathml>和<i>x</i><mathml id="48"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mrow><mi>l</mi><mo>-</mo><mn>1</mn></mrow></msubsup></mrow></math></mathml>分别表示降采样层的输出和输入, <i>down</i> () 表示降采样层 (池化层) 。</p>
                </div>
                <div class="p1">
                    <p id="49">全连接层是将卷积神经网络提取到的特征图进行全连接, 每个神经元的输出可以表示为:</p>
                </div>
                <div class="p1">
                    <p id="50"><i>F</i> (<i>x</i><sup><i>l</i></sup>) =<i>f</i> (<i>w</i><sup>T</sup><i>x</i><sup><i>l</i></sup>+<i>b</i>)      (4) </p>
                </div>
                <div class="p1">
                    <p id="51">式中:<i>F</i> (·) 表示全连接的输出, <i>w</i>为全连接权重, <i>b</i>为偏置, <i>f</i> (·) 为激活函数。</p>
                </div>
                <div class="p1">
                    <p id="52">对于分类输出, 本文选用softmax分类器, 对于属于的特征, softmax分类可表示为:</p>
                </div>
                <div class="p1">
                    <p id="53"><mathml id="54"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>f</mi><mo stretchy="false"> (</mo><mi>y</mi><mo>=</mo><mi>j</mi><mo stretchy="false">|</mo><mi>x</mi></mrow></math></mathml>;<mathml id="55"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>w</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mrow><mtext>e</mtext><msup><mrow></mrow><mrow><mi>w</mi><msub><mrow></mrow><mi>j</mi></msub><mo>×</mo><mi>x</mi></mrow></msup></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>l</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><mtext>e</mtext></mstyle><msup><mrow></mrow><mrow><mi>w</mi><msub><mrow></mrow><mi>l</mi></msub><mo>×</mo><mi>x</mi></mrow></msup></mrow></mfrac></mrow></math></mathml>      (5) </p>
                </div>
                <div class="p1">
                    <p id="56">计算输入类别<i>j</i>时的概率, 其中<i>w</i>表示的权重值, <i>x</i>表示输入的特征, <i>k</i>代表的是类别总数。</p>
                </div>
                <div class="p1">
                    <p id="57">本文除上述的卷积层等之外还使用了批归一化<sup><a class="sup">[12]</a></sup> (Batch Normalization, BN) 、dropout等方法抑制过拟合。</p>
                </div>
                <h3 id="58" name="58" class="anchor-tag"><b>2 多特征融合密集残差CNN</b></h3>
                <div class="p1">
                    <p id="59">神经网络中较浅层的特征图尺寸比较大, 对于较小的特征信息比较敏感, 但是缺少了对物体整体特征的表达, 较深层的特征图则与其相反, 能够很好地表达物体轮廓和外观等方面的信息, 但缺乏对小特征信息的敏感性。普通的卷积网络不能很好地利用这些特点, 张婷等<citation id="98" type="reference"><link href="21" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>提出的跨连接卷积神经网络能够有效地将较浅层次特征与深层次特征有效的结合, 构造出了良好的分类器, 实验结果表明跨连接分类器是有效的。随着网络模型的加深, 图像特征在传播过程中梯度相关性会逐渐减小, He等<citation id="99" type="reference"><link href="17" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>提出的ResNet深度残差网络跨连接方式以及Relu函数的使用, 使得图像特征在传播过程中保持了很大的梯度相关性, 从而得到更深层次结构的神经网络模型。Huang等<citation id="100" type="reference"><link href="19" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>提出的DenseNet密集型卷积神经网络使得整个神经网络中所有层之间都相互连接, 每一层网络的输入都来自之前所有网络的输出, 这样能确保神经网络中特征信息的最大流动。Ioffe等<citation id="101" type="reference"><link href="15" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>提出了BN网络, 通过对每个卷积层输出参数的归一化, 进而解决了神经网络在训练过程中梯度消失与梯度爆炸的问题。</p>
                </div>
                <div class="p1">
                    <p id="60">综合以上问题, 本文提出一种多特征密集残差卷积神经网络。</p>
                </div>
                <div class="p1">
                    <p id="61"> (1) 该模型结构不仅充分利用了卷积网络中层与层之间的紧密连接, 同时也提取了不同层次的特征, 多特征融合能够更好地提取人脸表情细微特征与整体轮廓特征, 密集网络可以更有效地提取有用的表情特征。</p>
                </div>
                <div class="p1">
                    <p id="62"> (2) DenseNet模型把所有特征进行连接, 造成了大量的冗余, 本文缩短每个block长度, 加大每个block特征图的数量, 对于每个卷积层使其输入的特征图尽量保持在2的幂次方, 这样能够加快运算速度。结合ResNet网络能够在减少冗余的情况下最大化信息流。具体结构模型见图1、图2。</p>
                </div>
                <div class="area_img" id="63">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201907034_063.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 CNN模型图" src="Detail/GetImg?filename=images/JYRJ201907034_063.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 CNN模型图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201907034_063.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="64">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201907034_064.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 DenseBlock (i) (i=1, 2, 3)" src="Detail/GetImg?filename=images/JYRJ201907034_064.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 DenseBlock (<i>i</i>) (<i>i</i>=1, 2, 3)   <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201907034_064.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="65"> (3) 添加了Island_loss层, 增大了不同表情类间距离<i>L</i><sub><i>C</i></sub>, 缩减了表情类内距离<i>L</i><sub><i>IL</i></sub>。</p>
                </div>
                <div class="p1">
                    <p id="66"><mathml id="67"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><msub><mrow></mrow><mi>C</mi></msub><mo>=</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mrow><mrow><mo>|</mo><mrow><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mi>c</mi><msub><mrow></mrow><mrow><mi>y</mi><mi>i</mi></mrow></msub></mrow><mo>|</mo></mrow></mrow></mstyle></mrow></math></mathml>      (6) </p>
                </div>
                <div class="p1">
                    <p id="68"><mathml id="69"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><msub><mrow></mrow><mrow><mi>Ι</mi><mi>L</mi></mrow></msub><mo>=</mo><mi>L</mi><msub><mrow></mrow><mi>C</mi></msub><mo>+</mo><mi>λ</mi><msub><mrow></mrow><mn>1</mn></msub><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>c</mi><msub><mrow></mrow><mi>j</mi></msub><mo>∈</mo><mi>Ν</mi></mrow></munder><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mtable columnalign="left" rowspacing="-0.1"><mtr><mtd><mi>c</mi><msub><mrow></mrow><mi>k</mi></msub><mo>∈</mo><mi>Ν</mi></mtd></mtr><mtr><mtd><mi>c</mi><msub><mrow></mrow><mi>k</mi></msub><mo>≠</mo><mi>c</mi><msub><mrow></mrow><mi>j</mi></msub></mtd></mtr></mtable></mrow></munder><mrow><mrow><mo> (</mo><mrow><mfrac><mrow><mi>c</mi><msub><mrow></mrow><mi>k</mi></msub><mo>⋅</mo><mi>c</mi><msub><mrow></mrow><mi>j</mi></msub></mrow><mrow><mrow><mo>|</mo><mrow><mi>c</mi><msub><mrow></mrow><mi>k</mi></msub></mrow><mo>|</mo></mrow><msub><mrow></mrow><mn>2</mn></msub><mrow><mo>|</mo><mrow><mi>c</mi><msub><mrow></mrow><mi>j</mi></msub></mrow><mo>|</mo></mrow><msub><mrow></mrow><mn>2</mn></msub></mrow></mfrac><mo>+</mo><mn>1</mn></mrow><mo>) </mo></mrow></mrow></mstyle></mrow></mstyle></mrow></math></mathml>      (7) </p>
                </div>
                <div class="p1">
                    <p id="70">式中:<i>x</i><sub><i>i</i></sub>表示全连接层输出的第<i>x</i>个样本。<i>c</i>表示所有同一类别<i>y</i><sub><i>i</i></sub>的所有样本的中心值。</p>
                </div>
                <div class="p1">
                    <p id="71"> (4) 密集块和残差网络的使用可以提高深度网络的梯度相关性, 同时密集块的使用也在一定程度上降低了网络参数量, 使得模型具有一定的抗过拟合效果。在最终的softmax分类之前进行融合。</p>
                </div>
                <div class="p1">
                    <p id="72">图1中虚线代表残差连接, 实线代表多尺度特征融合。本文使用了三个密集块连接, 在DenseBlock模块中, 起始卷积特征图个数为32, 随着模块增加, 每层卷积输出的特征图增加方式如图2所示, 根据密集块的顺序逐次按照2<sup> (<i>i</i>-1) </sup>次幂增加卷积层的个数, 这样既可以使输入卷积核的个数大部分为2的幂次方以增加计算机运算速度, 又可以增加特征图个数从而提取更多信息。</p>
                </div>
                <h3 id="73" name="73" class="anchor-tag"><b>3 实 验</b></h3>
                <div class="p1">
                    <p id="74">本文所做的实验是基于Python的Tensorflow进行的, 硬件平台Intel Core i7-7700, GPU为NVIDIA GeForce GTX1070, 显存为8 GB。</p>
                </div>
                <h4 class="anchor-tag" id="75" name="75"><b>3.1 数据集</b></h4>
                <div class="p1">
                    <p id="76">本实验所采用的数据集为CK+和FER2013数据集, 数据集包含了7种基本的表情:高兴、悲伤、愤怒、恐惧、惊讶、厌恶与中性, 如图3所示。</p>
                </div>
                <div class="area_img" id="77">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201907034_077.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 Ck+表情库7种表情实例" src="Detail/GetImg?filename=images/JYRJ201907034_077.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 Ck+表情库7种表情实例  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201907034_077.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>
                                <p class="img_note">Angry Disgust Fear Happy Neutral Sad Surprised</p>

                </div>
                <div class="p1">
                    <p id="78">CK+数据集<citation id="102" type="reference"><link href="23" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>:包含来自123个人的593个表情视频序列。这些视频包含了从中性表情到其他6种表情的变化, 本次实验仅选取每个序列表情张量最大的3幅图片, 检测出人脸部分并裁剪至256×256, 然后随机使用其中一幅图像再随机裁剪成两幅227×227像素的图片。最后将所有的图片归一化大小为64×64的图像。</p>
                </div>
                <div class="p1">
                    <p id="79">FER2013数据集:FER2013是2013年Kaggle比赛用的数据集, 图片均为网上爬取, 符合自然条件下的表情分布。数据集包含28 709幅Training data, 3 589幅Publictest data和3 589幅Privatetest data。每幅图都是像素为48×48的灰度图。该数据集中共有7种表情:高兴、悲伤、愤怒、恐惧、惊讶、厌恶与中性。</p>
                </div>
                <div class="area_img" id="80">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201907034_080.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 FER2013表情库7种表情实例" src="Detail/GetImg?filename=images/JYRJ201907034_080.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 FER2013表情库7种表情实例  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201907034_080.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>
                                <p class="img_note">Angry Disgust Fear Happy Neutral Sad Surprised</p>

                </div>
                <h4 class="anchor-tag" id="81" name="81"><b>3.2 实验结果分析</b></h4>
                <div class="p1">
                    <p id="82">CK+数据采用了交叉验证的方法, 将数据的图片分为5份, 每次取其中4份作为训练集, 另外一份作为测试集, 对5次测试的准确率求平均值作为整个数据集的准确结果。并且数据集中训练部分进行数据增强, 随机左右翻转、沿对角线翻转、调整图像亮度, 以及随机增加噪声点以减小外界环境对识别的干扰, 增强鲁棒性。</p>
                </div>
                <div class="p1">
                    <p id="83">FER2013数据集直接对测试数据集进行10crop:分别对图片左上角、左下角、右上角、右下角以及图片中间裁剪至44×44尺寸, 并进行翻转操作。训练数据直接随机裁剪至44×44的尺寸。表1表示的是本文所提出的方法在CK+数据集上的交叉验证结果。可以看出, 对于厌恶、高兴以及惊讶三种表情得到了较高的准确率, 生气和悲伤的准确率稍微下降一些, 其主要原因可能是生气和悲伤等与其他的表情之间存在模糊定义, 检测时引起识别率下降。图5给出的是其FER2013测试集的混淆矩阵。图6是对比改进前的DenseNet的网络模型和ResNet网络模型在训练过程中收敛的速度。可以看出改进后的网络的收敛速度与ResNet的相当, 远高于DenseNet。</p>
                </div>
                <div class="area_img" id="84">
                    <p class="img_tit"><b>表1 CK+不同种类表情的分类正确率</b> % <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="84" border="1"><tr><td><br />测试<br />集</td><td>生气</td><td>厌恶</td><td>害怕</td><td>高兴</td><td>中性</td><td>悲伤</td><td>惊讶</td><td>整体</td></tr><tr><td><br />0</td><td>89.19</td><td>98</td><td>100</td><td>100</td><td>93.94</td><td>97.06</td><td>98.08</td><td>96.44</td></tr><tr><td><br />1</td><td>92</td><td>96.43</td><td>96.30</td><td>100</td><td>89.80</td><td>90.91</td><td>96</td><td>94.82</td></tr><tr><td><br />2</td><td>100</td><td>96.55</td><td>80.55</td><td>100</td><td>96.42</td><td>95.83</td><td>100</td><td>96.10</td></tr><tr><td><br />3</td><td>86.67</td><td>100</td><td>96.67</td><td>100</td><td>95.45</td><td>94.44</td><td>100</td><td>96.43</td></tr><tr><td><br />4</td><td>92.11</td><td>94.87</td><td>97.14</td><td>98.08</td><td>91.53</td><td>91.67</td><td>100</td><td>95.45</td></tr><tr><td><br />整体</td><td>92</td><td>97.17</td><td>94.13</td><td>99.62</td><td>93.43</td><td>93.98</td><td>98.82</td><td>95.85</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="85">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201907034_085.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 卷积神经网络预测Fer2013混淆矩阵" src="Detail/GetImg?filename=images/JYRJ201907034_085.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 卷积神经网络预测Fer2013混淆矩阵  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201907034_085.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="86">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201907034_086.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 训练过程中的损失函数" src="Detail/GetImg?filename=images/JYRJ201907034_086.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 训练过程中的损失函数  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201907034_086.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="87">为了验证本文方法的有效性, 将文献<citation id="105" type="reference">[<a class="sup">1</a>,<a class="sup">2</a>,<a class="sup">3</a>,<a class="sup">5</a>,<a class="sup">6</a>]</citation>的方法与本文进行对比, 由表2可以看出, 本文的方法具有明显优势。这是因为本文使用了密集残差网络的结构, 能够有效地对特征进行提取, 利用跨连接融合的方式得到局部细节和全局轮廓, 因此能够很好地提升表情识别的准确率。文献<citation id="103" type="reference">[<a class="sup">1</a>]</citation>使用了Inception layer作为基本的模型结构, 这种模型使用了多个不同的卷积核进行卷积, 但是特征图与特征图之间没有有效密集连接, 无法更有效地进行特征梯度的传递。文献<citation id="104" type="reference">[<a class="sup">6</a>]</citation>使用了多分辨率特征融合的方法, 利用两个不相关的卷积层进行叠加融合, 但网络结构比较简单, 没有更好地利用不同层特征图之间信息传递, 这样在每个卷积层里面不能完全提取出表情的有效特征。</p>
                </div>
                <div class="area_img" id="88">
                    <p class="img_tit"><b>表2 不同算法的识别率对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="88" border="1"><tr><td>算法</td><td>FER2013:<br />准确率/%</td><td>算法</td><td>CK+:<br />准确率/%</td></tr><tr><td><br />Inception<sup>[1]</sup></td><td>66.40±0.60</td><td>Inception<sup>[1]</sup></td><td>93.20±1.40</td></tr><tr><td><br />VGG<sup>[3]</sup></td><td>72.70</td><td>FRR-CNN<sup>[2]</sup></td><td>92.06</td></tr><tr><td><br />Inception<sup>[3]</sup></td><td>71.60</td><td>LeNet-5<sup>[5]</sup></td><td>83.74</td></tr><tr><td><br />ResNet<sup>[3]</sup></td><td>72.41</td><td>多特征融合CNN<sup>[6]</sup></td><td>92.11</td></tr><tr><td><br />本文方法</td><td>73.22</td><td>本文方法</td><td>95.85</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h3 id="89" name="89" class="anchor-tag"><b>4 结 语</b></h3>
                <div class="p1">
                    <p id="90">本文提出了一种多特征密集残差型卷积神经网络, 该网络通过对不同的卷积层进行融合叠加, 充分利用了每个卷积层输出的特征。密集残差网络的使用能够有效减少在训练过程中梯度消失问题。多特征提取的方式使得网络能够在最终分类层时使用不同分辨率特征, 通过卷积网络训练结果的对比, 表明了本文的提取方法准确率更高和鲁棒性更好以及更快的收敛速度。</p>
                </div>
                <div class="p1">
                    <p id="91">基于深度学习的卷积神经网络, 能够自动提取输入数据的特征, 不需要人工干预, 在图像分类、检测与识别等计算机视觉方向得到了很好的应用, 其检测结果远远高于一般机器学习的方法。但是深度学习需要大量的有标签数据, 这对于缺少标签数据的表情识别领域提出了很大的挑战, 同时也是下一步要研究的重点。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Going deeper in facial expression recognition using deep neural networks.&amp;quot;">

                                <b>[1]</b> Mollahosseini A, Chan D, Mahoor M H.Going deeper in facial expression recognition using deep neural networks[C]//2016 IEEE Winter Conference on Applications of Computer Vision (WACV) .Lake Placid, NY, USA:IEEE, 2016:1-10.
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Facial Expression Recognition with FRR-CNN">

                                <b>[2]</b> Xie S, Hu H.Facial expression recognition with FRR-CNN[J].Electronics Letters, 2017, 53 (4) :235-237.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Facial expression recognition using convolutional neural networks:state of the art[EB]">

                                <b>[3]</b> Pramerdorfer C, Kampel M.Facial expression recognition using convolutional neural networks:state of the art[EB].arXiv preprint arXiv, 1612.02903, 2016.
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Regularizing the loss layer of CNNs for facial expression recognition using crowd sourced labels">

                                <b>[4]</b> Lu P, Li B, Shama S, et al.Regularizing the loss layer of CNNs for facial expression recognition using crowd sourced labels[C]//2017 21st Asia Pacific Symposium on Intelligent and Evolutionary Systems (IES) .Hanoi, Vietnam:IEEE, 2017:31-36.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201801015&amp;v=MTgwMDViRzRIOW5Ncm85RVlZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVadEZ5amhVcnpQS0NMZlk=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b> 李勇, 林小竹, 蒋梦莹.基于跨连接 LeNet-5网络的面部表情识别[J].自动化学报, 2018, 44 (1) :176-182.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JGDJ201807047&amp;v=MjQzNzVCWTRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVp0RnlqaFVyelBMeXJQWkxHNEg5bk1xSTk=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b> 何志超, 赵龙章, 陈闯.用于人脸表情识别的多分辨率特征融合CNN[J].激光与光电子学进展, 2018, 55 (7) :071503.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Batch normalization:Accelerating deep network training by reducing internal covariate shift">

                                <b>[7]</b> Ioffe S, Szegedy C.Batch Normalization:Accelerating Deep Network Training by Reducing Internal Covariate Shift[C]//32nd International Conference on Machine Learning, ICML 2015Lile, France:ICML, 2015:448-456.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep Residual Learning for Image Recognition">

                                <b>[8]</b> He K, Zhang X, Ren S, et al.Deep residual learning for image recognition[C]//Proceedings of the IEEE conference on computer vision and pattern recognition.Las Vegas, Nevada:IEEE, 2016:770-778.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Densely connected convolutional networks">

                                <b>[9]</b> Huang G, Liu Z, Weinberger K Q, et al.Densely connected convolutional networks[C]//Proceedings of the IEEE conference on computer vision and pattern recognition.Honolulu, Hawaii:IEEE, 2017.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_10" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201606006&amp;v=MjIzMjRZYkc0SDlmTXFZOUZZb1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnRGeWpoVXJ6UEtDTGY=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[10]</b> 张婷, 李玉鑑, 胡海鹤, 等.基于跨连卷积神经网络的性别分类模型[J].自动化学报, 2016, 42 (6) :858-865.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The extended CohnKanade dataset (CK+) A complete dataset for action unit and emotion-specified expression">

                                <b>[11]</b> Lucey P, Cohn J F, Kanade T, et al.The extended cohn-kanade dataset (ck+) :A complete dataset for action unit and emotion-specified expression[C]//Computer Vision and Pattern Recognition Workshops (CVPRW) , 2010 IEEE Computer Society Conference on.IEEE, 2010:94-101.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JYRJ201907034" />
        <input id="dpi" type="hidden" value="600" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201907034&amp;v=MDUwMDlqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVadEZ5amhVcnpQTHpUWlpMRzRIOWpNcUk5R1lJUUtESDg0dlI0VDY=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
