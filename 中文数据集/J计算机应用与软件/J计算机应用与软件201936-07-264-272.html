<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637135626790627500%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJYRJ201907046%26RESULT%3d1%26SIGN%3d%252fimPjlYf7raeY5Ui3%252bUVsrnZCxg%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JYRJ201907046&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JYRJ201907046&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201907046&amp;v=MDUwOTRqaFU3L0FMelRaWkxHNEg5ak1xSTlCWW9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVp0Rnk=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#23" data-title="&lt;b&gt;0 引 言&lt;/b&gt; "><b>0 引 言</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#31" data-title="&lt;b&gt;1 相关工作&lt;/b&gt; "><b>1 相关工作</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#32" data-title="&lt;b&gt;1.1 图像平均对比归一化系数MSCN&lt;/b&gt;"><b>1.1 图像平均对比归一化系数MSCN</b></a></li>
                                                <li><a href="#51" data-title="&lt;b&gt;1.2 概率矩阵分解PMF&lt;/b&gt;"><b>1.2 概率矩阵分解PMF</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#107" data-title="&lt;b&gt;2 基本动机&lt;/b&gt; "><b>2 基本动机</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#116" data-title="&lt;b&gt;3 方法设计&lt;/b&gt; "><b>3 方法设计</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#117" data-title="&lt;b&gt;3.1 预处理&lt;/b&gt;"><b>3.1 预处理</b></a></li>
                                                <li><a href="#120" data-title="&lt;b&gt;3.2 PMF估计参考块&lt;/b&gt;"><b>3.2 PMF估计参考块</b></a></li>
                                                <li><a href="#164" data-title="&lt;b&gt;3.3 特征提取&lt;/b&gt;"><b>3.3 特征提取</b></a></li>
                                                <li><a href="#201" data-title="&lt;b&gt;3.4 特征池化&lt;/b&gt;"><b>3.4 特征池化</b></a></li>
                                                <li><a href="#216" data-title="&lt;b&gt;3.5 IQA模型训练&lt;/b&gt;"><b>3.5 IQA模型训练</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#230" data-title="&lt;b&gt;4 实 验&lt;/b&gt; "><b>4 实 验</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#231" data-title="&lt;b&gt;4.1 实验数据库和评价标准&lt;/b&gt;"><b>4.1 实验数据库和评价标准</b></a></li>
                                                <li><a href="#238" data-title="&lt;b&gt;4.2 实验细节&lt;/b&gt;"><b>4.2 实验细节</b></a></li>
                                                <li><a href="#242" data-title="&lt;b&gt;4.3 实验结果&lt;/b&gt;"><b>4.3 实验结果</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#249" data-title="&lt;b&gt;5 结 语&lt;/b&gt; "><b>5 结 语</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#30" data-title="图1 算法流程图">图1 算法流程图</a></li>
                                                <li><a href="#114" data-title="图2 图像失真对MSCN的影响">图2 图像失真对MSCN的影响</a></li>
                                                <li><a href="#114" data-title="图2 图像失真对MSCN的影响">图2 图像失真对MSCN的影响</a></li>
                                                <li><a href="#163" data-title="图3 PMF对参考块的估计效果">图3 PMF对参考块的估计效果</a></li>
                                                <li><a href="#241" data-title="图4 PMF分解潜变量D值选择">图4 PMF分解潜变量D值选择</a></li>
                                                <li><a href="#244" data-title="&lt;b&gt;表1 MLIVE数据库性能比较结果&lt;/b&gt;"><b>表1 MLIVE数据库性能比较结果</b></a></li>
                                                <li><a href="#245" data-title="&lt;b&gt;表2 MDID2013数据库性能比较结果&lt;/b&gt;"><b>表2 MDID2013数据库性能比较结果</b></a></li>
                                                <li><a href="#246" data-title="&lt;b&gt;表3 The Live Challenge Database数据库性能比较结果&lt;/b&gt;"><b>表3 The Live Challenge Database数据库性能比较结果</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="266">


                                    <a id="bibliography_1" title="周景超, 戴汝为, 肖柏华.图像质量评价研究综述[J].计算机科学, 2008, 35 (7) :1-4." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJA200807002&amp;v=Mjg2MjNCYjdHNEh0bk1xSTlGWm9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVp0RnlqaFU3ekpMejc=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                        周景超, 戴汝为, 肖柏华.图像质量评价研究综述[J].计算机科学, 2008, 35 (7) :1-4.
                                    </a>
                                </li>
                                <li id="268">


                                    <a id="bibliography_2" title="李爱华.基于相位的结构相似度图像质量评价模型[J].计算机应用与软件, 2014, 31 (11) :233-236." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201411060&amp;v=MDE1OTMzenFxQnRHRnJDVVI3cWZadVp0RnlqaFU3ekpMelRaWkxHNEg5WE5ybzlEWklRS0RIODR2UjRUNmo1NE8=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                        李爱华.基于相位的结构相似度图像质量评价模型[J].计算机应用与软件, 2014, 31 (11) :233-236.
                                    </a>
                                </li>
                                <li id="270">


                                    <a id="bibliography_3" title="Wang Z, Bovik A C, Sheikh H R, et al.Image Quality Assessment:From Error Visibility to Structural Similarity[J].IEEE Transactions on Image Processing, 2004, 13 (4) :600-612." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image quality assessment: from error visibility to structural similarity">
                                        <b>[3]</b>
                                        Wang Z, Bovik A C, Sheikh H R, et al.Image Quality Assessment:From Error Visibility to Structural Similarity[J].IEEE Transactions on Image Processing, 2004, 13 (4) :600-612.
                                    </a>
                                </li>
                                <li id="272">


                                    <a id="bibliography_4" title="Sheikh H R, Bovik A C.Image information and visual quality[J].IEEE Transactions on Image Processing, 2006, 15 (2) :430-444." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image information and visual quality">
                                        <b>[4]</b>
                                        Sheikh H R, Bovik A C.Image information and visual quality[J].IEEE Transactions on Image Processing, 2006, 15 (2) :430-444.
                                    </a>
                                </li>
                                <li id="274">


                                    <a id="bibliography_5" title="Demirtas A M, Reibman A R, Jafarkhani H.Full-Reference Quality Estimation for Images With Different Spatial Resolutions[J].Image Processing IEEE Transactions on, 2014, 23 (5) :2069-2080." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Full-Reference Quality Estimation for Images With Different Spatial Resolutions">
                                        <b>[5]</b>
                                        Demirtas A M, Reibman A R, Jafarkhani H.Full-Reference Quality Estimation for Images With Different Spatial Resolutions[J].Image Processing IEEE Transactions on, 2014, 23 (5) :2069-2080.
                                    </a>
                                </li>
                                <li id="276">


                                    <a id="bibliography_6" title="Di Claudio E D, Jacovitti G.A Detail Based Method for Linear Full Reference Image Quality Prediction[J].IEEE Trans Image Process, 2018, 27 (1) :179-193." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Detail Based Method for Linear Full Reference Image Quality Prediction">
                                        <b>[6]</b>
                                        Di Claudio E D, Jacovitti G.A Detail Based Method for Linear Full Reference Image Quality Prediction[J].IEEE Trans Image Process, 2018, 27 (1) :179-193.
                                    </a>
                                </li>
                                <li id="278">


                                    <a id="bibliography_7" title="Soundararajan R, Bovik A C.RRED Indices:Reduced Reference Entropic Differencing for Image Quality Assessment[J].IEEE Transactions on Image Processing A Publication of the IEEE Signal Processing Society, 2012, 21 (2) :517-526." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=RRED Indices: Reduced Reference Entropic Differencing forImage Quality Assessment">
                                        <b>[7]</b>
                                        Soundararajan R, Bovik A C.RRED Indices:Reduced Reference Entropic Differencing for Image Quality Assessment[J].IEEE Transactions on Image Processing A Publication of the IEEE Signal Processing Society, 2012, 21 (2) :517-526.
                                    </a>
                                </li>
                                <li id="280">


                                    <a id="bibliography_8" title="Mittal A, Moorthy A K, Bovik A C.No-Reference Image Quality Assessment in the Spatial Domain[J].IEEE Transactions on Image Processing A Publication of the IEEE Signal Processing Society, 2012, 21 (12) :4695." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=No-reference image quality assessment in the spatial domain">
                                        <b>[8]</b>
                                        Mittal A, Moorthy A K, Bovik A C.No-Reference Image Quality Assessment in the Spatial Domain[J].IEEE Transactions on Image Processing A Publication of the IEEE Signal Processing Society, 2012, 21 (12) :4695.
                                    </a>
                                </li>
                                <li id="282">


                                    <a id="bibliography_9" title="Gu K, Zhai G, Liu M, et al.FISBLIM:A FIve-Step BLInd Metric for quality assessment of multiply distorted images[C]//2013 IEEE Workshop on Signal Processing Systems (Si PS) .IEEE, 2013." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=FISBLIM:a five-step blind metric for quality assessment of multiply distorted images">
                                        <b>[9]</b>
                                        Gu K, Zhai G, Liu M, et al.FISBLIM:A FIve-Step BLInd Metric for quality assessment of multiply distorted images[C]//2013 IEEE Workshop on Signal Processing Systems (Si PS) .IEEE, 2013.
                                    </a>
                                </li>
                                <li id="284">


                                    <a id="bibliography_10" title="Chandler D M.Seven Challenges in Image Quality Assessment:Past, Present, and Future Research[J].Isrn Signal Processing, 2013, 2013:Article ID 905685." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Seven challenges in image quality assessment:past,present,and future research">
                                        <b>[10]</b>
                                        Chandler D M.Seven Challenges in Image Quality Assessment:Past, Present, and Future Research[J].Isrn Signal Processing, 2013, 2013:Article ID 905685.
                                    </a>
                                </li>
                                <li id="286">


                                    <a id="bibliography_11" title="Ghadiyaram D, Bovik A C.Perceptual Quality Prediction on Authentically Distorted Images Using a Bag of Features Approach[J].Journal of Vision, 2017, 17 (1) :32." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Perceptual quality prediction on authentically distorted images using a bag of features approach">
                                        <b>[11]</b>
                                        Ghadiyaram D, Bovik A C.Perceptual Quality Prediction on Authentically Distorted Images Using a Bag of Features Approach[J].Journal of Vision, 2017, 17 (1) :32.
                                    </a>
                                </li>
                                <li id="288">


                                    <a id="bibliography_12" title="Li Q, Lin W, Fang Y.No-Reference Quality Assessment for Multiply-Distorted Images in Gradient Domain[J].IEEESignal Processing Letters, 2016, 23 (4) :541-545." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=No-reference quality assessment for multiply-distorted images in gradient domain">
                                        <b>[12]</b>
                                        Li Q, Lin W, Fang Y.No-Reference Quality Assessment for Multiply-Distorted Images in Gradient Domain[J].IEEESignal Processing Letters, 2016, 23 (4) :541-545.
                                    </a>
                                </li>
                                <li id="290">


                                    <a id="bibliography_13" title="Mittal A, Fellow, IEEE, et al.Making a’Completely Blind’Image Quality Analyzer[J].IEEE Signal Processing Letters, 2013, 20 (3) :209-212." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Making a “Completely Blind” Image Quality Analyzer">
                                        <b>[13]</b>
                                        Mittal A, Fellow, IEEE, et al.Making a’Completely Blind’Image Quality Analyzer[J].IEEE Signal Processing Letters, 2013, 20 (3) :209-212.
                                    </a>
                                </li>
                                <li id="292">


                                    <a id="bibliography_14" title="Zhang L, Zhang L, Bovik A C.A feature-enriched completely blind image quality evaluator[J].IEEE Transactions on Image Processing, 2015, 24 (8) :2579-2591." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Feature-Enriched Completely Blind Image Quality Evaluator">
                                        <b>[14]</b>
                                        Zhang L, Zhang L, Bovik A C.A feature-enriched completely blind image quality evaluator[J].IEEE Transactions on Image Processing, 2015, 24 (8) :2579-2591.
                                    </a>
                                </li>
                                <li id="294">


                                    <a id="bibliography_15" title="Saha A, Wu Q M J.Utilizing image scales towards totally training free blind image quality assessment[J].IEEETransactions on Image Processing, 2015, 24 (6) :1879-1892." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Utilizing image scales towards totally training free blind image quality assessment">
                                        <b>[15]</b>
                                        Saha A, Wu Q M J.Utilizing image scales towards totally training free blind image quality assessment[J].IEEETransactions on Image Processing, 2015, 24 (6) :1879-1892.
                                    </a>
                                </li>
                                <li id="296">


                                    <a id="bibliography_16" title="Zhang Y, Chandler D M.Opinion-Unaware Blind Quality Assessment of Multiply and Singly Distorted Images via Distortion Parameter Estimation[J].IEEE Transactions on Image Processing, 2018, 27 (11) :5433-5448." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Opinion-Unaware Blind Quality Assessment of Multiply and Singly Distorted Images via Distortion Parameter Estimation">
                                        <b>[16]</b>
                                        Zhang Y, Chandler D M.Opinion-Unaware Blind Quality Assessment of Multiply and Singly Distorted Images via Distortion Parameter Estimation[J].IEEE Transactions on Image Processing, 2018, 27 (11) :5433-5448.
                                    </a>
                                </li>
                                <li id="298">


                                    <a id="bibliography_17" title="Mnih A, Salakhutdinov R R.Probabilistic matrix factorization[C]//Advances in neural information processing systems.2008:1257-1264." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Probabilistic Matrix Factorization">
                                        <b>[17]</b>
                                        Mnih A, Salakhutdinov R R.Probabilistic matrix factorization[C]//Advances in neural information processing systems.2008:1257-1264.
                                    </a>
                                </li>
                                <li id="300">


                                    <a id="bibliography_18" title="Wang N, Yeung D Y.Bayesian robust matrix factorization for image and video processing[C]//Proceedings of the IEEE International Conference on Computer Vision.2013:1785-1792." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Bayesian robust matrix factorization for image and video processing">
                                        <b>[18]</b>
                                        Wang N, Yeung D Y.Bayesian robust matrix factorization for image and video processing[C]//Proceedings of the IEEE International Conference on Computer Vision.2013:1785-1792.
                                    </a>
                                </li>
                                <li id="302">


                                    <a id="bibliography_19" title="Li S, Zhang F, Ma L, et al.Image quality assessment by separately evaluating detail losses and additive impairments[J].IEEE Transactions on Multimedia, 2011, 13 (5) :935-949." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image Quality Assessment by Separately Evaluating Detail Losses and Additive Impairments">
                                        <b>[19]</b>
                                        Li S, Zhang F, Ma L, et al.Image quality assessment by separately evaluating detail losses and additive impairments[J].IEEE Transactions on Multimedia, 2011, 13 (5) :935-949.
                                    </a>
                                </li>
                                <li id="304">


                                    <a id="bibliography_20" title="Dabov K, Foi A, Katkovnik V, et al.Image denoising by sparse 3-D transform-domain collaborative filtering[J].IEEE Transactions on image processing, 2007, 16 (8) :2080-2095." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image Denoising by Sparse 3-D Transform-Domain Collaborative Filtering">
                                        <b>[20]</b>
                                        Dabov K, Foi A, Katkovnik V, et al.Image denoising by sparse 3-D transform-domain collaborative filtering[J].IEEE Transactions on image processing, 2007, 16 (8) :2080-2095.
                                    </a>
                                </li>
                                <li id="306">


                                    <a id="bibliography_21" title="Ye P, Kumar J, Kang L, et al.Unsupervised feature learning framework for no-reference image quality assessment[C]//2012 IEEE conference on computer vision and pattern recognition.IEEE, 2012:1098-1105." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Unsupervised feature learning framework for no-referenceimage quality assessment">
                                        <b>[21]</b>
                                        Ye P, Kumar J, Kang L, et al.Unsupervised feature learning framework for no-reference image quality assessment[C]//2012 IEEE conference on computer vision and pattern recognition.IEEE, 2012:1098-1105.
                                    </a>
                                </li>
                                <li id="308">


                                    <a id="bibliography_22" title="刘国军, 高丽霞, 陈丽奇.广义平均的全参考型图像质量评价池化策略[J].光学精密工程, 2017, 25 (3) :742-748." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXJM201703027&amp;v=MTkwMDFyQ1VSN3FmWnVadEZ5amhVN3pKSWpYQlk3RzRIOWJNckk5SFk0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0Y=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[22]</b>
                                        刘国军, 高丽霞, 陈丽奇.广义平均的全参考型图像质量评价池化策略[J].光学精密工程, 2017, 25 (3) :742-748.
                                    </a>
                                </li>
                                <li id="310">


                                    <a id="bibliography_23" >
                                        <b>[23]</b>
                                    Saad M A, Bovik A C, Charrier C.Blind image quality assessment:A natural scene statistics approach in the DCT domain[J].IEEE transactions on Image Processing, 2012, 21 (8) :3339-3352.</a>
                                </li>
                                <li id="312">


                                    <a id="bibliography_24" title="付燕, 张妍.基于ε-SVR的JPEG无参考图像质量评价[J].计算机应用与软件, 2014, 31 (10) :213-215, 238." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201410052&amp;v=MDU5OTc3ekpMelRaWkxHNEg5WE5yNDlBWm9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVp0RnlqaFU=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[24]</b>
                                        付燕, 张妍.基于ε-SVR的JPEG无参考图像质量评价[J].计算机应用与软件, 2014, 31 (10) :213-215, 238.
                                    </a>
                                </li>
                                <li id="314">


                                    <a id="bibliography_25" title="高飞, 高新波.主动特征学习及其在盲图像质量评价中的应用[J].计算机学报, 2014, 37 (10) :2227-2234." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJX201410016&amp;v=MDU5OTJGeWpoVTd6Skx6N0Jkckc0SDlYTnI0OUVZb1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnQ=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[25]</b>
                                        高飞, 高新波.主动特征学习及其在盲图像质量评价中的应用[J].计算机学报, 2014, 37 (10) :2227-2234.
                                    </a>
                                </li>
                                <li id="316">


                                    <a id="bibliography_26" title="Chang C C, Lin C J.LIBSVM:A library for support vector machines[J].ACM Transactions on Intelligent Systems and Technology, 2011, 2 (3) :1-27." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000002769&amp;v=MDE4MjZ6SUlGb1VhUk09TmlmSVk3SzdIdGpOcjQ5RlpPc05DM293b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[26]</b>
                                        Chang C C, Lin C J.LIBSVM:A library for support vector machines[J].ACM Transactions on Intelligent Systems and Technology, 2011, 2 (3) :1-27.
                                    </a>
                                </li>
                                <li id="318">


                                    <a id="bibliography_27" title="Jayaraman D, Mittal A, Moorthy A K, et al.Objective quality assessment of multiply distorted images[C]//Signals, Systems and Computers (ASILOMAR) , 2012 Conference Record of the Forty Sixth Asilomar Conference on.IEEE, 2012." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Objective quality assessment of multiply distorted images">
                                        <b>[27]</b>
                                        Jayaraman D, Mittal A, Moorthy A K, et al.Objective quality assessment of multiply distorted images[C]//Signals, Systems and Computers (ASILOMAR) , 2012 Conference Record of the Forty Sixth Asilomar Conference on.IEEE, 2012.
                                    </a>
                                </li>
                                <li id="320">


                                    <a id="bibliography_28" title="Gu K, Zhai G, Yang X, et al.Hybrid No-Reference Quality Metric for Singly and Multiply Distorted Images[J].IEEETransactions on Broadcasting, 2014, 60 (3) :555-567." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Hybrid noreference quality metric for singly and multiply distorted images">
                                        <b>[28]</b>
                                        Gu K, Zhai G, Yang X, et al.Hybrid No-Reference Quality Metric for Singly and Multiply Distorted Images[J].IEEETransactions on Broadcasting, 2014, 60 (3) :555-567.
                                    </a>
                                </li>
                                <li id="322">


                                    <a id="bibliography_29" title="Xue W, Mou X, Zhang L, et al.Blind image quality assessment using joint statistics of gradient magnitude and Laplacian features[J].IEEE Transactions on Image Processing, 2014, 23 (11) :4850-4862." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Blind image quality assessment using joint statistics of gradient magnitude and Laplacian features">
                                        <b>[29]</b>
                                        Xue W, Mou X, Zhang L, et al.Blind image quality assessment using joint statistics of gradient magnitude and Laplacian features[J].IEEE Transactions on Image Processing, 2014, 23 (11) :4850-4862.
                                    </a>
                                </li>
                                <li id="324">


                                    <a id="bibliography_30" title="Saad M A, Bovik A C, Charrier C.Blind image quality assessment:A natural scene statistics approach in the DCT domain[J].IEEE transactions on Image Processing, 2012, 21 (8) :3339-3352." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Blind Image Quality Assessment: A Natural Scene Statistics Approach in the DCT Domain">
                                        <b>[30]</b>
                                        Saad M A, Bovik A C, Charrier C.Blind image quality assessment:A natural scene statistics approach in the DCT domain[J].IEEE transactions on Image Processing, 2012, 21 (8) :3339-3352.
                                    </a>
                                </li>
                                <li id="326">


                                    <a id="bibliography_31" title="Li Q, Lin W, Xu J, et al.Blind image quality assessment using statistical structural and luminance features[J].IEEETransactions on Multimedia, 2016, 18 (12) :2457-2469." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Blind Image Quality Assessment Using Statistical">
                                        <b>[31]</b>
                                        Li Q, Lin W, Xu J, et al.Blind image quality assessment using statistical structural and luminance features[J].IEEETransactions on Multimedia, 2016, 18 (12) :2457-2469.
                                    </a>
                                </li>
                                <li id="328">


                                    <a id="bibliography_32" title="Gu K, Zhai G, Yang X, et al.Using free energy principle for blind image quality assessment[J].IEEE Transactions on Multimedia, 2015, 17 (1) :50-63." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Using free energy principle for blind image quality assessment,&amp;quot;">
                                        <b>[32]</b>
                                        Gu K, Zhai G, Yang X, et al.Using free energy principle for blind image quality assessment[J].IEEE Transactions on Multimedia, 2015, 17 (1) :50-63.
                                    </a>
                                </li>
                                <li id="330">


                                    <a id="bibliography_33" title="Hadizadeh H, Baji?I V.Color Gaussian Jet features for noreference quality assessment of multiply-distorted images[J].IEEE Signal Processing Letters, 2016, 23 (12) :1717-1721." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Color Gaussian Jet Features For No-Reference Quality Assessment of Multiply-Distorted Images">
                                        <b>[33]</b>
                                        Hadizadeh H, Baji?I V.Color Gaussian Jet features for noreference quality assessment of multiply-distorted images[J].IEEE Signal Processing Letters, 2016, 23 (12) :1717-1721.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JYRJ" target="_blank">计算机应用与软件</a>
                2019,36(07),264-272 DOI:10.3969/j.issn.1000-386x.2019.07.045            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于概率矩阵分解的多失真图像质量评估算法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E5%90%8C%E4%B9%90&amp;code=37744513&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王同乐</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E6%85%88&amp;code=34070047&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王慈</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%8D%8E%E4%B8%9C%E5%B8%88%E8%8C%83%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF%E7%B3%BB&amp;code=0092795&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">华东师范大学计算机科学与技术系</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>图像质量评价 (Image Quality Assessment, IQA) 是视觉感知模型研究的重要分支, 使用IQA算法自动化地评估图像质量有广阔的应用前景。基于概率矩阵分解 (Probability Matrix Factorization) 提出一种多失真图像质量评价算法, 主要贡献有:提出一种图像质量评价的新思路, 即利用PMF方法从失真图像估计参考图像, 把失真图像和估计参考图像之间的信息损失作为图像质量的度量;构建新颖的特征向量描述这种信息损失;使用支持向量回归 (Support Vector Regression, SVR) 完成图像质量模型的训练。提出的算法在多个公开的图像质量评价数据库上超过了经典方法, 实验结果证明该方法与人的主观质量评价具有更好的一致性。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%9B%B2%E5%9B%BE%E5%83%8F%E8%B4%A8%E9%87%8F%E8%AF%84%E4%BB%B7&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">盲图像质量评价;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%A6%82%E7%8E%87%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">概率矩阵分解;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E5%9B%9E%E5%BD%92&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">支持向量回归;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%BA%BA%E7%9C%BC%E8%A7%86%E8%A7%89%E7%B3%BB%E7%BB%9F&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">人眼视觉系统;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    王同乐, 硕士生, 主研领域:图像质量评估, 视觉感知模型。;
                                </span>
                                <span>
                                    王慈, 副教授。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-12-04</p>

            </div>
                    <h1><b>MULTI-DISTORTION IMAGE QUALITY ASSESSMENT BASED ON PROBABILISTIC MATRIX FACTORIZATION</b></h1>
                    <h2>
                    <span>Wang Tongle</span>
                    <span>Wang Ci</span>
            </h2>
                    <h2>
                    <span>Department of Computer Science and Technology, East China Normal University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Image quality assessment is an important branch of visual perception modeling research. Using IQA algorithm to automatically evaluate the image quality has broad application prospects. In this paper, we proposed a multiply-distorted IQA method based on probability matrix factorization. The main contributions are as follows: firstly, we came up with a new idea for IQA, which used the PMF method to estimate the reference image from the distorted image, and the information loss between the distorted image and the estimated reference version was considered as a quality measure. Secondly, we constructed a novel feature vector to describe this information loss. Finally, the IQA model was trained with SVR. The proposed method is superior to the classical IQAs in several public image quality assessment databases, which demonstrates our method has the better performance in terms of the coherence with human subjective rating.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Blind%20image%20quality%20assessment&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Blind image quality assessment;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Probabilistic%20matrix%20factorization&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Probabilistic matrix factorization;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Support%20vector%20regression&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Support vector regression;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Human%20visual%20system&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Human visual system;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-12-04</p>
                            </div>


        <!--brief start-->
                        <h3 id="23" name="23" class="anchor-tag"><b>0 引 言</b></h3>
                <div class="p1">
                    <p id="24">图像在采集、压缩、传输和存储过程中会产生各种各样的失真, 失真使得人眼对于图像的视觉观感变差<citation id="332" type="reference"><link href="266" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>, 因此在接收端进行图像质量评价极为重要。图像质量评价有客观和主观之分, 主观图像质量评价采用平均主观意见分或者主观差异评分衡量人的主观视觉感受。虽然主观意见评分合理准确, 但是非常耗时耗力, 因此通过图像质量评价算法进行自动化地客观评分非常必要<citation id="333" type="reference"><link href="268" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>。IQA算法可作为图像处理系统的基本指标, 也可作为图像去噪、增强和修复算法的性能比较标准, 所以图像质量评价算法具有很大的应用和研究价值。</p>
                </div>
                <div class="p1">
                    <p id="25">过去的数十年浮现了大量优秀的IQA算法, 从不同的分类角度概括为以下三类。</p>
                </div>
                <div class="p1">
                    <p id="26">根据是否需要参考图像, 可分为全参考、半参考和无参考质量评价算法。早期著名的全参考算法有SSIM<citation id="334" type="reference"><link href="270" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>和VIF<citation id="335" type="reference"><link href="272" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>等, SSIM基于人眼视觉系统对结构信息敏感的假设, 从亮度、对比度和结构三个方面度量失真图和参考图的相似性。VIF将HVS视为一个退化通道, 通过在小波域中计算互信息比值定义信息保真度。文献<citation id="336" type="reference">[<a class="sup">5</a>]</citation>和文献<citation id="337" type="reference">[<a class="sup">6</a>]</citation>是近期比较优秀的全参考算法, 文献<citation id="338" type="reference">[<a class="sup">5</a>]</citation>基于VIF思想解决了参考图像和失真图像分辨率大小不一致的全参考图像质量评价问题。文献<citation id="339" type="reference">[<a class="sup">6</a>]</citation>将失真视为真实细节损失和虚假细节增加的过程, 利用梯度和梯度残差信息分别对损失的细节和增加的细节加以描述。考虑到全参考算法需要使用参考图像信息的局限性, 文献<citation id="340" type="reference">[<a class="sup">7</a>]</citation>中提出了从使用全部参考信息到几乎不用任何参考信息的半参考算法, 由于半参考算法仍然需要使用参考信息, 所以研究者更多的是设计无参考算法。2012年Mittal等提出了基于自然场景统计的BRISQUE<citation id="341" type="reference"><link href="280" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>, BRISQUE不但具有性能优势而且计算复杂度低, 对无参考算法研究来说具有里程碑意义。总之, 全参考算法评价准确, 但参考图像一般不容易获得;半参考是全参考向无参考算法的过度, 需要使用参考图像的部分信息;无参考算法不需要任何参考信息, 但性能得不到保证。</p>
                </div>
                <div class="p1">
                    <p id="27">根据图像失真类型, IQA算法可分为单失真 (只包含一种降质类型) 和多失真评价算法。近年来多失真IQA研究多于单失真, 因此多失真图像质量评价成为研究热点。 Gu等<citation id="342" type="reference"><link href="282" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>首先提出了多失真图像质量评估的六步法, 受Challer<citation id="343" type="reference"><link href="284" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>的研究启发, 该工作之后引入免能量项以描述不同失真类型之间的相互作用, 但是此类方法针对具体的失真类型, 对未知失真类型效果并不理想。Ghadiyaram等<citation id="344" type="reference"><link href="286" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>为描述失真, 从一组质量相关的感知图中提取了大量统计特征, 虽然该方法在自然多失真数据库上取得了最佳结果, 但计算复杂度较高。Li等<citation id="345" type="reference"><link href="288" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>为了刻画失真对图像带来的结构信息影响, 提出了基于梯度图计算LBP加权直方图的GWH-GLBP方法, GWH-GLBP在合成的多失真数据库上性能优异, 但面对失真因素复杂的自然失真图像性能略显不足。总之, 从实用性角度来看, 单失真质量评估算法针对只包含一种失真类型的图像设计, 使用场景十分有限, 现实生活中的失真图像往往包含了多种失真, 并且各种失真类型之间存在相互作用, 因此多失真评价方法才更加贴合实际应用。</p>
                </div>
                <div class="p1">
                    <p id="28">根据是否需要训练IQA模型, 图像质量评价算法有有训练和无训练之分。随着IQA研究的不断深入, 人们更希望设计出泛化能力较强的无训练IQA算法。文献<citation id="346" type="reference">[<a class="sup">13</a>]</citation>收集了一组无失真图像的自然场景统计特征, 使用多元高斯模型拟合特征参数, 将拟合结果作为无失真图像的理想统计分布, 失真分布与理想分布之间的距离被作为失真度量, 该方法开启了无训练IQA研究的先河。文献<citation id="347" type="reference">[<a class="sup">14</a>]</citation>在文献<citation id="348" type="reference">[<a class="sup">13</a>]</citation>的基础上丰富特征表达, 提出了更具竞争优势的IL-NIQE。文献<citation id="349" type="reference">[<a class="sup">15</a>]</citation>发现原始分辨率图像与其低通/高通版本的不相似性随着原始分辨率图像失真程度的变化而改变, 据此提出一种多尺度空间下的无训练方法。文献<citation id="350" type="reference">[<a class="sup">11</a>]</citation>为了同时适应单失真和多失真图像质量评估, 首先判断失真类型, 对不同失真类型预测不同失真参数, 最后聚合失真参数形成质量分数。 总之, 有训练算法需要训练IQA模型, 无训练算法直接输出质量分数, 但无训练算法普遍存在性能瓶颈。</p>
                </div>
                <div class="p1">
                    <p id="29">本文提出一种基于PMF的多失真图像质量评价算法。首先对失真图像进行降噪以去除冗余信息影响, 然后计算降噪图像的平均对比归一化系数 (Mean Subtracted Contrast Normalized coefficients, MSCN) , 从中筛选信息保真度较高的低频MSCN系数块, 并使用PMF方法对其参考块进行估计。失真系数块和估计的参考块符合一定的统计分布, 因此对二者的MSCN系数进行曲线拟合以提取统计特征。为降低特征维度, 对提取的块特征进行了特征聚合, 最后使用聚合特征构建出描述失真图像信息损失的特征向量, 借助SVR完成IQA模型的训练, 图 1给出了算法流程图。本文方法没有使用参考图像, 但需要SVR训练IQA模型, 因此是一种基于训练的无参考图像质量评估算法。</p>
                </div>
                <div class="area_img" id="30">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201907046_030.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 算法流程图" src="Detail/GetImg?filename=images/JYRJ201907046_030.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 算法流程图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201907046_030.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h3 id="31" name="31" class="anchor-tag"><b>1 相关工作</b></h3>
                <h4 class="anchor-tag" id="32" name="32"><b>1.1 图像平均对比归一化系数MSCN</b></h4>
                <div class="p1">
                    <p id="33">Mittal在文献<citation id="351" type="reference">[<a class="sup">8</a>]</citation>中提出了MSCN的概念, 给定一幅图像<i>X</i>, 其MSCN系数通过下式给出:</p>
                </div>
                <div class="p1">
                    <p id="34"><mathml id="35"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>X</mi><mo>^</mo></mover><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mrow><mi>X</mi><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo><mo>-</mo><mi>χ</mi><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo></mrow><mrow><mi>ζ</mi><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo><mo>+</mo><mi>C</mi></mrow></mfrac></mrow></math></mathml>      (1) </p>
                </div>
                <div class="p1">
                    <p id="36">其中:</p>
                </div>
                <div class="p1">
                    <p id="37"><i>χ</i> (<i>x</i>, <i>y</i>) =<mathml id="38"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mo>-</mo><mi>Κ</mi></mrow><mi>Κ</mi></munderover><mspace width="0.25em" /></mstyle><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>l</mi><mo>=</mo><mo>-</mo><mi>L</mi></mrow><mi>L</mi></munderover><mi>ω</mi></mstyle><msub><mrow></mrow><mrow><mi>k</mi><mo>, </mo><mi>l</mi></mrow></msub><mi>X</mi><msub><mrow></mrow><mrow><mi>k</mi><mo>, </mo><mi>l</mi></mrow></msub><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo></mrow></math></mathml>      (2) </p>
                </div>
                <div class="p1">
                    <p id="39"><i>ζ</i> (<i>x</i>, <i>y</i>) =<mathml id="40"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msqrt><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mo>-</mo><mi>Κ</mi></mrow><mi>Κ</mi></munderover><mspace width="0.25em" /></mstyle><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>l</mi><mo>=</mo><mo>-</mo><mi>L</mi></mrow><mi>L</mi></munderover><mi>ω</mi></mstyle><msub><mrow></mrow><mrow><mi>k</mi><mo>, </mo><mi>l</mi></mrow></msub><mo stretchy="false"> (</mo><mi>X</mi><msub><mrow></mrow><mrow><mi>k</mi><mo>, </mo><mi>l</mi></mrow></msub><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo><mo>-</mo><mi>χ</mi><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo><msup><mrow></mrow><mn>2</mn></msup></mrow></msqrt><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml></p>
                </div>
                <div class="p1">
                    <p id="41">式中:<i>x</i>∈{1, 2, …, <i>H</i>}, <i>j</i>∈{1, 2, …, <i>W</i>}是图像像素索引, <i>H</i>和<i>W</i>分别表示图像的高度和宽度;<i>ω</i>={<i>ω</i><sub><i>k</i>, <i>l</i></sub>|<i>k</i>=-<i>K</i>, …, <i>K</i>, <i>l</i>=-<i>L</i>, …, <i>L</i>}是高斯加权窗口;<i>X</i> (<i>x</i>, <i>y</i>) 表示图像<i>X</i>在空间域坐标 (<i>x</i>, <i>y</i>) 点的像素值;<i>C</i>是一个很小的常数, 以防止除数为零;局部均值<i>χ</i> (<i>x</i>, <i>y</i>) 以及标准差<i>ζ</i> (<i>x</i>, <i>y</i>) 分别用来表示图像<i>X</i>在 (<i>x</i>, <i>y</i>) 点的平均亮度以及对比度。</p>
                </div>
                <div class="p1">
                    <p id="42">令<i>θ</i>表示<mathml id="43"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>X</mi><mo>^</mo></mover></math></mathml>的均值, <i>δ</i>表示<mathml id="44"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>X</mi><mo>^</mo></mover></math></mathml>的标准差, 即:</p>
                </div>
                <div class="p1">
                    <p id="45"><mathml id="46"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtext>ϑ</mtext><mo>=</mo><mfrac><mn>1</mn><mrow><mi>Η</mi><mi>W</mi></mrow></mfrac></mrow></math></mathml><mathml id="47"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>x</mi><mo>=</mo><mn>1</mn></mrow><mi>Η</mi></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>y</mi><mo>=</mo><mn>1</mn></mrow><mi>W</mi></munderover><mover accent="true"><mi>X</mi><mo>^</mo></mover></mstyle></mrow></mstyle></mrow></math></mathml> (<i>x</i>, <i>y</i>)      (4) </p>
                </div>
                <div class="p1">
                    <p id="48"><mathml id="49"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>δ</mi><mo>=</mo><msqrt><mrow><mfrac><mn>1</mn><mrow><mi>Η</mi><mi>W</mi><mo>-</mo><mn>1</mn></mrow></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>x</mi><mo>=</mo><mn>1</mn></mrow><mi>Η</mi></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>y</mi><mo>=</mo><mn>1</mn></mrow><mi>W</mi></munderover><mo stretchy="false"> (</mo></mstyle></mrow></mstyle><mover accent="true"><mi>X</mi><mo>^</mo></mover><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>y</mi><mo stretchy="false">) </mo><mo>-</mo><mtext>ϑ</mtext><mo stretchy="false">) </mo><msup><mrow></mrow><mn>2</mn></msup></mrow></msqrt></mrow></math></mathml>      (5) </p>
                </div>
                <div class="p1">
                    <p id="50">本文定义低频MSCN系数取值范围在[ϑ-2<i>δ</i>, ϑ+2<i>δ</i>], 否则为高频MSCN系数。</p>
                </div>
                <h4 class="anchor-tag" id="51" name="51"><b>1.2 概率矩阵分解PMF</b></h4>
                <div class="p1">
                    <p id="52">Ruslan Salakhutdinov在文献<citation id="352" type="reference">[<a class="sup">17</a>]</citation>中提出了PMF方法, 给定观测矩阵<b><i>R</i>, <i>R</i></b>∈<b><i>R</i></b><sup><i>Q</i>×<i>S</i></sup>, 存在一个较小的潜变量<i>D</i>, 使得<b><i>R</i></b>=<b><i>U</i></b><sup>T</sup>×<b><i>V</i></b>, 其中<b><i>U</i></b>∈<b><i>R</i></b><sup><i>D</i>×<i>Q</i></sup>, <b><i>V</i></b>∈<b><i>R</i></b><sup><i>D</i>×<i>S</i></sup>, <b><i>U</i>、<i>V</i></b>为未知的低维矩阵, <b><i>R</i></b>为秩不超过<i>D</i>的低秩矩阵。<b><i>R</i></b>中的观测值受系统噪音影响且存在大量的缺失值, 因此需要根据已有观测数据计算合适的<b><i>U</i>、<i>V</i></b>对<b><i>R</i></b>中的缺失值进行预测。为求解上述矩阵分解问题, 假设<b><i>R</i></b>的条件概率分布如下:</p>
                </div>
                <div class="p1">
                    <p id="53"><mathml id="54"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>p</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">R</mi><mo stretchy="false">|</mo><mi mathvariant="bold-italic">U</mi><mo>, </mo><mi mathvariant="bold-italic">V</mi><mo>, </mo><mi>σ</mi><msubsup><mrow></mrow><mi mathvariant="bold-italic">R</mi><mn>2</mn></msubsup><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∏</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Q</mi></munderover><mrow></mrow></mstyle><mstyle displaystyle="true"><munderover><mo>∏</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>S</mi></munderover><mrow></mrow></mstyle></mrow></math></mathml>[<i>N</i> (<i>R</i><sub><i>ij</i></sub>|<i>g</i> (<b><i>U</i></b><mathml id="55"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mtext>Τ</mtext></msubsup></mrow></math></mathml><b><i>V</i></b><sub><i>j</i></sub>) , <i>σ</i><mathml id="56"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi mathvariant="bold-italic">R</mi><mn>2</mn></msubsup></mrow></math></mathml>) ]<sup><b><i>I</i></b><sup><b><i>R</i></b></sup><sub><i>ij</i></sub></sup>      (6) </p>
                </div>
                <div class="p1">
                    <p id="57">式中:<i>N</i> (<i>x</i>|<i>μ</i>, <i>σ</i><sup>2</sup>) 是均值为<i>μ</i>, 方差为<i>σ</i><sup>2</sup>的高斯概率密度函数, <i>I</i><mathml id="58"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mi>R</mi></msubsup></mrow></math></mathml>为指示函数, <i>R</i><sub><i>ij</i></sub>为缺失值取0, 否则取1, <i>g</i> (<i>x</i>) =1/ (1+e<sup>-<i>x</i></sup>) , 并假设<b><i>U</i></b>和<b><i>V</i></b>有均值为0的高斯先验:</p>
                </div>
                <div class="p1">
                    <p id="59"><i>p</i> (<b><i>U</i></b>|<i>σ</i><mathml id="60"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi mathvariant="bold-italic">U</mi><mn>2</mn></msubsup></mrow></math></mathml>) =<mathml id="61"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munderover><mo>∏</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Q</mi></munderover><mi>Ν</mi></mstyle><mo stretchy="false"> (</mo><mi>U</mi><msub><mrow></mrow><mi>i</mi></msub><mo>∨</mo><mn>0</mn><mo>, </mo><mi>σ</mi><msubsup><mrow></mrow><mi mathvariant="bold-italic">U</mi><mn>2</mn></msubsup><mi>Ι</mi><mo stretchy="false">) </mo></mrow></math></mathml>      (7) </p>
                </div>
                <div class="p1">
                    <p id="62"><b><i>p</i></b> (<b><i>V</i></b>|<i>σ</i><mathml id="63"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi mathvariant="bold-italic">V</mi><mn>2</mn></msubsup></mrow></math></mathml>) =<mathml id="64"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munderover><mo>∏</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>S</mi></munderover><mi>Ν</mi></mstyle><mo stretchy="false"> (</mo><mi>V</mi><msub><mrow></mrow><mi>j</mi></msub><mo>∨</mo><mn>0</mn><mo>, </mo><mi>σ</mi><msubsup><mrow></mrow><mi mathvariant="bold-italic">V</mi><mn>2</mn></msubsup><mi>Ι</mi><mo stretchy="false">) </mo></mrow></math></mathml>      (8) </p>
                </div>
                <div class="p1">
                    <p id="65">根据最大化后验原则:</p>
                </div>
                <div class="p1">
                    <p id="66"><b><i>p</i></b> (<b><i>U</i>, <i>V</i>|<i>R</i></b>, <i>σ</i><mathml id="67"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi mathvariant="bold-italic">R</mi><mn>2</mn></msubsup></mrow></math></mathml>, <i>σ</i><mathml id="68"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi mathvariant="bold-italic">U</mi><mn>2</mn></msubsup></mrow></math></mathml>, <i>σ</i><mathml id="69"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi mathvariant="bold-italic">V</mi><mn>2</mn></msubsup></mrow></math></mathml>) ∝<b><i>p</i></b> (<b><i>R</i>|<i>U</i>, <i>V</i></b>, <i>σ</i><mathml id="70"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi mathvariant="bold-italic">R</mi><mn>2</mn></msubsup></mrow></math></mathml>) ×<b><i>p</i></b> (<b><i>U</i></b>|<i>σ</i><mathml id="71"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi mathvariant="bold-italic">U</mi><mn>2</mn></msubsup></mrow></math></mathml>) ×<b><i>p</i></b> (<b><i>V</i></b>|<i>σ</i><mathml id="72"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi mathvariant="bold-italic">V</mi><mn>2</mn></msubsup></mrow></math></mathml>) =</p>
                </div>
                <div class="p1">
                    <p id="73" class="code-formula">
                        <mathml id="73"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munderover><mo>∏</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Q</mi></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∏</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>S</mi></munderover><mo stretchy="false">[</mo></mstyle></mrow></mstyle><mi>Ν</mi><mo stretchy="false"> (</mo><mi>R</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo stretchy="false">|</mo><mi>g</mi><mo stretchy="false"> (</mo><mi>U</mi><msubsup><mrow></mrow><mi>i</mi><mtext>Τ</mtext></msubsup><mi>V</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mo>, </mo><mi>σ</mi><msubsup><mrow></mrow><mi mathvariant="bold-italic">R</mi><mn>2</mn></msubsup><mo stretchy="false">) </mo><mo stretchy="false">]</mo><msup><mrow></mrow><mrow><mi>Ι</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mi>R</mi></msubsup></mrow></msup><mo>×</mo><mstyle displaystyle="true"><munderover><mo>∏</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Q</mi></munderover><mi>Ν</mi></mstyle><mo stretchy="false"> (</mo><mi>U</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">|</mo><mn>0</mn><mo>, </mo><mi>σ</mi><msubsup><mrow></mrow><mi mathvariant="bold-italic">U</mi><mn>2</mn></msubsup><mi>Ι</mi><mo stretchy="false">) </mo><mo>×</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="74"><mathml id="75"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munderover><mo>∏</mo><mrow><mi mathvariant="bold-italic">j</mi><mo>=</mo><mn>1</mn></mrow><mi mathvariant="bold-italic">S</mi></munderover><mrow></mrow></mstyle></mrow></math></mathml><b><i>N</i> (<i>V</i></b><sub><b><i>j</i></b></sub><citation id="353">|0</citation>, <i>σ</i><mathml id="76"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi mathvariant="bold-italic">V</mi><mn>2</mn></msubsup></mrow></math></mathml><b><i>I</i></b>)      (9) </p>
                </div>
                <div class="p1">
                    <p id="77">取对数得:</p>
                </div>
                <div class="p1">
                    <p id="78"><b>ln<i>p</i></b> (<b><i>U</i>, <i>V</i>|<i>R</i></b>, <i>σ</i><mathml id="79"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi mathvariant="bold-italic">R</mi><mn>2</mn></msubsup></mrow></math></mathml>, <i>σ</i><mathml id="80"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi mathvariant="bold-italic">U</mi><mn>2</mn></msubsup></mrow></math></mathml>, <i>σ</i><mathml id="81"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi mathvariant="bold-italic">V</mi><mn>2</mn></msubsup></mrow></math></mathml>) ∝</p>
                </div>
                <div class="p1">
                    <p id="82" class="code-formula"><mathml id="83"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mo>-</mo><mfrac><mn>1</mn><mrow><mn>2</mn><mi mathvariant="bold-italic">σ</mi><msubsup><mrow></mrow><mi mathvariant="bold-italic">R</mi><mn>2</mn></msubsup></mrow></mfrac></mrow></math></mathml><mathml id="84"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Q</mi></munderover><mrow></mrow></mstyle><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>S</mi></munderover><mrow></mrow></mstyle></mrow></math></mathml></p>
                </div>
                <div class="p1">
                    <p id="85" class="code-formula">
                        <mathml id="85"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>Ι</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mi>R</mi></msubsup><mo stretchy="false"> (</mo><mi>R</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>-</mo><mi>g</mi><mo stretchy="false"> (</mo><mi>U</mi><msubsup><mrow></mrow><mi>i</mi><mtext>Τ</mtext></msubsup><mi>V</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mo stretchy="false">) </mo><msup><mrow></mrow><mn>2</mn></msup><mo>-</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mfrac><mn>1</mn><mrow><mn>2</mn><mi>σ</mi><msubsup><mrow></mrow><mi>U</mi><mn>2</mn></msubsup></mrow></mfrac></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="86"><mathml id="87"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Q</mi></munderover><mi>U</mi></mstyle><msubsup><mrow></mrow><mi>i</mi><mtext>Τ</mtext></msubsup><mi>U</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mfrac><mn>1</mn><mrow><mn>2</mn><mi>σ</mi><msubsup><mrow></mrow><mi>V</mi><mn>2</mn></msubsup></mrow></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>S</mi></munderover><mrow></mrow></mstyle></mrow></math></mathml><i>V</i><mathml id="88"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mtext>Τ</mtext></msubsup></mrow></math></mathml><i>V</i><sub><i>j</i></sub>-</p>
                </div>
                <div class="p1">
                    <p id="89"><mathml id="90"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mrow><mo> (</mo><mrow><mrow><mo> (</mo><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Q</mi></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>S</mi></munderover><mi>Ι</mi></mstyle></mrow></mstyle><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mi mathvariant="bold-italic">R</mi></msubsup></mrow><mo>) </mo></mrow><mi>ln</mi><mi>σ</mi><msubsup><mrow></mrow><mi mathvariant="bold-italic">R</mi><mn>2</mn></msubsup><mo>+</mo><mi>Q</mi><mi>D</mi><mtext>l</mtext><mtext>n</mtext><mi>σ</mi><msubsup><mrow></mrow><mi mathvariant="bold-italic">U</mi><mn>2</mn></msubsup><mo>+</mo><mi>S</mi><mi>D</mi><mtext>l</mtext><mtext>n</mtext><mi>σ</mi><msubsup><mrow></mrow><mi mathvariant="bold-italic">V</mi><mn>2</mn></msubsup></mrow><mo>) </mo></mrow></mrow></math></mathml>+<i>C</i>      (10) </p>
                </div>
                <div class="p1">
                    <p id="91">因为<i>C</i>为常数, <i>I</i><mathml id="92"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mi mathvariant="bold-italic">R</mi></msubsup></mrow></math></mathml>、<i>σ</i><mathml id="93"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi mathvariant="bold-italic">R</mi><mn>2</mn></msubsup></mrow></math></mathml>、<i>σ</i><mathml id="94"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi mathvariant="bold-italic">U</mi><mn>2</mn></msubsup></mrow></math></mathml>, <i>σ</i><mathml id="95"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi mathvariant="bold-italic">V</mi><mn>2</mn></msubsup></mrow></math></mathml>为预设常量, 最大化上述后验, 等价于最小化下述能量函数:</p>
                </div>
                <div class="p1">
                    <p id="96" class="code-formula"><mathml id="97"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">E</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">U</mi><mo>, </mo><mi mathvariant="bold-italic">V</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mn>1</mn><mn>2</mn></mfrac></mrow></math></mathml><mathml id="98"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Q</mi></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>S</mi></munderover><mi>Ι</mi></mstyle></mrow></mstyle><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mi mathvariant="bold-italic">R</mi></msubsup><mo stretchy="false"> (</mo><mi>R</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>-</mo><mi>g</mi><mo stretchy="false"> (</mo><mi>U</mi><msubsup><mrow></mrow><mi>i</mi><mtext>Τ</mtext></msubsup><mi>V</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mo stretchy="false">) </mo><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mfrac><mrow><mi>λ</mi><msub><mrow></mrow><mi mathvariant="bold-italic">U</mi></msub></mrow><mn>2</mn></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Q</mi></munderover><mrow></mrow></mstyle></mrow></math></mathml>‖<i>U</i><sub><i>i</i></sub>‖<mathml id="99"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>F</mi><mi>r</mi><mi>o</mi></mrow><mn>2</mn></msubsup></mrow></math></mathml></p>
                </div>
                <div class="p1">
                    <p id="100" class="code-formula">
                        <mathml id="100"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mi>λ</mi><msub><mrow></mrow><mi mathvariant="bold-italic">V</mi></msub></mrow><mn>2</mn></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>S</mi></munderover><mo stretchy="false">∥</mo></mstyle><mi>V</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mrow><mi>F</mi><mi>r</mi><mi>o</mi></mrow><mn>2</mn></msubsup><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="101">式中:<i>λ</i><sub><b><i>U</i></b></sub>=<i>σ</i><mathml id="102"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi mathvariant="bold-italic">R</mi><mn>2</mn></msubsup></mrow></math></mathml>/<i>σ</i><mathml id="103"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi mathvariant="bold-italic">U</mi><mn>2</mn></msubsup></mrow></math></mathml>, <i>λ</i><sub><b><i>V</i></b></sub>=<i>σ</i><mathml id="104"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi mathvariant="bold-italic">R</mi><mn>2</mn></msubsup></mrow></math></mathml>/<i>σ</i><mathml id="105"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi mathvariant="bold-italic">V</mi><mn>2</mn></msubsup></mrow></math></mathml>, ‖·‖<mathml id="106"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi mathvariant="bold">F</mi><mi mathvariant="bold">r</mi><mi mathvariant="bold">o</mi></mrow><mn>2</mn></msubsup></mrow></math></mathml>表示<b>Frobenius</b>范数。一般采用随机梯度下降算法求解上述目标函数, 求得最优的 (<b><i>U</i>, <i>V</i></b>) 就可以对<b><i>R</i></b>中的缺失值进行估计, 本为采用文献<citation id="354" type="reference">[<a class="sup">18</a>]</citation>实现的鲁棒概率矩阵分解。</p>
                </div>
                <h3 id="107" name="107" class="anchor-tag"><b>2 基本动机</b></h3>
                <div class="p1">
                    <p id="108">高清无失真的参考图像通常被认为不含有任何失真类型, 但现实中的失真图像往往受多种失真因素影响, 相比参考图像而言, 这些失真类型对图像造成的失真效果可以概括为两类:① 造成了信息损失;② 增加了冗余信息<citation id="355" type="reference"><link href="302" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>。比如模糊通常造成细节信息的损失, 噪声导致冗余信息的增加, Jpeg压缩引入了模糊和块效应, 块效应造成的一系列不连续边缘点也可以视为一种冗余噪声。本文打算去除冗余信息影响, 得到仅包含信息损失的图像并从中估计其已损失的信息, 这与PMF的思想恰好一致。PMF假设观测矩阵存在大量信息缺失, 通过矩阵分解方法可以预测缺失信息。</p>
                </div>
                <div class="p1">
                    <p id="109">研究表明图像被失真破坏后, 其MSCN系数受到影响<citation id="356" type="reference"><link href="280" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>, 图 2证实了这种现象。图 2 (a) 为一幅参考图像, 图 2 (b) 为图 2 (a) 经过噪声和模糊破坏的多失真图像, 图 2 (d) 为图 2 (a) 和图 2 (b) 的MSCN系数分布, 从图 2 (d) 可以看出参考图像的MSCN系数被改变, 但难以辨别哪一部分是由信息损失造成的影响。因此, 对图 2 (c) 去除了图 2 (b) 中叠加的冗余噪声, 得到仅受模糊影响的信息损失图像, 为了从信息损失图像MSCN系数中寻找信息损失较少的系数, 以便使用这些系数进行精确的估计, 我们将信息损失图像的MSCN系数划分成低频和高频进行详细分析。</p>
                </div>
                <div class="p1">
                    <p id="110">图 2 (e) 为信息损失图像和参考图像高频MSCN系数分布, 图 2 (f) 为低频MSCN系数分布, 其中高频系数是指在[-2<i>δ</i>, 2<i>δ</i>]之外的系数, 低频与之相反, <i>δ</i>为信息损失图像MSCN系数标准差。为了便于观察, 图 2 (e) 中的高频系数取了绝对值, 通过采用互信作为度量, 图 2 (e) 中两个分布的互信息为0.13, 图 2 (f) 中互信息为0.72, 这证实了信息损失图像低频MSCN系数信息保真度较高。</p>
                </div>
                <div class="area_img" id="114">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201907046_11400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 图像失真对MSCN的影响" src="Detail/GetImg?filename=images/JYRJ201907046_11400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 图像失真对MSCN的影响  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201907046_11400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="114">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201907046_11401.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 图像失真对MSCN的影响" src="Detail/GetImg?filename=images/JYRJ201907046_11401.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 图像失真对MSCN的影响  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201907046_11401.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="115">综上, 一个自然的想法就是从信息损失图像中寻找信息保真度较高的低频MSCN系数对其参考MSCN系数进行估计, 然后根据失真图像MSCN系数相对于其参考MSCN系数信息损失程度对图像进行质量评估。</p>
                </div>
                <h3 id="116" name="116" class="anchor-tag"><b>3 方法设计</b></h3>
                <h4 class="anchor-tag" id="117" name="117"><b>3.1 预处理</b></h4>
                <div class="p1">
                    <p id="118">多失真图像中包含的冗余噪声信息可以通过降噪去除, 但降噪过程不能引入新的低频成份, 否则信息损失图像原本的低频MSCN系数受到影响。经查阅文献, 近年来优秀的去噪方法BM3D<citation id="357" type="reference"><link href="304" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>满足要求, BM3D结合空域和频域去噪方法, 在去除噪声的同时对图像原有的低频系数保留较好, 能够取得不错的效果, 因此本文在去噪步骤采用BM3D算法。</p>
                </div>
                <div class="p1">
                    <p id="119">去噪后得到信息损失图像, 信息损失图像的MSCN系数采用式 (1) 计算, 从式 (1) 可以看出信息损失图像的MSCN系数和失真图像具有相同的尺寸, 对整个MSCN系数直接使用PMF方法存在高维分解困难, 并且计算复杂度较高, 所以采用分块估计方法。</p>
                </div>
                <h4 class="anchor-tag" id="120" name="120"><b>3.2 PMF估计参考块</b></h4>
                <div class="p1">
                    <p id="121">对于一幅失真图像<i>X</i>, 其MSCN系数用<mathml id="122"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>X</mi><mo>^</mo></mover></math></mathml>表示, 其去噪后的获得的信息损失图像为<i>Y</i>, <i>Y</i>的MSCN系数用<mathml id="123"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>Y</mi><mo>^</mo></mover></math></mathml>表示, 将<mathml id="124"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>X</mi><mo>^</mo></mover></math></mathml>和<mathml id="125"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>Y</mi><mo>^</mo></mover></math></mathml>分解为同样大小的<i>n</i>个小块, <mathml id="126"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>X</mi><mo>^</mo></mover><mo>=</mo><mo stretchy="false">{</mo><mover accent="true"><mi>R</mi><mo>^</mo></mover><msub><mrow></mrow><mrow><mi>x</mi><mn>1</mn></mrow></msub><mo>, </mo><mover accent="true"><mi>R</mi><mo>^</mo></mover><msub><mrow></mrow><mrow><mi>x</mi><mn>2</mn></mrow></msub><mo>, </mo><mo>⋯</mo><mo>, </mo><mover accent="true"><mi>R</mi><mo>^</mo></mover><msub><mrow></mrow><mrow><mi>x</mi><mi>n</mi></mrow></msub><mo stretchy="false">}</mo><mo>, </mo><mover accent="true"><mi>Y</mi><mo>^</mo></mover><mo>=</mo><mo stretchy="false">{</mo><mover accent="true"><mi>R</mi><mo>^</mo></mover><msub><mrow></mrow><mrow><mi>y</mi><mn>1</mn></mrow></msub><mo>, </mo><mover accent="true"><mi>R</mi><mo>^</mo></mover><msub><mrow></mrow><mrow><mi>y</mi><mn>2</mn></mrow></msub><mo>, </mo><mo>⋯</mo><mo>, </mo><mover accent="true"><mi>R</mi><mo>^</mo></mover><msub><mrow></mrow><mrow><mi>y</mi><mi>n</mi></mrow></msub><mo stretchy="false">}</mo></mrow></math></mathml>, 根据以下原则从<mathml id="127"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>Y</mi><mo>^</mo></mover></math></mathml>中筛选出低频系数块, 用于后续PMF分解。</p>
                </div>
                <div class="area_img" id="128">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JYRJ201907046_12800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="130">式中:<mathml id="131"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtext>ϑ</mtext><msub><mrow></mrow><mrow><mover accent="true"><mtext>R</mtext><mo>^</mo></mover><msub><mrow></mrow><mtext>y</mtext></msub></mrow></msub></mrow></math></mathml>为<mathml id="132"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mtext>R</mtext><mo>^</mo></mover><msub><mrow></mrow><mtext>y</mtext></msub></mrow></math></mathml>为<mathml id="133"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mtext>R</mtext><mo>^</mo></mover><msub><mrow></mrow><mtext>y</mtext></msub></mrow></math></mathml>先取绝对值, 再计算平均值, <mathml id="134"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtext>δ</mtext><msub><mrow></mrow><mrow><mover accent="true"><mtext>R</mtext><mo>^</mo></mover><msub><mrow></mrow><mtext>b</mtext></msub></mrow></msub></mrow></math></mathml>为<mathml id="135"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mtext>R</mtext><mo>^</mo></mover><msub><mrow></mrow><mtext>y</mtext></msub></mrow></math></mathml>的标准差。第一个式子保证了低频的要求, 第二个式子保证了块内低频系数分布均匀。</p>
                </div>
                <div class="p1">
                    <p id="136" class="code-formula">
                        <mathml id="136"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mtext>Y</mtext></mstyle><mo>∼</mo></mover><mo>=</mo><mo stretchy="false">{</mo><mover><mstyle mathsize="140%" displaystyle="true"><mtext>R</mtext></mstyle><mo>∼</mo></mover><msub><mrow></mrow><mrow><mtext>y</mtext><mn>1</mn></mrow></msub><mo>, </mo><mover><mstyle mathsize="140%" displaystyle="true"><mtext>R</mtext></mstyle><mo>∼</mo></mover><msub><mrow></mrow><mrow><mtext>y</mtext><mn>2</mn></mrow></msub><mo>, </mo><mo>⋯</mo><mo>, </mo><mover><mstyle mathsize="140%" displaystyle="true"><mtext>R</mtext></mstyle><mo>∼</mo></mover><msub><mrow></mrow><mrow><mtext>y</mtext><mtext>m</mtext></mrow></msub><mo stretchy="false">}</mo><mo>, </mo><mover><mstyle mathsize="140%" displaystyle="true"><mtext>X</mtext></mstyle><mo>∼</mo></mover><mo>=</mo><mo stretchy="false">{</mo><mover><mstyle mathsize="140%" displaystyle="true"><mtext>R</mtext></mstyle><mo>∼</mo></mover><msub><mrow></mrow><mrow><mtext>x</mtext><mn>1</mn></mrow></msub><mo>, </mo><mover><mstyle mathsize="140%" displaystyle="true"><mtext>R</mtext></mstyle><mo>∼</mo></mover><msub><mrow></mrow><mrow><mtext>x</mtext><mn>2</mn></mrow></msub><mo>, </mo><mo>⋯</mo><mo>, </mo><mover><mstyle mathsize="140%" displaystyle="true"><mtext>R</mtext></mstyle><mo>∼</mo></mover><msub><mrow></mrow><mrow><mtext>x</mtext><mtext>m</mtext></mrow></msub><mo stretchy="false">}</mo><mtext> </mtext><mtext>m</mtext><mo>≤</mo><mtext>n</mtext></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="137">式中:<mathml id="138"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mtext>Y</mtext></mstyle><mo>∼</mo></mover></mrow></math></mathml>表示经过筛选的低频系数块集合, <mathml id="139"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mtext>X</mtext></mstyle><mo>∼</mo></mover></mrow></math></mathml>表示对应于<mathml id="140"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mtext>Y</mtext></mstyle><mo>∼</mo></mover></mrow></math></mathml>的失真系数块集合, m为经过筛选后的块个数, 由于<mathml id="141"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mtext>R</mtext></mstyle><mo>∼</mo></mover><msub><mrow></mrow><mtext>y</mtext></msub></mrow></math></mathml>相对于其参考图像而言, 损失了部分信息, 所以使用<i>PMF</i>方法对<mathml id="142"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mtext>R</mtext></mstyle><mo>∼</mo></mover><msub><mrow></mrow><mtext>y</mtext></msub></mrow></math></mathml>的参考块进行估计。</p>
                </div>
                <div class="p1">
                    <p id="143">根据<i>PMF</i>假设, 存在<mathml id="144"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtext>U</mtext><msub><mrow></mrow><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mtext>R</mtext></mstyle><mo>∼</mo></mover><msub><mrow></mrow><mtext>y</mtext></msub></mrow></msub><mo>∈</mo><mtext>R</mtext><msup><mrow></mrow><mrow><mtext>D</mtext><mo>×</mo><mtext>Q</mtext></mrow></msup><mo>, </mo><mspace width="0.25em" /><mtext>V</mtext><msub><mrow></mrow><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mtext>R</mtext></mstyle><mo>∼</mo></mover><msub><mrow></mrow><mtext>y</mtext></msub></mrow></msub><mo>∈</mo><mtext>R</mtext><msup><mrow></mrow><mrow><mtext>D</mtext><mo>×</mo><mtext>S</mtext></mrow></msup></mrow></math></mathml>, 使得<mathml id="145"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mtext>R</mtext></mstyle><mo>∼</mo></mover><msub><mrow></mrow><mtext>y</mtext></msub><mo>=</mo><mtext>U</mtext><msubsup><mrow></mrow><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mtext>R</mtext></mstyle><mo>∼</mo></mover><msub><mrow></mrow><mtext>y</mtext></msub></mrow><mi>Τ</mi></msubsup><mo>×</mo><mtext>V</mtext><msub><mrow></mrow><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mtext>R</mtext></mstyle><mo>∼</mo></mover><msub><mrow></mrow><mtext>y</mtext></msub></mrow></msub><mo>, </mo><mtext>Q</mtext></mrow></math></mathml>和S分别为<mathml id="146"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mtext>R</mtext></mstyle><mo>∼</mo></mover><msub><mrow></mrow><mtext>y</mtext></msub></mrow></math></mathml>的长和宽, 即块大小, D&lt;&lt;Q为潜变量。根据式 (11) 求解最佳的和U<sub>R<sub>y</sub></sub>和V<sub>R<sub>y</sub></sub>, 但值得注意的是式 (11) 中的g (x) 不再取标准的<i>sigmoid</i>函数, 我们取g (x) =6/ (1+<i>e</i><sup>-x</sup>) -3, 这是因为观察到<i>MSCN</i>系数通常分布在[-3, 3]。</p>
                </div>
                <div class="p1">
                    <p id="147">当得到最佳的<mathml id="148"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtext>U</mtext><msub><mrow></mrow><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mtext>R</mtext></mstyle><mo>∼</mo></mover><msub><mrow></mrow><mtext>y</mtext></msub></mrow></msub></mrow></math></mathml>和<mathml id="149"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtext>V</mtext><msub><mrow></mrow><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mtext>R</mtext></mstyle><mo>∼</mo></mover><msub><mrow></mrow><mtext>y</mtext></msub></mrow></msub></mrow></math></mathml>, 即可对<mathml id="150"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mtext>R</mtext></mstyle><mo>∼</mo></mover><msub><mrow></mrow><mtext>y</mtext></msub></mrow></math></mathml>对应的参考块进行估计。假设<mathml id="151"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mtext>R</mtext></mstyle><mo>∼</mo></mover><msubsup><mrow></mrow><mtext>y</mtext><mo>*</mo></msubsup></mrow></math></mathml>是对应于<mathml id="152"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mtext>R</mtext></mstyle><mo>∼</mo></mover><msubsup><mrow></mrow><mtext>y</mtext><mo>*</mo></msubsup></mrow></math></mathml>的参考块, 则<mathml id="153"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mtext>R</mtext></mstyle><mo>∼</mo></mover><msubsup><mrow></mrow><mtext>y</mtext><mo>*</mo></msubsup><mo>=</mo></mrow><mtext>U</mtext><msubsup><mrow></mrow><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mtext>R</mtext></mstyle><mo>∼</mo></mover><msub><mrow></mrow><mtext>y</mtext></msub></mrow><mi>Τ</mi></msubsup><mo>×</mo><mtext>V</mtext><msub><mrow></mrow><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mtext>R</mtext></mstyle><mo>∼</mo></mover><msub><mrow></mrow><mtext>y</mtext></msub></mrow></msub></mrow></math></mathml>。需要对<mathml id="154"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mtext>Y</mtext></mstyle><mo>∼</mo></mover><mo>=</mo><mo stretchy="false">{</mo><mover><mstyle mathsize="140%" displaystyle="true"><mtext>R</mtext></mstyle><mo>∼</mo></mover><msub><mrow></mrow><mrow><mtext>y</mtext><mn>1</mn></mrow></msub><mo>, </mo><mover><mstyle mathsize="140%" displaystyle="true"><mtext>R</mtext></mstyle><mo>∼</mo></mover><msub><mrow></mrow><mrow><mtext>y</mtext><mn>2</mn></mrow></msub><mo>, </mo><mo>⋯</mo><mo>, </mo><mover><mstyle mathsize="140%" displaystyle="true"><mtext>R</mtext></mstyle><mo>∼</mo></mover><msub><mrow></mrow><mrow><mtext>y</mtext><mtext>m</mtext></mrow></msub><mo stretchy="false">}</mo></mrow></math></mathml>中的所有<mathml id="155"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mtext>R</mtext></mstyle><mo>∼</mo></mover><msub><mrow></mrow><mtext>y</mtext></msub></mrow></math></mathml>做上述估计, 估计完成后得到参考块集合<mathml id="156"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mtext>Y</mtext></mstyle><mo>∼</mo></mover><msup><mrow></mrow><mo>=</mo></msup><mo stretchy="false">{</mo><mover><mstyle mathsize="140%" displaystyle="true"><mtext>R</mtext></mstyle><mo>∼</mo></mover><msubsup><mrow></mrow><mrow><mtext>y</mtext><mn>1</mn></mrow><mo>*</mo></msubsup><mo>, </mo><mover><mstyle mathsize="140%" displaystyle="true"><mtext>R</mtext></mstyle><mo>∼</mo></mover><msubsup><mrow></mrow><mrow><mtext>y</mtext><mn>2</mn></mrow><mo>*</mo></msubsup><mo>, </mo><mo>⋯</mo><mo>, </mo><mover><mstyle mathsize="140%" displaystyle="true"><mtext>R</mtext></mstyle><mo>∼</mo></mover><msubsup><mrow></mrow><mrow><mtext>y</mtext><mtext>m</mtext></mrow><mo>*</mo></msubsup><mo stretchy="false">}</mo></mrow></math></mathml>。图 3给出了一组从<mathml id="157"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mtext>R</mtext></mstyle><mo>∼</mo></mover><msub><mrow></mrow><mtext>y</mtext></msub></mrow></math></mathml>中估计出来的<mathml id="158"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mtext>R</mtext></mstyle><mo>∼</mo></mover><msubsup><mrow></mrow><mtext>y</mtext><mo>*</mo></msubsup></mrow></math></mathml>, 第一列为对应于<mathml id="159"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mtext>R</mtext></mstyle><mo>∼</mo></mover><msub><mrow></mrow><mtext>y</mtext></msub></mrow></math></mathml>的真实参考块, 第二列为<mathml id="160"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mtext>R</mtext></mstyle><mo>∼</mo></mover><msub><mrow></mrow><mtext>y</mtext></msub></mrow></math></mathml>, 第三列为估计的参考块<mathml id="161"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mtext>R</mtext></mstyle><mo>∼</mo></mover><msubsup><mrow></mrow><mtext>y</mtext><mo>*</mo></msubsup></mrow></math></mathml>, 可以看到估计参考块<mathml id="162"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mtext>R</mtext></mstyle><mo>∼</mo></mover><msubsup><mrow></mrow><mtext>y</mtext><mo>*</mo></msubsup></mrow></math></mathml>和真实参考块非常相似, 仅有很少的一部分信息没有被估计出来, 这说明使用<i>PMF</i>方法对参考块的估计十分有效。</p>
                </div>
                <div class="area_img" id="163">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201907046_163.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 PMF对参考块的估计效果" src="Detail/GetImg?filename=images/JYRJ201907046_163.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 PMF对参考块的估计效果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201907046_163.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="164" name="164"><b>3.3 特征提取</b></h4>
                <div class="p1">
                    <p id="165">自然图像MSCN系数分布呈近高斯状, 当被失真影响时, MSCN分布曲线发生扭曲, 不同的失真类型产生不同形状的扭曲。基于上述事实, 我们采用广义高斯分布 (Generalized Gaussian Distribution, GGD) 对MSCN进行拟合, 获取拟合参数作为特征表示。零均值的GGD的概率密度函数如下:</p>
                </div>
                <div class="p1">
                    <p id="166"><mathml id="167"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>f</mi><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>α</mi><mo>, </mo><mi>ρ</mi><msup><mrow></mrow><mn>2</mn></msup><mo stretchy="false">) </mo><mo>=</mo><mfrac><mi>α</mi><mrow><mn>2</mn><mi>β</mi><mi>Γ</mi><mo stretchy="false"> (</mo><mn>1</mn><mo>/</mo><mi>α</mi><mo stretchy="false">) </mo></mrow></mfrac><mrow><mi>exp</mi></mrow><mrow><mo> (</mo><mrow><mo>-</mo><mrow><mo> (</mo><mrow><mfrac><mrow><mo stretchy="false">|</mo><mi>x</mi><mo stretchy="false">|</mo></mrow><mi>β</mi></mfrac></mrow><mo>) </mo></mrow><msup><mrow></mrow><mi>α</mi></msup></mrow><mo>) </mo></mrow></mrow></math></mathml>      (13) </p>
                </div>
                <div class="p1">
                    <p id="168">式中:<i>α</i>为GGD的形状参数, <i>ρ</i><sup>2</sup>为方差;<i>Γ</i> (<i>a</i>) 和<i>β</i>函数的定义如下:</p>
                </div>
                <div class="p1">
                    <p id="169"><mathml id="170"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>β</mi><mo>=</mo><mi>ρ</mi><msqrt><mrow><mfrac><mrow><mi>Γ</mi><mo stretchy="false"> (</mo><mn>1</mn><mo>/</mo><mi>α</mi><mo stretchy="false">) </mo></mrow><mrow><mi>Γ</mi><mo stretchy="false"> (</mo><mn>3</mn><mo>/</mo><mi>α</mi><mo stretchy="false">) </mo></mrow></mfrac></mrow></msqrt></mrow></math></mathml>      (14) </p>
                </div>
                <div class="p1">
                    <p id="171"><i>Γ</i> (<i>a</i>) =∫<sup>∞</sup><sub>0</sub><i>t</i><sup><i>a</i>-1</sup>e<sup>-<i>t</i></sup>d<i>t a</i>&gt;0      (15) </p>
                </div>
                <div class="p1">
                    <p id="172">GGD拟合完成后获得两个特征参数, 分别是<i>α</i>和<i>ρ</i><sup>2</sup>。</p>
                </div>
                <div class="p1">
                    <p id="173">简单地采用GGD形状参数<i>α</i>和方差<i>ρ</i><sup>2</sup>还不足以描述形态各异的MSCN分布, 因此对四个方向的MSCN系数还分别做了非对称广义高斯分布 (Asymmetric Generalized Gaussian Distribution, AGGD) 拟合。四个方向定义如下:</p>
                </div>
                <div class="p1">
                    <p id="174"><i>Hr</i> (<i>x</i>, <i>y</i>) =<i>I</i> (<i>x</i>, <i>y</i>) <i>I</i> (<i>x</i>, <i>y</i>+1)      (16) </p>
                </div>
                <div class="p1">
                    <p id="175"><i>Vr</i> (<i>x</i>, <i>y</i>) =<i>I</i> (<i>x</i>, <i>y</i>) <i>I</i> (<i>x</i>+1, <i>y</i>)      (17) </p>
                </div>
                <div class="p1">
                    <p id="176"><i>Dr</i>1 (<i>x</i>, <i>y</i>) =<i>I</i> (<i>x</i>, <i>y</i>) <i>I</i> (<i>x</i>+1, <i>y</i>+1)      (18) </p>
                </div>
                <div class="p1">
                    <p id="177"><i>Dr</i>2 (<i>x</i>, <i>y</i>) =<i>I</i> (<i>x</i>, <i>y</i>) <i>I</i> (<i>x</i>+1, <i>y</i>-1)      (19) </p>
                </div>
                <div class="p1">
                    <p id="178">式中:<b><i>I</i></b>为待拟合MSCN系数矩阵, <i>I</i> (<i>x</i>, <i>y</i>) 表示坐标为 (<i>x</i>, <i>y</i>) 处的系数, 水平、垂直、主对角和副对角方向 (<i>x</i>, <i>y</i>) 位置的系数<i>Hr</i> (<i>x</i>, <i>y</i>) 、<i>Vr</i> (<i>x</i>, <i>y</i>) 、<i>Dr</i>1 (<i>x</i>, <i>y</i>) 和<i>Dr</i>2 (<i>x</i>, <i>y</i>) 通过上述四个公式获得。使用的拟合函数AGGD概率密度如下:</p>
                </div>
                <div class="p1">
                    <p id="179" class="code-formula">
                        <mathml id="179"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>f</mi><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>v</mi><mo>, </mo><mi>ρ</mi><msubsup><mrow></mrow><mi>l</mi><mn>2</mn></msubsup><mo>, </mo><mi>ρ</mi><msubsup><mrow></mrow><mi>r</mi><mn>2</mn></msubsup><mo stretchy="false">) </mo><mo>=</mo><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mfrac><mi>v</mi><mrow><mo stretchy="false"> (</mo><mi>β</mi><msub><mrow></mrow><mi>l</mi></msub><mo>+</mo><mi>β</mi><msub><mrow></mrow><mi>r</mi></msub><mo stretchy="false">) </mo><mi>Γ</mi><mrow><mo> (</mo><mrow><mfrac><mn>1</mn><mi>v</mi></mfrac></mrow><mo>) </mo></mrow></mrow></mfrac><mi>exp</mi><mrow><mo> (</mo><mrow><mo>-</mo><mrow><mo> (</mo><mrow><mfrac><mrow><mo>-</mo><mi>x</mi></mrow><mrow><mi>β</mi><msub><mrow></mrow><mi>l</mi></msub></mrow></mfrac></mrow><mo>) </mo></mrow><msup><mrow></mrow><mi>v</mi></msup></mrow><mo>) </mo></mrow><mtext> </mtext><mi>x</mi><mo>&lt;</mo><mn>0</mn></mtd></mtr><mtr><mtd><mfrac><mi>v</mi><mrow><mo stretchy="false"> (</mo><mi>β</mi><msub><mrow></mrow><mi>l</mi></msub><mo>+</mo><mi>β</mi><msub><mrow></mrow><mi>r</mi></msub><mo stretchy="false">) </mo><mi>Γ</mi><mrow><mo> (</mo><mrow><mfrac><mn>1</mn><mi>v</mi></mfrac></mrow><mo>) </mo></mrow></mrow></mfrac><mi>exp</mi><mrow><mo> (</mo><mrow><mo>-</mo><mrow><mo> (</mo><mrow><mfrac><mi>x</mi><mrow><mi>β</mi><msub><mrow></mrow><mi>l</mi></msub></mrow></mfrac></mrow><mo>) </mo></mrow><msup><mrow></mrow><mi>v</mi></msup></mrow><mo>) </mo></mrow><mtext> </mtext><mi>x</mi><mo>≥</mo><mn>0</mn></mtd></mtr></mtable></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mn>0</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="180">式中:<mathml id="181"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>β</mi><msub><mrow></mrow><mi>l</mi></msub><mo>=</mo><mi>ρ</mi><msub><mrow></mrow><mi>l</mi></msub><msqrt><mrow><mfrac><mrow><mi>Γ</mi><mrow><mo> (</mo><mrow><mfrac><mn>1</mn><mi>ν</mi></mfrac></mrow><mo>) </mo></mrow></mrow><mrow><mi>Γ</mi><mrow><mo> (</mo><mrow><mfrac><mn>3</mn><mi>ν</mi></mfrac></mrow><mo>) </mo></mrow></mrow></mfrac></mrow></msqrt><mo>, </mo><mi>β</mi><msub><mrow></mrow><mi>r</mi></msub><mo>=</mo><mi>ρ</mi><msub><mrow></mrow><mi>r</mi></msub><msqrt><mrow><mfrac><mrow><mi>Γ</mi><mrow><mo> (</mo><mrow><mfrac><mn>1</mn><mi>ν</mi></mfrac></mrow><mo>) </mo></mrow></mrow><mrow><mi>Γ</mi><mrow><mo> (</mo><mrow><mfrac><mn>3</mn><mi>ν</mi></mfrac></mrow><mo>) </mo></mrow></mrow></mfrac></mrow></msqrt><mo>, </mo><mi>ν</mi></mrow></math></mathml>反映了AGGD分布的形状, <i>ρ</i><mathml id="182"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>l</mi><mn>2</mn></msubsup></mrow></math></mathml>、<i>ρ</i><mathml id="183"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>r</mi><mn>2</mn></msubsup></mrow></math></mathml>分别为AGGD分布的左方差、右方差, 分别控制左右两边的延伸程度。AGGD完成每个方向的拟合后获得4个特征参数, 分别是<i>ν</i>、<i>ρ</i><mathml id="184"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>l</mi><mn>2</mn></msubsup></mrow></math></mathml>、<i>ρ</i><mathml id="185"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>r</mi><mn>2</mn></msubsup></mrow></math></mathml>和<i>η</i>, 其中<i>η</i>为AGGD的均值, <mathml id="186"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>η</mi><mo>=</mo><mo stretchy="false"> (</mo><mi>ρ</mi><msub><mrow></mrow><mi>r</mi></msub><mo>-</mo><mi>ρ</mi><msub><mrow></mrow><mi>l</mi></msub><mo stretchy="false">) </mo><msqrt><mrow><mfrac><mrow><mi>Γ</mi><mrow><mo> (</mo><mrow><mfrac><mn>2</mn><mi>ν</mi></mfrac></mrow><mo>) </mo></mrow></mrow><mrow><mi>Γ</mi><mrow><mo> (</mo><mrow><mfrac><mn>1</mn><mi>ν</mi></mfrac></mrow><mo>) </mo></mrow></mrow></mfrac></mrow></msqrt></mrow></math></mathml>。</p>
                </div>
                <div class="p1">
                    <p id="187">综上, 特征提取过程总共获得18个特征参数, 分别是GGD的两个参数 (<i>α</i>, <i>ρ</i><sup>2</sup>) , AGGD的四个参数 (<i>ν</i>, <i>ρ</i><mathml id="188"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>l</mi><mn>2</mn></msubsup></mrow></math></mathml>, <i>ρ</i><mathml id="189"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>r</mi><mn>2</mn></msubsup></mrow></math></mathml>, <i>η</i>) , AGGD需要拟合四个方向, 所以AGGD获得16个参数。</p>
                </div>
                <div class="p1">
                    <p id="190">PMF过程中分离出来的失真块集合<mathml id="191"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi>X</mi></mstyle><mo>∼</mo></mover><mo>=</mo><mo stretchy="false">{</mo><mover><mstyle mathsize="140%" displaystyle="true"><mi>R</mi></mstyle><mo>∼</mo></mover><msub><mrow></mrow><mrow><mi>x</mi><mn>1</mn></mrow></msub><mo>, </mo><mover><mstyle mathsize="140%" displaystyle="true"><mi>R</mi></mstyle><mo>∼</mo></mover><msub><mrow></mrow><mrow><mi>x</mi><mn>2</mn></mrow></msub><mo>, </mo><mo>⋯</mo><mo>, </mo><mover><mstyle mathsize="140%" displaystyle="true"><mi>R</mi></mstyle><mo>∼</mo></mover><msub><mrow></mrow><mrow><mi>x</mi><mi>m</mi></mrow></msub><mo stretchy="false">}</mo></mrow></math></mathml>和估计参考块集合<mathml id="192"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi>Y</mi></mstyle><mo>∼</mo></mover><msup><mrow></mrow><mo>*</mo></msup><mo>=</mo><mo stretchy="false">{</mo><mover><mstyle mathsize="140%" displaystyle="true"><mi>R</mi></mstyle><mo>∼</mo></mover><msubsup><mrow></mrow><mrow><mi>y</mi><mn>1</mn></mrow><mo>*</mo></msubsup><mo>, </mo><mover><mstyle mathsize="140%" displaystyle="true"><mi>R</mi></mstyle><mo>∼</mo></mover><msubsup><mrow></mrow><mrow><mi>y</mi><mn>2</mn></mrow><mo>*</mo></msubsup><mo>, </mo><mo>⋯</mo><mo>, </mo><mover><mstyle mathsize="140%" displaystyle="true"><mi>R</mi></mstyle><mo>∼</mo></mover><msubsup><mrow></mrow><mrow><mi>y</mi><mi>m</mi></mrow><mo>*</mo></msubsup><mo stretchy="false">}</mo></mrow></math></mathml>都需要进行上述特征拟合。若将MSCN拟合过程记作<mathml id="193"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi>f</mi></mstyle><mo>∼</mo></mover><mo>, </mo></mrow></math></mathml>拟合完成后获得的特征向量记作<mathml id="194"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi mathvariant="bold-italic">f</mi></mstyle><mo>∼</mo></mover><mo stretchy="false"> (</mo><mo>⋅</mo><mo stretchy="false">) </mo></mrow></math></mathml>, 其中<mathml id="195"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi mathvariant="bold-italic">f</mi></mstyle><mo>∼</mo></mover><mo stretchy="false"> (</mo><mo>⋅</mo><mo stretchy="false">) </mo></mrow></math></mathml>为包含18个特征参数的列向量, <mathml id="196"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi>X</mi></mstyle><mo>∼</mo></mover></mrow></math></mathml>和<mathml id="197"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi>Y</mi></mstyle><mo>∼</mo></mover><msup><mrow></mrow><mo>*</mo></msup></mrow></math></mathml>所有块拟合完毕的特征向量矩阵分别用<mathml id="198"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi mathvariant="bold-italic">F</mi></mstyle><mo>∼</mo></mover><msub><mrow></mrow><mi>x</mi></msub></mrow></math></mathml>和<mathml id="199"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi mathvariant="bold-italic">F</mi></mstyle><mo>∼</mo></mover><msubsup><mrow></mrow><mi>y</mi><mo>*</mo></msubsup></mrow></math></mathml>表示, 那么<mathml id="200"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi mathvariant="bold-italic">F</mi></mstyle><mo>∼</mo></mover><msub><mrow></mrow><mi>x</mi></msub><mo>=</mo><mo stretchy="false">[</mo><mover><mstyle mathsize="140%" displaystyle="true"><mi mathvariant="bold-italic">f</mi></mstyle><mo>∼</mo></mover><mo stretchy="false"> (</mo><mover><mstyle mathsize="140%" displaystyle="true"><mi>R</mi></mstyle><mo>∼</mo></mover><msub><mrow></mrow><mrow><mi>x</mi><mn>1</mn></mrow></msub><mo stretchy="false">) </mo><mo>, </mo><mover><mstyle mathsize="140%" displaystyle="true"><mi mathvariant="bold-italic">f</mi></mstyle><mo>∼</mo></mover><mo stretchy="false"> (</mo><mover><mstyle mathsize="140%" displaystyle="true"><mi>R</mi></mstyle><mo>∼</mo></mover><msub><mrow></mrow><mrow><mi>x</mi><mn>2</mn></mrow></msub><mo stretchy="false">) </mo><mo>, </mo><mo>⋯</mo><mo>, </mo><mover><mstyle mathsize="140%" displaystyle="true"><mi mathvariant="bold-italic">f</mi></mstyle><mo>∼</mo></mover><mo stretchy="false"> (</mo><mover><mstyle mathsize="140%" displaystyle="true"><mi>R</mi></mstyle><mo>∼</mo></mover><msub><mrow></mrow><mrow><mi>x</mi><mi>m</mi></mrow></msub><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mo>, </mo><mover><mstyle mathsize="140%" displaystyle="true"><mi mathvariant="bold-italic">F</mi></mstyle><mo>∼</mo></mover><msubsup><mrow></mrow><mi>y</mi><mo>*</mo></msubsup><mo>=</mo><mo stretchy="false">[</mo><mover><mstyle mathsize="140%" displaystyle="true"><mi mathvariant="bold-italic">f</mi></mstyle><mo>∼</mo></mover><mo stretchy="false"> (</mo><mover><mstyle mathsize="140%" displaystyle="true"><mi>R</mi></mstyle><mo>∼</mo></mover><msubsup><mrow></mrow><mrow><mi>y</mi><mn>1</mn></mrow><mo>*</mo></msubsup><mo stretchy="false">) </mo><mo>, </mo><mover><mstyle mathsize="140%" displaystyle="true"><mi mathvariant="bold-italic">f</mi></mstyle><mo>∼</mo></mover><mo stretchy="false"> (</mo><mover><mstyle mathsize="140%" displaystyle="true"><mi>R</mi></mstyle><mo>∼</mo></mover><msubsup><mrow></mrow><mrow><mi>y</mi><mn>2</mn></mrow><mo>*</mo></msubsup><mo stretchy="false">) </mo><mo>, </mo><mo>⋯</mo><mo>, </mo><mover><mstyle mathsize="140%" displaystyle="true"><mi mathvariant="bold-italic">f</mi></mstyle><mo>∼</mo></mover><mo stretchy="false"> (</mo><mover><mstyle mathsize="140%" displaystyle="true"><mi>R</mi></mstyle><mo>∼</mo></mover><msubsup><mrow></mrow><mrow><mi>y</mi><mi>m</mi></mrow><mo>*</mo></msubsup><mo stretchy="false">) </mo><mo stretchy="false">]</mo></mrow></math></mathml>。</p>
                </div>
                <h4 class="anchor-tag" id="201" name="201"><b>3.4 特征池化</b></h4>
                <div class="p1">
                    <p id="202">特征池化是将基于块的特征聚合为整个图的特征, 降低特征维度的方法, 图像质量评估领域常用的特征池化方式有最大值池化<citation id="358" type="reference"><link href="306" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>、平均值池化<citation id="359" type="reference"><link href="308" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>和百分比池化<citation id="360" type="reference"><link href="310" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>等, 本文采用简单的平均值池化原则。基于失真系数块<mathml id="203"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi>R</mi></mstyle><mo>∼</mo></mover><msub><mrow></mrow><mi>x</mi></msub></mrow></math></mathml>和估计参考块<mathml id="204"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi>R</mi></mstyle><mo>∼</mo></mover><msubsup><mrow></mrow><mi>y</mi><mo>*</mo></msubsup></mrow></math></mathml>的特征需要池化为失真图像<i>X</i>和估计参考图像<i>Y</i><sup>*</sup>的特征, 特征池化针对特征向量矩阵<mathml id="205"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi mathvariant="bold-italic">F</mi></mstyle><mo>∼</mo></mover><msub><mrow></mrow><mi>x</mi></msub></mrow></math></mathml>和<mathml id="206"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi mathvariant="bold-italic">F</mi></mstyle><mo>∼</mo></mover><msubsup><mrow></mrow><mi>y</mi><mo>*</mo></msubsup></mrow></math></mathml>分别进行:</p>
                </div>
                <div class="p1">
                    <p id="207"><mathml id="208"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>f</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false"> (</mo><mi>X</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mn>1</mn><mi>m</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi>f</mi></mstyle><mo>∼</mo></mover></mrow></mstyle><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false"> (</mo><mover><mstyle mathsize="140%" displaystyle="true"><mi>R</mi></mstyle><mo>∼</mo></mover><msub><mrow></mrow><mrow><mi>x</mi><mi>j</mi></mrow></msub><mo stretchy="false">) </mo><mtext> </mtext><mi>i</mi><mo>=</mo><mn>1</mn><mo>, </mo><mn>2</mn><mo>, </mo><mo>⋯</mo><mn>1</mn><mn>8</mn><mo>, </mo><mi>j</mi><mo>=</mo><mn>1</mn><mo>, </mo><mn>2</mn><mo>⋯</mo><mo>, </mo><mi>m</mi></mrow></math></mathml>      (21) </p>
                </div>
                <div class="p1">
                    <p id="209"><mathml id="210"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>f</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false"> (</mo><mi>Y</mi><msup><mrow></mrow><mo>*</mo></msup><mo stretchy="false">) </mo><mo>=</mo><mfrac><mn>1</mn><mi>m</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi>f</mi></mstyle><mo>∼</mo></mover></mrow></mstyle><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false"> (</mo><mover><mstyle mathsize="140%" displaystyle="true"><mi>R</mi></mstyle><mo>∼</mo></mover><msubsup><mrow></mrow><mrow><mi>y</mi><mi>j</mi></mrow><mo>*</mo></msubsup><mo stretchy="false">) </mo><mtext> </mtext><mi>i</mi><mo>=</mo><mn>1</mn><mo>, </mo><mn>2</mn><mo>, </mo><mo>⋯</mo><mn>1</mn><mn>8</mn><mo>, </mo><mi>j</mi><mo>=</mo><mn>1</mn><mo>, </mo><mn>2</mn><mo>⋯</mo><mo>, </mo><mi>m</mi></mrow></math></mathml>      (22) </p>
                </div>
                <div class="p1">
                    <p id="211">式中:<mathml id="212"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi>f</mi></mstyle><mo>∼</mo></mover><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false"> (</mo><mover><mstyle mathsize="140%" displaystyle="true"><mi>R</mi></mstyle><mo>∼</mo></mover><msub><mrow></mrow><mrow><mi>x</mi><mi>j</mi></mrow></msub><mo stretchy="false">) </mo></mrow></math></mathml>表示第<i>j</i>个失真块<mathml id="213"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi>R</mi></mstyle><mo>∼</mo></mover><msub><mrow></mrow><mrow><mi>x</mi><mi>j</mi></mrow></msub></mrow></math></mathml>的特征向量第<i>i</i>维, <i>f</i><sub><i>i</i></sub> (<i>X</i>) 表示失真图像<i>X</i>特征向量的第<i>i</i>维, <mathml id="214"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi>f</mi></mstyle><mo>∼</mo></mover><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false"> (</mo><mover><mstyle mathsize="140%" displaystyle="true"><mi>R</mi></mstyle><mo>∼</mo></mover><msubsup><mrow></mrow><mrow><mi>y</mi><mi>j</mi></mrow><mo>*</mo></msubsup><mo stretchy="false">) </mo></mrow></math></mathml>和<i>f</i><sub><i>i</i></sub> (<i>Y</i><sup>*</sup>) 的含义与此同理。经过特征池化, <i>X</i>的特征向量为<i>f</i> (<i>X</i>) , <i>Y</i><sup>*</sup>的特征向量为<i>f</i> (<i>Y</i><sup>*</sup>) 。<i>f</i> (<i>X</i>) 描述了失真图像现有的信息, <i>f</i> (<i>Y</i><sup>*</sup>) 描述其参考图像应有的信息, 因此构建下述特征向量表述失真图像的信息损失:</p>
                </div>
                <div class="p1">
                    <p id="215"><i>v</i><sub><i>X</i></sub>=[<i>f</i> (<i>X</i>) , <i>f</i> (<i>Y</i><sup>*</sup>) -<i>f</i> (<i>X</i>) ]      (23) </p>
                </div>
                <h4 class="anchor-tag" id="216" name="216"><b>3.5 IQA模型训练</b></h4>
                <div class="p1">
                    <p id="217">基于训练的图像质量评价方法需要训练IQA模型, 支持向量回归SVR常被用来作为训练工具<citation id="361" type="reference"><link href="312" rel="bibliography" /><link href="314" rel="bibliography" /><sup>[<a class="sup">24</a>,<a class="sup">25</a>]</sup></citation>。IQA模型获取分为两个阶段, 即训练阶段和测试阶段, 训练阶段使用SVR从训练样本学习IQA模型, 测试阶段使用训练阶段获得的IQA模型进行质量分数预测。</p>
                </div>
                <div class="p1">
                    <p id="218">标准SVR具有如下的损失函数:</p>
                </div>
                <div class="p1">
                    <p id="219" class="code-formula">
                        <mathml id="219"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mrow><mi>w</mi><mo>, </mo><mi>b</mi><mo>, </mo><mi>ξ</mi><mo>, </mo><mi>ξ</mi><msup><mrow></mrow><mo>*</mo></msup></mrow></munder><mfrac><mn>1</mn><mn>2</mn></mfrac><mi>w</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mi>w</mi><mo>+</mo><mi>C</mi><mrow><mo>{</mo><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>l</mi></munderover><mi>ξ</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mo>+</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>l</mi></munderover><mi>ξ</mi></mstyle><msubsup><mrow></mrow><mi>i</mi><mo>*</mo></msubsup></mrow><mo>}</mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mn>4</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="220">subject to <i>w</i><sup>T</sup><i>φ</i> (<i>x</i><sub><i>i</i></sub>) +<i>b</i>-<i>y</i><sub><i>i</i></sub>≤<i>∈</i>+<i>ξ</i><sub><i>i</i></sub></p>
                </div>
                <div class="p1">
                    <p id="221"><i>y</i><sub><i>i</i></sub>-<i>w</i><sup>T</sup><i>φ</i> (<i>x</i><sub><i>i</i></sub>) -<i>b</i>≤<i>∈</i>+<i>ξ</i><mathml id="222"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mo>*</mo></msubsup></mrow></math></mathml></p>
                </div>
                <div class="p1">
                    <p id="223"><i>ξ</i><sub><i>i</i></sub>, <i>ξ</i><mathml id="224"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mo>*</mo></msubsup></mrow></math></mathml>&gt;0 <i>i</i>=1, 2, …, <i>l</i></p>
                </div>
                <div class="p1">
                    <p id="225">其中:<i>x</i><sub><i>i</i></sub>表示第<i>i</i>个样本的特征向量, <i>y</i><sub><i>i</i></sub>表示第<i>i</i>个样本的标签, 常量<i>C</i>&gt;0, <i>∈</i>&gt;0, <i>K</i> (<i>x</i><sub><i>i</i></sub>, <i>x</i><sub><i>j</i></sub>) 表示核函数。</p>
                </div>
                <div class="p1">
                    <p id="226"><i>K</i> (<i>x</i><sub><i>i</i></sub>, <i>x</i><sub><i>j</i></sub>) =ϕ (<i>x</i><sub><i>i</i></sub>) <sup>T</sup>ϕ (<i>x</i><sub><i>j</i></sub>)      (25) </p>
                </div>
                <div class="p1">
                    <p id="227">一般采用径向基核函数RBF (Radial Basis Function) :</p>
                </div>
                <div class="p1">
                    <p id="228"><i>K</i> (<i>x</i><sub><i>i</i></sub>, <i>x</i><sub><i>j</i></sub>) =exp (-<i>γ</i>‖<i>x</i><sub><i>i</i></sub>-<i>x</i><sub><i>j</i></sub>‖<sup>2</sup>)      (26) </p>
                </div>
                <div class="p1">
                    <p id="229">在我们的训练过程, 特征聚合阶段获取的第<i>i</i>幅图像的特征向量<i>v</i><sub><i>Xi</i></sub>对应于标准SVR的<i>x</i><sub><i>i</i></sub>, 失真图像的DMOS或者MOS值对应于标准SVR的<i>y</i><sub><i>i</i></sub>。本文采用LibSVM<citation id="362" type="reference"><link href="316" rel="bibliography" /><sup>[<a class="sup">26</a>]</sup></citation>中实现的SVR, 选择RBF作为核函数完成上述IQA模型的训练。</p>
                </div>
                <h3 id="230" name="230" class="anchor-tag"><b>4 实 验</b></h3>
                <h4 class="anchor-tag" id="231" name="231"><b>4.1 实验数据库和评价标准</b></h4>
                <div class="p1">
                    <p id="232">著名的多失真图像质量评价数据集有MLIVE<citation id="363" type="reference"><link href="318" rel="bibliography" /><sup>[<a class="sup">27</a>]</sup></citation>、MDID2013<citation id="364" type="reference"><link href="320" rel="bibliography" /><sup>[<a class="sup">28</a>]</sup></citation>和The Live Challenge Database<citation id="365" type="reference"><link href="286" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>。MLIVE总共有465幅图像, 其中参考图像15幅, 其余450幅是这15幅图像的降质版本。MD总共分为两个Part。Part1有225幅降质图像, 它是由15幅参考图像先进行高斯模糊, 后使用Jpeg压缩产生。Part2也有225幅图像, 它是由15幅参考图像先进行高斯模糊, 后叠加高斯白噪声产生。MDID2013包含了348幅图像, 其中参考图像12幅, 其余336幅为失真图像, 失真图像是由参考图像依次经过高斯模糊, JPEG压缩和高斯白噪声破坏所得, MDID2013和MLIVE最重要的区别是MDID2013每幅失真图像包含了三种失真类型, 而MLIVE每幅失真图像仅包含两种失真类型。The Live Challenge Database是第一个真实失真图像质量评价数据库, 它包含了1 169幅失真图像, 不同于前两个质量评价数据库, 它没有参考图像, 而且The Live Challenge Database的失真图像都来自于自然界, 属于自然退化导致的失真, 失真图像包含了复杂的降质类型, 因此The Live Challenge Database对图像质量评价算法提出了新的挑战。</p>
                </div>
                <div class="p1">
                    <p id="233">所有的图像质量评价数据库都提供了失真图像的质量分数值, 质量分数是由图像方面的专家经过合理的打分程序给出的MOS或者DMOS分数, MOS分数值越大, 图像越清晰, 视觉效果越好, 反之亦然, 需要注意的是MOS和DMOS的含义恰好相反。通过计算IQA算法给出的图像质量分数和图像专家给出的MOS或DMOS分数值之间的相关性, 就可以衡量图像质量评价算法的性能。</p>
                </div>
                <div class="p1">
                    <p id="234">IQA算法常用性能评价标准有斯皮尔曼等级相关系数、皮尔逊线性相关系数和均方根误差, 后两个评价标准需要将算法给出的质量分数经过非线性拟合再进行计算, 常用的5参数拟合函数如下:</p>
                </div>
                <div class="p1">
                    <p id="235"><mathml id="236"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>f</mi><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mo>=</mo><mi>γ</mi><msub><mrow></mrow><mn>1</mn></msub><mrow><mo> (</mo><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo>-</mo><mfrac><mn>1</mn><mrow><mi>exp</mi><mo stretchy="false"> (</mo><mi>γ</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false"> (</mo><mi>x</mi><mo>-</mo><mi>γ</mi><msub><mrow></mrow><mn>3</mn></msub><mo stretchy="false">) </mo><mo stretchy="false">) </mo></mrow></mfrac></mrow><mo>) </mo></mrow><mo>+</mo><mi>γ</mi><msub><mrow></mrow><mn>4</mn></msub><mi>x</mi><mo>+</mo><mi>γ</mi><msub><mrow></mrow><mn>5</mn></msub></mrow></math></mathml>      (27) </p>
                </div>
                <div class="p1">
                    <p id="237">式中:{<i>γ</i><sub>1</sub>, <i>γ</i><sub>2</sub>, <i>γ</i><sub>3</sub>, <i>γ</i><sub>4</sub>, <i>γ</i><sub>5</sub>}为拟合参数, 不同的质量评价数据库有不同的拟合参数, 需要训练获得。</p>
                </div>
                <h4 class="anchor-tag" id="238" name="238"><b>4.2 实验细节</b></h4>
                <div class="p1">
                    <p id="239">基于训练的IQA算法需要将图像库划分为训练集和测试集, 通常80%的图像用来训练IQA模型, 20%的图像用来测试, 训练集和测试集不得重叠。本文对各个图像质量评价数据库的测试结果遵守上述实验原则, 为了全面测试IQA的泛化能力, 我们每一次测试都将数据库按照80%训练, 20%测试进行划分, 并且计算SROCC、PLCC、RMSE三个指标, 实验结果最终报告的是三种指标100次的平均值。</p>
                </div>
                <div class="p1">
                    <p id="240">另外, 本文需要讨论的是算法中的3个可变参数, 它们分别是分块的大小<i>Q</i>和<i>S</i>, PMF方法分解潜变量<i>D</i>。在我们的实现中取<i>Q</i>=<i>S</i>, 所以可变参数只有<i>Q</i>和<i>D</i>, 并且PMF分解潜变量<i>D</i>&lt;&lt;<i>Q</i>, 根据文献<citation id="366" type="reference">[<a class="sup">17</a>]</citation>的建议, <i>D</i>的取值为5～10的整数为佳, 太大的<i>D</i>将会造成过拟合现象, 为了寻找最优的<i>D</i>值, 我们将窗口的大小设为潜变量<i>D</i>的10倍, 即<i>Q</i>=10<i>D</i>。MLIVE数据库被用来调试<i>D</i>值, 采用100次的平均SROCC值作为度量。图4给出了SROCC值随<i>D</i>值的变化情况, 当<i>D</i>=8时, SROCC取得最大值。在后续其他数据库的实验中, 我们并没有再对<i>D</i>值进行调试, 固定取<i>D</i>=8、<i>Q</i>=<i>S</i>=80。</p>
                </div>
                <div class="area_img" id="241">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201907046_241.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 PMF分解潜变量D值选择" src="Detail/GetImg?filename=images/JYRJ201907046_241.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 PMF分解潜变量D值选择  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201907046_241.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="242" name="242"><b>4.3 实验结果</b></h4>
                <div class="p1">
                    <p id="243">经典的盲图像质量评价算法有BRISQUE<citation id="367" type="reference"><link href="280" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>、NIQE<citation id="368" type="reference"><link href="290" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>、ILNIQE<citation id="369" type="reference"><link href="292" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>、GMLOG<citation id="370" type="reference"><link href="322" rel="bibliography" /><sup>[<a class="sup">29</a>]</sup></citation>、BLIINDS2<citation id="371" type="reference"><link href="324" rel="bibliography" /><sup>[<a class="sup">30</a>]</sup></citation>、GWH-GLBP<citation id="372" type="reference"><link href="288" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>、NRSL<citation id="373" type="reference"><link href="326" rel="bibliography" /><sup>[<a class="sup">31</a>]</sup></citation>、FRIQUEE<citation id="374" type="reference"><link href="286" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>、NFERM<citation id="375" type="reference"><link href="328" rel="bibliography" /><sup>[<a class="sup">32</a>]</sup></citation>和COLORJET<citation id="376" type="reference"><link href="330" rel="bibliography" /><sup>[<a class="sup">33</a>]</sup></citation>。其中BRISQUE、NIQE、ILNIQE、GMLOG、BLIINDS2、NRSL和NFERM为单失真评价算法, GWH-GLBP、FRIQUEE和COLORJET为多失真评价算法。表1-表3给出了本文算法和其他10种图像质量评价算法在MLIVE、MDID2013和The Live Challenge Database上的性能比较结果。</p>
                </div>
                <div class="area_img" id="244">
                    <p class="img_tit"><b>表1 MLIVE数据库性能比较结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="244" border="1"><tr><td rowspan="2"><br />算法</td><td colspan="4"><br />MLIVE</td></tr><tr><td><br />SROCC</td><td>PLCC</td><td>RMSE</td><td>T-test</td></tr><tr><td><br />BRISQUE</td><td>0.904 9</td><td>0.920 1</td><td>7.180 4</td><td>1</td></tr><tr><td><br />NIQE</td><td>0.772 5</td><td>0.837 7</td><td>10.329 2</td><td>1</td></tr><tr><td><br />ILNIQE</td><td>0.906 6</td><td>0.908 7</td><td>7.664 2</td><td>1</td></tr><tr><td><br />GMLOG</td><td>0.247 5</td><td>0.292 5</td><td>18.084 7</td><td>1</td></tr><tr><td><br />BLIINDS2</td><td>0.891 0</td><td>0.884 4</td><td>7.792 6</td><td>1</td></tr><tr><td><br />NRSL</td><td>0.932 3</td><td>0.946 2</td><td>5.943 6</td><td>1</td></tr><tr><td><br />NFERM</td><td>0.893 3</td><td>0.897 4</td><td>7.691 8</td><td>1</td></tr><tr><td><br />FRIQUEE</td><td><b><i>0</i>.<i>954 2</i></b></td><td>0.949 1</td><td>5.867 1</td><td>1</td></tr><tr><td><br />GWH-GLBP</td><td>0.943 7</td><td>0.949 4</td><td>5.866 0</td><td>1</td></tr><tr><td><br />COLORJET</td><td>0.953 3</td><td><b><i>0</i>.<i>956 1</i></b></td><td><b><i>5</i>.<i>510 0</i></b></td><td>1</td></tr><tr><td><br />本文方法</td><td><b><i>0</i>.<i>960 1</i></b></td><td><b><i>0</i>.<i>964 8</i></b></td><td><b><i>5</i>.<i>493 2</i></b></td><td>0</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="245">
                    <p class="img_tit"><b>表2 MDID2013数据库性能比较结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="245" border="1"><tr><td rowspan="2"><br />算法</td><td colspan="4"><br />MLIVE</td></tr><tr><td><br />SROCC</td><td>PLCC</td><td>RMSE</td><td>T-test</td></tr><tr><td><br />BRISQUE</td><td>0.682 6</td><td>0.667 5</td><td>0.036 5</td><td>1</td></tr><tr><td><br />NIQE</td><td>0.545 0</td><td>0.576 7</td><td>0.041 5</td><td>1</td></tr><tr><td><br />ILNIQE</td><td>0.700 3</td><td>0.700 1</td><td>0.034 9</td><td>1</td></tr><tr><td><br />GMLOG</td><td>0.535 4</td><td>0.547 0</td><td>0.042 5</td><td>1</td></tr><tr><td><br />BLIINDS2</td><td>0.798 6</td><td>0.799 1</td><td>0.030 8</td><td>1</td></tr><tr><td><br />NRSL</td><td>0.875 2</td><td>0.886 7</td><td>0.021 1</td><td>1</td></tr><tr><td><br />NFERM</td><td>0.843 3</td><td>0.855 2</td><td>0.029 5</td><td>1</td></tr><tr><td><br />FRIQUEE</td><td>0.919 8</td><td>0.912 7</td><td>0.018 5</td><td>1</td></tr><tr><td><br />GWH-GLBP</td><td>0.896 7</td><td>0.912 1</td><td>0.019 7</td><td>1</td></tr><tr><td><br />COLORJET</td><td><b><i>0</i>.<i>921 1</i></b></td><td><b><i>0</i>.<i>920 3</i></b></td><td><b><i>0</i>.<i>016 4</i></b></td><td>1</td></tr><tr><td><br />本文方法</td><td><b><i>0</i>.<i>932 1</i></b></td><td><b><i>0</i>.<i>934 8</i></b></td><td><b><i>0</i>.<i>012 5</i></b></td><td>0</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="246">
                    <p class="img_tit"><b>表3 The Live Challenge Database数据库性能比较结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="246" border="1"><tr><td rowspan="2"><br />算法</td><td colspan="4"><br />MLIVE</td></tr><tr><td><br />SROCC</td><td>PLCC</td><td>RMSE</td><td>T-test</td></tr><tr><td><br />BRISQUE</td><td>0.611 2</td><td>0.645 0</td><td>15.490 1</td><td>1</td></tr><tr><td><br />NIQE</td><td>0.447 3</td><td>0.504 4</td><td>17.361 3</td><td>1</td></tr><tr><td><br />ILNIQE</td><td>0.442 1</td><td>0.521 8</td><td>17.271 4</td><td>1</td></tr><tr><td><br />GMLOG</td><td>0.594 4</td><td>0.628 2</td><td>15.744 1</td><td>1</td></tr><tr><td><br />BLIINDS2</td><td>0.120 6</td><td>0.216 6</td><td>19.528 0</td><td>1</td></tr><tr><td><br />NRSL</td><td>0.616 2</td><td>0.647 2</td><td>15.175 8</td><td>1</td></tr><tr><td><br />NFERM</td><td>0.577 1</td><td>0.616 4</td><td>15.979 1</td><td>1</td></tr><tr><td><br />FRIQUEE</td><td><b><i>0</i>.<i>687 4</i></b></td><td><b><i>0</i>.<i>715 1</i></b></td><td><b><i>0</i>.<i>792 8</i></b></td><td>0</td></tr><tr><td><br />GWH-GLBP</td><td>0.566 3</td><td>0.608 2</td><td>16.026 3</td><td>1</td></tr><tr><td><br />COLORJET</td><td>0.605 1</td><td>0.649 9</td><td>15.463 8</td><td>1</td></tr><tr><td><br />本文方法</td><td><b><i>0</i>.<i>658 8</i></b></td><td><b><i>0</i>.<i>732 3</i></b></td><td><b><i>13</i>.<i>231 1</i></b></td><td>0</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="247">实验结果对SROCC、PLCC和RMSE各自排名前二的算法采用斜黑体表示, 从实验结果可以看出:本文方法在MLIVE数据库上超过了所有算法, SROCC超出近年来多失真经典算法GWH-GLBP1.7%, 相比最新的FRIQUEE也表现出更好的性能。所有算法在MDID2013数据库上的测试性能都有所下降, 因为MDID2013图像失真类型为Blur+Noise+Jpeg, 比MLIVE失真要复杂, 但本文方法仍然保持了良好的性能优势, 各项指标均排据第一。The Live Challenge Database上所有算法性能急剧下降, 其主要原因是The Live Challenge Database为自然降质数据集, 完全是由自然因素导致的图像降质, 不再是可预测的常见降质类型。虽然如此, 本文方法和经典算法相比依然性能靠前, 比GWH-GLBP提高了16%, 比在前两个数据库上性能较好的COLORJET提高了8.3%。FRIQUEE比本文方法出色是因为它从各种感知相关的映射图上提取了多达560维的特征向量 (本文仅36维) , 它能捕捉的失真更加广泛, 但它比本文方法在时间复杂度上要高出很多。</p>
                </div>
                <div class="p1">
                    <p id="248">为了进一步说明我们算法的优越性, 表1-表3还给出了100次SROCC的T检验的结果, T检验的备择假设是本文方法的平均SROCC相关性大于对比算法, T检验结果“1”表示以95%的置信度断定本文算法平均SROCC相关性大于比较算法, “0”表示两种算法性能在统计上不可区分。T检验结果表明了我们提出的算法在性能上比其他竞争算法具有更好的稳定性。</p>
                </div>
                <h3 id="249" name="249" class="anchor-tag"><b>5 结 语</b></h3>
                <div class="p1">
                    <p id="250">本文提出了一种基于概率矩阵分解的多失真图像质量评估算法。从估计参考图像的角度出发, 首先从多失真图像中去除冗余信息影响, 得到信息损失图像, 从中分离其信息保真度较高的低频MSCN系数块。使用PMF方法对低频MSCN系数块对应的参考块进行估计, 通过对失真系数块和估计的参考块抽取统计特征, 构建失真图像的特征描述。最后使用SVR完成质量分数预测模型的训练。三个公开数据库的实验结果证明了本文方法的有效性, 同时证明本文提出的方法与人眼视觉质量评价具有更好的一致性。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="266">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJA200807002&amp;v=MTM3MzBadEZ5amhVN3pKTHo3QmI3RzRIdG5NcUk5RlpvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnU=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b>周景超, 戴汝为, 肖柏华.图像质量评价研究综述[J].计算机科学, 2008, 35 (7) :1-4.
                            </a>
                        </p>
                        <p id="268">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201411060&amp;v=MDI4MjRaTEc0SDlYTnJvOURaSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnRGeWpoVTd6Skx6VFo=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b>李爱华.基于相位的结构相似度图像质量评价模型[J].计算机应用与软件, 2014, 31 (11) :233-236.
                            </a>
                        </p>
                        <p id="270">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image quality assessment: from error visibility to structural similarity">

                                <b>[3]</b>Wang Z, Bovik A C, Sheikh H R, et al.Image Quality Assessment:From Error Visibility to Structural Similarity[J].IEEE Transactions on Image Processing, 2004, 13 (4) :600-612.
                            </a>
                        </p>
                        <p id="272">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image information and visual quality">

                                <b>[4]</b>Sheikh H R, Bovik A C.Image information and visual quality[J].IEEE Transactions on Image Processing, 2006, 15 (2) :430-444.
                            </a>
                        </p>
                        <p id="274">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Full-Reference Quality Estimation for Images With Different Spatial Resolutions">

                                <b>[5]</b>Demirtas A M, Reibman A R, Jafarkhani H.Full-Reference Quality Estimation for Images With Different Spatial Resolutions[J].Image Processing IEEE Transactions on, 2014, 23 (5) :2069-2080.
                            </a>
                        </p>
                        <p id="276">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Detail Based Method for Linear Full Reference Image Quality Prediction">

                                <b>[6]</b>Di Claudio E D, Jacovitti G.A Detail Based Method for Linear Full Reference Image Quality Prediction[J].IEEE Trans Image Process, 2018, 27 (1) :179-193.
                            </a>
                        </p>
                        <p id="278">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=RRED Indices: Reduced Reference Entropic Differencing forImage Quality Assessment">

                                <b>[7]</b>Soundararajan R, Bovik A C.RRED Indices:Reduced Reference Entropic Differencing for Image Quality Assessment[J].IEEE Transactions on Image Processing A Publication of the IEEE Signal Processing Society, 2012, 21 (2) :517-526.
                            </a>
                        </p>
                        <p id="280">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=No-reference image quality assessment in the spatial domain">

                                <b>[8]</b>Mittal A, Moorthy A K, Bovik A C.No-Reference Image Quality Assessment in the Spatial Domain[J].IEEE Transactions on Image Processing A Publication of the IEEE Signal Processing Society, 2012, 21 (12) :4695.
                            </a>
                        </p>
                        <p id="282">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=FISBLIM:a five-step blind metric for quality assessment of multiply distorted images">

                                <b>[9]</b>Gu K, Zhai G, Liu M, et al.FISBLIM:A FIve-Step BLInd Metric for quality assessment of multiply distorted images[C]//2013 IEEE Workshop on Signal Processing Systems (Si PS) .IEEE, 2013.
                            </a>
                        </p>
                        <p id="284">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Seven challenges in image quality assessment:past,present,and future research">

                                <b>[10]</b>Chandler D M.Seven Challenges in Image Quality Assessment:Past, Present, and Future Research[J].Isrn Signal Processing, 2013, 2013:Article ID 905685.
                            </a>
                        </p>
                        <p id="286">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Perceptual quality prediction on authentically distorted images using a bag of features approach">

                                <b>[11]</b>Ghadiyaram D, Bovik A C.Perceptual Quality Prediction on Authentically Distorted Images Using a Bag of Features Approach[J].Journal of Vision, 2017, 17 (1) :32.
                            </a>
                        </p>
                        <p id="288">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=No-reference quality assessment for multiply-distorted images in gradient domain">

                                <b>[12]</b>Li Q, Lin W, Fang Y.No-Reference Quality Assessment for Multiply-Distorted Images in Gradient Domain[J].IEEESignal Processing Letters, 2016, 23 (4) :541-545.
                            </a>
                        </p>
                        <p id="290">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Making a “Completely Blind” Image Quality Analyzer">

                                <b>[13]</b>Mittal A, Fellow, IEEE, et al.Making a’Completely Blind’Image Quality Analyzer[J].IEEE Signal Processing Letters, 2013, 20 (3) :209-212.
                            </a>
                        </p>
                        <p id="292">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Feature-Enriched Completely Blind Image Quality Evaluator">

                                <b>[14]</b>Zhang L, Zhang L, Bovik A C.A feature-enriched completely blind image quality evaluator[J].IEEE Transactions on Image Processing, 2015, 24 (8) :2579-2591.
                            </a>
                        </p>
                        <p id="294">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Utilizing image scales towards totally training free blind image quality assessment">

                                <b>[15]</b>Saha A, Wu Q M J.Utilizing image scales towards totally training free blind image quality assessment[J].IEEETransactions on Image Processing, 2015, 24 (6) :1879-1892.
                            </a>
                        </p>
                        <p id="296">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Opinion-Unaware Blind Quality Assessment of Multiply and Singly Distorted Images via Distortion Parameter Estimation">

                                <b>[16]</b>Zhang Y, Chandler D M.Opinion-Unaware Blind Quality Assessment of Multiply and Singly Distorted Images via Distortion Parameter Estimation[J].IEEE Transactions on Image Processing, 2018, 27 (11) :5433-5448.
                            </a>
                        </p>
                        <p id="298">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Probabilistic Matrix Factorization">

                                <b>[17]</b>Mnih A, Salakhutdinov R R.Probabilistic matrix factorization[C]//Advances in neural information processing systems.2008:1257-1264.
                            </a>
                        </p>
                        <p id="300">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Bayesian robust matrix factorization for image and video processing">

                                <b>[18]</b>Wang N, Yeung D Y.Bayesian robust matrix factorization for image and video processing[C]//Proceedings of the IEEE International Conference on Computer Vision.2013:1785-1792.
                            </a>
                        </p>
                        <p id="302">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image Quality Assessment by Separately Evaluating Detail Losses and Additive Impairments">

                                <b>[19]</b>Li S, Zhang F, Ma L, et al.Image quality assessment by separately evaluating detail losses and additive impairments[J].IEEE Transactions on Multimedia, 2011, 13 (5) :935-949.
                            </a>
                        </p>
                        <p id="304">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image Denoising by Sparse 3-D Transform-Domain Collaborative Filtering">

                                <b>[20]</b>Dabov K, Foi A, Katkovnik V, et al.Image denoising by sparse 3-D transform-domain collaborative filtering[J].IEEE Transactions on image processing, 2007, 16 (8) :2080-2095.
                            </a>
                        </p>
                        <p id="306">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Unsupervised feature learning framework for no-referenceimage quality assessment">

                                <b>[21]</b>Ye P, Kumar J, Kang L, et al.Unsupervised feature learning framework for no-reference image quality assessment[C]//2012 IEEE conference on computer vision and pattern recognition.IEEE, 2012:1098-1105.
                            </a>
                        </p>
                        <p id="308">
                            <a id="bibliography_22" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXJM201703027&amp;v=MDY3NDBESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVadEZ5amhVN3pKSWpYQlk3RzRIOWJNckk5SFk0UUs=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[22]</b>刘国军, 高丽霞, 陈丽奇.广义平均的全参考型图像质量评价池化策略[J].光学精密工程, 2017, 25 (3) :742-748.
                            </a>
                        </p>
                        <p id="310">
                            <a id="bibliography_23" >
                                    <b>[23]</b>
                                Saad M A, Bovik A C, Charrier C.Blind image quality assessment:A natural scene statistics approach in the DCT domain[J].IEEE transactions on Image Processing, 2012, 21 (8) :3339-3352.
                            </a>
                        </p>
                        <p id="312">
                            <a id="bibliography_24" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201410052&amp;v=MjY0NzZab1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnRGeWpoVTd6Skx6VFpaTEc0SDlYTnI0OUE=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[24]</b>付燕, 张妍.基于ε-SVR的JPEG无参考图像质量评价[J].计算机应用与软件, 2014, 31 (10) :213-215, 238.
                            </a>
                        </p>
                        <p id="314">
                            <a id="bibliography_25" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJX201410016&amp;v=MjEyMTJPM3pxcUJ0R0ZyQ1VSN3FmWnVadEZ5amhVN3pKTHo3QmRyRzRIOVhOcjQ5RVlvUUtESDg0dlI0VDZqNTQ=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[25]</b>高飞, 高新波.主动特征学习及其在盲图像质量评价中的应用[J].计算机学报, 2014, 37 (10) :2227-2234.
                            </a>
                        </p>
                        <p id="316">
                            <a id="bibliography_26" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000002769&amp;v=MDM0MDl0ak5yNDlGWk9zTkMzb3dvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp0RmlubFVyeklJRm9VYVJNPU5pZklZN0s3SA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[26]</b>Chang C C, Lin C J.LIBSVM:A library for support vector machines[J].ACM Transactions on Intelligent Systems and Technology, 2011, 2 (3) :1-27.
                            </a>
                        </p>
                        <p id="318">
                            <a id="bibliography_27" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Objective quality assessment of multiply distorted images">

                                <b>[27]</b>Jayaraman D, Mittal A, Moorthy A K, et al.Objective quality assessment of multiply distorted images[C]//Signals, Systems and Computers (ASILOMAR) , 2012 Conference Record of the Forty Sixth Asilomar Conference on.IEEE, 2012.
                            </a>
                        </p>
                        <p id="320">
                            <a id="bibliography_28" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Hybrid noreference quality metric for singly and multiply distorted images">

                                <b>[28]</b>Gu K, Zhai G, Yang X, et al.Hybrid No-Reference Quality Metric for Singly and Multiply Distorted Images[J].IEEETransactions on Broadcasting, 2014, 60 (3) :555-567.
                            </a>
                        </p>
                        <p id="322">
                            <a id="bibliography_29" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Blind image quality assessment using joint statistics of gradient magnitude and Laplacian features">

                                <b>[29]</b>Xue W, Mou X, Zhang L, et al.Blind image quality assessment using joint statistics of gradient magnitude and Laplacian features[J].IEEE Transactions on Image Processing, 2014, 23 (11) :4850-4862.
                            </a>
                        </p>
                        <p id="324">
                            <a id="bibliography_30" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Blind Image Quality Assessment: A Natural Scene Statistics Approach in the DCT Domain">

                                <b>[30]</b>Saad M A, Bovik A C, Charrier C.Blind image quality assessment:A natural scene statistics approach in the DCT domain[J].IEEE transactions on Image Processing, 2012, 21 (8) :3339-3352.
                            </a>
                        </p>
                        <p id="326">
                            <a id="bibliography_31" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Blind Image Quality Assessment Using Statistical">

                                <b>[31]</b>Li Q, Lin W, Xu J, et al.Blind image quality assessment using statistical structural and luminance features[J].IEEETransactions on Multimedia, 2016, 18 (12) :2457-2469.
                            </a>
                        </p>
                        <p id="328">
                            <a id="bibliography_32" target="_blank" href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Using free energy principle for blind image quality assessment,&amp;quot;">

                                <b>[32]</b>Gu K, Zhai G, Yang X, et al.Using free energy principle for blind image quality assessment[J].IEEE Transactions on Multimedia, 2015, 17 (1) :50-63.
                            </a>
                        </p>
                        <p id="330">
                            <a id="bibliography_33" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Color Gaussian Jet Features For No-Reference Quality Assessment of Multiply-Distorted Images">

                                <b>[33]</b>Hadizadeh H, Baji?I V.Color Gaussian Jet features for noreference quality assessment of multiply-distorted images[J].IEEE Signal Processing Letters, 2016, 23 (12) :1717-1721.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JYRJ201907046" />
        <input id="dpi" type="hidden" value="600" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201907046&amp;v=MDUwOTRqaFU3L0FMelRaWkxHNEg5ak1xSTlCWW9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVp0Rnk=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
