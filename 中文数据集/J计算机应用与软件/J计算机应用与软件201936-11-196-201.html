<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637134079746975000%26DBCODE%3dCJFD%26TABLEName%3dCJFDTEMP%26FileName%3dJYRJ201911033%26RESULT%3d1%26SIGN%3d2xYHshIYPNtwoGjvDHa2%252bomexPw%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JYRJ201911033&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JYRJ201911033&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201911033&amp;v=MDQ5NjZPZVplVnVGeS9oVUx2Qkx6VFpaTEc0SDlqTnJvOUdaNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkw=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#41" data-title="&lt;b&gt;0 引 言&lt;/b&gt; "><b>0 引 言</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#45" data-title="&lt;b&gt;1 算法研究&lt;/b&gt; "><b>1 算法研究</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#46" data-title="&lt;b&gt;1.1 YOLO算法&lt;/b&gt;"><b>1.1 YOLO算法</b></a></li>
                                                <li><a href="#52" data-title="&lt;b&gt;1.2 极限学习机算法&lt;/b&gt;"><b>1.2 极限学习机算法</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#71" data-title="&lt;b&gt;2 基于YOLO和极限学习机的安全带检测模型&lt;/b&gt; "><b>2 基于YOLO和极限学习机的安全带检测模型</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#88" data-title="&lt;b&gt;3 实 验&lt;/b&gt; "><b>3 实 验</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#89" data-title="&lt;b&gt;3.1 数据集的采集&lt;/b&gt;"><b>3.1 数据集的采集</b></a></li>
                                                <li><a href="#94" data-title="&lt;b&gt;3.2 主驾驶区域检测&lt;/b&gt;"><b>3.2 主驾驶区域检测</b></a></li>
                                                <li><a href="#106" data-title="&lt;b&gt;3.3 安全带检测&lt;/b&gt;"><b>3.3 安全带检测</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#121" data-title="&lt;b&gt;4 结 语&lt;/b&gt; "><b>4 结 语</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#51" data-title="图1 YOLO V3网络结构">图1 YOLO V3网络结构</a></li>
                                                <li><a href="#75" data-title="图2 基于YOLO和极限学习机的安全带检测模型">图2 基于YOLO和极限学习机的安全带检测模型</a></li>
                                                <li><a href="#87" data-title="图3 IOU-k折线图">图3 IOU-k折线图</a></li>
                                                <li><a href="#155" data-title="图4 部分样本示意图">图4 部分样本示意图</a></li>
                                                <li><a href="#104" data-title="图5 主驾驶区域检测的时间开销对比">图5 主驾驶区域检测的时间开销对比</a></li>
                                                <li><a href="#105" data-title="图6 主驾驶区域检测示例">图6 主驾驶区域检测示例</a></li>
                                                <li><a href="#117" data-title="图7 安全带检测示例">图7 安全带检测示例</a></li>
                                                <li><a href="#119" data-title="&lt;b&gt;表1 四种方法检测结果对比&lt;/b&gt;"><b>表1 四种方法检测结果对比</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="156">


                                    <a id="bibliography_1" title=" Guo H,Lin H,Zhang S,et al.Image-based seat belt detection[C]//IEEE International Conference on Vehicular Electronics and Safety.Beijing:IEEE,2011:161-164." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image-based seat belt detection">
                                        <b>[1]</b>
                                         Guo H,Lin H,Zhang S,et al.Image-based seat belt detection[C]//IEEE International Conference on Vehicular Electronics and Safety.Beijing:IEEE,2011:161-164.
                                    </a>
                                </li>
                                <li id="158">


                                    <a id="bibliography_2" title=" Chen Y,Li G.Safety belt detetion system based on Adaboost[J].Electronic Measurement Technology,2015(4):123-127." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZCL201504028&amp;v=MDczNzZVTHZCSVRmSVlyRzRIOVRNcTQ5SGJJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdUZ5L2g=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         Chen Y,Li G.Safety belt detetion system based on Adaboost[J].Electronic Measurement Technology,2015(4):123-127.
                                    </a>
                                </li>
                                <li id="160">


                                    <a id="bibliography_3" title=" 付春芬.基于深度学习的安全带检测方法研究[D].武汉:华中科技大学,2015." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=1015909450.nh&amp;v=MTM5MzFKcjVFYlBJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdUZ5L2hVTHZCVkYyNkc3cTRGOVg=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         付春芬.基于深度学习的安全带检测方法研究[D].武汉:华中科技大学,2015.
                                    </a>
                                </li>
                                <li id="162">


                                    <a id="bibliography_4" title=" 谢锦,蔡自兴,邓海涛,等.基于图像不变特征深度学习的交通标志分类[J].计算机辅助设计与图形学学报,2017,29(4):632-640." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJF201704008&amp;v=MjYzMDg2ajU0TzN6cXFCdEdGckNVUkxPZVplVnVGeS9oVUx2Qkx6N0JhTEc0SDliTXE0OUZiSVFLREg4NHZSNFQ=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                         谢锦,蔡自兴,邓海涛,等.基于图像不变特征深度学习的交通标志分类[J].计算机辅助设计与图形学学报,2017,29(4):632-640.
                                    </a>
                                </li>
                                <li id="164">


                                    <a id="bibliography_5" title=" 杨凯杰,章东平,杨力.深度学习的汽车驾驶员安全带检测[J].中国计量大学学报,2017,28(3):326-333." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZGJL201703010&amp;v=MTY5NDR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ1RnkvaFVMdkJQeXJCWXJHNEg5Yk1ySTlFWklRS0RIODQ=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                         杨凯杰,章东平,杨力.深度学习的汽车驾驶员安全带检测[J].中国计量大学学报,2017,28(3):326-333.
                                    </a>
                                </li>
                                <li id="166">


                                    <a id="bibliography_6" title=" 施辉,陈先桥,杨英.改进YOLO v3的安全帽佩戴检测方法[J/OL].计算机工程与应用:1-9[2019-03-13].http://kns.cnki.net/kcms/detail/11.2127.TP.20190311.1538.012.html." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSGG201911034&amp;v=MzI0MzlHRnJDVVJMT2VaZVZ1RnkvaFVMdkJMejdNYWJHNEg5ak5ybzlHWUlRS0RIODR2UjRUNmo1NE8zenFxQnQ=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                         施辉,陈先桥,杨英.改进YOLO v3的安全帽佩戴检测方法[J/OL].计算机工程与应用:1-9[2019-03-13].http://kns.cnki.net/kcms/detail/11.2127.TP.20190311.1538.012.html.
                                    </a>
                                </li>
                                <li id="168">


                                    <a id="bibliography_7" title=" Redmon J,Divvala S,Girshick R,et al.You only look once:unified,real time object detection[C]//Computer Vision and Pattern Recognition.2016:779-788." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=You only look once:Unified,real-time object detection">
                                        <b>[7]</b>
                                         Redmon J,Divvala S,Girshick R,et al.You only look once:unified,real time object detection[C]//Computer Vision and Pattern Recognition.2016:779-788.
                                    </a>
                                </li>
                                <li id="170">


                                    <a id="bibliography_8" title=" Szegedy C,Liu W,Jia Y Q,et al.Going deeper with convolutions[C]//Proceedings of 2015 IEEE Conference on Computer Vision and Pattern Recognition.Boston,MA,USA:IEEE,2015:1-9." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Going deeper with convolutions">
                                        <b>[8]</b>
                                         Szegedy C,Liu W,Jia Y Q,et al.Going deeper with convolutions[C]//Proceedings of 2015 IEEE Conference on Computer Vision and Pattern Recognition.Boston,MA,USA:IEEE,2015:1-9.
                                    </a>
                                </li>
                                <li id="172">


                                    <a id="bibliography_9" title=" Redmon J,Farhadi A.YOLOv3:An incremental improvement[C]//IEEE Conference on Computer Vision and Pattern Recognition.2018." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=YOLOv3:An Incremental Improvement">
                                        <b>[9]</b>
                                         Redmon J,Farhadi A.YOLOv3:An incremental improvement[C]//IEEE Conference on Computer Vision and Pattern Recognition.2018.
                                    </a>
                                </li>
                                <li id="174">


                                    <a id="bibliography_10" title=" He K,Zhang X,Ren S,et al.Deep residual learning for image recognition[C]//Proceedings of the IEEE conference on computer vision and pattern recognition.2016:770-778." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep residual learning for image recognition">
                                        <b>[10]</b>
                                         He K,Zhang X,Ren S,et al.Deep residual learning for image recognition[C]//Proceedings of the IEEE conference on computer vision and pattern recognition.2016:770-778.
                                    </a>
                                </li>
                                <li id="176">


                                    <a id="bibliography_11" title=" Redmon J,Farhadi A.Yolo9000:better,faster,stronger[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.2017:6517-6525." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=YOLO9000:Better Faster,Stronger">
                                        <b>[11]</b>
                                         Redmon J,Farhadi A.Yolo9000:better,faster,stronger[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.2017:6517-6525.
                                    </a>
                                </li>
                                <li id="178">


                                    <a id="bibliography_12" title=" Huang G B,Zhu Q Y,Siew C K.Extreme learning machine:a new learning scheme of feedforward neural networks[C]//Proceedings of IEEE International Conference on Neural Networks.Piscataway:IEEE Press,2004:985-990." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Extreme Learning Machine:a New Learning Scheme of Feedforward Neural Networks">
                                        <b>[12]</b>
                                         Huang G B,Zhu Q Y,Siew C K.Extreme learning machine:a new learning scheme of feedforward neural networks[C]//Proceedings of IEEE International Conference on Neural Networks.Piscataway:IEEE Press,2004:985-990.
                                    </a>
                                </li>
                                <li id="180">


                                    <a id="bibliography_13" title=" Huang G B,Zhu Q Y,Siew C K.Extreme learning machine:theory and applications[J].Neurocomputing,2006,70(1):489-501." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501913101&amp;v=MjQ2MjZETnFvOUViZW9NRFh3NG9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVWIvSUoxb1hiaHM9TmlmT2ZiSzdIdA==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[13]</b>
                                         Huang G B,Zhu Q Y,Siew C K.Extreme learning machine:theory and applications[J].Neurocomputing,2006,70(1):489-501.
                                    </a>
                                </li>
                                <li id="182">


                                    <a id="bibliography_14" title=" 陆思源,陆志海,王水花,等.极限学习机综述[J].测控技术,2018,37(10):3-9." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=IKJS201810002&amp;v=MTEzMjNVUkxPZVplVnVGeS9oVUx2QkxDYkJmYkc0SDluTnI0OUZab1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckM=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[14]</b>
                                         陆思源,陆志海,王水花,等.极限学习机综述[J].测控技术,2018,37(10):3-9.
                                    </a>
                                </li>
                                <li id="184">


                                    <a id="bibliography_15" title=" Ren S,Girshick R,Girshick R,et al.Faster R-CNN:Towards real-time object detection with region proposal networks[J].IEEE Transactions on Pattern Analysis &amp;amp; Machine Intelligence,2017,39(6):1137-1149." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Faster R-CNN:Towards Real-Time Object Detection with Region Proposal Networks">
                                        <b>[15]</b>
                                         Ren S,Girshick R,Girshick R,et al.Faster R-CNN:Towards real-time object detection with region proposal networks[J].IEEE Transactions on Pattern Analysis &amp;amp; Machine Intelligence,2017,39(6):1137-1149.
                                    </a>
                                </li>
                                <li id="186">


                                    <a id="bibliography_16" title=" Dai J,Li Y,He K,et al.R-FCN:Object detection via region-based fully convolutional networks[C]//Neural Information Processing Systems.2016:379-387." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=R-FCN:Object Detection via Region-based Fully Convolutional Networks">
                                        <b>[16]</b>
                                         Dai J,Li Y,He K,et al.R-FCN:Object detection via region-based fully convolutional networks[C]//Neural Information Processing Systems.2016:379-387.
                                    </a>
                                </li>
                                <li id="188">


                                    <a id="bibliography_17" title=" Rahman M A,Wang Y.Optimizing intersection-over-union in deep neural networks for image segmentation[M]//Advances in Visual Computing.Springer International Publishing,2016:234-244." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Optimizing Intersection-Over-Union in Deep Neural Networks for Image Segmentation">
                                        <b>[17]</b>
                                         Rahman M A,Wang Y.Optimizing intersection-over-union in deep neural networks for image segmentation[M]//Advances in Visual Computing.Springer International Publishing,2016:234-244.
                                    </a>
                                </li>
                                <li id="190">


                                    <a id="bibliography_18" title=" 郭琳,秦世引.遥感图像飞机目标高效搜检深度学习优化算法[J].北京航空航天大学学报,2019,45(1):159-173." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=BJHK201901019&amp;v=MDU1MzdZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdUZ5L2hVTHZCSnlmRFpiRzRIOWpNcm85RWI=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[18]</b>
                                         郭琳,秦世引.遥感图像飞机目标高效搜检深度学习优化算法[J].北京航空航天大学学报,2019,45(1):159-173.
                                    </a>
                                </li>
                                <li id="192">


                                    <a id="bibliography_19" title=" 郑志强,刘妍妍,潘长城,李国宁.改进YOLO V3遥感图像飞机识别应用[J].电光与控制,2019,26(4):28-32." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DGKQ201904006&amp;v=MjE1Mzh2QklTckFmN0c0SDlqTXE0OUZZb1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnVGeS9oVUw=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[19]</b>
                                         郑志强,刘妍妍,潘长城,李国宁.改进YOLO V3遥感图像飞机识别应用[J].电光与控制,2019,26(4):28-32.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JYRJ" target="_blank">计算机应用与软件</a>
                2019,36(11),196-201 DOI:10.3969/j.issn.1000-386x.2019.11.032            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于YOLO和极限学习机的驾驶员安全带检测模型研究</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%94%B0%E5%9D%A4&amp;code=43235601&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">田坤</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%8E%E5%86%A0&amp;code=08240615&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">李冠</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%B5%B5%E5%8D%AB%E4%B8%9C&amp;code=08242107&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">赵卫东</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%B1%B1%E4%B8%9C%E7%A7%91%E6%8A%80%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0049968&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">山东科技大学计算机科学与工程学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>针对现有的驾驶员安全带检测算法存在的定位精度差、实时性低的问题,提出一种基于YOLO和极限学习机相结合的驾驶员安全带检测模型。利用YOLO网络快速定位主驾驶区域,提取主驾驶区域特征,传递给极限学习机,训练成一个安全带检测分类器。实验结果表明,与传统的安全带检测算法相比,该方法在驾驶员安全带检测中准确率更高,检测速度大大提升。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%AE%89%E5%85%A8%E5%B8%A6%E6%A3%80%E6%B5%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">安全带检测;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=YOLO&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">YOLO;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">特征提取;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%9E%81%E9%99%90%E5%AD%A6%E4%B9%A0%E6%9C%BA(ELM)&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">极限学习机(ELM);</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    田坤，硕士生，主研领域:智能信息处理。;
                                </span>
                                <span>
                                    李冠，副教授。;
                                </span>
                                <span>
                                    赵卫东，教授。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-02-27</p>

                    <p>

                            <b>基金：</b>
                                                        <span>山东省研究生教育创新计划一般项目(SDYC16022);</span>
                    </p>
            </div>
                    <h1><b>DRIVER'S SEATBELT DETECTION BASED ON YOLO AND EXTREME LEARNING MACHINE</b></h1>
                    <h2>
                    <span>Tian Kun</span>
                    <span>Li Guan</span>
                    <span>Zhao Weidong</span>
            </h2>
                    <h2>
                    <span>College of Computer Science and Engineering,Shandong University of Science and Technology</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Aiming at the problem of poor positioning accuracy and low real-time performance of existing driver's seatbelt detection algorithms, this paper proposed a driver's seatbelt detection model based on combination of YOLO and extreme learning machine. We used YOLO network to locate the main driving area quickly, extracted the features of the main driving area, and then transferred them to the extreme learning machine to train a seatbelt detection classifier. The experimental results show that, compared with the traditional seatbelt detection algorithm, the method has higher accuracy and faster detection speed in driver's seatbelt detection.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Seatbelt%20detection&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Seatbelt detection;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=YOLO&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">YOLO;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Feature%20extraction&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Feature extraction;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Extreme%20learning%20machine%20(ELM)&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Extreme learning machine (ELM);</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2019-02-27</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="41" name="41" class="anchor-tag"><b>0 引 言</b></h3>
                <div class="p1">
                    <p id="42">汽车事故调查表明,如果系安全带,在发生正面撞车时,可使死亡率减少57%,侧面撞车时可减少44%,翻车时可减少80%。因此驾驶员安全带检测逐渐成为智慧交通领域的一个研究热点。基于计算机视觉的检测方法由于其信息获取简单、对人和车辆影响较小及非接触性等特点,成为最有前景和价值的检测方法。</p>
                </div>
                <div class="p1">
                    <p id="43">目前,国内外研究人员在安全带检测方面做了大量的研究。由于检测工具和应用环境的不同,检测方法也不尽相同。Guo等<citation id="194" type="reference"><link href="156" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>提出的基于图像处理的安全带检测方法。首先通过边缘检测和霍夫直线变化方法对图像进行预处理,定位驾驶员区域,然后通过基于边缘检测和Hough变换的方法对安全带进行检测。然而该方法容易受到光照、车辆颜色的影响,对图片质量和拍摄角度要求也较高,并且缺乏鲁棒性,因此很难普及推广。Chen等<citation id="195" type="reference"><link href="158" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>提出的基于Adabooast的安全带检测方法。首先对车窗区域进行一个粗定位,找到多个车窗候选区,然后再对驾驶员区域进行粗定位,找到多个驾驶员候选区,接着对安全带进行检测,找到多个安全带检测区域,最后通过高斯混合模型处理后得到安全带的最终检测结果。然而该方法对图像质量要求较高,并且检测精度达不到推广的地步。付春芬<citation id="196" type="reference"><link href="160" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>提出的基于深度学习的安全带检测方法研究,首先运用帧差法定位交通视频中的车辆,然后利用边缘检测和积分投影定位驾驶员车窗区域,最后训练卷积神经网络模型检测安全带。然而该方法训练速度较慢,并且容易受到光照和遮挡因素的影响。谢锦等<citation id="197" type="reference"><link href="162" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>提出的基于图像不变特征深度学习的交通标志分类。首先基于慢特征分析的深度学习框架自动学习得到每个阶段的特征映射矩阵,然后将第一阶段和第二阶段特征联合输出作为交通标志的特征,最后使用支持向量机进行交通标志分类。运用该方法在对安全带进行检测分类时,能有效地进行特征提取,并且泛化能力较强,但检测速度达不到实时检测的要求。杨凯杰等<citation id="198" type="reference"><link href="164" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>提出的基于深度学习的安全带检测方法。该方法包括两个检测器D1、D2和一个分类器C1,首先将高清图片输入检测器D1,定位汽车的挡风玻璃区域,然后再用检测器D2定位到主驾驶区域,最后将主驾驶区域图片送入二元分类器C1判断是否系安全带。但是该方法时效性不高,处理速度较慢。施辉等<citation id="199" type="reference"><link href="166" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>提出改进的YOLO V3的安全帽佩戴检测方法。首先采用图像金字塔结构获取不同尺度的特征图,用于位置和类别预测,然后对数据集目标框进行聚类,确定目标框参数,最后训练迭代过程中改变输入图像尺寸,增加模型对尺度的适应性。该方法能够满足实时性检测的要求,但是YOLO算法在提升检测速度的同时也损失了部分的准确率。</p>
                </div>
                <div class="p1">
                    <p id="44">本文将YOLO网络和极限学习机结合起来应用到道路监控视频中的汽车驾驶员安全带检测。首先将视频中的汽车图片提取出来输入到YOLO网络中,利用YOLO网络的深层次特征提取、高精度检测分类特性对主驾驶区域进行目标提取;然后将提取的主驾驶区域特征传递给极限学习机,利用极限学习机训练速度快、泛化能力强的优点,训练成一个二元分类器;最后对驾驶员是否佩戴安全带进行准确分类。实验时通过Nvidia 1060显卡进行加速,处理速度可以达到198 帧/s,基本可以实现对道路监控视频中驾驶员安全带的实时检测。</p>
                </div>
                <h3 id="45" name="45" class="anchor-tag"><b>1 算法研究</b></h3>
                <h4 class="anchor-tag" id="46" name="46"><b>1.1 YOLO算法</b></h4>
                <div class="p1">
                    <p id="47">YOLO算法<citation id="200" type="reference"><link href="168" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>是一个基于单个神经网络的目标检测系统,以回归的方式对目标进行检测,输入图像后,直接在图像的多个位置上回归出目标的位置以及其分类类别。YOLO算法的创新之处在于采用了端到端的网络结构设计,其最大优势就是能够实现实时检测。YOLO的目标检测网络由24个卷积层和2个全连接层组成。其中卷积层的功能是提取图像特征,全连接层的功能是预测图像位置和类别置信概率。YOLO分类网络结构采用的是GoogLeNet<citation id="201" type="reference"><link href="170" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>,它将输入图像分成<i>S</i>×<i>S</i>个格子,如果某个物体的中心位置的坐标落入到其中一个格子内,那么这个格子就负责检测出该物体。YOLO网络的输出结果为一个张量,计算公式如下:</p>
                </div>
                <div class="p1">
                    <p id="48"><i>S</i>×<i>S</i>×(5×<i>B</i>+<i>C</i>)      (1)</p>
                </div>
                <div class="p1">
                    <p id="49">式中:<i>S</i>×<i>S</i>为划分网格数目;<i>B</i>为每个网格预测的物体边界框数目;<i>C</i>为类别个数。每个网格要预测<i>B</i>个边界框还要预测<i>C</i>个类别。</p>
                </div>
                <div class="p1">
                    <p id="50">在最新的YOLO V3网络<citation id="202" type="reference"><link href="172" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>中使用了一个53层的卷积网络,包含52层卷积层和1个全连接层,加入了多个连续的3×3和1×1的卷积,命名为Darknet-53。Joseph Redmon的实验表明,在平衡分类准确率和效率时,Darknet-53模型比ResNet-101、 ResNet-152<citation id="203" type="reference"><link href="174" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>和Darknet-19<citation id="204" type="reference"><link href="176" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>表现得更加优秀。一方面,Darknet-53网络采用全卷积结构,YOLO V3前向传播过程中,张量的尺寸变换是通过改变卷积核的步长来实现的。另一方面,Darknet-53网络引入了residual结构,得益于ResNet的residual结构,训练深层网络的难度大大减小。因此Darknet-53网络做到53层,精度提升比较明显。YOLO V3网络结构如图1所示。</p>
                </div>
                <div class="area_img" id="51">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201911033_051.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 YOLO V3网络结构" src="Detail/GetImg?filename=images/JYRJ201911033_051.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 YOLO V3网络结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201911033_051.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="52" name="52"><b>1.2 极限学习机算法</b></h4>
                <div class="p1">
                    <p id="53">极限学习机(Extreme Learning Machine,ELM)是由新加坡南洋理工大学的黄广斌教授提出的一种针对单隐层前馈神经网络的新算法<citation id="205" type="reference"><link href="178" rel="bibliography" /><link href="180" rel="bibliography" /><link href="182" rel="bibliography" /><sup>[<a class="sup">12</a>,<a class="sup">13</a>,<a class="sup">14</a>]</sup></citation>。ELM可以对初始输入权重和偏置输入任意的参数,并得到相应的输出权重。假设有<i>N</i>个任意的样本(<i>X</i><sub><i>i</i></sub>,<i>t</i><sub><i>i</i></sub>),其中<i>X</i><sub><i>i</i></sub>=[<i>x</i><sub><i>i</i></sub><sub>1</sub>,<i>x</i><sub><i>i</i></sub><sub>2</sub>,…,<i>x</i><sub><i>in</i></sub>]<sup>T</sup>∈<b>R</b><sup><i>n</i></sup>,<i>t</i><sub><i>i</i></sub>=[<i>t</i><sub><i>i</i></sub><sub>1</sub>,<i>t</i><sub><i>i</i></sub><sub>2</sub>,…,<i>t</i><sub><i>im</i></sub>]<sup>T</sup>∈<b>R</b><sup><i>m</i></sup>。对于一个有<i>K</i>个隐层节点的单隐层神经网络可以表示为:</p>
                </div>
                <div class="p1">
                    <p id="54"><mathml id="123"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Κ</mi></munderover><mi>β</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mi>g</mi><mo stretchy="false">(</mo><mi>W</mi><msub><mrow></mrow><mi>i</mi></msub><mo>⋅</mo><mi>X</mi><msub><mrow></mrow><mi>j</mi></msub><mo>+</mo><mi>b</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mi>o</mi><msub><mrow></mrow><mi>j</mi></msub></mrow></math></mathml><i>j</i>=1,2,…,<i>N</i>      (2)</p>
                </div>
                <div class="p1">
                    <p id="55">式中:<i>g</i>(<i>x</i>)为激活函数;<i>W</i><sub><i>i</i></sub>=[<i>w</i><sub><i>i</i></sub><sub>1</sub>,<i>w</i><sub><i>i</i></sub><sub>2</sub>,…,<i>w</i><sub><i>in</i></sub>]<sup>T</sup>为输入权重参数;<i>β</i><sub><i>i</i></sub>为输出权重参数;<i>b</i><sub><i>i</i></sub>是第<i>i</i>个隐层单元的偏置参数;<i>W</i><sub><i>i</i></sub>·<i>X</i><sub><i>j</i></sub>表示<i>W</i><sub><i>i</i></sub>和<i>X</i><sub><i>j</i></sub>的内积值。</p>
                </div>
                <div class="p1">
                    <p id="56">在单隐层神经网络中,最小化输出误差值是其学习的目标,其公式为:</p>
                </div>
                <div class="p1">
                    <p id="57"><mathml id="124"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mrow><mrow><mo>|</mo><mrow><mi>o</mi><msub><mrow></mrow><mi>j</mi></msub><mo>-</mo><mi>t</mi><msub><mrow></mrow><mi>j</mi></msub></mrow><mo>|</mo></mrow></mrow></mstyle><mo>=</mo><mn>0</mn></mrow></math></mathml>      (3)</p>
                </div>
                <div class="p1">
                    <p id="58">即存在<i>β</i><sub><i>i</i></sub>、<i>W</i><sub><i>i</i></sub>和<i>b</i><sub><i>i</i></sub>,使得<mathml id="125"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Κ</mi></munderover><mi>β</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mi>g</mi><mo stretchy="false">(</mo><mi>W</mi><msub><mrow></mrow><mi>i</mi></msub><mo>⋅</mo><mi>X</mi><msub><mrow></mrow><mi>j</mi></msub><mo>+</mo><mi>b</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mi>t</mi><msub><mrow></mrow><mi>j</mi></msub></mrow></math></mathml>,<i>j</i>=1,2,…,<i>N</i>,可以用矩阵表示为:</p>
                </div>
                <div class="p1">
                    <p id="59"><i><b>Hβ</b></i><b>=</b><i><b>T</b></i>      (4)</p>
                </div>
                <div class="p1">
                    <p id="60">式中:<i><b>H</b></i>是隐层节点的输出;<i>β</i>为输出权重;<i><b>T</b></i>为期望输出。<i><b>H</b></i><b>、</b><i><b>β</b></i><b>、</b><i><b>T</b></i>分别为:</p>
                </div>
                <div class="p1">
                    <p id="61" class="code-formula">
                        <mathml id="61"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi mathvariant="bold-italic">Η</mi><mo>=</mo><mrow><mo>[</mo><mrow><mtable><mtr><mtd><mi>g</mi><mo stretchy="false">(</mo><mi>W</mi><msub><mrow></mrow><mn>1</mn></msub><mo>⋅</mo><mi>X</mi><msub><mrow></mrow><mn>1</mn></msub><mo>+</mo><mi>b</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">)</mo></mtd><mtd><mi>g</mi><mo stretchy="false">(</mo><mi>W</mi><msub><mrow></mrow><mn>2</mn></msub><mo>⋅</mo><mi>X</mi><msub><mrow></mrow><mn>1</mn></msub><mo>+</mo><mi>b</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">)</mo></mtd><mtd><mo>⋯</mo></mtd><mtd><mi>g</mi><mo stretchy="false">(</mo><mi>W</mi><msub><mrow></mrow><mi>L</mi></msub><mo>⋅</mo><mi>X</mi><msub><mrow></mrow><mn>1</mn></msub><mo>+</mo><mi>b</mi><msub><mrow></mrow><mi>L</mi></msub><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mi>g</mi><mo stretchy="false">(</mo><mi>W</mi><msub><mrow></mrow><mn>1</mn></msub><mo>⋅</mo><mi>X</mi><msub><mrow></mrow><mn>2</mn></msub><mo>+</mo><mi>b</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">)</mo></mtd><mtd><mi>g</mi><mo stretchy="false">(</mo><mi>W</mi><msub><mrow></mrow><mn>2</mn></msub><mo>⋅</mo><mi>X</mi><msub><mrow></mrow><mn>2</mn></msub><mo>+</mo><mi>b</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">)</mo></mtd><mtd><mo>⋯</mo></mtd><mtd><mi>g</mi><mo stretchy="false">(</mo><mi>W</mi><msub><mrow></mrow><mi>L</mi></msub><mo>⋅</mo><mi>X</mi><msub><mrow></mrow><mn>2</mn></msub><mo>+</mo><mi>b</mi><msub><mrow></mrow><mi>L</mi></msub><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mo>⋮</mo></mtd><mtd><mo>⋮</mo></mtd><mtd></mtd><mtd><mo>⋮</mo></mtd></mtr><mtr><mtd><mi>g</mi><mo stretchy="false">(</mo><mi>W</mi><msub><mrow></mrow><mn>1</mn></msub><mo>⋅</mo><mi>X</mi><msub><mrow></mrow><mi>Ν</mi></msub><mo>+</mo><mi>b</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">)</mo></mtd><mtd><mi>g</mi><mo stretchy="false">(</mo><mi>W</mi><msub><mrow></mrow><mn>2</mn></msub><mo>⋅</mo><mi>X</mi><msub><mrow></mrow><mi>Ν</mi></msub><mo>+</mo><mi>b</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">)</mo></mtd><mtd><mo>⋯</mo></mtd><mtd><mi>g</mi><mo stretchy="false">(</mo><mi>W</mi><msub><mrow></mrow><mi>L</mi></msub><mo>⋅</mo><mi>X</mi><msub><mrow></mrow><mi>Ν</mi></msub><mo>+</mo><mi>b</mi><msub><mrow></mrow><mi>L</mi></msub><mo stretchy="false">)</mo></mtd></mtr></mtable></mrow><mo>]</mo></mrow></mtd></mtr><mtr><mtd><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>5</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="area_img" id="62">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JYRJ201911033_06200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="64">为了能够训练单隐层神经网络,我们希望得到<mathml id="126"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>W</mi><mo>^</mo></mover><msub><mrow></mrow><mi>i</mi></msub></mrow></math></mathml>、<mathml id="127"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>b</mi><mo>^</mo></mover><msub><mrow></mrow><mi>i</mi></msub></mrow></math></mathml>和<mathml id="128"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>β</mi><mo>^</mo></mover><msub><mrow></mrow><mi>i</mi></msub></mrow></math></mathml>,使得:</p>
                </div>
                <div class="p1">
                    <p id="65" class="code-formula">
                        <mathml id="65"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>|</mo><mrow><mi>Η</mi><mo stretchy="false">(</mo><mover accent="true"><mi>W</mi><mo>^</mo></mover><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mover accent="true"><mi>b</mi><mo>^</mo></mover><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mover accent="true"><mi>β</mi><mo>^</mo></mover><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mi>Τ</mi></mrow><mo>|</mo></mrow><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mrow><mi>W</mi><mo>,</mo><mi>b</mi><mo>,</mo><mi>β</mi></mrow></munder><mrow><mo>|</mo><mrow><mi>Η</mi><mo stretchy="false">(</mo><mi>W</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mi>b</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mi>β</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mi>Τ</mi></mrow><mo>|</mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>7</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="66">式中:<i>i</i>=1,2,…,<i>L</i>,这等价于最小化损失函数如下:</p>
                </div>
                <div class="p1">
                    <p id="67"><mathml id="129"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>E</mi><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mrow><mrow><mo>(</mo><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></munderover><mi>β</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mi>g</mi><mo stretchy="false">(</mo><mi>W</mi><msub><mrow></mrow><mi>i</mi></msub><mo>⋅</mo><mi>X</mi><msub><mrow></mrow><mi>j</mi></msub><mo>+</mo><mi>b</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo>-</mo><mi>t</mi><msub><mrow></mrow><mi>j</mi></msub></mrow><mo>)</mo></mrow></mrow></mstyle><msup><mrow></mrow><mn>2</mn></msup></mrow></math></mathml>      (8)</p>
                </div>
                <div class="p1">
                    <p id="68">为了训练单隐层神经网络,可以将其转化为求解一个线性系统<i><b>Hβ</b></i><b>=</b><i><b>T</b></i>的问题,输出权重可以被确定为:</p>
                </div>
                <div class="p1">
                    <p id="69"><mathml id="130"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi mathvariant="bold-italic">β</mi><mo>^</mo></mover><mo>=</mo><mi mathvariant="bold-italic">Η</mi><msup><mrow></mrow><mo>+</mo></msup><mi mathvariant="bold-italic">Τ</mi></mrow></math></mathml>      (9)</p>
                </div>
                <div class="p1">
                    <p id="70">式中:<i><b>H</b></i><sup>+</sup>是矩阵<i><b>H</b></i>的Moore-Penrose广义逆矩阵;求得的解<mathml id="131"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">β</mi><mo>^</mo></mover></math></mathml>的范数最小并且唯一。</p>
                </div>
                <h3 id="71" name="71" class="anchor-tag"><b>2 基于YOLO和极限学习机的安全带检测模型</b></h3>
                <div class="p1">
                    <p id="72">YOLO模型在物体检测方面有很大的优势,主要表现在以下几个方面:YOLO目标检测效率很高,因为检测过程非常简单,只需要将图像输入神经网络就可以获得检测结果;YOLO能够很好地避免背景错误,产生false positives,由于在目标检测时能很好地利用上下文信息,所以在背景上很难预测出错误的物体信息;YOLO可以学到物体的泛化特征,从而迁移到其他领域。YOLO虽然在物体检测方面速度很快,但在分类准确率方面达不到较高的水准。</p>
                </div>
                <div class="p1">
                    <p id="73">极限学习机可以直接用于多物体分类,需要手动设置的参数只有隐含层结点个数。其优点表现在以下几个方面:在训练样本前,只需要设置隐藏层神经元的个数,在执行算法时不需要人工调整参数;相比传统训练算法,迭代次数少,收敛速度快,减少了训练时间;唯一最优解大大保证了网络的泛化性能。极限学习机在处理多物体分类时是很好的分类算法,但前提是有好的分类特征。</p>
                </div>
                <div class="p1">
                    <p id="74">为了适应和提高在道路监控视频中驾驶员安全带检测的准确率和速度,本文将YOLO网络和极限学习机算法相结合,充分发挥两者的优点,应用于实时的目标检测分类。因此提出了基于YOLO和极限学习机相结合的安全带检测模型,如图2所示。</p>
                </div>
                <div class="area_img" id="75">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201911033_075.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 基于YOLO和极限学习机的安全带检测模型" src="Detail/GetImg?filename=images/JYRJ201911033_075.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 基于YOLO和极限学习机的安全带检测模型  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201911033_075.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="76">基于YOLO和极限学习机相结合的安全带检测模型主要包含三个部分:</p>
                </div>
                <div class="p1">
                    <p id="77">(1) 定位主驾驶区域部分:采用YOLO V3特征提取的主要网络结构Darknet-53。这些卷积层是从各个主流网络结构选取性能比较好的卷积层进行整合得到。对训练样本的主驾驶区域进行特征学习,得到YOLO网络输出特征层的参数。</p>
                </div>
                <div class="p1">
                    <p id="78">(2) 特征转换部分:将YOLO V3网络提取到的主驾驶区域特征进行特征转换,然后传递给极限学习机。</p>
                </div>
                <div class="p1">
                    <p id="79">(3) 安全带分类部分:将转换的特征层参数输入到极限学习机进行训练,训练成一个准确率很高的安全带检测分类器,最后极限学习机对分类结果进行输出。</p>
                </div>
                <div class="p1">
                    <p id="80">该模型还对驾驶员安全带数据集的目标框进行聚类,确定anchor参数,主要原因是在YOLO算法中引入了Faster R-CNN<citation id="206" type="reference"><link href="184" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>中的anchor boxes<citation id="207" type="reference"><link href="186" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>的方法,使用anchor boxes作为先验框对图像中的目标进行检测。本文采用的YOLO V3算法的初始候选框的anchor参数,最初是根据COCO数据集进行聚类确定的,聚类结果为9。由于COCO数据集中类别很多,而本文实验数据集种类相对较少,因此初始候选框不再适用于汽车主驾驶区域的检测。在主驾驶区域检测中,主驾驶区域的长宽比不会因为环境背景等发生大的改变,所以需要重新对数据集进行聚类分析来选择最优的anchor参数。</p>
                </div>
                <div class="p1">
                    <p id="81">设置先验框的主要目的是提高预测框与ground truth之间的交并比。IOU是产生候选框(candidate bound)与原标记框(ground truth bound)的交叠率<citation id="208" type="reference"><link href="188" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>,即它们的交集与并集的比值。传统的K-means聚类算法常用曼哈顿距离、欧氏距离等方式对两点间的距离进行计算。然而使用欧氏距离函数会让大的框比小的框产生更多的错误,考虑到主驾驶区域检测数据集中驾驶员尺寸问题,本文采用的聚类中的距离函数如下:</p>
                </div>
                <div class="p1">
                    <p id="82"><i>d</i>(<i>box</i>,<i>centroid</i>)=1-<i>IOU</i>(<i>box</i>,<i>centroid</i>)      (10)</p>
                </div>
                <div class="p1">
                    <p id="83">式中:<i>centroid</i>代表簇的中心;<i>box</i>代表样本聚类结果;<i>IOU</i>(<i>box</i>,<i>centroid</i>)代表簇中心框和聚类框的交并比。交并比<i>IOU</i>表示预测框的准确程度,其公式为:</p>
                </div>
                <div class="p1">
                    <p id="84"><mathml id="132"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ι</mi><mi>Ο</mi><mi>U</mi><mo stretchy="false">(</mo><mi>b</mi><mi>b</mi><msub><mrow></mrow><mrow><mtext>g</mtext><mtext>t</mtext></mrow></msub><mo>,</mo><mi>b</mi><mi>b</mi><msub><mrow></mrow><mrow><mtext>d</mtext><mtext>t</mtext></mrow></msub><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><mi>b</mi><mi>b</mi><msub><mrow></mrow><mrow><mtext>g</mtext><mtext>t</mtext></mrow></msub><mstyle displaystyle="true"><mo>∩</mo><mi>b</mi></mstyle><mi>b</mi><msub><mrow></mrow><mrow><mtext>d</mtext><mtext>t</mtext></mrow></msub></mrow><mrow><mi>b</mi><mi>b</mi><msub><mrow></mrow><mrow><mtext>g</mtext><mtext>t</mtext></mrow></msub><mstyle displaystyle="true"><mo>∪</mo><mi>b</mi></mstyle><mi>b</mi><msub><mrow></mrow><mrow><mtext>d</mtext><mtext>t</mtext></mrow></msub></mrow></mfrac></mrow></math></mathml>      (11)</p>
                </div>
                <div class="p1">
                    <p id="85">式中:<i>bb</i><sub>gt</sub>表示真实框,<i>bb</i><sub>dt</sub>表示预测框。</p>
                </div>
                <div class="p1">
                    <p id="86">本文选择 416×416 作为模型输入尺寸,对主驾驶区域检测数据集进行聚类分析,选取<i>k</i>=[1,9]。通过计算不同<i>k</i>值下的<i>IOU</i> 值,选取<i>k</i>的最优值。<i>IOU</i> 与 <i>k</i>的关系如图 3所示。根据图中的折线可知,在<i>k</i>=5之后,曲线逐渐变得平缓。考虑到网络的计算量,最终采用<i>k</i>=5的聚类结果。</p>
                </div>
                <div class="area_img" id="87">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201911033_087.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 IOU-k折线图" src="Detail/GetImg?filename=images/JYRJ201911033_087.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 IOU-k折线图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201911033_087.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h3 id="88" name="88" class="anchor-tag"><b>3 实 验</b></h3>
                <h4 class="anchor-tag" id="89" name="89"><b>3.1 数据集的采集</b></h4>
                <div class="p1">
                    <p id="90">本文训练模型所用图片来自于道路交通视频监控所拍摄的图像,一共选取了6 000张图片。数据集分为训练集和测试集,训练集包含正负样本,其中正样本2 733张,负样本2 267张;测试集包含正样本861张,负样本639张。为了提高训练模型的效率,需要对数据集进行人工标注。部分样本示例如图4所示。</p>
                </div>
                <div class="area_img" id="155">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201911033_15500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 部分样本示意图" src="Detail/GetImg?filename=images/JYRJ201911033_15500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 部分样本示意图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201911033_15500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="93">实验使用Core i7- 8750H的处理器,8 GB的内存,NVIDIA GeForce GTX 1060的显卡,CUDA9.1,CUDNN7.1的GPU加速库,Windows 10操作系统,Darknet深度学习框架。</p>
                </div>
                <h4 class="anchor-tag" id="94" name="94"><b>3.2 主驾驶区域检测</b></h4>
                <h4 class="anchor-tag" id="95" name="95">(1) 数据预处理。</h4>
                <div class="p1">
                    <p id="96">本文通过使用LableImg工具标记训练数据集的每张图片中的汽车驾驶员区域。当对图片标注完成时,软件会在原图目录中生成与原图对应的XML文件,该文件记录了原图的路径、标记框的相对位置、标记框中物体的类别等信息。</p>
                </div>
                <div class="p1">
                    <p id="97">为了更好地验证模型的效果,本文对检测结果进行定量分析,我们使用检出率(DR)、漏检率(MAR)以及平均时间开销作为目标检测的评价指标<citation id="210" type="reference"><link href="190" rel="bibliography" /><link href="192" rel="bibliography" /><sup>[<a class="sup">18</a>,<a class="sup">19</a>]</sup></citation>:</p>
                </div>
                <div class="p1">
                    <p id="98" class="code-formula">
                        <mathml id="98"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>D</mi><mi>R</mi><mo>=</mo><mfrac><mtable columnalign="left"><mtr><mtd><mtext>正</mtext><mtext>确</mtext><mtext>检</mtext><mtext>测</mtext><mtext>到</mtext><mtext>主</mtext><mtext>驾</mtext><mtext>驶</mtext><mtext>区</mtext><mtext>域</mtext><mtext>的</mtext><mtext>图</mtext><mtext>片</mtext><mtext>数</mtext><mtext>目</mtext></mtd></mtr><mtr><mtd></mtd></mtr></mtable><mrow><mtext>待</mtext><mtext>检</mtext><mtext>测</mtext><mtext>图</mtext><mtext>片</mtext><mtext>数</mtext><mtext>量</mtext><mtext>的</mtext><mtext>总</mtext><mtext>数</mtext></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>2</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="area_img" id="99">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JYRJ201911033_09900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="101"><i>DR</i>反映了在所有的检测目标中,正确检测出主驾驶区域所占的比例,而<i>MAR</i>反映了在所有的检测目标中,主驾驶区域漏检数量所占的比例。</p>
                </div>
                <h4 class="anchor-tag" id="102" name="102">(2) 主驾驶区域检测分析。</h4>
                <div class="p1">
                    <p id="103">目前,汽车主驾驶区域检测常用的方法主要有文献<citation id="211" type="reference">[<a class="sup">3</a>]</citation>中的Faster RCNN、SSD和YOLO V3三种。为了验证本文采用的YOLO V3算法对主驾驶区域检测有较好的效果,对三种方法进行主驾驶区域检测时间开销对比实验,实验结果如图5所示。从主驾驶区域检测的时间开销对比图中可以看出,YOLO V3方法在对主驾驶区域检测速度上具有明显优势,能够为驾驶员安全带的实时检测提供有力支持。本文利用YOLO V3对主驾驶区域检测实验结果:检出率为97.6%,漏检率为1.7%。主驾驶区域的检测示例如图6所示。</p>
                </div>
                <div class="area_img" id="104">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201911033_104.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 主驾驶区域检测的时间开销对比" src="Detail/GetImg?filename=images/JYRJ201911033_104.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 主驾驶区域检测的时间开销对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201911033_104.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="105">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201911033_105.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 主驾驶区域检测示例" src="Detail/GetImg?filename=images/JYRJ201911033_105.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 主驾驶区域检测示例  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201911033_105.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="106" name="106"><b>3.3 安全带检测</b></h4>
                <h4 class="anchor-tag" id="107" name="107">1) 极限学习机算法求解过程:</h4>
                <div class="p1">
                    <p id="108">输入:训练主驾驶区域样本和对应的期望输出。</p>
                </div>
                <div class="p1">
                    <p id="109">输出:经过训练好的驾驶员安全带分类模型。</p>
                </div>
                <div class="p1">
                    <p id="110">(1) 对主驾驶区域样本进行初始化训练;</p>
                </div>
                <div class="p1">
                    <p id="111">(2) 根据样本的特征数确定隐含层神经元的个数,对输入层与隐含层的连接权值<i>w</i>和隐含层神经元的阈值<i>b</i>随机赋值;</p>
                </div>
                <div class="p1">
                    <p id="112">(3) 选择sigmoid函数作为隐含层神经元的激活函数,从而计算隐含层输出矩阵<i><b>H</b></i>;</p>
                </div>
                <div class="p1">
                    <p id="113">(4) 最后根据公式<mathml id="133"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi mathvariant="bold-italic">β</mi><mo>^</mo></mover><mo>=</mo><mi mathvariant="bold-italic">Η</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mrow><mo>(</mo><mrow><mfrac><mi>Ι</mi><mi>C</mi></mfrac><mo>+</mo><mi mathvariant="bold-italic">Η</mi><mi mathvariant="bold-italic">Η</mi><msup><mrow></mrow><mtext>Τ</mtext></msup></mrow><mo>)</mo></mrow><msup><mrow></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mi mathvariant="bold-italic">Τ</mi></mrow></math></mathml>,计算出输出层权值<mathml id="134"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi mathvariant="bold-italic">β</mi><mo>^</mo></mover></math></mathml>。</p>
                </div>
                <div class="p1">
                    <p id="114">原始图像大小为416×416像素,经过YOLO V3对主驾驶区域定位后,分别对检测到的每幅主驾驶区域图像下采样至24×21形成504维特征向量。ELM分类器中隐藏层单元数为200,激活函数为sigmoid函数<i>g</i>(<i>x</i>)=1/(1+exp(-<i>x</i>))。</p>
                </div>
                <h4 class="anchor-tag" id="115" name="115">2) 安全带检测分析:</h4>
                <div class="p1">
                    <p id="116">根据训练样本得到ELM分类器,使用ELM分类器对测试主驾驶员图像进行分类,得到识别率<i>accuracy</i>为0.945 1。安全带检测示例如图7所示。</p>
                </div>
                <div class="area_img" id="117">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201911033_117.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 安全带检测示例" src="Detail/GetImg?filename=images/JYRJ201911033_117.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 安全带检测示例  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201911033_117.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="118">为了验证本文提出的基于YOLO和极限学习机相结合的驾驶员安全带检测模型的有效性,本文与YOLO V3方法、文献<citation id="212" type="reference">[<a class="sup">3</a>]</citation>的双网络安全带检测方法和文献<citation id="213" type="reference">[<a class="sup">6</a>]</citation>中改进的YOLO V3方法做了对照实验。四种安全带检测方法均采用实验数据集作为输入,同时对安全带检测的评价采用检出率和检测速度两个指标,实验对比结果如表1所示。</p>
                </div>
                <div class="area_img" id="119">
                    <p class="img_tit"><b>表1 四种方法检测结果对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="119" border="1"><tr><td><br />检测方法</td><td>检出率</td><td>检测速度/(帧·s<sup>-1</sup>)</td></tr><tr><td><br />YOLO V3</td><td>90.36</td><td>58</td></tr><tr><td><br />文献[5]</td><td>92.83</td><td>15</td></tr><tr><td><br />文献[6]</td><td>92.11</td><td>65</td></tr><tr><td><br />本文方法</td><td>94.51</td><td>198</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="120">从表1可以看出,本文所提出的基于YOLO和极限学习机相结合的驾驶员安全带检测模型检测方法检出率为94.51%,检测速度为198帧/s,都远高于其他三种方法的检出率和检测速度。</p>
                </div>
                <h3 id="121" name="121" class="anchor-tag"><b>4 结 语</b></h3>
                <div class="p1">
                    <p id="122">本文提出了将YOLO网络和极限学习机相结合的驾驶员安全带检测模型,采用深度学习多层次网络训练和模型级联的方式。利用YOLO网络快速定位且实时性高的优点,快速定位主驾驶区域,并且保证输出区域最优。同时利用极限学习机训练速度快、泛化能力强的优点来作为一个二元分类器,提高了安全带区域分类的准确性和系统的鲁棒性。相比于传统的卷积神经网络对图像的处理方法,本文在检测速度上大大提升,能基本满足道路监控视频中驾驶员安全带实时检测的需要,并且准确率较高。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="156">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image-based seat belt detection">

                                <b>[1]</b> Guo H,Lin H,Zhang S,et al.Image-based seat belt detection[C]//IEEE International Conference on Vehicular Electronics and Safety.Beijing:IEEE,2011:161-164.
                            </a>
                        </p>
                        <p id="158">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZCL201504028&amp;v=MTYxMTh0R0ZyQ1VSTE9lWmVWdUZ5L2hVTHZCSVRmSVlyRzRIOVRNcTQ5SGJJUUtESDg0dlI0VDZqNTRPM3pxcUI=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> Chen Y,Li G.Safety belt detetion system based on Adaboost[J].Electronic Measurement Technology,2015(4):123-127.
                            </a>
                        </p>
                        <p id="160">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=1015909450.nh&amp;v=MDIwODJDVVJMT2VaZVZ1RnkvaFVMdkJWRjI2RzdxNEY5WEpyNUViUElRS0RIODR2UjRUNmo1NE8zenFxQnRHRnI=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> 付春芬.基于深度学习的安全带检测方法研究[D].武汉:华中科技大学,2015.
                            </a>
                        </p>
                        <p id="162">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJF201704008&amp;v=MjI5Mjk5Yk1xNDlGYklRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ1RnkvaFVMdkJMejdCYUxHNEg=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b> 谢锦,蔡自兴,邓海涛,等.基于图像不变特征深度学习的交通标志分类[J].计算机辅助设计与图形学学报,2017,29(4):632-640.
                            </a>
                        </p>
                        <p id="164">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZGJL201703010&amp;v=MTY1NDFyQ1VSTE9lWmVWdUZ5L2hVTHZCUHlyQllyRzRIOWJNckk5RVpJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0Y=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b> 杨凯杰,章东平,杨力.深度学习的汽车驾驶员安全带检测[J].中国计量大学学报,2017,28(3):326-333.
                            </a>
                        </p>
                        <p id="166">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSGG201911034&amp;v=MDQ4MzMzenFxQnRHRnJDVVJMT2VaZVZ1RnkvaFVMdkJMejdNYWJHNEg5ak5ybzlHWUlRS0RIODR2UjRUNmo1NE8=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b> 施辉,陈先桥,杨英.改进YOLO v3的安全帽佩戴检测方法[J/OL].计算机工程与应用:1-9[2019-03-13].http://kns.cnki.net/kcms/detail/11.2127.TP.20190311.1538.012.html.
                            </a>
                        </p>
                        <p id="168">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=You only look once:Unified,real-time object detection">

                                <b>[7]</b> Redmon J,Divvala S,Girshick R,et al.You only look once:unified,real time object detection[C]//Computer Vision and Pattern Recognition.2016:779-788.
                            </a>
                        </p>
                        <p id="170">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Going deeper with convolutions">

                                <b>[8]</b> Szegedy C,Liu W,Jia Y Q,et al.Going deeper with convolutions[C]//Proceedings of 2015 IEEE Conference on Computer Vision and Pattern Recognition.Boston,MA,USA:IEEE,2015:1-9.
                            </a>
                        </p>
                        <p id="172">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=YOLOv3:An Incremental Improvement">

                                <b>[9]</b> Redmon J,Farhadi A.YOLOv3:An incremental improvement[C]//IEEE Conference on Computer Vision and Pattern Recognition.2018.
                            </a>
                        </p>
                        <p id="174">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep residual learning for image recognition">

                                <b>[10]</b> He K,Zhang X,Ren S,et al.Deep residual learning for image recognition[C]//Proceedings of the IEEE conference on computer vision and pattern recognition.2016:770-778.
                            </a>
                        </p>
                        <p id="176">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=YOLO9000:Better Faster,Stronger">

                                <b>[11]</b> Redmon J,Farhadi A.Yolo9000:better,faster,stronger[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.2017:6517-6525.
                            </a>
                        </p>
                        <p id="178">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Extreme Learning Machine:a New Learning Scheme of Feedforward Neural Networks">

                                <b>[12]</b> Huang G B,Zhu Q Y,Siew C K.Extreme learning machine:a new learning scheme of feedforward neural networks[C]//Proceedings of IEEE International Conference on Neural Networks.Piscataway:IEEE Press,2004:985-990.
                            </a>
                        </p>
                        <p id="180">
                            <a id="bibliography_13" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501913101&amp;v=MzExMTVHZXJxUVRNbndaZVp1SHlqbVViL0lKMW9YYmhzPU5pZk9mYks3SHRETnFvOUViZW9NRFh3NG9CTVQ2VDRQUUgvaXJSZA==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[13]</b> Huang G B,Zhu Q Y,Siew C K.Extreme learning machine:theory and applications[J].Neurocomputing,2006,70(1):489-501.
                            </a>
                        </p>
                        <p id="182">
                            <a id="bibliography_14" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=IKJS201810002&amp;v=MDQ2MzlLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnVGeS9oVUx2QkxDYkJmYkc0SDluTnI0OUZab1E=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[14]</b> 陆思源,陆志海,王水花,等.极限学习机综述[J].测控技术,2018,37(10):3-9.
                            </a>
                        </p>
                        <p id="184">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Faster R-CNN:Towards Real-Time Object Detection with Region Proposal Networks">

                                <b>[15]</b> Ren S,Girshick R,Girshick R,et al.Faster R-CNN:Towards real-time object detection with region proposal networks[J].IEEE Transactions on Pattern Analysis &amp; Machine Intelligence,2017,39(6):1137-1149.
                            </a>
                        </p>
                        <p id="186">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=R-FCN:Object Detection via Region-based Fully Convolutional Networks">

                                <b>[16]</b> Dai J,Li Y,He K,et al.R-FCN:Object detection via region-based fully convolutional networks[C]//Neural Information Processing Systems.2016:379-387.
                            </a>
                        </p>
                        <p id="188">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Optimizing Intersection-Over-Union in Deep Neural Networks for Image Segmentation">

                                <b>[17]</b> Rahman M A,Wang Y.Optimizing intersection-over-union in deep neural networks for image segmentation[M]//Advances in Visual Computing.Springer International Publishing,2016:234-244.
                            </a>
                        </p>
                        <p id="190">
                            <a id="bibliography_18" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=BJHK201901019&amp;v=Mjk2NThRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ1RnkvaFVMdkJKeWZEWmJHNEg5ak1ybzlFYlk=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[18]</b> 郭琳,秦世引.遥感图像飞机目标高效搜检深度学习优化算法[J].北京航空航天大学学报,2019,45(1):159-173.
                            </a>
                        </p>
                        <p id="192">
                            <a id="bibliography_19" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DGKQ201904006&amp;v=MjI3MDR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ1RnkvaFVMdkJJU3JBZjdHNEg5ak1xNDlGWW9RS0RIODQ=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[19]</b> 郑志强,刘妍妍,潘长城,李国宁.改进YOLO V3遥感图像飞机识别应用[J].电光与控制,2019,26(4):28-32.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JYRJ201911033" />
        <input id="dpi" type="hidden" value="96" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201911033&amp;v=MDQ5NjZPZVplVnVGeS9oVUx2Qkx6VFpaTEc0SDlqTnJvOUdaNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkw=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="0" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
