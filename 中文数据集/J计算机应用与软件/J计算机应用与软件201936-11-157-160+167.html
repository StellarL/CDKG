<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637134079506506250%26DBCODE%3dCJFD%26TABLEName%3dCJFDTEMP%26FileName%3dJYRJ201911027%26RESULT%3d1%26SIGN%3doUrnLQEOTUWvpCXei78%252feSbJG%252fk%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JYRJ201911027&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JYRJ201911027&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201911027&amp;v=MTQzMjNVUkxPZVplVnVGeS9oVUwzTkx6VFpaTEc0SDlqTnJvOUhZNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckM=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#31" data-title="&lt;b&gt;0 引 言&lt;/b&gt; "><b>0 引 言</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#36" data-title="&lt;b&gt;1 基于AE-CNN的手势识别算法&lt;/b&gt; "><b>1 基于AE-CNN的手势识别算法</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#37" data-title="&lt;b&gt;1.1 前向过程分析&lt;/b&gt;"><b>1.1 前向过程分析</b></a></li>
                                                <li><a href="#44" data-title="&lt;b&gt;1.2 反向过程分析&lt;/b&gt;"><b>1.2 反向过程分析</b></a></li>
                                                <li><a href="#48" data-title="&lt;b&gt;1.3 AE-CNN的算法实现&lt;/b&gt;"><b>1.3 AE-CNN的算法实现</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#58" data-title="&lt;b&gt;2 算法步骤&lt;/b&gt; "><b>2 算法步骤</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#70" data-title="&lt;b&gt;3 实 验&lt;/b&gt; "><b>3 实 验</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#72" data-title="&lt;b&gt;3.1 实验设置&lt;/b&gt;"><b>3.1 实验设置</b></a></li>
                                                <li><a href="#83" data-title="&lt;b&gt;3.2 样本扩充方法&lt;/b&gt;"><b>3.2 样本扩充方法</b></a></li>
                                                <li><a href="#86" data-title="&lt;b&gt;3.3 对比实验&lt;/b&gt;"><b>3.3 对比实验</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#101" data-title="&lt;b&gt;4 结 语&lt;/b&gt; "><b>4 结 语</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#50" data-title="图1 AE-CNN的结构">图1 AE-CNN的结构</a></li>
                                                <li><a href="#74" data-title="图2 实验所用的的手势示例">图2 实验所用的的手势示例</a></li>
                                                <li><a href="#76" data-title="&lt;b&gt;表1 卷机神经网络参数&lt;/b&gt;"><b>表1 卷机神经网络参数</b></a></li>
                                                <li><a href="#89" data-title="图3 原始CNN算法和本文算法的收敛曲线">图3 原始CNN算法和本文算法的收敛曲线</a></li>
                                                <li><a href="#92" data-title="&lt;b&gt;表2 不同算法识别率对比&lt;/b&gt;"><b>表2 不同算法识别率对比</b></a></li>
                                                <li><a href="#94" data-title="&lt;b&gt;表3 多组样本情况下的识别率&lt;/b&gt;"><b>表3 多组样本情况下的识别率</b></a></li>
                                                <li><a href="#97" data-title="&lt;b&gt;表4 多组样本情况下的训练耗时&lt;/b&gt;"><b>表4 多组样本情况下的训练耗时</b></a></li>
                                                <li><a href="#100" data-title="&lt;b&gt;表5 样本扩增实验结果&lt;/b&gt;"><b>表5 样本扩增实验结果</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="127">


                                    <a id="bibliography_1" title=" 李彦冬,郝宗波,雷航.卷积神经网络研究综述[J].计算机应用,2016,36(9):2508-2515." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201609029&amp;v=MjU1NDFyQ1VSTE9lWmVWdUZ5L2hVTDNNTHo3QmQ3RzRIOWZNcG85SGJZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0Y=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         李彦冬,郝宗波,雷航.卷积神经网络研究综述[J].计算机应用,2016,36(9):2508-2515.
                                    </a>
                                </li>
                                <li id="129">


                                    <a id="bibliography_2" title=" Xie S,Hu H.Facial expression recognition with FRR-CNN[J].Electronics Letters,2017,53(4):235-237." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Facial Expression Recognition with FRR-CNN">
                                        <b>[2]</b>
                                         Xie S,Hu H.Facial expression recognition with FRR-CNN[J].Electronics Letters,2017,53(4):235-237.
                                    </a>
                                </li>
                                <li id="131">


                                    <a id="bibliography_3" title=" 王飞,张莹,张东波,等.基于捷径的卷积神经网络在人脸识别中的应用研究[J].电子测量与仪器学报,2018(4):80-86." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=DKFX&amp;filename=DZIY201804012&amp;v=MjUyMzMzenFxQnRHRnJDVVJMT2VaZVZ1RnkvaFVMM01JVGZDZDdHNEg5bk1xNDlFWm9RS0RIODR2UjRUNmo1NE8=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         王飞,张莹,张东波,等.基于捷径的卷积神经网络在人脸识别中的应用研究[J].电子测量与仪器学报,2018(4):80-86.
                                    </a>
                                </li>
                                <li id="133">


                                    <a id="bibliography_4" title=" 陈军清,张毓.基于语言模型的神经网络的文本情感分析[J].现代计算机(专业版),2018(5):24-28." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XDJS201807007&amp;v=MTcxMTFNcUk5Rlk0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdUZ5L2hVTDNNUFNuQmZiRzRIOW4=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                         陈军清,张毓.基于语言模型的神经网络的文本情感分析[J].现代计算机(专业版),2018(5):24-28.
                                    </a>
                                </li>
                                <li id="135">


                                    <a id="bibliography_5" title=" 舒泓新,蔡晓东,梁晓曦,等.一种基于卷积神经网络的视频行人识别方法:中国,CN105631415A[P].2016." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SCPD&amp;filename=CN105631415A&amp;v=MDg2MzJFQ3ptVWJzPUppTzZIcmErSGRESXJvbzBDKzRQRDMxTHh4WVQ2em9PUzNmbXBXRmFlN0tWVEx1ZFpPZHU=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                         舒泓新,蔡晓东,梁晓曦,等.一种基于卷积神经网络的视频行人识别方法:中国,CN105631415A[P].2016.
                                    </a>
                                </li>
                                <li id="137">


                                    <a id="bibliography_6" title=" 苏军雄,见雪婷,刘玮,等.基于可变形卷积神经网络的手势识别方法[J].计算机与现代化,2018(4):62-67." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYXH201804013&amp;v=MDEyNjBMelRUWnJHNEg5bk1xNDlFWjRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ1RnkvaFVMM00=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                         苏军雄,见雪婷,刘玮,等.基于可变形卷积神经网络的手势识别方法[J].计算机与现代化,2018(4):62-67.
                                    </a>
                                </li>
                                <li id="139">


                                    <a id="bibliography_7" title=" Huang X,Lin R.An I-CNN based speech classification algorithm for custom service[C]// 2018 IEEE World Congress on Services (SERVICES),2018:33-34." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=An I-CNN based speech classification algorithm for custom service">
                                        <b>[7]</b>
                                         Huang X,Lin R.An I-CNN based speech classification algorithm for custom service[C]// 2018 IEEE World Congress on Services (SERVICES),2018:33-34.
                                    </a>
                                </li>
                                <li id="141">


                                    <a id="bibliography_8" title=" 周风余,尹建芹,杨阳,等.基于时序深度置信网络的在线人体动作识别[J].自动化学报,2016,42(7):1030-1039." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201607006&amp;v=MDc5MzFNcUk5RllvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdUZ5L2hVTDNNS0NMZlliRzRIOWY=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                         周风余,尹建芹,杨阳,等.基于时序深度置信网络的在线人体动作识别[J].自动化学报,2016,42(7):1030-1039.
                                    </a>
                                </li>
                                <li id="143">


                                    <a id="bibliography_9" title=" Ranzato M A,Poultney C,Chopra S.Efficient learning of sparse representations with an energy-based model[C]// Proceedings of the 2007 Advances in Neural Information Processing Systems.USA:MIT Press,2007:1137-1144." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Efficient learning of sparse representations with an energy based model">
                                        <b>[9]</b>
                                         Ranzato M A,Poultney C,Chopra S.Efficient learning of sparse representations with an energy-based model[C]// Proceedings of the 2007 Advances in Neural Information Processing Systems.USA:MIT Press,2007:1137-1144.
                                    </a>
                                </li>
                                <li id="145">


                                    <a id="bibliography_10" title=" Hamda M,Mahmoudi A.Hand gesture recognition using kinect&#39;s geometric and HOG features[C]// Proceedings of the 2nd international Conference on Big Data,Cloud and Applications.ACM,2017." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Hand Gesture Recognition Using Kinect&amp;#39;&amp;#39;s Geometric and HOG Features">
                                        <b>[10]</b>
                                         Hamda M,Mahmoudi A.Hand gesture recognition using kinect&#39;s geometric and HOG features[C]// Proceedings of the 2nd international Conference on Big Data,Cloud and Applications.ACM,2017.
                                    </a>
                                </li>
                                <li id="147">


                                    <a id="bibliography_11" title=" Malysa G,Malysa G,Wang D,et al.Hidden Markov Model-based gesture recognition with FMCW radar[C]// 2016 IEEE Global Conference on Signal and Information Processing (GlobalSIP).IEEE,2017." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Hidden Markov Model-based gesture recognition with FMCW radar">
                                        <b>[11]</b>
                                         Malysa G,Malysa G,Wang D,et al.Hidden Markov Model-based gesture recognition with FMCW radar[C]// 2016 IEEE Global Conference on Signal and Information Processing (GlobalSIP).IEEE,2017.
                                    </a>
                                </li>
                                <li id="149">


                                    <a id="bibliography_12" title=" Yamashita T,Watasue T.Hand posture recognition based on bottom-up structured deep convolutional neural network with curriculum learning[C]// 2014 IEEE International Conference on Image Processing (ICIP).IEEE,2015:853-857." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Hand posture recognition based on bottom-up structured deep convolutional neural network with curriculum learning">
                                        <b>[12]</b>
                                         Yamashita T,Watasue T.Hand posture recognition based on bottom-up structured deep convolutional neural network with curriculum learning[C]// 2014 IEEE International Conference on Image Processing (ICIP).IEEE,2015:853-857.
                                    </a>
                                </li>
                                <li id="151">


                                    <a id="bibliography_13" title=" Barros P,Magg S,Weber C,et al.A multichannel convolutional neural network for hand posture recognition[C]// International Conference on Artificial Neural Networks.Springer,Cham,2014:403-410." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A multichannel convolutional neural network for hand posture recognition">
                                        <b>[13]</b>
                                         Barros P,Magg S,Weber C,et al.A multichannel convolutional neural network for hand posture recognition[C]// International Conference on Artificial Neural Networks.Springer,Cham,2014:403-410.
                                    </a>
                                </li>
                                <li id="153">


                                    <a id="bibliography_14" title=" John V,Mita S,Liu Z,et al.Pedestrian detection in thermal images using adaptive fuzzy C-means clustering and convolutional neural networks[C]// Proceedings of the 14th IAPR International Conference on Machine Vision Applications.Tokyo,Japan:IEEE,2015:246-249." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Pedestrian detection in thermal images using adaptive fuzzy C-means clustering and convolutional neural networks">
                                        <b>[14]</b>
                                         John V,Mita S,Liu Z,et al.Pedestrian detection in thermal images using adaptive fuzzy C-means clustering and convolutional neural networks[C]// Proceedings of the 14th IAPR International Conference on Machine Vision Applications.Tokyo,Japan:IEEE,2015:246-249.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JYRJ" target="_blank">计算机应用与软件</a>
                2019,36(11),157-160+167 DOI:10.3969/j.issn.1000-386x.2019.11.026            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于AE-CNN的手势识别算法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E4%BB%98%E4%BC%98&amp;code=25081169&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">付优</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E4%BB%BB%E8%8A%B3&amp;code=08525086&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">任芳</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%B1%B1%E8%A5%BF%E5%BB%BA%E7%AD%91%E8%81%8C%E4%B8%9A%E6%8A%80%E6%9C%AF%E5%AD%A6%E9%99%A2%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%B7%A5%E7%A8%8B%E7%B3%BB&amp;code=0148601&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">山西建筑职业技术学院计算机工程系</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E9%99%95%E8%A5%BF%E5%B8%88%E8%8C%83%E5%A4%A7%E5%AD%A6%E6%95%B0%E5%AD%A6%E4%B8%8E%E4%BF%A1%E6%81%AF%E7%A7%91%E5%AD%A6%E5%AD%A6%E9%99%A2&amp;code=0068380&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">陕西师范大学数学与信息科学学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>在手势识别的过程中,手势的多样性和复杂程度会对手势识别率造成很大的影响。随着深度学习的快速发展,卷积神经网络在手势识别领域取得了突破性进展。但基于卷积神经网络的方法仍存在收敛速度慢、识别率低等问题,因此手势识别很难取得较好成果。为了解决卷积神经网络在手势识别中存在的收敛速度慢、识别率低问题,提出一种AE-CNN的手势识别算法。实验结果表明,该算法收敛速度快、识别准确率高,并且没有明显增加识别过程的耗时性。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%89%8B%E5%8A%BF%E8%AF%86%E5%88%AB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">手势识别;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">卷积神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度学习;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    付优，讲师，主研领域:计算机应用技术。;
                                </span>
                                <span>
                                    任芳，副教授。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-03-05</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金项目(61602072);</span>
                    </p>
            </div>
                    <h1><b>HAND GESTURE RECOGNITION ALGORITHM BASED ON AE-CNN</b></h1>
                    <h2>
                    <span>Fu You</span>
                    <span>Ren Fang</span>
            </h2>
                    <h2>
                    <span>Department of Computer Engineering,Shanxi College of Architectural</span>
                    <span>College of Mathematics and Information Science, Shaanxi Normal University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>In the process of gesture recognition, the diversity and complexity of gesture greatly influence the recognition rate. With the rapid development of deep learning, the convolution neural network(CNN) has made a breakthrough in the field of gesture recognition. The existing methods based on CNN still have some problems such as slow convergence speed and low recognition rate, so it is difficult to achieve good results in gesture recognition. To solve these problems, this paper proposes an AE-CNN recognition algorithm. The results show that the proposed algorithm has fast convergence speed, high recognition rate, and does not significantly increase the time consumption of the recognition process.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Hand%20gesture%20recognition&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Hand gesture recognition;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Convolutional%20neural%20network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Convolutional neural network;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Deep%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Deep learning;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2019-03-05</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="31" name="31" class="anchor-tag"><b>0 引 言</b></h3>
                <div class="p1">
                    <p id="32">在机器学习领域中,深度学习是一个崭新的研究方向,其本质是一个具有多隐层的非线性网络模型:对大规模原始数据样本进行训练,然后通过网络模型对原始数据样本的特征进行提取,最终实现对原始数据进行预测或分类。</p>
                </div>
                <div class="p1">
                    <p id="33">在计算机视觉和图像识别领域,卷积神经网络(CNN)取得了显著的成绩<citation id="161" type="reference"><link href="127" rel="bibliography" /><link href="129" rel="bibliography" /><link href="131" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>,<a class="sup">3</a>]</sup></citation>。此外,深度学习广泛应用于数据挖掘、自然语言处理<citation id="155" type="reference"><link href="133" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>、行人检测<citation id="156" type="reference"><link href="135" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>、手势识别<citation id="157" type="reference"><link href="137" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>和语音识别<citation id="158" type="reference"><link href="139" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>等领域。与深度置信网络<citation id="159" type="reference"><link href="141" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>、S层自动编码<citation id="160" type="reference"><link href="143" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>等其他深度神经网络相比,CNN可以直接处理二维图像。当二维图像转换为一维信号时,会损失输入数据的空间结构特征,而通过CNN算法进行处理时可避免这种损失。因此,基于CNN的识别算法是目前流行的图像识别方法之一,并且其识别结果更加可靠。</p>
                </div>
                <div class="p1">
                    <p id="34">手势识别是人机交互系统发展的一个重要领域。常见的手势识别方式很多,如:基于几何特征的识别方法通过提取手势的几何特征信息进行识别,具有良好的稳定性,但其不能通过提升样本量的同时进行识别率的提升<citation id="162" type="reference"><link href="145" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>;基于隐马尔科夫模型的识别方法虽然具有描述手势时空变化的能力,但是该方法的识别速度却不尽如人意<citation id="163" type="reference"><link href="147" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>。随着卷积神经网络在计算机视觉的迅速发展,卷积神经网络在手势识别上的应用有着突破进展,如:Takayoshi<citation id="164" type="reference"><link href="149" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>在输入卷积网络之前讨论了预处理过程和手势识别过程的融合,实现了端到端手势识别,并使得手势识别率得到了提升;Barros等<citation id="165" type="reference"><link href="151" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>创造性地使用立体声卷积核进行手势识别,获得了更好的手势识别识别率。虽然上述方法取得了良好的识别结果,但它们仍存在收敛速度慢、识别率低的问题。</p>
                </div>
                <div class="p1">
                    <p id="35">为了进一步提高CNN的收敛速度,降低训练难度,本文提出了一种AE-CNN算法。通过对CNN的特征提取和分类过程的研究,分析分类错误的原因,最终提出了一种基于AE-CNN的手势识别算法。首先,通过神经网络的识别和连续迭代对特征残差进行特性提取;然后进行局部自适应增强;最后,特征参数通过隐层的反向反馈,进行有效地训练。同时,为了解决样本数量少的问题,本文提出了一种新的样本扩增方法,即通过对原样本进行平移、旋转、缩放和波纹形变等处理,增加了样本量。</p>
                </div>
                <h3 id="36" name="36" class="anchor-tag"><b>1 基于AE-CNN的手势识别算法</b></h3>
                <h4 class="anchor-tag" id="37" name="37"><b>1.1 前向过程分析</b></h4>
                <div class="p1">
                    <p id="38">在CNN前向过程中,特征提取操作实现了卷积和池化效果。假设有<i>m</i>个卷积层,第<i>i</i>个输入特征为<i>M</i><sub><i>i</i></sub>,对应的卷积核为<i>C</i><sub><i>i</i></sub>。偏值为<i>B</i><sub>1</sub>,激活函数为<i>f</i>,则卷积层的输出特征<i>F</i><sub><i>c</i></sub>表示为:</p>
                </div>
                <div class="p1">
                    <p id="39"><mathml id="103"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>F</mi><msub><mrow></mrow><mi>c</mi></msub><mo>=</mo><mi>f</mi><mspace width="0.25em" /><mrow><mo>(</mo><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mi>c</mi></mstyle><mi>o</mi><mi>n</mi><mo stretchy="false">(</mo><mi>Μ</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mi>C</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo>+</mo><mi>B</mi><msub><mrow></mrow><mn>1</mn></msub></mrow><mo>)</mo></mrow></mrow></math></mathml>      (1)</p>
                </div>
                <div class="p1">
                    <p id="40">式中:<i>con</i>表示卷积函数。</p>
                </div>
                <div class="p1">
                    <p id="41">对卷积输出特性进行池化操作,进一步减小维数;然后,通过对全连接层进行权值变换和激活,可获得分类结果。全连接层的分类函数可表示为:</p>
                </div>
                <div class="p1">
                    <p id="42"><i>F</i><sub><i>o</i></sub>=<i>f</i>(<i>WT</i>+<i>B</i><sub>2</sub>)      (2)</p>
                </div>
                <div class="p1">
                    <p id="43">式中:<i>T</i>是完全连接层的输入特性;<i>W</i>是相应的权重;<i>B</i><sub>2</sub>是偏差值。</p>
                </div>
                <h4 class="anchor-tag" id="44" name="44"><b>1.2 反向过程分析</b></h4>
                <div class="p1">
                    <p id="45">分类误差在CNN的反向过程中被传播到隐层,实现了卷积核和全连接矩阵权值与偏置的更新。更新后的参数用于下一次迭代的前向过程,因此CNN的迭代更新提高了识别率。假设步长为<i>η</i>,在CNN隐层中,权值<i>ω</i>和偏差<i>b</i>由<i>ω</i><sub>1</sub>、<i>b</i><sub>1</sub>更新为<i>ω</i><sub>2</sub>、<i>b</i><sub>2</sub>,更新函数如下:</p>
                </div>
                <div class="p1">
                    <p id="46"><i>ω</i><sub>2</sub>=<i>ω</i><sub>1</sub>-<i>η</i>ᐁ<i>ω</i>      (3)</p>
                </div>
                <div class="p1">
                    <p id="47"><i>b</i><sub>2</sub>=<i>b</i><sub>1</sub>-<i>η</i>ᐁ<i>b</i>      (4)</p>
                </div>
                <h4 class="anchor-tag" id="48" name="48"><b>1.3 AE-CNN的算法实现</b></h4>
                <div class="p1">
                    <p id="49">AE-CNN结构如图1所示。前向过程中包含了目标分类和提取特征。误差的反向反馈包含在反向过程中。输入数据经过前向处理后进行分类,获得分类结果。每个结果对应一个唯一的分类。输入数据真实属于的类别称为真值类别,真值类的对应值为1,另一个类的对应值为0。</p>
                </div>
                <div class="area_img" id="50">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201911027_050.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 AE-CNN的结构" src="Detail/GetImg?filename=images/JYRJ201911027_050.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 AE-CNN的结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201911027_050.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="51">在隐层中,目标误差函数<i>E</i>(<i>ω</i>,<i>b</i>)用于度量每个参数对输入数据的学习效果。为了使分类<i>y</i><sub><i>i</i></sub>尽可能接近真值分类<i>y</i>′<sub><i>i</i></sub>,在CNN的前向与反向过程之间增加自适应增强模块。在增强模块中,对分类结果进行特征分析,然后通过调整增强系数对特征误差进行自适应地调整。最后,通过反向过程将增强后的特征误差反馈到隐层,达到降低目标错误函数输出和提升下一次分类效果的目的。当相近的两个误差输出不大于阈值时,则可以判断收敛和学习任务均已实现。关于一个<i>n</i>的分类问题,它的目标错误函数表示如下:</p>
                </div>
                <div class="p1">
                    <p id="52"><mathml id="104"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>E</mi><mo stretchy="false">(</mo><mi>ω</mi><mo>,</mo><mi>b</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><mn>2</mn></mfrac></mrow></math></mathml><mathml id="105"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>-</mo><mn>1</mn></mrow><mi>n</mi></munderover><mo stretchy="false">(</mo></mstyle><mi>y</mi><msub><mrow></mrow><mi>j</mi></msub><mo>-</mo><msup><mi>y</mi><mo>′</mo></msup><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo><msup><mrow></mrow><mn>2</mn></msup><mo>=</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mo stretchy="false">(</mo></mstyle><mi>e</mi><mi>r</mi><mi>r</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo><msup><mrow></mrow><mn>2</mn></msup></mrow></math></mathml>      (5)</p>
                </div>
                <div class="p1">
                    <p id="53">式中:<i>j</i>表示分类个数;<i>err</i>是其分类误差。根据梯度下降法,权值和偏值的梯度变化可表示为:</p>
                </div>
                <div class="p1">
                    <p id="54"><mathml id="106"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo>∇</mo><mi>ω</mi><mo>=</mo><mfrac><mrow><mo>∂</mo><mi>E</mi><mo stretchy="false">(</mo><mi>ω</mi><mo>,</mo><mi>b</mi><mo stretchy="false">)</mo></mrow><mrow><mo>∂</mo><mi>ω</mi></mrow></mfrac><mo>=</mo><mi>d</mi><mo>⋅</mo><mi>x</mi></mrow></math></mathml>      (6)</p>
                </div>
                <div class="p1">
                    <p id="55"><mathml id="107"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo>∇</mo><mi>b</mi><mo>=</mo><mfrac><mrow><mo>∂</mo><mi>E</mi><mo stretchy="false">(</mo><mi>ω</mi><mo>,</mo><mi>b</mi><mo stretchy="false">)</mo></mrow><mrow><mo>∂</mo><mi>b</mi></mrow></mfrac><mo>=</mo><mi>d</mi></mrow></math></mathml>      (7)</p>
                </div>
                <div class="p1">
                    <p id="56"><i>d</i>=(<i>y</i><sub><i>j</i></sub>-<i>y</i>′<sub><i>j</i></sub>)<i>f</i> ′=<i>err</i><sub><i>j</i></sub>·<i>f</i> ′      (8)</p>
                </div>
                <div class="p1">
                    <p id="57">式中:<i>d</i>是剩余值;<i>x</i>是输入特性中的值;<i>f</i>′是激活函数<i>f</i>的导数。</p>
                </div>
                <h3 id="58" name="58" class="anchor-tag"><b>2 算法步骤</b></h3>
                <div class="p1">
                    <p id="59">基于AE-CNN手势识别算法的步骤为:</p>
                </div>
                <div class="p1">
                    <p id="60">1) 分类误差的计算。分类结果通过CNN的前向处理得到,然后将分类真值与分类结果进行对比,可计算得出分类误差。</p>
                </div>
                <div class="p1">
                    <p id="61">2) 分类结果特征的提取。分析输出层分类结果,将分类结果的两个最大值设为特征值。</p>
                </div>
                <div class="p1">
                    <p id="62">3) 计算增强系数。增强系数是由基于迭代的时间和前向的分类结果计算得到。若分类正确,则与分类特性对应的错误值将增加<i>α</i><sub>1</sub>倍。相反,当分类错误时,其会增加<i>α</i><sub>2</sub>倍。因此,自适应增强系数<i>α</i>可以表示为:</p>
                </div>
                <div class="p1">
                    <p id="63"><mathml id="108"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>α</mi><mo>=</mo><mi>k</mi><mrow><mo>(</mo><mrow><mtext>e</mtext><msup><mrow></mrow><mrow><mfrac><mn>1</mn><mi>g</mi></mfrac></mrow></msup><mo>-</mo><mfrac><mtext>e</mtext><mi>g</mi></mfrac></mrow><mo>)</mo></mrow><mo>+</mo><mi>θ</mi><mo>+</mo><mi>c</mi></mrow></math></mathml>      (9)</p>
                </div>
                <div class="p1">
                    <p id="64">式中:<i>k</i>是乘积系数;<i>g</i>是当前迭代的时间;<i>θ</i>是修正系数;<i>c</i>是常数。根据不同的数据集,用式(9)计算<i>α</i><sub>1</sub>和<i>α</i><sub>2</sub>的值。</p>
                </div>
                <div class="p1">
                    <p id="65">4) 自适应增强分类特征对应的误差值。假设<i>err</i>是分类特性的错误值,<i>s</i>是大小写正确分类中的标志。此时,增强函数<i>err</i>′<sub><i>s</i></sub>由下式计算得到:</p>
                </div>
                <div class="p1">
                    <p id="66"><i>err</i>′<sub><i>s</i></sub>=<i>α</i><sub><i>s</i></sub>·<i>err</i><sub><i>s</i></sub>      (10)</p>
                </div>
                <div class="p1">
                    <p id="67">5) 增强残差的计算。增强残差包括剩余误差的残差和特征误差后的残差,由式(8)表示。</p>
                </div>
                <div class="p1">
                    <p id="68">6) 将增强后的残余反馈给隐层。在隐层中,权值和偏差按式(6)和式(7)计算。</p>
                </div>
                <div class="p1">
                    <p id="69">7) 更新模型。最后利用式(3)和式(4)中的函数更新隐层的权值和偏差。</p>
                </div>
                <h3 id="70" name="70" class="anchor-tag"><b>3 实 验</b></h3>
                <div class="p1">
                    <p id="71">在实验部分,将从算法的收敛性能、识别率和训练时间三个方面对本文算法进行验证。</p>
                </div>
                <h4 class="anchor-tag" id="72" name="72"><b>3.1 实验设置</b></h4>
                <div class="p1">
                    <p id="73">实验采用了Jochen Triesch数据库(JTD)。这个数据库包含10个手势,来自24个不同背景的人,一个亮的,一个暗的,在复杂背景前完成的。所有图片的大小为128×128,并集中在手势。JTD数据库的示例如图2所示。</p>
                </div>
                <div class="area_img" id="74">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201911027_074.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 实验所用的的手势示例" src="Detail/GetImg?filename=images/JYRJ201911027_074.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 实验所用的的手势示例  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201911027_074.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="75">在样本集上执行二进制处理和边界删除。最后,大小统一为28×28像素。CNN采用随机梯度下降算法。在两个卷积层中,卷积核的个数分别设为6和72。其他参数设置如表1所示。</p>
                </div>
                <div class="area_img" id="76">
                    <p class="img_tit"><b>表1 卷机神经网络参数</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="76" border="1"><tr><td><br />层名</td><td>特征图大小</td><td>层数</td></tr><tr><td><br />输入层</td><td>28×28</td><td>1</td></tr><tr><td><br />卷积核</td><td>5×5</td><td>2</td></tr><tr><td><br />池化域</td><td>2×2</td><td>2</td></tr><tr><td><br />全连接层</td><td>192×1</td><td>1</td></tr><tr><td><br />输出层</td><td>10×1</td><td>1</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="77">实验中使用的自适应增强系数为<i>α</i><sub>1</sub>、<i>α</i><sub>2</sub>,表达式如下:</p>
                </div>
                <div class="p1">
                    <p id="78"><mathml id="109"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>α</mi><msub><mrow></mrow><mn>1</mn></msub><mo>=</mo><mfrac><mn>5</mn><mn>2</mn></mfrac><mrow><mo>(</mo><mrow><mtext>e</mtext><msup><mrow></mrow><mrow><mfrac><mn>1</mn><mi>g</mi></mfrac></mrow></msup><mo>-</mo><mfrac><mtext>e</mtext><mi>g</mi></mfrac></mrow><mo>)</mo></mrow><mo>+</mo><mi>θ</mi><msub><mrow></mrow><mn>1</mn></msub></mrow></math></mathml>      (11)</p>
                </div>
                <div class="p1">
                    <p id="79"><mathml id="110"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>θ</mi><msub><mrow></mrow><mn>1</mn></msub><mo>=</mo><mfrac><mrow><mn>2</mn><mn>1</mn></mrow><mn>5</mn></mfrac><mrow><mi>sgn</mi></mrow><mo stretchy="false">(</mo><mi>g</mi><mo>-</mo><mn>1</mn><mo stretchy="false">)</mo><mtext>e</mtext><msup><mrow></mrow><mrow><mfrac><mn>1</mn><mrow><mi>ln</mi><mo stretchy="false">(</mo><mi>g</mi><mo>+</mo><mn>4</mn><mo stretchy="false">)</mo></mrow></mfrac></mrow></msup><mo>+</mo><mfrac><mrow><mn>1</mn><mn>6</mn></mrow><mi>g</mi></mfrac></mrow></math></mathml>      (12)</p>
                </div>
                <div class="p1">
                    <p id="80"><mathml id="111"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>α</mi><msub><mrow></mrow><mn>2</mn></msub><mo>=</mo><mfrac><mn>5</mn><mn>2</mn></mfrac><mrow><mo>(</mo><mrow><mtext>e</mtext><msup><mrow></mrow><mrow><mfrac><mn>1</mn><mi>g</mi></mfrac></mrow></msup><mo>-</mo><mfrac><mtext>e</mtext><mi>g</mi></mfrac></mrow><mo>)</mo></mrow><mo>+</mo><mi>θ</mi><msub><mrow></mrow><mn>2</mn></msub></mrow></math></mathml>      (13)</p>
                </div>
                <div class="p1">
                    <p id="81"><mathml id="112"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>θ</mi><msub><mrow></mrow><mn>2</mn></msub><mo>=</mo><mrow><mi>sgn</mi></mrow><mo stretchy="false">(</mo><mi>g</mi><mo>-</mo><mn>1</mn><mo stretchy="false">)</mo><mrow><mo>[</mo><mrow><mi>ln</mi><mo stretchy="false">(</mo><mi>g</mi><mo>+</mo><mn>4</mn><mo stretchy="false">)</mo><mo>-</mo><mfrac><mn>7</mn><mn>5</mn></mfrac></mrow><mo>]</mo></mrow><mo>+</mo><mfrac><mn>3</mn><mrow><mn>2</mn><mi>g</mi></mrow></mfrac></mrow></math></mathml>      (14)</p>
                </div>
                <div class="p1">
                    <p id="82">式中:<i>g</i>是迭代时间;<i>θ</i><sub>1</sub>、<i>θ</i><sub>2</sub>是相应的修正函数。</p>
                </div>
                <h4 class="anchor-tag" id="83" name="83"><b>3.2 样本扩充方法</b></h4>
                <div class="p1">
                    <p id="84">深度学习训练需要大量的样本进行训练,在样本容量缺乏的情况下,通常需要人为地增加训练样本的数量。常用的增强方法有弹性变形、噪声和仿射变换等。在不改动手势结构关系的基础上,提出了一种结合平移、旋转和缩放的波形扭曲的方法,从而实现对训练样本量的扩充。</p>
                </div>
                <div class="p1">
                    <p id="85">波纹扭曲是一种算法,它通过正弦函数<mathml id="113"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>y</mi><mo>=</mo><mi>A</mi><mrow><mi>sin</mi></mrow><mrow><mo>(</mo><mrow><mfrac><mrow><mn>2</mn><mtext>π</mtext></mrow><mi>Τ</mi></mfrac><mo>×</mo><mi>x</mi></mrow><mo>)</mo></mrow></mrow></math></mathml>来转换原始图像坐标。通过正弦函数的振幅<i>A</i>和周期<i>T</i>的调整,可以快速生成大量的假样本。例如,当正弦波的振幅和周期分别控制在区间[0,6]和[80,120],旋转角度的字符在±10°之内,水平和垂直方向平移的范围在10%以内,缩放的比例在10%,随机生成的30幅伪样本图像。</p>
                </div>
                <h4 class="anchor-tag" id="86" name="86"><b>3.3 对比实验</b></h4>
                <h4 class="anchor-tag" id="87" name="87">1) 收敛性能对比。</h4>
                <div class="p1">
                    <p id="88">原始CNN算法和本文算法的收敛曲线如图3所示。结果表明,原始算法的收敛性指数和算法的收敛性均随训练频率的增加而降低,并在第60次迭代中实现了收敛。因此通过对比收敛曲线可得知,本文算法在迭代过程中收敛速度快于原算法,并且收敛效果较好。</p>
                </div>
                <div class="area_img" id="89">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201911027_089.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 原始CNN算法和本文算法的收敛曲线" src="Detail/GetImg?filename=images/JYRJ201911027_089.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 原始CNN算法和本文算法的收敛曲线  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201911027_089.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="90" name="90">2) 识别率对比。</h4>
                <div class="p1">
                    <p id="91">将所提出的算法与深度神经网络模型<citation id="166" type="reference"><link href="153" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>中的多尺度梯度进行比较,得到的识别率如表2所示。文献<citation id="167" type="reference">[<a class="sup">14</a>]</citation>中使用的特征分别为一般梯度特征、多尺度梯度特征、多尺度Gabor特征要点、梯度取向直方图(HOG)以及局部二值模式(LBP)。分类器都是5层CNN。从表2中可以看出,与其他几种方法相比,本文算法的识别率得到了提高。</p>
                </div>
                <div class="area_img" id="92">
                    <p class="img_tit"><b>表2 不同算法识别率对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="92" border="1"><tr><td><br />方法</td><td>识别率/%</td></tr><tr><td><br />LBP+CNN</td><td>86.921</td></tr><tr><td><br />GIST+CNN</td><td>91.866</td></tr><tr><td><br />HOG+CNN</td><td>93.971</td></tr><tr><td><br />梯度特征+CNN</td><td>95.261</td></tr><tr><td><br />多尺度梯度特征+CNN</td><td>97.329</td></tr><tr><td><br />本文算法</td><td>98.352</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="93">为了加强检验算法的鲁棒性和稳定性,随机选取20个字符作为一组样本,执行4次,得到另外4组样本。然后对另外4组样本进行实验。表3所示为识别率结果,可以看出,不同样本集的识别结果是稳定的,因此提出的算法具备较强的鲁棒性。</p>
                </div>
                <div class="area_img" id="94">
                    <p class="img_tit"><b>表3 多组样本情况下的识别率</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="94" border="1"><tr><td><br />组号</td><td>最终识别率/%</td></tr><tr><td><br />1</td><td>98.352</td></tr><tr><td><br />2</td><td>98.841</td></tr><tr><td><br />3</td><td>98.562</td></tr><tr><td><br />4</td><td>98.725</td></tr><tr><td><br />5</td><td>98.785</td></tr><tr><td><br />平均识别率</td><td>98.653<br /></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="95" name="95">3) 训练时间对比。</h4>
                <div class="p1">
                    <p id="96">对于5组样本,分别计算原始CNN算法和本文提出算法的训练时间。由表4可以看出,原来的CNN算法和本文提出的算法训练时间几乎是一样的,其时间差异小到可以忽略不计。</p>
                </div>
                <div class="area_img" id="97">
                    <p class="img_tit"><b>表4 多组样本情况下的训练耗时</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="97" border="1"><tr><td><br />组号</td><td>原始算法耗时/s</td><td>本文算法耗时/s</td></tr><tr><td><br />1</td><td>3 998.03</td><td>4 016.27</td></tr><tr><td><br />2</td><td>4 438.82</td><td>4 587.55</td></tr><tr><td><br />3</td><td>4 183.72</td><td>4 285.42</td></tr><tr><td><br />4</td><td>4 119.45</td><td>4 626.07</td></tr><tr><td><br />5</td><td>3 992.79</td><td>4 284.28</td></tr><tr><td><br />平均耗时</td><td>4 146.56</td><td>4 359.92</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="98" name="98">4) 样本扩增实验。</h4>
                <div class="p1">
                    <p id="99">为了验证样本扩增试验的性能,对上述5组数据进行样本扩增。该算法的平均识别率和平均训练时间如表5所示。可以看出,样本扩增已延长了一定的训练时间,但它对识别率的贡献更为重要。因此,样本增强是深度学习模型训练的必要步骤。</p>
                </div>
                <div class="area_img" id="100">
                    <p class="img_tit"><b>表5 样本扩增实验结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="100" border="1"><tr><td><br />样本</td><td>识别率/%</td><td>耗时/s</td></tr><tr><td><br />原始样本</td><td>98.653</td><td>4 359.92</td></tr><tr><td><br />原始样本+扩增样本</td><td>98.892</td><td>5 842.28</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h3 id="101" name="101" class="anchor-tag"><b>4 结 语</b></h3>
                <div class="p1">
                    <p id="102">通过对CNN训练过程中误差产生的原因及其反馈模型进行分析,提出了基于AE-CNN的手势算法。实验结果表明,本文算法收敛速度快、识别准确率高,并且没有明显增加识别过程的耗时性。另外,为处理样本量缺乏的难题,本文提出了一种基于波形扭曲的样本量扩增方法,进一步提高本文算法的识别率。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="127">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201609029&amp;v=MzE4MDRkN0c0SDlmTXBvOUhiWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnVGeS9oVUwzTUx6N0I=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> 李彦冬,郝宗波,雷航.卷积神经网络研究综述[J].计算机应用,2016,36(9):2508-2515.
                            </a>
                        </p>
                        <p id="129">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Facial Expression Recognition with FRR-CNN">

                                <b>[2]</b> Xie S,Hu H.Facial expression recognition with FRR-CNN[J].Electronics Letters,2017,53(4):235-237.
                            </a>
                        </p>
                        <p id="131">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=DKFX&amp;filename=DZIY201804012&amp;v=MTYwOTBuTXE0OUVab1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnVGeS9oVUwzTUlUZkNkN0c0SDk=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> 王飞,张莹,张东波,等.基于捷径的卷积神经网络在人脸识别中的应用研究[J].电子测量与仪器学报,2018(4):80-86.
                            </a>
                        </p>
                        <p id="133">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XDJS201807007&amp;v=MDQ1NTZVTDNNUFNuQmZiRzRIOW5NcUk5Rlk0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdUZ5L2g=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b> 陈军清,张毓.基于语言模型的神经网络的文本情感分析[J].现代计算机(专业版),2018(5):24-28.
                            </a>
                        </p>
                        <p id="135">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SCPD&amp;filename=CN105631415A&amp;v=MDI3NTk9SmlPNkhyYStIZERJcm9vMEMrNFBEMzFMeHhZVDZ6b09TM2ZtcFdGYWU3S1ZUTHVkWk9kdUVDem1VYnM=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b> 舒泓新,蔡晓东,梁晓曦,等.一种基于卷积神经网络的视频行人识别方法:中国,CN105631415A[P].2016.
                            </a>
                        </p>
                        <p id="137">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYXH201804013&amp;v=MTczNjdUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ1RnkvaFVMM01MelRUWnJHNEg5bk1xNDlFWjRRS0RIODR2UjQ=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b> 苏军雄,见雪婷,刘玮,等.基于可变形卷积神经网络的手势识别方法[J].计算机与现代化,2018(4):62-67.
                            </a>
                        </p>
                        <p id="139">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=An I-CNN based speech classification algorithm for custom service">

                                <b>[7]</b> Huang X,Lin R.An I-CNN based speech classification algorithm for custom service[C]// 2018 IEEE World Congress on Services (SERVICES),2018:33-34.
                            </a>
                        </p>
                        <p id="141">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO201607006&amp;v=MDAwODI4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnVGeS9oVUwzTUtDTGZZYkc0SDlmTXFJOUZZb1FLREg=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b> 周风余,尹建芹,杨阳,等.基于时序深度置信网络的在线人体动作识别[J].自动化学报,2016,42(7):1030-1039.
                            </a>
                        </p>
                        <p id="143">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Efficient learning of sparse representations with an energy based model">

                                <b>[9]</b> Ranzato M A,Poultney C,Chopra S.Efficient learning of sparse representations with an energy-based model[C]// Proceedings of the 2007 Advances in Neural Information Processing Systems.USA:MIT Press,2007:1137-1144.
                            </a>
                        </p>
                        <p id="145">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Hand Gesture Recognition Using Kinect&amp;#39;&amp;#39;s Geometric and HOG Features">

                                <b>[10]</b> Hamda M,Mahmoudi A.Hand gesture recognition using kinect's geometric and HOG features[C]// Proceedings of the 2nd international Conference on Big Data,Cloud and Applications.ACM,2017.
                            </a>
                        </p>
                        <p id="147">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Hidden Markov Model-based gesture recognition with FMCW radar">

                                <b>[11]</b> Malysa G,Malysa G,Wang D,et al.Hidden Markov Model-based gesture recognition with FMCW radar[C]// 2016 IEEE Global Conference on Signal and Information Processing (GlobalSIP).IEEE,2017.
                            </a>
                        </p>
                        <p id="149">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Hand posture recognition based on bottom-up structured deep convolutional neural network with curriculum learning">

                                <b>[12]</b> Yamashita T,Watasue T.Hand posture recognition based on bottom-up structured deep convolutional neural network with curriculum learning[C]// 2014 IEEE International Conference on Image Processing (ICIP).IEEE,2015:853-857.
                            </a>
                        </p>
                        <p id="151">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A multichannel convolutional neural network for hand posture recognition">

                                <b>[13]</b> Barros P,Magg S,Weber C,et al.A multichannel convolutional neural network for hand posture recognition[C]// International Conference on Artificial Neural Networks.Springer,Cham,2014:403-410.
                            </a>
                        </p>
                        <p id="153">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Pedestrian detection in thermal images using adaptive fuzzy C-means clustering and convolutional neural networks">

                                <b>[14]</b> John V,Mita S,Liu Z,et al.Pedestrian detection in thermal images using adaptive fuzzy C-means clustering and convolutional neural networks[C]// Proceedings of the 14th IAPR International Conference on Machine Vision Applications.Tokyo,Japan:IEEE,2015:246-249.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JYRJ201911027" />
        <input id="dpi" type="hidden" value="96" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201911027&amp;v=MTQzMjNVUkxPZVplVnVGeS9oVUwzTkx6VFpaTEc0SDlqTnJvOUhZNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckM=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="0" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
