<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637135595869377500%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJYRJ201909030%26RESULT%3d1%26SIGN%3dGHKHNsxYGUmiORb6iIkmsyhaXOA%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JYRJ201909030&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JYRJ201909030&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201909030&amp;v=MDkyMDM0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVadEZ5bmhXN3JPTHpUWlpMRzRIOWpNcG85R1pJUUtESDg=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#45" data-title="&lt;b&gt;0 引 言&lt;/b&gt; "><b>0 引 言</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#53" data-title="&lt;b&gt;1 音频特征提取和设计&lt;/b&gt; "><b>1 音频特征提取和设计</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#54" data-title="&lt;b&gt;1.1 数据库&lt;/b&gt;"><b>1.1 数据库</b></a></li>
                                                <li><a href="#59" data-title="&lt;b&gt;1.2 数据预处理&lt;/b&gt;"><b>1.2 数据预处理</b></a></li>
                                                <li><a href="#61" data-title="&lt;b&gt;1.3 音频特征提取&lt;/b&gt;"><b>1.3 音频特征提取</b></a></li>
                                                <li><a href="#77" data-title="&lt;b&gt;1.4 多尺度的音频差分归一化算法&lt;/b&gt;"><b>1.4 多尺度的音频差分归一化算法</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#96" data-title="&lt;b&gt;2 基于深度学习的音频抑郁症识别&lt;/b&gt; "><b>2 基于深度学习的音频抑郁症识别</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#97" data-title="&lt;b&gt;2.1 音频抑郁回归预测网络&lt;/b&gt;"><b>2.1 音频抑郁回归预测网络</b></a></li>
                                                <li><a href="#101" data-title="&lt;b&gt;2.2 基于DR AudioNet的改进&lt;/b&gt;"><b>2.2 基于DR AudioNet的改进</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#105" data-title="&lt;b&gt;3 实验结果与分析&lt;/b&gt; "><b>3 实验结果与分析</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#126" data-title="&lt;b&gt;4 结 语&lt;/b&gt; "><b>4 结 语</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#99" data-title="图1 Depression AudioNet网络结构图">图1 Depression AudioNet网络结构图</a></li>
                                                <li><a href="#104" data-title="图2 本文提出的网络模型整体架构图">图2 本文提出的网络模型整体架构图</a></li>
                                                <li><a href="#115" data-title="&lt;b&gt;表1 在2014AVEC测试集上三个模型识别抑郁症的结果比较&lt;/b&gt;"><b>表1 在2014AVEC测试集上三个模型识别抑郁症的结果比较</b></a></li>
                                                <li><a href="#117" data-title="&lt;b&gt;表2 相关性实验在2014AVEC测试集上的结果比较&lt;/b&gt;"><b>表2 相关性实验在2014AVEC测试集上的结果比较</b></a></li>
                                                <li><a href="#121" data-title="图3 三个模型训练的损失函数变化曲线">图3 三个模型训练的损失函数变化曲线</a></li>
                                                <li><a href="#122" data-title="图4 真实标签与预测值的比较图">图4 真实标签与预测值的比较图</a></li>
                                                <li><a href="#123" data-title="&lt;b&gt;表3 联合优化的相关性实验在2014AVEC测试集上的结果比较&lt;/b&gt;"><b>表3 联合优化的相关性实验在2014AVEC测试集上的结果比较</b></a></li>
                                                <li><a href="#125" data-title="&lt;b&gt;表4 在2014AVEC测试集上的只用音频数据的相关实验的结果比较&lt;/b&gt;"><b>表4 在2014AVEC测试集上的只用音频数据的相关实验的结果比较</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" title=" World Health Organization.Depression and Other Common Mental Disorders,Global Health Estimates[M].Technical Report,2017." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Depression and Other Common Mental Disorders,Global Health Estimates">
                                        <b>[1]</b>
                                         World Health Organization.Depression and Other Common Mental Disorders,Global Health Estimates[M].Technical Report,2017.
                                    </a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" title=" BDI-II[M]//Michalos A C.Encyclopedia of Quality of Life and Well-Being Research.Springer,Dordrecht,2014." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=BDI-II">
                                        <b>[2]</b>
                                         BDI-II[M]//Michalos A C.Encyclopedia of Quality of Life and Well-Being Research.Springer,Dordrecht,2014.
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title=" Wan L,Wang Q,Papir A,et al.Generalized End-to-End Loss for Speaker Verification[EB].arXiv:1710.10467,2017." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Generalized End-to-End Loss for Speaker Verification[EB]">
                                        <b>[3]</b>
                                         Wan L,Wang Q,Papir A,et al.Generalized End-to-End Loss for Speaker Verification[EB].arXiv:1710.10467,2017.
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" title=" Dahl G E,Yu D,Deng L,et al.Context-Dependent Pre-Trained Deep Neural Networks for Large-Vocabulary Speech Recognition[J].IEEE Transactions on Audio,Speech and Language Processing,2012,20(1):30-42." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition">
                                        <b>[4]</b>
                                         Dahl G E,Yu D,Deng L,et al.Context-Dependent Pre-Trained Deep Neural Networks for Large-Vocabulary Speech Recognition[J].IEEE Transactions on Audio,Speech and Language Processing,2012,20(1):30-42.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" title=" Wu C,Huang C,Chen H.Text-Independent Speech Emotion Recognition Using Frequency Adaptive Features[J].Multimedia Tools and Applications,2018,77(2):1-11." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Text-Independent Speech Emotion Recognition Using Frequency Adaptive Features">
                                        <b>[5]</b>
                                         Wu C,Huang C,Chen H.Text-Independent Speech Emotion Recognition Using Frequency Adaptive Features[J].Multimedia Tools and Applications,2018,77(2):1-11.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" title=" Qawaqneh Z,Mallouh A A,Barkana B D.Deep Neural Network Framework and Transformed MFCCs for Speaker&#39;s Age and Gender Classification[J].Knowledge-Based Systems,2017,115:5-14." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES88F4F2EF24F776C5E125F9DC71C6FD12&amp;v=MjkzNTV3YU5XNnJmb3padTk1QzNzL3ZCTm02ejE0UG5hVzN4VTBDclRpTWJ1ZENPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkJyTFUwNXR0aHhieTN4YTQ9TmlmT2ZidQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                         Qawaqneh Z,Mallouh A A,Barkana B D.Deep Neural Network Framework and Transformed MFCCs for Speaker&#39;s Age and Gender Classification[J].Knowledge-Based Systems,2017,115:5-14.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" title=" France D J,Shiavi R G,Silverman S,et al.Acoustical Properties of Speech as Indicators of Depression and Suicidal Risk[J].IEEE Transactions on Biomedical Engineering,2000,47(7):829-837." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Acoustical properties of speech as indicators of depression and suicidal risk">
                                        <b>[7]</b>
                                         France D J,Shiavi R G,Silverman S,et al.Acoustical Properties of Speech as Indicators of Depression and Suicidal Risk[J].IEEE Transactions on Biomedical Engineering,2000,47(7):829-837.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" title=" Scherer S,Stratou G,Gratch J,et al.Investigating Voice Quality as a Speaker-Independent Indicator of Depression and PTSD[C]//Proceedings of Interspeech 2013,ISCA,2013:847-851." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Investigating Voice Quality as a Speaker-Independent Indicator of Depression and PTSD">
                                        <b>[8]</b>
                                         Scherer S,Stratou G,Gratch J,et al.Investigating Voice Quality as a Speaker-Independent Indicator of Depression and PTSD[C]//Proceedings of Interspeech 2013,ISCA,2013:847-851.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" title=" Williamson J R,Quatieri T F,Helfer B S,et al.Vocal Biomarkers of Depression Based on Motor Incoordination[C]//Proceedings of the 3rd ACM international workshop on Audio/visual emotion challenge.ACM,2013:41-48." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Vocal biomarkers of depression based on motor incoordination">
                                        <b>[9]</b>
                                         Williamson J R,Quatieri T F,Helfer B S,et al.Vocal Biomarkers of Depression Based on Motor Incoordination[C]//Proceedings of the 3rd ACM international workshop on Audio/visual emotion challenge.ACM,2013:41-48.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_10" title=" Jan A,Meng H,Gaus Y F A,et al.Automatic Depression Scale Prediction using Facial Expression Dynamics and Regression[C]//Proceedings of the 4th International Workshop on Audio/Visual Emotion Challenge.ACM,2014:73-80." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Automatic depression scale prediction using facial expression dynamics and regression">
                                        <b>[10]</b>
                                         Jan A,Meng H,Gaus Y F A,et al.Automatic Depression Scale Prediction using Facial Expression Dynamics and Regression[C]//Proceedings of the 4th International Workshop on Audio/Visual Emotion Challenge.ACM,2014:73-80.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_11" title=" Mitra V,Shriberg E,Mclaren M,et al.The SRI AVEC-2014 Evaluation System[C]//Proceedings of the 4th International Workshop on Audio/Visual Emotion Challenge.ACM,2014:93-101." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The SRI AVEC-2014 Evaluation System">
                                        <b>[11]</b>
                                         Mitra V,Shriberg E,Mclaren M,et al.The SRI AVEC-2014 Evaluation System[C]//Proceedings of the 4th International Workshop on Audio/Visual Emotion Challenge.ACM,2014:93-101.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_12" title=" Bengio Y,Courville A,Vincent P.Representation Learning:A Review and New Perspectives[J].IEEE Transactions on Pattern Analysis &amp;amp; Machine Intelligence,2012,35(8):1798-1828." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Representation learning:A review and new perspectives">
                                        <b>[12]</b>
                                         Bengio Y,Courville A,Vincent P.Representation Learning:A Review and New Perspectives[J].IEEE Transactions on Pattern Analysis &amp;amp; Machine Intelligence,2012,35(8):1798-1828.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_13" title=" Lecun Y,Bengio Y,Hinton G.Deep learning[J].Nature,2015,521(7553):436-444." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep learning">
                                        <b>[13]</b>
                                         Lecun Y,Bengio Y,Hinton G.Deep learning[J].Nature,2015,521(7553):436-444.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_14" title=" Ma X,Yang H,Chen Q,et al.DepAudioNet:An Efficient Deep Model for Audio based Depression Classification[C]//Proceedings of the 6th International Workshop on Audio/Visual Emotion Challenge.ACM,2016:35-42." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=DepAudioNet An Efficient Deep Model for Audio based Depression Classification">
                                        <b>[14]</b>
                                         Ma X,Yang H,Chen Q,et al.DepAudioNet:An Efficient Deep Model for Audio based Depression Classification[C]//Proceedings of the 6th International Workshop on Audio/Visual Emotion Challenge.ACM,2016:35-42.
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_15" title=" Lang H,Cui C.Automated Depression Analysis Using Convolutional Neural Networks from Speech[J].Journal of Biomedical Informatics,2018,83:103-111." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES3A3DEC75124461974B5B35AD8DA5D538&amp;v=MDY0MTR1SFlmT0dRbGZCckxVMDV0dGh4YnkzeGE0PU5pZk9mYkRKSGFXNTNJaEFaZWtMQ0hvNHhoRVhtRG9QUzNxVDJCcEJDTGZnUUxtWENPTnZGU2lXV3I3SklGcG1hQg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[15]</b>
                                         Lang H,Cui C.Automated Depression Analysis Using Convolutional Neural Networks from Speech[J].Journal of Biomedical Informatics,2018,83:103-111.
                                    </a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_16" title=" Chao L,Tao J,Yang M,et al.Multi Task Sequence Learning for Depression Scale Prediction from Video[C]//International Conference on Affective Computing and Intelligent Interaction(ACII).IEEE Computer Society,2015:526-531." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multi Task Sequence Learning for Depression Scale Prediction from Video">
                                        <b>[16]</b>
                                         Chao L,Tao J,Yang M,et al.Multi Task Sequence Learning for Depression Scale Prediction from Video[C]//International Conference on Affective Computing and Intelligent Interaction(ACII).IEEE Computer Society,2015:526-531.
                                    </a>
                                </li>
                                <li id="35">


                                    <a id="bibliography_17" title=" Valstar M,Schuller B,Smith K,et al.AVEC 2014:3D Dimensional Affect and Depression Recognition Challenge[C]//Proceedings of the 4th International Workshop on Audio/Visual Emotion Challenge.ACM,2014:3-10." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Avec 2014:3d dimensional affect and depression recognition challenge">
                                        <b>[17]</b>
                                         Valstar M,Schuller B,Smith K,et al.AVEC 2014:3D Dimensional Affect and Depression Recognition Challenge[C]//Proceedings of the 4th International Workshop on Audio/Visual Emotion Challenge.ACM,2014:3-10.
                                    </a>
                                </li>
                                <li id="37">


                                    <a id="bibliography_18" title=" Beritelli F,Grasso R.A Pattern Recognition System for Environmental Sound Classification Based on MFCCs and Neural Networks[C]//2008 2nd International Conference on Signal Processing and Communication Systems.IEEE,2008." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A pattern recognition system for environmental sound classification based on MFCCs and neural networks">
                                        <b>[18]</b>
                                         Beritelli F,Grasso R.A Pattern Recognition System for Environmental Sound Classification Based on MFCCs and Neural Networks[C]//2008 2nd International Conference on Signal Processing and Communication Systems.IEEE,2008.
                                    </a>
                                </li>
                                <li id="39">


                                    <a id="bibliography_19" title=" Cummins N,Sethu V,Epps J,et al.Probabilistic Acoustic Volume Analysis for Speech Affected by Depression[C]//INTERSPEECH 2014,15th Annual Conference of the International Speech Communication Association.2014:1238-1242." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Probabilistic Acoustic Volume Analysis for Speech Affected by Depression">
                                        <b>[19]</b>
                                         Cummins N,Sethu V,Epps J,et al.Probabilistic Acoustic Volume Analysis for Speech Affected by Depression[C]//INTERSPEECH 2014,15th Annual Conference of the International Speech Communication Association.2014:1238-1242.
                                    </a>
                                </li>
                                <li id="41">


                                    <a id="bibliography_20" title=" Deng L,Abdelhamid O,Yu D.A Deep Convolutional Neural Network Using Heterogeneous Pooling for Trading Acoustic Invariance with Phonetic Confusion[C]//2013 IEEE International Conference on Acoustics,Speech and Signal Processing.IEEE,2013:6669-6673." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A deep convolutional neural network using heterogeneous pooling for trading acoustic invariance with phonetic confusion">
                                        <b>[20]</b>
                                         Deng L,Abdelhamid O,Yu D.A Deep Convolutional Neural Network Using Heterogeneous Pooling for Trading Acoustic Invariance with Phonetic Confusion[C]//2013 IEEE International Conference on Acoustics,Speech and Signal Processing.IEEE,2013:6669-6673.
                                    </a>
                                </li>
                                <li id="43">


                                    <a id="bibliography_21" title=" Jan A,Meng H,Gaus Y,et al.Artificial Intelligent System for Automatic Depression Level Analysis through Visual and Vocal Expressions[J].IEEE Transactions on Cognitive &amp;amp; Developmental Systems,2018,10(3):668-680." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Artificial Intelligent System for Automatic Depression Level Analysis through Visual and Vocal Expressions">
                                        <b>[21]</b>
                                         Jan A,Meng H,Gaus Y,et al.Artificial Intelligent System for Automatic Depression Level Analysis through Visual and Vocal Expressions[J].IEEE Transactions on Cognitive &amp;amp; Developmental Systems,2018,10(3):668-680.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JYRJ" target="_blank">计算机应用与软件</a>
                2019,36(09),161-167 DOI:10.3969/j.issn.1000-386x.2019.09.029            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于深度学习的音频抑郁症识别</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%8E%E9%87%91%E9%B8%A3&amp;code=42745794&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">李金鸣</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E4%BB%98%E5%B0%8F%E9%9B%81&amp;code=33426703&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">付小雁</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E9%A6%96%E9%83%BD%E5%B8%88%E8%8C%83%E5%A4%A7%E5%AD%A6%E4%BF%A1%E6%81%AF%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0044636&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">首都师范大学信息工程学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E7%94%B5%E5%AD%90%E7%B3%BB%E7%BB%9F%E5%8F%AF%E9%9D%A0%E6%80%A7%E6%8A%80%E6%9C%AF%E5%8C%97%E4%BA%AC%E5%B8%82%E9%87%8D%E7%82%B9%E5%AE%9E%E9%AA%8C%E5%AE%A4&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">电子系统可靠性技术北京市重点实验室</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>抑郁症以显著而持久的心境低落为主要临床特征,是心境障碍的主要类型,严重影响人们的日常生活和工作。研究人员发现,抑郁症患者与正常人在言语方面存在明显差别。提出一种基于卷积神经网络和长短时期记忆网络的音频抑郁回归模型(DR AudioNet)。从特征设计和网络架构两方面进行研究,提出多尺度的音频差分归一化(MADN)特征提取算法。MADN特征描述了非个性化讲话的特性,并根据音频段前后相邻两段的MADN特征设计基于DR AudioNet优化的两个网络模型。实验结果表明,该方法能够有效地识别抑郁程度。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%8A%91%E9%83%81%E7%97%87%E8%87%AA%E5%8A%A8%E8%AF%8A%E6%96%AD&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">抑郁症自动诊断;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%AF%AD%E9%9F%B3%E4%BF%A1%E5%8F%B7%E5%A4%84%E7%90%86&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">语音信号处理;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%9F%B3%E9%A2%91%E7%89%B9%E5%BE%81&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">音频特征;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    李金鸣,硕士生,主研领域:模式识别,感知计算。;
                                </span>
                                <span>
                                    付小雁,副教授。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-01-28</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金项目(61876112,61601311,61603022);</span>
                    </p>
            </div>
                    <h1><b>AUDIO DEPRESSION RECOGNITION BASED ON DEEP LEARNING</b></h1>
                    <h2>
                    <span>Li Jinming</span>
                    <span>Fu Xiaoyan</span>
            </h2>
                    <h2>
                    <span>College of Information Engineering, Capital Normal University</span>
                    <span>Beijing Key Laboratory of Electronic System Reliability Technology</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Depression is characterized by significant and persistent low mood, and is the main type of mood disorder, which seriously affects people's daily work and life. The researchers found that there were many significant differences in speech between normal people and people with depression. This paper presented an audio depression regression prediction model(DR AudioNet) based on convolutional neural networks and long short-term memory. Through research on feature extraction and network architecture, a multiscale audio delta normalization(MADN) feature extraction algorithm was proposed. MADN features described the characteristics of non-personalization speech, and two network models based on DR AudioNet optimization were designed according to the MADN feature of the two adjacent segments. The experimental results show that the method can effectively identify the degree of depression.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Automated%20depression%20diagnosis&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Automated depression diagnosis;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Speech%20signal%20processing&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Speech signal processing;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Deep%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Deep learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Audio%20feature&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Audio feature;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2019-01-28</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="45" name="45" class="anchor-tag"><b>0 引 言</b></h3>
                <div class="p1">
                    <p id="46">随着现代生活的快速发展,心理健康问题得到社会各界越来越多的关注。抑郁症(major depressive disorder,MDD)是心境障碍的主要类别,以显著而持久的心境低落或丧失兴趣与愉悦感为主要临床特征<citation id="128" type="reference"><link href="3" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>。轻度患者会伴有情绪低落、心情压抑、焦虑、 兴趣丧失、自我评价过低等症状;重度抑郁症患者会悲观厌世、绝望、幻觉妄想、身体功能减退,甚至产生自杀行为。虽然抑郁症严重影响人们的生活与日常工作,但是抑郁症患者可以通过药物、心理和物理方式治愈或缓解病情。</p>
                </div>
                <div class="p1">
                    <p id="47">贝克抑郁量表II(Beck Depression Inventory-II, BDI-II)是目前应用最为广泛的抑郁症状自评量表<citation id="129" type="reference"><link href="5" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>。BDI-II具有良好的信度与效度,可以作为自评工具用于评估患抑郁症的程度。在BDI-II中有21个单项选择题,受试者必须根据自己的真实情况在每个问题的4个选项中选择最符合的一个,4个答案对应的评分分别是0、1、2、3,BDI-II最终分值范围为0～63。不同的分数段代表不同的抑郁程度:0～13表示没有抑郁症;14～19表示患有轻度抑郁症;20～28表示患有中度抑郁症;29～63表示患有重度抑郁症。BDI-II值越大,表示个体的抑郁程度越严重,对患者和他人的伤害也就越大。</p>
                </div>
                <div class="p1">
                    <p id="48">目前,抑郁症的诊断以问卷调查为主,以医师判断为辅。其准确度严重依赖于患者的配合程度以及医师的专业水平和经验,并且抑郁症患者的早期诊断和再评估会受到很多限制。近年来,随着抑郁症患者数量的不断增加,快速并准确地诊断抑郁症是相关医疗人员面临的重大问题。因此,通过计算机技术提供一种客观的评估和快速识别抑郁程度的方法将有助于抑郁症患者的早期诊断和治疗。</p>
                </div>
                <div class="p1">
                    <p id="49">随着生物特征识别技术的发展,研究人员通过分析语音信号,可以得到讲话人身份信息、年龄、性别、讲话内容和情感等多种信息<citation id="130" type="reference"><link href="7" rel="bibliography" /><link href="9" rel="bibliography" /><link href="11" rel="bibliography" /><link href="13" rel="bibliography" /><sup>[<a class="sup">3</a>,<a class="sup">4</a>,<a class="sup">5</a>,<a class="sup">6</a>]</sup></citation>。同时,国外的研究人员也发现并证实人的音频特征与抑郁程度之间具有显著的关联性。文献<citation id="131" type="reference">[<a class="sup">7</a>,<a class="sup">8</a>]</citation>表明,抑郁症患者人群与正常人群在语音特征行为方面存在语速偏慢、语调单一且悲观等明显的差异。又由于语音具有容易采集,所受限制少和成本低的特点,基于音频的抑郁症识别已成为自动抑郁症检测的主要手段之一。自动语音抑郁检测(Automatic Speech Depression Detection, ASDD)是利用计算机分析说话人的语音信号及其变化过程,发现说话人内心的情绪和心理活动。目前ASDD的方法可以分为两类:传统的机器学习方法和深度学习方法。</p>
                </div>
                <div class="p1">
                    <p id="50">特征选择是传统ASDD机器学习方法的关键,特征的选择直接关系到抑郁症识别结果的准确性。目前,最常用的特征包括梅尔频率倒谱系数(MFCCs)、能量、过零率、共振峰特征、光谱特征等。提取特征后再采用机器学习方法研究特征与抑郁程度的关系,这些机器学习方法包括高斯混合模型(GMM)<citation id="132" type="reference"><link href="19" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>、偏最小二乘(PLS)<citation id="133" type="reference"><link href="21" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>和支持向量回归(SVR)<citation id="134" type="reference"><link href="23" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>。这种方法的主要优点是可以在不需要大量数据的情况下对模型进行训练。缺点是很难判断特征的质量,可能会丢失一些关键特征,从而降低识别的准确性。</p>
                </div>
                <div class="p1">
                    <p id="51">与传统的机器学习方法相比,深度学习技术具有可以提取高层语义特征的优点,在最近几年取得了突破性的进展<citation id="138" type="reference"><link href="25" rel="bibliography" /><link href="27" rel="bibliography" /><sup>[<a class="sup">12</a>,<a class="sup">13</a>]</sup></citation>。目前,有研究者将深度学习方法应用于ASDD。Huang等<citation id="135" type="reference"><link href="29" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>在2016年AVEC中介绍了一种基于音频的抑郁症分类的深度学习方法,提取音频的MFCCs特征作为模型的输入,其中设计了两层卷积神经网络(CNN)、一层长短时期记忆网络(LSTM)和两层全连接层(FC)用于预测音频受试者是否抑郁。在文献<citation id="136" type="reference">[<a class="sup">15</a>]</citation>中,作者基于局部二值模式(LBP)特征做了改进,设计了中值鲁棒扩展的局部二值模式(MRELBP),然后运用深度卷积神经网络(DCNN)进行预测抑郁分值。Chao等<citation id="137" type="reference"><link href="33" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>提取了音频和视频的特征,并将其融合为异常行为的标志,然后利用长短时记忆递归神经网络(LSTM-RNN)来描述动态时间信息。他们采用多任务学习来提高结果的准确性,并在AVEC2014数据集上评估了模型的预测能力。</p>
                </div>
                <div class="p1">
                    <p id="52">虽然之前的研究取得了一定的成果,但是仍存在一些挑战。研究表明,抑郁症患者与正常人在情绪上存在显著差异,比如抑郁症患者常伴有抑郁、悲伤、焦虑、担忧的情绪。为了解决训练数据较少的问题和充分利用情感特征,本文设计了基于上下文情感信息的多尺度音频差分归一化(MADN)特征和新的网络模型框架。在不减少样本数量的情况下,增加样本长度可以获得更多的训练数据。首先,将当前样本的特征输入模型对抑郁相关特征进行编码。然后,将当前样本的相邻两段的MADN特征再按次序分别输入上面训练好的模型进行微调和优化。通过与目前最优的方法对比表明,本文提出的音频抑郁症识别算法在抑郁症的诊断中提高了预测精确度。</p>
                </div>
                <h3 id="53" name="53" class="anchor-tag"><b>1 音频特征提取和设计</b></h3>
                <h4 class="anchor-tag" id="54" name="54"><b>1.1 数据库</b></h4>
                <div class="p1">
                    <p id="55">AVEC2014<citation id="139" type="reference"><link href="35" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>抑郁症数据库是视听抑郁语言语料库(AVid-Corpus)的一个子集。音频是在自然条件下通过麦克风采集的,被试者的年龄在18岁到63岁之间,平均年龄为31.5岁。每个受试者被记录一次到四次不等,同一个受试者的每两次采集时间间隔大约为两周。AVEC2014数据库中的每个音频的时长在6秒到4分8秒之间。AVEC2014数据库分为NORTHWIND和FREEFORM两部分:</p>
                </div>
                <div class="p1">
                    <p id="56">(1) NORTHWIND: 提供文章给每个录制者朗读,例如“Die Sonne und der Wind”,然后通过麦克风分别记录他们朗读的音频信息。</p>
                </div>
                <div class="p1">
                    <p id="57">(2) FREEFORM: 这一部分是被试者与工作人员通过耳机麦克风交流,由工作人员询问一个或多个问题,然后被试者进行回答,例如“What is your favorite dish?”,“What was your best gift, and why?”,“Discuss a sad childhood memory”等,此时麦克风采集的只是被试者一端的音频信息。</p>
                </div>
                <div class="p1">
                    <p id="58">AVEC2014数据库中的150对NORTHWIND和FREEFORM音频分成了训练集、验证集和测试集,各包含50个音频,并且根据录制者的BDI-II值为每个音频设置标签。</p>
                </div>
                <h4 class="anchor-tag" id="59" name="59"><b>1.2 数据预处理</b></h4>
                <div class="p1">
                    <p id="60">为了获得最优的特征,对音频样本进行预处理。由于被试者在听问题时是没有语音信息的,所以要把这一部分非语音段裁剪掉。首先,对于每一个音频文件,去除长时间的静音段(非语音),其余部分被拼接成一个新的音频文件。然后,每个有效的音频文件被分割成相同长度的且没有重叠的音频片段,每个音频段由60帧组成,音频分帧时选用汉明窗,每帧1 024个数据点,前一帧与后一帧的交叠部分是帧长的1/2。音频采样率为44 100 Hz,所以一个音频片段覆盖的时间为[(60+1)×1 024/2]/44 100=0.708 s。对于2014AVEC数据库中的数据,经过预处理后一共得到7 548个音频片段样本,其中训练集包含5 100段,测试集包含2 448段。</p>
                </div>
                <h4 class="anchor-tag" id="61" name="61"><b>1.3 音频特征提取</b></h4>
                <h4 class="anchor-tag" id="62" name="62">(1) 梅尔频率倒谱系数(MFCCs)。</h4>
                <div class="p1">
                    <p id="63">MFCCs是语音信号处理中最常用的特征,它具有符合人类听觉和低维度的优点<citation id="140" type="reference"><link href="37" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>。Mel频率倒谱系数将人耳的听觉感知特性和语音信号产生机制有效结合。下式解释了如何将音频的普通频域尺度转换为Mel频率刻度:</p>
                </div>
                <div class="p1">
                    <p id="64"><mathml id="65"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>f</mi><msub><mrow></mrow><mrow><mtext>m</mtext><mtext>e</mtext><mtext>l</mtext></mrow></msub><mo>=</mo><mn>2</mn><mspace width="0.25em" /><mn>5</mn><mn>9</mn><mn>5</mn><mrow><mi>log</mi></mrow><mrow><mo>(</mo><mrow><mn>1</mn><mo>+</mo><mfrac><mrow><mi>f</mi><msub><mrow></mrow><mrow><mtext>Η</mtext><mtext>z</mtext></mrow></msub></mrow><mrow><mn>7</mn><mn>0</mn><mn>0</mn></mrow></mfrac></mrow><mo>)</mo></mrow></mrow></math></mathml>      (1)</p>
                </div>
                <div class="p1">
                    <p id="66">式中: <i>f</i><sub>mel</sub>表示Mel频率刻度,<i>f</i><sub>Hz</sub>代表普通频率。通常,MFCCs的计算使用一组滤波器,这组滤波器的中心频率是按照Mel频率刻度均匀间隔排列的,每个滤波器的三角形的两个底点的频率分别等于相邻的两个滤波器的中心频率。设滤波器的个数为<i>M</i>,滤波后得到的输出为<i>X</i>(<i>m</i>), <i>m</i>=1,2,…,<i>M</i>;设<i>l</i>(<i>m</i>)、<i>c</i>(<i>m</i>)、<i>h</i>(<i>m</i>)分别为第<i>m</i>个三角形滤波器的下限频率、中心频率和上限频率,则相邻三角形滤波器的下限、中心和上限频率有如下关系:</p>
                </div>
                <div class="p1">
                    <p id="67"><i>c</i>(<i>m</i>)=<i>h</i>(<i>m</i>-1)=<i>l</i>(<i>m</i>+1)      (2)</p>
                </div>
                <div class="p1">
                    <p id="68">将滤波器组d的输出进行对数运算,然后再进行反离散余弦变换即得到MFCCs。</p>
                </div>
                <div class="p1">
                    <p id="69"><i>C</i><sub><i>n</i></sub>=<mathml id="70"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>m</mi><mo>=</mo><mn>1</mn></mrow><mi>Μ</mi></munderover><mrow></mrow></mstyle></mrow></math></mathml><mathml id="71"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>log</mi></mrow><mi>X</mi><mo stretchy="false">(</mo><mi>m</mi><mo stretchy="false">)</mo><mrow><mi>cos</mi></mrow><mrow><mo>(</mo><mrow><mo stretchy="false">(</mo><mi>m</mi><mo>-</mo><mn>0</mn><mo>.</mo><mn>5</mn><mo stretchy="false">)</mo><mfrac><mrow><mi>n</mi><mtext>π</mtext></mrow><mi>Μ</mi></mfrac></mrow><mo>)</mo></mrow><mtext> </mtext><mi>n</mi><mo>=</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mo>⋯</mo><mo>,</mo><mi>L</mi><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>3</mn><mo stretchy="false">)</mo></mrow></math></mathml></p>
                </div>
                <div class="p1">
                    <p id="72">式中: <i>L</i>为MFCCs系数的个数,通常取值为12～16,滤波器的个数取值在20～40之间。本文取<i>L</i>=12、<i>M</i>=40。</p>
                </div>
                <h4 class="anchor-tag" id="73" name="73">(2) 共振峰、能量、过零率。</h4>
                <div class="p1">
                    <p id="74">短时能量是指一帧语音信号的平均能量,反映了语音信号的幅度变化。短时能量可以用来区分有声与无声,由于抑郁症患者会有吐字不清和停顿变长的症状,而且随着抑郁程度的增加这些症状会更明显突出,所以短时能量特征用来进一步分析讲话过程中的停顿信息。</p>
                </div>
                <div class="p1">
                    <p id="75">短时过零率可以从背景声中找出语音信号,也可以反映有话和无话的信息,因此采用短时能量和短时过零率相结合来提取讲话的持续时间和停顿时间信息。</p>
                </div>
                <div class="p1">
                    <p id="76">为了能更加准确地识别抑郁症,我们以发声器官为出发点寻找相关的研究来帮助选取合适的特征,进而提高算法的预测精度。文献<citation id="141" type="reference">[<a class="sup">19</a>]</citation>表明,抑郁症患者的声道会比正常人的更紧张,声带动力也会随着抑郁程度而变化,而且人的神经所处的紧张程度不同,导致声道在发相同音的时候产生形变,从而出现差异。而共振峰是指在声音的频谱中能量相对集中的一些区域,反映了声道(共振腔)的物理特征,并且,共振峰代表了发音信息的最直接的来源。因此选取共振峰特征来进一步的分析抑郁症患者与正常人的音频信息的不同。</p>
                </div>
                <h4 class="anchor-tag" id="77" name="77"><b>1.4 多尺度的音频差分归一化算法</b></h4>
                <div class="p1">
                    <p id="78">众所周知,临床医生得到的患者音频信息越多,那么对于此患者的抑郁诊断结果就越准确。然而,抑郁症数据库的音频数据有限,由于音频段数与单个样本的音频时长是反比例关系,所以单个样本时长增加会导致总样本段数的减少,单个样本的数据维度增加也会大大增加计算的复杂度,影响运算速度和识别的准确率。这也是当前研究基于音频的抑郁症识别急需解决的问题。</p>
                </div>
                <div class="p1">
                    <p id="79">在现实生活中,不同的说话者有不同的音量和音色特点,有的人天生说话嗓门比较高,有的人天生声音浑厚,说话者的这种个性化说话特点会导致抑郁症识别的准确率下降。对每一帧音频提取的MFCCs、短时能量、短时过零率和共振峰频率特征除了包含大量与抑郁相关的特征外,还掺杂了说话者个性说话特点,这是由于其静态特性导致的,而说话者的个性化说话特点会减弱抑郁症识别模型的泛化能力。因此,我们提出了多尺度的音频差分归一化(Multiscale Audio Delta Normalization, MADN)的算法,用于获取局部非个性化的抑郁特征。基于音频差分的特征反映同一个说话者说话过程中的音频变化信息,不易受到个性化说话特点的影响。由于不同特征的数据量级是不同的,因此采用不同的尺度对特征进行归一化处理。为了获得说话者音频的局部变化信息,根据滑动窗口选取相应的语音帧进行归一化,而不是通过与一整段的音频进行比较。按照滑动窗口选取相应的语音帧可以增强局部音频变化的动态性,更有效地体现了非个性化音频特征。为便于理解,MADN算法流程如下:</p>
                </div>
                <div class="p1">
                    <p id="80">1) 输入原始音频文件。</p>
                </div>
                <div class="p1">
                    <p id="81">2) 读取音频文件并对所有的音频文件进行预处理。</p>
                </div>
                <div class="p1">
                    <p id="82">3) 提取MFCCs、短时能量、过零率和共振峰频率特征且用<i>V</i>(<i>n</i>,<i>f</i>)表示,<i>f</i>是语音的帧数,每一帧包含<i>n</i>个元素。</p>
                </div>
                <div class="p1">
                    <p id="83">4) 通过相邻两帧音频特征<i>V</i>(<i>n</i>,<i>f</i>)进行差分计算得到<i>D</i>(<i>n</i>,<i>f</i>)。<i>D</i>(<i>n</i>,<i>f</i>)代表了音频的时序变化,减弱了说话者的讲话个性化信息,在同一抑郁程度下特征值的分布相对比较稳定。计算方法如下:</p>
                </div>
                <div class="p1">
                    <p id="84"><i>D</i>(<i>n</i>,<i>f</i>)=<i>V</i>(<i>N</i>,<i>f</i>+1)-<i>V</i>(<i>n</i>,<i>f</i>)      (4)</p>
                </div>
                <div class="p1">
                    <p id="85"><i>f</i>=1,2,…,<i>F</i>-1</p>
                </div>
                <div class="p1">
                    <p id="86">5) 对不同特征进行不同尺度的归一化:</p>
                </div>
                <div class="p1">
                    <p id="87"><mathml id="88"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>F</mi><mo stretchy="false">(</mo><mi>n</mi><mo>,</mo><mi>f</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><mi>D</mi><mo stretchy="false">(</mo><mi>n</mi><mo>,</mo><mi>f</mi><mo stretchy="false">)</mo><mo>-</mo><mi>D</mi><msub><mrow></mrow><mrow><mi>min</mi></mrow></msub><mo stretchy="false">(</mo><mi>n</mi><mo>,</mo><mi>f</mi><msub><mrow></mrow><mi>n</mi></msub><mo>∶</mo><mi>F</mi><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false">)</mo></mrow><mrow><mi>D</mi><msub><mrow></mrow><mrow><mi>max</mi></mrow></msub><mo stretchy="false">(</mo><mi>n</mi><mo>,</mo><mi>f</mi><msub><mrow></mrow><mi>n</mi></msub><mo>∶</mo><mi>F</mi><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false">)</mo><mo>-</mo><mi>D</mi><msub><mrow></mrow><mrow><mi>min</mi></mrow></msub><mo stretchy="false">(</mo><mi>n</mi><mo>,</mo><mi>f</mi><msub><mrow></mrow><mi>n</mi></msub><mo>∶</mo><mi>F</mi><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false">)</mo></mrow></mfrac></mrow></math></mathml>      (5)</p>
                </div>
                <div class="p1">
                    <p id="89"><i>n</i>=1,2,…,<i>N</i></p>
                </div>
                <div class="p1">
                    <p id="90">式中:<i>F</i><sub><i>n</i></sub>与<i>f</i><sub><i>n</i></sub>的取值代表不同的尺度和滑动窗口,其公式如下:</p>
                </div>
                <div class="area_img" id="91">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JYRJ201909030_09100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="95">6) 输出: <i>F</i>(<i>n</i>,<i>f</i>)即为不同尺度归一化后的特征。</p>
                </div>
                <h3 id="96" name="96" class="anchor-tag"><b>2 基于深度学习的音频抑郁症识别</b></h3>
                <h4 class="anchor-tag" id="97" name="97"><b>2.1 音频抑郁回归预测网络</b></h4>
                <div class="p1">
                    <p id="98">深度学习技术在近几年得到快速发展,在语音信号处理领域取得了良好的成果,它可以学习生成高级语义信息,丰富手工设计特征。Huang等<citation id="142" type="reference"><link href="29" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>在2016年AVEC竞赛中设计了一个用于识别是否抑郁的二分类的网络结构,主要由卷积神经网络(CNN)和长短期记忆神经网络(LSTM)组成,此模型的输入是音频信息,输出是对应的个体是否抑郁音频。本文基于这个网络模型做了两个方面的优化和改进:(1) 基于只用MFCCs特征作为网络的输入的缺点,提出了MFCCs、短时能量、短时过零率以及共振峰特征的互补结合作为模型的输入;(2) 基于抑郁分类的模型改进为抑郁回归预测的模型,由于不同程度的抑郁症患者需要不同的治疗,所以预测抑郁症患者的BDI-II值是很有必要的。本文优化的深度模型,即音频抑郁回归预测网络(Depression Regression AudioNet, DRAudioNet)的网络结构如图1所示。在传统的深度卷积神经网络(DCNN)模式识别中,输入图像和卷积核的形状往往都是方形的,而语音信号的数据维数是一维的,不能直接使用图像处理的方法进行处理。为了解决这个问题,在实验中对音频段中每一帧语音都提取了MFCCs、过零率、能量和共振峰频率特征,然后,每个片段的60帧语音的特征组成一个二维的矩阵。</p>
                </div>
                <div class="area_img" id="99">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201909030_099.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 Depression AudioNet网络结构图" src="Detail/GetImg?filename=images/JYRJ201909030_099.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 Depression AudioNet网络结构图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201909030_099.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="100">在语音特征的二维矩阵表示中,水平轴表示时间,垂直轴表示频率信息。相同的频谱模式在不同的频率区间可以表示完全不同的音频,而CNN用于图像处理的方形的卷积核和池化操作会造成不同的音频之间的混淆,削弱识别能力<citation id="143" type="reference"><link href="41" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>。因此,实验中尝试在整个频率轴上使用一维卷积代替方形滤波器来解决这个问题。卷积层可以有效地捕获丰富的高阶语义信息;池化层的目的是降低特征图的维度,对于相对位置的小变化引入不变性,以此提高精度和减少运算复杂度。二维的输入特征经过卷积和池化操作得到一维深层特征,然后,将这些特征导入LSTM层以提取长期依赖信息。最后,在网络架构的末端是两个全连接层,用于在时间轴上编码音频的长期变化并预测抑郁得分。</p>
                </div>
                <h4 class="anchor-tag" id="101" name="101"><b>2.2 基于DR AudioNet的改进</b></h4>
                <div class="p1">
                    <p id="102">DR AudioNet网络只运用了当前音频片段的特征,为了能够运用MADN算法提取当前段的前后相邻两段音频的非个性化抑郁特征,本文研究了目前常用的网络模型融合的方法和特点。传统的神经网络融合通常是将几个网络的预测值进行线性加权或者采用随机梯度的方式进行加权融合,参与训练的样本数量并没有增加,只是增加了特征的种类,是一种并行的融合方式。</p>
                </div>
                <div class="p1">
                    <p id="103">根据MADN算法得到非个性化抑郁特征是在DR AudioNet网络中输入特征的前后相邻两段音频上得到的,它们具有相同的尺寸和BDI-II值,以及时间上的关联性。因此,本文提出一种新的网络模型。整体架构图如图2所示。可以看到,后一个模型在前一个模型的基础上训练,也就是说,前面模型的参数共享给后面的模型。首先,使用DR AudioNet网络对训练数据集提取MFCCs、短时能量、短时过零率和共振峰频率特征(统称:特征V1)进行训练。DR AudioNet网络可以单独预测抑郁分值,但是在说话者个性化信息上处理欠佳,因而在此模型的基础上,运用当前音频段的前一段的MADN特征(称为特征V2)进行训练模型二,学习到了与抑郁相关的特征又减弱了说话者个性化语音的干扰。最后在模型二的基础上,提取当前音频段的后一段的MADN特征(称为特征V3)进行训练模型三。通过结合两种不同特征的优点,提取出更加准确的抑郁特征信息。</p>
                </div>
                <div class="area_img" id="104">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201909030_104.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 本文提出的网络模型整体架构图" src="Detail/GetImg?filename=images/JYRJ201909030_104.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 本文提出的网络模型整体架构图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201909030_104.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h3 id="105" name="105" class="anchor-tag"><b>3 实验结果与分析</b></h3>
                <div class="p1">
                    <p id="106">为了评价提出的算法对抑郁症的识别效果,本文以平均绝对误差(Mean Absolute Error,MAE)和均方根误差(Root Mean Square Error,RMSE)作为评价指标。MAE和RMSE的值越小说明所提出的算法对抑郁症的预测越准确。MAE能够很好地反映预测值和真实值之间的误差。假设有<i>N</i>个样本,每个样本的真实标签值<i>y</i><sub><i>i</i></sub>(<i>i</i>=1,2,…,<i>N</i>),预测值为<mathml id="107"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>y</mi><mo>^</mo></mover><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">(</mo><mi>i</mi><mo>=</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mo>⋯</mo><mo>,</mo><mi>Ν</mi><mo stretchy="false">)</mo></mrow></math></mathml>,那么平均绝对误差表示所有的预测值和真实值之间绝对误差的平均值,计算公式如下:</p>
                </div>
                <div class="p1">
                    <p id="108"><i>MAE</i>=<mathml id="109"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mn>1</mn><mi>Ν</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mo stretchy="false">|</mo></mstyle><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mover accent="true"><mi>y</mi><mo>^</mo></mover><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">|</mo></mrow></math></mathml>      (8)</p>
                </div>
                <div class="p1">
                    <p id="110"><i>RMSE</i>表示所有预测值与真实值之间误差的均方根,被用来衡量预测值与真实值之间的偏差,计算公式如下:</p>
                </div>
                <div class="p1">
                    <p id="111"><i>RMSE</i>=<mathml id="112"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msqrt><mrow><mfrac><mn>1</mn><mi>Ν</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mo stretchy="false">(</mo></mstyle><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mover accent="true"><mi>y</mi><mo>^</mo></mover><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><msup><mrow></mrow><mn>2</mn></msup></mrow></msqrt></mrow></math></mathml>      (9)</p>
                </div>
                <div class="p1">
                    <p id="113">为了证明本文提出的提取特征算法以及深度模型在抑郁症回归预测的有效性,在2014AVEC数据集上分别进行了三个模型的实验。</p>
                </div>
                <div class="p1">
                    <p id="114">DR AudioNet网络的输入特征尺寸为17×60,设定batchsize为32,两层卷积层都有64个卷积核,卷积核的大小都为3×1,LSTM层的cell个数设置为128个,第一层全连接层的节点数也是128,最后一层全连接层只有一个节点输出预测分数。将2014AVEC中的验证集归并到训练集,在测试集上评估DR AudioNet网络模型的整体性能。结果如表1所示,可以看出,得到的RMSE和MAE分别为9.70和7.52。</p>
                </div>
                <div class="area_img" id="115">
                    <p class="img_tit"><b>表1 在2014AVEC测试集上三个模型识别抑郁症的结果比较</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="115" border="1"><tr><td><br />识别方法</td><td>RMSE</td><td>MAE</td></tr><tr><td><br />模型M1</td><td>9.70</td><td>7.52</td></tr><tr><td><br />模型M2</td><td>9.46</td><td>7.30</td></tr><tr><td><br />模型M3</td><td>9.15</td><td>7.17</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="116">为了验证本文提出的特征V1和DR AudioNet网络的有效性,我们与目前最优的基于音频的抑郁症识别的研究结果进行了对比,结果如表2所示。其中,文献<citation id="144" type="reference">[<a class="sup">21</a>]</citation>分别提取了MFCCs特征和AVEC2014委员会提供的低水平描述子(low level descriptors,LLD),然后运用PLS、LR回归方法进行抑郁症的预测;文献<citation id="145" type="reference">[<a class="sup">15</a>]</citation>设计了深度学习模型并运用MRELBP和LLD特征进行训练。AVEC2014委员会提供的LLD特征包括MFCCs、短时能量和短时过零率在内的38种音频描述子。分析文献<citation id="146" type="reference">[<a class="sup">21</a>]</citation>的研究可以发现,运用MFCCs特征得到的效果比运用LLD的实验结果要好,这也就是说明多种特征的结合反而会降低识别精度。而本文有效融合了与抑郁症状最为相关的MFCCs、共振峰、短时能量和短时过零率特征,比只运用MFCCs更有优势。</p>
                </div>
                <div class="area_img" id="117">
                    <p class="img_tit"><b>表2 相关性实验在2014AVEC测试集上的结果比较</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="117" border="1"><tr><td>研究</td><td>特征</td><td>方法</td><td>RMSE</td><td>MAE</td></tr><tr><td rowspan="4"><br />文献[21]</td><td rowspan="2"><br />MFCCs</td><td><br />PLS</td><td>10.42</td><td>8.04</td></tr><tr><td><br />LR</td><td>10.28</td><td>8.07</td></tr><tr><td rowspan="2"><br />LLD</td><td><br />PLS</td><td>10.46</td><td>8.42</td></tr><tr><td><br />LR</td><td>10.73</td><td>8.45</td></tr><tr><td rowspan="2"><br />文献[15]</td><td><br />MRELBP</td><td>DCNN</td><td>10.56</td><td>8.65</td></tr><tr><td><br />LLD</td><td>DCNN</td><td>10.64</td><td>8.89</td></tr><tr><td>本文</td><td>特征V1</td><td>DR AudioNet</td><td>9.70</td><td>7.52</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="118">文献<citation id="147" type="reference">[<a class="sup">15</a>]</citation>中的深度卷积神经网络(DCNN)模型由卷积层、池化层和全连接层组成。由于音频是随时间变化的语音信号,为了提取音频之间的时序信息,本文设计的DR AudioNet模型由CNN、MP、LSTM和FC组成,其中CNN对音频的短期深度特征进行编码,LSTM提取音频之间的长期依赖信息。通过结果对比发现,本文模型M1的实验效果优于目前最新的文献<citation id="148" type="reference">[<a class="sup">21</a>]</citation>和文献<citation id="149" type="reference">[<a class="sup">15</a>]</citation>的方法效果。</p>
                </div>
                <div class="p1">
                    <p id="119">在模型M2中,利用MADN算法对模型M1中当前音频段的前一段音频进行处理,得到特征V2,V2表达了前一段音频的抑郁变化特征。模型M2运用特征V2对模型V1进行微调。从表1可以看出,模型M2得到的RMSE和MAE分别为9.46和7.30。通过MADN特征以及模型M2在模型M1的基础上微调减少了个性化特征对抑郁识别的影响,同时能够提取更加丰富的语义信息和更加准确的特征信息,使得模型M2的RMSE与MAE分别比模型M1降低了0.24和0.18,进一步证明了模型优化的有效性。</p>
                </div>
                <div class="p1">
                    <p id="120">在模型M3中,选取特征V3对模型M2进行联合优化。同样,此时特征V3的样本是模型M1中的特征V1样本的后一段音频。利用MADN算法对模型M1中当前音频段的后一段音频进行处理,得到特征V3。从表1可以看到,经过特征V3联合优化之后模型的RMSE和MAE分别为9.15和7.17,相比于模型M2又进一步降低了误差。图3是三个模型的损失函数变化曲线,可以看到模型M3的损失函数收敛速度更快。这三个模型的实验结果表明,本文提出的在DR AudioNet网络的基础上运用相邻两段音频的MADN特征对网络模型进行联合优化进一步降低了音频抑郁识别误差,有效地融合了MADN特征对说话人非个性化的抑郁特征,更加有利于抑郁识别模型的回归预测。图4显示了模型M3在2014AVEC测试集中的预测值与真值标签(BDI-II)的比较。文献<citation id="150" type="reference">[<a class="sup">15</a>]</citation>运用不同的特征分别训练深度模型,然后设计了四个全连接层(FC)对前面提到的分支模型进行融合,最终进行抑郁分数的预测。本文提出的网络联合优化模型与文献<citation id="151" type="reference">[<a class="sup">15</a>]</citation>提出的融合多个深度模型的实验结果对比如表3所示,可以看出,本文提出的联合优化模型框架有更好的性能。</p>
                </div>
                <div class="area_img" id="121">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201909030_121.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 三个模型训练的损失函数变化曲线" src="Detail/GetImg?filename=images/JYRJ201909030_121.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 三个模型训练的损失函数变化曲线  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201909030_121.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="122">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201909030_122.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 真实标签与预测值的比较图" src="Detail/GetImg?filename=images/JYRJ201909030_122.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 真实标签与预测值的比较图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201909030_122.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="123">
                    <p class="img_tit"><b>表3 联合优化的相关性实验在2014AVEC测试集上的结果比较</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="123" border="1"><tr><td>研究</td><td>融合方法</td><td>RMSE</td><td>MAE</td></tr><tr><td rowspan="2"><br />文献[15]</td><td><br />平均值</td><td>10.12</td><td>8.22</td></tr><tr><td><br />联合调优(FC×4)</td><td>9.99</td><td>8.19</td></tr><tr><td>本文</td><td>联合优化(M1,M2,M3)</td><td>9.15</td><td>7.17</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="124">与包括2014AVEC提供的Baseline在内的其他仅仅使用音频数据的最新技术的识别效果比较见表4。通过对比实验可以得出结论,本文提出的运用特征V1与DR AudioNet网络能有效地进行抑郁症程度的回归预测。在提取特征时运用不同的尺度对特征进行的归一化,有效地融合了不同的特征并保留了更加重要的音频抑郁特征。同时,运用V2和V3对DR AudioNet进行联合优化,有效地融合了MADN特征对说话人非个性化的抑郁特征。相比于其他目前最优的只用音频数据的算法有效地降低了抑郁识别误差,在2014AVEC数据集上的RMSE和MAE分别降到了9.15和7.17,证明了本文提出的基于音频的特征算法和网络模型在识别效果上优于其他方法。</p>
                </div>
                <div class="area_img" id="125">
                    <p class="img_tit"><b>表4 在2014AVEC测试集上的只用音频数据的相关实验的结果比较</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="125" border="1"><tr><td><br />研究</td><td>RMSE</td><td>MAE</td></tr><tr><td><br />文献[17]</td><td>12.57</td><td>10.04</td></tr><tr><td><br />文献[10]</td><td>11.30</td><td>9.10</td></tr><tr><td><br />文献[11]</td><td>11.10</td><td>8.83</td></tr><tr><td><br />文献[21]</td><td>10.28</td><td>8.07</td></tr><tr><td><br />文献[15]</td><td>9.99</td><td>8.19</td></tr><tr><td><br />本文</td><td>9.15</td><td>7.17</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h3 id="126" name="126" class="anchor-tag"><b>4 结 语</b></h3>
                <div class="p1">
                    <p id="127">本文提出了获取局部音频的相邻两段的非个性化的抑郁特征(MADN),这种基于音频时序变化的特征反映了讲话者的音频变化信息,并减少了讲话人的讲话个性化特点,显示了与BDI-II值的较强的关联性。本文设计了一种新的网络模型,通过当前音频段的前后相邻两段的MADN特征,对前一个模型进行优化,提高了模型和特征的表达能力,进一步提高了模型预测BDI-II值的准确度。在今后的工作中,我们将探索自然语言处理中的文本处理,分析个体回答问题的文本信息,然后运用语音特征和文本特征的多模态融合,以进一步提高识别抑郁症的准确率。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Depression and Other Common Mental Disorders,Global Health Estimates">

                                <b>[1]</b> World Health Organization.Depression and Other Common Mental Disorders,Global Health Estimates[M].Technical Report,2017.
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=BDI-II">

                                <b>[2]</b> BDI-II[M]//Michalos A C.Encyclopedia of Quality of Life and Well-Being Research.Springer,Dordrecht,2014.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Generalized End-to-End Loss for Speaker Verification[EB]">

                                <b>[3]</b> Wan L,Wang Q,Papir A,et al.Generalized End-to-End Loss for Speaker Verification[EB].arXiv:1710.10467,2017.
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition">

                                <b>[4]</b> Dahl G E,Yu D,Deng L,et al.Context-Dependent Pre-Trained Deep Neural Networks for Large-Vocabulary Speech Recognition[J].IEEE Transactions on Audio,Speech and Language Processing,2012,20(1):30-42.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Text-Independent Speech Emotion Recognition Using Frequency Adaptive Features">

                                <b>[5]</b> Wu C,Huang C,Chen H.Text-Independent Speech Emotion Recognition Using Frequency Adaptive Features[J].Multimedia Tools and Applications,2018,77(2):1-11.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES88F4F2EF24F776C5E125F9DC71C6FD12&amp;v=MjUxMzZDM3MvdkJObTZ6MTRQbmFXM3hVMENyVGlNYnVkQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQnJMVTA1dHRoeGJ5M3hhND1OaWZPZmJ1d2FOVzZyZm96WnU5NQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b> Qawaqneh Z,Mallouh A A,Barkana B D.Deep Neural Network Framework and Transformed MFCCs for Speaker's Age and Gender Classification[J].Knowledge-Based Systems,2017,115:5-14.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Acoustical properties of speech as indicators of depression and suicidal risk">

                                <b>[7]</b> France D J,Shiavi R G,Silverman S,et al.Acoustical Properties of Speech as Indicators of Depression and Suicidal Risk[J].IEEE Transactions on Biomedical Engineering,2000,47(7):829-837.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Investigating Voice Quality as a Speaker-Independent Indicator of Depression and PTSD">

                                <b>[8]</b> Scherer S,Stratou G,Gratch J,et al.Investigating Voice Quality as a Speaker-Independent Indicator of Depression and PTSD[C]//Proceedings of Interspeech 2013,ISCA,2013:847-851.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Vocal biomarkers of depression based on motor incoordination">

                                <b>[9]</b> Williamson J R,Quatieri T F,Helfer B S,et al.Vocal Biomarkers of Depression Based on Motor Incoordination[C]//Proceedings of the 3rd ACM international workshop on Audio/visual emotion challenge.ACM,2013:41-48.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Automatic depression scale prediction using facial expression dynamics and regression">

                                <b>[10]</b> Jan A,Meng H,Gaus Y F A,et al.Automatic Depression Scale Prediction using Facial Expression Dynamics and Regression[C]//Proceedings of the 4th International Workshop on Audio/Visual Emotion Challenge.ACM,2014:73-80.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The SRI AVEC-2014 Evaluation System">

                                <b>[11]</b> Mitra V,Shriberg E,Mclaren M,et al.The SRI AVEC-2014 Evaluation System[C]//Proceedings of the 4th International Workshop on Audio/Visual Emotion Challenge.ACM,2014:93-101.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Representation learning:A review and new perspectives">

                                <b>[12]</b> Bengio Y,Courville A,Vincent P.Representation Learning:A Review and New Perspectives[J].IEEE Transactions on Pattern Analysis &amp; Machine Intelligence,2012,35(8):1798-1828.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep learning">

                                <b>[13]</b> Lecun Y,Bengio Y,Hinton G.Deep learning[J].Nature,2015,521(7553):436-444.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=DepAudioNet An Efficient Deep Model for Audio based Depression Classification">

                                <b>[14]</b> Ma X,Yang H,Chen Q,et al.DepAudioNet:An Efficient Deep Model for Audio based Depression Classification[C]//Proceedings of the 6th International Workshop on Audio/Visual Emotion Challenge.ACM,2016:35-42.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_15" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES3A3DEC75124461974B5B35AD8DA5D538&amp;v=MTM4NDhDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZCckxVMDV0dGh4YnkzeGE0PU5pZk9mYkRKSGFXNTNJaEFaZWtMQ0hvNHhoRVhtRG9QUzNxVDJCcEJDTGZnUUxtWA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[15]</b> Lang H,Cui C.Automated Depression Analysis Using Convolutional Neural Networks from Speech[J].Journal of Biomedical Informatics,2018,83:103-111.
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multi Task Sequence Learning for Depression Scale Prediction from Video">

                                <b>[16]</b> Chao L,Tao J,Yang M,et al.Multi Task Sequence Learning for Depression Scale Prediction from Video[C]//International Conference on Affective Computing and Intelligent Interaction(ACII).IEEE Computer Society,2015:526-531.
                            </a>
                        </p>
                        <p id="35">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Avec 2014:3d dimensional affect and depression recognition challenge">

                                <b>[17]</b> Valstar M,Schuller B,Smith K,et al.AVEC 2014:3D Dimensional Affect and Depression Recognition Challenge[C]//Proceedings of the 4th International Workshop on Audio/Visual Emotion Challenge.ACM,2014:3-10.
                            </a>
                        </p>
                        <p id="37">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A pattern recognition system for environmental sound classification based on MFCCs and neural networks">

                                <b>[18]</b> Beritelli F,Grasso R.A Pattern Recognition System for Environmental Sound Classification Based on MFCCs and Neural Networks[C]//2008 2nd International Conference on Signal Processing and Communication Systems.IEEE,2008.
                            </a>
                        </p>
                        <p id="39">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Probabilistic Acoustic Volume Analysis for Speech Affected by Depression">

                                <b>[19]</b> Cummins N,Sethu V,Epps J,et al.Probabilistic Acoustic Volume Analysis for Speech Affected by Depression[C]//INTERSPEECH 2014,15th Annual Conference of the International Speech Communication Association.2014:1238-1242.
                            </a>
                        </p>
                        <p id="41">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A deep convolutional neural network using heterogeneous pooling for trading acoustic invariance with phonetic confusion">

                                <b>[20]</b> Deng L,Abdelhamid O,Yu D.A Deep Convolutional Neural Network Using Heterogeneous Pooling for Trading Acoustic Invariance with Phonetic Confusion[C]//2013 IEEE International Conference on Acoustics,Speech and Signal Processing.IEEE,2013:6669-6673.
                            </a>
                        </p>
                        <p id="43">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Artificial Intelligent System for Automatic Depression Level Analysis through Visual and Vocal Expressions">

                                <b>[21]</b> Jan A,Meng H,Gaus Y,et al.Artificial Intelligent System for Automatic Depression Level Analysis through Visual and Vocal Expressions[J].IEEE Transactions on Cognitive &amp; Developmental Systems,2018,10(3):668-680.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JYRJ201909030" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201909030&amp;v=MDkyMDM0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVadEZ5bmhXN3JPTHpUWlpMRzRIOWpNcG85R1pJUUtESDg=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
