<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637134086634787500%26DBCODE%3dCJFD%26TABLEName%3dCJFDTEMP%26FileName%3dJYRJ201910006%26RESULT%3d1%26SIGN%3ds6NLwfNai1tRoxMNFcHfu1d9zNc%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JYRJ201910006&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JYRJ201910006&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201910006&amp;v=MDk2MzdCdEdGckNVUkxPZVplVnVGeTdrVnJ6T0x6VFpaTEc0SDlqTnI0OUZZb1FLREg4NHZSNFQ2ajU0TzN6cXE=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#55" data-title="&lt;b&gt;0 引 言&lt;/b&gt; "><b>0 引 言</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#57" data-title="&lt;b&gt;1 相关工作&lt;/b&gt; "><b>1 相关工作</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#59" data-title="&lt;b&gt;2 基本方法&lt;/b&gt; "><b>2 基本方法</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#60" data-title="&lt;b&gt;2.1 信息增益率&lt;/b&gt;"><b>2.1 信息增益率</b></a></li>
                                                <li><a href="#71" data-title="&lt;b&gt;2.2 主成分分析&lt;/b&gt;"><b>2.2 主成分分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#95" data-title="&lt;b&gt;3 混合特征选择模型&lt;/b&gt; "><b>3 混合特征选择模型</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#97" data-title="&lt;b&gt;3.1 数据预处理&lt;/b&gt;"><b>3.1 数据预处理</b></a></li>
                                                <li><a href="#105" data-title="&lt;b&gt;3.2 特征初选子集&lt;/b&gt;"><b>3.2 特征初选子集</b></a></li>
                                                <li><a href="#114" data-title="&lt;b&gt;3.3 特征再选子集&lt;/b&gt;"><b>3.3 特征再选子集</b></a></li>
                                                <li><a href="#149" data-title="&lt;b&gt;3.4 混合选择模型&lt;/b&gt;"><b>3.4 混合选择模型</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#154" data-title="&lt;b&gt;4 实验分析&lt;/b&gt; "><b>4 实验分析</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#155" data-title="&lt;b&gt;4.1 语料集&lt;/b&gt;"><b>4.1 语料集</b></a></li>
                                                <li><a href="#159" data-title="&lt;b&gt;4.2 实验结果&lt;/b&gt;"><b>4.2 实验结果</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#174" data-title="&lt;b&gt;5 结 语&lt;/b&gt; "><b>5 结 语</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#151" data-title="图1 混合特征选择模型">图1 混合特征选择模型</a></li>
                                                <li><a href="#158" data-title="&lt;b&gt;表1 DatasetCats1和DatasetCats1&lt;/b&gt;"><b>表1 DatasetCats1和DatasetCats1</b></a></li>
                                                <li><a href="#164" data-title="&lt;b&gt;表2 DatasetCats1直接输入分类器的分类性能(相似主题&lt;/b&gt;)"><b>表2 DatasetCats1直接输入分类器的分类性能(相似主题</b>)</a></li>
                                                <li><a href="#165" data-title="&lt;b&gt;表3 DatasetCats2直接输入分类器的分类性能(不同主题&lt;/b&gt;)"><b>表3 DatasetCats2直接输入分类器的分类性能(不同主题</b>)</a></li>
                                                <li><a href="#168" data-title="&lt;b&gt;表4 DatasetCats1混合特征选择后的分类性能(相似主题&lt;/b&gt;)"><b>表4 DatasetCats1混合特征选择后的分类性能(相似主题</b>)</a></li>
                                                <li><a href="#170" data-title="&lt;b&gt;表5 DatasetCats2混合特征选择后的分类性能(不同主题&lt;/b&gt;)"><b>表5 DatasetCats2混合特征选择后的分类性能(不同主题</b>)</a></li>
                                                <li><a href="#173" data-title="图2 平均准确率">图2 平均准确率</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" title=" Miniwatts Marketing Group.Internet world status[OL].2017.www.internetworldstats.com/stats.htm." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Internet world status[OL]">
                                        <b>[1]</b>
                                         Miniwatts Marketing Group.Internet world status[OL].2017.www.internetworldstats.com/stats.htm.
                                    </a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" title=" Li J,Ren F J.A hybird approach for word emotion recognition[J].IEEJ Transaction on Electricaland Electronic Engineering,2013,8(6):616-626." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A hybird approach for word emotion recognition">
                                        <b>[2]</b>
                                         Li J,Ren F J.A hybird approach for word emotion recognition[J].IEEJ Transaction on Electricaland Electronic Engineering,2013,8(6):616-626.
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title=" Luo K H,Deng Z H,Yu H L,et al.Automatic identification and recognition of sentiment words using an optimization-based model with propagation[J].International Journal of Intelligent System,2015,30(5):537-549." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Automatic identification and recognition of sentiment words using an optimization-based model with propagation">
                                        <b>[3]</b>
                                         Luo K H,Deng Z H,Yu H L,et al.Automatic identification and recognition of sentiment words using an optimization-based model with propagation[J].International Journal of Intelligent System,2015,30(5):537-549.
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" title=" Wawre S V,Deshmukh S N.Sentiment classification using machine learning[J].International Journal of Science and Research,2016,5(4):819-821." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Sentiment classification using machine learning">
                                        <b>[4]</b>
                                         Wawre S V,Deshmukh S N.Sentiment classification using machine learning[J].International Journal of Science and Research,2016,5(4):819-821.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" title=" Aggarwal C C,Zhai C.A survey of text classification algorithms,mining text data[M].Heidelberg:Springer,2012." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A survey of text classification algorithms">
                                        <b>[5]</b>
                                         Aggarwal C C,Zhai C.A survey of text classification algorithms,mining text data[M].Heidelberg:Springer,2012.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" title=" Marin A,Holenstein R,Sarikaya R,et al.Learning phrase patterns for text classification using a knowledge graph and unlabeled data[C]//Proceedings of the Annual Conference of the International Speech Communication Association,INTERSPEECH 2014.2014:253-257." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning phrase patterns for text classification using a knowledge graph and unlabeled data">
                                        <b>[6]</b>
                                         Marin A,Holenstein R,Sarikaya R,et al.Learning phrase patterns for text classification using a knowledge graph and unlabeled data[C]//Proceedings of the Annual Conference of the International Speech Communication Association,INTERSPEECH 2014.2014:253-257.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" title=" Grimmer J,Stewart B M.Text as data:the promise and pitfalls of automatic content analysis methods for political texts[J].Political Analysis,2013,21(3):267-297." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Text as Data;The Promise and Pitfalls of Automatic Content Analysis Methods for Political Texts,&amp;quot;">
                                        <b>[7]</b>
                                         Grimmer J,Stewart B M.Text as data:the promise and pitfalls of automatic content analysis methods for political texts[J].Political Analysis,2013,21(3):267-297.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" title=" Wang D,Zhang H,Liu R,et al.t-Test feature selection approach based on term frequency for text categorization[J].Pattern Recognition Letters,2014,45(11):1-10." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14032300044876&amp;v=MjA2MjlCTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVWIvSUpsOFJhUnM9TmlmT2ZiSzhIdExPckk5RlpPOExCSHMvbw==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                         Wang D,Zhang H,Liu R,et al.t-Test feature selection approach based on term frequency for text categorization[J].Pattern Recognition Letters,2014,45(11):1-10.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" title=" Zhang W,Yoshida T,Tang X.A comparative study of TF*IDF,LSI and multi-words for text classification[J].Expert Systems with Applications,2011,38(3):2758-2765." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501640078&amp;v=MjE0Mzg4UERIc3hvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVViL0lKbDhSYVJzPU5pZk9mYks3SHRETnFvOUVZdQ==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                         Zhang W,Yoshida T,Tang X.A comparative study of TF*IDF,LSI and multi-words for text classification[J].Expert Systems with Applications,2011,38(3):2758-2765.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_10" title=" Uysal A K,Gunal S.A novel probabilistic feature selection method for text classification[J].Knowledge-Based Systems,2012,36:226-235." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501718943&amp;v=MzE4Nzd5am1VYi9JSmw4UmFScz1OaWZPZmJLN0h0RE5xbzlFWStvSEJYZzZvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SA==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[10]</b>
                                         Uysal A K,Gunal S.A novel probabilistic feature selection method for text classification[J].Knowledge-Based Systems,2012,36:226-235.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_11" title=" Chen K W,Zhang Z P,Long J.Turning from TF-IDF to TF-IGM for term weighting in text classification[J].Expert Systems with Application,2016,18:245-260." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Turning from TF-IDF to TFIGM for term weighting in text classification">
                                        <b>[11]</b>
                                         Chen K W,Zhang Z P,Long J.Turning from TF-IDF to TF-IGM for term weighting in text classification[J].Expert Systems with Application,2016,18:245-260.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_12" title=" Su J,Shirab J S,Matwin S.Large scale text classification using semi-supervised Multinomial Naive Bayes[C]//Proceedings of the 28th International Conference on Machine Learning,ICML 2011.2011:97-104." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Large scale text classification using semi-supervised multinomial Naive Bayes">
                                        <b>[12]</b>
                                         Su J,Shirab J S,Matwin S.Large scale text classification using semi-supervised Multinomial Naive Bayes[C]//Proceedings of the 28th International Conference on Machine Learning,ICML 2011.2011:97-104.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_13" title=" Wang D,Zhang H,Liu R,et al.Unsupervised feature selection through Gram-Schmidt orthogonalization:A word co-occurrence perspective[J].Neurocomputing,2016,173(3):845-854." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESEDB4ABE996030A126419B0AD2354D01A&amp;v=Mjg4ODhOaWZPZmNiTWJOVzkzZnBNYmUwUEQzeEl6aFFWN2o1ME9uK1QyQkEyZkxiZ1JidnVDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZDcGJRMzVkaGh3cm02dzZFPQ==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[13]</b>
                                         Wang D,Zhang H,Liu R,et al.Unsupervised feature selection through Gram-Schmidt orthogonalization:A word co-occurrence perspective[J].Neurocomputing,2016,173(3):845-854.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_14" title=" Rehman A,Javed K,Babri H A.Feature selection based on a normalized difference measure for text classification[J].Information Processing &amp;amp; Management,2017,53(2):473-489." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESFCD8800628D9CFDC3067D4915A35831A&amp;v=MzA2MTdsZkNwYlEzNWRoaHdybTZ3NkU9TmlmT2ZjWExhdG5FcjQ5RFp1TjdCUTlQdTJVUTZqbDZQSHZyclJkRWVyZWNScnZ1Q09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUQ==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[14]</b>
                                         Rehman A,Javed K,Babri H A.Feature selection based on a normalized difference measure for text classification[J].Information Processing &amp;amp; Management,2017,53(2):473-489.
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_15" title=" Zong W,Wu F,Chu L K,et al.A discriminative and semantic feature selection method for text categorization[J].International Journal of Production Economics,2015,165:215-222." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES9FE44B2BA2F43AD894EED9FE4A873B18&amp;v=MjY5MTFYQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQ3BiUTM1ZGhod3JtNnc2RT1OaWZPZmJyT2E5WEkzWTAzRmVsNUNIOUl1eDRhN2tvSVBIYVUyUlpFY2JXWE43dQ==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[15]</b>
                                         Zong W,Wu F,Chu L K,et al.A discriminative and semantic feature selection method for text categorization[J].International Journal of Production Economics,2015,165:215-222.
                                    </a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_16" title=" Uguz H.A two-stage feature selection method for text categorization by using information gain,principal component analysis and genetic algorithm[J].Knowledge-Based Systems,2011,24(7):1024-1032." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501719222&amp;v=MTIzODJLN0h0RE5xbzlFWStvR0RuNDdvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVViL0lKbDhSYVJzPU5pZk9mYg==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[16]</b>
                                         Uguz H.A two-stage feature selection method for text categorization by using information gain,principal component analysis and genetic algorithm[J].Knowledge-Based Systems,2011,24(7):1024-1032.
                                    </a>
                                </li>
                                <li id="35">


                                    <a id="bibliography_17" title=" Xu Y,Jones G J,Li J,et al.A study on mutual information-based feature selection for text categorization[J].Journal of Computational Information Systems,2007,3(3):1007-1012." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Study on mutual information-based feature selection for text categorization">
                                        <b>[17]</b>
                                         Xu Y,Jones G J,Li J,et al.A study on mutual information-based feature selection for text categorization[J].Journal of Computational Information Systems,2007,3(3):1007-1012.
                                    </a>
                                </li>
                                <li id="37">


                                    <a id="bibliography_18" title=" Yang Y,Pedersen J O.A comparative study on feature selection in text categorization[C]//Proceedings of the fourteenth international conference on machine learning,1997:412-420." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A comparative study on feature selection in text categorization">
                                        <b>[18]</b>
                                         Yang Y,Pedersen J O.A comparative study on feature selection in text categorization[C]//Proceedings of the fourteenth international conference on machine learning,1997:412-420.
                                    </a>
                                </li>
                                <li id="39">


                                    <a id="bibliography_19" title=" Lehmann N.Principal components selection given extensively many variables[J].Statistics &amp;amp; Probability Letters,2005,74(1):51-58." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012300451192&amp;v=MTYyNzlUNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVViL0lKbDhSYVJzPU5pZk9mYks3SHRET3JJOUZZTzRPRFhVN29CTQ==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[19]</b>
                                         Lehmann N.Principal components selection given extensively many variables[J].Statistics &amp;amp; Probability Letters,2005,74(1):51-58.
                                    </a>
                                </li>
                                <li id="41">


                                    <a id="bibliography_20" title=" Li Y,Yang Q,Lai S,et al.A new speculative execution algorithm based on C4.5 decision tree for hadoop[C]// Intelligent Computation in Big Data Era(ICYCSEE),2015:284-291." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A New Speculative Execution Algorithm Based on C4.5 Decision Tree for Hadoop">
                                        <b>[20]</b>
                                         Li Y,Yang Q,Lai S,et al.A new speculative execution algorithm based on C4.5 decision tree for hadoop[C]// Intelligent Computation in Big Data Era(ICYCSEE),2015:284-291.
                                    </a>
                                </li>
                                <li id="43">


                                    <a id="bibliography_21" title=" Mohammed A A,Minhas R,Wu Q M J,et al.Human face recognition based on multidimensional PCA and extreme learning machine[J].Pattern Recognition,2011,44(10-11):2588-2597." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011600738114&amp;v=MzExNjNIRFgwOW9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVWIvSUpsOFJhUnM9TmlmT2ZiSzdIdEROcVk5RlkrZw==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[21]</b>
                                         Mohammed A A,Minhas R,Wu Q M J,et al.Human face recognition based on multidimensional PCA and extreme learning machine[J].Pattern Recognition,2011,44(10-11):2588-2597.
                                    </a>
                                </li>
                                <li id="45">


                                    <a id="bibliography_22" title=" Kilic E,Ates N,Karakaya A,et al.Two new feature extraction methods for text classification:TESDF and SADF[C]//Signal Processing &amp;amp; Communications Applications Conference.IEEE,2015:475-478." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Two new feature extraction methods for text classification:TESDF and SADF">
                                        <b>[22]</b>
                                         Kilic E,Ates N,Karakaya A,et al.Two new feature extraction methods for text classification:TESDF and SADF[C]//Signal Processing &amp;amp; Communications Applications Conference.IEEE,2015:475-478.
                                    </a>
                                </li>
                                <li id="47">


                                    <a id="bibliography_23" title=" Jrennie.20 Newsgroups[OL].2008.http://people.csail.mit.edu/jrennie/20Newsgroups." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=20Newsgroups">
                                        <b>[23]</b>
                                         Jrennie.20 Newsgroups[OL].2008.http://people.csail.mit.edu/jrennie/20Newsgroups.
                                    </a>
                                </li>
                                <li id="49">


                                    <a id="bibliography_24" title=" Li B Y,Wang Q W,Hu J L.Multi-SVM classifier system with piecewise interpolation[J].IEEJ Transaction on Electricaland Electronic Engineering,2013,8(2):132-138." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJWD&amp;filename=SJWD130305000833&amp;v=MDQwNTN4TTh6eFVTbURkOVNIN24zeEU5ZmJ2bktyaWZadTl1RkNybFU3bkpKVjBkTmlmY2FySzdIdExNcW85RlpPTU1E&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[24]</b>
                                         Li B Y,Wang Q W,Hu J L.Multi-SVM classifier system with piecewise interpolation[J].IEEJ Transaction on Electricaland Electronic Engineering,2013,8(2):132-138.
                                    </a>
                                </li>
                                <li id="51">


                                    <a id="bibliography_25" title=" 蔡永泉,王玉栋.以特征值关联项改进贝叶斯分类器正确率[J].计算机应用与软件,2017,34(8):286-290,311." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201708051&amp;v=MTk0OTZWcnpCTHpUWlpMRzRIOWJNcDQ5QVpZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdUZ5N2s=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[25]</b>
                                         蔡永泉,王玉栋.以特征值关联项改进贝叶斯分类器正确率[J].计算机应用与软件,2017,34(8):286-290,311.
                                    </a>
                                </li>
                                <li id="53">


                                    <a id="bibliography_26" title=" 吴国文,庄千料.一种改进的增量式贝叶斯文本分类算法[J].计算机应用与软件,2017,34(6):226-229,249." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201706041&amp;v=MzAzOTdCdEdGckNVUkxPZVplVnVGeTdrVnJ6Qkx6VFpaTEc0SDliTXFZOUJaWVFLREg4NHZSNFQ2ajU0TzN6cXE=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[26]</b>
                                         吴国文,庄千料.一种改进的增量式贝叶斯文本分类算法[J].计算机应用与软件,2017,34(6):226-229,249.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JYRJ" target="_blank">计算机应用与软件</a>
                2019,36(10),23-29+80 DOI:10.3969/j.issn.1000-386x.2019.10.005            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>一种基于PCA的文本特征混合选择方法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BC%A0%E6%89%AC%E6%AD%A6&amp;code=41511875&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">张扬武</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%8E%E5%9B%BD%E5%92%8C&amp;code=10080676&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">李国和</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E7%AB%8B%E6%A2%85&amp;code=10468749&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王立梅</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%AE%97%E6%81%92&amp;code=22252536&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">宗恒</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%B5%B5%E6%99%B6%E6%98%8E&amp;code=10469131&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">赵晶明</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E7%9F%B3%E6%B2%B9%E5%A4%A7%E5%AD%A6(%E5%8C%97%E4%BA%AC)%E5%9C%B0%E7%90%83%E7%89%A9%E7%90%86%E4%B8%8E%E4%BF%A1%E6%81%AF%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0093802&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国石油大学(北京)地球物理与信息工程学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E6%94%BF%E6%B3%95%E5%A4%A7%E5%AD%A6%E6%B3%95%E6%B2%BB%E4%BF%A1%E6%81%AF%E7%AE%A1%E7%90%86%E5%AD%A6%E9%99%A2&amp;code=0012403&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国政法大学法治信息管理学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E7%9F%B3%E6%B2%B9%E5%A4%A7%E5%AD%A6(%E5%8C%97%E4%BA%AC)%E7%9F%B3%E6%B2%B9%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E5%8C%97%E4%BA%AC%E5%B8%82%E9%87%8D%E7%82%B9%E5%AE%9E%E9%AA%8C%E5%AE%A4&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国石油大学(北京)石油数据挖掘北京市重点实验室</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>由于文本分类中的特征空间高维稀疏,传统单一的降维方法难以满足实际大数据分类需求。针对这种情况,提出一种两阶段的混合特征选择方法。第一阶段计算每个特征词的信息增益率并进行排序,然后根据设定的阈值进行特征词的选择。第二阶段利用主成分分析方法将第一阶段输出的仍保持高维特性的高维特征空间映射到低维新特征空间。实验结果表明,与单一的传统方法比较,混合特征选择方法实现了二次降维,不但减少了计算开销,还提高了分类性能。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%99%8D%E7%BB%B4&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">降维;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">文本分类;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">主成分分析;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%BF%A1%E6%81%AF%E5%A2%9E%E7%9B%8A%E7%8E%87&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">信息增益率;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">特征选择;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    张扬武，副教授，主研领域:数据挖掘，机器学习。;
                                </span>
                                <span>
                                    李国和，教授。;
                                </span>
                                <span>
                                    王立梅，教授。;
                                </span>
                                <span>
                                    宗恒，副教授。;
                                </span>
                                <span>
                                    赵晶明，副教授。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-07-31</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金项目(60473125);</span>
                                <span>中国石油大学(北京)克拉玛依校区科研启动基金项目(RCYJ2016B-03-001);</span>
                    </p>
            </div>
                    <h1><b>A METHOD OF HYBRID SELECTION FOR TEXT FEATURE BASED ON PCA</b></h1>
                    <h2>
                    <span>Zhang Yangwu</span>
                    <span>Li Guohe</span>
                    <span>Wang Limei</span>
                    <span>Zong Heng</span>
                    <span>Zhao Jingming</span>
            </h2>
                    <h2>
                    <span>College of Geophysics and Information Engineering, China University of Petroleum</span>
                    <span>School of Information Management for Law, China University of Political Science and Law</span>
                    <span>Beijing Key Lab of Data Mining for Petroleum Data, China University of Petroleum</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>The feature space in text classification is high-dimensional and sparse, so the method of traditional single dimensionality reduction cannot meet the actual needs of big data classification. In view of this situation, this paper proposed a two-stage hybrid feature selection method. In the first stage, the information gain rate of each feature term was calculated and sorted, and then these feature terms were selected according to the set threshold. In the second stage, we used PCA to map the high-dimensional feature space of the first stage output to the low-dimensional new feature space. The experimental results show that compared with the single traditional method, the hybrid feature selection method achieves the second dimensionality reduction, which not only reduces the computational cost, but also improves the classification performance.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Dimensionality%20reduction&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Dimensionality reduction;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Text%20classification&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Text classification;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=PCA&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">PCA;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Information%20gain%20rate&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Information gain rate;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Feature%20selection&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Feature selection;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-07-31</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="55" name="55" class="anchor-tag"><b>0 引 言</b></h3>
                <div class="p1">
                    <p id="56">随着互联网的快速发展,网络数据量快速增长,数据共享越来越丰富。截至到2017年3月,全世界网民数量达到37亿<citation id="202" type="reference"><link href="3" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>。过去的几年中,网络为人们创造了各种便利条件。进入大数据时代,信息传播从单一类型逐渐过渡到复合类型,不仅是难以控制,而且也很难确定和捕捉到。一些机构开始为公司提供信息跟踪服务,关注特别领域的话题跟踪的主要工作就是对评价文本进行分类,分类方法有基于规则的和基于统计的。基于规则的方法是按照已有的语法规则来学习一些情感词,在已知情感词的极性基础上加入句法分析,提取情感词所描述的属性<citation id="203" type="reference"><link href="5" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>。基于规则的分类方法在理解能力、先验知识和迁移能力这些方面不具备优势,理解能力并不是依赖语法就能完成的,同一个词在不同领域中的含义也是不一样的,迁移能力明显达不到人类的水平<citation id="204" type="reference"><link href="7" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>。近年来,逐渐采用机器学习方法来进行文本分类,这是一类基于统计的方法。机器学习通过从数据中学习模型和经验,让用户获得一个更接近事实和客观的洞察力和解释结果<citation id="205" type="reference"><link href="9" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>。用于学习的文档集称为语料集,通常分成训练集和测试集两部分。训练集包括那些已经标记好类别的文档,而测试集是为了验证模型性能,包括那些未标记的文档。通过机器学习模型,将标记好的文本输入到模型进行训练,获得稳定的分类器。然后在训练好的机器学习模型上,为未标记的文本准确地确定一个类别。文本自动分类技术是在给定的分类体系下,对未知类别的文本根据其特征自动判定其类别归属的过程。因此,在自然语言处理、信息检索、邮件分类、话题跟踪和数字图书馆等方面有着广泛的应用前景<citation id="206" type="reference"><link href="11" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>。随着大数据时代的到来,特征降维在文本分类领域中具有非常重要的意义,也是主要挑战之一。</p>
                </div>
                <h3 id="57" name="57" class="anchor-tag"><b>1 相关工作</b></h3>
                <div class="p1">
                    <p id="58">文本分类包括三个过程:特征选择、特征抽取和文本分类<citation id="207" type="reference"><link href="13" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>。特征选择后的特征是原来特征的一个子集,而特征抽取后的新特征是原来特征的一个映射。文本数据经常包含一些非常频繁出现的词语,以及一些很少出现的术语<citation id="208" type="reference"><link href="15" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>。最为广泛使用的文本模型是词袋模型(BoW),完全忽略了文档中词语的顺序,只考虑单词是否出现以及出现的次数。向量空间模型(Vector Space Model, VSM) 把文本表示成高维特征空间中的一个行向量,向量中的每一维度表示在词典中的对应词的权重,即特征词的词频(tf)<citation id="209" type="reference"><link href="11" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>。高维的文本用权重的形式来表示,文本向量空间采用这种方法将文本集变成词典中相应的词的权重矩阵<citation id="210" type="reference"><link href="17" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>。最初,特征词的权重用词频来表示,后来综合使用词频和逆向文档频率(Inverse Document Frequency, IDF)来表示特征词权值,即TF-IDF<citation id="216" type="reference"><link href="19" rel="bibliography" /><link href="21" rel="bibliography" /><sup>[<a class="sup">9</a>,<a class="sup">10</a>]</sup></citation>。向量空间模型将文本内容处理转换为向量空间中的向量计算,用向量空间的相似度来表示文本的语义相似度,简单直观,易于理解<citation id="211" type="reference"><link href="23" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>。在文本预处理中,可以发现文本向量空间具有特征维度高和矩阵稀疏特点。这不仅带来分类的时间开销过大,还会导致维数灾难问题。因此,对特征进行降维显得十分重要<citation id="217" type="reference"><link href="25" rel="bibliography" /><link href="27" rel="bibliography" /><sup>[<a class="sup">12</a>,<a class="sup">13</a>]</sup></citation>。提供给分类器的输入特征应该是与类别相关的,以减少执行时间并提高准确性<citation id="218" type="reference"><link href="29" rel="bibliography" /><link href="31" rel="bibliography" /><sup>[<a class="sup">14</a>,<a class="sup">15</a>]</sup></citation>。目前存在特征选择和特征抽取两种方法对特征进行降维<citation id="212" type="reference"><link href="33" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>。特征选择的目标是为分类器提供没有不相关和冗余特征的数据,许多特征选择算法通过使用特征排名度量作为主要或辅助机制来选择特征。信息增益(IG)是广泛使用的度量,用于确定机器学习领域中的分类任务的特征熵<citation id="213" type="reference"><link href="35" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>,而信息增益率是在同样特征均匀度下的信息增益,避免特征取值过于分散而带来的无效信息增益,这是一种特征选择方法。它通过包含或删除特征词来估计文档类别而决定特征所含的分类信息<citation id="214" type="reference"><link href="37" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>。主成分分析(PCA)是一种建立在统计技术基础上的降维方法,旨在最小化原始数据中的方差损失。作为一种特征抽取方法,PCA通过最大化离差将高维向量空间中的数据投影到低维向量空间中去<citation id="215" type="reference"><link href="39" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>,可以被视为用于特征抽取的有效技术,适用于各种各样的数据。</p>
                </div>
                <h3 id="59" name="59" class="anchor-tag"><b>2 基本方法</b></h3>
                <h4 class="anchor-tag" id="60" name="60"><b>2.1 信息增益率</b></h4>
                <div class="p1">
                    <p id="61">信息增益率采用熵度量原理<citation id="219" type="reference"><link href="41" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>。用<i>E</i>(<i>L</i>)表示样本集<i>L</i>被分为2个类别的不确定性,类别有正类和负类。<i>E</i>(<i>L</i>)值越大,表示将某个样本划分为正类或负类的不确定性越高:</p>
                </div>
                <div class="p1">
                    <p id="62"><i>E</i>(<i>L</i>)=<mathml id="176"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>c</mi><mo>=</mo><mn>1</mn></mrow><mrow><mo stretchy="false">|</mo><mi>C</mi><mo stretchy="false">|</mo></mrow></munderover><mrow></mrow></mstyle></mrow></math></mathml>-<i>p</i><sub><i>c</i></sub>log<sub>2</sub><i>p</i><sub><i>c</i></sub>=<mathml id="177"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>c</mi><mo>=</mo><mn>1</mn></mrow><mrow><mo stretchy="false">|</mo><mi>C</mi><mo stretchy="false">|</mo></mrow></munderover><mrow></mrow></mstyle></mrow></math></mathml><mathml id="178"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo>-</mo><mfrac><mrow><mo stretchy="false">|</mo><mi>L</mi><msub><mrow></mrow><mi>C</mi></msub><mo stretchy="false">|</mo></mrow><mrow><mo stretchy="false">|</mo><mi>L</mi><mo stretchy="false">|</mo></mrow></mfrac><mrow><mi>log</mi></mrow><msub><mrow></mrow><mn>2</mn></msub><mfrac><mrow><mo stretchy="false">|</mo><mi>L</mi><msub><mrow></mrow><mi>C</mi></msub><mo stretchy="false">|</mo></mrow><mrow><mo stretchy="false">|</mo><mi>L</mi><mo stretchy="false">|</mo></mrow></mfrac></mrow></math></mathml>      (1)</p>
                </div>
                <div class="p1">
                    <p id="63">式中:<i>p</i><sub><i>c</i></sub>取值有<i>p</i><sub>1</sub>和<i>p</i><sub>2</sub>,以及其他。<i>p</i><sub>1</sub>为样本集<i>L</i>中属于类别1的概率,<i>p</i><sub>2</sub>为样本集<i>L</i>中属于类别2的概率。|<i>L</i>|为样本集<i>L</i>的样本个数,也就是<i>m</i>。<i>L</i><sub>1</sub>为类别1上的样本子集,<i>L</i><sub>2</sub>为类别2上的样本子集,即|<i>L</i><sub>1</sub>|+|<i>L</i><sub>2</sub>|+…+|<i>L</i><sub><i>C</i></sub>|=<i>m</i>。样本集按照属性进行划分,<i>E</i>(<i>L</i>,<i>v</i><sub><i>i</i></sub>)表示按属性<i>v</i><sub><i>i</i></sub>划分样本集<i>L</i>导致的期望熵:</p>
                </div>
                <div class="p1">
                    <p id="64"><i>E</i>(<i>L</i>,<i>v</i><sub><i>i</i></sub>)=<mathml id="179"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>v</mi><mi>r</mi><mo>∈</mo><mi>V</mi><mi>a</mi><mi>l</mi><mi>u</mi><mi>e</mi><mi>s</mi><mo stretchy="false">(</mo><mi>v</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></munder><mrow></mrow></mstyle></mrow></math></mathml><mathml id="180"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mo stretchy="false">|</mo><mi>L</mi><msub><mrow></mrow><mrow><mi>v</mi><mi>r</mi></mrow></msub><mo stretchy="false">|</mo></mrow><mrow><mo stretchy="false">|</mo><mi>L</mi><mo stretchy="false">|</mo></mrow></mfrac><mi>E</mi><mo stretchy="false">(</mo><mi>L</mi><msub><mrow></mrow><mrow><mi>v</mi><mi>r</mi></mrow></msub><mo stretchy="false">)</mo></mrow></math></mathml>      (2)</p>
                </div>
                <div class="p1">
                    <p id="65">式中:<i>Values</i>(<i>v</i><sub><i>i</i></sub>)为属性<i>v</i><sub><i>i</i></sub>上的所有取值的集合,<i>L</i><sub><i>vr</i></sub>为<i>L</i>中按照属性<i>v</i><sub><i>i</i></sub>取值为<i>r</i>的样本构成的子集,|<i>L</i><sub><i>vr</i></sub>|为样本子集<i>L</i><sub><i>vr</i></sub>的样本个数。<i>SplitInfo</i>(<i>L</i>,<i>v</i><sub><i>i</i></sub>)表示按属性<i>v</i><sub><i>i</i></sub>划分样本集的广度和均匀度:</p>
                </div>
                <div class="p1">
                    <p id="66"><i>SplitInfo</i>(<i>L</i>,<i>v</i><sub><i>i</i></sub>)=<mathml id="181"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>v</mi><mi>r</mi><mo>∈</mo><mtext>V</mtext><mtext>a</mtext><mtext>l</mtext><mtext>u</mtext><mtext>e</mtext><mtext>s</mtext><mo stretchy="false">(</mo><mi>v</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></munder><mrow></mrow></mstyle></mrow></math></mathml><mathml id="182"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo>-</mo><mfrac><mrow><mo stretchy="false">|</mo><mi>L</mi><msub><mrow></mrow><mrow><mi>v</mi><mi>r</mi></mrow></msub><mo stretchy="false">|</mo></mrow><mrow><mo stretchy="false">|</mo><mi>L</mi><mo stretchy="false">|</mo></mrow></mfrac><mrow><mi>log</mi></mrow><msub><mrow></mrow><mn>2</mn></msub><mfrac><mrow><mo stretchy="false">|</mo><mi>L</mi><msub><mrow></mrow><mrow><mi>v</mi><mi>r</mi></mrow></msub><mo stretchy="false">|</mo></mrow><mrow><mo stretchy="false">|</mo><mi>L</mi><mo stretchy="false">|</mo></mrow></mfrac></mrow></math></mathml>      (3)</p>
                </div>
                <div class="p1">
                    <p id="67">信息增益用<i>Gain</i>(<i>L</i>,<i>v</i><sub><i>i</i></sub>)表示,用以衡量样本集<i>L</i>按照属性<i>v</i><sub><i>i</i></sub>划分样本空间后的信息熵的下降:</p>
                </div>
                <div class="p1">
                    <p id="68"><i>Gain</i>(<i>L</i>,<i>v</i><sub><i>i</i></sub>)=<i>E</i>(<i>L</i>)-<i>E</i>(<i>L</i>,<i>v</i><sub><i>i</i></sub>)      (4)</p>
                </div>
                <div class="p1">
                    <p id="69">信息增益率用<i>GainRatio</i>(<i>L</i>,<i>v</i><sub><i>i</i></sub>)来表示,用以反映信息增益<i>Gain</i>(<i>L</i>,<i>v</i><sub><i>i</i></sub>)和属性<i>v</i><sub><i>i</i></sub>的均匀度的比值:</p>
                </div>
                <div class="p1">
                    <p id="70"><mathml id="183"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>G</mi><mi>a</mi><mi>i</mi><mi>n</mi><mi>R</mi><mi>a</mi><mi>t</mi><mi>i</mi><mi>o</mi><mo stretchy="false">(</mo><mi>L</mi><mo>,</mo><mi>v</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><mi>G</mi><mi>a</mi><mi>i</mi><mi>n</mi><mo stretchy="false">(</mo><mi>L</mi><mo>,</mo><mi>v</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><mrow><mi>S</mi><mi>p</mi><mi>l</mi><mi>i</mi><mi>t</mi><mi>Ι</mi><mi>n</mi><mi>f</mi><mi>o</mi><mo stretchy="false">(</mo><mi>L</mi><mo>,</mo><mi>v</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></mfrac></mrow></math></mathml>      (5)</p>
                </div>
                <h4 class="anchor-tag" id="71" name="71"><b>2.2 主成分分析</b></h4>
                <div class="p1">
                    <p id="72">PCA是一种通过正交变换将存在相关性的高维度向量转换成一组线性无关的低维度向量的数学方法<citation id="220" type="reference"><link href="35" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>。PCA在代数上表现为将原随机向量的协方差阵变换成对角矩阵。用<i><b>x</b></i><sub><i>i</i></sub>表示原始向量空间中的第<i>i</i>个样本的文本向量,原始特征空间维度为<i>n</i>,<i>x</i><sub><i>ij</i></sub>为词典中对应词<i>j</i>的特征值。</p>
                </div>
                <div class="p1">
                    <p id="73" class="code-formula">
                        <mathml id="73"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mrow><mi>i</mi><mn>1</mn></mrow></msub><mo>,</mo><mi>x</mi><msub><mrow></mrow><mrow><mi>i</mi><mn>2</mn></mrow></msub><mo>,</mo><mo>⋯</mo><mo>,</mo><mi>x</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>n</mi></mrow></msub><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mi mathvariant="bold-italic">A</mi><mo>=</mo><mrow><mo>[</mo><mrow><mtable><mtr><mtd><mi>x</mi><msub><mrow></mrow><mrow><mn>1</mn><mn>1</mn></mrow></msub></mtd><mtd><mi>x</mi><msub><mrow></mrow><mrow><mn>1</mn><mn>2</mn></mrow></msub></mtd><mtd><mo>⋯</mo></mtd><mtd><mi>x</mi><msub><mrow></mrow><mrow><mn>1</mn><mi>n</mi></mrow></msub></mtd></mtr><mtr><mtd><mi>x</mi><msub><mrow></mrow><mrow><mn>2</mn><mn>1</mn></mrow></msub></mtd><mtd><mi>x</mi><msub><mrow></mrow><mrow><mn>2</mn><mn>2</mn></mrow></msub></mtd><mtd><mo>⋯</mo></mtd><mtd><mi>x</mi><msub><mrow></mrow><mrow><mn>2</mn><mi>n</mi></mrow></msub></mtd></mtr><mtr><mtd><mo>⋮</mo></mtd><mtd><mo>⋮</mo></mtd><mtd></mtd><mtd><mo>⋮</mo></mtd></mtr><mtr><mtd><mi>x</mi><msub><mrow></mrow><mrow><mi>m</mi><mn>1</mn></mrow></msub></mtd><mtd><mi>x</mi><msub><mrow></mrow><mrow><mi>m</mi><mn>2</mn></mrow></msub></mtd><mtd><mo>⋯</mo></mtd><mtd><mi>x</mi><msub><mrow></mrow><mrow><mi>m</mi><mi>n</mi></mrow></msub></mtd></mtr></mtable></mrow><mo>]</mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>6</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="74">对数据集进行标准化转换,先求均值和方差:</p>
                </div>
                <div class="p1">
                    <p id="75"><mathml id="184"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi mathvariant="bold-italic">x</mi><mo stretchy="true">¯</mo></mover><msub><mrow></mrow><mi>j</mi></msub><mo>=</mo></mrow></math></mathml><mathml id="185"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mi>x</mi></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow><mi>m</mi></mfrac></mrow></math></mathml>      (7)</p>
                </div>
                <div class="p1">
                    <p id="76"><i><b>s</b></i><mathml id="186"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi mathvariant="bold-italic">j</mi><mn>2</mn></msubsup></mrow></math></mathml>=<mathml id="187"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mo stretchy="false">(</mo></mstyle><mi>x</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>-</mo><mover accent="true"><mi>x</mi><mo stretchy="true">¯</mo></mover><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo><msup><mrow></mrow><mn>2</mn></msup></mrow><mrow><mi>m</mi><mo>-</mo><mn>1</mn></mrow></mfrac></mrow></math></mathml>      (8)</p>
                </div>
                <div class="p1">
                    <p id="77">式中:<i><b>m</b></i>为样本集数量,<i><b>x</b></i><sub><i><b>ij</b></i></sub>为第<i><b>i</b></i>个样本第<i><b>j</b></i>维特征值,<mathml id="188"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi mathvariant="bold-italic">x</mi><mo stretchy="true">¯</mo></mover><msub><mrow></mrow><mi>j</mi></msub></mrow></math></mathml>为原始高维度向量空间的第<i><b>j</b></i>维上的均值,<i><b>s</b></i><mathml id="189"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi mathvariant="bold-italic">j</mi><mn>2</mn></msubsup></mrow></math></mathml>为该分量上的方差。标准化数据集为:</p>
                </div>
                <div class="p1">
                    <p id="78"><mathml id="190"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">Ζ</mi><msub><mrow></mrow><mrow><mi mathvariant="bold-italic">i</mi><mi mathvariant="bold-italic">j</mi></mrow></msub><mo>=</mo><mfrac><mrow><mi mathvariant="bold-italic">x</mi><msub><mrow></mrow><mrow><mi mathvariant="bold-italic">i</mi><mi mathvariant="bold-italic">j</mi></mrow></msub><mo>-</mo><mover accent="true"><mi mathvariant="bold-italic">x</mi><mo stretchy="true">¯</mo></mover><msub><mrow></mrow><mi mathvariant="bold-italic">j</mi></msub></mrow><mrow><mi mathvariant="bold-italic">s</mi><msub><mrow></mrow><mi mathvariant="bold-italic">j</mi></msub></mrow></mfrac><mtext> </mtext><mi mathvariant="bold-italic">i</mi><mo>=</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mo>⋯</mo><mo>,</mo><mi mathvariant="bold-italic">m</mi><mtext> </mtext><mi mathvariant="bold-italic">j</mi><mo>=</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mo>⋯</mo><mo>,</mo><mi mathvariant="bold-italic">n</mi></mrow></math></mathml>      (9)</p>
                </div>
                <div class="p1">
                    <p id="79"><b>PCA</b>寻求最大方差的投影方向,设最大方差的投影方向为<i><b>v</b></i>的单位列向量,目标函数为:</p>
                </div>
                <div class="p1">
                    <p id="80"><mathml id="191"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtext>a</mtext><mtext>r</mtext><mtext>g</mtext><mtext>m</mtext><mtext>a</mtext><mtext>x</mtext><msub><mrow></mrow><mi>v</mi></msub><mo>=</mo><mfrac><mn>1</mn><mn>2</mn></mfrac></mrow></math></mathml><mathml id="192"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mrow></mrow></mstyle></mrow></math></mathml><mathml id="193"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mn>1</mn><mrow><mi>m</mi><mo>-</mo><mn>1</mn></mrow></mfrac><mo stretchy="false">(</mo><mi>Ζ</mi><msub><mrow></mrow><mi>i</mi></msub><mi>v</mi><mo stretchy="false">)</mo><msup><mrow></mrow><mn>2</mn></msup></mrow></math></mathml>      (10)</p>
                </div>
                <div class="p1">
                    <p id="81"><i><b>v</b></i><sup>T</sup>为<i><b>v</b></i>的转置矩阵,满足下列约束条件:</p>
                </div>
                <div class="p1">
                    <p id="82"><i><b>v</b></i><sup>T</sup><i><b>v</b></i>=1      (11)</p>
                </div>
                <div class="p1">
                    <p id="83">构造拉格朗日乘子式:</p>
                </div>
                <div class="p1">
                    <p id="84"><image href="images/JYRJ201910006_194.jpg" type="" display="inline" placement="inline"><alt></alt></image>(<i>v</i>)=<mathml id="195"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mrow></mrow></mstyle></mrow></math></mathml><mathml id="196"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mn>1</mn><mrow><mi>m</mi><mo>-</mo><mn>1</mn></mrow></mfrac><mo stretchy="false">(</mo><mi>Ζ</mi><msub><mrow></mrow><mi>i</mi></msub><mi mathvariant="bold-italic">v</mi><mo stretchy="false">)</mo><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mi>λ</mi><mo stretchy="false">(</mo><mn>1</mn><mo>-</mo><mi mathvariant="bold-italic">v</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">v</mi><mo stretchy="false">)</mo></mrow></math></mathml>      (12)</p>
                </div>
                <div class="p1">
                    <p id="85">其中:</p>
                </div>
                <div class="p1">
                    <p id="86"><i><b>Cov</b></i>=<mathml id="197"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>,</mo><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mrow></mrow></mstyle></mrow></math></mathml><mathml id="198"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mn>1</mn><mrow><mi>m</mi><mo>-</mo><mn>1</mn></mrow></mfrac><mo stretchy="false">(</mo><mi>Ζ</mi><msub><mrow></mrow><mi>i</mi></msub><mo>⋅</mo><mi>Ζ</mi><msub><mrow></mrow><mi>j</mi></msub><msup><mrow></mrow><mtext>Τ</mtext></msup><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><mrow><mi>m</mi><mo>-</mo><mn>1</mn></mrow></mfrac><mi mathvariant="bold-italic">Ζ</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mo>⋅</mo><mi mathvariant="bold-italic">Ζ</mi></mrow></math></mathml>      (13)</p>
                </div>
                <div class="p1">
                    <p id="87">求偏导数得出:</p>
                </div>
                <div class="p1">
                    <p id="88">|<i>Cov</i>-<i>λ</i><i><b>I</b></i>|=<b>0</b>      (14)</p>
                </div>
                <div class="p1">
                    <p id="89">由式(13)和式(14)式可以看出,最大投影方向是协方差矩阵的最大特征值所对应的特征向量。协方差矩阵<i><b>Cov</b></i>为<i>n</i>×<i>m</i>的方阵, 如果<i>m</i>&lt;<i>n</i>,那么协方差矩阵的秩为<i>m</i>:</p>
                </div>
                <div class="p1">
                    <p id="90"><i>Rank</i>(<i><b>Cov</b></i>)=<i>m</i>      (15)</p>
                </div>
                <div class="p1">
                    <p id="91">其特征向量数量为<i>m</i>,特征值数量为<i>m</i>,且满足:</p>
                </div>
                <div class="p1">
                    <p id="92"><i>λ</i><sub>1</sub>≥<i>λ</i><sub>2</sub>≥…≥<i>λ</i><sub><i>m</i></sub>≥0      (16)</p>
                </div>
                <div class="p1">
                    <p id="93"><i><b>V</b></i><sub><i>m</i></sub>=[<i>v</i><sub>1</sub><i>v</i><sub>2</sub> … <i>v</i><sub><i>m</i></sub>]      (17)</p>
                </div>
                <div class="p1">
                    <p id="94">显然,将样本投影到<i>λ</i><sub>1</sub>对应的特征向量方向<i>v</i><sub>1</sub>后的方差最大,投影到<i>v</i><sub>2</sub>方向后方差次之,依次减小。从分类间隔角度来看,在向量空间中进行样本划分依据同类间样本间隔小而异类间样本间隔大。因此,投影到第一主成分(特征值最大的特征向量)方向上的方差最大,反映了经过主成分特征抽取后信息量损失最小<citation id="221" type="reference"><link href="43" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>。</p>
                </div>
                <h3 id="95" name="95" class="anchor-tag"><b>3 混合特征选择模型</b></h3>
                <div class="p1">
                    <p id="96">实际上,用于文本分类的词语非常多,并且在文本向量空间上构造的原始特征空间具有相对较高的维度,可以高达数万个维度。 因此,减少文本分类的数据维度是必不可少的<citation id="222" type="reference"><link href="45" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>。根据信息增益率和主成分分析方法,分为3个步骤进行降维。第一步:进行数据预处理,在分词后统计词频,去掉停止词,将文本表示成文本向量。第二步:计算每个词语(即每列)的信息增益率,按降序排序,选择那些大于阈值的特征词用来构造特征初选子集。第三步:采用主成分分析对特征初选子集进行数学转换,将其映射到低维空间,构造特征再选子集。</p>
                </div>
                <h4 class="anchor-tag" id="97" name="97"><b>3.1 数据预处理</b></h4>
                <div class="p1">
                    <p id="98">数据预处理的任务是采用TF方法将文本表示成文本向量,包括消除标点符号、去除停止词和统计词频。TF的基本思想是词语的重要性与它在文档中出现的次数成正比,与词语出现的次序无关。在python语言的工具中有一些函数可以用来进行文本语料的预处理,例如tokenize函数可以进行去掉标点符号进行分词,stemmed可以去掉停止词,counter可以统计词频。经过预处理之后,TF的文本向量矩阵表示成:</p>
                </div>
                <div class="area_img" id="99">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JYRJ201910006_09900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="101">式中:<i>v</i>是词典中词语总数,<i>m</i>是语料集中的文本总数,<i>w</i><sub><i>ij</i></sub>是第<i>i</i>篇文档中第<i>j</i>词的统计词频。文本类别标签是<i>Y</i>:</p>
                </div>
                <div class="area_img" id="226">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JYRJ201910006_22600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="104">式中:<i>y</i><sub><i>i</i></sub>是类别1,2,…,|<i>C</i>|之一。</p>
                </div>
                <h4 class="anchor-tag" id="105" name="105"><b>3.2 特征初选子集</b></h4>
                <div class="p1">
                    <p id="106">根据式(1)-式(5),依次计算<i><b>W</b></i>中每一列的信息增益率。每个词语对应的信息增益率如下:</p>
                </div>
                <div class="p1">
                    <p id="107">[<i>rig</i><sub>1</sub>,<i>rig</i><sub>2</sub>,…,<i>rig</i><sub><i>i</i></sub>,…,<i>rig</i><sub><i>v</i></sub>]      (20)</p>
                </div>
                <div class="p1">
                    <p id="108">式中:<i>rig</i><sub><i>i</i></sub>是第<i>i</i>个词语(即第<i>i</i>列)的信息增益率,信息增益率越低说明该词语在分类作用上越低。设置阈值,将低于阈值的列从矩阵中删除,剩余的列构成特征初选子集<i><b>Wf</b></i>,并按照信息增益率大小降序排列:</p>
                </div>
                <div class="area_img" id="109">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JYRJ201910006_10900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="111">特征初选子集中的值<i>wf</i><sub><i>ij</i></sub>是词频统计值,没有考虑该词在第<i>i</i>篇文档之外出现的情况。假设一种极端情况,如果该词在每一篇文档中都出现,它的分类作用应该等于无。因此,考虑到逆向文档频率对文本向量值的影响。TF-IDF的基本思想是词语的重要性与它在文档中出现的次数成正比,与此同时,还与其在语料库其他文档中出现的频率成反比。在python语言的工具包中,内建函数TfidfVectorizer可以用来将文本表示成TF-IDF值的文本向量:</p>
                </div>
                <div class="p1">
                    <p id="112" class="code-formula">
                        <mathml id="112"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">Τ</mi><mi mathvariant="bold-italic">F</mi><mi mathvariant="bold-italic">Ι</mi><mi mathvariant="bold-italic">D</mi><mi mathvariant="bold-italic">F</mi><mo>=</mo><mrow><mo>[</mo><mrow><mtable><mtr><mtd><mi>Τ</mi><mi>F</mi><mi>Ι</mi><mi>D</mi><mi>F</mi><msub><mrow></mrow><mrow><mn>1</mn><mn>1</mn></mrow></msub></mtd><mtd><mi>Τ</mi><mi>F</mi><mi>Ι</mi><mi>D</mi><mi>F</mi><msub><mrow></mrow><mrow><mn>1</mn><mn>2</mn></mrow></msub></mtd><mtd><mo>⋯</mo></mtd><mtd><mi>Τ</mi><mi>F</mi><mi>Ι</mi><mi>D</mi><mi>F</mi><msub><mrow></mrow><mrow><mn>1</mn><mi>n</mi></mrow></msub></mtd></mtr><mtr><mtd><mi>Τ</mi><mi>F</mi><mi>Ι</mi><mi>D</mi><mi>F</mi><msub><mrow></mrow><mrow><mn>2</mn><mn>1</mn></mrow></msub></mtd><mtd><mi>Τ</mi><mi>F</mi><mi>Ι</mi><mi>D</mi><mi>F</mi><msub><mrow></mrow><mrow><mn>2</mn><mn>2</mn></mrow></msub></mtd><mtd><mo>⋯</mo></mtd><mtd><mi>Τ</mi><mi>F</mi><mi>Ι</mi><mi>D</mi><mi>F</mi><msub><mrow></mrow><mrow><mn>2</mn><mi>n</mi></mrow></msub></mtd></mtr><mtr><mtd><mo>⋮</mo></mtd><mtd><mo>⋮</mo></mtd><mtd></mtd><mtd><mo>⋮</mo></mtd></mtr><mtr><mtd><mi>Τ</mi><mi>F</mi><mi>Ι</mi><mi>D</mi><mi>F</mi><msub><mrow></mrow><mrow><mi>m</mi><mn>1</mn></mrow></msub></mtd><mtd><mi>Τ</mi><mi>F</mi><mi>Ι</mi><mi>D</mi><mi>F</mi><msub><mrow></mrow><mrow><mi>m</mi><mn>2</mn></mrow></msub></mtd><mtd><mo>⋯</mo></mtd><mtd><mi>Τ</mi><mi>F</mi><mi>Ι</mi><mi>D</mi><mi>F</mi><msub><mrow></mrow><mi>m</mi></msub></mtd></mtr></mtable></mrow><mo>]</mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>2</mn><mn>2</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="113">原始文本特征空间<i>v</i>是词典中单词总数,经过第一次降维的特征出现子集的维数是<i>n</i>,减少的维度是<i>v</i>-<i>n</i>。</p>
                </div>
                <h4 class="anchor-tag" id="114" name="114"><b>3.3 特征再选子集</b></h4>
                <div class="p1">
                    <p id="115">特征初选子集<i><b>TFIDF</b></i>是一个<i>m</i>×<i>n</i>的样本空间,依据式(7)-式(9),将矩阵<i><b>TFIDF</b></i>进行z标准化操作,即<i>Z</i>=<i>zscore</i>(<i><b>TFIDF</b></i>),对数据进行修正以满足均值和标准差要求。</p>
                </div>
                <div class="p1">
                    <p id="116"><b>定义1 </b><i><b>Cov</b></i>是具有<i>n</i>阶的对称协方差矩阵,并且存在<i>n</i>个单位列向量的正交矩阵<i><b>V</b></i>,即:</p>
                </div>
                <div class="area_img" id="117">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JYRJ201910006_11700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="119">正交矩阵<i><b>V</b></i>由对应于主对角线上的特征值的特征向量组成。三角矩阵用<i>Λ</i>表示,并且在等式的两边左乘<i><b>V</b></i>,根据式(13),得到如下等式:</p>
                </div>
                <div class="p1">
                    <p id="120"><mathml id="199"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mn>1</mn><mrow><mi>m</mi><mo>-</mo><mn>1</mn></mrow></mfrac><mi mathvariant="bold-italic">Ζ</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mo>⋅</mo><mi mathvariant="bold-italic">Ζ</mi><mo>⋅</mo><mi mathvariant="bold-italic">V</mi><mo>=</mo><mi mathvariant="bold-italic">V</mi><mo>⋅</mo><mi mathvariant="bold-italic">Λ</mi></mrow></math></mathml>      (24)</p>
                </div>
                <div class="p1">
                    <p id="121">用<i><b>V</b></i><sup>T</sup>右乘式(24)两边,得到:</p>
                </div>
                <div class="p1">
                    <p id="122"><mathml id="200"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mn>1</mn><mrow><mi>m</mi><mo>-</mo><mn>1</mn></mrow></mfrac><mo>=</mo><mi mathvariant="bold-italic">Ζ</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mo>⋅</mo><mi mathvariant="bold-italic">Ζ</mi><mo>=</mo><mi mathvariant="bold-italic">V</mi><mo>⋅</mo><mi mathvariant="bold-italic">Λ</mi><mo>⋅</mo><mi mathvariant="bold-italic">V</mi><msup><mrow></mrow><mtext>Τ</mtext></msup></mrow></math></mathml>      (25)</p>
                </div>
                <div class="p1">
                    <p id="123"><b>定义2</b> 存在m×m的正交矩阵<i><b>U</b></i>能够满足:</p>
                </div>
                <div class="p1">
                    <p id="124"><i><b>U</b></i><sup>T</sup>·<i><b>U</b></i><b>=</b><i><b>I</b></i>      (26)</p>
                </div>
                <div class="p1">
                    <p id="125">式中:<i><b>I</b></i>是单位矩阵。三角矩阵<i>Λ</i>的对角元素是非负实数,因此,可以分解为:</p>
                </div>
                <div class="p1">
                    <p id="126"><mathml id="201"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">Λ</mi><mo>=</mo><mfrac><mn>1</mn><mrow><mi>m</mi><mo>-</mo><mn>1</mn></mrow></mfrac><mi mathvariant="bold-italic">S</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mo>⋅</mo><mi mathvariant="bold-italic">S</mi></mrow></math></mathml>      (27)</p>
                </div>
                <div class="p1">
                    <p id="127">同时,三角矩阵<i>Λ</i>的秩为<i>m</i>,且<i>λ</i><sub><i>m</i></sub><sub>+1</sub>=0,<i>λ</i><sub><i>m</i></sub><sub>+2</sub>=0,…,<i>λ</i><sub><i>n</i></sub>=0,因此,根据式(27),<i><b>S</b></i>可以扩展为<i>n</i>×<i>n</i>的矩阵:</p>
                </div>
                <div class="p1">
                    <p id="128" class="code-formula">
                        <mathml id="128"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">S</mi><mo>=</mo><mrow><mo>[</mo><mrow><mtable><mtr><mtd><msqrt><mrow><mfrac><mrow><mi>λ</mi><msub><mrow></mrow><mn>1</mn></msub></mrow><mrow><mi>m</mi><mo>-</mo><mn>1</mn></mrow></mfrac></mrow></msqrt></mtd><mtd></mtd><mtd></mtd><mtd></mtd><mtd></mtd><mtd></mtd><mtd></mtd></mtr><mtr><mtd></mtd><mtd><msqrt><mrow><mfrac><mrow><mi>λ</mi><msub><mrow></mrow><mn>1</mn></msub></mrow><mrow><mi>m</mi><mo>-</mo><mn>2</mn></mrow></mfrac></mrow></msqrt></mtd><mtd></mtd><mtd></mtd><mtd></mtd><mtd></mtd><mtd></mtd></mtr><mtr><mtd><mo stretchy="false">[</mo><mn>3</mn><mo stretchy="false">]</mo><mo>⋱</mo></mtd><mtd></mtd><mtd></mtd><mtd></mtd><mtd></mtd><mtd></mtd><mtd></mtd></mtr><mtr><mtd><mo stretchy="false">[</mo><mn>4</mn><mo stretchy="false">]</mo><msqrt><mrow><mfrac><mrow><mi>λ</mi><msub><mrow></mrow><mn>1</mn></msub></mrow><mrow><mi>m</mi><mo>-</mo><mn>1</mn></mrow></mfrac></mrow></msqrt></mtd><mtd></mtd><mtd></mtd><mtd></mtd><mtd></mtd><mtd></mtd><mtd></mtd></mtr><mtr><mtd><mo stretchy="false">[</mo><mn>5</mn><mo stretchy="false">]</mo><mn>0</mn></mtd><mtd></mtd><mtd></mtd><mtd></mtd><mtd></mtd><mtd></mtd><mtd></mtd></mtr><mtr><mtd><mo stretchy="false">[</mo><mn>6</mn><mo stretchy="false">]</mo><mo>⋱</mo></mtd><mtd></mtd><mtd></mtd><mtd></mtd><mtd></mtd><mtd></mtd><mtd></mtd></mtr><mtr><mtd><mo stretchy="false">[</mo><mn>7</mn><mo stretchy="false">]</mo><mn>0</mn></mtd><mtd></mtd><mtd></mtd><mtd></mtd><mtd></mtd><mtd></mtd><mtd></mtd></mtr></mtable></mrow><mo>]</mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>2</mn><mn>8</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="129">根据式(25)-式(27),可以推导出:</p>
                </div>
                <div class="p1">
                    <p id="130" class="code-formula">
                        <mathml id="130"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi mathvariant="bold-italic">V</mi><mo>⋅</mo><mi mathvariant="bold-italic">Λ</mi><mo>⋅</mo><mi mathvariant="bold-italic">V</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mo>=</mo><mfrac><mn>1</mn><mrow><mi>m</mi><mo>-</mo><mn>1</mn></mrow></mfrac><mi mathvariant="bold-italic">V</mi><mo>⋅</mo><mi mathvariant="bold-italic">S</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mo>⋅</mo><mi mathvariant="bold-italic">U</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mo>⋅</mo><mi mathvariant="bold-italic">U</mi><mo>⋅</mo><mi mathvariant="bold-italic">S</mi><mo>⋅</mo><mi mathvariant="bold-italic">V</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mo>=</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mfrac><mn>1</mn><mrow><mi>m</mi><mo>-</mo><mn>1</mn></mrow></mfrac><mo stretchy="false">(</mo><mi mathvariant="bold-italic">U</mi><mo>⋅</mo><mi mathvariant="bold-italic">S</mi><mo>⋅</mo><mi mathvariant="bold-italic">V</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mo stretchy="false">)</mo><msup><mrow></mrow><mtext>Τ</mtext></msup><mo>⋅</mo><mo stretchy="false">(</mo><mi mathvariant="bold-italic">U</mi><mo>⋅</mo><mi mathvariant="bold-italic">S</mi><mo>⋅</mo><mi mathvariant="bold-italic">V</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>2</mn><mn>9</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="131">将式(25)的左边代入式(29),得到:</p>
                </div>
                <div class="p1">
                    <p id="132"><i><b>Z</b></i><b>=</b><i><b>U</b></i><b>·</b><i><b>S</b></i><b>·</b><i><b>V</b></i><sup>T</sup>      (30)</p>
                </div>
                <div class="p1">
                    <p id="133">如果特征初选子集<i><b>TFIDF</b></i>的行数大于列数,即<i>m</i>&gt;<i>n</i>,<i><b>Z</b></i>投影到<i><b>V</b></i>上:</p>
                </div>
                <div class="p1">
                    <p id="134"><i><b>Z</b></i><b>·</b><i><b>V</b></i><b>=</b><i><b>U</b></i><b>·</b><i><b>S</b></i>      (31)</p>
                </div>
                <div class="p1">
                    <p id="135">如果特征初选子集<i><b>TFIDF</b></i>的行数小于列数,即<i>m</i>&lt;<i>n</i>,<i><b>Z</b></i><sup><i>T</i></sup>投影到<i><b>U</b></i>上:</p>
                </div>
                <div class="p1">
                    <p id="136"><i><b>Z</b></i><sup>T</sup>·<i><b>U</b></i><b>=(</b><i><b>U</b></i><b>·</b><i><b>S</b></i><b>·</b><i><b>V</b></i><sup>T</sup>)<sup>T</sup>·<i><b>U</b></i>=<i><b>V</b></i><b>·</b><i><b>S</b></i><sup>T</sup>=<i><b>V</b></i><b>·</b><i><b>S</b></i>      (32)</p>
                </div>
                <div class="p1">
                    <p id="137">因此,<i><b>TFIDF</b></i>既可以在列向量方向取得降维也可以通过行向量方向,这取决于特征数量和样本数量的大小关系。换言之,第二次降维的幅度为|<i>m</i>-<i>n</i>|。</p>
                </div>
                <div class="p1">
                    <p id="138">根据式(23)中的三角矩阵的特征值降序排列,特征值<i>λ</i><sub>1</sub>对应特征向量<i><b>v</b></i><sub>1</sub>,特征值<i>λ</i><sub>2</sub>对应特征向量<i><b>v</b></i><sub>2</sub>,…,特征值<i>λ</i><sub><i>n</i></sub>对应特征向量<i><b>v</b></i><sub><i>n</i></sub>。由这些特征向量构成正交投影矩阵<i><b>V</b></i>:</p>
                </div>
                <div class="p1">
                    <p id="139"><i><b>V</b></i>=[<i>v</i><sub>1</sub><i>v</i><sub>2</sub> … <i>v</i><sub><i>m</i></sub><i>v</i><sub><i>m</i></sub><sub>+1</sub> … <i>v</i><sub><i>n</i></sub>]      (33)</p>
                </div>
                <div class="p1">
                    <p id="140">根据式(28),假设<i>m</i>&lt;<i>n</i>:</p>
                </div>
                <div class="p1">
                    <p id="141" class="code-formula">
                        <mathml id="141"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>σ</mi><mo>=</mo><msqrt><mrow><mfrac><mi>λ</mi><mrow><mi>m</mi><mo>-</mo><mn>1</mn></mrow></mfrac></mrow></msqrt></mrow></math></mathml>
                    </p>
                </div>
                <div class="area_img" id="142">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JYRJ201910006_14200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="144">由于<i>σ</i><sub><i>m</i></sub><sub>+1</sub>=0,<i>σ</i><sub><i>m</i></sub><sub>+2</sub>=0,…,<i>σ</i><sub><i>n</i></sub>=0,根据式(32)推导出:</p>
                </div>
                <div class="p1">
                    <p id="145"><i><b>V</b></i><b>·</b><i><b>S</b></i>=[<i>σ</i><sub>1</sub><i><b>v</b></i><sub>1</sub>,<i>σ</i><sub>2</sub><i><b>v</b></i><sub>2</sub>,…,<i>σ</i><sub><i>m</i></sub><i><b>v</b></i><sub><i>m</i></sub>]=<i><b>Z</b></i><b>·</b><i><b>V</b></i><sub><i>m</i></sub>      (35)</p>
                </div>
                <div class="p1">
                    <p id="146"><b>定义3</b> 主成分矩阵是<i><b>TFIDF</b></i>的标准化矩阵<i><b>Z</b></i>在特征向量<i><b>V</b></i>上的投影,称为矩阵<i><b>PC</b></i>:</p>
                </div>
                <div class="p1">
                    <p id="147"><i><b>PC</b></i><b>=</b><i><b>Z</b></i><b>·</b><i><b>V</b></i><sub><i>m</i></sub>=[<i>pc</i><sub>1</sub>,<i>pc</i><sub>2</sub>,…,<i>pc</i><sub><i>m</i></sub>]      (36)</p>
                </div>
                <div class="p1">
                    <p id="148">一般情况下,词典中的词语数量数万以上,语料库中的文本数量也是成千上万,将特征初选子集<i><b>TFIDF</b></i>投影到<i><b>V</b></i><sub><i>m</i></sub>上构成特征再选子集,这种映射实现了降维。</p>
                </div>
                <h4 class="anchor-tag" id="149" name="149"><b>3.4 混合选择模型</b></h4>
                <div class="p1">
                    <p id="150">混合特征选择模型包括三个部分。首先是预处理模块,其次是特征初选模块(PFS),然后是特征再选模块(SFS)。模型结构如图1所示。</p>
                </div>
                <div class="area_img" id="151">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201910006_151.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 混合特征选择模型" src="Detail/GetImg?filename=images/JYRJ201910006_151.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 混合特征选择模型  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201910006_151.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="152">Preprocessing模块的输入是语料库,对文本进行分词和去掉停止词之后,统计词频,该模块输出为矩阵<i>W</i>。PFS模块是特征初选模块,对词频矩阵<i>W</i>的每列计算信息增益率,选择那些信息增益率大于阈值的列来构成特征初选子集,然后根据TF-IDF思想计算特征初选子集的赋值,该模块输出为矩阵<i><b>TFIDF</b></i>。SFS模块是特征再选模块,将输入的<i><b>TFIDF</b></i>映射为在主成分向量上的投影矩阵,即<i><b>Z</b></i><b>(</b><i><b>TFIDE</b></i><b>)·</b><i><b>V</b></i><sub><i>m</i></sub>。</p>
                </div>
                <div class="p1">
                    <p id="153"><i>m</i>个主成分[<i>pc</i><sub>1</sub>,<i>pc</i><sub>2</sub>,…,<i>pc</i><sub><i>m</i></sub>]构成特征再选矩阵,其中,第一主成分<i>pc</i><sub>1</sub>的离差最大,第二主成分<i>pc</i><sub>2</sub>的离差次之,依此类推。特征再选矩阵<i><b>PC</b></i>被输入到分类器进行训练。</p>
                </div>
                <h3 id="154" name="154" class="anchor-tag"><b>4 实验分析</b></h3>
                <h4 class="anchor-tag" id="155" name="155"><b>4.1 语料集</b></h4>
                <div class="p1">
                    <p id="156">20 NewsGroup语料库是机器学习中的标准数据集,涵盖来自20个不同新闻组的18 828个文档。 为了具有普遍性和可重复性,实验选择20 NewsGroup作为文本集<citation id="223" type="reference"><link href="47" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>。将近20 000篇文档被平均分为20个不同组,有些新闻组具有相似的共同的大主题,例如,rec.autos和rec.autos都具有运动类主题。</p>
                </div>
                <div class="p1">
                    <p id="157">很显然,训练集中的文本大主题是否具有相关性对模型的分类结果具有很大影响,因此,实验设计两组文本集:DatasetCats1和DatasetCats2,如表1所示。DatasetCats1都具有与体育有关的主题内容,经过python自然语言工具包预处理后,共有30 466个词语和3 979篇文档,2 389篇文档用于训练,1 590篇文档用于测试验证。DatasetCats2具有不同的主题内容,经过python自然语言工具包预处理后,共有36 712个词语和3 936篇文档,2 363篇文档用于训练,1 573篇文档用于测试验证。</p>
                </div>
                <div class="area_img" id="158">
                    <p class="img_tit"><b>表1 DatasetCats1和DatasetCats1</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="158" border="1"><tr><td><br />DatasetCats1</td><td>DatasetCats2</td></tr><tr><td><br />rec.autos</td><td>comp.graphics</td></tr><tr><td><br />rec.motorcycles</td><td>misc.forsale</td></tr><tr><td><br />rec.sport.baseball</td><td>sci.crypt</td></tr><tr><td><br />rec.sport.hockey</td><td>soc.religion.christian</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="159" name="159"><b>4.2 实验结果</b></h4>
                <div class="p1">
                    <p id="160">效果评估函数根据混淆矩阵计算分类器的准确率、召回率和F1度量。其中,准确率衡量标记为正类的样本中实际为正类的百分比,反映了当一个样本被判定为正类时,实际为正类的概率。召回率反映了正确识别的正类数量在实际正类数量中的比例。F1度量是一种准确率和召回率调和均值,它赋予准确率和召回率相等的权重。</p>
                </div>
                <div class="p1">
                    <p id="161">一般常用的文本分类器有支持向量机(Support Vector Machine, SVM)和朴素贝叶斯。(Naive Bayes, NB)支持向量机通过寻求最大分类间隔,实现结构化风险最小来提高分类模型学习和泛化能力<citation id="224" type="reference"><link href="49" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>。朴素贝叶斯分类器是一系列简单的概率分类器,根据贝叶斯概率原理,基于在特征之间具有很强的独立性假设之上,其模型包括多项式模型和伯努利模型<citation id="225" type="reference"><link href="51" rel="bibliography" /><link href="53" rel="bibliography" /><sup>[<a class="sup">25</a>,<a class="sup">26</a>]</sup></citation>,多项式朴素贝叶斯通过后验概率进行文本分类,容易实现,运行速度快。实验选择支持向量机和多项式朴素贝叶斯作为分类器用以比较分类性能。</p>
                </div>
                <div class="p1">
                    <p id="162">实验分为两步:</p>
                </div>
                <div class="p1">
                    <p id="163">第一步,将数据集DatasetCats1和DatasetCats2直接输入给分类器进行训练,在数据集DatasetCats1上的分类性能如表2所示,在数据集DatasetCats2上的分类性能如表3所示。</p>
                </div>
                <div class="area_img" id="164">
                    <p class="img_tit"><b>表2 DatasetCats1直接输入分类器的分类性能(相似主题</b>) <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="164" border="1"><tr><td rowspan="2"><br />主题</td><td colspan="4"><br />NB/SVM</td></tr><tr><td><br />准确率</td><td>召回率</td><td>f1</td><td>测试</td></tr><tr><td><br />autos</td><td>0.95/0.96</td><td>0.97/0.98</td><td>0.96/0.97</td><td>396/396</td></tr><tr><td><br />motorcycles</td><td>0.99/0.98</td><td>0.96/0.96</td><td>0.97/0.97</td><td>398/398</td></tr><tr><td><br />baseball</td><td>0.98/0.97</td><td>0.94/0.94</td><td>0.96/0.95</td><td>397/397</td></tr><tr><td><br />hockey</td><td>0.94/0.95</td><td>0.98/0.97</td><td>0.96/0.96</td><td>399/399</td></tr><tr><td><br />平均</td><td>0.96/0.96</td><td>0.96/0.96</td><td>0.96/0.96</td><td></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="165">
                    <p class="img_tit"><b>表3 DatasetCats2直接输入分类器的分类性能(不同主题</b>) <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="165" border="1"><tr><td rowspan="2"><br />主题</td><td colspan="4"><br />NB/SVM</td></tr><tr><td><br />准确率</td><td>召回率</td><td>f1</td><td>测试</td></tr><tr><td><br />graphics</td><td>0.96/0.93</td><td>0.82/0.94</td><td>0.89/0.93</td><td>389/389</td></tr><tr><td><br />forsale</td><td>0.99/0.95</td><td>0.88/0.98</td><td>0.93/0.96</td><td>390/390</td></tr><tr><td><br />crypt</td><td>0.85/0.98</td><td>0.98/0.95</td><td>0.91/0.96</td><td>396/396</td></tr><tr><td><br />christian</td><td>0.91/0.98</td><td>0.99/0.97</td><td>0.95/0.98</td><td>398/398</td></tr><tr><td><br />平均</td><td>0.93/0.96</td><td>0.92/0.96</td><td>0.92/0.96</td><td></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="166">从表2和表3的平均分类性能数值中可以看出,在DatasetCats1(相似主题)数据集上,两个分类器的分类性能相当,在DatasetCats2(不同主题)数据集上,SVM分类器好于NB分类器。</p>
                </div>
                <div class="p1">
                    <p id="167">第二步,在数据集DatasetCats1和DatasetCats2经过混合特征选择后,再输入给分类器进行训练。将数据集DatasetCats1分为DatasetCats1_train训练集和DatasetCats1_test测试集。DatasetCats1_train训练集的文本向量矩阵为2 389×30 466,维数为30 466,在特征初选模块PFS中,计算它们的信息增益率,选择其中的6 702列,重新按照TF-IDF计算文本向量,构造特征初选子集DatasetCats1_train_PFS,该矩阵为2 389×6 702。根据式(33)和式(36),将DatasetCats1_train_PFS矩阵投影到主成分空间[<i>v</i><sub>1</sub><i>v</i><sub>2</sub>…<i>v</i><sub>2 389</sub>],构造特征再选子集DatasetCats1_train_SFS,该矩阵为2 389×2 389。然后将其输入到NB分类器和SVM分类器进行训练。DatasetCats1_test测试集为1 590×30 466,选择上述索引的6702列,构造DatasetCats1_test_PFS,该矩阵为1 590×6 702。将DatasetCats1_ test _PFS矩阵投影到主成分空间[<i>v</i><sub>1</sub><i>v</i><sub>2</sub>…<i>v</i><sub>2 389</sub>],构造DatasetCats1_test_SFS,该矩阵为1 590×2 389,用训练好的分类器预测测试文本向量,分类结果如表4。通过两次降维,维度减少了92%。</p>
                </div>
                <div class="area_img" id="168">
                    <p class="img_tit"><b>表4 DatasetCats1混合特征选择后的分类性能(相似主题</b>) <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="168" border="1"><tr><td rowspan="2"><br />主题</td><td colspan="4"><br />NB/SVM</td></tr><tr><td><br />准确率</td><td>召回率</td><td>f1</td><td>测试</td></tr><tr><td><br />autos</td><td>0.97/0.97</td><td>0.98/0.98</td><td>0.97/0.97</td><td>396/396</td></tr><tr><td><br />motorcycles</td><td>0.98/0.96</td><td>0.96/0.97</td><td>0.97/0.96</td><td>398/398</td></tr><tr><td><br />baseball</td><td>0.98/0.98</td><td>0.95/0.95</td><td>0.96/0.96</td><td>397/397</td></tr><tr><td><br />hockey</td><td>0.94/0.97</td><td>0.97/0.97</td><td>0.96/0.97</td><td>399/399</td></tr><tr><td><br />平均</td><td>0.96/0.97</td><td>0.96/0.97</td><td>0.96/0.97</td><td></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="169">将数据集DatasetCats2分为DatasetCats2_train训练集和DatasetCats2_test测试集。DatasetCats2_train训练集的文本向量矩阵为2 363×36 712,维数为36 712,在特征初选模块PFS中,选择其中的6 903列,构造特征初选子集DatasetCats2_train_PFS,该矩阵为2 363×6 903。将DatasetCats2_train_PFS矩阵投影到主成分空间[<i>v</i><sub>1</sub><i>v</i><sub>2</sub>…<i>v</i><sub>2 363</sub>],构造特征再选子集DatasetCats2_train_SFS,该矩阵为2 363×2 363。然后将其输入到NB分类器和SVM分类器进行训练。DatasetCats2_test测试集为1 573×36 712,选择上述索引的6 903列,构造DatasetCats2_test_PFS,该矩阵为1 573×6 903。将DatasetCats2_ test _PFS矩阵投影到主成分空间[<i>v</i><sub>1</sub><i>v</i><sub>2</sub>…<i>v</i><sub>2 363</sub>],构造DatasetCats2_test_SFS,该矩阵为1 573×2 363,用训练好的分类器预测测试文本向量,分类结果如表5所示。通过两次降维,维度减少了94%。</p>
                </div>
                <div class="area_img" id="170">
                    <p class="img_tit"><b>表5 DatasetCats2混合特征选择后的分类性能(不同主题</b>) <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="170" border="1"><tr><td rowspan="2"><br />主题</td><td colspan="4"><br />NB/SVM</td></tr><tr><td><br />准确率</td><td>召回率</td><td>f1</td><td>测试</td></tr><tr><td><br />graphics</td><td>0.95/0.99</td><td>0.93/0.96</td><td>0.94/0.97</td><td>389/389</td></tr><tr><td><br />forsale</td><td>0.96/0.98</td><td>0.89/0.98</td><td>0.93/0.98</td><td>390/390</td></tr><tr><td><br />crypt</td><td>0.95/0.98</td><td>0.97/0.97</td><td>0.96/0.97</td><td>396/396</td></tr><tr><td><br />christian</td><td>0.92/0.93</td><td>0.98/0.97</td><td>0.95/0.95</td><td>398/398</td></tr><tr><td><br />平均</td><td>0.94/0.97</td><td>0.95/0.97</td><td>0.94/0.97</td><td></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="171">对比表2和表4,在相似主题的数据集上,对于直接输入文本向量和经过混合特征选择,NB分类器的平均准确率都为0.96;SVM分类器的平均准确率,前者为0.96,后者为0.97。两个分类器的平均准确率提升大约0.5%。</p>
                </div>
                <div class="p1">
                    <p id="172">对比表3和表5,在不同主题的数据集上,对于直接输入文本向量和经过混合特征选择, NB分类器的平均准确率,前者为0.93,后者为0.94; SVM分类器的平均准确率,前者为0.96,后者为0.97。两个分类器的平均准确率提升大约1%。混合特征选择方法在两个数据集上的分类性能如图2所示。</p>
                </div>
                <div class="area_img" id="173">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201910006_173.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 平均准确率" src="Detail/GetImg?filename=images/JYRJ201910006_173.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 平均准确率  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201910006_173.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h3 id="174" name="174" class="anchor-tag"><b>5 结 语</b></h3>
                <div class="p1">
                    <p id="175">为了有效降低特征空间维度,基于PCA的混合特征选择方法将信息增益率和主成分分析方法结合起来,通过将特征初选子集映射到主成分空间,实现二次降维。实验结果表明,在相似主题数据集上采用该方法的降维效果达到92%,平均准确率提升大约0.5%;而在不同主题数据集上的降维效果达到94%,平均准确率提升大约1%。在大数据时代,对于高维与稀疏的文本集,混合特征选择方法不但满足了特征降维需求,大大减少了计算开销,而且也提高了分类性能。此外,实验选择的文本数据集存在一定的主题相关度,实验结果也表明主题分布对特征降维与分类性能有影响,这将是下一步开展的研究方向。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Internet world status[OL]">

                                <b>[1]</b> Miniwatts Marketing Group.Internet world status[OL].2017.www.internetworldstats.com/stats.htm.
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A hybird approach for word emotion recognition">

                                <b>[2]</b> Li J,Ren F J.A hybird approach for word emotion recognition[J].IEEJ Transaction on Electricaland Electronic Engineering,2013,8(6):616-626.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Automatic identification and recognition of sentiment words using an optimization-based model with propagation">

                                <b>[3]</b> Luo K H,Deng Z H,Yu H L,et al.Automatic identification and recognition of sentiment words using an optimization-based model with propagation[J].International Journal of Intelligent System,2015,30(5):537-549.
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Sentiment classification using machine learning">

                                <b>[4]</b> Wawre S V,Deshmukh S N.Sentiment classification using machine learning[J].International Journal of Science and Research,2016,5(4):819-821.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A survey of text classification algorithms">

                                <b>[5]</b> Aggarwal C C,Zhai C.A survey of text classification algorithms,mining text data[M].Heidelberg:Springer,2012.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning phrase patterns for text classification using a knowledge graph and unlabeled data">

                                <b>[6]</b> Marin A,Holenstein R,Sarikaya R,et al.Learning phrase patterns for text classification using a knowledge graph and unlabeled data[C]//Proceedings of the Annual Conference of the International Speech Communication Association,INTERSPEECH 2014.2014:253-257.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Text as Data;The Promise and Pitfalls of Automatic Content Analysis Methods for Political Texts,&amp;quot;">

                                <b>[7]</b> Grimmer J,Stewart B M.Text as data:the promise and pitfalls of automatic content analysis methods for political texts[J].Political Analysis,2013,21(3):267-297.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14032300044876&amp;v=MDE1MzRkR2VycVFUTW53WmVadUh5am1VYi9JSmw4UmFScz1OaWZPZmJLOEh0TE9ySTlGWk84TEJIcy9vQk1UNlQ0UFFIL2lyUg==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b> Wang D,Zhang H,Liu R,et al.t-Test feature selection approach based on term frequency for text categorization[J].Pattern Recognition Letters,2014,45(11):1-10.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501640078&amp;v=MjUxMjlSZEdlcnFRVE1ud1plWnVIeWptVWIvSUpsOFJhUnM9TmlmT2ZiSzdIdEROcW85RVl1OFBESHN4b0JNVDZUNFBRSC9pcg==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b> Zhang W,Yoshida T,Tang X.A comparative study of TF*IDF,LSI and multi-words for text classification[J].Expert Systems with Applications,2011,38(3):2758-2765.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_10" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501718943&amp;v=MTkyMTdycVFUTW53WmVadUh5am1VYi9JSmw4UmFScz1OaWZPZmJLN0h0RE5xbzlFWStvSEJYZzZvQk1UNlQ0UFFIL2lyUmRHZQ==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[10]</b> Uysal A K,Gunal S.A novel probabilistic feature selection method for text classification[J].Knowledge-Based Systems,2012,36:226-235.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Turning from TF-IDF to TFIGM for term weighting in text classification">

                                <b>[11]</b> Chen K W,Zhang Z P,Long J.Turning from TF-IDF to TF-IGM for term weighting in text classification[J].Expert Systems with Application,2016,18:245-260.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Large scale text classification using semi-supervised multinomial Naive Bayes">

                                <b>[12]</b> Su J,Shirab J S,Matwin S.Large scale text classification using semi-supervised Multinomial Naive Bayes[C]//Proceedings of the 28th International Conference on Machine Learning,ICML 2011.2011:97-104.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_13" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESEDB4ABE996030A126419B0AD2354D01A&amp;v=MDE2ODB6aFFWN2o1ME9uK1QyQkEyZkxiZ1JidnVDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZDcGJRMzVkaGh3cm02dzZFPU5pZk9mY2JNYk5XOTNmcE1iZTBQRDN4SQ==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[13]</b> Wang D,Zhang H,Liu R,et al.Unsupervised feature selection through Gram-Schmidt orthogonalization:A word co-occurrence perspective[J].Neurocomputing,2016,173(3):845-854.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_14" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESFCD8800628D9CFDC3067D4915A35831A&amp;v=MjYxNDZON0JROVB1MlVRNmpsNlBIdnJyUmRFZXJlY1JydnVDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZDcGJRMzVkaGh3cm02dzZFPU5pZk9mY1hMYXRuRXI0OURadQ==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[14]</b> Rehman A,Javed K,Babri H A.Feature selection based on a normalized difference measure for text classification[J].Information Processing &amp; Management,2017,53(2):473-489.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_15" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES9FE44B2BA2F43AD894EED9FE4A873B18&amp;v=Mjk0ODZ1WENPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkNwYlEzNWRoaHdybTZ3NkU9TmlmT2Zick9hOVhJM1kwM0ZlbDVDSDlJdXg0YTdrb0lQSGFVMlJaRWNiV1hONw==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[15]</b> Zong W,Wu F,Chu L K,et al.A discriminative and semantic feature selection method for text categorization[J].International Journal of Production Economics,2015,165:215-222.
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_16" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501719222&amp;v=MjUzMzVHZXJxUVRNbndaZVp1SHlqbVViL0lKbDhSYVJzPU5pZk9mYks3SHRETnFvOUVZK29HRG40N29CTVQ2VDRQUUgvaXJSZA==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[16]</b> Uguz H.A two-stage feature selection method for text categorization by using information gain,principal component analysis and genetic algorithm[J].Knowledge-Based Systems,2011,24(7):1024-1032.
                            </a>
                        </p>
                        <p id="35">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Study on mutual information-based feature selection for text categorization">

                                <b>[17]</b> Xu Y,Jones G J,Li J,et al.A study on mutual information-based feature selection for text categorization[J].Journal of Computational Information Systems,2007,3(3):1007-1012.
                            </a>
                        </p>
                        <p id="37">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A comparative study on feature selection in text categorization">

                                <b>[18]</b> Yang Y,Pedersen J O.A comparative study on feature selection in text categorization[C]//Proceedings of the fourteenth international conference on machine learning,1997:412-420.
                            </a>
                        </p>
                        <p id="39">
                            <a id="bibliography_19" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012300451192&amp;v=MTY0NTFSYVJzPU5pZk9mYks3SHRET3JJOUZZTzRPRFhVN29CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVWIvSUpsOA==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[19]</b> Lehmann N.Principal components selection given extensively many variables[J].Statistics &amp; Probability Letters,2005,74(1):51-58.
                            </a>
                        </p>
                        <p id="41">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A New Speculative Execution Algorithm Based on C4.5 Decision Tree for Hadoop">

                                <b>[20]</b> Li Y,Yang Q,Lai S,et al.A new speculative execution algorithm based on C4.5 decision tree for hadoop[C]// Intelligent Computation in Big Data Era(ICYCSEE),2015:284-291.
                            </a>
                        </p>
                        <p id="43">
                            <a id="bibliography_21" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011600738114&amp;v=MDM2MDdPZmJLN0h0RE5xWTlGWStnSERYMDlvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp1SHlqbVViL0lKbDhSYVJzPU5pZg==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[21]</b> Mohammed A A,Minhas R,Wu Q M J,et al.Human face recognition based on multidimensional PCA and extreme learning machine[J].Pattern Recognition,2011,44(10-11):2588-2597.
                            </a>
                        </p>
                        <p id="45">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Two new feature extraction methods for text classification:TESDF and SADF">

                                <b>[22]</b> Kilic E,Ates N,Karakaya A,et al.Two new feature extraction methods for text classification:TESDF and SADF[C]//Signal Processing &amp; Communications Applications Conference.IEEE,2015:475-478.
                            </a>
                        </p>
                        <p id="47">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=20Newsgroups">

                                <b>[23]</b> Jrennie.20 Newsgroups[OL].2008.http://people.csail.mit.edu/jrennie/20Newsgroups.
                            </a>
                        </p>
                        <p id="49">
                            <a id="bibliography_24" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJWD&amp;filename=SJWD130305000833&amp;v=Mjg3NjRtRGQ5U0g3bjN4RTlmYnZuS3JpZlp1OXVGQ3JsVTduSkpWMGROaWZjYXJLN0h0TE1xbzlGWk9NTUR4TTh6eFVT&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[24]</b> Li B Y,Wang Q W,Hu J L.Multi-SVM classifier system with piecewise interpolation[J].IEEJ Transaction on Electricaland Electronic Engineering,2013,8(2):132-138.
                            </a>
                        </p>
                        <p id="51">
                            <a id="bibliography_25" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201708051&amp;v=MTIxNTdCdEdGckNVUkxPZVplVnVGeTdrVnJ6Qkx6VFpaTEc0SDliTXA0OUFaWVFLREg4NHZSNFQ2ajU0TzN6cXE=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[25]</b> 蔡永泉,王玉栋.以特征值关联项改进贝叶斯分类器正确率[J].计算机应用与软件,2017,34(8):286-290,311.
                            </a>
                        </p>
                        <p id="53">
                            <a id="bibliography_26" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201706041&amp;v=MDk0MDhaZVZ1Rnk3a1ZyekJMelRaWkxHNEg5Yk1xWTlCWllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2U=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[26]</b> 吴国文,庄千料.一种改进的增量式贝叶斯文本分类算法[J].计算机应用与软件,2017,34(6):226-229,249.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JYRJ201910006" />
        <input id="dpi" type="hidden" value="600" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201910006&amp;v=MDk2MzdCdEdGckNVUkxPZVplVnVGeTdrVnJ6T0x6VFpaTEc0SDlqTnI0OUZZb1FLREg4NHZSNFQ2ajU0TzN6cXE=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="0" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
