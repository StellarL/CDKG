<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637134076692131250%26DBCODE%3dCJFD%26TABLEName%3dCJFDTEMP%26FileName%3dJYRJ201911010%26RESULT%3d1%26SIGN%3d18ykqa3g5PW3wqPAP5yWwOfZd%252bw%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JYRJ201911010&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JYRJ201911010&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201911010&amp;v=MjA0ODI4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnVGeS9nVmJ2TEx6VFpaTEc0SDlqTnJvOUVaSVFLREg=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#43" data-title="&lt;b&gt;0 引 言&lt;/b&gt; "><b>0 引 言</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#47" data-title="&lt;b&gt;1 随机蕨算法&lt;/b&gt; "><b>1 随机蕨算法</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#48" data-title="&lt;b&gt;1.1 基本随机蕨算法&lt;/b&gt;"><b>1.1 基本随机蕨算法</b></a></li>
                                                <li><a href="#68" data-title="&lt;b&gt;1.2 改进的随机蕨算法&lt;/b&gt;"><b>1.2 改进的随机蕨算法</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#91" data-title="&lt;b&gt;2 跟踪注册算法设计&lt;/b&gt; "><b>2 跟踪注册算法设计</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#94" data-title="&lt;b&gt;2.1 离线训练&lt;/b&gt;"><b>2.1 离线训练</b></a></li>
                                                <li><a href="#98" data-title="&lt;b&gt;2.2 在线检测匹配&lt;/b&gt;"><b>2.2 在线检测匹配</b></a></li>
                                                <li><a href="#104" data-title="&lt;b&gt;2.3 三维位姿估计&lt;/b&gt;"><b>2.3 三维位姿估计</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#122" data-title="&lt;b&gt;3 实 验&lt;/b&gt; "><b>3 实 验</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#131" data-title="&lt;b&gt;4 结 语&lt;/b&gt; "><b>4 结 语</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#50" data-title="图1 随机蕨结构">图1 随机蕨结构</a></li>
                                                <li><a href="#72" data-title="&lt;b&gt;表1 不同特征检测算子检测速度对比&lt;/b&gt;"><b>表1 不同特征检测算子检测速度对比</b></a></li>
                                                <li><a href="#93" data-title="图2 基于改进随机蕨的增强现实跟踪注册算法流程图">图2 基于改进随机蕨的增强现实跟踪注册算法流程图</a></li>
                                                <li><a href="#102" data-title="图3 匹配点数随随机蕨数目递增的变化趋势">图3 匹配点数随随机蕨数目递增的变化趋势</a></li>
                                                <li><a href="#103" data-title="图4 单点匹配时间随随机蕨数目递增的变化趋势">图4 单点匹配时间随随机蕨数目递增的变化趋势</a></li>
                                                <li><a href="#108" data-title="图5 检测目标投影和摄像机运动关系">图5 检测目标投影和摄像机运动关系</a></li>
                                                <li><a href="#125" data-title="图6 基于随机蕨宽基线匹配检测效果">图6 基于随机蕨宽基线匹配检测效果</a></li>
                                                <li><a href="#127" data-title="&lt;b&gt;表2 不同方法的增强现实平均跟踪注册单帧耗时&lt;/b&gt;"><b>表2 不同方法的增强现实平均跟踪注册单帧耗时</b></a></li>
                                                <li><a href="#167" data-title="图7 基于改进随机蕨的无标识增强现实跟踪注册效果">图7 基于改进随机蕨的无标识增强现实跟踪注册效果</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="168">


                                    <a id="bibliography_1" title=" Lima J P,Roberto R,Sim&#245;es F,et al.Markerless tracking system for augmented reality in the automotive industry[J].Expert Systems with Applications,2017,82:100-114." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES47078A393208075BB106A193B558274B&amp;v=MTQ2NzNyN3RDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZDcGJRMzVkaGh3NzI1eEtzPU5pZk9mYmUvSHRiRTNveE1aK2tQQkh3K3ltUmg2ejk3T1g3cnIyQXdmTHFXUQ==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         Lima J P,Roberto R,Sim&#245;es F,et al.Markerless tracking system for augmented reality in the automotive industry[J].Expert Systems with Applications,2017,82:100-114.
                                    </a>
                                </li>
                                <li id="170">


                                    <a id="bibliography_2" title=" Guan T,Duan L,Yu J,et al.Real-time camera pose estimation for wide-area augmented reality applications[J].IEEE Computer Graphics and Applications,2011,31(3):56-68." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Real-Time Camera Pose Estimation for Wide-Area Augmented Reality Applications">
                                        <b>[2]</b>
                                         Guan T,Duan L,Yu J,et al.Real-time camera pose estimation for wide-area augmented reality applications[J].IEEE Computer Graphics and Applications,2011,31(3):56-68.
                                    </a>
                                </li>
                                <li id="172">


                                    <a id="bibliography_3" title=" Vista F IV,Lee D J,Chong K T.Remote activation and confidence factor setting of ARToolKit with data association for tracking multiple markers[J].International Journal of Control and Automation,2013,6(6):243-252." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Remote Activation and Confidence Factor Setting of ARToolKit with Data Association for Tracking Multiple Markers">
                                        <b>[3]</b>
                                         Vista F IV,Lee D J,Chong K T.Remote activation and confidence factor setting of ARToolKit with data association for tracking multiple markers[J].International Journal of Control and Automation,2013,6(6):243-252.
                                    </a>
                                </li>
                                <li id="174">


                                    <a id="bibliography_4" title=" Fiala M.ARTag:An improved marker system based on AR toolkit[J].National Research Council Publication,2004,4(7):166-174." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=ARTag:an improved marker system based on ARToolkit">
                                        <b>[4]</b>
                                         Fiala M.ARTag:An improved marker system based on AR toolkit[J].National Research Council Publication,2004,4(7):166-174.
                                    </a>
                                </li>
                                <li id="176">


                                    <a id="bibliography_5" title=" Du Y,Miao Z,Cen Y.Markless augmented reality registration algorithm based on ORB[C]//International Conference on Signal Processing.IEEE,2015:1236-1240." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Markless augmented reality registration algorithm based on ORB">
                                        <b>[5]</b>
                                         Du Y,Miao Z,Cen Y.Markless augmented reality registration algorithm based on ORB[C]//International Conference on Signal Processing.IEEE,2015:1236-1240.
                                    </a>
                                </li>
                                <li id="178">


                                    <a id="bibliography_6" title=" Shi J B,Tomasi C.Good features to track[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition,1994:593-600." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Good features to track">
                                        <b>[6]</b>
                                         Shi J B,Tomasi C.Good features to track[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition,1994:593-600.
                                    </a>
                                </li>
                                <li id="180">


                                    <a id="bibliography_7" title=" Li J,Laganiere R,Roth G.Online estimation of trifocal tensor for augmented live video[C]//Proceedings of the IEEE and ACM International Symposium on Mixed and Augmented Reality,2004:182-190." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Online estimation of trifocal tensor for augmented live video">
                                        <b>[7]</b>
                                         Li J,Laganiere R,Roth G.Online estimation of trifocal tensor for augmented live video[C]//Proceedings of the IEEE and ACM International Symposium on Mixed and Augmented Reality,2004:182-190.
                                    </a>
                                </li>
                                <li id="182">


                                    <a id="bibliography_8" title=" Yuan M L,Ong S K,Nee A Y.Registration using natural features for augmented reality systems[J].IEEE Transactions on Visualization &amp;amp; Computer Graphics,2006,12(4):569-580." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Registration Using Natural Features for Augmented Reality Systems">
                                        <b>[8]</b>
                                         Yuan M L,Ong S K,Nee A Y.Registration using natural features for augmented reality systems[J].IEEE Transactions on Visualization &amp;amp; Computer Graphics,2006,12(4):569-580.
                                    </a>
                                </li>
                                <li id="184">


                                    <a id="bibliography_9" title=" 康波,任鹏.一种基于自然纹理特征的增强现实跟踪算法[J].系统工程与电子技术,2009,31(10):2480-2484." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XTYD200910043&amp;v=MDc0NTZaNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnVGeS9nVmJ2TFBUblNhckc0SHRqTnI0OUI=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                         康波,任鹏.一种基于自然纹理特征的增强现实跟踪算法[J].系统工程与电子技术,2009,31(10):2480-2484.
                                    </a>
                                </li>
                                <li id="186">


                                    <a id="bibliography_10" title=" 肖慧,陆奎.基于SIFT自然特征的AR系统研究和实现[J].计算机应用与软件,2014,31(5):244-246,263." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201405063&amp;v=MDkzMjg2ajU0TzN6cXFCdEdGckNVUkxPZVplVnVGeS9nVmJ2TEx6VFpaTEc0SDlYTXFvOURaNFFLREg4NHZSNFQ=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[10]</b>
                                         肖慧,陆奎.基于SIFT自然特征的AR系统研究和实现[J].计算机应用与软件,2014,31(5):244-246,263.
                                    </a>
                                </li>
                                <li id="188">


                                    <a id="bibliography_11" title=" Lowe D G.Distinctive image features from scale-invariant key-points[J].International Journal of Computer Vision,2004,20(2):91-110." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002830901&amp;v=MDAxMjJyT0pGMD1OajdCYXJPNEh0SE9wNHhGYmVzT1kzazV6QmRoNGo5OVNYcVJyeG94Y01IN1I3cWVidWR0RlNqbFZM&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[11]</b>
                                         Lowe D G.Distinctive image features from scale-invariant key-points[J].International Journal of Computer Vision,2004,20(2):91-110.
                                    </a>
                                </li>
                                <li id="190">


                                    <a id="bibliography_12" title=" Ozuysal M,Calonder M,Lepeti V.Fast keypoint recognition using random ferns[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2010,32(3):448-461." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fast Keypoint Recognition Using Random Ferns">
                                        <b>[12]</b>
                                         Ozuysal M,Calonder M,Lepeti V.Fast keypoint recognition using random ferns[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2010,32(3):448-461.
                                    </a>
                                </li>
                                <li id="192">


                                    <a id="bibliography_13" title=" Lepetit V,Pilet J,Fua P.Point matching as a classification problem for fast and robust object pose estimation[C]//Proceedings of the 2004 IEEE computer society conference on Computer vision and pattern recognition.IEEE,2004:244-250." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Point matching as a classification problem for fast and robust object pose estimation">
                                        <b>[13]</b>
                                         Lepetit V,Pilet J,Fua P.Point matching as a classification problem for fast and robust object pose estimation[C]//Proceedings of the 2004 IEEE computer society conference on Computer vision and pattern recognition.IEEE,2004:244-250.
                                    </a>
                                </li>
                                <li id="194">


                                    <a id="bibliography_14" title=" Li D,Shi R,Zhao X,et al.Research on augmented reality of printed matter based on random ferns and orientation gradient with integral graph[C]//China Academic Conference on Printing &amp;amp; Packaging and Media Technology.Springer,Singapore,2016:257-264." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Research on augmented reality of printed matter based on random ferns and orientation gradient with integral graph">
                                        <b>[14]</b>
                                         Li D,Shi R,Zhao X,et al.Research on augmented reality of printed matter based on random ferns and orientation gradient with integral graph[C]//China Academic Conference on Printing &amp;amp; Packaging and Media Technology.Springer,Singapore,2016:257-264.
                                    </a>
                                </li>
                                <li id="196">


                                    <a id="bibliography_15" title=" Vergel M V,Andradecetto J,Sanfeliu A,et al.Boosted random ferns for object detection[J].IEEE Transactions on Pattern Analysis &amp;amp; Machine Intelligence,2018,40(2):272-288." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Boosted Random Ferns for Object Detection &amp;quot;">
                                        <b>[15]</b>
                                         Vergel M V,Andradecetto J,Sanfeliu A,et al.Boosted random ferns for object detection[J].IEEE Transactions on Pattern Analysis &amp;amp; Machine Intelligence,2018,40(2):272-288.
                                    </a>
                                </li>
                                <li id="198">


                                    <a id="bibliography_16" title=" 杨晨晖,刘守达,王子明,等.基于随机蕨的实时车辆匹配[J].厦门大学学报(自然版),2014,53(2):206-211." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XDZK201402014&amp;v=MTQzMDFIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ1RnkvZ1ZidkxQU25SWmJHNEg5WE1yWTlFWUlRS0Q=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[16]</b>
                                         杨晨晖,刘守达,王子明,等.基于随机蕨的实时车辆匹配[J].厦门大学学报(自然版),2014,53(2):206-211.
                                    </a>
                                </li>
                                <li id="200">


                                    <a id="bibliography_17" title=" Rosten E,Drummond T.Machine learning for high-speed corner detection[C]//European Conference on Computer Vision.Berlin,Germany:Springer-Verlag,2006:430-443." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Machine learning for high speed corner detection">
                                        <b>[17]</b>
                                         Rosten E,Drummond T.Machine learning for high-speed corner detection[C]//European Conference on Computer Vision.Berlin,Germany:Springer-Verlag,2006:430-443.
                                    </a>
                                </li>
                                <li id="202">


                                    <a id="bibliography_18" title=" 罗元,付瑜,张毅.基于FAST角点和仿射改进的随机蕨丛的单目视觉实时匹配算法[J].机器人,2014(3):271-278." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JQRR201403003&amp;v=MDI3NzlLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnVGeS9nVmJ2TEx6elpmTEc0SDlYTXJJOUZaNFE=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[18]</b>
                                         罗元,付瑜,张毅.基于FAST角点和仿射改进的随机蕨丛的单目视觉实时匹配算法[J].机器人,2014(3):271-278.
                                    </a>
                                </li>
                                <li id="204">


                                    <a id="bibliography_19" title=" 林倞,王涌天,刘越,等.基于模板跟踪的实时无标志点注册算法[J].中国图象图形学报,2008,13(9):1812-1819." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZGTB200809028&amp;v=MjEzNDFyQ1VSTE9lWmVWdUZ5L2dWYnZMUHlyZmJMRzRIdG5NcG85SGJJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0Y=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[19]</b>
                                         林倞,王涌天,刘越,等.基于模板跟踪的实时无标志点注册算法[J].中国图象图形学报,2008,13(9):1812-1819.
                                    </a>
                                </li>
                                <li id="206">


                                    <a id="bibliography_20" title=" Labrie M,Hebert P.Efficient camera motion and 3D recovery using an inertial sensor[C]//Proceedings of the Fourth Canadian Conference on Computer and Robot Vision.IEEE,2007:55-62." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Efficient camera motion and 3D recovery using an inertial sensor">
                                        <b>[20]</b>
                                         Labrie M,Hebert P.Efficient camera motion and 3D recovery using an inertial sensor[C]//Proceedings of the Fourth Canadian Conference on Computer and Robot Vision.IEEE,2007:55-62.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JYRJ" target="_blank">计算机应用与软件</a>
                2019,36(11),51-56+111 DOI:10.3969/j.issn.1000-386x.2019.11.009            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于改进随机蕨的跟踪注册方法研究</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9B%B9%E9%B9%8F%E9%9C%9E&amp;code=43235591&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">曹鹏霞</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%8E%E6%96%87%E6%96%B0&amp;code=35379069&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">李文新</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%A9%AC%E4%BC%9F%E8%8B%B9&amp;code=43235592&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">马伟苹</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E7%A9%BA%E9%97%B4%E6%8A%80%E6%9C%AF%E7%A0%94%E7%A9%B6%E9%99%A2%E5%85%B0%E5%B7%9E%E7%A9%BA%E9%97%B4%E6%8A%80%E6%9C%AF%E7%89%A9%E7%90%86%E7%A0%94%E7%A9%B6%E6%89%80&amp;code=0107803&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国空间技术研究院兰州空间技术物理研究所</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>针对无标识增强现实跟踪注册方法在复杂环境下存在跟踪注册失败、速度较慢的问题,提出一种基于改进随机蕨的无标识增强现实跟踪注册方法。该方法以真实场景中的目标物体图像作为模板,使用随机蕨分类器进行目标检测,解决环境光照变化或目标被遮挡跟踪注册失败的问题。使用FAST角点进行特征检测提升检测速度,对随机蕨的仿射过程进行改进来弥补特征点尺度不变性和仿射不变性缺失。通过该分类器进行特征匹配,进而估计三维位姿并渲染注册虚拟物体。实验结果证明,该方法具有较好的准确性、实时性和鲁棒性。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%97%A0%E6%A0%87%E8%AF%86&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">无标识;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%A2%9E%E5%BC%BA%E7%8E%B0%E5%AE%9E&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">增强现实;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%B7%9F%E8%B8%AA%E6%B3%A8%E5%86%8C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">跟踪注册;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%9A%8F%E6%9C%BA%E8%95%A8&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">随机蕨;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=FAST%E8%A7%92%E7%82%B9&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">FAST角点;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    曹鹏霞，博士生，主研领域:空间电子技术，增强现实。;
                                </span>
                                <span>
                                    李文新，研究员。;
                                </span>
                                <span>
                                    马伟苹，博士生。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-02-20</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金项目(61125101);</span>
                                <span>中国载人航天工程重大专项(RWZY640601);</span>
                    </p>
            </div>
                    <h1><b>TRACKING REGISTRATION METHOD BASED ON IMPROVED RANDOM FERNS</b></h1>
                    <h2>
                    <span>Cao Pengxia</span>
                    <span>Li Wenxin</span>
                    <span>Ma Weiping</span>
            </h2>
                    <h2>
                    <span>Lanzhou Institute of Physics, China Academy of Space Technology</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Markerless augmented reality tracking registration method fails in tracking registration and has slow speed in complex environment. To solve these problems, we propose a markerless tracking registration algorithm for augmented reality based on improved random ferns. The method used the target object image in the real scene as a template, and applied the random ferns classifier to perform target detection so as to solve the problem of tracking registration failure due to environmental illumination change and target occlusion. This paper employed FAST corner points for feature detection to improve detection speed, and the affine process of random ferns was improved to compensate for the loss of scale invariance and affine invariance of feature points. Feature matching was performed by the classifier, the three-dimensional pose was estimated and the virtual object was rendered and registered. Experimental results demonstrate that the algorithm has good accuracy, real-time performance and robustness.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Markerless&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Markerless;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Augmented%20reality&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Augmented reality;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Tracking%20registration&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Tracking registration;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Random%20fern&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Random fern;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=FAST%20corner%20points&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">FAST corner points;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2019-02-20</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="43" name="43" class="anchor-tag"><b>0 引 言</b></h3>
                <div class="p1">
                    <p id="44">增强现实<citation id="208" type="reference"><link href="168" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>(Augmented Reality, AR)是一种将计算机生成的3D模型、文本、图像、视频等虚拟的信息实时叠加到真实场景中并融合显示的技术,它在医疗卫生、军事领域、工业维修、娱乐游戏等领域具有广泛的应用。对增强现实系统进行研究需要涉及多项关键技术,如显示技术、跟踪注册技术、场景融合和相机标定(Calibration)技术等。如何能够快速、精确地计算出摄像机相对于真实场景的位姿信息,并以位姿信息为基础将虚拟信息与真实场景精准对齐的技术,即跟踪注册技术。跟踪注册技术是开发增强现实系统的核心和难点,已经成为增强现实走向更广泛应用亟需解决的一个关键问题<citation id="209" type="reference"><link href="170" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>。目前,基于增强现实的跟踪注册问题主要分为基于特殊标识的跟踪注册方法和无标识点的跟踪注册方法两大类。基于特殊标识的跟踪注册方法最具代表性的是ARToolKit<citation id="210" type="reference"><link href="172" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>和ARTag<citation id="211" type="reference"><link href="174" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>。这类方法具有计算量小、执行速度快、不需要复杂的硬件设备等优势。但是基于特殊标识的跟踪注册方法需要在真实场景中预先放置人工标识物,通过提取标识物的特征获得跟踪注册所需要的位姿信息从而实现跟踪注册。然而在真实场景中预先放置特殊的人工标识物,使用特殊标识物进行跟踪注册的方法无法解决环境光照变化和标志物被遮挡的问题,具有鲁棒性差的缺点。同时,特殊标识物在真实场景中也带来了视觉污染问题。在这种情况下,必须使用基于无标识的跟踪注册方法来解决增强现实中虚实场景的配准问题<citation id="212" type="reference"><link href="176" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>。因此,基于无标识的跟踪注册方法是目前发展的方向。</p>
                </div>
                <div class="p1">
                    <p id="45">无标识跟踪注册主要是根据已知场景参考模板与当前帧的自然特征对摄像机的姿势进行估计。主要有基于模板特征跟踪和基于模板图像匹配两种方法。基于模板特征跟踪的跟踪注册方法中Tomasi<citation id="213" type="reference"><link href="178" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>等提出的KLT(Kanade-Lucas-Tomasi)跟踪算法因其具有实时性优点而应用广泛。文献<citation id="218" type="reference">[<a class="sup">7</a>,<a class="sup">8</a>]</citation>成功地将KLT跟踪算法应用于无标识增强现的实跟踪注册过程。但是,KLT跟踪算法具有受光照条件影响较大的缺点,并且当目标变换较快或出现较大遮挡时,存在跟踪失败的问题,这使得其应用场景受限。基于模板图像匹配的跟踪注册方法主要解决宽基线匹配的问题<citation id="214" type="reference"><link href="184" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>。传统的宽基线匹配算法中,局部特征描述子(Scale-invariant feature transform,SIFT)因具有很强的稳健性而广泛应用于模式识别和图像匹配等领域<citation id="215" type="reference"><link href="186" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>。2004年,文献<citation id="216" type="reference">[<a class="sup">11</a>]</citation>第一次将SIFT算子应用于AR系统的跟踪注册。但是传统的宽基线匹配算法都具有计算复杂的特点,很难满足增强现实系统在实时性方面的要求。针对此问题,Lepetit<citation id="217" type="reference"><link href="190" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>等将宽基线图像匹配视为分类问题,构造了一种基于朴素贝叶斯的随机蕨(Random ferns)分类器,将运算量较大的部分放到分类器的训练过程离线完成,进而提高算法的实时性。</p>
                </div>
                <div class="p1">
                    <p id="46">将基于随机蕨分类器的宽基线匹配应用于AR跟踪注册虽然能够解决条件差、目标被部分遮挡时跟踪注册失败的问题,同时与传统自然特征匹配算法相比提高了算法的实时性。但是单纯的原始随机蕨算法仍然不能满足AR系统实时性的要求。因此,本文提出了一种改进随机蕨的无标识增强现实跟踪注册算法。使用具有快速特点的FAST角点进行特征检测来进一步提高算法的实时性。针对FAST 角点特征检测不具备尺度不变性和仿射不变性的问题,对随机蕨丛的仿射过程进行改进,确保算法的鲁棒性。</p>
                </div>
                <h3 id="47" name="47" class="anchor-tag"><b>1 随机蕨算法</b></h3>
                <h4 class="anchor-tag" id="48" name="48"><b>1.1 基本随机蕨算法</b></h4>
                <div class="p1">
                    <p id="49">随机蕨算法最早是由Lepetit等<citation id="219" type="reference"><link href="192" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>提出的,是随机森林算法的一种简化形式,且具有比随机森林算法更好的性能。它将随机森林的层次结构特征通过将每层结点选取相同的决策而改为非层次结构特征,从而将树结构转化为结构相对单一的蕨结构,如图1所示。</p>
                </div>
                <div class="area_img" id="50">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201911010_050.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 随机蕨结构" src="Detail/GetImg?filename=images/JYRJ201911010_050.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 随机蕨结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201911010_050.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="51">基于朴素贝叶斯的随机蕨分类器的基本思想与基于随机森林的特征匹配类似<citation id="220" type="reference"><link href="194" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>,把模板图像描述为<i>H</i>个特征点,建立特征点数据集<i>K</i>={<i>k</i><sub>1</sub>,<i>k</i><sub>2</sub>,…,<i>k</i><sub><i>H</i></sub>},把特征点<i>k</i><sub><i>i</i></sub>,<i>i</i>=1,2,…,<i>H</i>及以其为中心的图像块<i>p</i>(<i>k</i><sub><i>i</i></sub>)可能出现的形式作为一类<i>c</i><sub><i>i</i></sub>,<i>i</i>=1,2,…,<i>H</i>。在线检测匹配阶段,针对输入图像检测到的特征点<i>k</i><sup>input</sup>及以其对应的图像块<i>p</i>(<i>k</i><sup>input</sup>),求其最大可能归属于<i>H</i>个特征点中的一个类<i>c</i><sub><i>i</i></sub>。</p>
                </div>
                <div class="p1">
                    <p id="52">设<i>f</i><sub><i>j</i></sub>,<i>j</i>=1,2,…,<i>N</i>是输入图像特征点图像块<i>p</i>(<i>k</i><sup>input</sup>)的二元特征集,图像块<i>p</i>(<i>k</i><sup>input</sup>)的大小为<i>L</i>×<i>L</i>(一般取<i>L</i>=32),<i>f</i><sub><i>j</i></sub>的值取决于图像块<i>p</i>(<i>k</i><sup>input</sup>)在分类器训练阶段随机生成的两个像素点位置<i>d</i><sub><i>j</i></sub><sub>1</sub>和<i>d</i><sub><i>j</i></sub><sub>2</sub>的灰度值<i>I</i><sub><i>d</i></sub><sub></sub><sub><i>j</i></sub><sub>1  </sub>和<i>I</i><sub><i>d</i></sub><sub></sub><sub><i>j</i></sub><sub>2  </sub>的大小<citation id="221" type="reference"><link href="196" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>,即:</p>
                </div>
                <div class="area_img" id="53">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JYRJ201911010_05300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="55">将输入特征点图像块<i>p</i>(<i>k</i><sup>input</sup>)所属的类别定义为:</p>
                </div>
                <div class="p1">
                    <p id="56"><mathml id="133"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>c</mi><mo>^</mo></mover><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mtext>a</mtext><mtext>r</mtext><mtext>g</mtext><mtext>m</mtext><mtext>a</mtext><mtext>x</mtext></mrow></mstyle><mrow><mi>c</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></munder><mi>Ρ</mi><mo stretchy="false">(</mo><mi>C</mi><mo>=</mo><mi>c</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">|</mo><mi>f</mi><msub><mrow></mrow><mn>1</mn></msub><mo>,</mo><mi>f</mi><msub><mrow></mrow><mn>2</mn></msub><mo>,</mo><mo>⋯</mo><mo>,</mo><mi>f</mi><msub><mrow></mrow><mi>Ν</mi></msub><mo stretchy="false">)</mo></mrow></math></mathml>      (2)</p>
                </div>
                <div class="p1">
                    <p id="57">式中:C表示类别,由贝叶斯公式可得:</p>
                </div>
                <div class="p1">
                    <p id="58" class="code-formula">
                        <mathml id="58"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>Ρ</mi><mo stretchy="false">(</mo><mi>C</mi><mo>=</mo><mi>c</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">|</mo><mi>f</mi><msub><mrow></mrow><mn>1</mn></msub><mo>,</mo><mi>f</mi><msub><mrow></mrow><mn>2</mn></msub><mo>,</mo><mo>⋯</mo><mo>,</mo><mi>f</mi><msub><mrow></mrow><mi>Ν</mi></msub><mo stretchy="false">)</mo><mo>=</mo></mtd></mtr><mtr><mtd><mfrac><mrow><mi>Ρ</mi><mo stretchy="false">(</mo><mi>f</mi><msub><mrow></mrow><mn>1</mn></msub><mo>,</mo><mi>f</mi><msub><mrow></mrow><mn>2</mn></msub><mo>,</mo><mo>⋯</mo><mo>,</mo><mi>f</mi><msub><mrow></mrow><mi>Ν</mi></msub><mo stretchy="false">|</mo><mi>C</mi><mo>=</mo><mi>c</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mi>Ρ</mi><mo stretchy="false">(</mo><mi>C</mi><mo>=</mo><mi>c</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><mrow><mi>Ρ</mi><mo stretchy="false">(</mo><mi>f</mi><msub><mrow></mrow><mn>1</mn></msub><mo>,</mo><mi>f</mi><msub><mrow></mrow><mn>2</mn></msub><mo>,</mo><mo>⋯</mo><mo>,</mo><mi>f</mi><msub><mrow></mrow><mi>Ν</mi></msub><mo stretchy="false">)</mo></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>3</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="59">假设先验概率<i>P</i>(<i>C</i>)为均匀分布,式(3)分母部分与类别无关,因而式(2)可以转化为:</p>
                </div>
                <div class="p1">
                    <p id="60"><mathml id="134"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>c</mi><mo>^</mo></mover><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mtext>a</mtext><mtext>r</mtext><mtext>g</mtext><mtext>m</mtext><mtext>a</mtext><mtext>x</mtext></mrow></mstyle><mrow><mi>c</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></munder><mi>Ρ</mi><mo stretchy="false">(</mo><mi>f</mi><msub><mrow></mrow><mn>1</mn></msub><mo>,</mo><mi>f</mi><msub><mrow></mrow><mn>2</mn></msub><mo>,</mo><mo>⋯</mo><mo>,</mo><mi>f</mi><msub><mrow></mrow><mi>Ν</mi></msub><mo stretchy="false">|</mo><mi>C</mi><mo>=</mo><mi>c</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></math></mathml>      (4)</p>
                </div>
                <div class="p1">
                    <p id="61">由f<sub>j</sub>之间的独立性可得:</p>
                </div>
                <div class="p1">
                    <p id="62"><mathml id="135"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><mo stretchy="false">(</mo><mi>f</mi><msub><mrow></mrow><mn>1</mn></msub><mo>,</mo><mi>f</mi><msub><mrow></mrow><mn>2</mn></msub><mo>,</mo><mo>⋯</mo><mo>,</mo><mi>f</mi><msub><mrow></mrow><mi>Ν</mi></msub><mo stretchy="false">|</mo><mi>C</mi><mo>=</mo><mi>c</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∏</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mi>Ρ</mi></mstyle><mo stretchy="false">(</mo><mi>f</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">|</mo><mi>C</mi><mo>=</mo><mi>c</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></math></mathml>      (5)</p>
                </div>
                <div class="p1">
                    <p id="63">为了降低式(5)的存储量并保证<i>f</i><sub><i>j</i></sub>之间的相关性,随机蕨采用半朴素贝叶斯分类器<citation id="222" type="reference"><link href="198" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>。将特征<i>f</i><sub><i>j</i></sub>分为<i>M</i>组,即<i>M</i>个“蕨”,每组特征个数为<i>S</i>=<i>N</i>/<i>M</i>,即<i>S</i>个节点。在半朴素贝叶斯分类器条件下,我们认为不同蕨之间是相互独立的,同一蕨内节点之间具有相关性,因此式(5)可以简化为:</p>
                </div>
                <div class="p1">
                    <p id="64" class="code-formula">
                        <mathml id="64"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><mo stretchy="false">(</mo><mi>f</mi><msub><mrow></mrow><mn>1</mn></msub><mo>,</mo><mi>f</mi><msub><mrow></mrow><mn>2</mn></msub><mo>,</mo><mo>⋯</mo><mo>,</mo><mi>f</mi><msub><mrow></mrow><mi>Ν</mi></msub><mo stretchy="false">|</mo><mi>C</mi><mo>=</mo><mi>c</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∏</mo><mrow><mi>m</mi><mo>=</mo><mn>1</mn></mrow><mi>Μ</mi></munderover><mi>Ρ</mi></mstyle><mo stretchy="false">(</mo><mi>F</mi><msub><mrow></mrow><mi>m</mi></msub><mo stretchy="false">|</mo><mi>C</mi><mo>=</mo><mi>c</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>6</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="65">式中:<i>F</i><sub><i>m</i></sub>=[<i>f</i><sub><i>σ</i></sub><sub>(</sub><sub><i>m</i></sub><sub>,1)</sub>,<i>f</i><sub><i>σ</i></sub><sub>(</sub><sub><i>m</i></sub><sub>,2)</sub>,…,<i>f</i><sub><i>σ</i></sub><sub>(</sub><sub><i>m</i></sub><sub>,</sub><sub><i>S</i></sub><sub>)</sub>],<i>m</i>=1,2,…,<i>M</i>,表示第<i>m</i>个蕨;<i>f</i><sub><i>σ</i></sub><sub>(</sub><sub><i>m</i></sub><sub>,</sub><sub><i>j</i></sub><sub>)</sub>表示范围为1,2,…,<i>N</i>的随机数。由此可知<i>p</i>(<i>k</i><sup>input</sup>)的所属类别为:</p>
                </div>
                <div class="p1">
                    <p id="66"><mathml id="136"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>c</mi><mo>^</mo></mover><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mtext>a</mtext><mtext>r</mtext><mtext>g</mtext><mtext>m</mtext><mtext>a</mtext><mtext>x</mtext></mrow></mstyle><mrow><mi>c</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></munder><mstyle displaystyle="true"><munderover><mo>∏</mo><mrow><mi>m</mi><mo>=</mo><mn>1</mn></mrow><mi>Μ</mi></munderover><mi>Ρ</mi></mstyle><mo stretchy="false">(</mo><mi>F</mi><msub><mrow></mrow><mi>m</mi></msub><mo stretchy="false">|</mo><mi>C</mi><mo>=</mo><mi>c</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></math></mathml>      (7)</p>
                </div>
                <div class="p1">
                    <p id="67">要求解式(7)的<mathml id="137"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>c</mi><mo>^</mo></mover></math></mathml><sub><i>i</i></sub>只需计算每个蕨<i>F</i><sub><i>m</i></sub>和类<i>c</i><sub><i>i</i></sub>的类条件概率<i>P</i>(<i>F</i><sub><i>m</i></sub>|<i>C</i>=<i>c</i><sub><i>i</i></sub>)。</p>
                </div>
                <h4 class="anchor-tag" id="68" name="68"><b>1.2 改进的随机蕨算法</b></h4>
                <h4 class="anchor-tag" id="69" name="69"><b>1.2.1 基于FAST角点的特征检测</b></h4>
                <div class="p1">
                    <p id="70">特征检测是随机蕨分类器离线训练过程和在线检测匹配程的重要的第一步。为了给后续的分类过程以及虚拟渲染过程提供更多的时间,本方案采用比原算法Lepetit关键点的检测方法更加快速的FAST角点检测的方法来完成模板以及当前帧的特征点检测。</p>
                </div>
                <div class="p1">
                    <p id="71">2006年,Rosten 和Drummond<citation id="223" type="reference"><link href="200" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>提出了FAST角点特征检测算子。FAST 角点特征检测最大的优点是快速、具有杰出的实时性。它与Lepetit关键点、SIFT、MSER、SURF特征检测方法的检测速度对比如表1所示。</p>
                </div>
                <div class="area_img" id="72">
                    <p class="img_tit"><b>表1 不同特征检测算子检测速度对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="72" border="1"><tr><td><br />特征检测类型</td><td>检测时<br />间/s</td><td>平均特征/<br />(点数·帧<sup>-1</sup>)</td><td>检测速度/<br />(μs·point<sup>-1</sup>)</td></tr><tr><td><br />Lepetit关键点</td><td>6.576</td><td>150</td><td>175.366</td></tr><tr><td><br />SIFT</td><td>61.091</td><td>150</td><td>1 629.093</td></tr><tr><td><br />MSER</td><td>12.335</td><td>151</td><td>326.753</td></tr><tr><td><br />SURF</td><td>8.654</td><td>152</td><td>227.688</td></tr><tr><td><br />FAST</td><td>0.519</td><td>152</td><td>13.679</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="73">检测中,PC机为Pentium(R)Dual-coreT4400@2.20 GHz,ROM 2 GB的普通PC,特征检测所用视频时长10 s,分辨率为320×240,帧率25 帧/秒,平均每帧特征点数为150 点左右。</p>
                </div>
                <div class="p1">
                    <p id="74">FAST角点特征检测具有角点检测算子的共性,即具有旋转不变性和对光照不敏感的特点。然而SURF、SIFT等特征检测算法因加入了多尺度的高斯卷积核而具有尺度不变性,这一点却恰恰是具有最快检测速度的FAST 角点特征检测算法所欠缺的。针对这一问题,本文算法将FAST角点检测算法代替Lepetit关键点的检测方法应用于随机蕨的特征检测,并对随机蕨离线训练阶段的仿射过程进行改进来解决FAST角点特征不具备尺度不变性与仿射不变性的问题。</p>
                </div>
                <h4 class="anchor-tag" id="75" name="75"><b>1.2.2 仿射过程的改进</b></h4>
                <div class="p1">
                    <p id="76">仿射过程的作用主要是提取稳定点和生成训练片元,是随机蕨离线训练阶段的关键步骤。针对FAST 角点特征检测不具备尺度不变性和仿射不变性的问题,对随机蕨的仿射过程进行改进。由于摄像机模型在不同内外部参数条件下的三维场景与对应的二维成像视图间的透视映射关系,本方案将具有4自由度无平移向量的仿射变化来近似代替视图间的透视映射。用对模板图像做无平移仿射变化所得的仿射图像来选取稳定点集和生成训练片元可以在保证精确的对应关系的同时模拟目标在不同视角下的成像效果<citation id="224" type="reference"><link href="202" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>。因此,本方案通过对离线训练过程中的仿射过程进行改进来解决特征点不具备尺度不变性和仿射不变性的问题。将模板图像作为正视图,经仿射变换得到仿射视图,仿射视图模拟不同视角变换的当前帧视图。假设模板图像中的点<i>x</i>与当前帧图像中的点<i>x</i>′互为同名点,它们之间的关系可以用仿射变换近似表示为:</p>
                </div>
                <div class="area_img" id="164">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JYRJ201911010_16400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="79">忽略平移向量后同名点之间的关系可以表示为:</p>
                </div>
                <div class="area_img" id="80">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JYRJ201911010_08000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="82">非奇异仿射矩阵的线性矩阵<i><b>A</b></i>通过奇异值分解为:</p>
                </div>
                <div class="p1">
                    <p id="83"><i><b>A</b></i><b>=</b><i><b>UDV</b></i><sup>T</sup>=(<i><b>UV</b></i><sup>T</sup>)(<i><b>VDV</b></i><sup>T</sup>)=<i><b>R</b></i>(<i>θ</i>)<i><b>R</b></i>(ϕ)<i><b>DR</b></i>(-ϕ)      (10)</p>
                </div>
                <div class="area_img" id="84">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JYRJ201911010_08400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="area_img" id="86">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JYRJ201911010_08600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="area_img" id="165">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JYRJ201911010_16500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="90">因此,上述过程可以看作是将模板图像中的<i><b>x</b></i>经旋转变换<i><b>R</b></i>(-ϕ)、沿x和y方向不等距缩放<i><b>D</b></i>、回转变化<i><b>R</b></i>(ϕ)以及旋转变换<i><b>R</b></i>(<i>θ</i>)后得到<i>x</i>′。仿射变换矩阵<i><b>A</b></i>包含<i>θ</i>、ϕ、<i>λ</i><sub>1</sub>和<i>λ</i><sub>2</sub>四个参数。<i>θ</i>为旋转因子,ϕ为形变因子,<i>λ</i><sub>1</sub>和<i>λ</i><sub>2</sub>分别为x和y方向上的尺度因子。</p>
                </div>
                <h3 id="91" name="91" class="anchor-tag"><b>2 跟踪注册算法设计</b></h3>
                <div class="p1">
                    <p id="92">本文提出的基于改进随机蕨的无标识增强现实跟踪注册算法主要包括离线训练阶段、在线检测匹配阶段以及三维注册阶段,算法流程图如图2所示。离线训练阶段首先是提取目标图像一定数目的FAST特征点,提取稳定特征点和生成训练样本;然后将训练样本投入到一定规模的随机蕨中训练得到随机蕨分类器。在线检测匹配过程首先是提取一定数目的FAST特征点,将特征点对应的片元投入随机蕨分类器进行粗匹配,使用RANSAC剔除误匹配后进行三维位姿估计。三维注册阶段是根据每帧位姿估计结果使用OpenGL进行虚拟渲染。</p>
                </div>
                <div class="area_img" id="93">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201911010_093.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 基于改进随机蕨的增强现实跟踪注册算法流程图" src="Detail/GetImg?filename=images/JYRJ201911010_093.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 基于改进随机蕨的增强现实跟踪注册算法流程图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201911010_093.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="94" name="94"><b>2.1 离线训练</b></h4>
                <div class="p1">
                    <p id="95">首先在模板图像<i>I</i><sub>0</sub>上提取一定数目的FAST特征点。随机选取仿射参数并对模板图像<i>I</i><sub>0</sub>按照<i><b>A</b></i>(ϕ,<i>θ</i>,<i>λ</i><sub>1</sub>,<i>λ</i><sub>2</sub>)进行仿射变换得仿射视图<i>I</i>′,循环此过程得到<i>N</i><sub>total</sub>幅仿射视图。对仿射视图<i>I</i>′<sub><i>j</i></sub>提取FAST特征点,并将这些FAST特征点对应到模板图像<i>I</i><sub>0</sub>中,同时将某一特征点<i>k</i>在所有仿射视图中被检测到的次数记为<i>N</i><sub>detected</sub>,从而可以得到特征点<i>k</i>的被检测概率为<mathml id="138"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><mi>Ν</mi><msub><mrow></mrow><mrow><mtext>d</mtext><mtext>e</mtext><mtext>t</mtext><mtext>e</mtext><mtext>c</mtext><mtext>t</mtext><mtext>e</mtext><mtext>d</mtext></mrow></msub></mrow><mrow><mi>Ν</mi><msub><mrow></mrow><mrow><mtext>t</mtext><mtext>o</mtext><mtext>t</mtext><mtext>a</mtext><mtext>l</mtext></mrow></msub></mrow></mfrac><mo>。</mo></mrow></math></mathml>取概率<i>P</i>(<i>k</i>)最高的<i>H</i>个特征点作为模板图像的稳定特征点集,将稳定特征点集中的特征点及其对应的图像块所属的类为<i>c</i><sub><i>i</i></sub>,<i>i</i>=1,2,…,<i>H</i>作为分类器的初始类进行分类训练,实验中取<i>H</i>=100。</p>
                </div>
                <div class="p1">
                    <p id="96">然后在模板图像稳定特征点集基础上生成训练片元。对模板图像<i>I</i><sub>0</sub>按照<i><b>A</b></i>(ϕ,<i>θ</i>,<i>λ</i><sub>1</sub>,<i>λ</i><sub>2</sub>)进行仿射变换并按上述步骤得到稳定特征点集,之后以每个稳定特征点投影到仿射视图中对应点为中心,截取像素片元作为训练片元得到训练集<i>B</i><sub>train</sub>。具体参数设置为仿射参数设置为ϕ,<i>θ</i>∈[0,2π],<i>λ</i><sub>1</sub>,<i>λ</i><sub>2</sub>∈[0.6,1.5]。对每个类为<i>c</i><sub><i>i</i></sub>,<i>i</i>=1,2,…,<i>H</i>将旋转因子从1°～360°按步长为一度循环并随机选取仿射参数进行30次仿射变换,即每类得到10 800个训练片元。</p>
                </div>
                <div class="p1">
                    <p id="97">由以上两步,可以得到稳定特征点即初始类<i>c</i><sub><i>i</i></sub>和训练集<i>B</i><sub>train</sub>,之后便可进行离线训练。随机生成<i>M</i>×<i>S</i>的随机蕨丛,即包含<i>M</i>棵蕨,每棵蕨含有<i>S</i>个节点。每个节点随机生成一个判断函数,判断函数对每个训练片元在<i>L</i>×<i>L</i>(取<i>L</i>=32)范围内选取一对像素位置<i>d</i><sub><i>j</i></sub><sub>1</sub>和<i>d</i><sub><i>j</i></sub><sub>2</sub>。对每个初始类<i>c</i><sub><i>i</i></sub>根据随机蕨<i>M</i>×<i>S</i>对随机像素位置<i>d</i><sub><i>j</i></sub><sub>1</sub>和<i>d</i><sub><i>j</i></sub><sub>2</sub>的灰度值由式(1)计算类<i>c</i><sub><i>i</i></sub>的<i>M</i>个随机蕨中<i>S</i>个二元特征<i>f</i><sub><i>i</i></sub>的值,并根据<i>f</i><sub><i>i</i></sub>的值计算出式(7)中的每个随机蕨<i>F</i><sub><i>m</i></sub>和类<i>c</i><sub><i>i</i></sub>的条件概率<i>P</i>(<i>F</i><sub><i>m</i></sub>|<i>C</i>=<i>c</i><sub><i>i</i></sub>)。</p>
                </div>
                <h4 class="anchor-tag" id="98" name="98"><b>2.2 在线检测匹配</b></h4>
                <div class="p1">
                    <p id="99">在线检测阶段首先对当前帧提取FAST特征点<i>k</i><sup>input</sup>,将该特征点所在片元<i>patch</i><sup>input</sup>投入随机蕨分类器进行分类。由基本随机蕨算法可知,要对片元<i>patch</i><sup>input</sup> 进行分类就是根据式(7)寻找对应类<i>c</i><sub><i>i</i></sub>,而<i>p</i><sub><i>x</i></sub><sub>,</sub><sub><i>ci</i></sub>=<i>P</i>(<i>F</i><sub><i>m</i></sub>=<i>x</i>|<i>C</i>=<i>c</i><sub><i>i</i></sub>)可以通过离线训练得到,求得<mathml id="139"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>c</mi><mo>^</mo></mover></math></mathml><sub><i>i</i></sub>即可以认为特征点<i>k</i><sup>input</sup>所在片元与模板图像<mathml id="140"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>c</mi><mo>^</mo></mover></math></mathml><sub><i>i</i></sub>匹配。</p>
                </div>
                <div class="p1">
                    <p id="100">通过随机蕨丛分类器对当前帧和模板图像进行粗匹配之后,采用随机抽样一致性(RANSAC)算法来剔除误匹配。通常情况下,随机蕨蕨数规模越大,匹配的正确率就越高,但是耗时也越长。图3、图4为经粗匹配和RANSAC算法剔除误匹配后,匹配点数随随机蕨数目的变化趋势以及每个FAST特征点平均匹配时间随随机蕨数目的变化趋势,取随机蕨棵树<i>M</i>=30。</p>
                </div>
                <div class="area_img" id="102">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201911010_102.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 匹配点数随随机蕨数目递增的变化趋势" src="Detail/GetImg?filename=images/JYRJ201911010_102.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 匹配点数随随机蕨数目递增的变化趋势  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201911010_102.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="103">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201911010_103.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 单点匹配时间随随机蕨数目递增的变化趋势" src="Detail/GetImg?filename=images/JYRJ201911010_103.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 单点匹配时间随随机蕨数目递增的变化趋势  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201911010_103.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="104" name="104"><b>2.3 三维位姿估计</b></h4>
                <div class="p1">
                    <p id="105">通过前面两节可知,通过随机蕨丛分类器可以检测出当前帧中的目标,从而得到当前帧检测出来的目标与模板的对应关系为:</p>
                </div>
                <div class="p1">
                    <p id="106">(<i>X</i><sub>1,2,…,</sub><sub><i>N</i></sub>,<i>t</i>)=<i><b>H</b></i><sup><i>n</i></sup>(<i>X</i><sub>1,2,…,</sub><sub><i>N</i></sub>,<i>t</i><sub>0</sub>)</p>
                </div>
                <div class="p1">
                    <p id="107">式中:<i><b>H</b></i><sup><i>n</i></sup>为当前帧目标点集与模板点集之间的对应关系,即单应性矩阵。摄像机透射投影模型与根据模板使用随机蕨算法检测到的目标的投影关系如图5所示。使用将场景的坐标系定义在检测到的目标上的方法来简化透射投影方程,即检测目标平面为Z平面<citation id="225" type="reference"><link href="204" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>。</p>
                </div>
                <div class="area_img" id="108">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201911010_108.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 检测目标投影和摄像机运动关系" src="Detail/GetImg?filename=images/JYRJ201911010_108.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 检测目标投影和摄像机运动关系  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201911010_108.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="109">图中(<i>x</i><sub><i>p</i></sub>,<i>y</i><sub><i>p</i></sub>)为当前帧目标上的任意点的坐标,(<i>x</i><sub>0</sub>,<i>y</i><sub>0</sub>)为对应目标模板投影到摄像机平面上的坐标,(<i>x</i><sub><i>n</i></sub>,<i>y</i><sub><i>n</i></sub>)为其投影到摄像机平面上的坐标。因此有以下关系:</p>
                </div>
                <div class="area_img" id="110">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JYRJ201911010_11000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="area_img" id="112">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JYRJ201911010_11200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="area_img" id="114">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JYRJ201911010_11400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="116">视觉透射投影方程为<citation id="226" type="reference"><link href="206" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>:</p>
                </div>
                <div class="area_img" id="166">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JYRJ201911010_16600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="119">式中:λ是缩放因子,<i><b>K</b></i>是摄像机内参,[<i><b>R</b></i><b>|</b><i><b>T</b></i>]是摄像机外参,<i>R</i>=[<i>r</i><sub>1</sub><i>r</i><sub>2</sub><i>r</i><sub>3</sub>]是旋转矩阵。因为场景坐标定义在检测到的目标上,[<i><b>R</b></i><b>|</b><i><b>T</b></i>]就是我们需要求解的跟踪注册的三维位姿。因为检测目标平面为<i>Z</i>平面,所有<i>z</i><sub><i>p</i></sub>=0。将式(14)-式(16)代入式(17)得:</p>
                </div>
                <div class="p1">
                    <p id="120"><i><b>H</b></i><sup><i>n</i></sup>=<i><b>P</b></i><mathml id="141"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>W</mi><mi>n</mi></msubsup></mrow></math></mathml>(<i><b>P</b></i><mathml id="142"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>W</mi><mn>0</mn></msubsup></mrow></math></mathml>)<sup>-1</sup>=<i>λ</i><i><b>K</b></i>[<i>r</i><sub>1</sub><i>r</i><sub>2</sub>|<i><b>T</b></i><b>](</b><i><b>P</b></i><mathml id="143"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>W</mi><mn>0</mn></msubsup></mrow></math></mathml>)<sup>-1</sup>      (18)</p>
                </div>
                <div class="p1">
                    <p id="121">由旋转矩阵的特性可知|<i>r</i><sub>1</sub>|=|<i>r</i><sub>2</sub>|=|<i>r</i><sub>3</sub>|,并且<i>r</i><sub>1</sub>、<i>r</i><sub>2</sub>、<i>r</i><sub>3</sub>互相垂直,因此求出<i>r</i><sub>1</sub>、<i>r</i><sub>2</sub>即可得到<i>r</i><sub>3</sub>。而投影矩阵<i><b>P</b></i><mathml id="144"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>W</mi><mn>0</mn></msubsup></mrow></math></mathml>可以由(14)式计算得出(计算点数≥4),因此求解式(18)就可以得到目标的三维位姿[<i><b>R</b></i><b>|</b><i><b>T</b></i>]。</p>
                </div>
                <h3 id="122" name="122" class="anchor-tag"><b>3 实 验</b></h3>
                <div class="p1">
                    <p id="123">本文算法实验是在VS 2012和OpenCV 2.4.9环境下实现的,PC机为Pentium(R) Dual-coreT4400@2.20 GHz,ROM 2 GB的普通PC,采用普通摄像头Logitech C525获取图像。模板图像分辨率为600×480。</p>
                </div>
                <div class="p1">
                    <p id="124">实验使用基于朴素贝叶斯分类的随机蕨丛算法对模板进行宽基线匹配检测。图6为检测结果,可以看出,模板在不同角度、不同光照环境下都能被准确地检测出来。当目标模板被部分遮挡时,仍然能够准确地检测出目标。</p>
                </div>
                <div class="area_img" id="125">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201911010_12500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 基于随机蕨宽基线匹配检测效果" src="Detail/GetImg?filename=images/JYRJ201911010_12500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 基于随机蕨宽基线匹配检测效果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201911010_12500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="126">使用原始随机蕨丛算法得到目标后,对其进行三维位姿估计,使用OpenGL绘图便可以实现三维跟踪注册。虽然该方法能够解决环境光照条件变化以及目标被部分遮挡情况下跟踪注册失败的问题,并且与传统匹配算法相比能够把耗时较多的部分转为离线训练阶段,但对其运算时间进行测试,如表2所示,使用原始随机蕨丛算法仍不能满足增强现实系统实时性的要求。本文通过使用具有快速特点的FAST角点进行特征点检测来进一步提高算法的实时性,同时针对FAST特征的缺点对仿射过程进行改进从而保证改进算法在不同条件下的检测效果。</p>
                </div>
                <div class="area_img" id="127">
                    <p class="img_tit"><b>表2 不同方法的增强现实平均跟踪注册单帧耗时</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="127" border="1"><tr><td><br />步骤</td><td>基于原始随机<br />蕨的算法/ms</td><td>基于改进随机<br />蕨的算法/ms</td></tr><tr><td><br />预处理</td><td>1.2</td><td>1.2</td></tr><tr><td><br />特征点检测(150 points)</td><td>26.3</td><td>2.1</td></tr><tr><td><br />粗匹配(100 pairs)</td><td>18.3</td><td>18.3</td></tr><tr><td><br />RANSAC</td><td>1</td><td>1</td></tr><tr><td><br />位姿估计</td><td>9.4</td><td>9.4</td></tr><tr><td><br />OpenG绘图</td><td>1.2</td><td>1.2</td></tr><tr><td><br />总耗时</td><td>57.4</td><td>33.2</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="128">图7是利用本文算法针对不同条件下的目标进行虚实配置的实验效果。将本文提出的改进随机蕨算法应用于无标识增强现实跟踪注册,可以解决在光照条件变化、目标被部分遮挡情况下的跟踪注册失败的问题,基本能够满足AR系统实时性的要求。</p>
                </div>
                <div class="area_img" id="167">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201911010_16700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 基于改进随机蕨的无标识增强现实跟踪注册效果" src="Detail/GetImg?filename=images/JYRJ201911010_16700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 基于改进随机蕨的无标识增强现实跟踪注册效果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201911010_16700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h3 id="131" name="131" class="anchor-tag"><b>4 结 语</b></h3>
                <div class="p1">
                    <p id="132">本文提出了一种基于改进随机蕨的无标识增强现实跟踪注册算法。通过使用随机蕨算法解决在光照条件变化、目标被部分遮挡情况下的跟踪注册失败的问题,并通过使用FAST角点进行特征点提取来提高检测速度。针对FAST角点特征存在不具备仿射不变性和尺度不变性的问题,对随机蕨算法的仿射过程进行改进保证目标检测的精度。相对于传统的匹配算法和原始随机蕨算法,本文方法在保证目标检测精度的同时,单帧耗时约为33 ms,基本上能够满足增强现实系统实时性的要求。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="168">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES47078A393208075BB106A193B558274B&amp;v=MjgzMDZwbWFCdUhZZk9HUWxmQ3BiUTM1ZGhodzcyNXhLcz1OaWZPZmJlL0h0YkUzb3hNWitrUEJIdyt5bVJoNno5N09YN3JyMkF3ZkxxV1FyN3RDT052RlNpV1dyN0pJRg==&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> Lima J P,Roberto R,Simões F,et al.Markerless tracking system for augmented reality in the automotive industry[J].Expert Systems with Applications,2017,82:100-114.
                            </a>
                        </p>
                        <p id="170">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Real-Time Camera Pose Estimation for Wide-Area Augmented Reality Applications">

                                <b>[2]</b> Guan T,Duan L,Yu J,et al.Real-time camera pose estimation for wide-area augmented reality applications[J].IEEE Computer Graphics and Applications,2011,31(3):56-68.
                            </a>
                        </p>
                        <p id="172">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Remote Activation and Confidence Factor Setting of ARToolKit with Data Association for Tracking Multiple Markers">

                                <b>[3]</b> Vista F IV,Lee D J,Chong K T.Remote activation and confidence factor setting of ARToolKit with data association for tracking multiple markers[J].International Journal of Control and Automation,2013,6(6):243-252.
                            </a>
                        </p>
                        <p id="174">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=ARTag:an improved marker system based on ARToolkit">

                                <b>[4]</b> Fiala M.ARTag:An improved marker system based on AR toolkit[J].National Research Council Publication,2004,4(7):166-174.
                            </a>
                        </p>
                        <p id="176">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Markless augmented reality registration algorithm based on ORB">

                                <b>[5]</b> Du Y,Miao Z,Cen Y.Markless augmented reality registration algorithm based on ORB[C]//International Conference on Signal Processing.IEEE,2015:1236-1240.
                            </a>
                        </p>
                        <p id="178">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Good features to track">

                                <b>[6]</b> Shi J B,Tomasi C.Good features to track[C]//Proceedings of IEEE Conference on Computer Vision and Pattern Recognition,1994:593-600.
                            </a>
                        </p>
                        <p id="180">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Online estimation of trifocal tensor for augmented live video">

                                <b>[7]</b> Li J,Laganiere R,Roth G.Online estimation of trifocal tensor for augmented live video[C]//Proceedings of the IEEE and ACM International Symposium on Mixed and Augmented Reality,2004:182-190.
                            </a>
                        </p>
                        <p id="182">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Registration Using Natural Features for Augmented Reality Systems">

                                <b>[8]</b> Yuan M L,Ong S K,Nee A Y.Registration using natural features for augmented reality systems[J].IEEE Transactions on Visualization &amp; Computer Graphics,2006,12(4):569-580.
                            </a>
                        </p>
                        <p id="184">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XTYD200910043&amp;v=MjEzMDBQVG5TYXJHNEh0ak5yNDlCWjRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ1RnkvZ1Zidkw=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b> 康波,任鹏.一种基于自然纹理特征的增强现实跟踪算法[J].系统工程与电子技术,2009,31(10):2480-2484.
                            </a>
                        </p>
                        <p id="186">
                            <a id="bibliography_10" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201405063&amp;v=MDEzMzlLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnVGeS9nVmJ2TEx6VFpaTEc0SDlYTXFvOURaNFE=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[10]</b> 肖慧,陆奎.基于SIFT自然特征的AR系统研究和实现[J].计算机应用与软件,2014,31(5):244-246,263.
                            </a>
                        </p>
                        <p id="188">
                            <a id="bibliography_11" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002830901&amp;v=MDE4NzJiZXNPWTNrNXpCZGg0ajk5U1hxUnJ4b3hjTUg3UjdxZWJ1ZHRGU2psVkxyT0pGMD1OajdCYXJPNEh0SE9wNHhG&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[11]</b> Lowe D G.Distinctive image features from scale-invariant key-points[J].International Journal of Computer Vision,2004,20(2):91-110.
                            </a>
                        </p>
                        <p id="190">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fast Keypoint Recognition Using Random Ferns">

                                <b>[12]</b> Ozuysal M,Calonder M,Lepeti V.Fast keypoint recognition using random ferns[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2010,32(3):448-461.
                            </a>
                        </p>
                        <p id="192">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Point matching as a classification problem for fast and robust object pose estimation">

                                <b>[13]</b> Lepetit V,Pilet J,Fua P.Point matching as a classification problem for fast and robust object pose estimation[C]//Proceedings of the 2004 IEEE computer society conference on Computer vision and pattern recognition.IEEE,2004:244-250.
                            </a>
                        </p>
                        <p id="194">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Research on augmented reality of printed matter based on random ferns and orientation gradient with integral graph">

                                <b>[14]</b> Li D,Shi R,Zhao X,et al.Research on augmented reality of printed matter based on random ferns and orientation gradient with integral graph[C]//China Academic Conference on Printing &amp; Packaging and Media Technology.Springer,Singapore,2016:257-264.
                            </a>
                        </p>
                        <p id="196">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Boosted Random Ferns for Object Detection &amp;quot;">

                                <b>[15]</b> Vergel M V,Andradecetto J,Sanfeliu A,et al.Boosted random ferns for object detection[J].IEEE Transactions on Pattern Analysis &amp; Machine Intelligence,2018,40(2):272-288.
                            </a>
                        </p>
                        <p id="198">
                            <a id="bibliography_16" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XDZK201402014&amp;v=MjA2ODBGckNVUkxPZVplVnVGeS9nVmJ2TFBTblJaYkc0SDlYTXJZOUVZSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEc=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[16]</b> 杨晨晖,刘守达,王子明,等.基于随机蕨的实时车辆匹配[J].厦门大学学报(自然版),2014,53(2):206-211.
                            </a>
                        </p>
                        <p id="200">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Machine learning for high speed corner detection">

                                <b>[17]</b> Rosten E,Drummond T.Machine learning for high-speed corner detection[C]//European Conference on Computer Vision.Berlin,Germany:Springer-Verlag,2006:430-443.
                            </a>
                        </p>
                        <p id="202">
                            <a id="bibliography_18" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JQRR201403003&amp;v=MzE5MTQvZ1ZidkxMenpaZkxHNEg5WE1ySTlGWjRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ1Rnk=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[18]</b> 罗元,付瑜,张毅.基于FAST角点和仿射改进的随机蕨丛的单目视觉实时匹配算法[J].机器人,2014(3):271-278.
                            </a>
                        </p>
                        <p id="204">
                            <a id="bibliography_19" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZGTB200809028&amp;v=MTQ4MDVMT2VaZVZ1RnkvZ1ZidkxQeXJmYkxHNEh0bk1wbzlIYklRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[19]</b> 林倞,王涌天,刘越,等.基于模板跟踪的实时无标志点注册算法[J].中国图象图形学报,2008,13(9):1812-1819.
                            </a>
                        </p>
                        <p id="206">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Efficient camera motion and 3D recovery using an inertial sensor">

                                <b>[20]</b> Labrie M,Hebert P.Efficient camera motion and 3D recovery using an inertial sensor[C]//Proceedings of the Fourth Canadian Conference on Computer and Robot Vision.IEEE,2007:55-62.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JYRJ201911010" />
        <input id="dpi" type="hidden" value="96" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201911010&amp;v=MjA0ODI4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnVGeS9nVmJ2TEx6VFpaTEc0SDlqTnJvOUVaSVFLREg=&amp;uid=WEEvREcwSlJHSldRa1FhdXNXaEhoRFVsZkNHV25tS3FjcGR3SzhBTGdXQT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="0" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
