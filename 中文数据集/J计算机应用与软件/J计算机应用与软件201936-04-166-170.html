<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637135753799502500%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJYRJ201904027%26RESULT%3d1%26SIGN%3dx7bnemPeORurqYXW9UqyqodlRLU%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JYRJ201904027&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JYRJ201904027&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201904027&amp;v=MjMyMDFyQ1VSN3FmWnVadEZ5em1VTHZQTHpUWlpMRzRIOWpNcTQ5SFk0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0Y=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#33" data-title="&lt;b&gt;0 引 言&lt;/b&gt; "><b>0 引 言</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#38" data-title="&lt;b&gt;1 深度学习网络架构&lt;/b&gt; "><b>1 深度学习网络架构</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#39" data-title="&lt;b&gt;1.1 卷积神经网络 (CNN&lt;/b&gt;) "><b>1.1 卷积神经网络 (CNN</b>) </a></li>
                                                <li><a href="#43" data-title="&lt;b&gt;1.2 长短期记忆网络&lt;/b&gt;"><b>1.2 长短期记忆网络</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#47" data-title="&lt;b&gt;2 基于CNN-LSTM的声纹识别网络&lt;/b&gt; "><b>2 基于CNN-LSTM的声纹识别网络</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#48" data-title="&lt;b&gt;2.1 CNN-LSTM网络模型&lt;/b&gt;"><b>2.1 CNN-LSTM网络模型</b></a></li>
                                                <li><a href="#51" data-title="&lt;b&gt;2.2 网络模型的训练&lt;/b&gt;"><b>2.2 网络模型的训练</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#55" data-title="&lt;b&gt;3 实 验&lt;/b&gt; "><b>3 实 验</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#56" data-title="&lt;b&gt;3.1 实验平台&lt;/b&gt;"><b>3.1 实验平台</b></a></li>
                                                <li><a href="#62" data-title="&lt;b&gt;3.2 语音数据集和评价标准&lt;/b&gt;"><b>3.2 语音数据集和评价标准</b></a></li>
                                                <li><a href="#69" data-title="&lt;b&gt;3.3 实验结果及分析&lt;/b&gt;"><b>3.3 实验结果及分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#83" data-title="&lt;b&gt;4 结 语&lt;/b&gt; "><b>4 结 语</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#46" data-title="图1 LSTM时序关系图">图1 LSTM时序关系图</a></li>
                                                <li><a href="#50" data-title="图2 CNN-LSTM声纹识别网络结构图">图2 CNN-LSTM声纹识别网络结构图</a></li>
                                                <li><a href="#54" data-title="&lt;b&gt;表1 CNN-LSTM网络结构参数设置表&lt;/b&gt;"><b>表1 CNN-LSTM网络结构参数设置表</b></a></li>
                                                <li><a href="#73" data-title="图3 CNN-LSTM模型及LSTM和CNN测试集准确率变化图">图3 CNN-LSTM模型及LSTM和CNN测试集准确率变化图</a></li>
                                                <li><a href="#76" data-title="图4 CNN-LSTM模型及LSTM和CNN测试集损失值变化图">图4 CNN-LSTM模型及LSTM和CNN测试集损失值变化图</a></li>
                                                <li><a href="#77" data-title="&lt;b&gt;表2 CNN、LSTM和CNN-LSTM网络训练测试最值&lt;/b&gt;"><b>表2 CNN、LSTM和CNN-LSTM网络训练测试最值</b></a></li>
                                                <li><a href="#80" data-title="&lt;b&gt;表3 CNN、LSTM、DNN和CNN-LSTM网络迭代次数对应准确率表&lt;/b&gt;"><b>表3 CNN、LSTM、DNN和CNN-LSTM网络迭代次数对应准确率表</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="95">


                                    <a id="bibliography_1" title=" Schmidhuber J. Deep learning in neural networks: an overview [J]. Neural Networks, 2014, 61 (3) : 85-94." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14110700296012&amp;v=MDQxNjV1SUpESDA3b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcnpJSkYwWGJoVT1OaWZPZmJLOEg5RE1xSTlGWg==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         Schmidhuber J. Deep learning in neural networks: an overview [J]. Neural Networks, 2014, 61 (3) : 85-94.
                                    </a>
                                </li>
                                <li id="97">


                                    <a id="bibliography_2" title=" Abdel-Hamid O, Mohamed A R, Jiang H, et al. Applying Convolutional Neural Networks concepts to hybrid NN-HMM model for speech recognition[C]//2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) . IEEE, 2012: 4277-4280." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Applying convolutional neural networks concepts to hybrid NN-HMM model for speech recognition">
                                        <b>[2]</b>
                                         Abdel-Hamid O, Mohamed A R, Jiang H, et al. Applying Convolutional Neural Networks concepts to hybrid NN-HMM model for speech recognition[C]//2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) . IEEE, 2012: 4277-4280.
                                    </a>
                                </li>
                                <li id="99">


                                    <a id="bibliography_3" title=" Simonyan K, Zisserman A. Very Deep Convolutional Networks for LargeScale Image Recognition [J]. Computer Science, 2014, 13 (2) : 120-131." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Very deep convolutional networks for large-scale image recognition">
                                        <b>[3]</b>
                                         Simonyan K, Zisserman A. Very Deep Convolutional Networks for LargeScale Image Recognition [J]. Computer Science, 2014, 13 (2) : 120-131.
                                    </a>
                                </li>
                                <li id="101">


                                    <a id="bibliography_4" title=" Variani E, Lei X, Mcdermott E, et al.Deep neural networks for small footprint text-dependent speaker verification[C]//2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) . IEEE, 2014." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep neural networks for small footprint text-dependent speaker verification">
                                        <b>[4]</b>
                                         Variani E, Lei X, Mcdermott E, et al.Deep neural networks for small footprint text-dependent speaker verification[C]//2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) . IEEE, 2014.
                                    </a>
                                </li>
                                <li id="103">


                                    <a id="bibliography_5" title=" Snyder D, Garcia-Romero D, Povey D, et al. Deep Neural Network Embeddings for Text-Independent Speaker Verification[C]//Proc. InterSpeech 2017:999-1003." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep Neural Network Embeddings for Text-Independent Speaker Verification">
                                        <b>[5]</b>
                                         Snyder D, Garcia-Romero D, Povey D, et al. Deep Neural Network Embeddings for Text-Independent Speaker Verification[C]//Proc. InterSpeech 2017:999-1003.
                                    </a>
                                </li>
                                <li id="105">


                                    <a id="bibliography_6" title=" Waibel A, Hanazawa T, Hinton G, et al.Phoneme recognition using time-delay neural networks[J]. IEEE transactions on acoustics, speech, and signal processing, 1989, 37 (3) : 328-339" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Phoneme recognition using time-delay neural networks">
                                        <b>[6]</b>
                                         Waibel A, Hanazawa T, Hinton G, et al.Phoneme recognition using time-delay neural networks[J]. IEEE transactions on acoustics, speech, and signal processing, 1989, 37 (3) : 328-339
                                    </a>
                                </li>
                                <li id="107">


                                    <a id="bibliography_7" title=" Abdel-Hamid O, Mohamed A R, Jiang H, et al. Convolutional Neural Networks for Speech Recognition[J]. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2014, 22 (10) :1533-1545." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM2F9583DBCBE0076E2252D2DAE8A39D9E&amp;v=MjI5NjVyN0pJRnBtYUJ1SFlmT0dRbGZCckxVMDV0dGh3THU4eEs4PU5pZklZN0hPRjlURXJQczNGNWw2REh3K3lXTVI2RHAvUEgyVzNXYzlDTEdkTWJQcUNPTnZGU2lXVw==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                         Abdel-Hamid O, Mohamed A R, Jiang H, et al. Convolutional Neural Networks for Speech Recognition[J]. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2014, 22 (10) :1533-1545.
                                    </a>
                                </li>
                                <li id="109">


                                    <a id="bibliography_8" title=" 余玲飞, 刘强. 基于深度循环网络的声纹识别方法研究及应用[J]. 计算机应用研究, 2019, 36 (1) :153-158." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSYJ201901036&amp;v=MTQ1MjhIOWpNcm85R1lvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVadEZ5em1VTHZQTHo3U1pMRzQ=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                         余玲飞, 刘强. 基于深度循环网络的声纹识别方法研究及应用[J]. 计算机应用研究, 2019, 36 (1) :153-158.
                                    </a>
                                </li>
                                <li id="111">


                                    <a id="bibliography_9" title=" Bhattacharya G, Alam J, Stafylakis T, et al. Deep Neural Network based Text-Dependent Speaker Recognition: Preliminary Results[C]//Odyssey 2016.21-24 Jun 2016, Bilbao, Spain." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep Neural Network based Text-Dependent Speaker Recognition: Preliminary Results">
                                        <b>[9]</b>
                                         Bhattacharya G, Alam J, Stafylakis T, et al. Deep Neural Network based Text-Dependent Speaker Recognition: Preliminary Results[C]//Odyssey 2016.21-24 Jun 2016, Bilbao, Spain.
                                    </a>
                                </li>
                                <li id="113">


                                    <a id="bibliography_10" title=" Heigold G, Moreno I, Bengio S, et al. End-to-End Text-Dependent Speaker Verification[C]//Acoustics, Speech and Signal Processing (ICASSP) , 2016 IEEE International Conference on. IEEE, 2016." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=&amp;quot;End-to-end text-dependent speaker verification,&amp;quot;">
                                        <b>[10]</b>
                                         Heigold G, Moreno I, Bengio S, et al. End-to-End Text-Dependent Speaker Verification[C]//Acoustics, Speech and Signal Processing (ICASSP) , 2016 IEEE International Conference on. IEEE, 2016.
                                    </a>
                                </li>
                                <li id="115">


                                    <a id="bibliography_11" title=" Chowdhury F A R R, Wang Q, Moreno I L, et al. Attention-Based Models for Text-Dependent Speaker Verification[C]//ICASSP 2018—2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) . IEEE, 2018." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Attention-Based Models for Text-Dependent Speaker Verification">
                                        <b>[11]</b>
                                         Chowdhury F A R R, Wang Q, Moreno I L, et al. Attention-Based Models for Text-Dependent Speaker Verification[C]//ICASSP 2018—2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) . IEEE, 2018.
                                    </a>
                                </li>
                                <li id="117">


                                    <a id="bibliography_12" title=" Zhang C, Koishida K. End-to-End Text-Independent Speaker Verification with Triplet Loss on Short Utterances[C]//Interspeech, 2017." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=End-to-End Text-Independent Speaker Verification with Triplet Loss on Short Utterances">
                                        <b>[12]</b>
                                         Zhang C, Koishida K. End-to-End Text-Independent Speaker Verification with Triplet Loss on Short Utterances[C]//Interspeech, 2017.
                                    </a>
                                </li>
                                <li id="119">


                                    <a id="bibliography_13" title=" Greff K, Srivastava R K, Koutn&#237;k, Jan, et al. LSTM: A Search Space Odyssey[J]. IEEE Transactions on Neural Networks &amp;amp; Learning Systems, 2015, 28 (10) :2222-2232." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=LSTM:A search space odyssey">
                                        <b>[13]</b>
                                         Greff K, Srivastava R K, Koutn&#237;k, Jan, et al. LSTM: A Search Space Odyssey[J]. IEEE Transactions on Neural Networks &amp;amp; Learning Systems, 2015, 28 (10) :2222-2232.
                                    </a>
                                </li>
                                <li id="121">


                                    <a id="bibliography_14" >
                                        <b>[14]</b>
                                     TensorFlow.谷歌深度学习框架[EB/OL]. 2018. https://www.tensorflow.org/?hl=zh-cn.</a>
                                </li>
                                <li id="123">


                                    <a id="bibliography_15" title=" Free ST Chinese Mandarin Corpus[DB/OL]. 2016. http://www.openslr.org/38/." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Free ST Chinese Mandarin Corpus">
                                        <b>[15]</b>
                                         Free ST Chinese Mandarin Corpus[DB/OL]. 2016. http://www.openslr.org/38/.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JYRJ" target="_blank">计算机应用与软件</a>
                2019,36(04),166-170 DOI:10.3969/j.issn.1000-386x.2019.04.026            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于CNN-LSTM网络的声纹识别研究</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%97%AB%E6%B2%B3&amp;code=23290520&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">闫河</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%91%A3%E8%8E%BA%E8%89%B3&amp;code=39481741&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">董莺艳</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E9%B9%8F&amp;code=22545900&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王鹏</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%BD%97%E6%88%90&amp;code=40711088&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">罗成</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%8E%E7%84%95&amp;code=40711089&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">李焕</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E9%87%8D%E5%BA%86%E7%90%86%E5%B7%A5%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0046910&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">重庆理工大学计算机科学与工程学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E9%87%8D%E5%BA%86%E7%90%86%E5%B7%A5%E5%A4%A7%E5%AD%A6%E4%B8%A4%E6%B1%9F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AD%A6%E9%99%A2&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">重庆理工大学两江人工智能学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>传统声纹识别方法过程复杂, 模型识别准确率低, 是声纹识别应用发展的关键问题。利用深度学习具有自主特征提取及分类的特点, 结合卷积神经网络 (CNN) 和长短期记忆网络 (LSTM) , 提出一种结合的网络模型学习声纹识别特征及对其进行身份认证。将原始语音转换为固定长度语谱图, 顺序进入CNN、LSTM, 结合网络进行训练以及声纹特征学习。通过对比CNN、LSTM以及DNN网络, 验证CNN-LSTM网络在声纹识别中具有较少迭代次数情况下高准确率的特性。经实验结果可以得出, 语音空间特征及时序特征均是声纹识别中重要的影响因素, 实验中的CNN-LSTM网络模型准确率达到95.42%, 损失低值达到0.097 3。该方法有利于实际声纹识别的应用。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%A3%B0%E7%BA%B9%E8%AF%86%E5%88%AB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">声纹识别;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=CNN-LSTM%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">CNN-LSTM网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%AF%AD%E8%B0%B1%E5%9B%BE&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">语谱图;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%97%B6%E5%BA%8F%E7%89%B9%E5%BE%81&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">时序特征;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    闫河, 教授, 主研领域:图像识别。;
                                </span>
                                <span>
                                    董莺艳, 硕士生。;
                                </span>
                                <span>
                                    王鹏, 硕士生。;
                                </span>
                                <span>
                                    罗成, 硕士生。;
                                </span>
                                <span>
                                    李焕, 硕士生。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-11-16</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金项目 (61173184);</span>
                                <span>重庆市自然科学基金项目 (cstc2018jcyjAX0694);</span>
                    </p>
            </div>
                    <h1><b>VOICEPRINT RECOGNITION BASED ON CNN-LSTM NETWORK</b></h1>
                    <h2>
                    <span>Yan He</span>
                    <span>Dong Yingyan</span>
                    <span>Wang Peng</span>
                    <span>Luo Cheng</span>
                    <span>Li Huan</span>
            </h2>
                    <h2>
                    <span>College of Computer Science and Engineering, Chongqing University of Technology</span>
                    <span>College of Artificial Intelligence, Chongqing University of Technology</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>The traditional voiceprint recognition method is complex with low recognition accuracy, which is a key issue in the development of voiceprint recognition applications. In this paper, we used deep learning with autonomous feature extraction and classification, combining with convolutional neural network (CNN) and long-term and short-term memory network (LSTM) . A combined network model was proposed to learn the features of voiceprint recognition and identity authentication. The original speech was converted into a fixed-length spectrogram, and sequentially entered into the combined network CNN and LSTM for training, and learning voiceprint feature. By comparing CNN, LSTM and DNN, We verified the high accuracy of the CNN-LSTM network in voiceprint recognition with fewer iterations. The experimental results show that the speech space features and time series features are important factors in voiceprint recognition. The accuracy of CNN-LSTM network model in the experiment reaches 95.42%, and the loss value is 0.0973. The method is benefical to the practical application of voiceprint recognition.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Voiceprint%20recognition&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Voiceprint recognition;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=CNN-LSTM%20Network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">CNN-LSTM Network;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Spectrogram&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Spectrogram;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Timing%20features&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Timing features;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-11-16</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="33" name="33" class="anchor-tag"><b>0 引 言</b></h3>
                <div class="p1">
                    <p id="34">声纹识别是生物特征识别中重要的组成部分, 由于声纹采集过程简单、且声音短期内具有不变的特性, 能够作为身份认证的关键特征。其过程是将说话人的语音特征提取, 并与原有特征进行比对, 当相似度达到一定阈值后确认身份。声纹识别中的声纹辨认常应用于刑侦破案、罪犯跟踪、国防监听、个性化应用等方面, 同时在证券交易、银行交易、公安取证、汽车声控锁等方面的声纹确认研究也逐渐开始。</p>
                </div>
                <div class="p1">
                    <p id="35">传统声纹识别包括语音信号预处理、特征提取及模型匹配三个阶段。其中声纹特征的提取是识别过程的基础, 特征表达的性能对后续识别效果影响较大, 由于计算机性能的急速发展, 原来受计算机内存限制的深度学习再次发展起来, 学术界在声纹识别方面的讨论也渐渐从传统方法转向了深度学习方法<citation id="128" type="reference"><link href="95" rel="bibliography" /><link href="97" rel="bibliography" /><link href="99" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>,<a class="sup">3</a>]</sup></citation>。在声纹特征方面, Google提出了d-vector<citation id="125" type="reference"><link href="101" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>特征, 该特征从DNN网络中最后一层提取激活后的数据, 进行L2正则化后累加, 得到d-vector特征向量。x-vector<citation id="126" type="reference"><link href="103" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>则是从TDNN网络<citation id="127" type="reference"><link href="105" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>中提取的降维特征, 其中TDNN是时延架构, 其输出层能够学习长期特征, 能利用10 s左右的语音材料, 获取用户声纹信息。就目前综合特征的表达, 语谱图逐渐进入了人们的视野, 其能综合时间方向上的频率和语音能量的特点, 形成具有综合表征意义、能代表一个人说话特征的语音频谱图。</p>
                </div>
                <div class="p1">
                    <p id="36">在网络模型方面, 也提出了具有针对性的声纹识别网络结构。学术界探讨较多的有CNN模型和RNN模型, 其中CNN网络模型被用来提取语音深层次的空间特征<citation id="129" type="reference"><link href="107" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>;循环神经网络RNN用来提取语音的时序列特征, 文献<citation id="130" type="reference">[<a class="sup">8</a>,<a class="sup">9</a>]</citation>对RNN网络在声纹识别效果优劣上进行了相关讨论。同时也有论文在海量标注数据下, 使用端到端的方法, 在训练速度和数据有效利用率方面进行了相关的研究<citation id="131" type="reference"><link href="113" rel="bibliography" /><link href="115" rel="bibliography" /><link href="117" rel="bibliography" /><sup>[<a class="sup">10</a>,<a class="sup">11</a>,<a class="sup">12</a>]</sup></citation>。但是由于语音信号复杂, 受环境和信道的影响较大, 仅用CNN提取声纹特征忽略了序列语音原本的序列特性, RNN网络虽然考虑了语音的序列特征, 但由于网络本身的激活函数原因, 在模型训练过程中会产生梯度消失和梯度爆炸的问题, 且不容易达到理想的识别效果。</p>
                </div>
                <div class="p1">
                    <p id="37">目前在声纹识别方面, 学术界对长短期记忆网络LSTM (Long Short Term Memory) <citation id="132" type="reference"><link href="121" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>在声纹识别方面的相关研究较少, 没有将网络长期学习的优势与声纹时序特征进行有效地结合。由于语音中通常包含具有个性声音的空间特征, 和说话语段之间的时序特征, 单独的网络结构无法将两种特征进行提取, 本文尝试将CNN和LSTM结合, 并在声纹识别的数据库中进行了有效验证, 得到语谱图的空间特征和时序特征对模型效果均有影响, CNN-LSTM网络的识别准确率和损失值均优于CNN和LSTM两个单独的网络。</p>
                </div>
                <h3 id="38" name="38" class="anchor-tag"><b>1 深度学习网络架构</b></h3>
                <h4 class="anchor-tag" id="39" name="39"><b>1.1 卷积神经网络 (CNN</b>) </h4>
                <div class="p1">
                    <p id="40">CNN是深度学习中应用广泛的网络框架之一, 由LeCun于2014年提出的一种前馈神经网络。该网络在模式分类领域应用尤为突出, 避免了前期对图像的复杂预处理过程, 同样以语谱图作为语音数据输入网络, 也可减少对语谱图的处理操作过程。</p>
                </div>
                <div class="p1">
                    <p id="41">CNN网络由两部分组成, 分别是卷积层和池化层。卷积层神经元之间进行不完全连接, 同样也使得网络拥有局部感受野, 局部感受野在输入的图像上交叉移动, 完成整幅图像的输入, 构建特征的第一个隐藏层。池化是通过减少卷积层之间的连接, 来降低运算复杂度。池化方法有多重多样, 常用的池化方法有最大值池化方法和平均值池化方法。由于输入图像大小与卷积核相比较大, 每次的卷积核移动都会产生数量较多的参数, 其中包括权值和偏置。卷积神经网络中权值和偏置采用共享方法, 使网络自由参数的个数减少, 加快网络的计算过程, 减少存储空间的占用。</p>
                </div>
                <div class="p1">
                    <p id="42">CNN网络对图像特征有着很强的学习能力, 上述共享权重的方法能够减少很大的计算量。对于由语音生成的语谱图也能过图像的方式学习具有个人特性的语音特征, 并进行建模。</p>
                </div>
                <h4 class="anchor-tag" id="43" name="43"><b>1.2 长短期记忆网络</b></h4>
                <div class="p1">
                    <p id="44">长短期记忆网络 (LSTM) 是循环神经网络 (RNN) 的一种升级网络, 通过记忆单元解决了学习长期依赖的问题, 根据之前的信息状态推断后续的信息状态, 进而建立前后信息之间的联系。该网络由Hochreiter &amp; Schmidhuber (1997) 提出, 并在近期由Alex Graves进行了改进和推广。LSTM的经典之处在于通过设计避免了长期依赖问题, 但每个重复单元模块中的设计却精致巧妙。相比普通的RNN, LSTM多出三个“门”结构, 分别为忘记门、输入门、输出门, 对输入的信息进行不同的处理。该设计方案解决了RNN网络中梯度消失的问题。</p>
                </div>
                <div class="p1">
                    <p id="45">LSTM细胞中包含1个或多个细胞核, 用来表示单元的当前状态, 上述“三个门”输出分别连接乘法单元, 来控制状态变化。LSTM细胞之间的分别接受不同时刻的特征输入, 通过细胞计算后, 对应输出, 其关键为前后连接的神经单元之间的输入关系, 在接受该时刻的输入的同时, 也接受上一时刻信息的输入。具体LSTM网络时序关系如图1所示。</p>
                </div>
                <div class="area_img" id="46">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201904027_046.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 LSTM时序关系图" src="Detail/GetImg?filename=images/JYRJ201904027_046.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 LSTM时序关系图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201904027_046.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h3 id="47" name="47" class="anchor-tag"><b>2 基于CNN-LSTM的声纹识别网络</b></h3>
                <h4 class="anchor-tag" id="48" name="48"><b>2.1 CNN-LSTM网络模型</b></h4>
                <div class="p1">
                    <p id="49">CNN与LSTM均是深度学习中使用的主流算法, 但对于处理不同类型的数据也各有所长。CNN擅长提取数据局部特征, 作用于空间上的抽象及泛化, 能够在空间维度上提取表征能力强的高层特征, LSTM网络能够扩展时间特征, 处理具有先后顺序特征的数据信息。语音转换为语谱图的数据, 以图片的形式输入网络, 需要考虑其空间上的特征联系, 也要考虑时间维度上关联信息。基于以上特点, 本文结合CNN网络及LSTM网络的特点, 采用网络串联的方式, 对两个网络进行结合。得到本文使用的CNN-LSTM和LSTM-CNN网络模型, 充分利用了两个网络空间、时间的表征能力, 构建模型如图2所示。</p>
                </div>
                <div class="area_img" id="50">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201904027_050.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 CNN-LSTM声纹识别网络结构图" src="Detail/GetImg?filename=images/JYRJ201904027_050.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 CNN-LSTM声纹识别网络结构图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201904027_050.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="51" name="51"><b>2.2 网络模型的训练</b></h4>
                <div class="p1">
                    <p id="52">本文采取网络结构串联的方式, 将CNN与LSTM网络连接, 形成CNN-LSTM网络模型。由于语谱图可以反映说话人在各个时刻语音频谱随时间的变化, 不同人的语谱图中包含个性的说话人信息, 且语谱图作为图片的形式, 输入深度学习网络, 通过CNN网络能够更好地提取高表征形式的特征, 故将原始语音通过分帧加窗及快速傅里叶变换后, 得到可以送入网络的语谱图, 其大小为106×80的3通道彩色图像, 即输入网络的数据维度为106×80×3。本文的实验中说话人个数为10, 说话人标签以独热编码的形式进行处理, 以矩阵的形式输入网络。</p>
                </div>
                <div class="p1">
                    <p id="53">本文所搭建的模型在训练前期先随机断开20%的神经元连接, 防止由于数据维度多, 网络层数少而产生过拟合现象。CNN网络的卷积核数为20, 卷积核大小为3×3, 通常较小的卷积核能够对数据特征识别更加细微, 且计算量较少。卷积层的激活函数选用relu。池化层大小为4×4, 池化方法选择最大池化, 即选择4×4范围内最大的数值作为新的池化层特征数据。经过卷积池化后的数据送入LSTM网络进行时序特征提取, 由于LSTM为循环神经网络, 为了防止网络内对数据的过分学习, 在循环的神经单元内部及循环之间分别加入Dropout和Recurrent_dropout, 分别对同一LTSM单元中神经元之间的连接和不同循环LSTM单元之间的连接进行一定比例暂时断开, 本文进行断开的比例为0.2, 即有20%的内部神经元和外部循环单元进行断开。最后接入网络的是Softmax全连接层, 对说话人身份进行识别, 实验中的人数为10, 故分类个数为10, 表1为上述结构参数设置的形象化表达。</p>
                </div>
                <div class="area_img" id="54">
                    <p class="img_tit"><b>表1 CNN-LSTM网络结构参数设置表</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="54" border="1"><tr><td><br />Relu激活函数</td></tr><tr><td><br />Dense (10) </td></tr><tr><td><br />Recurrent_dropout (0.2) </td></tr><tr><td><br />Dropout (0.2) </td></tr><tr><td><br />LSTM (25) </td></tr><tr><td><br />最大池化 (4) </td></tr><tr><td><br />Con1D (20, 3) </td></tr><tr><td><br />Dropout (0.2) </td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h3 id="55" name="55" class="anchor-tag"><b>3 实 验</b></h3>
                <h4 class="anchor-tag" id="56" name="56"><b>3.1 实验平台</b></h4>
                <div class="p1">
                    <p id="57">本实验基于Python的深度学习框架TensorFlow<citation id="133" type="reference"><link href="121" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>环境下进行。实验环境如下:</p>
                </div>
                <div class="p1">
                    <p id="58"> (1) 处理器:Inter (R) Xeon (R) CPU @2.20GHz。</p>
                </div>
                <div class="p1">
                    <p id="59"> (2) 安装内存:32.0 GB。</p>
                </div>
                <div class="p1">
                    <p id="60"> (3) 操作系统:Windows 7旗舰版64位操作系统。</p>
                </div>
                <div class="p1">
                    <p id="61">为了评估本文网络模型的有效性, 使用CNN-LSTM网络进行声纹识别实验, 实验数据采用上述同一数据集。</p>
                </div>
                <h4 class="anchor-tag" id="62" name="62"><b>3.2 语音数据集和评价标准</b></h4>
                <div class="p1">
                    <p id="63">实验采用的语音数据集为来自Open Speech and Language Resources的Free ST Chinese Mandarin Corpus<citation id="134" type="reference"><link href="123" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>, 由Surfingtech提供的免费中文普通话语料库, 其中包含855位说话人, 每人包含120个语音片段, 总计102 600个片段。语音采样频率为16 000 Hz。语料库中的语音片段按照8∶2的比例对训练集和测试集进行划分, 其中每人的语音片段中随机抽取80%部分作为模型的训练集, 训练个数为96个, 剩余20%部分24个作为验证测试集, 对模型的准确率等性能进行验证评价, 分别对比相同训练集下不同迭代次数对准确率及损失函数的影响。</p>
                </div>
                <div class="p1">
                    <p id="64">本文采用准确度ACC (accuracy) 和损失函数 (Loss) 作为评价标准, 其中实验中使用对数损失函数。计算方法如下所示:</p>
                </div>
                <div class="p1">
                    <p id="65"><mathml id="66"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>A</mi><mi>C</mi><mi>C</mi><mo>=</mo><mfrac><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>Τ</mi></mstyle><mi>Ρ</mi><mi>i</mi></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mo stretchy="false"> (</mo></mstyle><mi>Τ</mi><mi>Ρ</mi><mi>i</mi><mo>+</mo><mi>F</mi><mi>Ν</mi><mi>i</mi><mo stretchy="false">) </mo></mrow></mfrac></mrow></math></mathml>      (1) </p>
                </div>
                <div class="p1">
                    <p id="67"><i>Loss</i>=<i>L</i> (<i>Y</i>, <i>P</i> (<i>Y</i>|<i>X</i>) ) =-log <i>P</i> (<i>Y</i>|<i>X</i>)      (2) </p>
                </div>
                <div class="p1">
                    <p id="68">式中:<i>n</i>表示说话人数目;<i>P</i><sub><i>i</i></sub>表示第<i>i</i>个说话人的精确度;<i>TP</i><sub><i>i</i></sub>、<i>FN</i><sub><i>i</i></sub>分别表示第<i>i</i>个说话人中正确分类的数目和错误分类的数目。<i>Y</i>表示类别正确的分类, <i>P</i> (<i>Y</i>|<i>X</i>) 表示正确分类的概率, <i>Loss</i>表示为指定分类<i>Y</i>的情况下, 概率越大, 样本与目标值越接近, 则损失越小。</p>
                </div>
                <h4 class="anchor-tag" id="69" name="69"><b>3.3 实验结果及分析</b></h4>
                <div class="p1">
                    <p id="70">本文实验中采用10个说话人的每人120个语音片段, 由于选取的语音片段时常在3～4 s范围内, 为了统一语谱图大小, 将语音片段中不足4 s的以留白方式处理, 使其统一至4 s, 然后通过傅里叶变化将语音的时序信息、频率信息和语音数据能量绘制成106 dpi×80 dpi大小的语谱图, 作为网络模型的输入数据。</p>
                </div>
                <div class="p1">
                    <p id="71">本文通过对空间特征和时序特征提取, 构建了CNN-LSTM模型, 实验中设置的迭代次数为20次, 为了验证本文的CNN-LSTM模型对声纹识别的有效性和鲁棒性, 实验对比了目前在深度学习领域取得优异成果的CNN网络和LSTM网络, 二者分别是模型中的单独部分与softmax分类器相结合组成的两个网络模型。</p>
                </div>
                <div class="p1">
                    <p id="72">图3为实验结果图, 分别为CNN-LSTM、CNN、LSTM在测试集中的准确率和损失函数变化。</p>
                </div>
                <div class="area_img" id="73">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201904027_073.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 CNN-LSTM模型及LSTM和CNN测试集准确率变化图" src="Detail/GetImg?filename=images/JYRJ201904027_073.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 CNN-LSTM模型及LSTM和CNN测试集准确率变化图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201904027_073.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="74">由图3可知, 在测试过程中CNN-LSTM网络的准确率图像包含CNN和LSTM网络, 即说明CNN-LSTM网络对于声纹识别的准确率基本高于其他两个网络。CNN-LSTM网络的准确率随着迭代次数平稳上升, 但CNN和LSTM网络在上升过程中波动较大, 表明这两种模型对于数据的学习平稳性能低于CNN-LSMT网络。针对CNN和LSTM网络分析, LSTM网络的准确率明显高于CNN网络, 表明语谱图序列中时序列的特征提取性能LSTM网络高于CNN网络, 且表明语谱图中具有有效的时序列特征影响声纹识别结果。实验中测试集中CNN-LSTM准确率96.05% (训练集数据只表示最优结果) , 测试准确率95.42%。</p>
                </div>
                <div class="p1">
                    <p id="75">图4为CNN-LSTM和CNN、LSTM网络在测试时的损失函数变化图, 由图4可以看出, CNN-LSTM网络一直处于下降趋势, 且变化过程平稳。最终达到训练损失函数值0.086 9, 测试损失函数0.097 3的低值。说明CNN-LSTM网络的鲁棒性较高。CNN和LSTM网络的损失函数值在下降到约0.3左右后变化缓慢趋于平稳, 且CNN的损失函数在下降过程中抖动频繁, 表明该模型不具有较稳定的鲁棒性能, 表2为CNN-LSTM及CNN、LSTM网络在训练和测试过程中准确率 (ACC) 最大值和损失函数 (Loss) 最小值的比较。</p>
                </div>
                <div class="area_img" id="76">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201904027_076.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 CNN-LSTM模型及LSTM和CNN测试集损失值变化图" src="Detail/GetImg?filename=images/JYRJ201904027_076.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 CNN-LSTM模型及LSTM和CNN测试集损失值变化图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201904027_076.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="77">
                    <p class="img_tit"><b>表2 CNN、LSTM和CNN-LSTM网络训练测试最值</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="77" border="1"><tr><td><br />网络结构</td><td>训练阶段<br />ACC<br />最大值</td><td>测试阶段<br />ACC<br />最大值</td><td>训练阶段<br />Loss<br />最小值</td><td>测试阶段<br />Loss<br />最小值</td></tr><tr><td><br />CNN</td><td>89.65%</td><td>87.48%</td><td>0.273 4</td><td>0.276 6</td></tr><tr><td><br />LSTM</td><td>92.33%</td><td>91.57%</td><td>0.241 7</td><td>0.251 4</td></tr><tr><td><br />CNN-LSTM</td><td>96.05%</td><td>95.42%</td><td>0.086 9</td><td>0.097 3</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="78">如表2所示, CNN-LSTM网络在训练和测试时准确率和损失函数均优于其他两个网络结构下的数值。更加印证了语音特征中包含空间特征和时序特征, 单一特征对声纹识别影响较大, 无法满足实际使用。</p>
                </div>
                <div class="p1">
                    <p id="79">目前参考文献范围内, 基于深度神经网络DNN (Deep Neural Network) 在声纹识别应用中取得令人瞩目的成绩, 在相同数据集下, 采用DNN网络进行对比实验, 由于DNN网络结构深, 对特征进行充分学习和网络参数进行完全训练需要大量的循环迭代次数。实验迭代结果如表3所示, 表示迭代次数中不同网络测试集中准确率结果。</p>
                </div>
                <div class="area_img" id="80">
                    <p class="img_tit"><b>表3 CNN、LSTM、DNN和CNN-LSTM网络迭代次数对应准确率表</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="80" border="1"><tr><td><br />迭代次数</td><td>CNN/%</td><td>LSTM/%</td><td>DNN/%</td><td>CNN-LSTM/%</td></tr><tr><td><br />5</td><td>83.19</td><td>89.88</td><td>78.96</td><td>94.18</td></tr><tr><td><br />10</td><td>82.55</td><td>90.21</td><td>80.15</td><td>93.83</td></tr><tr><td><br />15</td><td>86.27</td><td>90.75</td><td>81.98</td><td>95.33</td></tr><tr><td><br />20</td><td>86.01</td><td>91.53</td><td>83.53</td><td>95.42</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="81">如表3所示, 在迭代次数较少时, DNN网络在以上几个网络中表现性能并不凸显, 这受到网络深层结构和参数设置的限制。由于DNN网络在20次迭代中, 准确率成逐渐上升趋势, 但没有趋于平缓, 进行实验后, 发现迭代次数达到300左右时, 准确率趋于平稳并达到94.15%。</p>
                </div>
                <div class="p1">
                    <p id="82">本文设计的CNN-LSTM网络能够在较少次数的迭代中达到95.42%的准确率, 从时间效率和准确率上均优于现有DNN网络, 故更加验证了本文提出的基于语音的空间特征和时序特征相结合的CNN-LSTM网络的有效性。</p>
                </div>
                <h3 id="83" name="83" class="anchor-tag"><b>4 结 语</b></h3>
                <div class="p1">
                    <p id="84">通过对比CNN-LSTM和CNN、LSTM网络模型, 对声纹识别进行测试, 发现CNN-LSTM网络模型能够很好地对语音空间特征及时间特征进行学习, 对说话人身份识别认证有着较高的准确率。达到了95.12%的准确率和0.097 3的损失低值。通过与CNN、LSTM及DNN网络进行对比实验, 验证了该模型在声纹识别方向的优势。本文在传统声纹识别的基础上, 提出了新的高准确率识别方法, 为声纹识别实际应用提供了新的思路及方法。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="95">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14110700296012&amp;v=MTQ5MDY5Rlp1SUpESDA3b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcnpJSkYwWGJoVT1OaWZPZmJLOEg5RE1xSQ==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> Schmidhuber J. Deep learning in neural networks: an overview [J]. Neural Networks, 2014, 61 (3) : 85-94.
                            </a>
                        </p>
                        <p id="97">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Applying convolutional neural networks concepts to hybrid NN-HMM model for speech recognition">

                                <b>[2]</b> Abdel-Hamid O, Mohamed A R, Jiang H, et al. Applying Convolutional Neural Networks concepts to hybrid NN-HMM model for speech recognition[C]//2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) . IEEE, 2012: 4277-4280.
                            </a>
                        </p>
                        <p id="99">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Very deep convolutional networks for large-scale image recognition">

                                <b>[3]</b> Simonyan K, Zisserman A. Very Deep Convolutional Networks for LargeScale Image Recognition [J]. Computer Science, 2014, 13 (2) : 120-131.
                            </a>
                        </p>
                        <p id="101">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep neural networks for small footprint text-dependent speaker verification">

                                <b>[4]</b> Variani E, Lei X, Mcdermott E, et al.Deep neural networks for small footprint text-dependent speaker verification[C]//2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) . IEEE, 2014.
                            </a>
                        </p>
                        <p id="103">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep Neural Network Embeddings for Text-Independent Speaker Verification">

                                <b>[5]</b> Snyder D, Garcia-Romero D, Povey D, et al. Deep Neural Network Embeddings for Text-Independent Speaker Verification[C]//Proc. InterSpeech 2017:999-1003.
                            </a>
                        </p>
                        <p id="105">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Phoneme recognition using time-delay neural networks">

                                <b>[6]</b> Waibel A, Hanazawa T, Hinton G, et al.Phoneme recognition using time-delay neural networks[J]. IEEE transactions on acoustics, speech, and signal processing, 1989, 37 (3) : 328-339
                            </a>
                        </p>
                        <p id="107">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM2F9583DBCBE0076E2252D2DAE8A39D9E&amp;v=MTAyMjE5VEVyUHMzRjVsNkRIdyt5V01SNkRwL1BIMlczV2M5Q0xHZE1iUHFDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZCckxVMDV0dGh3THU4eEs4PU5pZklZN0hPRg==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b> Abdel-Hamid O, Mohamed A R, Jiang H, et al. Convolutional Neural Networks for Speech Recognition[J]. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2014, 22 (10) :1533-1545.
                            </a>
                        </p>
                        <p id="109">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSYJ201901036&amp;v=MjQyMjhadVp0Rnl6bVVMdlBMejdTWkxHNEg5ak1ybzlHWW9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWY=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b> 余玲飞, 刘强. 基于深度循环网络的声纹识别方法研究及应用[J]. 计算机应用研究, 2019, 36 (1) :153-158.
                            </a>
                        </p>
                        <p id="111">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep Neural Network based Text-Dependent Speaker Recognition: Preliminary Results">

                                <b>[9]</b> Bhattacharya G, Alam J, Stafylakis T, et al. Deep Neural Network based Text-Dependent Speaker Recognition: Preliminary Results[C]//Odyssey 2016.21-24 Jun 2016, Bilbao, Spain.
                            </a>
                        </p>
                        <p id="113">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=&amp;quot;End-to-end text-dependent speaker verification,&amp;quot;">

                                <b>[10]</b> Heigold G, Moreno I, Bengio S, et al. End-to-End Text-Dependent Speaker Verification[C]//Acoustics, Speech and Signal Processing (ICASSP) , 2016 IEEE International Conference on. IEEE, 2016.
                            </a>
                        </p>
                        <p id="115">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Attention-Based Models for Text-Dependent Speaker Verification">

                                <b>[11]</b> Chowdhury F A R R, Wang Q, Moreno I L, et al. Attention-Based Models for Text-Dependent Speaker Verification[C]//ICASSP 2018—2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) . IEEE, 2018.
                            </a>
                        </p>
                        <p id="117">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=End-to-End Text-Independent Speaker Verification with Triplet Loss on Short Utterances">

                                <b>[12]</b> Zhang C, Koishida K. End-to-End Text-Independent Speaker Verification with Triplet Loss on Short Utterances[C]//Interspeech, 2017.
                            </a>
                        </p>
                        <p id="119">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=LSTM:A search space odyssey">

                                <b>[13]</b> Greff K, Srivastava R K, Koutník, Jan, et al. LSTM: A Search Space Odyssey[J]. IEEE Transactions on Neural Networks &amp; Learning Systems, 2015, 28 (10) :2222-2232.
                            </a>
                        </p>
                        <p id="121">
                            <a id="bibliography_14" >
                                    <b>[14]</b>
                                 TensorFlow.谷歌深度学习框架[EB/OL]. 2018. https://www.tensorflow.org/?hl=zh-cn.
                            </a>
                        </p>
                        <p id="123">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Free ST Chinese Mandarin Corpus">

                                <b>[15]</b> Free ST Chinese Mandarin Corpus[DB/OL]. 2016. http://www.openslr.org/38/.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JYRJ201904027" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201904027&amp;v=MjMyMDFyQ1VSN3FmWnVadEZ5em1VTHZQTHpUWlpMRzRIOWpNcTQ5SFk0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0Y=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="1" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
