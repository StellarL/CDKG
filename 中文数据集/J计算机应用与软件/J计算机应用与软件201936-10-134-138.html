<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637135556195158750%26DBCODE%3dCJFD%26TABLEName%3dCJFDTEMP%26FileName%3dJYRJ201910025%26RESULT%3d1%26SIGN%3d%252bJ2KDGBNUjjK3mOgXzwXDd8EUZ8%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JYRJ201910025&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JYRJ201910025&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201910025&amp;v=MjAwODJDVVI3cWZadVp0RmlEaFViM0FMelRaWkxHNEg5ak5yNDlIWVlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnI=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#33" data-title="&lt;b&gt;0 引 言&lt;/b&gt; "><b>0 引 言</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#37" data-title="&lt;b&gt;1 间隔损失函数&lt;/b&gt; "><b>1 间隔损失函数</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#38" data-title="&lt;b&gt;1.1 问题描述&lt;/b&gt;"><b>1.1 问题描述</b></a></li>
                                                <li><a href="#45" data-title="&lt;b&gt;1.2 损失函数&lt;/b&gt;"><b>1.2 损失函数</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#50" data-title="&lt;b&gt;2 正则化的半监督度量学习&lt;/b&gt; "><b>2 正则化的半监督度量学习</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#51" data-title="&lt;b&gt;2.1 半监督假设正则项&lt;/b&gt;"><b>2.1 半监督假设正则项</b></a></li>
                                                <li><a href="#61" data-title="&lt;b&gt;2.2 稀疏正则项&lt;/b&gt;"><b>2.2 稀疏正则项</b></a></li>
                                                <li><a href="#67" data-title="&lt;b&gt;2.3 问题优化&lt;/b&gt;"><b>2.3 问题优化</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#87" data-title="&lt;b&gt;3 模型求解&lt;/b&gt; "><b>3 模型求解</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#111" data-title="&lt;b&gt;4 实 验&lt;/b&gt; "><b>4 实 验</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#112" data-title="&lt;b&gt;4.1 实验设置&lt;/b&gt;"><b>4.1 实验设置</b></a></li>
                                                <li><a href="#115" data-title="&lt;b&gt;4.2 实验结果&lt;/b&gt;"><b>4.2 实验结果</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#123" data-title="&lt;b&gt;5 结 语&lt;/b&gt; "><b>5 结 语</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#40" data-title="图1 三样本组约束示意图">图1 三样本组约束示意图</a></li>
                                                <li><a href="#117" data-title="&lt;b&gt;表1 数据集信息&lt;/b&gt;"><b>表1 数据集信息</b></a></li>
                                                <li><a href="#120" data-title="图2 算法组1的错误率比较">图2 算法组1的错误率比较</a></li>
                                                <li><a href="#121" data-title="图3 算法组2的错误率比较">图3 算法组2的错误率比较</a></li>
                                                <li><a href="#122" data-title="图4 算法组3的错误率比较">图4 算法组3的错误率比较</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" title=" 梅江元.基于马氏距离的度量学习算法研究及应用[D].黑龙江:哈尔滨工业大学航天学院,2016." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CDFD&amp;filename=1016739755.nh&amp;v=MDQ0ODU3cWZadVp0RmlEaFViM0FWRjI2R0xTN0Y5YkpxcEViUElRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         梅江元.基于马氏距离的度量学习算法研究及应用[D].黑龙江:哈尔滨工业大学航天学院,2016.
                                    </a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" title=" 吕璐,蔡晓东,曾燕,等.一种基于融合深度卷积神经网络与度量学习的人脸识别方法[J].现代电子技术,2018,41(9):58-59." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XDDJ201809014&amp;v=MzIxNTBadEZpRGhVYjNBUFNuUFpMRzRIOW5NcG85RVlJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnU=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         吕璐,蔡晓东,曾燕,等.一种基于融合深度卷积神经网络与度量学习的人脸识别方法[J].现代电子技术,2018,41(9):58-59.
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title=" Guillaumin M,Verbeek J,Schmid C.Is that you?Metric learning approaches for identification[C]//The 12th IEEE International Conference on Computer Vision.IEEE,2009:498-505." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Is that you?Metric learning approaches for face identification">
                                        <b>[3]</b>
                                         Guillaumin M,Verbeek J,Schmid C.Is that you?Metric learning approaches for identification[C]//The 12th IEEE International Conference on Computer Vision.IEEE,2009:498-505.
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" title=" Schroff F,Kalenichenko D,Philbin J.FaceNet:A unified embedding for face recognition and clustering[EB].arXiv preprint arXiv:1503.03832,2015." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=FaceNet:A unified embedding for face recognition and clustering[EB]">
                                        <b>[4]</b>
                                         Schroff F,Kalenichenko D,Philbin J.FaceNet:A unified embedding for face recognition and clustering[EB].arXiv preprint arXiv:1503.03832,2015.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" title=" Zhang R M,Lin L,Zhang R,et al.Bit-scalable deep hashing with regularized similarity learning for image retrieval and person re-identification[J].IEEE Transactions on Image Processing,2015,24(12):4766-4779." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Bitscalable deep hashing with regularized similarity learning for image retrieval and person re-identification">
                                        <b>[5]</b>
                                         Zhang R M,Lin L,Zhang R,et al.Bit-scalable deep hashing with regularized similarity learning for image retrieval and person re-identification[J].IEEE Transactions on Image Processing,2015,24(12):4766-4779.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" title=" Bhattarai B,Sharma G,Jurie F.CP-mtML:Coupled projection multi-task metric learning for large scale face retrieval[EB].arXiv preprint arXiv:1604.02975,2016." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=CP-mtML:Coupled projection multi-task metric learning for large scale face retrieval[EB]">
                                        <b>[6]</b>
                                         Bhattarai B,Sharma G,Jurie F.CP-mtML:Coupled projection multi-task metric learning for large scale face retrieval[EB].arXiv preprint arXiv:1604.02975,2016.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" title=" 阎少梅.基于度量学习的图像分类算法研究[D].安徽:安徽大学电子信息工程学院,2018." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=1018087297.nh&amp;v=MDc0MTR6cXFCdEdGckNVUjdxZlp1WnRGaURoVWIzQVZGMjZGck93R2RQRnFKRWJQSVFLREg4NHZSNFQ2ajU0TzM=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                         阎少梅.基于度量学习的图像分类算法研究[D].安徽:安徽大学电子信息工程学院,2018.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" title=" Zhu X.Semi-supervised learning literature suvery[D].Computer Science TR 1530,University of Wisconsin Madison,2008." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Semi-supervised learning literature survey">
                                        <b>[8]</b>
                                         Zhu X.Semi-supervised learning literature suvery[D].Computer Science TR 1530,University of Wisconsin Madison,2008.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" title=" Joachims T.Transductive inference for text classification using support vector macines[C]//Proceedings of the Sixteenth International Conference on Machine Learning.Morgan Kaufmann Publishers Inc.1999." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Transductive inference for text classification using support vector machines">
                                        <b>[9]</b>
                                         Joachims T.Transductive inference for text classification using support vector macines[C]//Proceedings of the Sixteenth International Conference on Machine Learning.Morgan Kaufmann Publishers Inc.1999.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_10" title=" Hady M F A,Schwenker F.Semi-supervised learning[J].Intelligent Systems Reference Library,2013,49(2):215-239." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Semi-supervised Learning">
                                        <b>[10]</b>
                                         Hady M F A,Schwenker F.Semi-supervised learning[J].Intelligent Systems Reference Library,2013,49(2):215-239.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_11" title=" Chapelle O,Zien A.Semi-supervised classification by low density separation[C]//Tenth International Workshop on Artificial Intelligence &amp;amp; Statistics.2004." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Semi-supervised classification by low density separation">
                                        <b>[11]</b>
                                         Chapelle O,Zien A.Semi-supervised classification by low density separation[C]//Tenth International Workshop on Artificial Intelligence &amp;amp; Statistics.2004.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_12" title=" Tibshirani R.Regression shrinkage and selection via the lasso[J].Journal of the Royal Statistical Society Series B(Methodological),1996,58(1):267-288." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJST&amp;filename=SJST14120603924782&amp;v=MTg4OTZLRm9XYUJvPU5pZlllcks4SDlQTXFZOUdiZWtMQzNRN29CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaW5sVXJ6Sg==&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[12]</b>
                                         Tibshirani R.Regression shrinkage and selection via the lasso[J].Journal of the Royal Statistical Society Series B(Methodological),1996,58(1):267-288.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_13" title=" Weinberger K Q.Distance metric learning for large margin nearest neighbor classification[J].The Journal of Machine Learning Research,2009,10:207-244." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Distance metric learning for large margin nearest neighbor classification">
                                        <b>[13]</b>
                                         Weinberger K Q.Distance metric learning for large margin nearest neighbor classification[J].The Journal of Machine Learning Research,2009,10:207-244.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_14" title=" Yang L,Jin R,Mummert L,et al.A boosting framework for visuality-preserving distance metric learning and its application to medical image retrieval[J].IEEE Transactions on Pattern Analysis &amp;amp; Machine Intelligence,2010,32(1):30." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Boosting Framework for Visuality-Preserving Distance Metric Learning and Its Application to Medical Image Retrieval">
                                        <b>[14]</b>
                                         Yang L,Jin R,Mummert L,et al.A boosting framework for visuality-preserving distance metric learning and its application to medical image retrieval[J].IEEE Transactions on Pattern Analysis &amp;amp; Machine Intelligence,2010,32(1):30.
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_15" title=" Wang Q Y,Yuen P C,Feng G,et al.Semi-supervised metric learning via topology preserving multiple semi-supervised assumptions[J].Pattern Recognition,2013,46:2580-2582." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13050900089671&amp;v=MTY3MDRUTW53WmVadEZpbmxVcnpKS0ZvV2FCbz1OaWZPZmJLN0h0VE1wbzlGWk9NR0NuczRvQk1UNlQ0UFFIL2lyUmRHZXJxUQ==&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[15]</b>
                                         Wang Q Y,Yuen P C,Feng G,et al.Semi-supervised metric learning via topology preserving multiple semi-supervised assumptions[J].Pattern Recognition,2013,46:2580-2582.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JYRJ" target="_blank">计算机应用与软件</a>
                2019,36(10),134-138 DOI:10.3969/j.issn.1000-386x.2019.10.024            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于半监督假设的半监督稀疏度量学习</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E5%80%A9%E5%BD%B1&amp;code=40808143&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王倩影</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%8E%E7%82%9C&amp;code=41366555&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">李炜</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%B2%B3%E5%8C%97%E7%BB%8F%E8%B4%B8%E5%A4%A7%E5%AD%A6%E6%95%B0%E5%AD%A6%E4%B8%8E%E7%BB%9F%E8%AE%A1%E5%AD%A6%E5%AD%A6%E9%99%A2&amp;code=0053585&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">河北经贸大学数学与统计学学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>传统的有监督度量学习算法没有利用大量存在的无标记样本,且得到的度量矩阵复杂,难以了解不同原始特征的重要程度。针对这些情况,提出基于半监督假设的半监督稀疏度量学习算法。根据三样本组约束建立间隔损失函数;基于平滑假设、聚类假设、流形假设这三个半监督假设建立半监督正则项,并利用L<sub>1</sub>范数建立稀疏正则项;利用梯度下降法求解目标函数。实验结果表明,该算法学习得到的度量能有效地使不同类别的样本间距离增大,度量矩阵具有稀疏性,分界面穿过低密度区域,该算法在UCI的样本数据集上具有良好的分类准确性。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">度量学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">半监督学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%81%87%E8%AE%BE&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">半监督假设;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%A8%80%E7%96%8F&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">稀疏;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    王倩影，博士，主研领域:模式识别，数据挖掘。;
                                </span>
                                <span>
                                    李炜，硕士生。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-01-14</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金青年科学基金项目(61602148);</span>
                                <span>2018年度河北经贸大学研究生创新计划项目;</span>
                    </p>
            </div>
                    <h1><b>SEMI-SUPERVISED SPARSE METRIC LEARNING BASED ON SEMI-SUPERVISED ASSUMPTION</b></h1>
                    <h2>
                    <span>Wang Qianying</span>
                    <span>Li Wei</span>
            </h2>
                    <h2>
                    <span>College of Mathematics and Statistics, Hebei University of Economics and Business</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Traditional supervised metric learning algorithms do not take advantage of the large number of unlabeled samples, and the resulting metric matrix is complex. So it is difficult to understand the importance of different input features. To solve these problems, we proposed a semi-supervised sparse metric learning algorithm based on semi-supervised assumption. The interval loss function was established according to the triplet constraints. Then, we established a semi-supervised regularization based on the three semi-supervised assumptions: smoothness assumption, cluster assumption and manifold assumption, and established a sparse regularization term by using L<sub>1</sub> norm. The gradient descent method was used to solve the objective function. The experimental results show that the metrics learned by the algorithm can effectively increase the distance between different types of samples, and the metrics matrix is sparse. The interface crosses the low-density region. The algorithm has good classification accuracy on the UCI sample.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Metric%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Metric learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Semi-supervised%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Semi-supervised learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Semi-supervised%20assumption&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Semi-supervised assumption;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Sparse&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Sparse;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2019-01-14</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="33" name="33" class="anchor-tag"><b>0 引 言</b></h3>
                <div class="p1">
                    <p id="34">度量学习<citation id="144" type="reference"><link href="3" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>的本质是学习一个映射空间,使得同类样本间的距离更近,异类样本间的距离更远。近年来,度量学习在众多领域得到了广泛应用,如人脸识别<citation id="146" type="reference"><link href="5" rel="bibliography" /><link href="7" rel="bibliography" /><link href="9" rel="bibliography" /><sup>[<a class="sup">2</a>,<a class="sup">3</a>,<a class="sup">4</a>]</sup></citation>、图像检索<citation id="147" type="reference"><link href="11" rel="bibliography" /><link href="13" rel="bibliography" /><link href="15" rel="bibliography" /><sup>[<a class="sup">5</a>,<a class="sup">6</a>,<a class="sup">7</a>]</sup></citation>等。根据不同的训练样本,度量学习可以分为无监督度量学习、有监督度量学习和半监督度量学习。无监督度量学习的训练样本为无标记数据,有监督度量学习的训练样本给定了正负限制的样本对,但没有将无标记样本利用起来。因此人们尝试将大量无标记样本数据加入到有标记样本中一起训练来进行学习,由此产生了半监督度量学习<citation id="145" type="reference"><link href="17" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="35">由于现实应用中存在大量无标记样本,半监督度量学习是当前的一个研究热点。Joachims等<citation id="148" type="reference"><link href="19" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>依据半监督支持向量机(S<sup>3</sup>VM)提出了基于标记切换的组合优化算法,使S<sup>3</sup>VM在数据集上取得了不错的效果。Chapelle等<citation id="150" type="reference"><link href="21" rel="bibliography" /><link href="23" rel="bibliography" /><sup>[<a class="sup">10</a>,<a class="sup">11</a>]</sup></citation>提出了半监督学习有关高维数据的三个假设:光滑假设、聚类假设和流形假设,并据此提出了低密度分割算法,得到了很好的分类效果。现今对半监督度量学习方法的研究只利用了三个半监督假设中的一项或两项,没有一个方法满足所有的三个半监督假设。而且在大数据时代,数据呈现出维度高的特点,常见的度量学习方法基于原始特征产生度量,使得度量矩阵很复杂。利用高维数据的潜在稀疏性建立稀疏正则化模型,可以有效地处理高维数据。文献<citation id="149" type="reference">[<a class="sup">12</a>]</citation>据此提出了基于L<sub>1</sub>正则化的模型lasso。但目前存在的稀疏正则化模型没有结合半监督度量学习中的三个半监督假设,把无标记样本充分利用起来。</p>
                </div>
                <div class="p1">
                    <p id="36">为了充分利用无标记样本,本文从间隔损失函数入手,依据三个半监督假设,建立了半监督假设正则项,并结合稀疏正则项,提出了基于半监督假设的半监督稀疏度量学习算法。最后通过实验验证了本文所提算法的有效性。</p>
                </div>
                <h3 id="37" name="37" class="anchor-tag"><b>1 间隔损失函数</b></h3>
                <h4 class="anchor-tag" id="38" name="38"><b>1.1 问题描述</b></h4>
                <div class="p1">
                    <p id="39">在学习一个度量时,样本对的限制是指两个给定的样本是否在同一类,若在一类则称为一个正约束,若不在一类则称为一个负约束。所要学习的度量是要使得属于同一类的两个样本距离更近,属于不同类的两个样本距离更远。三样本为一组的约束是对样本对约束的拓展。在三样本组约束(<i>x</i><sub><i>i</i></sub>,<i>x</i><sub><i>j</i></sub>,<i>x</i><sub><i>k</i></sub>)中,(<i>x</i><sub><i>i</i></sub>,<i>x</i><sub><i>j</i></sub>)之间的距离要求比(<i>x</i><sub><i>i</i></sub>,<i>x</i><sub><i>k</i></sub>)之间的距离小。因此,若(<i>x</i><sub><i>i</i></sub>,<i>x</i><sub><i>j</i></sub>)是一个正约束,(<i>x</i><sub><i>i</i></sub>,<i>x</i><sub><i>k</i></sub>)是一个负约束,则(<i>x</i><sub><i>i</i></sub>,<i>x</i><sub><i>j</i></sub>,<i>x</i><sub><i>k</i></sub>)就是一个三样本组约束。但反之不成立,即并不能由(<i>x</i><sub><i>i</i></sub>,<i>x</i><sub><i>j</i></sub>)之间的距离比(<i>x</i><sub><i>i</i></sub>,<i>x</i><sub><i>k</i></sub>)之间的距离小,得到(<i>x</i><sub><i>i</i></sub>,<i>x</i><sub><i>j</i></sub>)属于同一类,(<i>x</i><sub><i>i</i></sub>,<i>x</i><sub><i>k</i></sub>)属于不同类的结果。当给定一些三样本组约束时,我们将要学习一个满足如下条件的度量:如图1所示,对每一个三样本组约束学习的度量要使得(<i>x</i><sub><i>i</i></sub>,<i>x</i><sub><i>j</i></sub>)之间的距离小于(<i>x</i><sub><i>i</i></sub>,<i>x</i><sub><i>k</i></sub>)之间的距离。</p>
                </div>
                <div class="area_img" id="40">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201910025_040.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 三样本组约束示意图" src="Detail/GetImg?filename=images/JYRJ201910025_040.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 三样本组约束示意图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201910025_040.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="41">本文中用<i><b>X</b></i>=[<i>x</i><sub>1</sub>,<i>x</i><sub>2</sub>,…,<i>x</i><sub><i>n</i></sub>]表示包含所有样本的矩阵,<i>X</i><sup><i>l</i></sup>表示所有的有标记样本,<i>X</i><sup><i>u</i></sup>表示所有无标记样本。对于马氏距离,<i>D</i><sub><i><b>M</b></i></sub>(<i>x</i><sub><i>i</i></sub>,<i>x</i><sub><i>j</i></sub>)由一个半正定矩阵<i><b>M</b></i>决定,其是通过度量学习后的样本<i>x</i><sub><i>i</i></sub>和样本<i>x</i><sub><i>j</i></sub>之间的距离,<i>D</i><mathml id="125"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi mathvariant="bold-italic">Μ</mi><mn>2</mn></msubsup></mrow></math></mathml>(<i><b>x</b></i><sub><i><b>i</b></i></sub>,<i><b>x</b></i><sub><i><b>j</b></i></sub>)=(<i><b>x</b></i><sub><i><b>i</b></i></sub>-<i><b>x</b></i><sub><i><b>j</b></i></sub>)<sup><b>T</b></sup><i><b>M</b></i>(<i>x</i><sub><i>i</i></sub>-<i>x</i><sub><i>j</i></sub>)表示的是马氏距离的平方。示性矩阵<i><b>Y</b></i>=[<i>y</i><sub><i>ij</i></sub>]<mathml id="126"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mo>,</mo><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mtext>l</mtext></msubsup></mrow></math></mathml>,<i>y</i><sub><i>ij</i></sub>∈{0,1},<i>y</i><sub><i>ij</i></sub>=1表示样本<i>x</i><sub><i>i</i></sub>和样本<i>x</i><sub><i>j</i></sub>属于同一类,<i>y</i><sub><i>ij</i></sub>=0表示样本<i>x</i><sub><i>i</i></sub>和样本<i>x</i><sub><i>j</i></sub>属于不同的类。</p>
                </div>
                <div class="p1">
                    <p id="42">类标信息可以转化成三样本组约束。每个三样本组约束由三个样本(<i>x</i><sub><i>i</i></sub>,<i>x</i><sub><i>j</i></sub>,<i>x</i><sub><i>k</i></sub>)组成,其中,<i>x</i><sub><i>i</i></sub>是所要讨论的样本。希望学习得到这样的距离<i>D</i><sub><i><b>M</b></i></sub>(<i>x</i><sub><i>i</i></sub>,<i>x</i><sub><i>j</i></sub>)和<i>D</i><sub><i><b>M</b></i></sub>(<i>x</i><sub><i>i</i></sub>,<i>x</i><sub><i>k</i></sub>)满足:</p>
                </div>
                <div class="p1">
                    <p id="43" class="code-formula">
                        <mathml id="43"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>φ</mi><mo>=</mo><mo stretchy="false">{</mo><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mi>x</mi><msub><mrow></mrow><mi>j</mi></msub><mo>,</mo><mi>x</mi><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">|</mo><mi>D</mi><msub><mrow></mrow><mi mathvariant="bold-italic">Μ</mi></msub><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mi>x</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo><mo>&lt;</mo><mi>D</mi><msub><mrow></mrow><mi mathvariant="bold-italic">Μ</mi></msub><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mi>x</mi><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">}</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="44">目标是学习一个半正定矩阵<i><b>M</b></i>使与之相对应的距离{<i>D</i><sub><i><b>M</b></i></sub>(<i>x</i><sub><i>i</i></sub>,<i>x</i><sub><i>j</i></sub>)}<mathml id="127"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi mathvariant="bold-italic">i</mi><mo>,</mo><mi mathvariant="bold-italic">j</mi><mo>=</mo><mn>1</mn></mrow><mi mathvariant="bold-italic">l</mi></msubsup></mrow></math></mathml>满足三样本组约束。</p>
                </div>
                <h4 class="anchor-tag" id="45" name="45"><b>1.2 损失函数</b></h4>
                <div class="p1">
                    <p id="46">本文参考LMNN的损失函数,对有标记样本的损失函数定义如下:</p>
                </div>
                <div class="p1">
                    <p id="47" class="code-formula">
                        <mathml id="47"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mi>min</mi></mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>,</mo><mi>j</mi><mo>,</mo><mi>k</mi></mrow><mi>l</mi></munderover><mi>η</mi></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo stretchy="false">(</mo><mn>1</mn><mo>-</mo><mi>y</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>k</mi></mrow></msub><mo stretchy="false">)</mo><mo stretchy="false">[</mo><mn>1</mn><mo>+</mo><mi>D</mi><msubsup><mrow></mrow><mi mathvariant="bold-italic">Μ</mi><mn>2</mn></msubsup><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mi>x</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo><mo>-</mo><mi>D</mi><msubsup><mrow></mrow><mi mathvariant="bold-italic">Μ</mi><mn>2</mn></msubsup><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mi>x</mi><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">]</mo><msub><mrow></mrow><mo>+</mo></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="48">式中:[<i><b>z</b></i>]<sub>+</sub>=<b>max(</b><i><b>z</b></i>,0);<i>η</i><sub><i><b>ij</b></i></sub>∈{0,1}为邻域示性因子,表示样本<i><b>x</b></i><sub><i><b>j</b></i></sub>是否是样本<i><b>x</b></i><sub><i><b>i</b></i></sub>的一个目标近邻,若<i><b>x</b></i><sub><i><b>i</b></i></sub>与<i><b>x</b></i><sub><i><b>j</b></i></sub>是近邻则<i>η</i><sub><i><b>ij</b></i></sub>=1,否则<i>η</i><sub><i><b>ij</b></i></sub>=0;<i><b>l</b></i>为有标记数据样本的个数。对于一个三样本组约束(<i><b>x</b></i><sub><i><b>i</b></i></sub>,<i><b>x</b></i><sub><i><b>j</b></i></sub>,<i><b>x</b></i><sub><i><b>k</b></i></sub>)中的<i><b>x</b></i><sub><i><b>i</b></i></sub>,如果<i><b>x</b></i><sub><i><b>i</b></i></sub>到<i><b>x</b></i><sub><i><b>j</b></i></sub>的距离的平方<i><b>D</b></i><mathml id="128"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi mathvariant="bold-italic">Μ</mi><mn>2</mn></msubsup></mrow></math></mathml>(<i><b>x</b></i><sub><i><b>i</b></i></sub>,<i><b>x</b></i><sub><i><b>j</b></i></sub>)加上一个单位比<i><b>x</b></i><sub><i><b>i</b></i></sub>到<i><b>x</b></i><sub><i><b>k</b></i></sub>距离的平方<i><b>D</b></i><mathml id="129"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi mathvariant="bold-italic">Μ</mi><mn>2</mn></msubsup></mrow></math></mathml>(<i><b>x</b></i><sub><i><b>i</b></i></sub>,<i><b>x</b></i><sub><i><b>k</b></i></sub>)大,说明<i><b>x</b></i><sub><i><b>i</b></i></sub>与<i><b>x</b></i><sub><i><b>k</b></i></sub>之间离得不足够远,则间隔损失函数就会产生惩罚。</p>
                </div>
                <div class="p1">
                    <p id="49">文献<citation id="151" type="reference">[<a class="sup">13</a>,<a class="sup">14</a>]</citation>证明了该损失函数的有效性,但此函数在运用过程中对噪声数据较为敏感,容易出现过拟合现象,并且没有将无标记样本利用起来。为了解决这些问题,引入半监督假设正则项将无标记样本充分利用起来,过拟合通常发生在特征(参数)较多的时候,引入<b>L</b><sub>1</sub>正则项,<b>L</b><sub>1</sub>正则化会产生稀疏解,部分分量会变成0,相当于对原始特征做了特征提取。</p>
                </div>
                <h3 id="50" name="50" class="anchor-tag"><b>2 正则化的半监督度量学习</b></h3>
                <h4 class="anchor-tag" id="51" name="51"><b>2.1 半监督假设正则项</b></h4>
                <div class="p1">
                    <p id="52">数据分布可以由样本及其近邻所反映,因此我们可以通过样本间的相似度以及区域密度来描述样本及其近邻间的关系。若给定样本集<i><b>X</b></i>=[<i>x</i><sub>1</sub>,<i>x</i><sub>2</sub>,…,<i>x</i><sub><i>n</i></sub>],以及与其相对应的相似矩阵<i><b>S</b></i>=[<i>S</i><sub><i>ij</i></sub>],本文根据三个半监督假设来建立正则项。提出的正则项为:</p>
                </div>
                <div class="p1">
                    <p id="53"><mathml id="130"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>r</mi><mi>g</mi><mo stretchy="false">(</mo><mi>Μ</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><mn>4</mn></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>β</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>j</mi><mo>∈</mo><mi>Ν</mi><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></munder><mi>S</mi></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mi>D</mi><msubsup><mrow></mrow><mi mathvariant="bold-italic">Μ</mi><mn>2</mn></msubsup><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mi>x</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo></mrow></math></mathml>      (3)</p>
                </div>
                <div class="p1">
                    <p id="54">式中:</p>
                </div>
                <div class="area_img" id="55">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JYRJ201910025_05500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="57"><i>N</i>(<i>i</i>)是<i>x</i><sub><i>i</i></sub>由欧氏距离确定的邻域点的集合,在正则项中引入的<i>S</i><sub><i>ij</i></sub>是<i>x</i><sub><i>i</i></sub>和<i>x</i><sub><i>j</i></sub>之间的相似度。根据聚类假设,引入密度指标<i>β</i><sub><i>i</i></sub>∈<b>R</b><sup>+</sup>,它是一个有关样本<i>x</i><sub><i>i</i></sub>密度的函数。</p>
                </div>
                <div class="p1">
                    <p id="58">结合间隔损失函数和提出的正则项,我们得到一个新的度量学习方法:</p>
                </div>
                <div class="p1">
                    <p id="59" class="code-formula">
                        <mathml id="59"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mrow><mi>min</mi></mrow><mi>λ</mi><msub><mrow></mrow><mn>1</mn></msub><mfrac><mn>1</mn><mn>4</mn></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>β</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>j</mi><mo>∈</mo><mi>Ν</mi><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></munder><mi>S</mi></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mi>D</mi><msubsup><mrow></mrow><mi mathvariant="bold-italic">Μ</mi><mn>2</mn></msubsup><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mi>x</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo><mo>+</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>i</mi><mi>j</mi><mi>k</mi></mrow></munder><mi>η</mi></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo stretchy="false">(</mo><mn>1</mn><mo>-</mo><mi>y</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>k</mi></mrow></msub><mo stretchy="false">)</mo><mo stretchy="false">[</mo><mn>1</mn><mo>+</mo><mi>D</mi><msubsup><mrow></mrow><mi mathvariant="bold-italic">Μ</mi><mn>2</mn></msubsup><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mi>x</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo><mo>-</mo><mi>D</mi><msubsup><mrow></mrow><mi mathvariant="bold-italic">Μ</mi><mn>2</mn></msubsup><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mi>x</mi><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">]</mo><msub><mrow></mrow><mo>+</mo></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>5</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="60">式中:<i>λ</i><sub>1</sub>用是来调整正则项的权重参数。</p>
                </div>
                <h4 class="anchor-tag" id="61" name="61"><b>2.2 稀疏正则项</b></h4>
                <div class="p1">
                    <p id="62">通常度量学习任务中的特征数量较多,在预测或分类时,难以对特征进行选择,但是如果代入这些特征得到的模型是一个稀疏模型,即只有少数特征对这个模型有贡献,绝大部分特征是没有贡献的,此时我们可以只关注这些对模型有贡献的特征。L<sub>1</sub>正则化有助于生成这样一个稀疏权值矩阵,进而用于特征提取。</p>
                </div>
                <div class="p1">
                    <p id="63">假设半正定矩阵<i><b>M</b></i>的<i>n</i>个特征值记为<mathml id="131"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false">{</mo><mi>e</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">|</mo><mi>i</mi><mo>=</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mo>⋯</mo><mo>,</mo><mi>n</mi><mo stretchy="false">}</mo></mrow></math></mathml>,特征值的L<sub>1</sub>范数就等于<mathml id="132"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><mrow><mo>|</mo><mrow><mi>e</mi><msub><mrow></mrow><mi>k</mi></msub></mrow><mo>|</mo></mrow></mrow></mstyle></mrow></math></mathml>,即为矩阵<i><b>M</b></i>的迹<i>tr</i>(<i><b>M</b></i>)。通过在式(5)中加入特征值的L<sub>1</sub>正则项来得到一个稀疏模型。</p>
                </div>
                <div class="p1">
                    <p id="64">目标函数变为:</p>
                </div>
                <div class="p1">
                    <p id="65" class="code-formula">
                        <mathml id="65"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mrow><mi>min</mi></mrow><mi>λ</mi><msub><mrow></mrow><mn>1</mn></msub><mfrac><mn>1</mn><mn>4</mn></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>β</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>j</mi><mo>∈</mo><mi>Ν</mi><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></munder><mi>S</mi></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mi>D</mi><msubsup><mrow></mrow><mi>Μ</mi><mn>2</mn></msubsup><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mi>x</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo><mo>+</mo><mi>λ</mi><msub><mrow></mrow><mn>2</mn></msub><mi>t</mi><mi>r</mi><mo stretchy="false">(</mo><mi>Μ</mi><mo stretchy="false">)</mo><mo>+</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>i</mi><mi>j</mi><mi>k</mi></mrow></munder><mi>η</mi></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo stretchy="false">(</mo><mn>1</mn><mo>-</mo><mi>y</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>k</mi></mrow></msub><mo stretchy="false">)</mo><mo stretchy="false">[</mo><mn>1</mn><mo>+</mo><mi>D</mi><msubsup><mrow></mrow><mi mathvariant="bold-italic">Μ</mi><mn>2</mn></msubsup><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mi>x</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo><mo>-</mo><mi>D</mi><msubsup><mrow></mrow><mi mathvariant="bold-italic">Μ</mi><mn>2</mn></msubsup><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mi>x</mi><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">]</mo><msub><mrow></mrow><mo>+</mo></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>6</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="66">式中:<i>λ</i><sub>2</sub>用是来调整正则项的权重参数。</p>
                </div>
                <h4 class="anchor-tag" id="67" name="67"><b>2.3 问题优化</b></h4>
                <div class="p1">
                    <p id="68">学习一个度量,我们可以看成是学习一个映射,把特征空间中的样本映射到另外一个新的空间中,新空间中的欧式距离即为所求的度量。具体地,学习一个马氏矩阵<i><b>M</b></i>等价于学习一个线性映射<i><b>L</b></i><sup>T</sup>:<b>R</b><sup><i>m</i></sup>→<b>R</b><sup><i>r</i></sup>,其中<i><b>L</b></i>=[<i>l</i><sub>1</sub>,<i>l</i><sub>2</sub>,…,<i>l</i><sub><i>r</i></sub>]∈<b>R</b><sup><i>m</i></sup><sup>×</sup><sup><i>r</i></sup>。因此,我们可以这样计算两个样本间的距离:</p>
                </div>
                <div class="area_img" id="69">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JYRJ201910025_06900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="71">式中:<i><b>M</b></i><b>=</b><i><b>LL</b></i><sup>T</sup>是所要学习的度量。</p>
                </div>
                <div class="p1">
                    <p id="72">为了简化目标函数,我们引入一个新的记号。对于要研究的样本<i>x</i><sub><i>i</i></sub>,引入权重矩阵<i><b>W</b></i><sup>(</sup><sup><i>i</i></sup><sup>)</sup>,这是一个对角阵:</p>
                </div>
                <div class="p1">
                    <p id="73" class="code-formula">
                        <mathml id="73"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>W</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>=</mo><mi>W</mi><msubsup><mrow></mrow><mrow><mi>j</mi><mi>i</mi></mrow><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>=</mo><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mi>β</mi><msub><mrow></mrow><mi>i</mi></msub><mi>S</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mtext> </mtext><mi>j</mi><mo>∈</mo><mi>Ν</mi><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mn>0</mn><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mtext>其</mtext><mtext>他</mtext></mtd></mtr></mtable></mrow></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="74">重新整理正则项:</p>
                </div>
                <div class="p1">
                    <p id="75" class="code-formula">
                        <mathml id="75"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>r</mi><mi>g</mi><mo stretchy="false">(</mo><mi>Μ</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><mn>4</mn></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>β</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>j</mi><mo>∈</mo><mi>Ν</mi><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></munder><mi>S</mi></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mi>D</mi><msubsup><mrow></mrow><mi mathvariant="bold-italic">Μ</mi><mn>2</mn></msubsup><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mi>x</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo><mo>=</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>,</mo><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>W</mi></mstyle></mrow></mstyle><msubsup><mrow></mrow><mrow><mi>j</mi><mi>k</mi></mrow><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msubsup><mi>D</mi><msubsup><mrow></mrow><mi mathvariant="bold-italic">Μ</mi><mn>2</mn></msubsup><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mi>j</mi></msub><mo>,</mo><mi>x</mi><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>8</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="76">根据式(8)得:</p>
                </div>
                <div class="p1">
                    <p id="77" class="code-formula">
                        <mathml id="77"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>r</mi><mi>g</mi><mo stretchy="false">(</mo><mi>Μ</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>,</mo><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>W</mi></mstyle></mrow></mstyle><msubsup><mrow></mrow><mrow><mi>j</mi><mi>k</mi></mrow><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msubsup><mi>D</mi><msubsup><mrow></mrow><mi mathvariant="bold-italic">Μ</mi><mn>2</mn></msubsup><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mi>j</mi></msub><mo>,</mo><mi>x</mi><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false">)</mo><mo>=</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>,</mo><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>W</mi></mstyle></mrow></mstyle><msubsup><mrow></mrow><mrow><mi>j</mi><mi>k</mi></mrow><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msubsup><mrow><mo>|</mo><mrow><mi mathvariant="bold-italic">L</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mi>j</mi></msub><mo>-</mo><mi>x</mi><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false">)</mo></mrow><mo>|</mo></mrow><msup><mrow></mrow><mn>2</mn></msup><mo>=</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mi>t</mi><mi>r</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">X</mi><mi mathvariant="bold-italic">U</mi><mi mathvariant="bold-italic">X</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">L</mi><mi mathvariant="bold-italic">L</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mo stretchy="false">)</mo><mo>=</mo><mi>t</mi><mi>r</mi><mo stretchy="false">(</mo><mi mathvariant="bold-italic">X</mi><mi mathvariant="bold-italic">U</mi><mi mathvariant="bold-italic">X</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">Μ</mi><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>9</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="78">式中:<mathml id="133"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">U</mi><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi mathvariant="bold-italic">U</mi></mstyle><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo>,</mo><mi mathvariant="bold-italic">U</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo>=</mo><mi mathvariant="bold-italic">D</mi><msubsup><mrow></mrow><mtext>w</mtext><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>-</mo><mi mathvariant="bold-italic">W</mi><msup><mrow></mrow><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup></mrow></math></mathml>是拉普拉斯矩阵,<i><b>D</b></i><mathml id="134"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>w</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow></math></mathml>是一个对角阵,其对角元素为<mathml id="135"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">D</mi><msubsup><mrow></mrow><mi>w</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">(</mo><mi>k</mi><mo>,</mo><mi>k</mi><mo stretchy="false">)</mo><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>j</mi></munder><mi>W</mi></mstyle><msubsup><mrow></mrow><mrow><mi>k</mi><mi>j</mi></mrow><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow></math></mathml>。</p>
                </div>
                <div class="p1">
                    <p id="79">目标函数最后变为下式:</p>
                </div>
                <div class="area_img" id="81">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JYRJ201910025_08100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="82">本文所提出的半监督稀疏度量学习方法有如下优点:</p>
                </div>
                <div class="p1">
                    <p id="83">(1) 根据平滑假设,一个样本点<i>x</i><sub><i>i</i></sub>要与其近邻点<i>N</i>(<i>i</i>)保持一致。因此样本点<i>x</i><sub><i>i</i></sub>与其近邻的距离不能太大。这个距离由<i>D</i><mathml id="136"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi mathvariant="bold-italic">Μ</mi><mn>2</mn></msubsup></mrow></math></mathml>(<i><b>x</b></i><sub><i><b>i</b></i></sub>,<i><b>x</b></i><sub><i><b>j</b></i></sub>)来定义,如果<i><b>D</b></i><mathml id="137"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi mathvariant="bold-italic">Μ</mi><mn>2</mn></msubsup></mrow></math></mathml>(<i><b>x</b></i><sub><i><b>i</b></i></sub>,<i><b>x</b></i><sub><i><b>j</b></i></sub>)距离太大,则表明此度量和局部领域的分布兼容性过低,这将会受到惩罚。</p>
                </div>
                <div class="p1">
                    <p id="84">(2) 聚类假设表明分界线(面)应该从低密度区域穿过,也就是说分布在高密度区域的样本点之间的距离应较小。式(6)正则项中的<i>β</i><sub><i><b>i</b></i></sub>可以保证分布在高密度区域样本点之间的距离被最小化,如果这些样本之间存在较大的距离将会受到较大的惩罚。</p>
                </div>
                <div class="p1">
                    <p id="85">(3) 根据流形假设,样本间的距离要沿着流形来测量。在受到半监督学习中基于样本图的启发后,我们在正则项中引入了相似度<i><b>S</b></i><sub><i><b>ij</b></i></sub>,这个相似性是根据高斯核来计算的,它可以引导新的度量。</p>
                </div>
                <div class="p1">
                    <p id="86">(4) 引入稀疏正则项,本文引入的的<b>L</b><sub>1</sub>正则项使得度量矩阵具有稀疏性,有助于了解不同原始特征的重要程度,满足应用对可理解性的需求。</p>
                </div>
                <h3 id="87" name="87" class="anchor-tag"><b>3 模型求解</b></h3>
                <div class="p1">
                    <p id="88">梯度下降法是一种常用的一阶优化方法,是求解优化问题最经典的方法之一。</p>
                </div>
                <div class="p1">
                    <p id="89">具体地,在第<i>t</i>个循环,给定当前的度量<i><b>M</b></i>(<i>t</i>),将不满足约束的三样本组记作{<i>φ</i>(<i>t</i>)},当前的约束条件指的是<i>D</i><mathml id="138"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi mathvariant="bold-italic">Μ</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow><mn>2</mn></msubsup></mrow></math></mathml>(<i>x</i><sub><i>i</i></sub>,<i>x</i><sub><i>j</i></sub>)+1&lt;<i>D</i><mathml id="139"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi mathvariant="bold-italic">Μ</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow><mn>2</mn></msubsup></mrow></math></mathml>(<i>x</i><sub><i>i</i></sub>,<i>x</i><sub><i>k</i></sub>),其中<i>D</i><mathml id="140"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi mathvariant="bold-italic">Μ</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow><mn>2</mn></msubsup></mrow></math></mathml>(<i>x</i><sub><i>i</i></sub>,<i>x</i><sub><i>j</i></sub>)=(<i>x</i><sub><i>i</i></sub>-<i>x</i><sub><i>j</i></sub>)<sup>T</sup><i><b>M</b></i>(<i>t</i>)(<i>x</i><sub><i>i</i></sub>-<i>x</i><sub><i>j</i></sub>)。对于这些<i>φ</i>(<i>t</i>),需要学习一个度量使得<i>D</i><mathml id="141"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi mathvariant="bold-italic">Μ</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow><mn>2</mn></msubsup></mrow></math></mathml>(<i>x</i><sub><i>i</i></sub>,<i>x</i><sub><i>j</i></sub>)变小,同时<i>D</i><mathml id="142"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi mathvariant="bold-italic">Μ</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow><mn>2</mn></msubsup></mrow></math></mathml>(<i>x</i><sub><i>i</i></sub>,<i>x</i><sub><i>k</i></sub>)变大。则目标函数变为:</p>
                </div>
                <div class="area_img" id="91">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JYRJ201910025_09100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="93">式中:<mathml id="143"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>|</mo><mrow><mo stretchy="false">{</mo><mi>φ</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo stretchy="false">}</mo></mrow><mo>|</mo></mrow></mrow></math></mathml>指集合{<i>φ</i>(<i>t</i>)}中元素的个数,<i><b>M</b></i>(<i>t</i>+1)则可以通过<i><b>M</b></i>(<i>t</i>)向<i>F</i><sub><i>t</i></sub>的梯度相反方向移动一个步长得到,即:</p>
                </div>
                <div class="p1">
                    <p id="94"><i><b>M</b></i>(<i>t</i>+1)=<i><b>M</b></i>(<i>t</i>)-<i>γ</i>ᐁ<i>F</i><sub><i>t</i></sub></p>
                </div>
                <div class="p1">
                    <p id="95">重复此过程,直到满足了所有三样本组约束,或者达到预给定好的循环次数。算法描述如算法1所示。</p>
                </div>
                <div class="p1">
                    <p id="96"><b>算法1</b> 梯度下降算法</p>
                </div>
                <div class="p1">
                    <p id="97">输入:有标记样本<i>X</i><sup>l</sup></p>
                </div>
                <div class="p1">
                    <p id="98">无标记本<i>X</i><sup>u</sup></p>
                </div>
                <div class="p1">
                    <p id="99">示性矩阵<i><b>Y</b></i></p>
                </div>
                <div class="p1">
                    <p id="100">输出:度量<i><b>M</b></i></p>
                </div>
                <div class="area_img" id="153">
                                <img alt="" src="Detail/GetImg?filename=images/JYRJ201910025_15300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                            <p class="img_tit"></p>

                </div>
                <h3 id="111" name="111" class="anchor-tag"><b>4 实 验</b></h3>
                <h4 class="anchor-tag" id="112" name="112"><b>4.1 实验设置</b></h4>
                <div class="p1">
                    <p id="113">在本节中,将把本文提出的基于半监督假设的半监督稀疏度量学习算法(RS3ML)与S<sup>3</sup>ML、半监督判别分析(SDA)、LRML、基于核方法的半监督度量学习算法Kernel-A和Kernel-β进行分析比较,通过比较结果来测试本文所提方法的有效性。实验中,以欧氏距离作为比较的基准。</p>
                </div>
                <div class="p1">
                    <p id="114">我们把类标信息分别转化为样本对约束和三样本组约束。本文所提出的算法的参数依据文献<citation id="152" type="reference">[<a class="sup">15</a>]</citation>进行设置。</p>
                </div>
                <h4 class="anchor-tag" id="115" name="115"><b>4.2 实验结果</b></h4>
                <div class="p1">
                    <p id="116">从University of California Irvine(UCI) machine learning repository中选出五个数据集对各种算法进行1-NN的分类实验。五个数据集分别为Wine、Iris、Dermatology、Glass Identification(Glass)、Balance Scale(Balance)。其中:Wine数据集中记录的是意大利同一地区三种不同的葡萄酒品种的相关信息,Balance中记录的是天平的重量和距离,Dermatology数据集用于判定鳞状疾病的类型,Glass数据集记录的是不同类型的玻璃的氧化物含量的数据,Iris中包含的是不同种类鸢尾花的一些信息。各个数据集的基本信息如表1所示。</p>
                </div>
                <div class="area_img" id="117">
                    <p class="img_tit"><b>表1 数据集信息</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="117" border="1"><tr><td><br />Dataset</td><td>Instances</td><td>Classes</td><td>Features</td><td><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>|</mo><mrow><mtext>X</mtext><msup><mrow></mrow><mi>l</mi></msup></mrow><mo>|</mo></mrow></mrow></math></td></tr><tr><td><br />Wine</td><td>178</td><td>3</td><td>13</td><td>15</td></tr><tr><td><br />Iris</td><td>150</td><td>3</td><td>4</td><td>15</td></tr><tr><td><br />Dermatology</td><td>358</td><td>6</td><td>34</td><td>30</td></tr><tr><td><br />Glass</td><td>214</td><td>6</td><td>9</td><td>30</td></tr><tr><td><br />Balance</td><td>625</td><td>3</td><td>4</td><td>15</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="118">实验中,所有的数据都被随机分为有标记数据集<i>X</i><sup><i>l</i></sup>和无标记数据集<i>X</i><sup><i>u</i></sup>,并且每类只给了五个有标记样本,这些有标记样本用来训练度量和K近邻分类器。每个实验将会在同一数据集上重复30次,每次试验都随机地选取训练样本,实验结果给出了这30次实验结果的均值。</p>
                </div>
                <div class="p1">
                    <p id="119">图2、图3和图4结合1-NN分类器给出了不同度量算法的识别结果。纵坐标均为重复30次实验所取得的平均分类错误率。可以看出,两个核方法Kernel-A和Kernel-β在数据集上的表现不太稳定。本文提出的RS3ML算法与S<sup>3</sup>ML、SDA等其他算法比较,在五个数据集上的分类错误率均为最低。实验结果表明,相比其他度量算法,RS3ML算法效果明显,学习性能更优。</p>
                </div>
                <div class="area_img" id="120">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201910025_120.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 算法组1的错误率比较" src="Detail/GetImg?filename=images/JYRJ201910025_120.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 算法组1的错误率比较  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201910025_120.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="121">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201910025_121.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 算法组2的错误率比较" src="Detail/GetImg?filename=images/JYRJ201910025_121.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 算法组2的错误率比较  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201910025_121.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="122">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201910025_122.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 算法组3的错误率比较" src="Detail/GetImg?filename=images/JYRJ201910025_122.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 算法组3的错误率比较  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201910025_122.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h3 id="123" name="123" class="anchor-tag"><b>5 结 语</b></h3>
                <div class="p1">
                    <p id="124">本文基于三个半监督假设提出了一个半监督稀疏度量学习算法。与其他方法不同的是,本文所提出的方法结合了所有三个半监督假设,充分利用了大量的未标记样本,并利用L<sub>1</sub>范数使得度量矩阵具有稀疏性,从而减少计算机存储负担,提高学得模型的可解释性。最后在公开数据上的实验验证了本文提出的方法的有效性。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CDFD&amp;filename=1016739755.nh&amp;v=MTc0NDRSN3FmWnVadEZpRGhVYjNBVkYyNkdMUzdGOWJKcXBFYlBJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1U=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> 梅江元.基于马氏距离的度量学习算法研究及应用[D].黑龙江:哈尔滨工业大学航天学院,2016.
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XDDJ201809014&amp;v=MDU3MTFNcG85RVlJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVadEZpRGhVYjNBUFNuUFpMRzRIOW4=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> 吕璐,蔡晓东,曾燕,等.一种基于融合深度卷积神经网络与度量学习的人脸识别方法[J].现代电子技术,2018,41(9):58-59.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Is that you?Metric learning approaches for face identification">

                                <b>[3]</b> Guillaumin M,Verbeek J,Schmid C.Is that you?Metric learning approaches for identification[C]//The 12th IEEE International Conference on Computer Vision.IEEE,2009:498-505.
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=FaceNet:A unified embedding for face recognition and clustering[EB]">

                                <b>[4]</b> Schroff F,Kalenichenko D,Philbin J.FaceNet:A unified embedding for face recognition and clustering[EB].arXiv preprint arXiv:1503.03832,2015.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Bitscalable deep hashing with regularized similarity learning for image retrieval and person re-identification">

                                <b>[5]</b> Zhang R M,Lin L,Zhang R,et al.Bit-scalable deep hashing with regularized similarity learning for image retrieval and person re-identification[J].IEEE Transactions on Image Processing,2015,24(12):4766-4779.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=CP-mtML:Coupled projection multi-task metric learning for large scale face retrieval[EB]">

                                <b>[6]</b> Bhattarai B,Sharma G,Jurie F.CP-mtML:Coupled projection multi-task metric learning for large scale face retrieval[EB].arXiv preprint arXiv:1604.02975,2016.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=1018087297.nh&amp;v=MDMwNjM2RnJPd0dkUEZxSkViUElRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVp0RmlEaFViM0FWRjI=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b> 阎少梅.基于度量学习的图像分类算法研究[D].安徽:安徽大学电子信息工程学院,2018.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Semi-supervised learning literature survey">

                                <b>[8]</b> Zhu X.Semi-supervised learning literature suvery[D].Computer Science TR 1530,University of Wisconsin Madison,2008.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Transductive inference for text classification using support vector machines">

                                <b>[9]</b> Joachims T.Transductive inference for text classification using support vector macines[C]//Proceedings of the Sixteenth International Conference on Machine Learning.Morgan Kaufmann Publishers Inc.1999.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Semi-supervised Learning">

                                <b>[10]</b> Hady M F A,Schwenker F.Semi-supervised learning[J].Intelligent Systems Reference Library,2013,49(2):215-239.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Semi-supervised classification by low density separation">

                                <b>[11]</b> Chapelle O,Zien A.Semi-supervised classification by low density separation[C]//Tenth International Workshop on Artificial Intelligence &amp; Statistics.2004.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_12" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJST&amp;filename=SJST14120603924782&amp;v=MTI0MDFCTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaW5sVXJ6SktGb1dhQm89TmlmWWVySzhIOVBNcVk5R2Jla0xDM1E3bw==&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[12]</b> Tibshirani R.Regression shrinkage and selection via the lasso[J].Journal of the Royal Statistical Society Series B(Methodological),1996,58(1):267-288.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Distance metric learning for large margin nearest neighbor classification">

                                <b>[13]</b> Weinberger K Q.Distance metric learning for large margin nearest neighbor classification[J].The Journal of Machine Learning Research,2009,10:207-244.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Boosting Framework for Visuality-Preserving Distance Metric Learning and Its Application to Medical Image Retrieval">

                                <b>[14]</b> Yang L,Jin R,Mummert L,et al.A boosting framework for visuality-preserving distance metric learning and its application to medical image retrieval[J].IEEE Transactions on Pattern Analysis &amp; Machine Intelligence,2010,32(1):30.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_15" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13050900089671&amp;v=MDUzMzlpclJkR2VycVFUTW53WmVadEZpbmxVcnpKS0ZvV2FCbz1OaWZPZmJLN0h0VE1wbzlGWk9NR0NuczRvQk1UNlQ0UFFILw==&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[15]</b> Wang Q Y,Yuen P C,Feng G,et al.Semi-supervised metric learning via topology preserving multiple semi-supervised assumptions[J].Pattern Recognition,2013,46:2580-2582.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JYRJ201910025" />
        <input id="dpi" type="hidden" value="600" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201910025&amp;v=MjAwODJDVVI3cWZadVp0RmlEaFViM0FMelRaWkxHNEg5ak5yNDlIWVlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnI=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="0" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
