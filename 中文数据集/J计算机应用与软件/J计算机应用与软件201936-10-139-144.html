<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637135556253127500%26DBCODE%3dCJFD%26TABLEName%3dCJFDTEMP%26FileName%3dJYRJ201910026%26RESULT%3d1%26SIGN%3d9HYSFK6dlmNkmMFC6L9txQ4geMI%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JYRJ201910026&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JYRJ201910026&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201910026&amp;v=MTQ1MDJDVVI3cWZadVp0RmlEaFVick1MelRaWkxHNEg5ak5yNDlIWW9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnI=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#31" data-title="&lt;b&gt;0 引 言&lt;/b&gt; "><b>0 引 言</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#33" data-title="&lt;b&gt;1 相关工作&lt;/b&gt; "><b>1 相关工作</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#37" data-title="&lt;b&gt;2 算法设计&lt;/b&gt; "><b>2 算法设计</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#39" data-title="&lt;b&gt;2.1 单阶段检测器结构&lt;/b&gt;"><b>2.1 单阶段检测器结构</b></a></li>
                                                <li><a href="#42" data-title="&lt;b&gt;2.2 倾斜矫正网络结构设计&lt;/b&gt;"><b>2.2 倾斜矫正网络结构设计</b></a></li>
                                                <li><a href="#54" data-title="&lt;b&gt;2.3 多任务损失函数&lt;/b&gt;"><b>2.3 多任务损失函数</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#72" data-title="&lt;b&gt;3 实 验&lt;/b&gt; "><b>3 实 验</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#73" data-title="&lt;b&gt;3.1 实验准备&lt;/b&gt;"><b>3.1 实验准备</b></a></li>
                                                <li><a href="#75" data-title="&lt;b&gt;3.2 角度矫正评价标准&lt;/b&gt;"><b>3.2 角度矫正评价标准</b></a></li>
                                                <li><a href="#80" data-title="&lt;b&gt;3.3 实验结果与分析&lt;/b&gt;"><b>3.3 实验结果与分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#90" data-title="&lt;b&gt;4 结 语&lt;/b&gt; "><b>4 结 语</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#41" data-title="图1 单阶段目标检测结构示意图">图1 单阶段目标检测结构示意图</a></li>
                                                <li><a href="#47" data-title="图2 条形码倾斜角度定义示意图">图2 条形码倾斜角度定义示意图</a></li>
                                                <li><a href="#48" data-title="图3 象限划分示意图">图3 象限划分示意图</a></li>
                                                <li><a href="#52" data-title="图4 带倾斜校正的检测头部示意图">图4 带倾斜校正的检测头部示意图</a></li>
                                                <li><a href="#53" data-title="图5 Anchor设置示意图">图5 Anchor设置示意图</a></li>
                                                <li><a href="#82" data-title="&lt;b&gt;表1 不同角度回归损失函数的矫正效果&lt;/b&gt;"><b>表1 不同角度回归损失函数的矫正效果</b></a></li>
                                                <li><a href="#85" data-title="&lt;b&gt;表2 不同倾斜矫正算法效果对比&lt;/b&gt;"><b>表2 不同倾斜矫正算法效果对比</b></a></li>
                                                <li><a href="#89" data-title="图6 条形码检测矫正结果示意图">图6 条形码检测矫正结果示意图</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" title=" Karrach L,Pivarciova E.The analyse of the various methods for location of Data Matrix codes in images[C]//IEEE 2018 ELEKTRO.Mikulov,Czech Republic,2018:1-6." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The analyse of the various methods for location of Data Matrix codes in images">
                                        <b>[1]</b>
                                         Karrach L,Pivarciova E.The analyse of the various methods for location of Data Matrix codes in images[C]//IEEE 2018 ELEKTRO.Mikulov,Czech Republic,2018:1-6.
                                    </a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" title=" Tang K,Lu H,Shi X.Image recognition with deep learning for library book identification[C]//Pacific Rim Conference on Multimedia.Springer,Cham,2018:684-696." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image recognition with deep learning for library book identification">
                                        <b>[2]</b>
                                         Tang K,Lu H,Shi X.Image recognition with deep learning for library book identification[C]//Pacific Rim Conference on Multimedia.Springer,Cham,2018:684-696.
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title=" Yun I,Kim J.Vision-based 1D barcode localization method for scale and rotation invariant[C]//TENCON 2017-2017 IEEE Region 10 Conference.IEEE,2017:2204-2208." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Vision-based 1D barcode localization method for scale and rotation invariant">
                                        <b>[3]</b>
                                         Yun I,Kim J.Vision-based 1D barcode localization method for scale and rotation invariant[C]//TENCON 2017-2017 IEEE Region 10 Conference.IEEE,2017:2204-2208.
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" title=" Xu L,Kamat V R,Menassa C C.Automatic extraction of 1D barcodes from video scans for drone-assisted inventory management in warehousing applications[J].International Journal of Logistics Research and Applications,2018,21(3):243-258." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=STJD&amp;filename=STJD08066EEC0C4C7B76CDA061C7C8E97370&amp;v=MjU0NjBZZk9HUWxmQnJMVTA1dHRnekx5OXhhdz1Oam5CYXJPd0h0Zksydm8yWkpnTGYzdEx5QkJnbms1OVRuNlJxMkU5REx1VFJyMmZDT052RlNpV1dyN0pJRnBtYUJ1SA==&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                         Xu L,Kamat V R,Menassa C C.Automatic extraction of 1D barcodes from video scans for drone-assisted inventory management in warehousing applications[J].International Journal of Logistics Research and Applications,2018,21(3):243-258.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" title=" 冯国文.基于SVM的复杂背景条码检测算法研究[D].浙江:浙江大学,2017." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=1017066864.nh&amp;v=MjQ5NjhHTm5LcTVFYlBJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVadEZpRGhVYnJNVkYyNkdiTys=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                         冯国文.基于SVM的复杂背景条码检测算法研究[D].浙江:浙江大学,2017.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" title=" Wang Z,Chen A,Li J,et al.1D barcode region detection based on the hough transform and support vector machine[C]//International Conference on Multimedia Modeling.Springer,Cham,2016:79-90." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=1D Barcode Region Detection Based on the Hough Transform and Support Vector Machine">
                                        <b>[6]</b>
                                         Wang Z,Chen A,Li J,et al.1D barcode region detection based on the hough transform and support vector machine[C]//International Conference on Multimedia Modeling.Springer,Cham,2016:79-90.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" title=" Cho H,Kim D,Park J,et al.2D barcode detection using images for drone-assisted inventory management[C]//2018 15th International Conference on Ubiquitous Robots (UR).IEEE,2018:461-465." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=2D barcode detection using images for drone-assisted inventory management">
                                        <b>[7]</b>
                                         Cho H,Kim D,Park J,et al.2D barcode detection using images for drone-assisted inventory management[C]//2018 15th International Conference on Ubiquitous Robots (UR).IEEE,2018:461-465.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" title=" Mulyaningtyas C,Imah E M.Barcode recognition using principal component analysis and support vector machine[C]//Mathematics,Informatics,Science,and Education International Conference(MISEIC 2018).Atlantis Press,2018." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Barcode recognition using principal component analysis and support vector machine">
                                        <b>[8]</b>
                                         Mulyaningtyas C,Imah E M.Barcode recognition using principal component analysis and support vector machine[C]//Mathematics,Informatics,Science,and Education International Conference(MISEIC 2018).Atlantis Press,2018.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" title=" Zhao Q,Ni F,Song Y,et al.Deep dual pyramid network for barcode segmentation using Barcode-30k database[EB].arXiv preprint arXiv:1807.11886,2018." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep dual pyramid network for barcode segmentation using Barcode-30k database[EB]">
                                        <b>[9]</b>
                                         Zhao Q,Ni F,Song Y,et al.Deep dual pyramid network for barcode segmentation using Barcode-30k database[EB].arXiv preprint arXiv:1807.11886,2018.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_10" title=" Li J,Zhao Q,Tan X,et al.Using deep ConvNet for robust 1D barcode detection[C]//International Conference on Intelligent and Interactive Systems and Applications.Springer,Cham,2017:261-267." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Using deep ConvNet for robust 1D barcode detection">
                                        <b>[10]</b>
                                         Li J,Zhao Q,Tan X,et al.Using deep ConvNet for robust 1D barcode detection[C]//International Conference on Intelligent and Interactive Systems and Applications.Springer,Cham,2017:261-267.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_11" title=" Ren S,He K,Girshick R,et al.Faster R-CNN:Towards real-time object detection with region proposal networks[J].IEEE Transactions on Pattern Analysis &amp;amp; Machine Intelligence,2015,39(6):1137-1149." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Faster R-CNN:towards real-time object detection with region proposal networks">
                                        <b>[11]</b>
                                         Ren S,He K,Girshick R,et al.Faster R-CNN:Towards real-time object detection with region proposal networks[J].IEEE Transactions on Pattern Analysis &amp;amp; Machine Intelligence,2015,39(6):1137-1149.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_12" title=" Hansen D K,Nasrollahi K,Rasmussen C B,et al.Real-time barcode detection and classification using deep learning[C]//Proceedings of the 9th International Joint Conference on Computational Intelligence—Volume 1:IJCCI.2017:321-327." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Real-time barcode detection and classification using deep learning">
                                        <b>[12]</b>
                                         Hansen D K,Nasrollahi K,Rasmussen C B,et al.Real-time barcode detection and classification using deep learning[C]//Proceedings of the 9th International Joint Conference on Computational Intelligence—Volume 1:IJCCI.2017:321-327.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_13" title=" Redmon J,Divvala S,Girshick R,et al.You only look once:Unified,real-time object detection[C]//Proceedings of the IEEE conference on computer vision and pattern recognition.2016:779-788." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=You only look once:Unified,real-time object detection">
                                        <b>[13]</b>
                                         Redmon J,Divvala S,Girshick R,et al.You only look once:Unified,real-time object detection[C]//Proceedings of the IEEE conference on computer vision and pattern recognition.2016:779-788.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_14" >
                                        <b>[14]</b>
                                     Liu W,Anguelov D,Erhan D,et al.Ssd:Single shot multibox detector[C]//European conference on computer vision.Springer,Cham,2016:21-37.</a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JYRJ" target="_blank">计算机应用与软件</a>
                2019,36(10),139-144 DOI:10.3969/j.issn.1000-386x.2019.10.025            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于多任务目标检测的条形码倾斜矫正算法研究</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%98%93%E5%B8%86&amp;code=42516421&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">易帆</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%8E%E5%8A%9F%E7%87%95&amp;code=31314641&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">李功燕</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%AE%B8%E7%BB%8D%E4%BA%91&amp;code=33984211&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">许绍云</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E7%A7%91%E5%AD%A6%E9%99%A2%E5%BE%AE%E7%94%B5%E5%AD%90%E7%A0%94%E7%A9%B6%E6%89%80&amp;code=0052330&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国科学院微电子研究所</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E7%A7%91%E5%AD%A6%E9%99%A2%E5%A4%A7%E5%AD%A6&amp;code=1698842&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国科学院大学</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>条形码识别技术在日常工作中发挥着巨大作用,尤其是在智能物流包裹分拣领域。该技术主要分为三个部分:条形码检测、矫正和译码。目前条形码检测和译码技术较为成熟,而在条形码倾斜矫正技术上研究效果一般。为提升条形码矫正效果,设计一种矫正算法。先对条形码倾斜程度进行分类,再进行角度回归,有效降低条形码矫正任务难度;并将该算法与单阶段检测器融合构成多任务目标检测算法,协同促进发挥检测和矫正的作用。实验表明:余弦距离角度损失函数更加适合角度回归任务,针对条形码倾斜程度分类有助于提升条形码矫正效果。与其他相关算法对比,该算法在矫正准确率、实际译码率和速度上均取得最优的效果。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%9D%A1%E5%BD%A2%E7%A0%81%E7%9F%AB%E6%AD%A3&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">条形码矫正;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%A4%9A%E4%BB%BB%E5%8A%A1%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">多任务目标检测;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8D%95%E9%98%B6%E6%AE%B5%E6%A3%80%E6%B5%8B%E5%99%A8&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">单阶段检测器;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%88%86%E7%B1%BB%E4%B8%8E%E5%9B%9E%E5%BD%92&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">分类与回归;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    易帆，硕士生，主研领域:图像处理，计算机视觉。;
                                </span>
                                <span>
                                    李功燕，研究员。;
                                </span>
                                <span>
                                    许绍云，助理研究员。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-02-27</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家重点研发计划项目(2018YFD0700300);</span>
                                <span>中国科学院弘光专项(KFJ-HGZX-012);</span>
                    </p>
            </div>
                    <h1><b>BARCODE TILT CORRECTION ALGORITHM BASED ON MULTI-TASK OBJECT DETECTION</b></h1>
                    <h2>
                    <span>Yi Fan</span>
                    <span>Li Gongyan</span>
                    <span>Xu Shaoyun</span>
            </h2>
                    <h2>
                    <span>Institute of Microelectronics, Chinese Academy of Sciences</span>
                    <span>University of Chinese Academy of Sciences</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Barcode recognition plays a huge role in daily work, especially in the field of intelligent logistics parcel sorting. It is mainly divided into three parts: barcode detection, correction and decoding. At present, barcode detection and decoding technology is relatively mature, however, barcode tilt correction research indicates a flat effect. In order to improve the barcode correction effect, a correction algorithm was designed. By classifying the tilt degree of the barcode and then performing angle regression, the difficulty of the barcode correction task was effectively reduced, and the algorithm was combined with the one-stage detector to form a multi-task object detection. The algorithm synergistically promoted the role of detection and correction. The experiments show that the cosine distance angle loss function is more suitable for the angle regression task. The classification of the barcode tilt degree can improve the barcode correction effect. Compared with other related algorithms, the proposed algorithm achieves the best results in correcting accuracy, decoding rate and speed.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Barcode%20correction&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Barcode correction;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Multi-task%20object%20detection&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Multi-task object detection;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=One-stage%20detector&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">One-stage detector;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Classification%20and%20regression&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Classification and regression;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2019-02-27</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="31" name="31" class="anchor-tag"><b>0 引 言</b></h3>
                <div class="p1">
                    <p id="32">条形码技术在实际生活中已经取得广泛应用,例如在物流快递、医疗、交通、仓储、生产自动化、票务单、商场等场景极大方便了人们的日常工作。通过将条形码图像输入计算机,按照预定的规则解码出条形码信息,不再需要人工手动输入,完全由机器完成,避免人工录入带来的失误,提高了生产效率。然而,实际场景中输入图像内容不全是条形码,还存在很多背景干扰信息,因此在识别条形码内容之前需要检测定位输入图像中条形码的位置。另外,考虑到条形码可能存在倾斜的情况,需要在检测定位条形码位置之后再进行角度矫正,条形码矫正结果的好坏直接影响后续条形码译码的准确率和速度。当前条形码技术研究主要集中在条形码检测定位上,矫正研究较少并与检测分开处理,无法协同发挥促进两者的相互作用。故本文以物流快递包裹自动化分拣场景为背景,基于深度学习中目标检测技术研究条形码倾斜矫正相关算法,以提高最终条形码倾斜矫正的效果。</p>
                </div>
                <h3 id="33" name="33" class="anchor-tag"><b>1 相关工作</b></h3>
                <div class="p1">
                    <p id="34">随着条形码技术的广泛运用和应用场景的复杂化,越来越多的条形码识别算法被提出,主要分为传统的数字图像处理和深度学习两类算法。</p>
                </div>
                <div class="p1">
                    <p id="35">在早期,条形码识别算法主要是基于传统的数字图像处理。文献<citation id="107" type="reference">[<a class="sup">1</a>,<a class="sup">2</a>]</citation>借助边缘检测算子对输入图像进行操作获取梯度特征区域,并通过自适应阈值滤除大部分背景,再通过主梯度及Hough变换获取倾斜方向矫正条形码,最后依据条码特征合并候选区域。文献<citation id="104" type="reference">[<a class="sup">3</a>]</citation>首先生成输入图像的方向直方图,去除背景区域和杂波,并分析基于局部熵的方向分割出条形码区域,最后分析合并条码区域。文献<citation id="105" type="reference">[<a class="sup">4</a>]</citation>采用Harris角点检测器和Hough变换算法估计待检测条码区域的旋转角度,再通过条形码区域的连通性和几何特性来直接识别单个视频帧中的多个条形码区域。文献<citation id="108" type="reference">[<a class="sup">5</a>,<a class="sup">6</a>,<a class="sup">7</a>]</citation>通过提取滑动窗口的局部二值模式(Local Binary Pattern,LBP)或者方向梯度直方图(Histogram of Oriented Gradient,HOG)特征,送入支持向量机(Support Vector Machine,SVM)分类器判断所属类别,进而将连通子块合并得到条码区域,最后由Hough变换算法矫正条形码。文献<citation id="106" type="reference">[<a class="sup">8</a>]</citation>利用主成分分析PCA算法提取条形码主要特征,再通过SVM算法判别条码区域的可能性。以上算法一般需要手工设计特征和规则,无法适应在物流包裹分拣场景下,条形码图片因为光照不均、背景干扰、包裹扭曲形变、模糊、污损等情况造成图片质量下降和特征不明显,进而算法性能大打折扣。</p>
                </div>
                <div class="p1">
                    <p id="36">近年来,深度学习技术快速发展,凭借着其强大的拟合和提取特征能力,已经在绝大多数图像领域取得优于传统图像处理算法的效果。文献<citation id="109" type="reference">[<a class="sup">9</a>]</citation>提出了一种基于双金字塔结构的分割网络BarcodeNet,分割提取条形码区域。文献<citation id="110" type="reference">[<a class="sup">10</a>]</citation>提出一种基于卷积神经网络的级联条形码检测算法。首先使用Faster-Rcnn<citation id="111" type="reference"><link href="23" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>目标检测算法获得条形码区域,再借助最大稳定极值区域(Maximally Stable Extremal Regions,MSERs)算法消除背景噪声并检测条码方向,最后通过自适应流形(Adaptive Manifold,AM)滤波器处理模糊条形码区域,整个算法流程由于多阶段处理速度较慢。文献<citation id="112" type="reference">[<a class="sup">12</a>]</citation>通过YOLO<citation id="113" type="reference"><link href="27" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>目标检测算法定位条形码区域,之后将条形码区域裁剪放缩至正方形送入一个角度矫正卷积神经网络预测条形码的倾斜角,然而这两部分网络提取特征存在重复冗余,并且直接回归预测倾斜角误差偏大。针对以上条形码检测矫正算法的不足,本文基于深度学习目标检测算法改进条码检测矫正的效果和速度。</p>
                </div>
                <h3 id="37" name="37" class="anchor-tag"><b>2 算法设计</b></h3>
                <div class="p1">
                    <p id="38">目标检测一直是计算机视觉领域的基础问题,主要关注的是图片中特定目标物体的位置。一个检测任务包含两个子任务:一是输出这一目标的类别信息,属于分类任务;二是输出目标的具体位置信息,属于定位任务。传统的目标检测算法主要通过区域选择、特征提取和分类回归三个步骤组成,然而,区域选择的策略效果差、时间复杂度高,手工设计提取的特征鲁棒性较差。随着深度学习的介入,这三个步骤可由一个端到端的卷积神经网络完成,极大地提高了目标检测的准确率。目前主流的基于卷积神经网络的目标检测算法主要分为两类:一个是以Faster-Rcnn为代表的两阶段检测器,通常先进行前背景分类和回归框粗调,再进行目标分类和回归框精调,检测准确率高但速度偏慢;另一个是以YOLO和SSD<citation id="114" type="reference"><link href="29" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>为代表的单阶段检测器,直接进行类别预测和位置回归,检测准确率稍低于Faster-Rcnn但速度快。由于实际生产环境中对条形码识别速度要求严格,故本文以单阶段检测器为基础,改进检测器结构,提出一种多任务目标检测的条形码矫正算法。</p>
                </div>
                <h4 class="anchor-tag" id="39" name="39"><b>2.1 单阶段检测器结构</b></h4>
                <div class="p1">
                    <p id="40">单阶段检测器模型整体上由基础网络(Backbone Network)和检测头部(Detection Head)构成,如图1所示。前者的作用主要是特征提取,给出输入图像不同大小、不同抽象层次的语义特征信息表示;后者则根据这些语义特征表示和监督信息完成待检测目标的分类和定位任务,分类的结果是输出类别标签,定位的结果是输出检测矩形框的位置信息,包含矩形框的中心位置坐标(<i>f</i><sup><i>x</i></sup>,<i>f</i><sup><i>y</i></sup>)和宽高(<i>f</i><sup><i>w</i></sup>,<i>f</i><sup><i>h</i></sup>)。检测头部网络负责的类别预测和位置回归两个任务一般是并行进行的,构成多任务的损失进行联合训练。</p>
                </div>
                <div class="area_img" id="41">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201910026_041.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 单阶段目标检测结构示意图" src="Detail/GetImg?filename=images/JYRJ201910026_041.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 单阶段目标检测结构示意图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201910026_041.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="42" name="42"><b>2.2 倾斜矫正网络结构设计</b></h4>
                <div class="p1">
                    <p id="43">首先对矫正角度进行定义,如图2所示,本文约定从水平轴出发逆时针旋转至与黑条纹重合的旋转角度为矫正角度值,记为<i>θ</i>。由于条形码首尾颠倒都能译码,矫正角度值的范围是<i>θ</i>∈[0,180]。单阶段检测器中基础网络提取的特征信息已经包含条形码的语义,无需再设计一个新的卷积神经网络用于回归角度信息,可以直接将倾斜矫正任务嵌入到检测头部网络中,只需在检测头部网络中增加3×3卷积核的数量来回归角度信息,但直接回归得到一个[0,180]范围内的角度值误差较大。本文设计一种策略,在回归倾斜角度之前,先对条码倾斜程度进行分类以降低直接回归角度的难度,有效减少回归误差。具体地,将[0,180]角度范围均匀划分成<i>M</i>个象限(如图3所示),<i>M</i>取值范围为正整数,对应的在检测头部增加<i>M</i>分类预测条码倾斜角度落在哪个象限,确定象限之后再对倾斜角进行回归,相应的角度回归量取值范围变成<mathml id="92"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>[</mo><mrow><mn>0</mn><mo>,</mo><mfrac><mrow><mn>1</mn><mn>8</mn><mn>0</mn></mrow><mi>Μ</mi></mfrac></mrow><mo>]</mo></mrow></mrow></math></mathml>。一般来说,倾斜角度回归网络输出值<i>x</i>通过Sigmoid激活函数归一化到[0,1]范围内,再乘以<mathml id="93"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mn>1</mn><mn>8</mn><mn>0</mn></mrow><mi>Μ</mi></mfrac></mrow></math></mathml>映射到<mathml id="94"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>[</mo><mrow><mn>0</mn><mo>,</mo><mfrac><mrow><mn>1</mn><mn>8</mn><mn>0</mn></mrow><mi>Μ</mi></mfrac></mrow><mo>]</mo></mrow></mrow></math></mathml>。通过设计上述策略,最终倾斜条形码角度值<i>θ</i>为:</p>
                </div>
                <div class="p1">
                    <p id="44" class="code-formula"><mathml id="95"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>θ</mi><mo>=</mo><mo stretchy="false">(</mo><mi>m</mi><mo>-</mo><mn>1</mn><mo stretchy="false">)</mo><mo>×</mo><mfrac><mrow><mn>1</mn><mn>8</mn><mn>0</mn></mrow><mi>Μ</mi></mfrac><mo>+</mo><mtext>S</mtext><mtext>i</mtext><mtext>g</mtext><mtext>m</mtext><mtext>o</mtext><mtext>i</mtext><mtext>d</mtext></mrow></math></mathml><mathml id="96"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>×</mo><mfrac><mrow><mn>1</mn><mn>8</mn><mn>0</mn></mrow><mi>Μ</mi></mfrac></mrow></math></mathml></p>
                </div>
                <div class="p1">
                    <p id="45"><i>Sigmoid</i><mathml id="97"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mtext>e</mtext><msup><mrow></mrow><mrow><mo>-</mo><mi>x</mi></mrow></msup></mrow></mfrac></mrow></math></mathml></p>
                </div>
                <div class="p1">
                    <p id="46">式中:m为分类网络输出的象限值。</p>
                </div>
                <div class="area_img" id="47">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201910026_047.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 条形码倾斜角度定义示意图" src="Detail/GetImg?filename=images/JYRJ201910026_047.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 条形码倾斜角度定义示意图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201910026_047.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="48">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201910026_048.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 象限划分示意图" src="Detail/GetImg?filename=images/JYRJ201910026_048.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 象限划分示意图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201910026_048.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="49">获得倾斜条形码角度值之后,通过仿射变换可以矫正条形码,由于倾斜角是从水平方向开始逆时针旋转定义的,故实际条形码需要逆时针旋转(90-<i>θ</i>)度(若旋转角为负值,顺时针旋转即可),旋转变换矩阵为:</p>
                </div>
                <div class="p1">
                    <p id="50" class="code-formula">
                        <mathml id="50"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">Τ</mi><mo>=</mo><mrow><mo>[</mo><mrow><mtable><mtr><mtd><mi>cos</mi><mo stretchy="false">(</mo><mn>9</mn><mn>0</mn><mo>-</mo><mi>θ</mi><mo stretchy="false">)</mo></mtd><mtd><mo>-</mo><mi>sin</mi><mo stretchy="false">(</mo><mn>9</mn><mn>0</mn><mo>-</mo><mi>θ</mi><mo stretchy="false">)</mo></mtd><mtd><mn>0</mn></mtd></mtr><mtr><mtd><mi>sin</mi><mo stretchy="false">(</mo><mn>9</mn><mn>0</mn><mo>-</mo><mi>θ</mi><mo stretchy="false">)</mo></mtd><mtd><mi>cos</mi><mo stretchy="false">(</mo><mn>9</mn><mn>0</mn><mo>-</mo><mi>θ</mi><mo stretchy="false">)</mo></mtd><mtd><mn>0</mn></mtd></mtr><mtr><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd><mtd><mn>1</mn></mtd></mtr></mtable></mrow><mo>]</mo></mrow><mo>=</mo><mrow><mo>[</mo><mrow><mtable><mtr><mtd><mi>sin</mi><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo></mtd><mtd><mo>-</mo><mi>cos</mi><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo></mtd><mtd><mn>0</mn></mtd></mtr><mtr><mtd><mi>cos</mi><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo></mtd><mtd><mi>sin</mi><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo></mtd><mtd><mn>0</mn></mtd></mtr><mtr><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd><mtd><mn>1</mn></mtd></mtr></mtable></mrow><mo>]</mo></mrow></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="51">最终,检测头部结构如图4所示。首先从特征提取基础网络中抽取特征图,假设特征图的大小为<i>S</i>×<i>S</i>×<i>D</i>,通过3×3卷积运算获得特征图待检测张量,将张量划分为<i>S</i>×<i>S</i>个格点,每个格点设置<i>B</i>个预设锚框(Anchor),如图5所示,这样每个格点最多可以检测<i>B</i>个中心落在此格点内的目标。每个格点向量中具体值含义为:对于每一个Anchor,输出检测矩形框的4个坐标信息(<i>f</i><sup><i>x</i></sup>,<i>f</i><sup><i>y</i></sup>,<i>f</i><sup><i>w</i></sup>,<i>f</i><sup><i>h</i></sup>);<i>N</i>个属性类别标签值,其中<i>N</i>-1个值代表实际类别数目,剩下1个代表背景类别;<i>M</i>个象限类别标签值;1个角度回归值。最终每个格点向量的长度为<i>B</i>×(4+<i>N</i>+<i>M</i>+1)。</p>
                </div>
                <div class="area_img" id="52">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201910026_052.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 带倾斜校正的检测头部示意图" src="Detail/GetImg?filename=images/JYRJ201910026_052.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 带倾斜校正的检测头部示意图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201910026_052.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="53">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201910026_053.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 Anchor设置示意图" src="Detail/GetImg?filename=images/JYRJ201910026_053.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 Anchor设置示意图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201910026_053.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="54" name="54"><b>2.3 多任务损失函数</b></h4>
                <div class="p1">
                    <p id="55">为了训练优化设计的网络结构,需要设定合理的损失函数,最终的损失主要包含三项:分类损失、边界框回归损失和角度矫正损失。</p>
                </div>
                <div class="p1">
                    <p id="56">分类损失主要涉及到属性分类损失和象限分类损失,考虑到正负样本比例会影响最终网络的训练效果,故设计一种类别均衡的Softmax交叉熵损失进行计算:</p>
                </div>
                <div class="p1">
                    <p id="57" class="code-formula">
                        <mathml id="57"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><msub><mrow></mrow><mrow><mtext>c</mtext><mtext>l</mtext><mtext>s</mtext></mrow></msub><mo stretchy="false">(</mo><mi>l</mi><mo>,</mo><mi>p</mi><mo stretchy="false">)</mo><mo>=</mo><mo>-</mo><mi>β</mi><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>∈</mo><mo>+</mo></mrow><mi>Ν</mi></munderover><mi>l</mi></mstyle><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mi>c</mi></msubsup><mrow><mi>log</mi></mrow><mo stretchy="false">(</mo><mi>p</mi><msubsup><mrow></mrow><mi>i</mi><mi>c</mi></msubsup><mo stretchy="false">)</mo><mo>-</mo><mo stretchy="false">(</mo><mn>1</mn><mo>-</mo><mi>β</mi><mo stretchy="false">)</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>∈</mo><mo>-</mo></mrow><mi>Μ</mi></munderover><mrow><mi>log</mi></mrow></mstyle><mo stretchy="false">(</mo><mi>p</mi><msubsup><mrow></mrow><mi>i</mi><mn>0</mn></msubsup><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="58" class="code-formula"><mathml id="98"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>p</mi><msubsup><mrow></mrow><mi>i</mi><mi>c</mi></msubsup><mo>=</mo><mfrac><mrow><mi>exp</mi><mo stretchy="false">(</mo><mover accent="true"><mi>p</mi><mo>^</mo></mover><msubsup><mrow></mrow><mi>i</mi><mi>c</mi></msubsup><mo stretchy="false">)</mo></mrow><mrow><mstyle displaystyle="true"><msub><mo>∑</mo><mi>c</mi></msub><mover accent="true"><mi>p</mi><mo>^</mo></mover></mstyle><msubsup><mrow></mrow><mi>i</mi><mi>c</mi></msubsup></mrow></mfrac></mrow></math></mathml><mathml id="99"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>β</mi><mo>=</mo><mn>1</mn><mo>-</mo><mfrac><mi>Ν</mi><mrow><mi>Μ</mi><mo>+</mo><mi>Ν</mi></mrow></mfrac></mrow></math></mathml></p>
                </div>
                <div class="p1">
                    <p id="59">式中:<i>l</i><mathml id="100"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mi>c</mi></msubsup></mrow></math></mathml>为示性函数,当第<i>i</i>个预测回归框与类别为<i>c</i>的真值(Ground Truth)标记框<i>j</i>匹配时<i>l</i><mathml id="101"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mi>c</mi></msubsup></mrow></math></mathml>值为1,否则为0;+和-分别代表正负样本;<i>p</i><mathml id="102"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>c</mi></msubsup></mrow></math></mathml>为输出类别为<i>c</i>的概率值;<i>p</i><mathml id="103"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mn>0</mn></msubsup></mrow></math></mathml>为负样本的概率值;<i>N</i>代表与真值框匹配的正样本数量;<i>M</i>代表未能匹配的负样本个数;<i>β</i>代表正负样本平衡因子。</p>
                </div>
                <div class="p1">
                    <p id="60">回归输出的位置坐标信息(<i>f</i><sup><i>x</i></sup>,<i>f</i><sup><i>y</i></sup>,<i>f</i><sup><i>w</i></sup>,<i>f</i><sup><i>h</i></sup>)其实代表一种偏移变换关系,中心点坐标信息的预测值(<i>f</i><sup><i>x</i></sup>,<i>f</i><sup><i>y</i></sup>)是相对于Anchor左上角坐标的偏移值,边界框宽高信息的预测值(<i>f</i><sup><i>w</i></sup>,<i>f</i><sup><i>h</i></sup>)是相对于Anchor长宽的变化比例,具体计算方式如下:</p>
                </div>
                <div class="p1">
                    <p id="61" class="code-formula">
                        <mathml id="61"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>f</mi><msup><mrow></mrow><mspace width="0.25em" /></msup><mi>x</mi><mo>=</mo><mfrac><mrow><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mi>p</mi></msub><mo>-</mo><mi>x</mi><msub><mrow></mrow><mi>a</mi></msub><mo stretchy="false">)</mo></mrow><mrow><mi>w</mi><msub><mrow></mrow><mi>a</mi></msub></mrow></mfrac><mtext> </mtext><mtext> </mtext><mi>f</mi><msup><mrow></mrow><mspace width="0.25em" /></msup><mi>w</mi><mo>=</mo><mrow><mi>log</mi></mrow><mrow><mo>(</mo><mrow><mfrac><mrow><mi>w</mi><msub><mrow></mrow><mi>p</mi></msub></mrow><mrow><mi>w</mi><msub><mrow></mrow><mi>a</mi></msub></mrow></mfrac></mrow><mo>)</mo></mrow></mtd></mtr><mtr><mtd><mi>f</mi><msup><mrow></mrow><mspace width="0.25em" /></msup><mi>y</mi><mo>=</mo><mfrac><mrow><mrow><mo>(</mo><mrow><mi>y</mi><msub><mrow></mrow><mi>p</mi></msub><mo>-</mo><mi>y</mi><msub><mrow></mrow><mi>a</mi></msub></mrow><mo>)</mo></mrow></mrow><mrow><mi>h</mi><msub><mrow></mrow><mi>a</mi></msub></mrow></mfrac><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mi>f</mi><msup><mrow></mrow><mspace width="0.25em" /></msup><mi>h</mi><mo>=</mo><mrow><mi>log</mi></mrow><mrow><mo>(</mo><mrow><mfrac><mrow><mi>h</mi><msub><mrow></mrow><mi>p</mi></msub></mrow><mrow><mi>h</mi><msub><mrow></mrow><mi>a</mi></msub></mrow></mfrac></mrow><mo>)</mo></mrow></mtd></mtr><mtr><mtd><mi>t</mi><msup><mrow></mrow><mi>x</mi></msup><mo>=</mo><mfrac><mrow><mrow><mo>(</mo><mrow><mi>x</mi><msub><mrow></mrow><mi>g</mi></msub><mo>-</mo><mi>x</mi><msub><mrow></mrow><mi>a</mi></msub></mrow><mo>)</mo></mrow></mrow><mrow><mi>w</mi><msub><mrow></mrow><mi>a</mi></msub></mrow></mfrac><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mi>t</mi><msup><mrow></mrow><mi>w</mi></msup><mo>=</mo><mrow><mi>log</mi></mrow><mrow><mo>(</mo><mrow><mfrac><mrow><mi>w</mi><msub><mrow></mrow><mi>g</mi></msub></mrow><mrow><mi>w</mi><msub><mrow></mrow><mi>a</mi></msub></mrow></mfrac></mrow><mo>)</mo></mrow></mtd></mtr><mtr><mtd><mi>t</mi><msup><mrow></mrow><mi>y</mi></msup><mo>=</mo><mfrac><mrow><mo stretchy="false">(</mo><mi>y</mi><msub><mrow></mrow><mi>g</mi></msub><mo>-</mo><mi>y</mi><msub><mrow></mrow><mi>a</mi></msub><mo stretchy="false">)</mo></mrow><mrow><mi>h</mi><msub><mrow></mrow><mi>a</mi></msub></mrow></mfrac><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mi>t</mi><msup><mrow></mrow><mi>h</mi></msup><mo>=</mo><mrow><mi>log</mi></mrow><mrow><mo>(</mo><mrow><mfrac><mrow><mi>h</mi><msub><mrow></mrow><mi>g</mi></msub></mrow><mrow><mi>h</mi><msub><mrow></mrow><mi>a</mi></msub></mrow></mfrac></mrow><mo>)</mo></mrow></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="62">式中:(<i>x</i><sub><i>p</i></sub>,<i>y</i><sub><i>p</i></sub>,<i>w</i><sub><i>p</i></sub>,<i>h</i><sub><i>p</i></sub>)、(<i>x</i><sub><i>a</i></sub>,<i>y</i><sub><i>a</i></sub>,<i>w</i><sub><i>a</i></sub>,<i>h</i><sub><i>a</i></sub>)和(<i>x</i><sub><i>g</i></sub>,<i>y</i><sub><i>g</i></sub>,<i>w</i><sub><i>g</i></sub>,<i>h</i><sub><i>g</i></sub>)分别是预测回归框、Anchor和真值标记框的中心点坐标和宽高;(<i>t</i><sup><i>x</i></sup>,<i>t</i><sup><i>y</i></sup>,<i>t</i><sup><i>w</i></sup>,<i>t</i><sup><i>h</i></sup>)代表真值标记框相对于Anchor的变换关系。边界框位置回归损失函数选择Smooth-L1:</p>
                </div>
                <div class="p1">
                    <p id="63" class="code-formula">
                        <mathml id="63"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><msub><mrow></mrow><mrow><mtext>l</mtext><mtext>o</mtext><mtext>c</mtext></mrow></msub><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>f</mi><mo>,</mo><mi>t</mi><mo stretchy="false">)</mo><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>∈</mo><mo>+</mo></mrow><mi>Ν</mi></munderover><mspace width="0.25em" /></mstyle><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>m</mi><mo>∈</mo><mo stretchy="false">{</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo>,</mo><mi>w</mi><mo>,</mo><mi>h</mi><mo stretchy="false">}</mo></mrow></munder><mi>l</mi></mstyle><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mi>p</mi></msubsup><mi>S</mi><mi>m</mi><mi>o</mi><mi>o</mi><mi>t</mi><mi>h</mi><msub><mrow></mrow><mrow><mi>L</mi><mn>1</mn></mrow></msub><mo stretchy="false">(</mo><mi>f</mi><msup><mrow></mrow><mspace width="0.25em" /></msup><mi>m</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mi>t</mi><msubsup><mrow></mrow><mi>j</mi><mi>m</mi></msubsup><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="64" class="code-formula">
                        <mathml id="64"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>S</mi><mi>m</mi><mi>o</mi><mi>o</mi><mi>t</mi><mi>h</mi><msub><mrow></mrow><mrow><mi>L</mi><mn>1</mn></mrow></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mn>0</mn><mo>.</mo><mn>5</mn><mi>x</mi><msup><mrow></mrow><mn>2</mn></msup><mspace width="0.25em" /><mrow><mo>|</mo><mi>x</mi><mo>|</mo></mrow><mo>&lt;</mo><mn>1</mn></mtd></mtr><mtr><mtd><mrow><mo>|</mo><mi>x</mi><mo>|</mo></mrow><mo>-</mo><mn>0</mn><mo>.</mo><mn>5</mn><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mtext>其</mtext><mtext>他</mtext></mtd></mtr></mtable></mrow></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="65">从Smooth-L1损失函数可以看出,借助Anchor作为中间桥梁,预测回归框不断向真值标记框靠近调整,同时引入偏移量和log变换限定输出量的范围,保证每一个Anchor负责检测周围附近单位以内的目标,使得模型更容易收敛并且损失保持在合理的量级上,可以看作一种标准化操作。</p>
                </div>
                <div class="p1">
                    <p id="66">回归角度损失函数若采用常见的L2损失函数,会导致损失函数对角度值偏差大的条形码更加敏感,使得最终网络对于小角度偏差的条形码矫正效果差。采用Smooth-L1损失函数能缓解上述问题,但是并不是针对角度量设计的。故本文采用余弦距离函数归一化角度值,保证网络最终的角度回归效果:</p>
                </div>
                <div class="p1">
                    <p id="67"><i>L</i><sub>angle</sub>=1-cos(<i>θ</i>-<i>θ</i><sup>*</sup>)</p>
                </div>
                <div class="p1">
                    <p id="68">式中:<i>θ</i>为预测回归角度;<i>θ</i><sup>*</sup>为真值标注角度。</p>
                </div>
                <div class="p1">
                    <p id="69">综上,将上述的分类损失、定位回归损失和角度回归损失合并,得到最终的多任务损失函数:</p>
                </div>
                <div class="p1">
                    <p id="70"><i>L</i>=<i>L</i><sub>cls</sub>+<i>λ</i><sub>1</sub><i>L</i><sub>loc</sub>+<i>λ</i><sub>2</sub><i>L</i><sub>angle</sub></p>
                </div>
                <div class="p1">
                    <p id="71">式中:<i>λ</i><sub>1</sub>和<i>λ</i><sub>2</sub>为平衡这三者的权重因子,本文通过交叉验证设置<i>λ</i><sub>1</sub>值为2,<i>λ</i><sub>2</sub>值为10。</p>
                </div>
                <h3 id="72" name="72" class="anchor-tag"><b>3 实 验</b></h3>
                <h4 class="anchor-tag" id="73" name="73"><b>3.1 实验准备</b></h4>
                <div class="p1">
                    <p id="74">本实验标注了5 320张来自物流包裹分拣现场的条形码图片,取出其中20%作为测试集,剩余作为训练集,并通过数据增强如小角度旋转、平移、翻折、缩放和裁剪等操作扩充训练样本集。输入图片大小为512×512,在送入模型网络之前进行去均值归一化等预处理操作。实验的软件环境为Ubuntu 16.04 LTS,使用的深度学习架构是keras;硬件的配置是:CPU为56 Intel(R) Xeon(R) CPU E5-2683 v3 @ 2.00 GHz;GPU为6块NVIDIA TITAN Xp,每块12 GB显存。</p>
                </div>
                <h4 class="anchor-tag" id="75" name="75"><b>3.2 角度矫正评价标准</b></h4>
                <div class="p1">
                    <p id="76">为了最终衡量条形码倾斜矫正的效果,通过以下角度评价标准进行计算:</p>
                </div>
                <div class="p1">
                    <p id="77" class="code-formula">
                        <mathml id="77"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>R</mi><msub><mrow></mrow><mi>α</mi></msub><mo>=</mo><mfrac><mrow><mo>#</mo><mo stretchy="false">(</mo><mi>i</mi><mo>∈</mo><mi>S</mi><mo stretchy="false">|</mo><mi>J</mi><mo stretchy="false">(</mo><mi>θ</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mi>θ</mi><msubsup><mrow></mrow><mi>i</mi><mo>*</mo></msubsup><mo stretchy="false">)</mo><mo>&lt;</mo><mi>α</mi><mo stretchy="false">)</mo></mrow><mrow><mrow><mo>|</mo><mi>S</mi><mo>|</mo></mrow></mrow></mfrac></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="78" class="code-formula">
                        <mathml id="78"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>J</mi><mo stretchy="false">(</mo><mi>θ</mi><mo>,</mo><mi>θ</mi><msup><mrow></mrow><mo>*</mo></msup><mo stretchy="false">)</mo><mo>=</mo><mrow><mo>|</mo><mrow><mi>θ</mi><mo>-</mo><mi>θ</mi><msup><mrow></mrow><mo>*</mo></msup></mrow><mo>|</mo></mrow></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="79">式中:<i>J</i>(<i>θ</i>,<i>θ</i><sup>*</sup>)代表预测回归角度<i>θ</i>和真值标注角度<i>θ</i><sup>*</sup>之间的绝对值误差;<i>α</i>为角度误差容忍因子;<i>S</i>为整个测试数据集中检测到条形码图片的数量;#()代表满足括号内条件的条形码图片个数;<i>R</i><sub><i>α</i></sub>为在某一角度误差容忍因子<i>α</i>下的矫正准确率。除此之外,为实际检验条形码矫正的效果,采用条形码译码开源算法ZXing和ZBar对矫正之后的图片进行识别。</p>
                </div>
                <h4 class="anchor-tag" id="80" name="80"><b>3.3 实验结果与分析</b></h4>
                <div class="p1">
                    <p id="81">首先,通过实验验证选择合理的角度回归损失函数。实验中单阶段目标检测算法选择YOLOv3和ResNet50-SSD,检测头部中象限分类值<i>M</i>设置为1,实验结果如表1所示。</p>
                </div>
                <div class="area_img" id="82">
                    <p class="img_tit"><b>表1 不同角度回归损失函数的矫正效果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note">%</p>
                    <table id="82" border="1"><tr><td><br />算法模型</td><td><i>R</i><sub>1</sub></td><td><i>R</i><sub>3</sub></td><td><i>R</i><sub>5</sub></td></tr><tr><td><br />YOLOv3+L2</td><td>61.24</td><td>79.43</td><td>87.58</td></tr><tr><td><br />YOLOv3+Smooth-L1</td><td>70.08</td><td>82.21</td><td>89.27</td></tr><tr><td><br />YOLOv3+Cosin</td><td><b>72.59</b></td><td>84.50</td><td><b>89.91</b></td></tr><tr><td><br />ResNet-SSD+L2</td><td>58.93</td><td>80.22</td><td>87.33</td></tr><tr><td><br />ResNet-SSD+Smooth-L1</td><td>69.89</td><td>83.03</td><td>88.79</td></tr><tr><td><br />ResNet-SSD+Cosin</td><td>71.31</td><td><b>84.73</b></td><td>89.27</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="83">从表1中可以看出,L2损失函数在<i>R</i><sub>1</sub>下表现远低于Smooth-L1和余弦距离损失函数,表明L2损失函数对于大角度误差更加敏感。另外,在<i>R</i><sub>1</sub>、<i>R</i><sub>3</sub>和<i>R</i><sub>5</sub>所有评价标准下,余弦距离损失函数均取得最优的表现效果,证明其更加适合角度回归相关任务。</p>
                </div>
                <div class="p1">
                    <p id="84">表2展示了本文设计的倾斜矫正算法与其他算法的对比结果。文献<citation id="115" type="reference">[<a class="sup">10</a>]</citation>采用YOLO目标检测算法和角度矫正卷积神经网络完成条码检测矫正,本文设计了YOLO+Ours(<i>M</i>=1)实验与之对比,其象限分类数<i>M</i>为1,即和文献<citation id="116" type="reference">[<a class="sup">10</a>]</citation>中角度矫正卷积神经网络一样直接回归倾斜角度。从结果可以得出,YOLO+Ours(<i>M</i>=1)不管在矫正准确率、实际译码率和耗时上都优于文献<citation id="117" type="reference">[<a class="sup">10</a>]</citation>,文献<citation id="118" type="reference">[<a class="sup">10</a>]</citation>设计的角度矫正网络存在冗余,没有充分利用YOLO目标检测算法已经通过卷积网络充分提取到的特征语义信息,卷积计算的共享程度低。因此本文直接将倾斜矫正任务嵌入到YOLO目标算法中取得更优效果。</p>
                </div>
                <div class="area_img" id="85">
                    <p class="img_tit"><b>表2 不同倾斜矫正算法效果对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="85" border="1"><tr><td>算法模型</td><td><i>R</i><sub>1</sub><br />/%</td><td><i>R</i><sub>3</sub><br />/%</td><td><i>R</i><sub>5</sub><br />/%</td><td>ZBar<br />/%</td><td>ZXing<br />/%</td><td>耗时<br />/ms</td></tr><tr><td><br />文献[10]</td><td>70.82</td><td>82.32</td><td>87.23</td><td>88.41</td><td>88.07</td><td>107</td></tr><tr><td><br />YOLO<br />(No Rotation)</td><td>/</td><td>/</td><td>/</td><td>22.17</td><td>21.30</td><td>73</td></tr><tr><td><br />YOLO+<br />GT Rotation</td><td>/</td><td>/</td><td>/</td><td><b>98.39</b></td><td><b>98.15</b></td><td>77</td></tr><tr><td><br />YOLO+<br />Hough</td><td>73.29</td><td>87.17</td><td>91.35</td><td>92.64</td><td>92.78</td><td>237</td></tr><tr><td><br />YOLO+Ours<br />(<i>M</i>=1)</td><td>71.35</td><td>83.91</td><td>88.29</td><td>89.31</td><td>89.17</td><td>83</td></tr><tr><td><br />YOLOv3+Ours<br />(<i>M</i>=1)</td><td>72.59</td><td>84.50</td><td>89.91</td><td>90.47</td><td>89.74</td><td><b>51</b></td></tr><tr><td><br />YOLOv3+Ours<br />(<i>M</i>=2)</td><td>74.26</td><td>89.38</td><td>93.47</td><td>93.89</td><td>93.45</td><td>53</td></tr><tr><td><br />YOLOv3+Ours<br />(<i>M</i>=3)</td><td>75.35</td><td>92.25</td><td>94.82</td><td>95.10</td><td>94.89</td><td>54</td></tr><tr><td><br />YOLOv3+Ours<br />(<i>M</i>=4)</td><td><b>75.42</b></td><td><b>92.73</b></td><td><b>95.03</b></td><td>95.21</td><td>94.93</td><td>56</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="86">作为空白对照,进行了YOLO(No Rotation)和YOLO+GT Rotation两组实验。YOLO(No Rotation)是直接进行条形码检测,不进行倾斜矫正操作,可以看出最终实际译码率非常低,说明倾斜矫正对于提升译码率至关重要。YOLO+GT Rotation是在YOLO算法检测到条形码之后,通过真值标注角度进行矫正,最终得到最高的译码率。除此之外,设计了基于传统数字图像算法Hough变换的实验YOLO+Hough,将YOLO算法检测出的条形码区域送入Hough变换算法进行矫正,YOLO+Hough在矫正准确率和实际译码率均取得不错效果,但是由于耗时237 ms,在实际物流包裹分拣场景下与其他算法相比并不占优。</p>
                </div>
                <div class="p1">
                    <p id="87">最后,以YOLOv3检测算法为基础设计了4组实验探究本文设计的倾斜矫正算法中象限分组数<i>M</i>对于最终结果的影响。结果表明,随着象限分组数<i>M</i>的增大,矫正准确率和实际译码率逐步提高,说明在直接回归倾斜角之前进行象限分组有助于降低回归角度的难度,但是在<i>M</i>大于等于3之后,提升效果有限。另外,<i>R</i><sub>1</sub>矫正准确率的提升幅度并没有<i>R</i><sub>3</sub>和<i>R</i><sub>5</sub>大,说明精确回归倾斜角度难度较大。</p>
                </div>
                <div class="p1">
                    <p id="88">图6展示了本文设计的倾斜矫正算法的效果示意图,倾斜位置的条形码图片能够很好地被矫正水平,完成了实际物流包裹分拣场景下条形码的矫正任务。</p>
                </div>
                <div class="area_img" id="89">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201910026_089.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 条形码检测矫正结果示意图" src="Detail/GetImg?filename=images/JYRJ201910026_089.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 条形码检测矫正结果示意图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201910026_089.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h3 id="90" name="90" class="anchor-tag"><b>4 结 语</b></h3>
                <div class="p1">
                    <p id="91">针对物流包裹分拣场景下的条形码识别问题,本文提出一种基于多任务目标检测的倾斜矫正算法。该算法以深度学习中的单阶段检测器为基础,嵌入了条形码倾斜角度预测的任务,通过先对条形码倾斜程度分类再回归角度的策略,大大降低了倾斜矫正任务的难度。最终通过实验验证本文算法在效果和速度上都明显优于其他算法,并且象限分组和余弦距离角度损失函数对最后矫正准确率提升明显。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The analyse of the various methods for location of Data Matrix codes in images">

                                <b>[1]</b> Karrach L,Pivarciova E.The analyse of the various methods for location of Data Matrix codes in images[C]//IEEE 2018 ELEKTRO.Mikulov,Czech Republic,2018:1-6.
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image recognition with deep learning for library book identification">

                                <b>[2]</b> Tang K,Lu H,Shi X.Image recognition with deep learning for library book identification[C]//Pacific Rim Conference on Multimedia.Springer,Cham,2018:684-696.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Vision-based 1D barcode localization method for scale and rotation invariant">

                                <b>[3]</b> Yun I,Kim J.Vision-based 1D barcode localization method for scale and rotation invariant[C]//TENCON 2017-2017 IEEE Region 10 Conference.IEEE,2017:2204-2208.
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=STJD&amp;filename=STJD08066EEC0C4C7B76CDA061C7C8E97370&amp;v=MTY4MzhuQmFyT3dIdGZLMnZvMlpKZ0xmM3RMeUJCZ25rNTlUbjZScTJFOURMdVRScjJmQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQnJMVTA1dHRnekx5OXhhdz1Oag==&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b> Xu L,Kamat V R,Menassa C C.Automatic extraction of 1D barcodes from video scans for drone-assisted inventory management in warehousing applications[J].International Journal of Logistics Research and Applications,2018,21(3):243-258.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=1017066864.nh&amp;v=MjA0MzA1NE8zenFxQnRHRnJDVVI3cWZadVp0RmlEaFVick1WRjI2R2JPK0dObktxNUViUElRS0RIODR2UjRUNmo=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b> 冯国文.基于SVM的复杂背景条码检测算法研究[D].浙江:浙江大学,2017.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=1D Barcode Region Detection Based on the Hough Transform and Support Vector Machine">

                                <b>[6]</b> Wang Z,Chen A,Li J,et al.1D barcode region detection based on the hough transform and support vector machine[C]//International Conference on Multimedia Modeling.Springer,Cham,2016:79-90.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=2D barcode detection using images for drone-assisted inventory management">

                                <b>[7]</b> Cho H,Kim D,Park J,et al.2D barcode detection using images for drone-assisted inventory management[C]//2018 15th International Conference on Ubiquitous Robots (UR).IEEE,2018:461-465.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Barcode recognition using principal component analysis and support vector machine">

                                <b>[8]</b> Mulyaningtyas C,Imah E M.Barcode recognition using principal component analysis and support vector machine[C]//Mathematics,Informatics,Science,and Education International Conference(MISEIC 2018).Atlantis Press,2018.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep dual pyramid network for barcode segmentation using Barcode-30k database[EB]">

                                <b>[9]</b> Zhao Q,Ni F,Song Y,et al.Deep dual pyramid network for barcode segmentation using Barcode-30k database[EB].arXiv preprint arXiv:1807.11886,2018.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Using deep ConvNet for robust 1D barcode detection">

                                <b>[10]</b> Li J,Zhao Q,Tan X,et al.Using deep ConvNet for robust 1D barcode detection[C]//International Conference on Intelligent and Interactive Systems and Applications.Springer,Cham,2017:261-267.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Faster R-CNN:towards real-time object detection with region proposal networks">

                                <b>[11]</b> Ren S,He K,Girshick R,et al.Faster R-CNN:Towards real-time object detection with region proposal networks[J].IEEE Transactions on Pattern Analysis &amp; Machine Intelligence,2015,39(6):1137-1149.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Real-time barcode detection and classification using deep learning">

                                <b>[12]</b> Hansen D K,Nasrollahi K,Rasmussen C B,et al.Real-time barcode detection and classification using deep learning[C]//Proceedings of the 9th International Joint Conference on Computational Intelligence—Volume 1:IJCCI.2017:321-327.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=You only look once:Unified,real-time object detection">

                                <b>[13]</b> Redmon J,Divvala S,Girshick R,et al.You only look once:Unified,real-time object detection[C]//Proceedings of the IEEE conference on computer vision and pattern recognition.2016:779-788.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_14" >
                                    <b>[14]</b>
                                 Liu W,Anguelov D,Erhan D,et al.Ssd:Single shot multibox detector[C]//European conference on computer vision.Springer,Cham,2016:21-37.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JYRJ201910026" />
        <input id="dpi" type="hidden" value="600" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201910026&amp;v=MTQ1MDJDVVI3cWZadVp0RmlEaFVick1MelRaWkxHNEg5ak5yNDlIWW9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnI=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="0" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
