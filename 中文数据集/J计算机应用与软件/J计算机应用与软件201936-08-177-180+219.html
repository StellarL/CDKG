<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637135610781565000%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJYRJ201908032%26RESULT%3d1%26SIGN%3daTKN03GInYPpxaOLYjwmq8Wriv8%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JYRJ201908032&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JYRJ201908032&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201908032&amp;v=MjcwMTZab1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnRGeWptVnIzQkx6VFpaTEc0SDlqTXA0OUc=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#37" data-title="&lt;b&gt;0 引 言&lt;/b&gt; "><b>0 引 言</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#43" data-title="&lt;b&gt;1 基于卷积的句子模型&lt;/b&gt; "><b>1 基于卷积的句子模型</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#47" data-title="&lt;b&gt;1.1 词嵌入层&lt;/b&gt;"><b>1.1 词嵌入层</b></a></li>
                                                <li><a href="#51" data-title="&lt;b&gt;1.2 卷积层&lt;/b&gt;"><b>1.2 卷积层</b></a></li>
                                                <li><a href="#57" data-title="&lt;b&gt;1.3 非线性层&lt;/b&gt;"><b>1.3 非线性层</b></a></li>
                                                <li><a href="#59" data-title="&lt;b&gt;1.4 最大池化层&lt;/b&gt;"><b>1.4 最大池化层</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#61" data-title="&lt;b&gt;2 匹配模型&lt;/b&gt; "><b>2 匹配模型</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#66" data-title="&lt;b&gt;2.1 交互层&lt;/b&gt;"><b>2.1 交互层</b></a></li>
                                                <li><a href="#72" data-title="&lt;b&gt;2.2 多层感知器&lt;/b&gt;"><b>2.2 多层感知器</b></a></li>
                                                <li><a href="#79" data-title="&lt;b&gt;2.3 训 练&lt;/b&gt;"><b>2.3 训 练</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#84" data-title="&lt;b&gt;3 实验结果与讨论&lt;/b&gt; "><b>3 实验结果与讨论</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#86" data-title="&lt;b&gt;3.1 数据集&lt;/b&gt;"><b>3.1 数据集</b></a></li>
                                                <li><a href="#89" data-title="&lt;b&gt;3.2 评价指标&lt;/b&gt;"><b>3.2 评价指标</b></a></li>
                                                <li><a href="#91" data-title="&lt;b&gt;3.3 词嵌入数据&lt;/b&gt;"><b>3.3 词嵌入数据</b></a></li>
                                                <li><a href="#93" data-title="&lt;b&gt;3.4 结果与讨论&lt;/b&gt;"><b>3.4 结果与讨论</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#96" data-title="&lt;b&gt;4 结 语&lt;/b&gt; "><b>4 结 语</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#45" data-title="图1 基于卷积的句子模型结构, 将输入句子映射为 中间的特征表示">图1 基于卷积的句子模型结构, 将输入句子映射为 中间的特征表示</a></li>
                                                <li><a href="#63" data-title="图2 整体卷积深度神经网络结构图, 用以语义匹配任务">图2 整体卷积深度神经网络结构图, 用以语义匹配任务</a></li>
                                                <li><a href="#88" data-title="&lt;b&gt;表1 NLPCC DBQA任务数据集&lt;/b&gt;"><b>表1 NLPCC DBQA任务数据集</b></a></li>
                                                <li><a href="#95" data-title="&lt;b&gt;表2 NLPCC DBQA数据集上的实验结果&lt;/b&gt;"><b>表2 NLPCC DBQA数据集上的实验结果</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" title=" Berger A, Caruana R, Cohn D, et al.Bridging the Lexical Chasm:Statistical Approaches to Answer-Finding[C]//Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval.ACM, 2000:192-199." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Bridging the Lexical Chasm: Statistical Approaches to Answer-?nding">
                                        <b>[1]</b>
                                         Berger A, Caruana R, Cohn D, et al.Bridging the Lexical Chasm:Statistical Approaches to Answer-Finding[C]//Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval.ACM, 2000:192-199.
                                    </a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" title=" Yu L, Hermann K M, Blunsom P, et al.Deep learning for answer sentence selection[EB].arXiv preprint arXiv:1412.1632, 2014." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep learning for answer sentence selection[EB]">
                                        <b>[2]</b>
                                         Yu L, Hermann K M, Blunsom P, et al.Deep learning for answer sentence selection[EB].arXiv preprint arXiv:1412.1632, 2014.
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title=" Liu P, Qiu X, Chen J, et al.Deep fusion LSTMs for text semantic matching[C]//Proceedings of Annual Meeting of the Association for Computational Linguistics, 2016." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep fusion LSTMs for text semantic matching">
                                        <b>[3]</b>
                                         Liu P, Qiu X, Chen J, et al.Deep fusion LSTMs for text semantic matching[C]//Proceedings of Annual Meeting of the Association for Computational Linguistics, 2016.
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" title=" Liu P, Qiu X, Huang X.Modelling interaction of sentence pair with coupled-lstms[EB].arXiv preprint arXiv:1605.05573, 2016." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Modelling interaction of sentence pair with coupled-lstms[EB]">
                                        <b>[4]</b>
                                         Liu P, Qiu X, Huang X.Modelling interaction of sentence pair with coupled-lstms[EB].arXiv preprint arXiv:1605.05573, 2016.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" title=" Hochreiter S, Schmidhuber J.Long short-term memory[J].Neural computation, 1997, 9 (8) :1735-1780." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJBK&amp;filename=SJBK15090500014198&amp;v=MTYzNDN4b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcnpJSUYwUmFCcz1OaWZKWmJLOUh0ak1xbzlGWk9vTERYVQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                         Hochreiter S, Schmidhuber J.Long short-term memory[J].Neural computation, 1997, 9 (8) :1735-1780.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" >
                                        <b>[6]</b>
                                     LeCun Y, Bottou L, Bengio Y, et al.Gradient-based learning applied to document recognition[J].Proceedings of the IEEE, 1998, 86 (11) :2278-2324.</a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" title=" Santos C, Tan M, Xiang B, et al.Attentive pooling networks[EB].arXiv preprint arXiv:1602.03609, 2016." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Attentive pooling networks[EB]">
                                        <b>[7]</b>
                                         Santos C, Tan M, Xiang B, et al.Attentive pooling networks[EB].arXiv preprint arXiv:1602.03609, 2016.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" title=" LeCun Y, Bengio Y.Convolutional networks for images, speech, and time series[M]//The handbook of brain theory and neural networks.MIT Press, 1998:255-258." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Convolutional Networks for Images,Speech,and Time Series">
                                        <b>[8]</b>
                                         LeCun Y, Bengio Y.Convolutional networks for images, speech, and time series[M]//The handbook of brain theory and neural networks.MIT Press, 1998:255-258.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" title=" Kalchbrenner N, Grefenstette E, Blunsom P.A convolutional neural network for modelling sentences[EB].arXiv preprint arXiv:1404.2188, 2014." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A convolutional neural network for modelling sentences[EB]">
                                        <b>[9]</b>
                                         Kalchbrenner N, Grefenstette E, Blunsom P.A convolutional neural network for modelling sentences[EB].arXiv preprint arXiv:1404.2188, 2014.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_10" title=" Wang Z, Mi H, Ittycheriah A.Sentence similarity learning by lexical decomposition and composition[EB].arXiv preprint arXiv:1602.07019, 2016." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Sentence similarity learning by lexical decomposition and composition[EB]">
                                        <b>[10]</b>
                                         Wang Z, Mi H, Ittycheriah A.Sentence similarity learning by lexical decomposition and composition[EB].arXiv preprint arXiv:1602.07019, 2016.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_11" title=" Hu B, Lu Z, Li H, et al.Convolutional neural network architectures for matching natural language sentences[C]//Proceedings of the 27th International Conference on Neural Information Processing Systems—Volume 2.MIT Press, 2014:2042-2050." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Convolutional Neural Network Architectures for Matching Natural Language Sentences">
                                        <b>[11]</b>
                                         Hu B, Lu Z, Li H, et al.Convolutional neural network architectures for matching natural language sentences[C]//Proceedings of the 27th International Conference on Neural Information Processing Systems—Volume 2.MIT Press, 2014:2042-2050.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_12" title=" Severyn A, Moschitti A.Learning to rank short text pairs with convolutional deep neural networks[C]//Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval.ACM, 2015:373-382." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning to rank short text pairs with convolutional deep neural networks">
                                        <b>[12]</b>
                                         Severyn A, Moschitti A.Learning to rank short text pairs with convolutional deep neural networks[C]//Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval.ACM, 2015:373-382.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_13" title=" Yin W, Sch&#252;tze H, Xiang B, et al.ABCNN:Attention-Based Convolutional Neural Network for Modeling Sentence Pairs[EB].arXiv preprint arXiv:1512.05193, 2015." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=ABCNN:Attention-Based Convolutional Neural Network for Modeling Sentence Pairs[EB]">
                                        <b>[13]</b>
                                         Yin W, Sch&#252;tze H, Xiang B, et al.ABCNN:Attention-Based Convolutional Neural Network for Modeling Sentence Pairs[EB].arXiv preprint arXiv:1512.05193, 2015.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_14" title=" Mikolov T, Sutskever I, Chen K, et al.Distributed Representations of Words and Phrases and their Compositionality[J].Advances in Neural Information Processing Systems, 2013, 26:3111-3119." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Distributed representations of words and phrases and their compositionality">
                                        <b>[14]</b>
                                         Mikolov T, Sutskever I, Chen K, et al.Distributed Representations of Words and Phrases and their Compositionality[J].Advances in Neural Information Processing Systems, 2013, 26:3111-3119.
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_15" title=" Bordes A, Glorot X, Weston J, et al.A semantic matching energy function for learning with multi-relational data[J].Machine Learning, 2014, 94 (2) :233-259." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD14030300067285&amp;v=Mjk0MjZadEZpbmxVcnpJSUYwUmFCcz1OajdCYXJLOEh0TE1ySTlGWk8wSURuUThvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[15]</b>
                                         Bordes A, Glorot X, Weston J, et al.A semantic matching energy function for learning with multi-relational data[J].Machine Learning, 2014, 94 (2) :233-259.
                                    </a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_16" title=" Bengio Y.Learning Deep Architectures for AI[J].Foundations and trends in Machine Learning, 2009, 2 (1) :1-127." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning deep architectures for AI">
                                        <b>[16]</b>
                                         Bengio Y.Learning Deep Architectures for AI[J].Foundations and trends in Machine Learning, 2009, 2 (1) :1-127.
                                    </a>
                                </li>
                                <li id="35">


                                    <a id="bibliography_17" title=" Bordes A, Weston J, Usunier N.Open question answering with weakly supervised embedding models[C]//Joint European Conference on Machine Learning and Knowledge Discovery in Databases.Springer, 2014:165-180." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Open question answering with weakly supervised embedding models">
                                        <b>[17]</b>
                                         Bordes A, Weston J, Usunier N.Open question answering with weakly supervised embedding models[C]//Joint European Conference on Machine Learning and Knowledge Discovery in Databases.Springer, 2014:165-180.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JYRJ" target="_blank">计算机应用与软件</a>
                2019,36(08),177-180+219 DOI:10.3969/j.issn.1000-386x.2019.08.031            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>卷积深度神经网络在基于文档的自动问答任务中的应用与改进</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%82%85%E5%81%A5&amp;code=42373483&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">傅健</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%A4%8D%E6%97%A6%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%AD%A6%E9%99%A2&amp;code=0075855&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">复旦大学计算机学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>基于文档的自动问答, 尤其是语义匹配, 其目标是计算两个文本之间的相似度。这是自然语言处理中的典型任务, 并且用以衡量对自然语言的理解程度。深度学习方法得益于可以自动化地学习到给定任务的最优特征表示, 在许多研究中取得成功, 也包括文本匹配。针对基于文档的自动问答, 提出一个基于卷积深度神经网络的语义匹配模型, 以便对每一对问题和文档提取特征, 并据此计算它们的得分。通过问题和文档之间的交互计算, 利用重叠词等文本特征, 在中文开放域上的自动问答任务中取得的实际效果证明了该模型的有效性。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">卷积神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%87%AA%E5%8A%A8%E9%97%AE%E7%AD%94&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">自动问答;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%AF%AD%E4%B9%89%E5%8C%B9%E9%85%8D&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">语义匹配;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">自然语言处理;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    傅健, 硕士生, 主研领域:自然语言处理。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-01-17</p>

            </div>
                    <h1><b>APPLICATION AND IMPROVEMENT OF CONVOLUTIONAL DEPTH NEURAL NETWORK IN DOCUMENT-BASED QUESTION ANSWERING TASK</b></h1>
                    <h2>
                    <span>Fu Jian</span>
            </h2>
                    <h2>
                    <span>School of Computer Science, Fudan University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Document-based automatic question answering, especially semantic matching, aims to compute the similarity or relevance between two documents. It is a typical task in NLP and considered as a touch-stone of natural language understanding. Deep learning has been successful in many studies, including text matching, because it can automatically learn the optimal feature representation of a given task. Aiming at document-based question answering, I proposed a structure based on convolution depth neural network to extract features from each pair of questions and documents and calculate their scores. The effectiveness of the model was proved by the interactive computation between questions and documents, as well as the use of overlapping words and other text features in the automatic question answering task in the Chinese open domain.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Convolutional%20neural%20network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Convolutional neural network;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Question%20answering&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Question answering;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Deep%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Deep learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Semantic%20match&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Semantic match;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Natural%20language%20processing&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Natural language processing;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2019-01-17</p>
                            </div>


        <!--brief start-->
                        <h3 id="37" name="37" class="anchor-tag"><b>0 引 言</b></h3>
                <div class="p1">
                    <p id="38">语义匹配是自然语言处理中非常重要的一个任务。它旨在将多个句子进行建模并计算它们的相似性或相关度, 是许多具体应用中非常中心的环节, 包括自动问答<citation id="98" type="reference"><link href="3" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>、答案句子选择<citation id="99" type="reference"><link href="5" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>、信息检索、释义识别和文本蕴涵<citation id="100" type="reference"><link href="7" rel="bibliography" /><link href="9" rel="bibliography" /><sup>[<a class="sup">3</a>,<a class="sup">4</a>]</sup></citation>等。</p>
                </div>
                <div class="p1">
                    <p id="39">在自动问答 (Question Answering) 任务中, 对自然语句的建模和理解是非常基本和重要的, 其难点通常在于自然语句由于时序和层次关系所带来的复杂结构。在许多神经网络模型中, 我们可以用长短时记忆网络LSTM<citation id="101" type="reference"><link href="11" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation> (Long Short-Term Memory) , 一种循环神经网络RNN的改进版本, 亦或者是本文使用的卷积神经网络CNN<citation id="102" type="reference"><link href="13" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation> (Convolutional Neural Network) , 去对句子和句对作建模。此外, 通常也会使用双向LSTM<citation id="103" type="reference"><link href="15" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>。CNN对局部的计算选择 (local selecting) 能力<citation id="104" type="reference"><link href="17" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>, 使得它拥有提取输入的有效特征和抽象特征。同时考虑到其便于修改的优点<citation id="105" type="reference"><link href="19" rel="bibliography" /><link href="21" rel="bibliography" /><sup>[<a class="sup">9</a>,<a class="sup">10</a>]</sup></citation>, 所以本文选择使用CNN去做句子建模。</p>
                </div>
                <div class="p1">
                    <p id="40">一个好的句子匹配算法, 除了需要对自然语句的内部结构进行建模之外, 还需要考虑它们之间的交互。我们可以利用这些句对之间丰富的匹配模式信息, 来获得更好的匹配得分。所以, 在基本的如ARC-I<citation id="106" type="reference"><link href="23" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>这样的句子匹配模型的基础上, 提出一种将两个句子进行词和词的对齐的方法 (如ARC-II<citation id="107" type="reference"><link href="23" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>) 。除了对齐和共同使用这些句对之外, 我们还可以通过相似度匹配<citation id="108" type="reference"><link href="25" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>达到这样的目的。具体地, 我们计算出一个相似度单元, 并代入到后续的计算中去。这里需要补充说明的是, 对于相似度的计算, 可以放在神经网络的上游, 也可以放在下游, 亦或者上下游同时计算<citation id="109" type="reference"><link href="27" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="41">在上述基础上, 我们也可以增加一些无需使用外部知识的特征, 从而配合神经网络, 进一步提高任务的表现 (见实验) , 例如TF-IDF、重叠词指示器<citation id="110" type="reference"><link href="5" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>、带逆文本频率指数加权 (IDF-weighted) 的重叠词特征等。</p>
                </div>
                <div class="p1">
                    <p id="42">综上所述, 我们提出应用了基于卷积深度神经网络的语义匹配模型, 不仅通过多层的卷积和池化, 利用了句子各自内部独立的结构信息, 而且能够捕捉每一对问题和答案之间丰富的模式信息。本文还将神经网络模型应用在了基于文档的中文自动问答任务 (NLPCC DBQA Task) 上, 取得了不错的效果, 并且能够配合额外的词重叠等构建的无外部知识特征进一步提升效果。</p>
                </div>
                <h3 id="43" name="43" class="anchor-tag"><b>1 基于卷积的句子模型</b></h3>
                <div class="p1">
                    <p id="44">受许多卷积网络模型的启发<citation id="111" type="reference"><link href="19" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>, 本文基于卷积的句子模型ConvNet如图1所示, 其目的是对每一对问题query和文档document都能学习出有效的中间特征表示, 并用于后续的语义匹配。模型将输入的句子序列, 在词嵌入处理之后, 再用多层 (或一层) 卷积层和最大池化层进行信息提取, 最终得到一个定长的向量表示, 并以此作为特征表示。需要额外说明的是, 卷积时可以使用多个过滤器, 其数量作为超参进行调节。</p>
                </div>
                <div class="area_img" id="45">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201908032_045.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 基于卷积的句子模型结构, 将输入句子映射为 中间的特征表示" src="Detail/GetImg?filename=images/JYRJ201908032_045.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 基于卷积的句子模型结构, 将输入句子映射为 中间的特征表示  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201908032_045.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="46">我们的句子模型按顺序由词嵌入层、卷积层、非线性层和最大池化层组成, 以下将依次对这4个神经网络层作详细介绍。</p>
                </div>
                <h4 class="anchor-tag" id="47" name="47"><b>1.1 词嵌入层</b></h4>
                <div class="p1">
                    <p id="48">词嵌入层的目的是将原先输入的字或词 (可以分词的话) 映射到一定维度的表示空间中, 从而赋予字或词更丰富的上下文信息或同级信息。常用的词嵌入方法主要为两种, 一是word2vec<citation id="112" type="reference"><link href="29" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>, 基于字或词和上下文周边文本的相互预测;二是GloVe, 基于字或词在所有文本中前后出现次数。总而言之, 词嵌入都利用的是字或词在上下文文本中丰富的统计信息。具体地, 在本文的自动问答任务中, 我们首先对问题和答案这两个输入序列先作中文分词之后 (利用分词工具, 如jieba等) , 再用word2vec对序列中的每个词作分布式表示, 详细步骤如下:</p>
                </div>
                <div class="p1">
                    <p id="49">首先, 整个神经网络的输入看作一串词的序列:[<i>w</i><sub>1</sub>, <i>w</i><sub>2</sub>, …, <i>w</i><sub><i>l</i></sub>], 且每个词均来自词表<b><i>V</i></b>。每个词会依据预先训练好的word2vec词嵌入, 从已有的嵌入矩阵<b><i>W</i></b>∈<i>R</i><sup>|<i>v</i>|×<i>d</i></sup>中获得对应的分布式向量<b><i>w</i></b>∈<i>R</i><sup><i>d</i></sup>。</p>
                </div>
                <div class="p1">
                    <p id="50">除了词嵌入表示之外, 我们还对句子中的每一个词增加了重叠指示器特征, 表示当前词是否在另一个句子中出现。这将作为额外的输入, 并参与后续匹配打分的计算 (见图1模型结构) 。最终, 我们对问题和文档, 都将分别得到句子矩阵<b><i>S</i></b>∈<i>R</i><sup><i>L</i>× (<i>d</i>+1) </sup>。</p>
                </div>
                <h4 class="anchor-tag" id="51" name="51"><b>1.2 卷积层</b></h4>
                <div class="p1">
                    <p id="52">卷积层即对当前的输入以卷积作为基本操作, 进行进一步处理。与全连接 (fully connected) 网络相比, 它们的计算方式相同, 只不过卷积的输入为其中的一段定长的窗口大小。通常卷积神经网络的输入为图片, 具体地, 图片输入可看作是长度、宽度、深度 (depth或channel) 的三围张量, 卷积时则用一个或多个拥有长度、宽度的filter在图片的每个channel上计算。那么应用到自然语言处理任务时, 我们可以把输入的channel看作1, 而卷积时filter的size设定为词的embedding。前面介绍过, 卷积的作用可以有效提取局部特征, 进一步地, 我们可以设置多个卷积窗口大小, 从而在局部特征外, 得到尽可能更好的全局特征。下面进行详细步骤介绍。</p>
                </div>
                <div class="p1">
                    <p id="53">卷积层的目标是提取特征或模式信息, 具体地, 在给定输入 (词级别) 序列<i>q</i><sup>emb</sup>=<i>r</i><sup><i>w</i><sub>1</sub></sup>, <i>r</i><sup><i>w</i><sub>2</sub></sup>, …, <i>r</i><sup><i>w</i><sub><i>l</i></sub></sup>之后, 我们定义矩阵<b><i>Z</i></b><sub><i>q</i></sub>=[<b><i>z</i></b><sub>1</sub>, <b><i>z</i></b><sub>2</sub>, …, <b><i>z</i></b><sub><i>l</i></sub>], 每一列包含一个向量<b><i>z</i></b><sub><i>i</i></sub>∈<i>R</i><sup>dws</sup>, 代表序列中<i>w</i><sub><i>s</i></sub>的词的嵌入表示。在我们对问题<i>q</i>, 用<i>c</i>个 (窗口大小不同的) filter进行卷积操作后, 得到:</p>
                </div>
                <div class="p1">
                    <p id="54"><b><i>Q</i></b>=<i>W</i><b><i>Z</i></b><sub><i>q</i></sub>+<i>b</i>      (1) </p>
                </div>
                <div class="p1">
                    <p id="55">式中:<b><i>Q</i></b>∈<i>R</i><sup><i>l</i>×<i>c</i></sup>中的每一行<i>m</i>包含了以<i>q</i>中的第<i>m</i>个词为中心的文本窗口提取出来的特征, <i>W</i>和<i>b</i>为神经网络训练中待学习的参数。需要说明的是, 卷积过滤器的数量为<i>c</i>, 以及词级别文本窗口大小<i>w</i><sub><i>s</i></sub>均为超参, 需要进行手动选择。</p>
                </div>
                <div class="p1">
                    <p id="56">接着, 我们用相同的方式计算出答案<i>a</i>的输出矩阵<b><i>A</i></b>。在这里, 神经网络可以选择共享参数或相互独立。</p>
                </div>
                <h4 class="anchor-tag" id="57" name="57"><b>1.3 非线性层</b></h4>
                <div class="p1">
                    <p id="58">为了使得神经网络能够学习到非线性决策边际, 从而更好地提取出特征的表示, 我们需要在线性的卷积层之后增加一层非线性激活函数α () 。需要说明的是, 这将会应用在之前卷积处理之后结果的每一位元素上。通常, 非线性激活函数有多种选择, 如relu (常见于图像任务重) 、sigmoid、tanh等。在本文中, 非线性激活函数默认使用的是双曲正切函数tanh, 它会将原先的输出重新映射到[-1, 1]之间。</p>
                </div>
                <h4 class="anchor-tag" id="59" name="59"><b>1.4 最大池化层</b></h4>
                <div class="p1">
                    <p id="60">在过了卷积层与非线性函数计算之后, 我们还需要池化层来将信息进行融合, 并且同时将表示进行压缩。在卷积中提取特征时, 通常采用最大池化或者平均池化。在本文中, 我们选择使用最大池化, 即在每一个filter中都选取最大值作为输出, 以便为后续的语义匹配提供有效信息。</p>
                </div>
                <h3 id="61" name="61" class="anchor-tag"><b>2 匹配模型</b></h3>
                <div class="p1">
                    <p id="62">整个句对匹配模型的结构如图2所示。我们基于卷积的句子模型 (前面所述) , 能够学习将输入的句对各自表示为中间向量表示, 进而我们可以继续利用它们去计算相似度。与传统做法类似<citation id="114" type="reference"><link href="17" rel="bibliography" /><link href="31" rel="bibliography" /><sup>[<a class="sup">8</a>,<a class="sup">15</a>]</sup></citation>:在我们获得两个句子的矩阵表示之后, 将其作为一个多层感知器<citation id="113" type="reference"><link href="33" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation> (multi-layer perceptron, MLP) 的输入, 并得到 (分类或分数) 输出。</p>
                </div>
                <div class="area_img" id="63">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201908032_063.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 整体卷积深度神经网络结构图, 用以语义匹配任务" src="Detail/GetImg?filename=images/JYRJ201908032_063.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 整体卷积深度神经网络结构图, 用以语义匹配任务  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201908032_063.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="64">但是, 这里与之前工作不同的是, 我们还可以进一步地提高两个句子之间的交互性, 例如计算额外的问题<i>q</i>与文档<i>d</i>的相似度得分、用attention方法对原来的结果进行重新权重上的表示等。</p>
                </div>
                <div class="p1">
                    <p id="65">接下来, 我们将描述如何使用之前得到的中间特征表示, 计算出相似度得分 (或称为相似度单元) , 以及介绍其余的神经网络层。</p>
                </div>
                <h4 class="anchor-tag" id="66" name="66"><b>2.1 交互层</b></h4>
                <div class="p1">
                    <p id="67">在得到输入句对各自的中间向量表示之后, 最常见的做法是将其进行拼接操作后作为输入一起带到后续的网络层 (如多层感知器) 中计算。但这种做法的一个弊端在于没有充分地提取出它们作为匹配的丰富模式信息, 故需要加上交互层, 用以将句对间的匹配信息表示出来。具体地, 我们可以选择将句对作为输入, 额外计算一些表征相似度的中间结果, 并一同带入后续网络层。下面进行详细的步骤介绍。</p>
                </div>
                <div class="p1">
                    <p id="68">在我们用基于卷积的神经网络句子模型分别对问题query和文档document进行计算后, 得到了中间向量的特征表示<i>x</i><sub>q</sub>和<i>x</i><sub>d</sub>。通常<i>x</i><sub>q</sub>和<i>x</i><sub>d</sub>均为二维张量, 第二维维度<i>d</i>相同 (也可以不同, 不同时需改变后面的参数矩阵大小) 。据此, 我们可以进一步计算句对的相似度得分 (或称为相似度单元) 。借鉴已有的方法<citation id="115" type="reference"><link href="35" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>, 我们可以通过如下方式计算出相似度单元:</p>
                </div>
                <div class="p1">
                    <p id="69"><i>sim</i>_<i>score</i> (<i>x</i><sub>q</sub>, <i>x</i><sub>d</sub>) =<i>x</i><sub>q</sub><b><i>M</i></b><i>x</i><mathml id="70"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mtext>d</mtext><mtext>Τ</mtext></msubsup></mrow></math></mathml>      (2) </p>
                </div>
                <div class="p1">
                    <p id="71">式中:<b><i>M</i></b>∈<i>R</i><sup><i>d</i>×<i>d</i></sup>是一个相似度矩阵, 作为参数, 随着训练进行优化、更新。需要说明的是, 在这里可以选择同时计算一个或多个这样的相似度单元 (相应地, 会用到一个或多个参数矩阵<b><i>M</i></b>) , 从而得到提取到更多的有效信息, 带入后续的计算中。</p>
                </div>
                <h4 class="anchor-tag" id="72" name="72"><b>2.2 多层感知器</b></h4>
                <div class="p1">
                    <p id="73">MLP是一个常见的基本的网络结构。它基于前向反馈网络, 含有多层网络层, 包括输入层、中间的隐层、输出层等, 每一层均为全连接层, 即下一层的输出都依赖上一层的所有神经元。整个多层感知器, 最终会得到一个分类或分数输出, 之后根据选择的目标函数进行反向传播, 更新前面的参数以使得相应的目标函数降低 (向梯度下降的方向) 。在本文中, 对于自动问答任务, 我们简单地将其看作是一个0-1 (二) 分类任务, 即正确答案看作1, 错误答案看作0 (多个正确语句的前后顺序不影响评价指标的高低) 。</p>
                </div>
                <div class="p1">
                    <p id="74">具体地, 本文使用的MLP除了输入层外, 还由一层隐藏层, 即一层线性全连接层, 和一个逻辑回归组成 (softmax层) , 最终得到一个二值分类的输出结果。隐藏层的计算如下所示:</p>
                </div>
                <div class="p1">
                    <p id="75"><i>f</i> (<b><i>w</i></b><sub>h</sub><i>x</i>+<i>b</i>)      (3) </p>
                </div>
                <div class="p1">
                    <p id="76">式中:<b><i>w</i></b><sub>h</sub>和<i>b</i>为隐藏层的参数向量, <b><i>w</i></b><sub>h</sub>为隐层的权重向量, <i>b</i>为激活阈值。另外, <i>f</i>是一个非线性激活函数 (默认为tanh函数) 。</p>
                </div>
                <div class="p1">
                    <p id="77">接下里类似地进行一层输出层的计算, 但在线性计算后无需跟一个非线性函数。而是对计算的结果进行逻辑回归计算 (以得到二值分类) , softmax函数会计算出所有可能标签 (是否为正确答案) 的概率分布, 如下所示:</p>
                </div>
                <div class="p1">
                    <p id="78"><i>p</i> (<i>y</i>=<i>j</i>|<i>x</i>) =e<sup><i>yj</i></sup>/∑e<sup><i>yk</i></sup>      (4) </p>
                </div>
                <h4 class="anchor-tag" id="79" name="79"><b>2.3 训 练</b></h4>
                <div class="p1">
                    <p id="80">在得到本文自动问答任务中是否为正确答案的概率分布表示之后, 需要为模型定义一个训练目标函数或者损失函数。根据具体任务的不同, 损失函数可以选择交叉熵损失函数、均方差误差损失函数、最大边际损失、其他距离衡量等。而在选取最终的目标函数之后, 我们模型的所有网络层的参数才能进行相应的更新, 如使用批量的随机梯度下降 (SGD) 等优化方法, 最终使得整个任务的performance提高。</p>
                </div>
                <div class="p1">
                    <p id="81">在本文的自动问答任务中, 我们将输出看作了0-1二值分类任务, 并选择随机梯度下降进行模型参数更新。具体地, 整个模型的训练目标为降低交叉熵损失函数为:</p>
                </div>
                <div class="p1">
                    <p id="82"> (<i>y</i>, <i>o</i>) =- (1/<i>N</i>) ∑<i>y</i><sub><i>n</i></sub>log<i>o</i><sub><i>n</i></sub><i>n</i>∈<i>N</i>      (5) </p>
                </div>
                <div class="p1">
                    <p id="83">式中:<i>y</i>为正确标签答案, <i>o</i>为预测得分, <i>N</i>为类别 (两类) 。模型对每一轮输入的batch-size的数据进行如上更新, 并进行多轮迭代, 最终以early-stop的结果得到最优的模型。</p>
                </div>
                <h3 id="84" name="84" class="anchor-tag"><b>3 实验结果与讨论</b></h3>
                <div class="p1">
                    <p id="85">接下来, 我们讨论将上述基于卷积的语义匹配模型应用到具体的自然语言处理任务中去, 并加以改进。数据集上, 我们选取了中文的自动问答任务, 其数据集来源于NLPCC的DBQA任务 (document-based QA Task) 。配合其他如词重叠等特征, 模型取得了实际有效的结果。</p>
                </div>
                <h4 class="anchor-tag" id="86" name="86"><b>3.1 数据集</b></h4>
                <div class="p1">
                    <p id="87">我们在开放域上的基于文档的中文自动问答数据集上进行了实验, 数据集如表1所示。其中, dbqa-train和dbqa-test的QA-pairs个数分别为181 882和122 531。经统计, 在中文分词后, 大多数问题语句的长度在不到20个词左右, 而大多数答案语句的长度在不到40个词左右。</p>
                </div>
                <div class="area_img" id="88">
                    <p class="img_tit"><b>表1 NLPCC DBQA任务数据集</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="88" border="1"><tr><td><br />数据集</td><td>QA-pairs个数</td></tr><tr><td><br />DBQA-train</td><td>181 882</td></tr><tr><td><br />DBQA-test</td><td>122 531</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="89" name="89"><b>3.2 评价指标</b></h4>
                <div class="p1">
                    <p id="90">关于任务的评价指标, 考虑到DBQA任务实际上定义为一个排序任务, 因此我们选择使用更合适的Mean Reciprocal Rank (MRR) 、Mean Average Precision (MAP) 这两种常见的搜索排序指标进行评价, 而不采用正确率。其中, 以MRR指标结果为主。</p>
                </div>
                <h4 class="anchor-tag" id="91" name="91"><b>3.3 词嵌入数据</b></h4>
                <div class="p1">
                    <p id="92">在词嵌入上, 我们用word2vec方法在中文wiki百科语料上训练了word embedding, 其中包含了超过230 000的中文文章。并且值得一提的是, 预训练的word embedding对任务效果具有很大的帮助。</p>
                </div>
                <h4 class="anchor-tag" id="93" name="93"><b>3.4 结果与讨论</b></h4>
                <div class="p1">
                    <p id="94">实验结果具体如表2所示。整个表格依次展示了: (1) 基于CNN的神经网络基准线的效果; (2) 额外增加的计算相似度单元所带来的效果提升; (3) 额外增加特征 (词重叠特征) 所带来的效果体征; (4) 基于模型并配合额外特征 (词重叠特征) 所共同带来的效果提升。可以看到, 相似度单元和词重叠特征都分别起到了很大的作用。并且, 基于卷积的语义匹配模型可以与额外的特征共同配合, 得到更好的任务效果。需要说明的是, 这几种神经网络的超参并未做过多的调整, 后续还有待做更多的探究。</p>
                </div>
                <div class="area_img" id="95">
                    <p class="img_tit"><b>表2 NLPCC DBQA数据集上的实验结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="95" border="1"><tr><td><br />模型</td><td>MAP</td><td>MRR</td></tr><tr><td><br />基于CNN的基本匹配模型</td><td>36.41</td><td>36.42</td></tr><tr><td><br />+相似度单元</td><td>59.14</td><td>59.21</td></tr><tr><td><br />+词重叠特征</td><td>77.17</td><td>77.24</td></tr><tr><td><br />+相似度单元+词重叠特征</td><td>84.90</td><td>84.96</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h3 id="96" name="96" class="anchor-tag"><b>4 结 语</b></h3>
                <div class="p1">
                    <p id="97">在本文中, 我们提出并运用了适用于语义匹配的基于卷积的深度神经网络结构, 它不仅考虑了对每个独立句子作层次化建模, 同时还提取出它们的匹配模式特征。并且, 在开放域上的基于文档的中文自动问答任务中, 配合词重叠等其他特征, 本模型取得了实际有效的结果。后续仍有待作进一步的研究。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Bridging the Lexical Chasm: Statistical Approaches to Answer-?nding">

                                <b>[1]</b> Berger A, Caruana R, Cohn D, et al.Bridging the Lexical Chasm:Statistical Approaches to Answer-Finding[C]//Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval.ACM, 2000:192-199.
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep learning for answer sentence selection[EB]">

                                <b>[2]</b> Yu L, Hermann K M, Blunsom P, et al.Deep learning for answer sentence selection[EB].arXiv preprint arXiv:1412.1632, 2014.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep fusion LSTMs for text semantic matching">

                                <b>[3]</b> Liu P, Qiu X, Chen J, et al.Deep fusion LSTMs for text semantic matching[C]//Proceedings of Annual Meeting of the Association for Computational Linguistics, 2016.
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Modelling interaction of sentence pair with coupled-lstms[EB]">

                                <b>[4]</b> Liu P, Qiu X, Huang X.Modelling interaction of sentence pair with coupled-lstms[EB].arXiv preprint arXiv:1605.05573, 2016.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJBK&amp;filename=SJBK15090500014198&amp;v=MzI0NDBaYks5SHRqTXFvOUZaT29MRFhVeG9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaW5sVXJ6SUlGMFJhQnM9TmlmSg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b> Hochreiter S, Schmidhuber J.Long short-term memory[J].Neural computation, 1997, 9 (8) :1735-1780.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" >
                                    <b>[6]</b>
                                 LeCun Y, Bottou L, Bengio Y, et al.Gradient-based learning applied to document recognition[J].Proceedings of the IEEE, 1998, 86 (11) :2278-2324.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Attentive pooling networks[EB]">

                                <b>[7]</b> Santos C, Tan M, Xiang B, et al.Attentive pooling networks[EB].arXiv preprint arXiv:1602.03609, 2016.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Convolutional Networks for Images,Speech,and Time Series">

                                <b>[8]</b> LeCun Y, Bengio Y.Convolutional networks for images, speech, and time series[M]//The handbook of brain theory and neural networks.MIT Press, 1998:255-258.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A convolutional neural network for modelling sentences[EB]">

                                <b>[9]</b> Kalchbrenner N, Grefenstette E, Blunsom P.A convolutional neural network for modelling sentences[EB].arXiv preprint arXiv:1404.2188, 2014.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Sentence similarity learning by lexical decomposition and composition[EB]">

                                <b>[10]</b> Wang Z, Mi H, Ittycheriah A.Sentence similarity learning by lexical decomposition and composition[EB].arXiv preprint arXiv:1602.07019, 2016.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Convolutional Neural Network Architectures for Matching Natural Language Sentences">

                                <b>[11]</b> Hu B, Lu Z, Li H, et al.Convolutional neural network architectures for matching natural language sentences[C]//Proceedings of the 27th International Conference on Neural Information Processing Systems—Volume 2.MIT Press, 2014:2042-2050.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning to rank short text pairs with convolutional deep neural networks">

                                <b>[12]</b> Severyn A, Moschitti A.Learning to rank short text pairs with convolutional deep neural networks[C]//Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval.ACM, 2015:373-382.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=ABCNN:Attention-Based Convolutional Neural Network for Modeling Sentence Pairs[EB]">

                                <b>[13]</b> Yin W, Schütze H, Xiang B, et al.ABCNN:Attention-Based Convolutional Neural Network for Modeling Sentence Pairs[EB].arXiv preprint arXiv:1512.05193, 2015.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Distributed representations of words and phrases and their compositionality">

                                <b>[14]</b> Mikolov T, Sutskever I, Chen K, et al.Distributed Representations of Words and Phrases and their Compositionality[J].Advances in Neural Information Processing Systems, 2013, 26:3111-3119.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_15" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD14030300067285&amp;v=MjIxNTlJSUYwUmFCcz1OajdCYXJLOEh0TE1ySTlGWk8wSURuUThvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp0RmlubFVyeg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[15]</b> Bordes A, Glorot X, Weston J, et al.A semantic matching energy function for learning with multi-relational data[J].Machine Learning, 2014, 94 (2) :233-259.
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning deep architectures for AI">

                                <b>[16]</b> Bengio Y.Learning Deep Architectures for AI[J].Foundations and trends in Machine Learning, 2009, 2 (1) :1-127.
                            </a>
                        </p>
                        <p id="35">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Open question answering with weakly supervised embedding models">

                                <b>[17]</b> Bordes A, Weston J, Usunier N.Open question answering with weakly supervised embedding models[C]//Joint European Conference on Machine Learning and Knowledge Discovery in Databases.Springer, 2014:165-180.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JYRJ201908032" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201908032&amp;v=MjcwMTZab1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnRGeWptVnIzQkx6VFpaTEc0SDlqTXA0OUc=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
