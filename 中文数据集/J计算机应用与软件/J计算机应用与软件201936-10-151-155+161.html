<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637135556331721250%26DBCODE%3dCJFD%26TABLEName%3dCJFDTEMP%26FileName%3dJYRJ201910028%26RESULT%3d1%26SIGN%3dCLXuLKbZaJ9g3tS9bPny0RnSLg4%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JYRJ201910028&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JYRJ201910028&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201910028&amp;v=MTgyMjBGckNVUjdxZlp1WnRGaURoVWJ2S0x6VFpaTEc0SDlqTnI0OUhiSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEc=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#49" data-title="&lt;b&gt;0 引 言&lt;/b&gt; "><b>0 引 言</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#54" data-title="&lt;b&gt;1 相关工作&lt;/b&gt; "><b>1 相关工作</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#55" data-title="&lt;b&gt;1.1 特定主题情感分析&lt;/b&gt;"><b>1.1 特定主题情感分析</b></a></li>
                                                <li><a href="#57" data-title="&lt;b&gt;1.2 长短时记忆网络&lt;/b&gt;"><b>1.2 长短时记忆网络</b></a></li>
                                                <li><a href="#69" data-title="&lt;b&gt;1.3 情感常识知识库&lt;/b&gt;"><b>1.3 情感常识知识库</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#71" data-title="&lt;b&gt;2 融合常识知识库的混合注意力LSTM模型&lt;/b&gt; "><b>2 融合常识知识库的混合注意力LSTM模型</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#73" data-title="&lt;b&gt;2.1 任务定义&lt;/b&gt;"><b>2.1 任务定义</b></a></li>
                                                <li><a href="#75" data-title="&lt;b&gt;2.2 词向量输入层&lt;/b&gt;"><b>2.2 词向量输入层</b></a></li>
                                                <li><a href="#78" data-title="&lt;b&gt;2.3 全局注意力&lt;/b&gt;"><b>2.3 全局注意力</b></a></li>
                                                <li><a href="#87" data-title="&lt;b&gt;2.4 位置注意力&lt;/b&gt;"><b>2.4 位置注意力</b></a></li>
                                                <li><a href="#95" data-title="&lt;b&gt;2.5 常识知识库的嵌入&lt;/b&gt;"><b>2.5 常识知识库的嵌入</b></a></li>
                                                <li><a href="#100" data-title="&lt;b&gt;2.6 模型训练&lt;/b&gt;"><b>2.6 模型训练</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#104" data-title="&lt;b&gt;3 实 验&lt;/b&gt; "><b>3 实 验</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#105" data-title="&lt;b&gt;3.1 数据集&lt;/b&gt;"><b>3.1 数据集</b></a></li>
                                                <li><a href="#108" data-title="&lt;b&gt;3.2  词向量训练&lt;/b&gt;"><b>3.2  词向量训练</b></a></li>
                                                <li><a href="#110" data-title="&lt;b&gt;3.3 超参数设置与训练&lt;/b&gt;"><b>3.3 超参数设置与训练</b></a></li>
                                                <li><a href="#112" data-title="&lt;b&gt;3.4 实验对比&lt;/b&gt;"><b>3.4 实验对比</b></a></li>
                                                <li><a href="#119" data-title="&lt;b&gt;3.5 实验结果与分析&lt;/b&gt;"><b>3.5 实验结果与分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#123" data-title="&lt;b&gt;4 结 语&lt;/b&gt; "><b>4 结 语</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#77" data-title="图1 融合外部常识库的混合注意力LSTM模型">图1 融合外部常识库的混合注意力LSTM模型</a></li>
                                                <li><a href="#99" data-title="&lt;b&gt;表1 AffectNet 断定的实例&lt;/b&gt;"><b>表1 AffectNet 断定的实例</b></a></li>
                                                <li><a href="#107" data-title="&lt;b&gt;表2 数据集的统计样例&lt;/b&gt;"><b>表2 数据集的统计样例</b></a></li>
                                                <li><a href="#121" data-title="&lt;b&gt;表3 各模型实验结果对比&lt;/b&gt;"><b>表3 各模型实验结果对比</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" title=" Cambria E,Das D,Bandyopadhyay S,et al.A practical guide to sentiment analysis[M].Springer,2017." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A practical guide to sentiment analysis">
                                        <b>[1]</b>
                                         Cambria E,Das D,Bandyopadhyay S,et al.A practical guide to sentiment analysis[M].Springer,2017.
                                    </a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" title=" Nguyen T H,Shirai K.Aspect-based sentiment analysis using tree kernel based relationextraction[C]//International Conference on Intelligent Text Processing &amp;amp; Computational Linguistics.2015." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Aspect-based sentiment analysis using tree kernel based relationextraction">
                                        <b>[2]</b>
                                         Nguyen T H,Shirai K.Aspect-based sentiment analysis using tree kernel based relationextraction[C]//International Conference on Intelligent Text Processing &amp;amp; Computational Linguistics.2015.
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title=" Wang B,Liakata M,Zubiaga A,et al.Multitarget-specific sentiment recognition on twitter[C]//Proceedings of the 15th conference of the European chapter of the association for computational linguistics:volume 1,Long Papers.Valencia:Association for Computational Linguistics;2017:483-93." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multitarget-specific sentiment recognition on twitter">
                                        <b>[3]</b>
                                         Wang B,Liakata M,Zubiaga A,et al.Multitarget-specific sentiment recognition on twitter[C]//Proceedings of the 15th conference of the European chapter of the association for computational linguistics:volume 1,Long Papers.Valencia:Association for Computational Linguistics;2017:483-93.
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" title=" Tang D,Qin B,Feng X,et al.Effective LSTMs for target-dependent sentiment classification[C]//Proceedings of COLING 2016,the 26th international conference on computational linguistics:technical papers.Osaka,2016:3298-3307." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Effective LSTMs for target-dependent sentiment classification">
                                        <b>[4]</b>
                                         Tang D,Qin B,Feng X,et al.Effective LSTMs for target-dependent sentiment classification[C]//Proceedings of COLING 2016,the 26th international conference on computational linguistics:technical papers.Osaka,2016:3298-3307.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" title=" Hochreiter S,Schmidhuber J.Long short-term memory[J].Neural Computation,1997,9(8):1735-1780." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJBK&amp;filename=SJBK15090500014198&amp;v=MjI5MzZEWFV4b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcnpKS0ZvV2JoQT1OaWZKWmJLOUh0ak1xbzlGWk9vTA==&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                         Hochreiter S,Schmidhuber J.Long short-term memory[J].Neural Computation,1997,9(8):1735-1780.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" title=" Chorowski J,Bahdanau D,Cho K,et al.End-to-end continuous speech recognition using attention-based recurrent NN:first results[EB].eprint arXiv:1412.1602,2014." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=End-to-end continuous speech recognition using attention-based recurrent NN:first results[EB]">
                                        <b>[6]</b>
                                         Chorowski J,Bahdanau D,Cho K,et al.End-to-end continuous speech recognition using attention-based recurrent NN:first results[EB].eprint arXiv:1412.1602,2014.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" title=" Tang D,Qin B,Feng X,et al.Target-dependent sentiment classification with long short term memory[EB].eprint arXiv:1512.01100,2015." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Target-dependent sentiment classification with long short term memory[EB]">
                                        <b>[7]</b>
                                         Tang D,Qin B,Feng X,et al.Target-dependent sentiment classification with long short term memory[EB].eprint arXiv:1512.01100,2015.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" title=" Pang B,Lee L.A sentimental education:sentiment analysis using subjectivity,summarization based on minimum cuts[C]//Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics,2004." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A sentimental education:Sentiment analysis using subjectivity summarization based on minimum cuts">
                                        <b>[8]</b>
                                         Pang B,Lee L.A sentimental education:sentiment analysis using subjectivity,summarization based on minimum cuts[C]//Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics,2004.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" >
                                        <b>[9]</b>
                                     Lecun Y,Bengio Y,Hinton G.Deep learning[J].Nature,2015,521(7553):436.</a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_10" title=" Pontiki M,Galanis D,Pavlopoulos J,et al.Semeval-2014 task4:Aspect based sentiment analysis[C]//Proceedings of the 8th International Workshop on Semantic Evaluation(SemEval 2014),2014:27-35." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Semeval-2014 task 4:Aspect based sentiment analysis">
                                        <b>[10]</b>
                                         Pontiki M,Galanis D,Pavlopoulos J,et al.Semeval-2014 task4:Aspect based sentiment analysis[C]//Proceedings of the 8th International Workshop on Semantic Evaluation(SemEval 2014),2014:27-35.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_11" title=" Mohammad S.Nrccanada-2014:Detecting aspects and sentiment in customer reviews[C]//Proceedings of the 8th International Workshop on Semantic Evaluation(SemEval 2014),2014:437-442." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Nrccanada-2014:Detecting aspects and sentiment in customer reviews">
                                        <b>[11]</b>
                                         Mohammad S.Nrccanada-2014:Detecting aspects and sentiment in customer reviews[C]//Proceedings of the 8th International Workshop on Semantic Evaluation(SemEval 2014),2014:437-442.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_12" title=" Dong L,Wei F,Tan C,et al.Adaptive recursive neural network for target-dependent twitter sentiment classification[C]//Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,2014:49-54." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Adaptive recursive neural network for target-dependent twitter sentiment classification">
                                        <b>[12]</b>
                                         Dong L,Wei F,Tan C,et al.Adaptive recursive neural network for target-dependent twitter sentiment classification[C]//Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,2014:49-54.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_13" title=" Wagner J,Arora P,Cortes S,et al.Dcu:Aspectbased polarity classification for semeval task 4[C]//Proceedings of the 8th International Workshop on Semantic Evaluation(SemEval 2014),2014:223-229." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=DCU:Aspect-based Polarity Classification for Sem Eval Task 4">
                                        <b>[13]</b>
                                         Wagner J,Arora P,Cortes S,et al.Dcu:Aspectbased polarity classification for semeval task 4[C]//Proceedings of the 8th International Workshop on Semantic Evaluation(SemEval 2014),2014:223-229.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_14" title=" Lakkaraju H,Socher R,Manning C.Aspect specific sentiment analysis using hierarchical deep learning[C]//NIPS Workshop on Deep Learning and Representation Learning,2014." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Aspect specific sentiment analysis using hierarchical deep learning">
                                        <b>[14]</b>
                                         Lakkaraju H,Socher R,Manning C.Aspect specific sentiment analysis using hierarchical deep learning[C]//NIPS Workshop on Deep Learning and Representation Learning,2014.
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_15" title=" Rahman A,Ng V.Coreference resolution with world knowledge[C]//Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:Human Language Technologies-Volume 1.2011:814-824." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Coreference Resolution with World Knowledge">
                                        <b>[15]</b>
                                         Rahman A,Ng V.Coreference resolution with world knowledge[C]//Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:Human Language Technologies-Volume 1.2011:814-824.
                                    </a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_16" title=" Ratinov L,Roth D.Design challenges and misconceptions in named entity recognition[C]//Proceedings of the Thirteenth Conference on Computational Natural Language Learning.2009:147-155." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Design Challenges and Misconceptions in Named Entity Recognition">
                                        <b>[16]</b>
                                         Ratinov L,Roth D.Design challenges and misconceptions in named entity recognition[C]//Proceedings of the Thirteenth Conference on Computational Natural Language Learning.2009:147-155.
                                    </a>
                                </li>
                                <li id="35">


                                    <a id="bibliography_17" title=" Ahn S,Choi H,Parnamaa T,et al.A neural knowledge language model[EB].eprint arXiv:1608.00318,2016." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A neural knowledge language model[EB]">
                                        <b>[17]</b>
                                         Ahn S,Choi H,Parnamaa T,et al.A neural knowledge language model[EB].eprint arXiv:1608.00318,2016.
                                    </a>
                                </li>
                                <li id="37">


                                    <a id="bibliography_18" title=" Cambria E,Fu J,Bisio F,et al.Affectivespace 2:Enabling affective intuition for concept-level sentiment analysis[C]//Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence.AAAI Press,2015:508-514." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Affective Space 2:Enabling Affective Intuition for ConceptLevel Sentiment Analysis">
                                        <b>[18]</b>
                                         Cambria E,Fu J,Bisio F,et al.Affectivespace 2:Enabling affective intuition for concept-level sentiment analysis[C]//Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence.AAAI Press,2015:508-514.
                                    </a>
                                </li>
                                <li id="39">


                                    <a id="bibliography_19" title=" Mikolov T,Sutskever I,Chen K,et al.Distributed Representations of Words and Phrases and their Compositionality[J].Advances in Neural Information Processing Systems,2013,26:3111-3119." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Distributed representations of words and phrases and their compositionality">
                                        <b>[19]</b>
                                         Mikolov T,Sutskever I,Chen K,et al.Distributed Representations of Words and Phrases and their Compositionality[J].Advances in Neural Information Processing Systems,2013,26:3111-3119.
                                    </a>
                                </li>
                                <li id="41">


                                    <a id="bibliography_20" title=" Sukhbaatar S,Szlam A,Weston J,et al.End-to-end memory networks[C]//Advances in Neural Information Processing Systems,2015:2431-2439." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=End-to-end memory networks">
                                        <b>[20]</b>
                                         Sukhbaatar S,Szlam A,Weston J,et al.End-to-end memory networks[C]//Advances in Neural Information Processing Systems,2015:2431-2439.
                                    </a>
                                </li>
                                <li id="43">


                                    <a id="bibliography_21" title=" Pennington J,Socher R,Manning C D.Glove:Global vectors for word representation[C]//Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing,2014:1532-1543." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Glove:Global Vectors for Word Representation">
                                        <b>[21]</b>
                                         Pennington J,Socher R,Manning C D.Glove:Global vectors for word representation[C]//Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing,2014:1532-1543.
                                    </a>
                                </li>
                                <li id="45">


                                    <a id="bibliography_22" title=" Tang D,Qin B,Liu T.Document modeling with gated recurrent neural network for sentiment classification[C]//Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,2015:1422-1432." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Document modeling with gated recurrent neural network for sentiment classification">
                                        <b>[22]</b>
                                         Tang D,Qin B,Liu T.Document modeling with gated recurrent neural network for sentiment classification[C]//Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,2015:1422-1432.
                                    </a>
                                </li>
                                <li id="47">


                                    <a id="bibliography_23" title=" 胡朝举,梁宁.基于深层注意力的LSTM的特定主题情感分析[J].计算机应用研究,2019,36(4):121-125." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSYJ201904027&amp;v=MjM4MDRSN3FmWnVadEZpRGhVYnZLTHo3U1pMRzRIOWpNcTQ5SFk0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1U=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[23]</b>
                                         胡朝举,梁宁.基于深层注意力的LSTM的特定主题情感分析[J].计算机应用研究,2019,36(4):121-125.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JYRJ" target="_blank">计算机应用与软件</a>
                2019,36(10),151-155+161 DOI:10.3969/j.issn.1000-386x.2019.10.027            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>嵌入常识的混合注意力LSTM用于主题情感分析</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E4%BB%BB%E6%99%93%E5%A5%8E&amp;code=07922858&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">任晓奎</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%83%AD%E5%A8%9F&amp;code=35435148&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">郭娟</a>
                                <a href="javascript:;">陶志勇</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E8%BE%BD%E5%AE%81%E5%B7%A5%E7%A8%8B%E6%8A%80%E6%9C%AF%E5%A4%A7%E5%AD%A6%E7%94%B5%E5%AD%90%E4%B8%8E%E4%BF%A1%E6%81%AF%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0034851&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">辽宁工程技术大学电子与信息工程学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E9%98%9C%E6%96%B0%E5%8A%9B%E5%85%B4%E7%A7%91%E6%8A%80%E6%9C%89%E9%99%90%E8%B4%A3%E4%BB%BB%E5%85%AC%E5%8F%B8&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">阜新力兴科技有限责任公司</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>传统神经网络模型在捕捉上下文信息时,缺乏对于某一主题重要线索的准确分析能力。对此提出嵌入常识知识库的混合注意力长短时记忆网络(LSTM)主题情感分析模型。通过引入全局注意力和位置注意力机制来改进长短时记忆网络LSTM;将常识知识库嵌入到LSTM的情感分类训练中。该模型在推断特定主题的情感极性时明确地抓住了每个上下文词的重要性,使分类更加准确。实验结果表明,混合注意力模型与常识知识库的引入,提高了主题情感分析的分类效果。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">情感分析;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%B8%B8%E8%AF%86%E7%9F%A5%E8%AF%86&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">常识知识;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">注意力机制;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=LSTM&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">LSTM;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    任晓奎，副教授，主研领域:通信与信息系统，人工智能。;
                                </span>
                                <span>
                                    郭娟，硕士生。;
                                </span>
                                <span>
                                    陶志勇，副教授。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-12-28</p>

                    <p>

                            <b>基金：</b>
                                                        <span>辽宁省自然科学基金项目(2015020100);</span>
                    </p>
            </div>
                    <h1><b>MXIED ATTENTION LSTM EMBEDDED COMMON SENSE FOR THEMATIC SENTIMENT ANALYSIS</b></h1>
                    <h2>
                    <span>Ren Xiaokui</span>
                    <span>Guo Juan</span>
                    <span>Tao Zhiyong</span>
            </h2>
                    <h2>
                    <span>Electronic and Information Engineering, Liaoning Technical University</span>
                    <span>Fuxin Lixing Technology Co., Ltd.</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Traditional neural network models lack the ability to accurately analyze important clues to a topic when capturing context information. In order to solve this problem, we proposed an AffectNet-mixed attention-LSTM(AMA-LSTM) thematic sentiment analysis model combining the common sense knowledge base. The LSTM was improved by introducing global attention and positional attention mechanisms, and then the common sense knowledge base was embedded into the sentiment classification training of LSTM. This model clearly grasps the importance of each context in inferring the emotional polarity of a particular topic, making classification more accurate. Experimental results show that the introduction of the mixed attention model and common sense knowledge base improves the classification effect of thematic affective analysis.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Sentiment%20analysis&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Sentiment analysis;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Commonsense%20knowledge&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Commonsense knowledge;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Attention%20mechanism&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Attention mechanism;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=LSTM&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">LSTM;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-12-28</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="49" name="49" class="anchor-tag"><b>0 引 言</b></h3>
                <div class="p1">
                    <p id="50">情感分析<citation id="135" type="reference"><link href="3" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>在博客、微博、在线社区、维基百科等在线平台上处理数据已成为一个研究热点。其中,主题情感分析扩展了其典型框架设置,即极性与特定主题(或产品特性)而不是整个文本单元相关联来聚集情感分析。</p>
                </div>
                <div class="p1">
                    <p id="51">深度学习模型<citation id="136" type="reference"><link href="5" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>在没有人工标注的特征工程<citation id="138" type="reference"><link href="7" rel="bibliography" /><link href="9" rel="bibliography" /><sup>[<a class="sup">3</a>,<a class="sup">4</a>]</sup></citation>前提下,应用于特定主题情感分析时,准确率很高,特别是神经序列模型,但传统的神经序列模型如长短时记忆网络以一种隐式的方式捕捉上下文信息<citation id="137" type="reference"><link href="15" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>,而不能明确显示某一主题的线索。此外,现有的研究在有效地融入深层神经网络的外部知识(如情感或常识知识)方面存在不足,情感常识的引入可以访问有限的训练数据中没有的外部信息。这些知识可以有效地帮助识别情感极性。</p>
                </div>
                <div class="p1">
                    <p id="52">自Pang等<citation id="139" type="reference"><link href="17" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>提出有关情感分析的工作之后,情感分析发展为基于传统的方法和基于深度学习的方法。随着Hinton等<citation id="140" type="reference"><link href="19" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>提出深度学习之后,越来越多的学习任务被应用到自然语言处理中。然而,神经网络模型在处理主题级情感分类主题还处于起步阶段,代表性的方法有Kiritchenko<citation id="141" type="reference"><link href="23" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>等基于特征的支持向量机和Dong<citation id="142" type="reference"><link href="25" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>等研究的神经网络模型。最近,目标依赖情感的分类可以从考虑目标信息中获益,如Tang<citation id="143" type="reference"><link href="9" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>等研究的目标依赖LSTM(TD-LSTM)和目标连接LSTM(TC-LSTM)。然而,这些模型只能考虑给定目标,而不能考虑主题信息。</p>
                </div>
                <div class="p1">
                    <p id="53">本文探讨了主题情感分类中主题与情感极性的潜在关联。为了捕获针对给定主题的重要信息,设计了一种基于全局注意力和位置注意力的混合注意力LSTM,并在基准数据集<citation id="144" type="reference"><link href="17" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>上评估所提出的方法,经验证该方法是有效的。</p>
                </div>
                <h3 id="54" name="54" class="anchor-tag"><b>1 相关工作</b></h3>
                <h4 class="anchor-tag" id="55" name="55"><b>1.1 特定主题情感分析</b></h4>
                <div class="p1">
                    <p id="56">特定主题情感分类是一种细粒度情感分类任务,目标是对于给定的句子和句子中出现的主题,推测句子对于主题的情感极性<citation id="145" type="reference"><link href="27" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>。早期关于主题情感分析的论文主要依靠特征工程来描述句子。在表征学习深度学习的成功激励下,Lakkaraju等<citation id="146" type="reference"><link href="29" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>利用深度神经网络生成句子嵌入(句子的密集向量表示),然后作为低维特征向量反馈给分类器。情感极性如Positive、Negative和Neutral。例如,“这个标准间的床很宽敞,但是通风效果很差!”对于主题“床”是Positive,而对于主题“通风效果”的极性是Negative。此外,通过注意力机制可以增强表达,对于句子中的每个单词,注意力向量都量化了它的情感显著性以及与给定主题的相关性。</p>
                </div>
                <h4 class="anchor-tag" id="57" name="57"><b>1.2 长短时记忆网络</b></h4>
                <div class="p1">
                    <p id="58">LSTM是由Hochreiter和Schmidhuber<citation id="147" type="reference"><link href="11" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>首次提出的,用来解决RNN面临的梯度消失和爆炸问题。典型的LSTM单元包含三个门:遗忘门、输入门和输出门。这些门决定了在当前时间步骤中流入和流出的信息。LSTM单元状态数学表示如下:</p>
                </div>
                <div class="p1">
                    <p id="59"><i>f</i><sub><i>i</i></sub>=<i>σ</i>(<i>W</i><sub><i>f</i></sub>[<i>x</i><sub><i>i</i></sub>,<i>h</i><sub><i>i</i></sub><sub>-1</sub>]+<i>b</i><sub><i>f</i></sub>)      (1)</p>
                </div>
                <div class="p1">
                    <p id="60"><i>I</i><sub><i>i</i></sub>=<i>σ</i>(<i>W</i><sub><i>I</i></sub>[<i>x</i><sub><i>i</i></sub>,<i>h</i><sub><i>i</i></sub><sub>-1</sub>]+<i>b</i><sub><i>I</i></sub>)      (2)</p>
                </div>
                <div class="p1">
                    <p id="61"><mathml id="125"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mi>C</mi></mstyle><mo>∼</mo></mover><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mrow><mi>tanh</mi></mrow><mo stretchy="false">(</mo><mi>W</mi><msub><mrow></mrow><mi>C</mi></msub><mo stretchy="false">[</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mi>h</mi><msub><mrow></mrow><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msub><mo stretchy="false">]</mo><mo>+</mo><mi>b</mi><msub><mrow></mrow><mi>C</mi></msub><mo stretchy="false">)</mo></mrow></math></mathml>      (3)</p>
                </div>
                <div class="p1">
                    <p id="62"><mathml id="126"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>C</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mi>f</mi><msub><mrow></mrow><mi>i</mi></msub><mo>×</mo><mi>C</mi><msub><mrow></mrow><mrow><mi>i</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>+</mo><mi>Ι</mi><msub><mrow></mrow><mi>i</mi></msub><mo>×</mo><mover><mstyle mathsize="140%" displaystyle="true"><mi>C</mi></mstyle><mo>∼</mo></mover><msub><mrow></mrow><mi>i</mi></msub></mrow></math></mathml>      (4)</p>
                </div>
                <div class="p1">
                    <p id="63"><i>o</i><sub><i>i</i></sub>=<i>σ</i>(<i>W</i><sub><i>o</i></sub>[<i>x</i><sub><i>i</i></sub>,<i>h</i><sub><i>i</i></sub><sub>-1</sub>]+<i>b</i><sub><i>o</i></sub>)      (5)</p>
                </div>
                <div class="p1">
                    <p id="64"><i>h</i><sub><i>i</i></sub>=<i>o</i><sub><i>i</i></sub>×tanh(<i>C</i><sub><i>i</i></sub>)      (6)</p>
                </div>
                <div class="p1">
                    <p id="65"><i>f</i><sub><i>i</i></sub>、<i>I</i><sub><i>i</i></sub>、<i>o</i><sub><i>i</i></sub>分别表示遗忘门、输入门和输出门;<i>W</i><sub><i>f</i></sub>、<i>W</i><sub><i>I</i></sub>、<i>W</i><sub><i>o</i></sub>、<i>b</i><sub><i>f</i></sub>、<i>b</i><sub><i>I</i></sub>、<i>b</i><sub><i>o</i></sub>代表每个门的权重矩阵和偏置量。<i>C</i><sub><i>i</i></sub>为单元状态,<i>h</i><sub><i>i</i></sub>为隐藏输出。单个LSTM通常只从一个方向对序列进行编码。然而,两个LSTM也可以堆叠起来作为双向使用编码器,简称双向LSTM。对于一个句子<i>s</i>={<i>w</i><sub>1</sub>,<i>w</i><sub>2</sub>,…,<i>w</i><sub><i>L</i></sub>},双向LSTM产生一系列隐藏的输出,如式(7)所示:</p>
                </div>
                <div class="area_img" id="66">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JYRJ201910028_06600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="68">式中:<i>H</i>的每个元素都是前向和后向LSTM单元对应的隐藏输出的连接。</p>
                </div>
                <h4 class="anchor-tag" id="69" name="69"><b>1.3 情感常识知识库</b></h4>
                <div class="p1">
                    <p id="70">情感常识知识库<citation id="150" type="reference"><link href="31" rel="bibliography" /><link href="33" rel="bibliography" /><sup>[<a class="sup">15</a>,<a class="sup">16</a>]</sup></citation>通常被作为特征的外部来源。神经序列模型<citation id="148" type="reference"><link href="35" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>利用知识概念的低维度连续表示作为附加输入。情感常识知识库,如AffectNet<citation id="149" type="reference"><link href="37" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>,包含了与一组丰富的情感属性相关的概念,这些情感属性不仅提供了概念层面的特征,而且还提供了指向各个主题及其情感极性的语义链接。摘要“情感网”是一种基于词汇的情感知识库,它是由带有情感极性注释的词义同步构成的。情感网络既不包含常识性概念,也不包含情感性属性,这是情感空间的主要特征。因此,必须使用随机初始化的嵌入来表示敏感词网络同步集。与AffectiveSpace一样,字同步集映射到相同的100维嵌入。借助词义消歧工具,将句子中的每个单词映射到其词义。</p>
                </div>
                <h3 id="71" name="71" class="anchor-tag"><b>2 融合常识知识库的混合注意力LSTM模型</b></h3>
                <div class="p1">
                    <p id="72">本节详细描述了提出的基于混合注意力的神经体系结构:首先提出了特定主题情感分析的任务定义;接着,描述了全局注意力模型和位置注意力模型;最后介绍了嵌入LSTM单元的知识嵌入扩展流程。</p>
                </div>
                <h4 class="anchor-tag" id="73" name="73"><b>2.1 任务定义</b></h4>
                <div class="p1">
                    <p id="74">给出一个句子<i>s</i>={<i>w</i><sub>1</sub>,<i>w</i><sub>2</sub>,…,<i>w</i><sub><i>i</i></sub>,<i>w</i><sub><i>m</i></sub>}由<i>m</i>个词组成的句子和出现在句子<i>s</i>中一个主题词<i>w</i><sub><i>i</i></sub>,特定主题情感分析旨在确定句子<i>s</i>对<i>w</i><sub><i>i</i></sub>的情感极性。在处理文本语料库时,将每个单词映射为一个低维的连续实值向量,也称为词嵌入<citation id="151" type="reference"><link href="39" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>。所有的词向量堆叠在一个词嵌入矩阵<mathml id="127"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><mo>∈</mo><mi>R</mi><msup><mrow></mrow><mrow><mi>d</mi><mo>×</mo><mrow><mo>|</mo><mi>V</mi><mo>|</mo></mrow></mrow></msup></mrow></math></mathml>中,其中<i>d</i>为词向量的维数,<mathml id="128"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>|</mo><mi>V</mi><mo>|</mo></mrow></mrow></math></mathml>为词汇量的大小。<i>w</i><sub><i>i</i></sub>的嵌入词被标记为<i>e</i><sub><i>i</i></sub>∈<i>R</i><sup><i>d</i></sup><sup>×1</sup>,它是嵌入矩阵<i>L</i>中的一列。</p>
                </div>
                <h4 class="anchor-tag" id="75" name="75"><b>2.2 词向量输入层</b></h4>
                <div class="p1">
                    <p id="76">句子<i>s</i>={<i>w</i><sub>1</sub>,<i>w</i><sub>2</sub>,…,<i>w</i><sub><i>i</i></sub>,…,<i>w</i><sub><i>n</i></sub>},主题词<i>w</i><sub><i>i</i></sub>,将每个词映射到它的嵌入向量,这些词向量被分成两部分:特定主题表示和上下文内容表示。图1说明了神经结构是如何工作的。给定一个句子<i>s</i>,首先执行查找操作,将输入的单词转换为词嵌入。基于LSTM的序列编码器,将词嵌入转换为隐藏输出序列。注意力组件构建在隐藏输出之上。注意力层包含一个全局注意力和位置注意力,通过两次注意力层从外部存储器<i>m</i>中自适应地选择权重突出的单词,最后将输出的向量作为句子的主题表示,进一步作为主题级情感分析的特征。</p>
                </div>
                <div class="area_img" id="77">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201910028_077.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 融合外部常识库的混合注意力LSTM模型" src="Detail/GetImg?filename=images/JYRJ201910028_077.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 融合外部常识库的混合注意力LSTM模型  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201910028_077.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="78" name="78"><b>2.3 全局注意力</b></h4>
                <div class="p1">
                    <p id="79">本文使用混合注意力模型来计算一个句子关于一个主题的表示。语境词对句子语义的贡献并不相等。此外,如果关注不同的主题,一个词的重要性应该是不同的。这里再次以“这个标准间的床很宽敞,但是通风效果很差!”为例。对于主题“床”,语境词“宽敞”比“差”更重要。相反,对于主题“通风效果”而言,“差”比“宽敞t”更重要。</p>
                </div>
                <div class="p1">
                    <p id="80">注意力模型以外部存储器<i>m</i>∈<i>R</i><sup><i>d</i></sup><sup>×</sup><sup><i>k</i></sup>和主题向量<i>v</i><sub>aspect</sub>∈<i>R</i><sup><i>d</i></sup><sup>×1</sup>作为输入,输出连续向量<i>vec</i>∈<i>R</i><sup><i>d</i></sup><sup>×1</sup>。输出向量是<i>m</i>中每一记忆内存的加权和,即:</p>
                </div>
                <div class="p1">
                    <p id="81"><mathml id="129"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>v</mi><mi>e</mi><mi>c</mi><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><mi>α</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mi>m</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></math></mathml>      (8)</p>
                </div>
                <div class="p1">
                    <p id="82">式中:<i>k</i>是记忆内存的的大小,<i>α</i><sub><i>i</i></sub>∈[0,1]是<i>m</i><sub><i>i</i></sub>的权重,其中<mathml id="130"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mi>i</mi></munder><mi>α</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mn>1</mn></mrow></math></mathml>。我们实现了一个基于神经网络的注意力模型。对于每个记忆内存<i>m</i><sub><i>i</i></sub>,我们使用前馈神经网络来计算它与主题的语义关联。计分函数计算如(9)所示,其中<i>W</i><sub><i>att</i></sub>∈<i>R</i><sup>1×2</sup><sup><i>d</i></sup>,<i>b</i><sub><i>att</i></sub>∈<i>R</i><sup>1×1</sup>。</p>
                </div>
                <div class="p1">
                    <p id="83"><i>g</i><sub><i>i</i></sub>=tanh(<i>W</i><sub><i>att</i></sub>[<i>m</i><sub><i>i</i></sub>;<i>v</i><sub>aspect</sub>]+<i>b</i><sub><i>att</i></sub>)      (9)</p>
                </div>
                <div class="p1">
                    <p id="84">然后获得{<i>g</i><sub>1</sub>,<i>g</i><sub>2</sub>,…,<i>g</i><sub><i>k</i></sub>},我们将它们输入softmax函数来计算最终的重要度分数{<i>α</i><sub>1</sub>,<i>α</i><sub>2</sub>,…,<i>α</i><sub><i>k</i></sub>}。</p>
                </div>
                <div class="p1">
                    <p id="85"><mathml id="131"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>α</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mfrac><mrow><mi>exp</mi><mo stretchy="false">(</mo><mi>g</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><mtext>e</mtext></mstyle><mtext>x</mtext><mtext>p</mtext><mo stretchy="false">(</mo><mi>g</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></mfrac></mrow></math></mathml>      (10)</p>
                </div>
                <div class="p1">
                    <p id="86">这种注意力模型有两个优点:该模型可以根据每个内存<i>m</i><sub><i>i</i></sub>片段与主题的语义相关性,为其自适应地分配一个注意力权重;这种注意力模型很容易与其他组件一起以端到端方式进行训练。</p>
                </div>
                <h4 class="anchor-tag" id="87" name="87"><b>2.4 位置注意力</b></h4>
                <div class="p1">
                    <p id="88">从直觉上讲,一个离主题更近的上下文词应该比一个更远的词更重要。所以将语境词的位置定义为它与原句序列中的相位的绝对距离。在此基础上,研究了在注意力模型中对位置信息进行编码的一种策略。详情如下:</p>
                </div>
                <div class="p1">
                    <p id="89">根据Sukhbaatar等<citation id="152" type="reference"><link href="41" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>,计算内存记忆向量<i>m</i><sub><i>i</i></sub>:</p>
                </div>
                <div class="p1">
                    <p id="90"><i>m</i><sub><i>i</i></sub>=<i>e</i><sub><i>i</i></sub>⨂<i>v</i><sub><i>i</i></sub>      (11)</p>
                </div>
                <div class="p1">
                    <p id="91">式中:⨂代表元素的相乘,<i>v</i><sub><i>i</i></sub>∈<i>R</i><sup><i>d</i></sup><sup>×1</sup>是一个单词<i>w</i><sub><i>i</i></sub>的位置向量,<i>v</i><sub><i>i</i></sub>中的每个元素计算如下:</p>
                </div>
                <div class="p1">
                    <p id="92"><i>v</i><mathml id="132"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>k</mi></msubsup></mrow></math></mathml>=(1-<i>l</i><sub><i>i</i></sub>/<i>n</i>)-(<i>k</i>/<i>d</i>)(1-2×<i>l</i><sub><i>i</i></sub>/<i>n</i>)      (12)</p>
                </div>
                <div class="p1">
                    <p id="93">式中:<i>n</i>是句子的长度,<i>k</i>是层的数目, <i>l</i><sub><i>i</i></sub>是<i>w</i><sub><i>i</i></sub>的位置。</p>
                </div>
                <div class="p1">
                    <p id="94">这种注意力模型有效地结合了位置信息,使主题词<i>w</i><sub><i>i</i></sub>的位置向量有更加精确的抽象表示,所研究的位置信息编码方法仍有进一步提高准确性的空间。</p>
                </div>
                <h4 class="anchor-tag" id="95" name="95"><b>2.5 常识知识库的嵌入</b></h4>
                <div class="p1">
                    <p id="96">为了提高情感分类的准确性,将常识知识作为知识来源嵌入到序列编码器中。将Ma<citation id="153" type="reference"><link href="37" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>等提出的AffectNet的概念映射到连续的低维嵌入,同时不丢失原始空间中的语义和情感关联。基于这种新的概念空间,本文将概念混合的信息嵌入到深度神经序列模型中,以更好地对句子的特定主题情感进行分类。AffectNet的目标是赋予这些概念两个重要的角色:(1) 协助过滤从一个时间步骤到下一个时间步骤的信息和(2) 提供补充的信息给记忆单元。在每次步骤<i>i</i>中,假设可以触发一组知识概念候选对象并将其映射到<i>d</i><sub><i>c</i></sub>维空间。<i>K</i>概念的集合为{<i>μ</i><sub><i>i</i></sub><sub>,1</sub>,<i>μ</i><sub><i>i</i></sub><sub>,2</sub>,…,<i>μ</i><sub><i>i</i></sub><sub>,</sub><sub><i>K</i></sub>}。将候选嵌入合并到单个向量中,如公式所示:</p>
                </div>
                <div class="p1">
                    <p id="97"><mathml id="133"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>μ</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mfrac><mn>1</mn><mi>Κ</mi></mfrac><mstyle displaystyle="true"><munder><mo>∑</mo><mi>j</mi></munder><mi>μ</mi></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub></mrow></math></mathml>      (13)</p>
                </div>
                <div class="p1">
                    <p id="98">例如表1所示,“Rotten fish”这一概念具有食物种类属性——食品可以直接与餐厅或食品质量等主题相关,而“joy”等属性对情感极性的划分具有正向作用。</p>
                </div>
                <div class="area_img" id="99">
                    <p class="img_tit"><b>表1 AffectNet 断定的实例</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="99" border="1"><tr><td>AffectNet</td><td>IsA-pet</td><td>KindOf-food</td><td>Arises-joy</td><td>…</td></tr><tr><td><br />Dog</td><td>0.981</td><td>0</td><td>0.789</td><td>…</td></tr><tr><td><br />Cupcake</td><td>0</td><td>0.922</td><td>0.910</td><td>…</td></tr><tr><td><br />Rotten fish</td><td>0</td><td>0.459</td><td>0</td><td>…</td></tr><tr><td><br />Police man</td><td>0</td><td>0</td><td>0</td><td>…</td></tr><tr><td><br />Win lottery</td><td>0</td><td>0</td><td>0.991</td><td>…</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="100" name="100"><b>2.6 模型训练</b></h4>
                <div class="p1">
                    <p id="101">我们将上一层中的输出向量作为特征,并将其提供给softmax层,以进行主题情感分类。该模型通过最小化情感分类的交叉熵误差进行监督训练,其损失函数如下所示,其中<i>T</i>表示所有训练实例,<i>C</i>为情感类别集合,(<i>s</i>;<i>a</i>)表示句子主题。</p>
                </div>
                <div class="p1">
                    <p id="102" class="code-formula">
                        <mathml id="102"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>l</mi><mi>o</mi><mi>s</mi><mi>s</mi><mo>=</mo><mo>-</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mo stretchy="false">(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo stretchy="false">)</mo><mo>∈</mo><mi>Τ</mi></mrow></munder><mspace width="0.25em" /></mstyle><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>c</mi><mo>∈</mo><mi>C</mi></mrow></munder><mi>Ρ</mi></mstyle><msubsup><mrow></mrow><mi>c</mi><mi>g</mi></msubsup><mo stretchy="false">(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo stretchy="false">)</mo><mo>⋅</mo><mrow><mi>log</mi></mrow><mo stretchy="false">(</mo><mi>Ρ</mi><msub><mrow></mrow><mi>c</mi></msub><mo stretchy="false">(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>4</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="103">式中:<i>P</i><sub><i>c</i></sub>(<i>s</i>,<i>a</i>)为我们系统产生的<i>c</i>类预测(<i>s</i>,<i>a</i>)的概率。<i>P</i><mathml id="134"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>c</mi><mi>g</mi></msubsup></mrow></math></mathml>(<i>s</i>,<i>a</i>)为1或0,表示是否为正确答案<i>c</i>。我们用反向传播法计算所有参数的梯度,并用随机梯度下降法对其进行更新。我们用300维的Glove向量进行词嵌入<citation id="154" type="reference"><link href="39" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>,这是从Web数据训练出来的,词汇量为1.9 M。我们用均匀分布<i>U</i>(-0.01,0.01)随机化其他参数,设置学习率为0.01。</p>
                </div>
                <h3 id="104" name="104" class="anchor-tag"><b>3 实 验</b></h3>
                <h4 class="anchor-tag" id="105" name="105"><b>3.1 数据集</b></h4>
                <div class="p1">
                    <p id="106">为了验证该模型的有效性,实验基于中文情感挖掘的酒店评论语料(ChnSentiCorp),ChnSentiCorp是中科院谭松波博士收集整理的一个酒店评论的语料,其公布的语料规模为10 000篇,被分为4个子集,本文选用ChnSentiCorp-Htl-ba-6000数据来进行实验,其为平衡语料,正负类各3 000篇。评价指标是分类准确率。数据集的统计样例如表2所示。</p>
                </div>
                <div class="area_img" id="107">
                    <p class="img_tit"><b>表2 数据集的统计样例</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="107" border="1"><tr><td><br />积极</td><td>消极</td></tr><tr><td><br />商务大床房,房间很大,床有2 m宽,整体感觉经济实惠不错</td><td>标准间太差房间还不如3星的,而且设施非常陈旧,建议酒店把老的标准间从新改善</td></tr><tr><td><br />非常好的酒店,四星的标准完全超值的享受,服务非常好</td><td>肯定我不会再住这里了,太陈旧了,霉味太重,感觉不好</td></tr><tr><td><br />环境很好,地点很方便,服务也很好,下回还会住</td><td>环境一般,住了之后让人感觉价格和服务不成比例</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="108" name="108"><b>3.2  词向量训练</b></h4>
                <div class="p1">
                    <p id="109">本文选用维基百科的中文语料作为训练的语料库,利用Google开源的word2vec tool来进行词向量的训练,然后对语料库进行预处理,并以此作为本模型的输入层。本文采用的word2vec tool的skip-gram模型,上下文窗口大小设置为5,词向量维度大小设为50,采样值大小设为1e-3,如果有词语不在预训练好的词向量中的话,则采用随机初始化方式来进行表示。</p>
                </div>
                <h4 class="anchor-tag" id="110" name="110"><b>3.3 超参数设置与训练</b></h4>
                <div class="p1">
                    <p id="111">本文提出的方案建模过程主要采用Tensorflow深度学习框架。基于长短期记忆网络和混合注意力模型的方法进行情感分析的实现方案在前文已经有了介绍。模型构建采用的是Tensorflow的序列模型框架,首先添加Embedding层作为模型的输入,其次添加LSTM模型,并在LSTM的模型后添加混合注意力机制层,在得到句子的向量表示时对评论文本中不同的词赋予不同的权值,然后由这些不同权值的词向量加权得到句子的向量表示。之后采用sigmoid函数对文本进行分类。另外,在模型训练过程中,采用dropout以防止过拟合。最后,编译过程采用梯度下降算法进行权重的更新迭代。</p>
                </div>
                <h4 class="anchor-tag" id="112" name="112"><b>3.4 实验对比</b></h4>
                <div class="p1">
                    <p id="113">将本文提出的融合常识知识库的混合注意力长短时记忆网络(LSTM)主题情感分析模型与以下方法进行对比:</p>
                </div>
                <div class="p1">
                    <p id="114">1) LSTM。基准LSTM模型不能获取到特定主题信息,尽管主题不同,但是得到的情感极性一致。</p>
                </div>
                <div class="p1">
                    <p id="115">2) TD-LSTM<citation id="155" type="reference"><link href="9" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>。使用前向和后向LSTM方法在主题词之前和之后提取信息。但是由于没有在TD-LSTM模型中应用注意力机制,并不能够获取文本中对于给定主题的重要词信息。</p>
                </div>
                <div class="p1">
                    <p id="116">3) TC-LSTM<citation id="156" type="reference"><link href="43" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>。改进了TD-LSTM模型。TC-LSTM模型<citation id="157" type="reference"><link href="41" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>在原来TD-LSTM的基础上,将主题向量引入一个句子的特征表示。能够更好地利用主题词和文本中每个词,将其连接组成一个文本的特征表示。</p>
                </div>
                <div class="p1">
                    <p id="117">4) ATAE-LSTM<citation id="158" type="reference"><link href="7" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>。AE-LSTM中使用主题信息的方式让主题词向量在注意力权重中起到了重要的作用。Wang提出的ATAE-LSTM模型,将主题词向量连接到每个单词的输入向量。</p>
                </div>
                <div class="p1">
                    <p id="118">5) AE-ATT-LSTM<citation id="159" type="reference"><link href="43" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>。将融合主题特征的深层注意力LSTM模型应用到特定主题情感分类任务上。通过共享权重的双向LSTM将主题词向量和文本词向量进行训练,得到主题特征和文本特征融合,经过深层注意力机制在得到相应分类结果。</p>
                </div>
                <h4 class="anchor-tag" id="119" name="119"><b>3.5 实验结果与分析</b></h4>
                <div class="p1">
                    <p id="120">本文采用嵌入外部常识的混合注意力LSTM方法在ChnSentiCorp-Htl-ba-600数据集上进行模型训练和交叉验证,得到的结果如表3所示。</p>
                </div>
                <div class="area_img" id="121">
                    <p class="img_tit"><b>表3 各模型实验结果对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="121" border="1"><tr><td><br />Models</td><td>Three-way</td><td>Pos./Neg.</td></tr><tr><td><br />LSTM</td><td>81.8</td><td>88.2</td></tr><tr><td><br />TD-LSTM</td><td>82.5</td><td>88.9</td></tr><tr><td><br />TC-LSTM</td><td>81.8</td><td>89.1</td></tr><tr><td><br />ATAE-LSTM</td><td>84.0</td><td>89.9</td></tr><tr><td><br />AE-ATT-LSTM</td><td>85.1</td><td>90.0</td></tr><tr><td><br />ES-ATT-LSTM</td><td>86.0</td><td>90.2</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="122">其中Three-way是数据集中的三种情感极性(Positive,Negative, Neutral)和两种情感极性(Positive,Negative)的分类准确率结果。本文所采用的方法较之其他模型得到了提高,因此可以得出,特定主题下嵌入外部常识知识库和混合注意力机制的引入,提高了分类准确率。</p>
                </div>
                <h3 id="123" name="123" class="anchor-tag"><b>4 结 语</b></h3>
                <div class="p1">
                    <p id="124">针对特定主题情感分类任务中,大多数方法对注意力机制的引用没有综合考虑位置影响,所以分类效果不准确。通过采用全局注意力和位置注意力的混合注意力机制来改进长短时记忆网络LSTM,该方法能够准确捕捉上下文信息。同时将有关情感概念的知识常识库融合到深度神经网络端到端的情感分类训练中,使分类更加准确。通过在数据集上不同的对比实验表明,该方法在准确率上有了进一步的提升,从而能更好地解决特定主题情感分析任务。在未来的研究中,将句子结构和词性(比如解析结果)整合到深层记忆网络中进行改进,是下一步研究的重点。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A practical guide to sentiment analysis">

                                <b>[1]</b> Cambria E,Das D,Bandyopadhyay S,et al.A practical guide to sentiment analysis[M].Springer,2017.
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Aspect-based sentiment analysis using tree kernel based relationextraction">

                                <b>[2]</b> Nguyen T H,Shirai K.Aspect-based sentiment analysis using tree kernel based relationextraction[C]//International Conference on Intelligent Text Processing &amp; Computational Linguistics.2015.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multitarget-specific sentiment recognition on twitter">

                                <b>[3]</b> Wang B,Liakata M,Zubiaga A,et al.Multitarget-specific sentiment recognition on twitter[C]//Proceedings of the 15th conference of the European chapter of the association for computational linguistics:volume 1,Long Papers.Valencia:Association for Computational Linguistics;2017:483-93.
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Effective LSTMs for target-dependent sentiment classification">

                                <b>[4]</b> Tang D,Qin B,Feng X,et al.Effective LSTMs for target-dependent sentiment classification[C]//Proceedings of COLING 2016,the 26th international conference on computational linguistics:technical papers.Osaka,2016:3298-3307.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJBK&amp;filename=SJBK15090500014198&amp;v=Mjc4NDRRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcnpKS0ZvV2JoQT1OaWZKWmJLOUh0ak1xbzlGWk9vTERYVXhvQk1UNlQ0UA==&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b> Hochreiter S,Schmidhuber J.Long short-term memory[J].Neural Computation,1997,9(8):1735-1780.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=End-to-end continuous speech recognition using attention-based recurrent NN:first results[EB]">

                                <b>[6]</b> Chorowski J,Bahdanau D,Cho K,et al.End-to-end continuous speech recognition using attention-based recurrent NN:first results[EB].eprint arXiv:1412.1602,2014.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Target-dependent sentiment classification with long short term memory[EB]">

                                <b>[7]</b> Tang D,Qin B,Feng X,et al.Target-dependent sentiment classification with long short term memory[EB].eprint arXiv:1512.01100,2015.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A sentimental education:Sentiment analysis using subjectivity summarization based on minimum cuts">

                                <b>[8]</b> Pang B,Lee L.A sentimental education:sentiment analysis using subjectivity,summarization based on minimum cuts[C]//Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics,2004.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" >
                                    <b>[9]</b>
                                 Lecun Y,Bengio Y,Hinton G.Deep learning[J].Nature,2015,521(7553):436.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Semeval-2014 task 4:Aspect based sentiment analysis">

                                <b>[10]</b> Pontiki M,Galanis D,Pavlopoulos J,et al.Semeval-2014 task4:Aspect based sentiment analysis[C]//Proceedings of the 8th International Workshop on Semantic Evaluation(SemEval 2014),2014:27-35.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Nrccanada-2014:Detecting aspects and sentiment in customer reviews">

                                <b>[11]</b> Mohammad S.Nrccanada-2014:Detecting aspects and sentiment in customer reviews[C]//Proceedings of the 8th International Workshop on Semantic Evaluation(SemEval 2014),2014:437-442.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Adaptive recursive neural network for target-dependent twitter sentiment classification">

                                <b>[12]</b> Dong L,Wei F,Tan C,et al.Adaptive recursive neural network for target-dependent twitter sentiment classification[C]//Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,2014:49-54.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=DCU:Aspect-based Polarity Classification for Sem Eval Task 4">

                                <b>[13]</b> Wagner J,Arora P,Cortes S,et al.Dcu:Aspectbased polarity classification for semeval task 4[C]//Proceedings of the 8th International Workshop on Semantic Evaluation(SemEval 2014),2014:223-229.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Aspect specific sentiment analysis using hierarchical deep learning">

                                <b>[14]</b> Lakkaraju H,Socher R,Manning C.Aspect specific sentiment analysis using hierarchical deep learning[C]//NIPS Workshop on Deep Learning and Representation Learning,2014.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Coreference Resolution with World Knowledge">

                                <b>[15]</b> Rahman A,Ng V.Coreference resolution with world knowledge[C]//Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:Human Language Technologies-Volume 1.2011:814-824.
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Design Challenges and Misconceptions in Named Entity Recognition">

                                <b>[16]</b> Ratinov L,Roth D.Design challenges and misconceptions in named entity recognition[C]//Proceedings of the Thirteenth Conference on Computational Natural Language Learning.2009:147-155.
                            </a>
                        </p>
                        <p id="35">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A neural knowledge language model[EB]">

                                <b>[17]</b> Ahn S,Choi H,Parnamaa T,et al.A neural knowledge language model[EB].eprint arXiv:1608.00318,2016.
                            </a>
                        </p>
                        <p id="37">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Affective Space 2:Enabling Affective Intuition for ConceptLevel Sentiment Analysis">

                                <b>[18]</b> Cambria E,Fu J,Bisio F,et al.Affectivespace 2:Enabling affective intuition for concept-level sentiment analysis[C]//Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence.AAAI Press,2015:508-514.
                            </a>
                        </p>
                        <p id="39">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Distributed representations of words and phrases and their compositionality">

                                <b>[19]</b> Mikolov T,Sutskever I,Chen K,et al.Distributed Representations of Words and Phrases and their Compositionality[J].Advances in Neural Information Processing Systems,2013,26:3111-3119.
                            </a>
                        </p>
                        <p id="41">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=End-to-end memory networks">

                                <b>[20]</b> Sukhbaatar S,Szlam A,Weston J,et al.End-to-end memory networks[C]//Advances in Neural Information Processing Systems,2015:2431-2439.
                            </a>
                        </p>
                        <p id="43">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Glove:Global Vectors for Word Representation">

                                <b>[21]</b> Pennington J,Socher R,Manning C D.Glove:Global vectors for word representation[C]//Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing,2014:1532-1543.
                            </a>
                        </p>
                        <p id="45">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Document modeling with gated recurrent neural network for sentiment classification">

                                <b>[22]</b> Tang D,Qin B,Liu T.Document modeling with gated recurrent neural network for sentiment classification[C]//Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,2015:1422-1432.
                            </a>
                        </p>
                        <p id="47">
                            <a id="bibliography_23" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSYJ201904027&amp;v=MjA5MzBadEZpRGhVYnZLTHo3U1pMRzRIOWpNcTQ5SFk0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnU=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[23]</b> 胡朝举,梁宁.基于深层注意力的LSTM的特定主题情感分析[J].计算机应用研究,2019,36(4):121-125.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JYRJ201910028" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201910028&amp;v=MTgyMjBGckNVUjdxZlp1WnRGaURoVWJ2S0x6VFpaTEc0SDlqTnI0OUhiSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEc=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="0" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
