<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637135613193440000%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJYRJ201908048%26RESULT%3d1%26SIGN%3dK0SyoJuTZGpxffmvuv3P8xABfs0%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JYRJ201908048&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JYRJ201908048&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201908048&amp;v=MzIxOTdCdEdGckNVUjdxZlp1WnRGeWptV3IzQUx6VFpaTEc0SDlqTXA0OUJiSVFLREg4NHZSNFQ2ajU0TzN6cXE=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#51" data-title="&lt;b&gt;0 引 言&lt;/b&gt; "><b>0 引 言</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#54" data-title="&lt;b&gt;1 分类算法的对比选择&lt;/b&gt; "><b>1 分类算法的对比选择</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#56" data-title="&lt;b&gt;1.1 分类算法简述&lt;/b&gt;"><b>1.1 分类算法简述</b></a></li>
                                                <li><a href="#101" data-title="&lt;b&gt;1.2 分类算法的选择&lt;/b&gt;"><b>1.2 分类算法的选择</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#104" data-title="&lt;b&gt;2 算法模型的改进优化&lt;/b&gt; "><b>2 算法模型的改进优化</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#106" data-title="&lt;b&gt;2.1 NCA特征降维&lt;/b&gt;"><b>2.1 NCA特征降维</b></a></li>
                                                <li><a href="#126" data-title="&lt;b&gt;2.2 超参数调优&lt;/b&gt;"><b>2.2 超参数调优</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#155" data-title="&lt;b&gt;3 应用实证&lt;/b&gt; "><b>3 应用实证</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#158" data-title="&lt;b&gt;3.1 选择训练数据和算法验证&lt;/b&gt;"><b>3.1 选择训练数据和算法验证</b></a></li>
                                                <li><a href="#167" data-title="&lt;b&gt;3.2 运用NCA进行特征降维&lt;/b&gt;"><b>3.2 运用NCA进行特征降维</b></a></li>
                                                <li><a href="#179" data-title="&lt;b&gt;3.3 引入成本函数的超参数调优&lt;/b&gt;"><b>3.3 引入成本函数的超参数调优</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#193" data-title="&lt;b&gt;4 结 语&lt;/b&gt; "><b>4 结 语</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#98" data-title="&lt;b&gt;表1 几种分类算法分析比较&lt;/b&gt;"><b>表1 几种分类算法分析比较</b></a></li>
                                                <li><a href="#161" data-title="&lt;b&gt;表2 样本特征值列表&lt;/b&gt;"><b>表2 样本特征值列表</b></a></li>
                                                <li><a href="#162" data-title="&lt;b&gt;续表2&lt;/b&gt;"><b>续表2</b></a></li>
                                                <li><a href="#164" data-title="图1 多个分类算法的初始比较图">图1 多个分类算法的初始比较图</a></li>
                                                <li><a href="#166" data-title="&lt;b&gt;表3 不同分类算法的初始性能比较&lt;/b&gt;"><b>表3 不同分类算法的初始性能比较</b></a></li>
                                                <li><a href="#174" data-title="图2 使用邻域分量分析NCA识别最相关的特征结果">图2 使用邻域分量分析NCA识别最相关的特征结果</a></li>
                                                <li><a href="#176" data-title="&lt;b&gt;表4 不同分类算法NCA降维后性能比较&lt;/b&gt;"><b>表4 不同分类算法NCA降维后性能比较</b></a></li>
                                                <li><a href="#181" data-title="图3 初步模型的混淆矩阵">图3 初步模型的混淆矩阵</a></li>
                                                <li><a href="#187" data-title="图4 超参数调优迭代过程和结果">图4 超参数调优迭代过程和结果</a></li>
                                                <li><a href="#187" data-title="图4 超参数调优迭代过程和结果">图4 超参数调优迭代过程和结果</a></li>
                                                <li><a href="#189" data-title="&lt;b&gt;表5 超参数调优后最佳估计可行点和最佳观测可行点比较&lt;/b&gt;"><b>表5 超参数调优后最佳估计可行点和最佳观测可行点比较</b></a></li>
                                                <li><a href="#191" data-title="图5 模型优化后的多分类标签混淆矩阵">图5 模型优化后的多分类标签混淆矩阵</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="213">


                                    <a id="bibliography_1" title=" 史甜.数据挖掘在高校贫困生认定系统中的应用研究[D].西安:西安科技大学, 2017." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=1017858672.nh&amp;v=MDg2NjVSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnRGeWptV3IzQVZGMjZHYnU5RnRmTHJaRWJQSVFLREg4NHY=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         史甜.数据挖掘在高校贫困生认定系统中的应用研究[D].西安:西安科技大学, 2017.
                                    </a>
                                </li>
                                <li id="215">


                                    <a id="bibliography_2" title=" 张建明.基于数据挖掘的高校贫困生认定系统设计和分析[D].南京:东南大学, 2015." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=1016050358.nh&amp;v=MTY2NjBGckNVUjdxZlp1WnRGeWptV3IzQVZGMjZHTE85SHRMSnA1RWJQSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEc=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         张建明.基于数据挖掘的高校贫困生认定系统设计和分析[D].南京:东南大学, 2015.
                                    </a>
                                </li>
                                <li id="217">


                                    <a id="bibliography_3" title=" 杨知玲.数据挖掘在高校贫困生评价中的应用研究[D].广州:华南理工大学, 2015." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=1016705915.nh&amp;v=MDE3NzA1NE8zenFxQnRHRnJDVVI3cWZadVp0RnlqbVdyM0FWRjI2R0xTNEc5ak5xcEViUElRS0RIODR2UjRUNmo=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         杨知玲.数据挖掘在高校贫困生评价中的应用研究[D].广州:华南理工大学, 2015.
                                    </a>
                                </li>
                                <li id="219">


                                    <a id="bibliography_4" title=" 李明江, 卢玉, 刘彦.一种基于C4.5决策树的贵州省高校贫困生评定方法[J].科技通报, 2013, 29 (8) :223-224, 233." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=KJTB201308078&amp;v=MTQ1OTlLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnRGeWptV3IzQUxpZmZiTEc0SDlMTXA0OUNiSVE=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                         李明江, 卢玉, 刘彦.一种基于C4.5决策树的贵州省高校贫困生评定方法[J].科技通报, 2013, 29 (8) :223-224, 233.
                                    </a>
                                </li>
                                <li id="221">


                                    <a id="bibliography_5" title=" 陈晓, 王树宝, 李建晶, 等.基于加权约束的决策树方法在贫困生认定中的应用研究[J].计算机应用与软件, 2014, 31 (12) :136-139." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201412033&amp;v=MDMzOTVxcUJ0R0ZyQ1VSN3FmWnVadEZ5am1XcjNBTHpUWlpMRzRIOVhOclk5R1o0UUtESDg0dlI0VDZqNTRPM3o=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                         陈晓, 王树宝, 李建晶, 等.基于加权约束的决策树方法在贫困生认定中的应用研究[J].计算机应用与软件, 2014, 31 (12) :136-139.
                                    </a>
                                </li>
                                <li id="223">


                                    <a id="bibliography_6" title=" 李明君.基于数据挖掘的贫困助学金认定方法研究[D].武汉:华中师范大学, 2017." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=1017266441.nh&amp;v=MTI0MTlBVkYyNkdiRytHTlhJcnBFYlBJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVadEZ5am1XcjM=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                         李明君.基于数据挖掘的贫困助学金认定方法研究[D].武汉:华中师范大学, 2017.
                                    </a>
                                </li>
                                <li id="225">


                                    <a id="bibliography_7" title=" 唐燕, 王苹.随机森林算法在中医药院校贫困生认定预测中的应用研究[J].中国医药导报, 2017, 14 (14) :164-168." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=YYCY201714042&amp;v=MDIwMTA1NE8zenFxQnRHRnJDVVI3cWZadVp0RnlqbVdyM0FQRFRJZDdHNEg5Yk5xNDlCWm9RS0RIODR2UjRUNmo=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                         唐燕, 王苹.随机森林算法在中医药院校贫困生认定预测中的应用研究[J].中国医药导报, 2017, 14 (14) :164-168.
                                    </a>
                                </li>
                                <li id="227">


                                    <a id="bibliography_8" title=" 王正杰, 杨伟丽, 王喆, 等.4种分类算法参数选择及分类特点研究[J].计算机与现代化, 2018 (2) :54-60." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYXH201802014&amp;v=MDcxMDdUNmo1NE8zenFxQnRHRnJDVVI3cWZadVp0RnlqbVdyM0FMelRUWnJHNEg5bk1yWTlFWUlRS0RIODR2UjQ=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                         王正杰, 杨伟丽, 王喆, 等.4种分类算法参数选择及分类特点研究[J].计算机与现代化, 2018 (2) :54-60.
                                    </a>
                                </li>
                                <li id="229">


                                    <a id="bibliography_9" title=" Wiki-Pedia.ID3-algorithm[DB/OL].2018-7-6.https://en.wikipedia.org/wiki/ID3_algorithm#See_also." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=ID3-algorithm">
                                        <b>[9]</b>
                                         Wiki-Pedia.ID3-algorithm[DB/OL].2018-7-6.https://en.wikipedia.org/wiki/ID3_algorithm#See_also.
                                    </a>
                                </li>
                                <li id="231">


                                    <a id="bibliography_10" title=" 卢东标.基于决策树的数据挖掘算法研究与应用[D].武汉:武汉理工大学, 2008." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=2008110317.nh&amp;v=MTUwMTlLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnRGeWptV3IzQVYxMjdGcks1SHRMTnFKRWJQSVE=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[10]</b>
                                         卢东标.基于决策树的数据挖掘算法研究与应用[D].武汉:武汉理工大学, 2008.
                                    </a>
                                </li>
                                <li id="233">


                                    <a id="bibliography_11" title=" 颜会娟, 秦杰.基于非线性SVM模型的木马检测方法[J].计算机工程, 2011, 37 (8) :121-123." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201108042&amp;v=MDUzMDg2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnRGeWptV3IzQUx6N0JiYkc0SDlETXA0OUJab1FLREg4NHZSNFQ=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[11]</b>
                                         颜会娟, 秦杰.基于非线性SVM模型的木马检测方法[J].计算机工程, 2011, 37 (8) :121-123.
                                    </a>
                                </li>
                                <li id="235">


                                    <a id="bibliography_12" >
                                        <b>[12]</b>
                                     Harrington P.Machine learning in Action[M].李锐, 李鹏, 曲亚东, 等译.北京:人民邮电出版社, 2013.</a>
                                </li>
                                <li id="237">


                                    <a id="bibliography_13" >
                                        <b>[13]</b>
                                     Liu Bing.Web Data Mining[M].俞勇, 薛贵荣, 韩定一, 译.北京:清华大学出版社, 2009.</a>
                                </li>
                                <li id="239">


                                    <a id="bibliography_14" >
                                        <b>[14]</b>
                                     Alpaydin E.Introduction to Machine Learning[M].范明, 昝红英, 牛常勇, 译.北京:机械工业出版社.2009, 6.</a>
                                </li>
                                <li id="241">


                                    <a id="bibliography_15" title=" 咸云浩, 张恒德, 谢永华, 等.多元逐步回归与卡尔曼滤波法在霾预报中应用[J].系统仿真学报, 2018, 30 (4) :1482-1489." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XTFZ201804034&amp;v=MTk5OTQ5R1lJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVadEZ5am1XcjNBUFRuTmRMRzRIOW5NcTQ=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[15]</b>
                                         咸云浩, 张恒德, 谢永华, 等.多元逐步回归与卡尔曼滤波法在霾预报中应用[J].系统仿真学报, 2018, 30 (4) :1482-1489.
                                    </a>
                                </li>
                                <li id="243">


                                    <a id="bibliography_16" title=" 刘丛山, 李祥宝, 杨煜普.一种基于近邻元分析的文本分类算法[J].计算机工程, 2012, 38 (15) :139-141." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201215040&amp;v=MTgzMDF6N0JiYkc0SDlQTnFvOUJaSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnRGeWptV3IzQUw=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[16]</b>
                                         刘丛山, 李祥宝, 杨煜普.一种基于近邻元分析的文本分类算法[J].计算机工程, 2012, 38 (15) :139-141.
                                    </a>
                                </li>
                                <li id="245">


                                    <a id="bibliography_17" title=" Zhou H T, Chen J, Dong G M, et al.Bearing fault recognition method based on neighbourhood component analysis and coupled hidden Markov model[J].Mechanical Systems and Signal Processing, 2016, 66/67:568-581." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESBDD9247C067CCEF87E2A25E2C1C17E92&amp;v=MTM5MDNPcTRnMlpPMElmdzlNdVI0VW56ME1TbnFYcm1FMENyT1RNTE9kQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQnJMVTA1dHRoeEx1MndxQT1OaWZPZmNITWF0ag==&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[17]</b>
                                         Zhou H T, Chen J, Dong G M, et al.Bearing fault recognition method based on neighbourhood component analysis and coupled hidden Markov model[J].Mechanical Systems and Signal Processing, 2016, 66/67:568-581.
                                    </a>
                                </li>
                                <li id="247">


                                    <a id="bibliography_18" title=" 邓帅.基于改进贝叶斯优化算法的CNN超参数优化方法[J/OL].计算机应用研究, 2019, 36 (7) .[2018-08-02].http://kns.cnki.net/kcms/detail/51.1196.TP.20180412.0812.030.html." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSYJ201907014&amp;v=MTM5NzVtV3IzQUx6N1NaTEc0SDlqTXFJOUVZSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnRGeWo=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[18]</b>
                                         邓帅.基于改进贝叶斯优化算法的CNN超参数优化方法[J/OL].计算机应用研究, 2019, 36 (7) .[2018-08-02].http://kns.cnki.net/kcms/detail/51.1196.TP.20180412.0812.030.html.
                                    </a>
                                </li>
                                <li id="249">


                                    <a id="bibliography_19" title=" Rasmussen C E, Williams C K I.Gaussian Processes for Machine Learning[M].MIT Press, 2005." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Gaussian Processes for Machine Learning">
                                        <b>[19]</b>
                                         Rasmussen C E, Williams C K I.Gaussian Processes for Machine Learning[M].MIT Press, 2005.
                                    </a>
                                </li>
                                <li id="251">


                                    <a id="bibliography_20" title=" Shahriari B, Swersky K, Wang Z, et al.Taking the Human Out of the Loop:A Review of Bayesian Optimization[J].Proceedings of the IEEE, 2016, 104 (1) :148-175." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Taking the Human Out of the Loop:A Review of Bayesian Optimization &amp;quot;">
                                        <b>[20]</b>
                                         Shahriari B, Swersky K, Wang Z, et al.Taking the Human Out of the Loop:A Review of Bayesian Optimization[J].Proceedings of the IEEE, 2016, 104 (1) :148-175.
                                    </a>
                                </li>
                                <li id="253">


                                    <a id="bibliography_21" title=" 崔佳旭, 杨博.贝叶斯优化方法和应用综述[J].软件学报, 2018, 29 (10) :176-198." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=RJXB201810011&amp;v=MzAzMjg2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnRGeWptV3IzQU55ZlRiTEc0SDluTnI0OUVaWVFLREg4NHZSNFQ=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[21]</b>
                                         崔佳旭, 杨博.贝叶斯优化方法和应用综述[J].软件学报, 2018, 29 (10) :176-198.
                                    </a>
                                </li>
                                <li id="255">


                                    <a id="bibliography_22" title=" 柴慧敏, 赵昀瑶, 方敏.利用先验正态分布的贝叶斯网络参数学习[J].系统工程与电子技术, 2018, 40 (10) :219-224." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XTYD201810031&amp;v=MTYzNDVSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnRGeWptV3IzQVBUblNhckc0SDluTnI0OUdaWVFLREg4NHY=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[22]</b>
                                         柴慧敏, 赵昀瑶, 方敏.利用先验正态分布的贝叶斯网络参数学习[J].系统工程与电子技术, 2018, 40 (10) :219-224.
                                    </a>
                                </li>
                                <li id="257">


                                    <a id="bibliography_23" title=" 黄良斌.高校贫困生认定标准与认定模型研究[J].职业教育研究, 2012 (4) :11-12." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JLKF201204006&amp;v=MTAwMDhadVp0RnlqbVdyM0FMeUhBYUxHNEg5UE1xNDlGWW9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWY=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[23]</b>
                                         黄良斌.高校贫困生认定标准与认定模型研究[J].职业教育研究, 2012 (4) :11-12.
                                    </a>
                                </li>
                                <li id="259">


                                    <a id="bibliography_24" title=" Martinez-Cantin R.BayesOpt:A Bayesian Optimization Library for Nonlinear Optimization, Experimental Design and Bandits[J].Journal of Machine Learning Research, 2014, 15:3735-3739." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=BayesOpt:A Bayesian optimization library for nonlinear optimization, experimental design and bandits">
                                        <b>[24]</b>
                                         Martinez-Cantin R.BayesOpt:A Bayesian Optimization Library for Nonlinear Optimization, Experimental Design and Bandits[J].Journal of Machine Learning Research, 2014, 15:3735-3739.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JYRJ" target="_blank">计算机应用与软件</a>
                2019,36(08),281-287+299 DOI:10.3969/j.issn.1000-386x.2019.08.047            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>NCA降维和贝叶斯优化调参对分类模型的改进</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%8E%E6%96%8C&amp;code=40010544&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">李斌</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E5%8D%AB%E6%98%9F&amp;code=40865705&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王卫星</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%B2%B3%E5%8D%97%E7%A7%91%E6%8A%80%E5%A4%A7%E5%AD%A6%E5%BA%94%E7%94%A8%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2%E7%8E%B0%E4%BB%A3%E6%95%99%E8%82%B2%E6%8A%80%E6%9C%AF%E4%B8%AD%E5%BF%83&amp;code=0207099&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">河南科技大学应用工程学院现代教育技术中心</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>高校贫困生的贫困程度判定可以归属于构建分类模型对样本数据进行训练。但单个分类模型的精准度要取决于处理样本数据的大小和类型复杂度, 在模型速度和准确性之间不易取舍。集成多个分类算法可以避免单个分类算法的过拟合。通过邻域分量分析 (Neighborhood Component Analysis, NCA) 进行特征降维降低初始分类模型的计算成本, 对误判损失引入一个成本函数进行惩罚的同时采用贝叶斯优化进行超参数调优。结果表明, 改进后的分类模型泛化能力得到明显提升。计算时间成本降低的同时, 误判率由初始的8%下降到5%, 模型的准确率提升了近4%。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">分类算法;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%A2%86%E5%9F%9F%E5%88%86%E9%87%8F%E5%88%86%E6%9E%90&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">领域分量分析;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%B4%9D%E5%8F%B6%E6%96%AF%E8%B0%83%E4%BC%98&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">贝叶斯调优;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=MATLAB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">MATLAB;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%B4%AB%E5%9B%B0%E7%94%9F%E5%88%A4%E5%88%AB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">贫困生判别;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    李斌, 高工, 主研领域:数据分析, 网络规划。;
                                </span>
                                <span>
                                    王卫星, 副教授。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-12-15</p>

                    <p>

                            <b>基金：</b>
                                                        <span>河南省2017年高等教育教学改革研究与实践项目 (2017SJGLX636);</span>
                    </p>
            </div>
                    <h1><b>IMPROVEMENT OF CLASSIFICATION MODEL BY NCA DIMENSION REDUCTION AND BAYESIAN OPTIMIZATION PARAMETER ADJUSTMENT</b></h1>
                    <h2>
                    <span>Li Bin</span>
                    <span>Wang Weixing</span>
            </h2>
                    <h2>
                    <span>Modern Education Technology Center, College of Applied Engineering, Henan University of Science and Technology</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Poverty levels of poor students in the university can be attributed to build a classification model of training sample data. But the model of a single classification accuracy depends on the size of the sample data and types of complexity, and it is difficult to choose between the speed and accuracy of the model. Integrating multiple classification algorithm can avoid a single classification algorithm of fitting. Through the neighborhood component analysis (NCA) for feature dimension reduction, we reduced initial classification model of calculating cost. For misjudgment loss, we introduced a cost function to punish and used bayesian optimization to super parameter tuning simultaneously. The results show that the generalization ability of improved classification model is improved significantly. At the same time, the computation time cost decreases, misjudgment rate decreases from 8% to 5%, and the accuracy of the model increases by nearly 4%.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Classification%20algorithm&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Classification algorithm;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Neighborhood%20component%20analysis%20(NCA)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Neighborhood component analysis (NCA) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Bayesian%20tuning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Bayesian tuning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=MATLAB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">MATLAB;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Poor%20student%20discriminant&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Poor student discriminant;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-12-15</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="51" name="51" class="anchor-tag"><b>0 引 言</b></h3>
                <div class="p1">
                    <p id="52">目前对高校贫困生进行判定的方法大都利用数据挖掘技术定量和定性结合。文献<citation id="261" type="reference">[<a class="sup">1</a>]</citation>通过能够面向多值属性的关联规则Apriori算法的改进提高了数据挖掘效率, 为高校贫困生认定工作提供了有利依据;文献<citation id="265" type="reference">[<a class="sup">2</a>,<a class="sup">3</a>,<a class="sup">4</a>]</citation>对数据预处理并使用C4.5算法, 将知识表示成树的形式, 采用错误预测率进行修剪, 分别归纳出决策树, 分析并选出其中较优结果, 原理简单且计算快速准确;文献<citation id="262" type="reference">[<a class="sup">5</a>]</citation>基于加权约束的决策树认定方法提高了贫困生认定效率;文献<citation id="263" type="reference">[<a class="sup">6</a>]</citation>结合Logistic回归、Native Bayes和k近邻三种分类预测模型综合比较认为k近邻模型能更好地判别出学生是否是贫困生;文献<citation id="264" type="reference">[<a class="sup">7</a>]</citation>在相同的数据集中证明随机森林算法分类正确率较高。</p>
                </div>
                <div class="p1">
                    <p id="53">上述学者针对贫困生判定的研究主要侧重于个别分类算法, 对算法的计算成本、性能优化缺乏深入分析, 评价方式比较单一化。本文认为高校贫困生识别可以在做好反复训练和评估模型的基础上, 集成多个分类算法, 运用NCA对特征参数降维以提升计算性能;引入成本惩罚函数并利用贝叶斯超参数调优对分类模型进行进一步优化, 以提升分类模型的预测准确率。</p>
                </div>
                <h3 id="54" name="54" class="anchor-tag"><b>1 分类算法的对比选择</b></h3>
                <div class="p1">
                    <p id="55">分类算法旨在构建分类预测的模型, 是人工智能、模式识别和数据挖掘领域中重要的数据处理方法<citation id="266" type="reference"><link href="227" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>。</p>
                </div>
                <h4 class="anchor-tag" id="56" name="56"><b>1.1 分类算法简述</b></h4>
                <h4 class="anchor-tag" id="57" name="57"><b>1.1.1 决策树CART</b></h4>
                <div class="p1">
                    <p id="58">CART (Classification and Regression tree) 分类回归树使用基尼指数 (Gini) , 采用二元切分法选择特征进行训练数据切割:</p>
                </div>
                <div class="p1">
                    <p id="59"><mathml id="60"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>G</mi><mi>i</mi><mi>n</mi><mi>i</mi><mo stretchy="false"> (</mo><mi>D</mi><mo stretchy="false">) </mo><mo>=</mo><mn>1</mn><mo>-</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow><mi>n</mi></munderover><mrow></mrow></mstyle></mrow></math></mathml><mathml id="61"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo> (</mo><mrow><mfrac><mrow><mi>D</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mi>D</mi></mfrac></mrow><mo>) </mo></mrow><msup><mrow></mrow><mn>2</mn></msup></mrow></math></mathml>      (1) </p>
                </div>
                <div class="p1">
                    <p id="62"><mathml id="63"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>G</mi><mi>i</mi><mi>n</mi><mi>i</mi><mo stretchy="false"> (</mo><mi>D</mi><mo stretchy="false">|</mo><mi>A</mi><mo stretchy="false">) </mo><mo>=</mo><mn>1</mn><mo>-</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow><mi>n</mi></munderover><mrow></mrow></mstyle></mrow></math></mathml><mathml id="64"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo> (</mo><mrow><mfrac><mrow><mi>D</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mi>D</mi></mfrac></mrow><mo>) </mo></mrow><mi>G</mi><mi>i</mi><mi>n</mi><mi>i</mi><mo stretchy="false"> (</mo><mi>D</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo></mrow></math></mathml>      (2) </p>
                </div>
                <div class="p1">
                    <p id="65">决策树算法的优点是计算复杂度不高, 输出结果易于理解, 对中间值的缺失不敏感, 缺点是易会产生过拟合问题<citation id="267" type="reference"><link href="229" rel="bibliography" /><link href="231" rel="bibliography" /><sup>[<a class="sup">9</a>,<a class="sup">10</a>]</sup></citation>。</p>
                </div>
                <h4 class="anchor-tag" id="66" name="66"><b>1.1.2 非线性SVM</b></h4>
                <div class="p1">
                    <p id="67">SVM支持向量机是将低维空间的输入数据投放到一个更高维的特征空间, 用线性决策边界分割在低维空间难以区分的正例和负例。在非线性问题上, 用内积ϕ (<i>x</i><sub><i>i</i></sub>) ·ϕ (<i>x</i><sub><i>j</i></sub>) 代替最优分类面中的点积。</p>
                </div>
                <div class="p1">
                    <p id="68">最大化目标函数为:</p>
                </div>
                <div class="p1">
                    <p id="69"><mathml id="70"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><msub><mrow></mrow><mi>D</mi></msub><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow><mi>n</mi></munderover><mi>a</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow></mrow></mstyle></mrow></math></mathml><i>y</i><sub><i>i</i></sub><i>y</i><sub><i>j</i></sub><i>a</i><sub><i>i</i></sub><i>a</i><sub><i>j</i></sub>〈ϕ (<i>x</i><sub><i>i</i></sub>) ·ϕ (<i>x</i><sub><i>j</i></sub>) 〉      (3) </p>
                </div>
                <div class="p1">
                    <p id="71">约束条件:</p>
                </div>
                <div class="p1">
                    <p id="72"><mathml id="73"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow></mrow></mstyle></mrow></math></mathml><i>y</i><sub><i>i</i></sub><i>a</i><sub><i>i</i></sub>=0, 0≤<i>a</i><sub><i>i</i></sub>≤<i>C i</i>=1, 2, …, <i>n</i>      (4) </p>
                </div>
                <div class="p1">
                    <p id="74">相应的分类器函数转化为:</p>
                </div>
                <div class="p1">
                    <p id="75"><mathml id="76"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>f</mi><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mo>=</mo><mi>s</mi><mi>i</mi><mi>g</mi><mi>n</mi><mrow><mo>[</mo><mrow><mrow><mo> (</mo><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><mi>a</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>〈</mo><mtext>ϕ</mtext><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>⋅</mo><mtext>ϕ</mtext><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mo>〉</mo><mo>+</mo><mi>b</mi></mrow><mo>) </mo></mrow></mrow><mo>]</mo></mrow></mrow></math></mathml>      (5) </p>
                </div>
                <div class="p1">
                    <p id="77">SVM的优点是泛化错误率低, 计算开销不大, 结果易解释;缺点是对主要适用于处理二分类问题, 参数调节和核函数的选择敏感, 但经过构造可以将多分类问题转化为二分类问题<citation id="268" type="reference"><link href="233" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>。</p>
                </div>
                <h4 class="anchor-tag" id="78" name="78"><b>1.1.3 k-最近邻算法</b></h4>
                <div class="p1">
                    <p id="79">k-最近邻给每个属性相等的权重进行基于距离的邻近比较。常用的邻近距离是欧几里德距离, 两个点或样本<i>X</i><sub>1</sub>= (<i>x</i><sub>11</sub>, <i>x</i><sub>12</sub>, …, <i>x</i><sub>1<i>n</i></sub>) 和<i>X</i><sub>2</sub>= (<i>x</i><sub>21</sub>, <i>x</i><sub>22</sub>, …, <i>x</i><sub>2<i>n</i></sub>) 的欧几里德距离为:</p>
                </div>
                <div class="p1">
                    <p id="80"><i>dist</i> (<i>X</i><sub>1</sub>, <i>X</i><sub>2</sub>) =<mathml id="81"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msqrt><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mo stretchy="false"> (</mo></mstyle><mi>x</mi><msub><mrow></mrow><mrow><mn>1</mn><mi>i</mi></mrow></msub><mo>-</mo><mi>x</mi><msub><mrow></mrow><mrow><mn>2</mn><mi>i</mi></mrow></msub><mo stretchy="false">) </mo><msup><mrow></mrow><mn>2</mn></msup></mrow></msqrt></mrow></math></mathml>      (6) </p>
                </div>
                <div class="p1">
                    <p id="82">k-最近邻分类算法的优点是无数据输入假定、噪声数据影响不大、精度略高;缺点是计算空间复杂度高。</p>
                </div>
                <h4 class="anchor-tag" id="83" name="83"><b>1.1.4 贝叶斯方法</b></h4>
                <div class="p1">
                    <p id="84">贝叶斯是基于贝叶斯定理与特征条件独立假设的分类方法, 在数据集<i>D</i>中令<i>A</i><sub>1</sub>, <i>A</i><sub>2</sub>, …, <i>A</i><sub>|<i>A</i>|</sub>为用离散值表示的属性集合, 令<i>C</i>为具有|<i>C</i>|个不同值的类别属性, 假设所有属性都是条件独立于类别<i>C</i>=<i>c</i><sub><i>j</i></sub>, 数学表示为:</p>
                </div>
                <div class="p1">
                    <p id="85"><i>P</i>= (<i>A</i><sub>1</sub>=<i>a</i><sub>1</sub>|<i>A</i><sub>2</sub>=<i>a</i><sub>2</sub>, …, <i>A</i><sub>|<i>A</i>|</sub>=<i>a</i><sub>|<i>A</i>|</sub>, <i>C</i>=<i>c</i><sub><i>j</i></sub>) =<i>P</i> (<i>A</i><sub>1</sub>=<i>a</i><sub><i>i</i></sub>|<i>C</i>=<i>c</i><sub><i>j</i></sub>) </p>
                </div>
                <div class="p1">
                    <p id="86">从训练数据中可以直接得到先验概率<i>P</i> (<i>C</i>=<i>c</i><sub><i>j</i></sub>) 和条件概率<i>P</i> (<i>A</i><sub>1</sub>=<i>a</i><sub><i>i</i></sub>) , 贝叶斯的分类公式为:</p>
                </div>
                <div class="p1">
                    <p id="87" class="code-formula">
                        <mathml id="87"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>c</mi><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mtext>a</mtext><mtext>r</mtext><mtext>g</mtext><mtext>m</mtext><mtext>a</mtext><mtext>x</mtext></mrow></mstyle><mrow><mi>c</mi><msub><mrow></mrow><mi>j</mi></msub></mrow></munder><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>C</mi><mo>=</mo><mi>c</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mstyle displaystyle="true"><munderover><mo>∏</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mo stretchy="false">|</mo><mi>A</mi><mo stretchy="false">|</mo></mrow></munderover><mi>Ρ</mi></mstyle><mo stretchy="false"> (</mo><mi>A</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mi>a</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">|</mo><mi>C</mi><mo>=</mo><mi>c</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>7</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="88">贝叶斯法的优点即使数据较少也可高效处理多类别问题;缺点是对于数据输入假设条件较为敏感。</p>
                </div>
                <h4 class="anchor-tag" id="89" name="89"><b>1.1.5 BP神经网络</b></h4>
                <div class="p1">
                    <p id="90">神经网络是由一个输入层、若干个隐含层和一个输出层组成的多层网络, 各层之间的连接方式通过权重值调节。若模型确定训练误差的理想输出是<i>t</i><sub><i>k</i></sub>, 实际输出是<i>z</i><sub><i>k</i></sub>, <i>c</i>代表输出向量的长度, <i>ω</i>代表网络的所有权值, <i>η</i>是学习速率, 那么总误差表示为:</p>
                </div>
                <div class="p1">
                    <p id="91"><i>J</i> (<i>ω</i>) =<mathml id="92"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>c</mi></munderover><mo stretchy="false"> (</mo></mstyle><mi>t</mi><msub><mrow></mrow><mi>k</mi></msub><mo>-</mo><mi>z</mi><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false">) </mo><msup><mrow></mrow><mn>2</mn></msup><mo>=</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mo stretchy="false">∥</mo><mi>t</mi><mo>-</mo><mi>z</mi><mo stretchy="false">∥</mo></mrow></math></mathml>      (8) </p>
                </div>
                <div class="p1">
                    <p id="93">基于梯度下降的误差反向传播算法BP神经网络是沿着减小误差的方向来调整权值:</p>
                </div>
                <div class="p1">
                    <p id="94"><mathml id="95"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtext>Δ</mtext><mi>ω</mi><mo>=</mo><mo>-</mo><mi>η</mi><mfrac><mrow><mo>∂</mo><mi>J</mi></mrow><mrow><mo>∂</mo><mi>ω</mi></mrow></mfrac></mrow></math></mathml>      (9) </p>
                </div>
                <div class="p1">
                    <p id="96">BP算法对网络拓扑及初始权重敏感, 泛化性能往往不能得到保证, 容易陷入局部最小<citation id="269" type="reference"><link href="235" rel="bibliography" /><link href="237" rel="bibliography" /><link href="239" rel="bibliography" /><sup>[<a class="sup">12</a>,<a class="sup">13</a>,<a class="sup">14</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="97">综上所述, 将几种典型的机器分类算法的对比总结如表1所示。</p>
                </div>
                <div class="area_img" id="98">
                    <p class="img_tit"><b>表1 几种分类算法分析比较</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="98" border="1"><tr><td>算法</td><td>训练<br />速度</td><td>内存<br />使用</td><td>是否<br />调优</td><td>预测<br />速度</td><td>一般评估</td></tr><tr><td>逻辑回归</td><td>快</td><td>小</td><td>最小</td><td>快</td><td>擅长解决有线性决策边界小问题</td></tr><tr><td><br />决策树</td><td>快</td><td>小</td><td>有些</td><td>快</td><td>通用性好, 但容易过拟合</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="99"><b>续表1</b></p>
                </div>
                <div class="area_img" id="100">
                    <p class="img_tit"> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="100" border="1"><tr><td>算法</td><td>训练<br />速度</td><td>内存<br />使用</td><td>是否<br />调优</td><td>预测<br />速度</td><td>一般评估</td></tr><tr><td>非线性SVM</td><td>慢</td><td>中等</td><td>有些</td><td>慢</td><td>擅长解决二进制问题, 能很好处理高维度数据</td></tr><tr><td><br />k-最近邻</td><td>最小</td><td>中等</td><td>最小</td><td>适中</td><td>精度较低, 但易于使用和解释</td></tr><tr><td><br />贝叶斯</td><td>快</td><td>中等</td><td>有些</td><td>快</td><td>广泛应用于文本</td></tr><tr><td><br />神经网络</td><td>慢</td><td>中到大</td><td>很多</td><td>适中</td><td>应用于分类、压缩、识别和预测</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="101" name="101"><b>1.2 分类算法的选择</b></h4>
                <div class="p1">
                    <p id="102">在机器学习领域里, 一方面高度灵活的模型由于拟合了噪声数据的细微变化易造成过拟合, 另一方面简单的模型可能又需有更多的假设条件。在模型速度、准确性和复杂性之间的权衡本已不易, 算法的选择还取决于要处理的数据的大小和类型以及如何运用从数据中获得的洞察力, 因此不存在一种万能的算法可以完美解决所有问题。</p>
                </div>
                <div class="p1">
                    <p id="103">在对高校贫困生预测判定建模时, 需要做好反复训练和评估模型的准备。既可运行所有算法进行比较, 也可从特定分类任务的经验最佳拟合算法开始。对每个训练的分类器, 要保留验证数据或反复使用交叉验证对精确度进行评估, 最终尝试集成多类分类算法克服训练数据的过拟合。</p>
                </div>
                <h3 id="104" name="104" class="anchor-tag"><b>2 算法模型的改进优化</b></h3>
                <div class="p1">
                    <p id="105">分类模型的改进优化意味着进一步提高其准确性和预测能力, 避免模型无法区分数据和噪声时过拟合。本文在对分类模型经反复评估初步确定后, 对模型的改进优化手段主要采取邻域向量分析NCA特征降维和贝叶斯超参数调优。</p>
                </div>
                <h4 class="anchor-tag" id="106" name="106"><b>2.1 NCA特征降维</b></h4>
                <div class="p1">
                    <p id="107">特征降维是向模型添加变量或移除不能改进模型性能的变量, 以在数据建模中提供最佳预测能力<citation id="270" type="reference"><link href="241" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>。特征降维不但可以降低计算成本和存储要求, 还能使预测结果更加精确。</p>
                </div>
                <div class="p1">
                    <p id="108">NCA是一种距离测度学习算法。该算法随机选择近邻, 通过优化留一法 (Leave-one-out, LOO) 的交叉检验结果来求得马氏距离中的变换矩阵。在这个过程中完成降维, 最后在低维空间对数据完成分类。</p>
                </div>
                <div class="p1">
                    <p id="109">数据集<i>X</i>={<i>x</i><sub>1</sub>, <i>x</i><sub>2</sub>, …, <i>x</i><sub><i>n</i></sub>}在<b><i>R</i></b><sup><i>D</i></sup>空间内分别具有类标签<i>c</i><sub>1</sub>, <i>c</i><sub>2</sub>, …, <i>c</i><sub><i>n</i></sub>, 限定马氏距离变换矩阵<b><i>Q</i>=<i>A</i></b><sup>T</sup><b><i>A</i></b>, 两个样本点之间的马氏距离定义为:</p>
                </div>
                <div class="p1">
                    <p id="110" class="code-formula">
                        <mathml id="110"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>d</mi><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>x</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mo>=</mo><msqrt><mrow><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mi>x</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">Q</mi><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mi>x</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo></mrow></msqrt><mo>=</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mtext> </mtext><msqrt><mrow><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">A</mi><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>-</mo><mi mathvariant="bold-italic">A</mi><mi>x</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><msup><mrow></mrow><mtext>Τ</mtext></msup><mi mathvariant="bold-italic">A</mi><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mi mathvariant="bold-italic">A</mi><mi>x</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo></mrow></msqrt></mtd></mtr><mtr><mtd><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mtext> </mtext><mi>i</mi><mo>, </mo><mi>j</mi><mo>=</mo><mn>1</mn><mo>, </mo><mn>2</mn><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>n</mi><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>0</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="111">样本点<i>x</i><sub><i>i</i></sub>随机选择一个<i>x</i><sub><i>j</i></sub>近邻并继承其类标签<i>c</i><sub><i>j</i></sub>的概率<i>P</i><sub><i>ij</i></sub>, 概率<i>P</i><sub><i>ij</i></sub>在变化空间中使用欧式距离定义如下:</p>
                </div>
                <div class="area_img" id="112">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JYRJ201908048_11200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="114">因为每个数据点都可以选择为近邻, 因此输入数据可以继承所有的类标签, 样本点<i>x</i><sub><i>i</i></sub>正确分类的概率为:</p>
                </div>
                <div class="p1">
                    <p id="115"><i>P</i><sub><i>i</i></sub>=<mathml id="116"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>j</mi><mo>∈</mo><mi>c</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></munder><mi>Ρ</mi></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow></math></mathml>      (12) </p>
                </div>
                <div class="p1">
                    <p id="117">NCA搜索变换矩阵<b><i>A</i></b>, 目标函数可以理解为要使得正确分类的点数最大化期望, 也就等同于最小化类间距离:</p>
                </div>
                <div class="p1">
                    <p id="118"><i>f</i> (<b><i>A</i></b>) =<mathml id="119"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mi>i</mi></munder><mi>Ρ</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>i</mi></munder><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>j</mi><mo>∈</mo><mi>c</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></munder><mi>Ρ</mi></mstyle></mrow></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow></math></mathml>      (13) </p>
                </div>
                <div class="p1">
                    <p id="120">这个无约束优化问题通过共轭梯度法或随机梯度法求出<b><i>A</i></b>, 使用微分的变换矩阵:</p>
                </div>
                <div class="p1">
                    <p id="121" class="code-formula"><mathml id="122"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mo>∂</mo><mi>f</mi><mo stretchy="false"> (</mo><mi mathvariant="bold-italic">A</mi><mo stretchy="false">) </mo></mrow><mrow><mo>∂</mo><mi mathvariant="bold-italic">A</mi></mrow></mfrac><mo>=</mo></mrow></math></mathml><mathml id="123"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo>-</mo><mn>2</mn><mi>A</mi><mstyle displaystyle="true"><munder><mo>∑</mo><mi>i</mi></munder><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>j</mi><mo>∈</mo><mi>c</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></munder><mi>Ρ</mi></mstyle></mrow></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mi>x</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mtext>Τ</mtext></msubsup><mo>-</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>k</mi></munder><mi>Ρ</mi></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mi>k</mi></mrow></msub><mi>x</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>k</mi></mrow></msub><mi>x</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mi>k</mi></mrow><mtext>Τ</mtext></msubsup><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>4</mn><mo stretchy="false">) </mo></mrow></math></mathml></p>
                </div>
                <div class="p1">
                    <p id="124">式中:<i>x</i><sub><i>ij</i></sub>=<i>x</i><sub><i>i</i></sub>-<i>x</i><sub><i>j</i></sub>, 当<b><i>A</i></b>是<i>d</i>×<i>D</i>的非方阵时, 经过NCA距离测度学习可以将样本降到<b><i>R</i></b><sup>D</sup>空间<citation id="271" type="reference"><link href="243" rel="bibliography" /><link href="245" rel="bibliography" /><sup>[<a class="sup">16</a>,<a class="sup">17</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="125">实际应用中, 由于共轭梯度法通过多次迭代才能得到目标函数最优解, 占用内存的同时耗时较大, 因此使用等价于共轭梯度的拟牛顿法基础上的L-BFGS (Limited-memory BFGS) 算法进行计算, 其中BFGS是四个提出这种拟牛顿法的四个人名的首字母。L-BFGS算法的核心是不再存储完整的矩阵, 而是存储计算过程中的向量序列, 且只利用最新的向量序列, 以大幅降低运算成本。</p>
                </div>
                <h4 class="anchor-tag" id="126" name="126"><b>2.2 超参数调优</b></h4>
                <div class="p1">
                    <p id="127">识别能提供最佳模型的参数集的过程可称为超参数调优。两个常用的参数调优方法是网格搜索和贝叶斯优化。虽然网格搜索能彻底搜索参数值组合的有限集, 但耗时太长并易遇到维度灾难。</p>
                </div>
                <div class="p1">
                    <p id="128">贝叶斯参数优化充分利用被测试点忽略的前一个点的信息<sup></sup><citation id="272" type="reference"><link href="247" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>。它根据先验分布假设一个搜集函数, 使用每次新采样点去测试目标函数的信息来更新目标函数的先验分布。然后测试由后验分布给出的全局最值最可能出现的位置点。贝叶斯优化虽需执行更多的迭代计算以确定下一个采样点, 但可以较少的评估就找到复杂非凸函数的最小值, 主要分三个步骤:</p>
                </div>
                <div class="p1">
                    <p id="129"> (1) 选择一个先验函数来表达关于被优化函数的假设。本文选择使用的高斯过程是一个随机变量的集合, 任意有限个随机变量都满足一个联合高斯分布<citation id="273" type="reference"><link href="229" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>。若<i>X</i>表示训练集{<i>x</i><sub>1</sub>, <i>x</i><sub>2</sub>, …, <i>x</i><sub><i>t</i></sub>}, <i>f</i>表示未知函数值集合{<i>f</i> (<i>x</i><sub>1</sub>) , <i>f</i> (<i>x</i><sub>2</sub>) , …, <i>f</i> (<i>x</i><sub><i>t</i></sub>) }, <i>Σ</i>表示<i>k</i> (<i>x</i>, <i>x</i>′) 构成的协方差矩阵Ⅱ, <i>θ</i>表示超参数, 当存在观测噪声且假设噪声<i>ε</i>满足独立同分布的高斯分布<i>p</i> (<i>ε</i>) =<font face="EU-HT">N</font> (0, <i>σ</i><sup>2</sup>) , 可以得到边际似然分布为:</p>
                </div>
                <div class="p1">
                    <p id="130"><i>P</i> (<i>y</i>|<i>X</i>, <i>θ</i>) <sub>2</sub>=∫<i>p</i> (<i>y</i>|<i>f</i>) <i>p</i> (<i>f</i>|<i>X</i>, <i>θ</i>) d<i>f</i>=<font face="EU-HT">N</font> (0, <i>Σ</i>+<i>σ</i><sup>2</sup><i>I</i>)      (15) </p>
                </div>
                <div class="p1">
                    <p id="131">式中:<i>y</i>表示观测值集合{<i>y</i><sub>1</sub>, <i>y</i><sub>2</sub>, …, <i>y</i><sub><i>t</i></sub>}。</p>
                </div>
                <div class="p1">
                    <p id="132"> (2) 通过ML极大似然估计对边际似然分布最大化得到优化超参数<mathml id="133"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>θ</mi><mo>^</mo></mover><msub><mrow></mrow><mi>t</mi></msub></mrow></math></mathml>。为超参数赋予先验分布<i>p</i> (<i>θ</i>) , 根据贝叶斯定理得到:</p>
                </div>
                <div class="p1">
                    <p id="134"><mathml id="135"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>θ</mi><mo stretchy="false">|</mo><mi>D</mi><msub><mrow></mrow><mrow><mn>1</mn><mo>∶</mo><mi>t</mi></mrow></msub><mo stretchy="false">) </mo><mo>=</mo><mfrac><mrow><mi>p</mi><mo stretchy="false"> (</mo><mi>D</mi><msub><mrow></mrow><mrow><mn>1</mn><mo>∶</mo><mi>t</mi></mrow></msub><mo stretchy="false">|</mo><mi>θ</mi><mo stretchy="false">) </mo><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>θ</mi><mo stretchy="false">) </mo></mrow><mrow><mi>p</mi><mo stretchy="false"> (</mo><mi>D</mi><msub><mrow></mrow><mrow><mn>1</mn><mo>∶</mo><mi>t</mi></mrow></msub><mo stretchy="false">) </mo></mrow></mfrac></mrow></math></mathml>      (16) </p>
                </div>
                <div class="p1">
                    <p id="136">然后通过最大后验估计MAP最大化式 (6) , 得到<mathml id="137"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>θ</mi><mo>^</mo></mover><msub><mrow></mrow><mi>t</mi></msub></mrow></math></mathml>。</p>
                </div>
                <div class="p1">
                    <p id="138"> (3) 根据<mathml id="139"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>θ</mi><mo>^</mo></mover><msub><mrow></mrow><mi>t</mi></msub></mrow></math></mathml>能够得到具体的采集函数:</p>
                </div>
                <div class="p1">
                    <p id="140"><mathml id="141"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>α</mi><mo>^</mo></mover><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mo>=</mo><mi>α</mi><mo stretchy="false"> (</mo><mi>x</mi></mrow></math></mathml>;<i>θ</i><sub><i>t</i></sub>)      (17) </p>
                </div>
                <div class="p1">
                    <p id="142">然后选择采集函数用来从后验模型构造一个效用函数, 来确定下一个采样点<citation id="274" type="reference"><link href="251" rel="bibliography" /><link href="253" rel="bibliography" /><link href="255" rel="bibliography" /><sup>[<a class="sup">20</a>,<a class="sup">21</a>,<a class="sup">22</a>]</sup></citation>。采集函数可以在具有低建模目标函数的点上对采样进行平衡, 并对尚未建模区域进行搜索。</p>
                </div>
                <div class="p1">
                    <p id="143">贝叶斯超参数调优的算法步骤如算法1所示。</p>
                </div>
                <div class="p1">
                    <p id="144"><b>算法1</b> 贝叶斯优化算法</p>
                </div>
                <div class="area_img" id="212">
                                <img alt="" src="Detail/GetImg?filename=images/JYRJ201908048_21200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="154">为提高找到最优参数值的机率, 并使超参数调优更加高效, 使用MATLAB中的贝叶斯优化工具执行超参数调优, 同时引入成本函数对错误分类进行惩罚。</p>
                </div>
                <h3 id="155" name="155" class="anchor-tag"><b>3 应用实证</b></h3>
                <div class="p1">
                    <p id="156">高校贫困学生的贫困成因多集中在家庭经济情况、生活水平、家庭劳动力状况、在校消费能力水平、消费习惯、学业水平、学习主动力等方面<citation id="275" type="reference"><link href="257" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="157">本文通过某高校2016-2017年度校园应用服务中积累的数据。首先选择训练数据进行分类学习, 反复训练和评估分类模型后选择合适的分类算法。然后采用NCA特征降维和贝叶斯参数调优对模型进行优化, 对某高校的贫困生的精准判定实现预测和评判。</p>
                </div>
                <h4 class="anchor-tag" id="158" name="158"><b>3.1 选择训练数据和算法验证</b></h4>
                <div class="p1">
                    <p id="159">样本数据会以各种形式和大小出现, 如高校贫困生的真实数据集可能较混乱、不完整且采用格式各异。对高校各个业务子系统中得到的原始数据进行预处理需采用专业数据处理工具和不同的预处理方法。</p>
                </div>
                <div class="p1">
                    <p id="160">将从高校各个应用系统中抽取出的数据进行标签标记、清理无效数据、分类汇总后得到完整的样本数据共9 909组。这些组样本数据初步特征值共有21种, 其中部分特征来源于学生调查问卷等, 并对部分数据进行了离散化处理, 如表2所示。</p>
                </div>
                <div class="area_img" id="161">
                    <p class="img_tit"><b>表2 样本特征值列表</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="161" border="1"><tr><td>序号</td><td>特征名称</td><td>字段</td><td>含义</td><td> 描述</td></tr><tr><td><br />1</td><td>num_consump</td><td>Int</td><td>消费<br />次数</td><td>年度内学校餐厅消费次数</td></tr><tr><td><br />2</td><td>sum_consump</td><td>Float</td><td>消费<br />平均</td><td>平均每次消费金额</td></tr><tr><td><br />3</td><td>var_consump</td><td>Float</td><td>消费<br />方差</td><td>消费额方差值</td></tr><tr><td><br />4</td><td>part-time job</td><td>Bit</td><td>打工</td><td>是否勤工俭学</td></tr><tr><td><br />5</td><td>num_borrow</td><td>Int</td><td>借阅数</td><td>年图书馆借阅图书数目</td></tr><tr><td><br />6</td><td>weight_average_core</td><td>Float</td><td>成绩</td><td>各科成绩加权平均分</td></tr><tr><td><br />7</td><td>repeat_core</td><td>Int</td><td>重修</td><td>课程重修门数</td></tr><tr><td><br />8</td><td>elecNum</td><td>Int</td><td>电子<br />产品</td><td>个人拥有电子产品数量</td></tr><tr><td><br />9</td><td>income_family</td><td>Int</td><td>家庭<br />收入</td><td>学生家庭月收入水平值</td></tr><tr><td><br />10</td><td>single_parent</td><td>Bit</td><td>单亲</td><td>是否单亲家庭</td></tr><tr><td><br />11</td><td>edu_multiple_child</td><td>Bit</td><td>子女<br />上学</td><td>学生家庭正受教育子女数</td></tr><tr><td><br />12</td><td>cost_living</td><td>Int</td><td>生活费</td><td>学生在校每月基本生活费</td></tr><tr><td><br />13</td><td>old_poor_remote</td><td>Bit</td><td>老少<br />边穷</td><td>学生家是否老少边穷区</td></tr><tr><td><br />14</td><td>in debt</td><td>Bit</td><td>债务</td><td>家庭是否因病或其他欠债</td></tr><tr><td><br />15</td><td>num_family</td><td>Int</td><td>家庭<br />人口</td><td>学生家庭人口数量</td></tr><tr><td><br />16</td><td>only_child</td><td>Bit</td><td>独生</td><td>是否独生子女</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="162">
                    <p class="img_tit"><b>续表2</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="162" border="1"><tr><td>序号</td><td>特征名称</td><td>字段</td><td>含义</td><td>描述</td></tr><tr><td><br />17</td><td>disease_family</td><td>Bit</td><td>患病</td><td>家庭是否有长期患病人口</td></tr><tr><td><br />18</td><td>score_mutual</td><td>Int</td><td>评分</td><td>同学贫困水平互评得分</td></tr><tr><td><br />19</td><td>health</td><td>Int</td><td>健康<br />情况</td><td>校医室就诊次数</td></tr><tr><td><br />20</td><td>tuition_defer</td><td>Bit</td><td>学费<br />缓交</td><td>是否办理学费缓交手续</td></tr><tr><td><br />21</td><td>classif_academic</td><td>Int</td><td>专业<br />类别</td><td>学生所学专业类别</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="163">在MATLAB中将经过初步清噪脱敏后的数据导入, 对数据样本采用<i>k</i>折交叉验证, <i>k</i>值取5, 每次以<i>k</i>-1份作为训练集, 1份作为验证集。得到验证集性能后, 将5次结果平均作为模型的性能指标, 以最大化使用模型训练的数据量, 得到泛化更好的模型。MATLAB中多个分类器的性能比较和分类初始结果如图1所示。</p>
                </div>
                <div class="area_img" id="164">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201908048_164.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 多个分类算法的初始比较图" src="Detail/GetImg?filename=images/JYRJ201908048_164.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 多个分类算法的初始比较图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201908048_164.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="165">从图1中可以看出, 训练样本明显地被分为common、poorer和poorest三类灰度程度不同的颜色, 其中的“×”为噪声数据。实证对比算法模型结果, 高校贫困生预测最初显示二次支持向量机 (SVM) 表现良好, 然后是线性支持向量机和决策树算法。不同分类器的时间消耗和准确率性能比较如表3所示。</p>
                </div>
                <div class="area_img" id="166">
                    <p class="img_tit"><b>表3 不同分类算法的初始性能比较</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="166" border="1"><tr><td>分类器</td><td>预测速度<br />/ (obs·s<sup>-1</sup>) </td><td>训练<br />时间/s</td><td>训练准<br />确率/%</td></tr><tr><td><br />Decision Tree</td><td>～49 000</td><td>10.491 0</td><td>99.1</td></tr><tr><td><br />Linear Discriminant</td><td>～55 000</td><td>4.903 7</td><td>98.6</td></tr><tr><td><br />Linear SVM</td><td>～58 000</td><td>10.832 0</td><td>99.3</td></tr><tr><td><br />Medium KNN</td><td>～3 300</td><td>11.577 0</td><td>98.9</td></tr><tr><td><br />Quadratic SVM</td><td>～58 000</td><td>7.105 8</td><td>99.4</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="167" name="167"><b>3.2 运用NCA进行特征降维</b></h4>
                <div class="p1">
                    <p id="168">在处理高校贫困生涉及的数据集包含大量特征和有限的观察值时, 运用NCA特征选择技术降维, 具体步骤如下:</p>
                </div>
                <div class="p1">
                    <p id="169"><b>Step1</b> 将训练数据分成5份, 使用CVpartition进行交叉验证, 赋值<i>λ</i>并创建一个数组阵列来存储损失函数值。</p>
                </div>
                <div class="p1">
                    <p id="170"><b>Step2</b> 使用每部分中的训练集, 为每个值训练NCA模型。使用NCA模型计算每部分中相应测试集的分类损失, 记录损失值。</p>
                </div>
                <div class="p1">
                    <p id="171"><b>Step3</b> 重复所有部分训练值和<i>λ</i>值, 计算得出每个<i>λ</i>值的每个部分的平均损失。绘制平均损失值与<i>λ</i>值之间的关系, 找到与最小平均损失对应的最佳<i>λ</i>值。</p>
                </div>
                <div class="p1">
                    <p id="172"><b>Step4</b> 使用最佳<i>λ</i>值拟合NCA模型, 使用计算效率更好的L-BFGS算法去求解目标函数, 标准化预测值绘制特征权重。</p>
                </div>
                <div class="p1">
                    <p id="173">图2显示了在MATLAB中使用邻域分量分析NCA识别的特征权重结果, 圆圈表示对应特征的特征权重。可以看出特征指标1 (num_consump) 、2 (sum_consump) 、3 (var_consump) 、9 (income_family) 、18 (score_mutual) 、12 (cost_living) 、6 (weight_average_core) 、8 (elecNum) 、14 (indebt) 、17 (disease_family) 、19 (tuition_defer) 的特征权重值高于相对阈值0.374 6。利用MATLAB中自带的NCA降维揭示了在贫困生特征中大约一半的特征对模型没有重要作用。因此, 我们可以减少特征数量, 从21个减至11个。</p>
                </div>
                <div class="area_img" id="174">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201908048_174.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 使用邻域分量分析NCA识别最相关的特征结果" src="Detail/GetImg?filename=images/JYRJ201908048_174.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 使用邻域分量分析NCA识别最相关的特征结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201908048_174.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="175">按照NCA降维后的特征选择, 重复前述分类算法, 比较不同算法降维后的各项性能参数如表4所示。</p>
                </div>
                <div class="area_img" id="176">
                    <p class="img_tit"><b>表4 不同分类算法NCA降维后性能比较</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="176" border="1"><tr><td rowspan="2"><br />分类器</td><td colspan="2"><br />预测速度<br />/ (obs·s<sup>-1</sup>) </td><td colspan="2">训练时间/s</td><td colspan="2">训练准确率/%</td></tr><tr><td>降维后</td><td>变化</td><td>降维后</td><td>变化</td><td>降维后</td><td>变化</td></tr><tr><td>Decision Tree</td><td>51 000</td><td>+2 000</td><td>8.589 6</td><td>-1.901 4</td><td>98.9</td><td>-0.2</td></tr><tr><td><br />Linear Discriminant</td><td>83 000</td><td>+28 000</td><td>2.364 9</td><td>-2.538 8</td><td>98.2</td><td>-0.4</td></tr><tr><td><br />Linear SVM</td><td>64 000</td><td>+6 000</td><td>8.576 6</td><td>-2.255 4</td><td>98.7</td><td>-0.6</td></tr><tr><td><br />Medium KNN</td><td>54 000</td><td>+1 100</td><td>8.094 9</td><td>-3.482 1</td><td>98.6</td><td>-0.3</td></tr><tr><td><br />Quadratic SVM</td><td>57 000</td><td>-1 000</td><td>9.591 5</td><td>+2.485 7</td><td>99.0</td><td>-0.4</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="177">从表4的几种分类算法的性能变化值可以明显看出, NCA降维后, 整体预测速度和计算时间变化明显, 特别是线性判别算法因为特征数的大幅减少而性能大幅提升, 决策树分类算法表现优异。</p>
                </div>
                <div class="p1">
                    <p id="178">使用单独的分类算法往往会过度拟合训练数据, 为了克服这种倾向, 可以尝试集成多个分类算法, 典型的比如Boosted Trees和Bagged Trees。测试表明这两种集成分类算法在降维后的准确率仍可以达到99.3%。从上述算法对比中也可以看出, 某些算法初始表现很好, 改进后表现一般, 有的反之。所以可以后退到特征提取阶段去寻找其他特征并降维, 在机器学习工作流程的不同阶段之间反复实验和对比, 寻找最佳模型。</p>
                </div>
                <h4 class="anchor-tag" id="179" name="179"><b>3.3 引入成本函数的超参数调优</b></h4>
                <div class="p1">
                    <p id="180">在高校贫困生预测分类模型中, 单单根据总体精确度分析性能很容易产生误导, 比如未能准确预测实际贫困相比错误地将正常情况学生误判为贫困要造成更大的不公平。图3所示的初步模型分类结果混淆矩阵, 将3%的贫困生误报为正常学生, 而将8%的普通学生分类为贫困和极度贫困。这将造成部分学生的评判结果失真, 不需补助的学生得到补助, 而急需补助的学生却失去应有的补助。</p>
                </div>
                <div class="area_img" id="181">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201908048_181.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 初步模型的混淆矩阵" src="Detail/GetImg?filename=images/JYRJ201908048_181.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 初步模型的混淆矩阵  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201908048_181.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="182">为了改进分类器, 引入成本函数对误分类进行惩罚, 补偿数据中较少的“异常”观察, 并使分类器偏向于较少的错误分类异常噪声, 将较高的错误分类成本分配给“异常”类。同时利用贝叶斯优化方法对模型参数进行超参数调优。由于Trees的表现优于SVM, 本文以生成树为效果目标, 步骤如下:</p>
                </div>
                <div class="p1">
                    <p id="183"><b>Step1</b> 因为是common、poorer和poorest多分类, 首先使用AdaBoostM1和Trees模型5倍交叉验证分类, 指定每个Trees最多被分割5次。然后对“common”的误分类分配一个高成本值20以进行惩罚, 即引入置信度的AdaBoostM2模型进行对比。</p>
                </div>
                <div class="p1">
                    <p id="184"><b>Step2</b> 在MATLAB中选用Bayseopt工具箱<citation id="276" type="reference"><link href="259" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>, 使用fitcensemble找到使交叉验证损失最小化5倍的超参数, 设置随机种子值并使用“expected-improvement-plus”采集函数确定下一个要评估的点, 并在置信区域内进行探索。为了重复并可视化, 将它们传递到OptimizeHyperparameters名称-值对中, 需要优化的参数默认为KernelScale和BoxConstraint。</p>
                </div>
                <div class="p1">
                    <p id="185"><b>Step3</b> 传递参数作为优化超参数的值后命令行中会出现迭代显示, 超参数调优结果如图4所示, 目标函数为回归的log (1+交叉验证损失) 和分类的误分类率。进行迭代以优化超参数、最小化分类器的交叉验证损失, 使用经过优化超参数训练的模型预测验证集的类标签, 可以看出经过迭代后泛化能力拟合。图4中的稍小圆点表明目标点, 稍大圆点标明采集函数值最大的位置并以此作为下一个采集点。最佳估计可行点是根据最新模型估计均值最低的采集点, 最佳观测可行点是目标函数评价返回值最低的采集点。</p>
                </div>
                <div class="area_img" id="187">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201908048_18700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 超参数调优迭代过程和结果" src="Detail/GetImg?filename=images/JYRJ201908048_18700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 超参数调优迭代过程和结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201908048_18700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="187">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201908048_18701.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 超参数调优迭代过程和结果" src="Detail/GetImg?filename=images/JYRJ201908048_18701.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 超参数调优迭代过程和结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201908048_18701.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="188">表5说明了采用集成分类AdaBoostM2经过贝叶斯超参数调优后最佳估计可行点和最佳观测可行点的比较结果。可以看出准确率由93.45%提升到了97.49%, 函数计算时间成本约降低了14 s, 优化效果明显。</p>
                </div>
                <div class="area_img" id="189">
                    <p class="img_tit"><b>表5 超参数调优后最佳估计可行点和最佳观测可行点比较</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="189" border="1"><tr><td><br />比较值</td><td>最佳观测可行点</td><td>最佳估计可行点</td></tr><tr><td><br />集成多分类方法</td><td>AdaBoostM1</td><td>AdaBoostM2</td></tr><tr><td><br />迭代次数</td><td>39</td><td>41</td></tr><tr><td><br />准确率</td><td>0.944 51</td><td>0.974 94</td></tr><tr><td><br />目标函数值</td><td>0.002 018 4</td><td>0.002 099 6</td></tr><tr><td><br />计算时间/s</td><td>177.931</td><td>163.806 4</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="190"><b>Step 4</b> 利用MATLAB中的混淆矩阵生成函数Confusion Matrix和热图生成函数Heatmap将经过训练的模型预测验证集的类标签, 生成优化后的多分类混淆矩阵并可视化, 如图5所示。</p>
                </div>
                <div class="area_img" id="191">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201908048_191.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 模型优化后的多分类标签混淆矩阵" src="Detail/GetImg?filename=images/JYRJ201908048_191.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 模型优化后的多分类标签混淆矩阵  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201908048_191.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="192">从优化后的多分类标签混淆矩阵可以看出, 经过NCA降维后引入成本函数惩罚并用贝叶斯超参数优化后的模型将初步模型8%的普通学生分类为贫困和极度贫困误报率减少到5%, 模型的准确率明显提升, 达到了优化效果。</p>
                </div>
                <h3 id="193" name="193" class="anchor-tag"><b>4 结 语</b></h3>
                <div class="p1">
                    <p id="194">高校贫困生预测判定建模运行了多种算法训练分类器, 单独的分类算法会过度拟合训练数据, 而且没有一种算法是万能最优, 反复训练试错才是选择最佳算法的前提。对比算法模型结果, 二次支持向量机 (SVM) 、线性支持向量机和决策树算法表现略优。使用NCA方法降维后, 整体预测速度和计算时间变化明显, 决策树分类算法表现优异。集成分类算法Boosted Trees和Bagged是提升泛化能力的合理有效选择。</p>
                </div>
                <div class="p1">
                    <p id="195">在初始模型上保留验证数据, 使用AdaBoostM1和Trees模型<i>k</i>折交叉验证反复评估, 与引入成本函数权重值调整的AdaBoostM2模型经贝叶斯超参数调优后对比。高校贫困生预测判定AdaBoostM2模型的准确率提升了近4%, 计算时间成本降低了14 s, 误判率由初始的8%改进到5%, 说明优化改进后的算法模型的泛化能力得到了一定的改进。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="213">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=1017858672.nh&amp;v=MTk0MzBadEZ5am1XcjNBVkYyNkdidTlGdGZMclpFYlBJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnU=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> 史甜.数据挖掘在高校贫困生认定系统中的应用研究[D].西安:西安科技大学, 2017.
                            </a>
                        </p>
                        <p id="215">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=1016050358.nh&amp;v=MDgxMzh0R0ZyQ1VSN3FmWnVadEZ5am1XcjNBVkYyNkdMTzlIdExKcDVFYlBJUUtESDg0dlI0VDZqNTRPM3pxcUI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> 张建明.基于数据挖掘的高校贫困生认定系统设计和分析[D].南京:东南大学, 2015.
                            </a>
                        </p>
                        <p id="217">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=1016705915.nh&amp;v=MTAwODhHOWpOcXBFYlBJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVadEZ5am1XcjNBVkYyNkdMUzQ=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> 杨知玲.数据挖掘在高校贫困生评价中的应用研究[D].广州:华南理工大学, 2015.
                            </a>
                        </p>
                        <p id="219">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=KJTB201308078&amp;v=MjY2NTZiSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnRGeWptV3IzQUxpZmZiTEc0SDlMTXA0OUM=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b> 李明江, 卢玉, 刘彦.一种基于C4.5决策树的贵州省高校贫困生评定方法[J].科技通报, 2013, 29 (8) :223-224, 233.
                            </a>
                        </p>
                        <p id="221">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201412033&amp;v=MTUwODg2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnRGeWptV3IzQUx6VFpaTEc0SDlYTnJZOUdaNFFLREg4NHZSNFQ=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b> 陈晓, 王树宝, 李建晶, 等.基于加权约束的决策树方法在贫困生认定中的应用研究[J].计算机应用与软件, 2014, 31 (12) :136-139.
                            </a>
                        </p>
                        <p id="223">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=1017266441.nh&amp;v=Mjg4MTJycEViUElRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVp0RnlqbVdyM0FWRjI2R2JHK0dOWEk=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b> 李明君.基于数据挖掘的贫困助学金认定方法研究[D].武汉:华中师范大学, 2017.
                            </a>
                        </p>
                        <p id="225">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=YYCY201714042&amp;v=MTYwNjZxZlp1WnRGeWptV3IzQVBEVElkN0c0SDliTnE0OUJab1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjc=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b> 唐燕, 王苹.随机森林算法在中医药院校贫困生认定预测中的应用研究[J].中国医药导报, 2017, 14 (14) :164-168.
                            </a>
                        </p>
                        <p id="227">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYXH201802014&amp;v=MjM0MDk5bk1yWTlFWUlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVp0RnlqbVdyM0FMelRUWnJHNEg=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b> 王正杰, 杨伟丽, 王喆, 等.4种分类算法参数选择及分类特点研究[J].计算机与现代化, 2018 (2) :54-60.
                            </a>
                        </p>
                        <p id="229">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=ID3-algorithm">

                                <b>[9]</b> Wiki-Pedia.ID3-algorithm[DB/OL].2018-7-6.https://en.wikipedia.org/wiki/ID3_algorithm#See_also.
                            </a>
                        </p>
                        <p id="231">
                            <a id="bibliography_10" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=2008110317.nh&amp;v=MTI4MzN5am1XcjNBVjEyN0ZySzVIdExOcUpFYlBJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVadEY=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[10]</b> 卢东标.基于决策树的数据挖掘算法研究与应用[D].武汉:武汉理工大学, 2008.
                            </a>
                        </p>
                        <p id="233">
                            <a id="bibliography_11" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201108042&amp;v=MDgxMzh0R0ZyQ1VSN3FmWnVadEZ5am1XcjNBTHo3QmJiRzRIOURNcDQ5QlpvUUtESDg0dlI0VDZqNTRPM3pxcUI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[11]</b> 颜会娟, 秦杰.基于非线性SVM模型的木马检测方法[J].计算机工程, 2011, 37 (8) :121-123.
                            </a>
                        </p>
                        <p id="235">
                            <a id="bibliography_12" >
                                    <b>[12]</b>
                                 Harrington P.Machine learning in Action[M].李锐, 李鹏, 曲亚东, 等译.北京:人民邮电出版社, 2013.
                            </a>
                        </p>
                        <p id="237">
                            <a id="bibliography_13" >
                                    <b>[13]</b>
                                 Liu Bing.Web Data Mining[M].俞勇, 薛贵荣, 韩定一, 译.北京:清华大学出版社, 2009.
                            </a>
                        </p>
                        <p id="239">
                            <a id="bibliography_14" >
                                    <b>[14]</b>
                                 Alpaydin E.Introduction to Machine Learning[M].范明, 昝红英, 牛常勇, 译.北京:机械工业出版社.2009, 6.
                            </a>
                        </p>
                        <p id="241">
                            <a id="bibliography_15" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XTFZ201804034&amp;v=MjI5OTdJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVadEZ5am1XcjNBUFRuTmRMRzRIOW5NcTQ5R1k=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[15]</b> 咸云浩, 张恒德, 谢永华, 等.多元逐步回归与卡尔曼滤波法在霾预报中应用[J].系统仿真学报, 2018, 30 (4) :1482-1489.
                            </a>
                        </p>
                        <p id="243">
                            <a id="bibliography_16" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201215040&amp;v=MjMwNTZaSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnRGeWptV3IzQUx6N0JiYkc0SDlQTnFvOUI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[16]</b> 刘丛山, 李祥宝, 杨煜普.一种基于近邻元分析的文本分类算法[J].计算机工程, 2012, 38 (15) :139-141.
                            </a>
                        </p>
                        <p id="245">
                            <a id="bibliography_17" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESBDD9247C067CCEF87E2A25E2C1C17E92&amp;v=MjY2MjNYcm1FMENyT1RNTE9kQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQnJMVTA1dHRoeEx1MndxQT1OaWZPZmNITWF0ak9xNGcyWk8wSWZ3OU11UjRVbnowTVNucQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[17]</b> Zhou H T, Chen J, Dong G M, et al.Bearing fault recognition method based on neighbourhood component analysis and coupled hidden Markov model[J].Mechanical Systems and Signal Processing, 2016, 66/67:568-581.
                            </a>
                        </p>
                        <p id="247">
                            <a id="bibliography_18" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSYJ201907014&amp;v=MDEzNDVSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnRGeWptV3IzQUx6N1NaTEc0SDlqTXFJOUVZSVFLREg4NHY=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[18]</b> 邓帅.基于改进贝叶斯优化算法的CNN超参数优化方法[J/OL].计算机应用研究, 2019, 36 (7) .[2018-08-02].http://kns.cnki.net/kcms/detail/51.1196.TP.20180412.0812.030.html.
                            </a>
                        </p>
                        <p id="249">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Gaussian Processes for Machine Learning">

                                <b>[19]</b> Rasmussen C E, Williams C K I.Gaussian Processes for Machine Learning[M].MIT Press, 2005.
                            </a>
                        </p>
                        <p id="251">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Taking the Human Out of the Loop:A Review of Bayesian Optimization &amp;quot;">

                                <b>[20]</b> Shahriari B, Swersky K, Wang Z, et al.Taking the Human Out of the Loop:A Review of Bayesian Optimization[J].Proceedings of the IEEE, 2016, 104 (1) :148-175.
                            </a>
                        </p>
                        <p id="253">
                            <a id="bibliography_21" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=RJXB201810011&amp;v=MTYyNTJGeWptV3IzQU55ZlRiTEc0SDluTnI0OUVaWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnQ=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[21]</b> 崔佳旭, 杨博.贝叶斯优化方法和应用综述[J].软件学报, 2018, 29 (10) :176-198.
                            </a>
                        </p>
                        <p id="255">
                            <a id="bibliography_22" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XTYD201810031&amp;v=MjU5MTRqbVdyM0FQVG5TYXJHNEg5bk5yNDlHWllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVp0Rnk=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[22]</b> 柴慧敏, 赵昀瑶, 方敏.利用先验正态分布的贝叶斯网络参数学习[J].系统工程与电子技术, 2018, 40 (10) :219-224.
                            </a>
                        </p>
                        <p id="257">
                            <a id="bibliography_23" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JLKF201204006&amp;v=MDU5ODRSN3FmWnVadEZ5am1XcjNBTHlIQWFMRzRIOVBNcTQ5RllvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1U=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[23]</b> 黄良斌.高校贫困生认定标准与认定模型研究[J].职业教育研究, 2012 (4) :11-12.
                            </a>
                        </p>
                        <p id="259">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=BayesOpt:A Bayesian optimization library for nonlinear optimization, experimental design and bandits">

                                <b>[24]</b> Martinez-Cantin R.BayesOpt:A Bayesian Optimization Library for Nonlinear Optimization, Experimental Design and Bandits[J].Journal of Machine Learning Research, 2014, 15:3735-3739.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JYRJ201908048" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201908048&amp;v=MzIxOTdCdEdGckNVUjdxZlp1WnRGeWptV3IzQUx6VFpaTEc0SDlqTXA0OUJiSVFLREg4NHZSNFQ2ajU0TzN6cXE=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
