<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637136397067940000%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJYRJ201903036%26RESULT%3d1%26SIGN%3dxM7fHsVvAejRmRyAFIq3N3QuXxM%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JYRJ201903036&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JYRJ201903036&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201903036&amp;v=MjAyOTJySTlHWW9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRmlEbFY3N0xMelRaWkxHNEg5ak0=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#21" data-title="&lt;b&gt;0 引 言&lt;/b&gt; "><b>0 引 言</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#27" data-title="&lt;b&gt;1 算法设计&lt;/b&gt; "><b>1 算法设计</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#28" data-title="&lt;b&gt;1.1 工作流程&lt;/b&gt;"><b>1.1 工作流程</b></a></li>
                                                <li><a href="#32" data-title="&lt;b&gt;1.2 手势图像的预处理&lt;/b&gt;"><b>1.2 手势图像的预处理</b></a></li>
                                                <li><a href="#42" data-title="&lt;b&gt;1.3 数据集介绍&lt;/b&gt;"><b>1.3 数据集介绍</b></a></li>
                                                <li><a href="#45" data-title="&lt;b&gt;1.4 手势识别的卷积神经网络的模型结构&lt;/b&gt;"><b>1.4 手势识别的卷积神经网络的模型结构</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#63" data-title="&lt;b&gt;2 网络模型训练及测试结果&lt;/b&gt; "><b>2 网络模型训练及测试结果</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#71" data-title="&lt;b&gt;3 结 语&lt;/b&gt; "><b>3 结 语</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#30" data-title="图1 基于卷积神经网络的手势训练模型的工作流程图">图1 基于卷积神经网络的手势训练模型的工作流程图</a></li>
                                                <li><a href="#31" data-title="图2 基于卷积神经网络的手势识别模型的工作流程图">图2 基于卷积神经网络的手势识别模型的工作流程图</a></li>
                                                <li><a href="#41" data-title="图3 基于肤色阈值的手势区域分割">图3 基于肤色阈值的手势区域分割</a></li>
                                                <li><a href="#44" data-title="图4 gesture数据集">图4 gesture数据集</a></li>
                                                <li><a href="#47" data-title="图5 手势识别的卷积神经网络的模型结构">图5 手势识别的卷积神经网络的模型结构</a></li>
                                                <li><a href="#66" data-title="图6 训练过程中迭代次数与交叉熵的变化曲线">图6 训练过程中迭代次数与交叉熵的变化曲线</a></li>
                                                <li><a href="#67" data-title="图7 训练过程中迭代次数与准确率的变化曲线">图7 训练过程中迭代次数与准确率的变化曲线</a></li>
                                                <li><a href="#70" data-title="&lt;b&gt;表1 各种算法结果对比&lt;/b&gt;"><b>表1 各种算法结果对比</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" title=" 朱雯文, 叶西宁. 基于卷积神经网络的手势识别算法[J]. 华东理工大学学报 (自然科学版) , 2018, 44 (2) : 260-269." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=HLDX201802018&amp;v=MTM1MTNZOUViSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnNGaURsVjc3TExTSFBkckc0SDluTXI=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         朱雯文, 叶西宁. 基于卷积神经网络的手势识别算法[J]. 华东理工大学学报 (自然科学版) , 2018, 44 (2) : 260-269.
                                    </a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" title=" Rossi M, Benatti S, Farella E, et al. Hybrid EMG classifier based on HMM and SVM for hand gesture recognition in prosthetics[C]//IEEE International Conference on Industrial Technology.IEEE, 2015:1700-1705." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Hybrid EMG classifier based on HMM and SVM for hand gesture recognition in prosthetics">
                                        <b>[2]</b>
                                         Rossi M, Benatti S, Farella E, et al. Hybrid EMG classifier based on HMM and SVM for hand gesture recognition in prosthetics[C]//IEEE International Conference on Industrial Technology.IEEE, 2015:1700-1705.
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title=" Prasuhn L, Oyamada Y, Mochizuki Y, et al. A HOG-based hand gesture recognition system on a mobile device[C]//IEEE International Conference on Image Processing. IEEE, 2015:3973-3977." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A HOG-based hand gesture recognition system on a mobile device">
                                        <b>[3]</b>
                                         Prasuhn L, Oyamada Y, Mochizuki Y, et al. A HOG-based hand gesture recognition system on a mobile device[C]//IEEE International Conference on Image Processing. IEEE, 2015:3973-3977.
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" title=" Jimenez J, Martin A, Uc V, et al. Mexican sign language alphanumerical gestures recognition using 3D haar-like features[J].IEEE Latin America Transactions, 2017, 15 (10) :2000-2005." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Mexican Sign Language Alphanumerical Gestures Recognition using 3D Haar-like Features &amp;quot;">
                                        <b>[4]</b>
                                         Jimenez J, Martin A, Uc V, et al. Mexican sign language alphanumerical gestures recognition using 3D haar-like features[J].IEEE Latin America Transactions, 2017, 15 (10) :2000-2005.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" title=" Flores C J L, Cutipa A E G, Enciso R L. Application of convolutional neural networks for static hand gestures recognition under different invariant features[C]//2017 IEEE XXIV International Conference on Electronics, Electrical Engineering and Computing (INTERCON) , Cusco, Peru, 2017." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Application of convolutional neural networks for static hand gestures recognition under different invariant features &amp;quot;">
                                        <b>[5]</b>
                                         Flores C J L, Cutipa A E G, Enciso R L. Application of convolutional neural networks for static hand gestures recognition under different invariant features[C]//2017 IEEE XXIV International Conference on Electronics, Electrical Engineering and Computing (INTERCON) , Cusco, Peru, 2017.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" title=" Shaik K B, Ganesan P, Kalist V, et al. Comparative study of skin color detection and segmentation in HSV and YCbCr color space[J]. Procedia Computer Science, 2015, 57: 41-48." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES6E57A9585CF23521D1D023FF0FC08B7E&amp;v=MDkzOTJHOWE5cG9wTllaaDVEbjg4elJkbjYwdDlTbnlVMmhKRENyS2NONzNxQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQnJMVTA1dHBnekxpN3dhcz1OaWZPZmJYTg==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                         Shaik K B, Ganesan P, Kalist V, et al. Comparative study of skin color detection and segmentation in HSV and YCbCr color space[J]. Procedia Computer Science, 2015, 57: 41-48.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" title=" Cheng X, Zhang Y H, Chen Y Q, et al. Pest identification via deep residual learning in complex background[J]. Computers and Electronics in Agriculture, 2017, 141:351-356." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Pest identification via deep residual learning in complex background">
                                        <b>[7]</b>
                                         Cheng X, Zhang Y H, Chen Y Q, et al. Pest identification via deep residual learning in complex background[J]. Computers and Electronics in Agriculture, 2017, 141:351-356.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" title=" 顾郑平, 朱敏. 基于深度学习的鱼类分类算法研究[J]. 计算机应用与软件, 2018, 35 (1) :200-205." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201801036&amp;v=MTE5NzlHRnJDVVI3cWZadVpzRmlEbFY3N0xMelRaWkxHNEg5bk1ybzlHWW9RS0RIODR2UjRUNmo1NE8zenFxQnQ=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                         顾郑平, 朱敏. 基于深度学习的鱼类分类算法研究[J]. 计算机应用与软件, 2018, 35 (1) :200-205.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" title=" Sridevi K, Sundarambal M, Muralidharan K, et al. FPGA implementation of hand gesture recognition system using neural networks[C]//International Conference on Intelligent Systems and Control. IEEE, 2017:34-39." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=FPGA implementation of hand gesture recognition system using neural networks">
                                        <b>[9]</b>
                                         Sridevi K, Sundarambal M, Muralidharan K, et al. FPGA implementation of hand gesture recognition system using neural networks[C]//International Conference on Intelligent Systems and Control. IEEE, 2017:34-39.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JYRJ" target="_blank">计算机应用与软件</a>
                2019,36(03),192-195+281 DOI:10.3969/j.issn.1000-386x.2019.03.035            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于改进的卷积神经网络的手势识别的研究</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%B0%A2%E9%93%AE%E6%A1%82&amp;code=07040445&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">谢铮桂</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E9%9F%A9%E5%B1%B1%E5%B8%88%E8%8C%83%E5%AD%A6%E9%99%A2&amp;code=0010724&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">韩山师范学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>手势作为一种自然语义表达方式, 在人机交互中发挥着重要的作用。针对手势图像复杂的背景影响识别准确性且传统方法中人工提取的图像特征难以适应手势多变性的问题, 提出一种基于肤色阈值和卷积神经网络的手势识别算法。实验结果表明:该算法在两个数据集下对手势的平均识别率均达到96%以上, 因此该算法是可行的。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%89%8B%E5%8A%BF%E8%AF%86%E5%88%AB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">手势识别;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">卷积神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%82%A4%E8%89%B2%E9%98%88%E5%80%BC&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">肤色阈值;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    谢铮桂, 高级实验师, 主研领域:深度学习, 智能算法。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-10-04</p>

                    <p>

                            <b>基金：</b>
                                                        <span>广东省自然科学基金项目 (2016A030307050);</span>
                    </p>
            </div>
                    <h1><b>GESTURE RECOGNITION BASED ON IMPROVED CONVOLUTIONAL NEURAL NETWORK</b></h1>
                    <h2>
                    <span>Xie Zhenggui</span>
            </h2>
                    <h2>
                    <span>Hanshan Normal University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>As a very natural method of semantic expression, gesture plays an important role in human computer interaction. The complex background of gesture images affect the accuracy of recognition. It is difficult to adapt to the variability of hand gestures in traditional methods. In order to solve the problems, we proposed a gesture recognition algorithm based on skin color threshold and convolution neural network. The experimental results show that the average recognition rate of the algorithm is over 96% in both datasets. So the algorithm is feasible.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Gesture%20recognition&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Gesture recognition;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Convolutional%20neural%20network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Convolutional neural network;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Skin%20color%20threshold&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Skin color threshold;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-10-04</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="21" name="21" class="anchor-tag"><b>0 引 言</b></h3>
                <div class="p1">
                    <p id="22">随着人机交互、计算机视觉技术的不断发展, 基于视觉的手势识别已成为国内外学者研究的重点, 也是计算机视觉领域的一个研究热点<citation id="73" type="reference"><link href="3" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>。手势作为一种自然语义表达方式, 在人机交互中发挥着重要的作用。</p>
                </div>
                <div class="p1">
                    <p id="23">目前已经有许多国内外学者对手势识别开展相关研究。文献<citation id="74" type="reference">[<a class="sup">2</a>]</citation>针对手势识别提出两种异构分类器组合的方法, 基于HMM来区分稳态数据和暂态数据, 最后使用SVM对手势肌电信号进行识别。文献<citation id="75" type="reference">[<a class="sup">3</a>]</citation>提出了基于方向梯度直方图 (HOG) 的手势识别方法。文献<citation id="76" type="reference">[<a class="sup">4</a>]</citation>使用Kinect传感器去捕获图像的3D Haar-like特征, 用Adaboost学习算法识别手语, 并通过将3D特征信息与增强算法相结合, 从而提高算法的分类效率。文献<citation id="77" type="reference">[<a class="sup">5</a>]</citation>采用卷积神经网络算法, 对手势数据集进行训练之前先对图像进行预处理, 分割出手势区域作为网络输入, 分割后的图像是一个64×64像素的三通道彩色图像。</p>
                </div>
                <div class="p1">
                    <p id="24">分析上述文献发现存在两个问题: (1) 文献<citation id="79" type="reference">[<a class="sup">2</a>,<a class="sup">3</a>,<a class="sup">4</a>]</citation>中提出的方法为传统的手势识别方法, 需要先人工提取图像特征, 再设计分类器。但人工提取图像特征难以适应手势图像复杂的背景及手势多变性, 加大了手势识别的难度, 降低了识别准确率。 (2) 文献<citation id="78" type="reference">[<a class="sup">5</a>]</citation>用彩色图像作为输入, 会增加卷积神经网络模型参数, 使模型的算法复杂度更高。</p>
                </div>
                <div class="p1">
                    <p id="25">问题 (1) 的解决方案是构建基于卷积神经网络的手势识别模型。近年来卷积神经网络在图像分类、物体检测和动作识别等领域之所以取得极大成功, 因为相比传统人工提取图像特征, 卷积神经网络能自动提取图像更高层的特征信息, 实现end-to-end分类, 以达到更高的识别率。</p>
                </div>
                <div class="p1">
                    <p id="26">针对问题 (2) , 可以构建基于灰度的卷积神经网络。用单通道gesture数据集训练CNN, 大大减少网络参数, 减少网络计算复杂度, 节省网络训练时间。再用真实场景的手势图像测试, 对输入图像先基于肤色阈值分割出手势区域并作掩膜处理, 再输入到CNN模型测试。</p>
                </div>
                <h3 id="27" name="27" class="anchor-tag"><b>1 算法设计</b></h3>
                <h4 class="anchor-tag" id="28" name="28"><b>1.1 工作流程</b></h4>
                <div class="p1">
                    <p id="29">传统手势识别算法进行手势识别时需要先对输入手势图像进行复杂的预处理和人工提取图像特征, 然后再进行特征分类。这种算法识别结果的好坏很大程度上取决于人工提取的特征是否合理, 而人工提取特征往往依靠经验, 主观性较强, 导致结果不准确。与传统识别方法不同, 本文提出的识别算法分两个阶段:先构建和训练CNN模型, 再测试CNN模型。第一阶段是先构建灰度CNN神经网络模型, 接着对gesture数据集预处理, 规整后作为CNN网络模型的输入。模型训练过程中自动学习输入样本的手势特征和分类, 训练完成后保存CNN模型的结构和参数, 如图1所示。第二阶段是先对自建手势图像数据集作预处理, 分割出手势区域并作掩膜处理, 再输入到训练好的CNN模型进行预测, 最终得到识别结果, 如图2所示。因为自建手势图像是通过摄像头采集包含了复杂的背景信息, 直接输入CNN模型预测会影响识别结果, 所以, 需对采集图像预处理。图像预处理是基于肤色阈值对图像中的手势部分进行分割并作掩膜处理。</p>
                </div>
                <div class="area_img" id="30">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201903036_030.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 基于卷积神经网络的手势训练模型的工作流程图" src="Detail/GetImg?filename=images/JYRJ201903036_030.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 基于卷积神经网络的手势训练模型的工作流程图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201903036_030.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="31">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201903036_031.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 基于卷积神经网络的手势识别模型的工作流程图" src="Detail/GetImg?filename=images/JYRJ201903036_031.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 基于卷积神经网络的手势识别模型的工作流程图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201903036_031.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="32" name="32"><b>1.2 手势图像的预处理</b></h4>
                <div class="p1">
                    <p id="33">为降低背景对识别结果的干扰, 在进行手势识别之前, 需要将手势从复杂的背景中分割出来。</p>
                </div>
                <div class="p1">
                    <p id="34">分析手势图像, 可以看出, 手势的肤色信息是手势区别于其他物体的重要特征之一。因此, 将肤色的亮度信息和色度信息分离, 进而基于肤色比较不同物体与手势在色度上的差别, 将手势部分分割出来。</p>
                </div>
                <div class="p1">
                    <p id="35">由于单纯的RGB色彩空间难以对手势的肤色信息进行检测, 而YCbCr色彩空间具有亮度和色度分离的特点, Y、Cb、Cr分别代表亮度、蓝色色度和红色色度分量。相比于RGB色彩空间, YCbCr色彩空间更符合人眼的视觉感知, 从而降低了图像分析的难度<citation id="80" type="reference"><link href="13" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="36">摄像头采集到的是RGB图像, 因此需要转换到YCbCr颜色空间, 色彩空间转换可以按下式计算:</p>
                </div>
                <div class="p1">
                    <p id="37"><i>Y</i>=0.257×<i>R</i>+0.564×<i>G</i>+0.098×<i>B</i>+1      (1) </p>
                </div>
                <div class="p1">
                    <p id="38"><i>Cb</i>=-0.148×<i>R</i>-0.291×<i>G</i>+0.439×<i>B</i>+12      (2) </p>
                </div>
                <div class="p1">
                    <p id="39"><i>Cr</i>=0.439×<i>R</i>-0.368×<i>G</i>-0.071×<i>B</i>+128      (3) </p>
                </div>
                <div class="p1">
                    <p id="40">采用基于阈值的分割方法, 将手势从图像背景中分离出来之后, 以二值图像的形式表现。如果Cb的值范围在77到127之间并且Cr的值范围在133到173之间则被认为是白色像素, 否则被认为是黑色像素。基于肤色阈值的手势分割结果如图3所示。</p>
                </div>
                <div class="area_img" id="41">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201903036_04100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 基于肤色阈值的手势区域分割" src="Detail/GetImg?filename=images/JYRJ201903036_04100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 基于肤色阈值的手势区域分割  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201903036_04100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="42" name="42"><b>1.3 数据集介绍</b></h4>
                <div class="p1">
                    <p id="43">为了验证算法有效性, 本文采用两个手势数据集。分别是自建手势数据集和gesture数据集。自建手势数据集是通过摄像头拍摄采集了真人的9种不同手势, 共有1 800张图像, 图像包含了复杂的背景信息。如果直接用自建手势数据集训练CNN网络, 必定影响识别率。因而需要对图像作预处理, 分割出图像中的手势区域作为CNN网络的输入, 但是如果CNN网络模型训练过程需要对每张训练图像作预处理, 会加大模型训练时间, 因此, 自建数据集适合测试CNN网络。本文选用gesture数据集作为训练数据集, gesture数据集提供了16 989张9个类别的手势灰度图像, 每张图大小是64×64像素, 如图4所示。模型训练前需要对数据集作预处理, 将图像大小调整为28×28像素, 并作归一化处理。图像的尺寸变小并且是灰度图, 加快了模型训练的速度。</p>
                </div>
                <div class="area_img" id="44">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201903036_044.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 gesture数据集" src="Detail/GetImg?filename=images/JYRJ201903036_044.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 gesture数据集  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201903036_044.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="45" name="45"><b>1.4 手势识别的卷积神经网络的模型结构</b></h4>
                <div class="p1">
                    <p id="46">卷积神经网络通过局部感受野和权值共享减少了神经网络参数的个数, 因而大大降低网络模型的复杂度和缓解网络模型过拟合的问题<citation id="81" type="reference"><link href="15" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>。卷积神经网络主要由卷积层、池化层和全连接层构成。本文通过构建基于手势灰度图像的卷积神经网络, 自动提取输入样本的手势特征和识别。网络模型由两个卷积层、两个池化层、两个全连接层组成。模型结构如图5所示。</p>
                </div>
                <div class="area_img" id="47">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201903036_047.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 手势识别的卷积神经网络的模型结构" src="Detail/GetImg?filename=images/JYRJ201903036_047.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 手势识别的卷积神经网络的模型结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201903036_047.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="48" name="48"><b>1.4.1</b> 卷积和池化</h4>
                <div class="p1">
                    <p id="49">卷积层中卷积核的作用是提取图像的特征信息, 对于卷积层的某一层, 用卷积核与上一层特征图作滑动卷积运算再加上一个偏置值得到输出。最后再通过激活函数就能得到卷积的结果, 即输出特征图<citation id="82" type="reference"><link href="15" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>, 如下所示:</p>
                </div>
                <div class="p1">
                    <p id="50"><mathml id="51"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>x</mi><msubsup><mrow></mrow><mi>j</mi><mi>l</mi></msubsup><mo>=</mo><mi>f</mi><mrow><mo> (</mo><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>i</mi><mo>∈</mo><mi>Μ</mi><msub><mrow></mrow><mi>j</mi></msub></mrow></munder><mi>x</mi></mstyle><msubsup><mrow></mrow><mi>j</mi><mrow><mi>l</mi><mo>-</mo><mn>1</mn></mrow></msubsup><mo>×</mo><mi>k</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mi>l</mi></msubsup><mo>+</mo><mi>b</mi><msubsup><mrow></mrow><mi>j</mi><mi>l</mi></msubsup></mrow><mo>) </mo></mrow></mrow></math></mathml>      (4) </p>
                </div>
                <div class="p1">
                    <p id="52">式中:<i>x</i><mathml id="53"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mi>l</mi></msubsup></mrow></math></mathml>为第<i>l</i>层的第<i>j</i>个特征图;<i>M</i><sub><i>j</i></sub>表示所有输入特征图;<i>k</i><mathml id="54"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow><mi>l</mi></msubsup></mrow></math></mathml>和<i>b</i><mathml id="55"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mi>l</mi></msubsup></mrow></math></mathml>代表第<i>l</i>层中的卷积核和偏置;<i>f</i> (·) 代表激活函数, 通常采用Relu函数或Sigmoid函数。</p>
                </div>
                <div class="p1">
                    <p id="56">池化层把输入的特征图分成多个不重叠的<i>n</i>×<i>n</i>块, 再对每个块采用最大值法或者均值法, 从而使得输出特征图缩小了<i>n</i>倍<citation id="83" type="reference"><link href="15" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="57">本文提出的模型中第一个卷积层用32个大小是3×3的卷积核, 滑动步长为1, 输出32个28×28的特征图。第一个池化层的池化采样的窗口大小为是2×2, 步长为2, 得到32个14×14的特征图。</p>
                </div>
                <div class="p1">
                    <p id="58">第二个卷积层用64个大小是3×3的卷积核, 滑动步长为1, 输出64个14×14的特征图。第二个池化层的池化采样的窗口大小为是2×2, 步长为2, 得到64个7×7的特征图。</p>
                </div>
                <div class="p1">
                    <p id="59">网络的激活函数采用Relu () 函数, 因为Relu () 函数能使部分神经元的输出为0, 从而使网络产生稀疏性, 减少网络的复杂度, 缓解了过拟合问题的发生。</p>
                </div>
                <h4 class="anchor-tag" id="60" name="60"><b>1.4.2</b> 全连接层</h4>
                <div class="p1">
                    <p id="61">本文提出的模型中第一个全连接层的输入为64张7×7的特征图, 因此, 需要1 024个神经元。为了防止卷积神经网络出现过拟合问题, 在第一个全连接层之后加入了Dropout层。Dropout算法是在训练过程中随机地按一定比例<i>p</i> (0≤<i>p</i>≤1) 的忽略节点, 阻止神经元过多, 由此减轻了传统全连接神经网络的过拟合问题, 有效地提高了网络的泛化性能<citation id="84" type="reference"><link href="17" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="62">在第二个全连接层的输出通过Softmax激活函数, 让识别结果以概率的形式输出。</p>
                </div>
                <h3 id="63" name="63" class="anchor-tag"><b>2 网络模型训练及测试结果</b></h3>
                <div class="p1">
                    <p id="64">用gesture数据集训练网络模型, 再用自建手势数据集测试模型。训练网络模型采用批量随机梯度下降法, 即从训练总样本中选取一个批次作为一个batch, 然后计算这个batch的总误差, 根据总误差来更新权值。使用批量随机梯度下降法的好处是网络的收敛速度更快。本文训练数据集每个批次batch的样本个数设置为2 000个。使用Adam优化器进行迭代, 学习率为1e-4, dropout设置0.75。</p>
                </div>
                <div class="p1">
                    <p id="65">将gesture数据集的16 989张分成两部分:13 989张图像作为训练集, 3 000张图像作为验证集。网络模型的构建和训练使用深度学习框架TensorFlow, 利用TensorBoard得到网络训练过程中交叉熵的变化率和准确率, 分别如图6和图7所示。</p>
                </div>
                <div class="area_img" id="66">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201903036_066.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 训练过程中迭代次数与交叉熵的变化曲线" src="Detail/GetImg?filename=images/JYRJ201903036_066.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 训练过程中迭代次数与交叉熵的变化曲线  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201903036_066.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="67">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201903036_067.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 训练过程中迭代次数与准确率的变化曲线" src="Detail/GetImg?filename=images/JYRJ201903036_067.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 训练过程中迭代次数与准确率的变化曲线  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201903036_067.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="68">将训练好的网络模型在自建的手势数据集上进行测试, 得到识别准确率为96%。</p>
                </div>
                <div class="p1">
                    <p id="69">现有的参考文献已提出了不同的方法来识别手势, 表1是从实现算法、识别的手势数量以及识别率三方面将本文提出的算法与文献提出的三种算法进行性能对比结果。其中, 文献<citation id="85" type="reference">[<a class="sup">2</a>]</citation>使用HMM+SVM方法对手势进行识别, 该方法先人工提取图像的特征, 再用SVM对手势肌电信号进行识别。虽然支持向量机具有训练速度快、非线性映射能力强等特点, 但其准确率低于卷积神经网络。文献<citation id="86" type="reference">[<a class="sup">9</a>]</citation>提出用一个三层径向基神经网络RBF (Radial Basis Function) 对24种美国手语手势进行识别。RBF是一个三层神经网络, 一个输出层, 一个隐藏层, 一个输出层, 属于浅层神经网络。与该网络相比, 较大深度的卷积神经网络具有更高的识别率。文献<citation id="88" type="reference">[<a class="sup">2</a>,<a class="sup">9</a>]</citation>都属于传统机器学习方法, 卷积神经网络CNN与其相比, 训练速度较慢, 但识别率更高, 并且具有更好的鲁棒性。文献<citation id="87" type="reference">[<a class="sup">5</a>]</citation>采用卷积神经网络CNN算法, 在对手势进行训练之前对图像进行预处理, 分割出手势区域作为网络输入, 该输入图像是一个64×64的三通道RGB图像, 而本文输入图像预处理成28×28的单通道灰度图像, 与其相比, 本文网络模型的参数减少, 降低了算法复杂度。</p>
                </div>
                <div class="area_img" id="70">
                    <p class="img_tit"><b>表1 各种算法结果对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="70" border="1"><tr><td>文献</td><td>算法</td><td>手势种类</td><td>识别率</td></tr><tr><td><br />[2]</td><td>HMM/SVM</td><td>6</td><td>92%</td></tr><tr><td><br />[9]</td><td>RBF</td><td>6</td><td>88%</td></tr><tr><td><br />[5]</td><td>预处理+CNN</td><td>24</td><td>96%</td></tr><tr><td><br />本文</td><td>预处理+CNN</td><td>9</td><td>96%</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h3 id="71" name="71" class="anchor-tag"><b>3 结 语</b></h3>
                <div class="p1">
                    <p id="72">手势图像中手势的多样性和复杂的背景信息会导致识别准确率低下。本文提出了一种基于肤色阈值的手势分割结合灰度卷积神经网络的手势识别模型。首先, 构建灰度手势图像的卷积神经网络模型, 采用gesture数据集对模型训练, 有效地减少模型复杂度, 降低模型训练时间。再用摄像头采集的不同背景下的手势图像进行测试, 需要先对手势图像在YCbCr颜色空间上用阈值法分割出手势区域并作掩膜处理。实验结果表明, 卷积神经网络能够高效地进行特征提取, 在两个数据集下对手势的平均识别率都达到96%以上, 因此能有效地解决手势识别问题。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=HLDX201802018&amp;v=MDQ0NzA1NE8zenFxQnRHRnJDVVI3cWZadVpzRmlEbFY3N0xMU0hQZHJHNEg5bk1yWTlFYklRS0RIODR2UjRUNmo=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> 朱雯文, 叶西宁. 基于卷积神经网络的手势识别算法[J]. 华东理工大学学报 (自然科学版) , 2018, 44 (2) : 260-269.
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Hybrid EMG classifier based on HMM and SVM for hand gesture recognition in prosthetics">

                                <b>[2]</b> Rossi M, Benatti S, Farella E, et al. Hybrid EMG classifier based on HMM and SVM for hand gesture recognition in prosthetics[C]//IEEE International Conference on Industrial Technology.IEEE, 2015:1700-1705.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A HOG-based hand gesture recognition system on a mobile device">

                                <b>[3]</b> Prasuhn L, Oyamada Y, Mochizuki Y, et al. A HOG-based hand gesture recognition system on a mobile device[C]//IEEE International Conference on Image Processing. IEEE, 2015:3973-3977.
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Mexican Sign Language Alphanumerical Gestures Recognition using 3D Haar-like Features &amp;quot;">

                                <b>[4]</b> Jimenez J, Martin A, Uc V, et al. Mexican sign language alphanumerical gestures recognition using 3D haar-like features[J].IEEE Latin America Transactions, 2017, 15 (10) :2000-2005.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Application of convolutional neural networks for static hand gestures recognition under different invariant features &amp;quot;">

                                <b>[5]</b> Flores C J L, Cutipa A E G, Enciso R L. Application of convolutional neural networks for static hand gestures recognition under different invariant features[C]//2017 IEEE XXIV International Conference on Electronics, Electrical Engineering and Computing (INTERCON) , Cusco, Peru, 2017.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES6E57A9585CF23521D1D023FF0FC08B7E&amp;v=MzE1MzNmT0dRbGZCckxVMDV0cGd6TGk3d2FzPU5pZk9mYlhORzlhOXBvcE5ZWmg1RG44OHpSZG42MHQ5U255VTJoSkRDcktjTjczcUNPTnZGU2lXV3I3SklGcG1hQnVIWQ==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b> Shaik K B, Ganesan P, Kalist V, et al. Comparative study of skin color detection and segmentation in HSV and YCbCr color space[J]. Procedia Computer Science, 2015, 57: 41-48.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Pest identification via deep residual learning in complex background">

                                <b>[7]</b> Cheng X, Zhang Y H, Chen Y Q, et al. Pest identification via deep residual learning in complex background[J]. Computers and Electronics in Agriculture, 2017, 141:351-356.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201801036&amp;v=MDI3NzFNcm85R1lvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVac0ZpRGxWNzdMTHpUWlpMRzRIOW4=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b> 顾郑平, 朱敏. 基于深度学习的鱼类分类算法研究[J]. 计算机应用与软件, 2018, 35 (1) :200-205.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=FPGA implementation of hand gesture recognition system using neural networks">

                                <b>[9]</b> Sridevi K, Sundarambal M, Muralidharan K, et al. FPGA implementation of hand gesture recognition system using neural networks[C]//International Conference on Intelligent Systems and Control. IEEE, 2017:34-39.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JYRJ201903036" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201903036&amp;v=MjAyOTJySTlHWW9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpzRmlEbFY3N0xMelRaWkxHNEg5ak0=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="1" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
