<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637135753883877500%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJYRJ201904030%26RESULT%3d1%26SIGN%3df4B5AoatA8d5IO4vnRBNDcI4vGs%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JYRJ201904030&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JYRJ201904030&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201904030&amp;v=MjQ5NDFyQ1VSN3FmWnVadEZ5em1VYjdOTHpUWlpMRzRIOWpNcTQ5R1pJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0Y=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#49" data-title="&lt;b&gt;0 引 言&lt;/b&gt; "><b>0 引 言</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#55" data-title="&lt;b&gt;1 相关工作&lt;/b&gt; "><b>1 相关工作</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#56" data-title=" (1) 肢体检测框架"> (1) 肢体检测框架</a></li>
                                                <li><a href="#59" data-title=" (2) 两步框架"> (2) 两步框架</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#69" data-title="&lt;b&gt;2 算法设计&lt;/b&gt; "><b>2 算法设计</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#72" data-title="&lt;b&gt;2.1 人体目标检测器&lt;/b&gt;"><b>2.1 人体目标检测器</b></a></li>
                                                <li><a href="#77" data-title="&lt;b&gt;2.2 基于强化学习的目标精细模型&lt;/b&gt;"><b>2.2 基于强化学习的目标精细模型</b></a></li>
                                                <li><a href="#108" data-title="&lt;b&gt;2.3 姿态检测器&lt;/b&gt;"><b>2.3 姿态检测器</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#111" data-title="&lt;b&gt;3 实 验&lt;/b&gt; "><b>3 实 验</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#113" data-title="&lt;b&gt;3.1 数据集&lt;/b&gt;"><b>3.1 数据集</b></a></li>
                                                <li><a href="#116" data-title="&lt;b&gt;3.2 实验结果&lt;/b&gt;"><b>3.2 实验结果</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#123" data-title="&lt;b&gt;4 结 语&lt;/b&gt; "><b>4 结 语</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#66" data-title="图1 不同包围框精度情况下的检测效果">图1 不同包围框精度情况下的检测效果</a></li>
                                                <li><a href="#68" data-title="图2 通过调整包围框实现精度的提升">图2 通过调整包围框实现精度的提升</a></li>
                                                <li><a href="#71" data-title="图3 本文算法的流程图">图3 本文算法的流程图</a></li>
                                                <li><a href="#101" data-title="图4 深度强化学习网络的结构">图4 深度强化学习网络的结构</a></li>
                                                <li><a href="#118" data-title="&lt;b&gt;表1 在MPII数据集上的验证结果 mAP&lt;/b&gt;"><b>表1 在MPII数据集上的验证结果 mAP</b></a></li>
                                                <li><a href="#119" data-title="图5 效果演示图一">图5 效果演示图一</a></li>
                                                <li><a href="#122" data-title="图6 效果演示图二">图6 效果演示图二</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="153">


                                    <a id="bibliography_1" title=" Newell A, Yang K, Deng J. Stacked Hourglass Networks for Human Pose Estimation[C]//European Conference on Computer Vision, 2016:483-499." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Stacked Hourglass Networks for Human Pose Estimation">
                                        <b>[1]</b>
                                         Newell A, Yang K, Deng J. Stacked Hourglass Networks for Human Pose Estimation[C]//European Conference on Computer Vision, 2016:483-499.
                                    </a>
                                </li>
                                <li id="155">


                                    <a id="bibliography_2" title=" Jain A. Articulated people detection and pose estimation: Reshaping the future[C]//Computer Vision and Pattern Recognition. IEEE, 2012:3178-3185." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Articulated people detection and pose estimation: Reshaping the future">
                                        <b>[2]</b>
                                         Jain A. Articulated people detection and pose estimation: Reshaping the future[C]//Computer Vision and Pattern Recognition. IEEE, 2012:3178-3185.
                                    </a>
                                </li>
                                <li id="157">


                                    <a id="bibliography_3" title=" Gkioxari G, Hariharan B, Girshick R, et al. Using k-Poselets for Detecting People and Localizing Their Keypoints[C]//IEEE Conference on Computer Vision and Pattern Recognition. IEEE Computer Society, 2014:3582-3589." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Using k-poselets for detecting people and localizing their keypoints">
                                        <b>[3]</b>
                                         Gkioxari G, Hariharan B, Girshick R, et al. Using k-Poselets for Detecting People and Localizing Their Keypoints[C]//IEEE Conference on Computer Vision and Pattern Recognition. IEEE Computer Society, 2014:3582-3589.
                                    </a>
                                </li>
                                <li id="159">


                                    <a id="bibliography_4" title=" Chen X, Yuille A. Parsing occluded people by flexible compositions[C]//Computer Vision and Pattern Recognition. IEEE, 2015:3945-3954." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Parsing occluded people by flexible compositions">
                                        <b>[4]</b>
                                         Chen X, Yuille A. Parsing occluded people by flexible compositions[C]//Computer Vision and Pattern Recognition. IEEE, 2015:3945-3954.
                                    </a>
                                </li>
                                <li id="161">


                                    <a id="bibliography_5" title=" Pishchulin L, Insafutdinov E, Tang S, et al. DeepCut: Joint Subset Partition and Labeling for Multi Person Pose Estimation[J]. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) .IEEE, 2016:4929-4937." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=DeepCut:Joint Subset Partition and Labeling for Multi Person Pose Estimation">
                                        <b>[5]</b>
                                         Pishchulin L, Insafutdinov E, Tang S, et al. DeepCut: Joint Subset Partition and Labeling for Multi Person Pose Estimation[J]. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) .IEEE, 2016:4929-4937.
                                    </a>
                                </li>
                                <li id="163">


                                    <a id="bibliography_6" title=" Insafutdinov E, Pishchulin L, Andres B, et al. DeeperCut: A Deeper, Stronger, and Faster Multi-person Pose Estimation Model[C]//European Conference on Computer Vision. Springer International Publishing, 2016:34-50." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=DeeperCut:A Deeper, Stronger, and Faster Multi-person Pose Estimation Model">
                                        <b>[6]</b>
                                         Insafutdinov E, Pishchulin L, Andres B, et al. DeeperCut: A Deeper, Stronger, and Faster Multi-person Pose Estimation Model[C]//European Conference on Computer Vision. Springer International Publishing, 2016:34-50.
                                    </a>
                                </li>
                                <li id="165">


                                    <a id="bibliography_7" >
                                        <b>[7]</b>
                                     Liu W, Anguelov D, Erhan D, et al. SSD: Single Shot MultiBox Detector[C]//European Conference on Computer Vision. Springer International Publishing, 2016:21-37.</a>
                                </li>
                                <li id="167">


                                    <a id="bibliography_8" title=" Sapp B, Toshev A, Taskar B. Cascaded models for articulated pose estimation[C]//European Conference on Computer Vision. Springer-Verlag, 2010:406-420." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Cascaded models for articulated pose es-timation">
                                        <b>[8]</b>
                                         Sapp B, Toshev A, Taskar B. Cascaded models for articulated pose estimation[C]//European Conference on Computer Vision. Springer-Verlag, 2010:406-420.
                                    </a>
                                </li>
                                <li id="169">


                                    <a id="bibliography_9" title=" Dantone M, Gall J, Leistner C, et al. Human Pose Estimation Using Body Parts Dependent Joint Regressors[C]//IEEE Conference on Computer Vision and Pattern Recognition. IEEE, 2013:3041-3048." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Human pose estima-tion using body parts dependent joint regressors">
                                        <b>[9]</b>
                                         Dantone M, Gall J, Leistner C, et al. Human Pose Estimation Using Body Parts Dependent Joint Regressors[C]//IEEE Conference on Computer Vision and Pattern Recognition. IEEE, 2013:3041-3048.
                                    </a>
                                </li>
                                <li id="171">


                                    <a id="bibliography_10" title=" Savarese S, Lee H, Telaprolu M, et al. An efficient branch-and-bound algorithm for optimal human pose estimation[C]//Computer Vision and Pattern Recognition.IEEE, 2012:1616-1623." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=An efficient branch-and-bound algorithm for optimal human pose estimation">
                                        <b>[10]</b>
                                         Savarese S, Lee H, Telaprolu M, et al. An efficient branch-and-bound algorithm for optimal human pose estimation[C]//Computer Vision and Pattern Recognition.IEEE, 2012:1616-1623.
                                    </a>
                                </li>
                                <li id="173">


                                    <a id="bibliography_11" title=" Kiefel M, Gehler P V. Human Pose Estimation with Fields of Parts[M]//Computer Vision—ECCV 2014. Springer International Publishing, 2014:331-346." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Human Pose Estimation with Fields of Parts">
                                        <b>[11]</b>
                                         Kiefel M, Gehler P V. Human Pose Estimation with Fields of Parts[M]//Computer Vision—ECCV 2014. Springer International Publishing, 2014:331-346.
                                    </a>
                                </li>
                                <li id="175">


                                    <a id="bibliography_12" title=" Hara K, Chellappa R. Computationally Efficient Regression on a Dependency Graph for Human Pose Estimation[C]//Computer Vision and Pattern Recognition.IEEE, 2013:3390-3397." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Computationally efficient regression on a dependency graph for human poseestimation">
                                        <b>[12]</b>
                                         Hara K, Chellappa R. Computationally Efficient Regression on a Dependency Graph for Human Pose Estimation[C]//Computer Vision and Pattern Recognition.IEEE, 2013:3390-3397.
                                    </a>
                                </li>
                                <li id="177">


                                    <a id="bibliography_13" title=" Toshev A, Szegedy C. DeepPose: Human Pose Estimation via Deep Neural Networks[C]//Computer Vision and Pattern Recognition. IEEE, 2014:1653-1660." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deeppose:Human pose estimation via deep neural networks">
                                        <b>[13]</b>
                                         Toshev A, Szegedy C. DeepPose: Human Pose Estimation via Deep Neural Networks[C]//Computer Vision and Pattern Recognition. IEEE, 2014:1653-1660.
                                    </a>
                                </li>
                                <li id="179">


                                    <a id="bibliography_14" title=" Ouyang W, Chu X, Wang X. Multi-source Deep Learning for Human Pose Estimation[C]//Computer Vision and Pattern Recognition. IEEE, 2014:2337-2344." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multi-source deep learning for human pose estimation">
                                        <b>[14]</b>
                                         Ouyang W, Chu X, Wang X. Multi-source Deep Learning for Human Pose Estimation[C]//Computer Vision and Pattern Recognition. IEEE, 2014:2337-2344.
                                    </a>
                                </li>
                                <li id="181">


                                    <a id="bibliography_15" title=" He K, Zhang X, Ren S, et al. Deep Residual Learning for Image Recognition[C]//2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) .IEEE, 2015:770-778." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep residual learning for image recognition">
                                        <b>[15]</b>
                                         He K, Zhang X, Ren S, et al. Deep Residual Learning for Image Recognition[C]//2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) .IEEE, 2015:770-778.
                                    </a>
                                </li>
                                <li id="183">


                                    <a id="bibliography_16" title=" Ren S, He K, Girshick R, et al. Faster R-CNN: towards real-time object detection with region proposal networks[C]//International Conference on Neural Information Processing Systems. MIT Press, 2015:91-99." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Faster R-CNN:towards real-time object detection with region proposal networks">
                                        <b>[16]</b>
                                         Ren S, He K, Girshick R, et al. Faster R-CNN: towards real-time object detection with region proposal networks[C]//International Conference on Neural Information Processing Systems. MIT Press, 2015:91-99.
                                    </a>
                                </li>
                                <li id="185">


                                    <a id="bibliography_17" title=" Caicedo J C, Lazebnik S. Active Object Localization with Deep Reinforcement Learning[C]//Proceedings of the 2015 IEEE International Conference on Computer Vision (ICCV) . IEEE Computer Society, 2015: 2488-2496." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Active object localization wife deep reinforcement learn-ing">
                                        <b>[17]</b>
                                         Caicedo J C, Lazebnik S. Active Object Localization with Deep Reinforcement Learning[C]//Proceedings of the 2015 IEEE International Conference on Computer Vision (ICCV) . IEEE Computer Society, 2015: 2488-2496.
                                    </a>
                                </li>
                                <li id="187">


                                    <a id="bibliography_18" title=" Bellver M, Giro-I-Nieto X, Marques F, et al. Hierarchical Object Detection with Deep Reinforcement Learning[J]. Advances in Parallel Computing, 2016, 31." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Hierarchical Object Detection with Deep Reinforcement Learning">
                                        <b>[18]</b>
                                         Bellver M, Giro-I-Nieto X, Marques F, et al. Hierarchical Object Detection with Deep Reinforcement Learning[J]. Advances in Parallel Computing, 2016, 31.
                                    </a>
                                </li>
                                <li id="189">


                                    <a id="bibliography_19" title=" Mnih V, Kavukcuoglu K, Silver D, et al. Human-level control through deep reinforcement learning.[J]. Nature, 2015, 518 (7540) :529." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Human-Level control through deep reinforcement learning">
                                        <b>[19]</b>
                                         Mnih V, Kavukcuoglu K, Silver D, et al. Human-level control through deep reinforcement learning.[J]. Nature, 2015, 518 (7540) :529.
                                    </a>
                                </li>
                                <li id="191">


                                    <a id="bibliography_20" >
                                        <b>[20]</b>
                                     Huang G, Liu Z, Laurens V D M, et al. Densely Connected Convolutional Networks[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2017) .IEEE, 2016:2261-2269.</a>
                                </li>
                                <li id="193">


                                    <a id="bibliography_21" title=" Simonyan K, Zisserman A. Very deep convolutional networks for large-scale image recognition[EB]. arXiv preprint arXiv:1409.1556, 2014." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Very deep convolutional networks for large-scale image recognition[EB]">
                                        <b>[21]</b>
                                         Simonyan K, Zisserman A. Very deep convolutional networks for large-scale image recognition[EB]. arXiv preprint arXiv:1409.1556, 2014.
                                    </a>
                                </li>
                                <li id="195">


                                    <a id="bibliography_22" title=" Iqbal U, Gall J. Multi-Person Pose Estimation with Local Joint-to-Person Associations[C]//European Conference on Computer Vision, ECCV 2016 Workshops, 2016:627-642." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multi-person Pose Estimation with Local Joint-to-Person Associations">
                                        <b>[22]</b>
                                         Iqbal U, Gall J. Multi-Person Pose Estimation with Local Joint-to-Person Associations[C]//European Conference on Computer Vision, ECCV 2016 Workshops, 2016:627-642.
                                    </a>
                                </li>
                                <li id="197">


                                    <a id="bibliography_23" title=" Levinkov E, Uhrig J, Tang S, et al. Joint graph decomposition &amp;amp; node labeling:Problem, algorithms, applications[C]//IEEE Conference on Computer Vision and Pattern Recognition (CVPR) . IEEE, 2017." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Joint graph decomposition &amp;amp; node labeling Problem,algorithms,applications">
                                        <b>[23]</b>
                                         Levinkov E, Uhrig J, Tang S, et al. Joint graph decomposition &amp;amp; node labeling:Problem, algorithms, applications[C]//IEEE Conference on Computer Vision and Pattern Recognition (CVPR) . IEEE, 2017.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JYRJ" target="_blank">计算机应用与软件</a>
                2019,36(04),186-191 DOI:10.3969/j.issn.1000-386x.2019.04.029            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于强化学习的多人姿态检测算法优化</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%BB%84%E9%93%8E&amp;code=39247308&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">黄铎</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BA%94%E5%A8%9C&amp;code=11595470&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">应娜</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%94%A1%E5%93%B2%E6%A0%8B&amp;code=41206673&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">蔡哲栋</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%9D%AD%E5%B7%9E%E7%94%B5%E5%AD%90%E7%A7%91%E6%8A%80%E5%A4%A7%E5%AD%A6%E9%80%9A%E4%BF%A1%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0073968&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">杭州电子科技大学通信工程学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>在多目标人体姿态检测算法过程中, 人体的定位精度依然不够精确。针对该问题, 采用速度与精度兼顾的SSD算法作为目标检测器获得人体的初步包围框, 定义该包围框为智能体, 引入强化学习。采用马尔科夫决策过程以及Q网络组成的目标精细模型对智能体训练其九种动作, 分别为左上角与右下角两个点的四方向进行迭代调整以及终止策略, 使得包围框达到更贴近人体的效果。结合先进的Stacked hourglass算法作为姿态检测器, 对调整后的包围框进行姿态预测。该算法的引入使得多目标人体检测算法在MPII数据集上的精度提升了1.6 mAP, 达到了73.7 mAP。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">强化学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%A7%BF%E6%80%81%E6%A3%80%E6%B5%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">姿态检测;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">模式识别;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    黄铎, 硕士生, 主研领域:图像处理。;
                                </span>
                                <span>
                                    应娜, 副教授。;
                                </span>
                                <span>
                                    蔡哲栋, 硕士生。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-10-16</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金项目 (61705055);</span>
                                <span>浙江省自然科学基金项目 (LY16F010013);</span>
                    </p>
            </div>
                    <h1><b>OPTIMIZATION OF ESTIMATION ALGORITHM FOR THE MULTI-PERSON POSE BASED ON REINFORCEMENT LEARNING</b></h1>
                    <h2>
                    <span>Huang Duo</span>
                    <span>Ying Na</span>
                    <span>Cai Zhedong</span>
            </h2>
                    <h2>
                    <span>School of Communication Engineering, Hangzhou Dianzi University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>The accuracy of locating people is imprecise in multi-objective human pose estimation. Aiming at this problem, we applied SSD with both speed and accuracy as an object detector to obtain the preliminary bounding box of human body. The algorithm defined the bounding box as an agent and introduced the reinforcement learning. Markov decision-making process and the objective fine model consisting of Q net were applied to train the agent for nine actions. The training actions included iteration adjustments and termination policy directing to two points from the top left corner and points from the bottom right corner respectively, which made the bounding box closer to the human body. The advanced Stacked hourglass algorithm was implemented as pose detector to predict the adjusted bounding box. The introduction of the referred algorithm improves the accuracy of multi-objective human pose estimation on MPII dataset by 1.6 mAP and reaches 73.7 mAP.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Deep%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Deep learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Reinforcement%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Reinforcement learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Pose%20estimation&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Pose estimation;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Pattern%20recognition&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Pattern recognition;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-10-16</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="49" name="49" class="anchor-tag"><b>0 引 言</b></h3>
                <div class="p1">
                    <p id="50">人体姿态检测问题是基本的计算机视觉问题, 目前已经有很多算法对这一问题进行了研究, 并且取得了不错的成效。</p>
                </div>
                <div class="p1">
                    <p id="51">在单人姿态检测问题中, 由于人体占据了图片的大部分像素, 所以对于检测器而言不会有许多干扰信息。这在姿态检测问题中是比较简单的一个部分。一些传统的方法采用了图像结构模型。例如, 在人体姿态检测问题中非常有效的树模型<citation id="199" type="reference"><link href="167" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>和随机森林模型<citation id="200" type="reference"><link href="169" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>。在文献<citation id="201" type="reference">[<a class="sup">10</a>]</citation>中广泛研究了一些基于图像的模型, 例如随机场模型<citation id="202" type="reference"><link href="173" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>和图像依赖模型<citation id="203" type="reference"><link href="175" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="52">最近, 深度学习成为了一种在模式识别方面非常有效的方法, 利用深度学习的人体姿态检测同样取得了很好的成绩。一些有代表性的方法例如深度姿态<citation id="204" type="reference"><link href="177" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>, 基于DNN的模型<citation id="205" type="reference"><link href="179" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>和基于CNN的模型<citation id="206" type="reference"><link href="153" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="53">以上所说的这些方法在人体信息足够完整的情况下可以有很好的表现, 但是在多人姿态检测问题中常常不会有如此精确的人体信息。</p>
                </div>
                <div class="p1">
                    <p id="54">而与单人姿态检测问题相比, 多人姿态检测则更具有挑战性 (意为在单张图片中有多个人体) 。针对多人姿态检测, 目前的算法大致分为两种。其一是使用两步检测框架, 其二是采用肢体框架。两步检测框架中的第一步, 是目标检测算法通过包围框的方式定位出人体, 第二步是分别将每个包围框做姿态检测。肢体框架则是独立地检测人体的各个部分, 再把各个部分拼接起来, 以形成多个人体姿态。两种方法各有利弊, 其中两步检测框架的检测精度高度依赖目标检测算法的精度, 包围框对人体的契合程度会影响姿态检测算法的精度;而肢体框架的图片中若多个人体的距离相近, 甚至互相遮挡, 肢体框架的拼接工作就会变得困难。</p>
                </div>
                <h3 id="55" name="55" class="anchor-tag"><b>1 相关工作</b></h3>
                <h4 class="anchor-tag" id="56" name="56"> (1) 肢体检测框架</h4>
                <div class="p1">
                    <p id="57">肢体检测框架有许多代表性的方法。Chen等<citation id="207" type="reference"><link href="159" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>提出了一种通过图像模型来解析被遮挡的人体的方法, 该方法可以将人体的各个部分灵活组合。Gkioxari等<citation id="208" type="reference"><link href="157" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>使用k-poselets来全局检测人和预测人体姿势, 通过加权平均来预测最终的姿态。Pishchulin等<citation id="209" type="reference"><link href="161" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>提出了DeepCut模型, 该模型先检测身体的每个部分, 然后通过整体线性程序来标记并且组装这些部分, 最后达到预测人体姿态的目的。Insafutdinov等<citation id="210" type="reference"><link href="181" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>提出了一个更强的基于ResNet的肢体检测器和一个更好的增量优化策略<citation id="211" type="reference"><link href="163" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="58">虽然基于肢体检测方法有良好的表现, 但是由于该方法只专注于小区域的检测, 缺少全局信息, 所以该方法依旧存在精度上的缺陷。</p>
                </div>
                <h4 class="anchor-tag" id="59" name="59"> (2) 两步框架</h4>
                <div class="p1">
                    <p id="60">在两步框架的研究中, 几乎都由人体检测器和姿态检测器两部分组成。Pishculin等<citation id="212" type="reference"><link href="155" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>采用了传统图像结构模型进行姿态预测。Insafutdinov等<citation id="213" type="reference"><link href="163" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>则采用了相似的两步框架, 由人体检测器Faster R-CNN<citation id="214" type="reference"><link href="183" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>和姿态预测器DeeperCut构成。他们的方法在MPII数据集上最高只能达到51.0 mAP的精度。由于计算机技术不断发展, 硬件水平快速上升的情况下, 物体检测器和单人姿态检测器都达到了一个相当好的水平, 而两步框架得益于两者的先进性, 在多人体检测问题中也取得了非常优异的表现。</p>
                </div>
                <div class="p1">
                    <p id="61">但如上文所说, 两步框架中依旧存在着人体监测框架不精确的问题, 本文致力于解决这个问题, 在两步框架中加入了强化学习的顺序决策过程, 用于调整包围框以提升算法精度。</p>
                </div>
                <div class="p1">
                    <p id="62">强化学习的概念从博弈论衍生而来, 通过不断的试错学习, 从而找到一种最佳的解决问题的方法。强化学习在诸多领域都是非常强有力的工具, 如在雅达利2600游戏机和AlphaGo等项目都有出色应用。</p>
                </div>
                <div class="p1">
                    <p id="63">在目标检测领域, 强化学习也发挥出很强大的作用。Juan等<citation id="215" type="reference"><link href="185" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>设计了一些包围框的动作, 通过对包围框的调整不断接近目标, 最终定位目标。Míriam等<citation id="216" type="reference"><link href="187" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>将图片分为五个区域, 挑选出目标存在的区域后重复这一步骤, 不断聚焦目标, 实现定位功能。</p>
                </div>
                <div class="p1">
                    <p id="64">本文采用的算法基于两步检测框架。如同上文所述, 包围框对人体的贴合程度会影响人体姿态检测的精度, 图1通过SSD300<citation id="217" type="reference"><link href="165" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>和hourglass算法<citation id="218" type="reference"><link href="153" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>的结合说明了这个问题。其中, 图1 (a) 表示在目标检测步骤中被判定为正确的情况 (比如交并比大于0.6) , 在人体姿态检测时依然会出现很严重的错误。这是因为通过目标检测算法得到的包围框虽然检测到了人体, 但是这些信息却不够精确, 导致人体姿态检测算法的精度受到了负面影响。</p>
                </div>
                <div class="area_img" id="66">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201904030_06600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 不同包围框精度情况下的检测效果" src="Detail/GetImg?filename=images/JYRJ201904030_06600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 不同包围框精度情况下的检测效果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201904030_06600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="67">为了解决人体检测的包围框信息不精确问题, 本文算法在两步检测框架之间引入了强化学习, 形成基于强化学习的两步检测框架, 调整包围框使其更贴近人体, 以提升精度。算法定义包围框为智能体, 建立了一个顺序决策过程。智能体通过卷积网络获取图片信息进行包围框的调整, 下一次迭代中智能体会根据调整后的包围框信息决定该次迭代中的动作。算法不断重复这一过程, 最终得到一个最适合人体姿态检测的包围框, 以此来提升精度。其中, 算法采用了马尔科夫决策过程, 设计了8种变形动作以及一种终止动作, 如图2所示, 8种动作分别为左上角点和右下角点的上下左右平移。这8种变形动作涵盖了包围框所有的动作, 以满足贴近人体的需求。算法在MPII数据集上做了训练和验证, 结果达到了74 mAP。</p>
                </div>
                <div class="area_img" id="68">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201904030_068.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 通过调整包围框实现精度的提升" src="Detail/GetImg?filename=images/JYRJ201904030_068.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 通过调整包围框实现精度的提升  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201904030_068.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h3 id="69" name="69" class="anchor-tag"><b>2 算法设计</b></h3>
                <div class="p1">
                    <p id="70">本文的算法如图3所示, 首先通过改进的目标检测器SSD回归出初步的人体包围框, 再通过深度强化学习网络判断包围框是否准确, 并对定位的包围框设计了8种变形操作以及一种终止动作, 即采用马尔可夫策略对其左上角与右下角坐标进行迭代调整, 调整动作集合为两个点的上下左右平移, 从而提升精度。最后, 由人体姿态检测器对调整后的包围框进行姿态检测。</p>
                </div>
                <div class="area_img" id="71">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201904030_071.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 本文算法的流程图" src="Detail/GetImg?filename=images/JYRJ201904030_071.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 本文算法的流程图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201904030_071.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="72" name="72"><b>2.1 人体目标检测器</b></h4>
                <div class="p1">
                    <p id="73">本文采用了兼顾精度与速度的目标检测算法SSD<citation id="219" type="reference"><link href="165" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation> (Single Shot MultiBox Detector) 。该算法采用VGG-16<citation id="220" type="reference"><link href="193" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>作为前置网络, 删除了末尾的全连接层, 并在卷积层后添加置信层和回归层。SSD在网络中选择6种不同尺度的特征图, 每张特征图的每个像素点都预设了一定数量的预设框。特征图经过回归层后会得到与预设框相同数量的包围框, 并且与预设框一一对应。预设框与真实包围框 (label) 进行匹配, IOU大于0.5的定义为正样本, 其余的为负样本。最后将与正样本对应的包围框与真实值进行loss计算。常用的SSD算法输入尺度为300, 但是小目标检测的精度不高。在本文中SSD算法的输入尺度为512, 置信层与回归层都为7层, 相比于300尺度的SSD算法, 更大的输入尺度在小目标检测的精度上有很大的改善。</p>
                </div>
                <div class="p1">
                    <p id="74">本文通过SSD对图像进行特征提取, 并回归得到目标的包围框坐标。</p>
                </div>
                <div class="p1">
                    <p id="75"><i>bbox</i>=<i>Conv</i><sub>SSD</sub> (<i>img</i>)      (1) </p>
                </div>
                <div class="p1">
                    <p id="76">式中:<i>img</i>是输入图片, <i>Conv</i><sub>SSD</sub> (·) 是SSD算法, <i>bbox</i>是回归得到的包围框坐标。</p>
                </div>
                <h4 class="anchor-tag" id="77" name="77"><b>2.2 基于强化学习的目标精细模型</b></h4>
                <div class="p1">
                    <p id="78">针对目标检测器信息不够精确的问题, 算法定义目标检测器所回归出来的包围框为智能体, 该智能体会与环境交互获取信息, 建立马尔科夫决策过程。在每一次迭代中, 智能体需要获取信息来决定一次变形动作, 在下一次的迭代中, 智能体会根据上一次变性之后的信息来决定再下一次迭代的变形动作, 直到确定目标最优或者达到限制的迭代次数为止。</p>
                </div>
                <h4 class="anchor-tag" id="79" name="79"><b>2.2.1</b> 马尔科夫决策过程参数</h4>
                <div class="p1">
                    <p id="80"><b>状态</b> 智能体迭代至当前情况下的信息, 状态是智能体决定动作的依据。</p>
                </div>
                <div class="p1">
                    <p id="81"><b>动作</b> 动作来自于动作空间, 智能体根据当前的状态和之前迭代的历史奖励信息, 选择能达到最大化期望奖励的动作。</p>
                </div>
                <div class="p1">
                    <p id="82"><b>奖励</b> 每次动作执行后, 算法计算出该状态下执行该动作的奖励。奖励代表着智能体是否正朝着最终目标进行动作。</p>
                </div>
                <h4 class="anchor-tag" id="83" name="83"><b>2.2.2 Q</b>学习</h4>
                <div class="p1">
                    <p id="84">智能体奖励的来源, 动作<i>A</i>和状态<i>st</i>, 受到函数<i>Q</i> (<i>st</i>, <i>A</i>) 控制, 该函数可以通过<i>Q</i>学习函数进行估计。智能体会通过函数<i>Q</i> (<i>st</i>, <i>A</i>) 选择可以获得奖励的动作。<i>Q</i>学习函数使用<i>Bellman</i>方程 (式 (2) ) 不断迭代更新动作选择策略, 其中<i>st</i>和<i>A</i>是当前的相对应的动作和状态, <i>R</i>是当前的奖励, <i>max</i><sub><i>a</i>′</sub><i>Q</i> (<i>st</i>′, <i>A</i>′) 表示未来的奖励, <i>γ</i>表示折扣因子。本文采用强化学习对<i>Q</i>网络进行训练, 以使其能近似于<i>Q</i>函数<citation id="221" type="reference"><link href="189" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="85"><i>Q</i> (<i>st</i>, <i>A</i>) =<i>R</i>+<i>γmax</i><sub><i>a</i>′</sub><i>Q</i> (<i>st</i>′, <i>A</i>′)      (2) </p>
                </div>
                <h4 class="anchor-tag" id="86" name="86"><b>2.2.3</b> 算法模型</h4>
                <div class="p1">
                    <p id="87">当前智能体的信息由通过不同尺度的卷积层对<i>bbox</i>抽取的特征所组成, 如式 (3) 所示。算法的状态<i>st</i>是由当前智能体的信息<i>ft</i>和历史动作<i>hv</i>所组成, 如式 (4) 所示, 其中函数<i>Cat</i> (·) 用于将<i>ft</i>和<i>hv</i>进行拼接。历史动作是一个向量, 向量中包含了前4次迭代中智能体发生形变而所选择的动作, 并入当前智能体的信息将有助于在训练的过程中稳定调整轨迹。算法将前4次迭代中的动作组成一个向量编入状态中, 每次迭代会有9种不同的动作提供选择, 所以一个历史动作向量是36维的。这种类型的向量也被文献<citation id="222" type="reference">[<a class="sup">17</a>]</citation>所采用。</p>
                </div>
                <div class="p1">
                    <p id="88"><i>ft</i>=<i>Conv</i> (<i>bbox</i>)      (3) </p>
                </div>
                <div class="p1">
                    <p id="89"><i>st</i>=<i>Cat</i> (<i>ft</i>, <i>hv</i>)      (4) </p>
                </div>
                <div class="p1">
                    <p id="90">智能体通过由卷积神经网络提取得到的特征进行决策, 选择当前状态下所应该选择的动作, 如式 (5) 所示, 式中<i>QNet</i>表示深度<i>Q</i>网络。算法设计了两种类型的动作:一是调整动作<i>a</i> (·) , 该类型的动作会调整包围框的形状 (式 (6) ) ;二是终止动作<i>t</i>, 该类型的动作一旦被选择, 调整过程即终止。其中的调整动作数量有8种, 分别是包围框左上角坐标的四个方向平移, 和包围框右下角坐标的四个方向平移。这样设计的理由是这8种动作涵盖包围框的所有动作可能, 相比于一般的包围框缩放和平移的规则动作, 这样设计可以使得包围框做出不规则的动作, 更有利于使包围框贴近人体。在迭代过程中智能体会不断根据当前的状态选择动作, 每次调整包围框后会获得新的状态, 再选择新的动作, 直到选择为终止动作为止。</p>
                </div>
                <div class="p1">
                    <p id="91"><i>a</i>=<i>QNet</i> (<i>st</i>)      (5) </p>
                </div>
                <div class="p1">
                    <p id="92"><i>bbox</i>′=<i>a</i> (<i>bbox</i>)      (6) </p>
                </div>
                <div class="p1">
                    <p id="93">算法选择由文献<citation id="223" type="reference">[<a class="sup">17</a>]</citation>中所提出的奖励公式作为该模型的奖励公式。调整动作的和终止动作奖励公式分别由式 (9) 和式 (10) 给出。因为智能体所选择的动作会产生新的包围框, 人体姿态检测器<i>Conv</i><sub>HAD</sub> (·) 会根据新的包围框产生新的精度<i>acc</i><sub>1</sub> (式 (7) ) ;算法定义不加入强化学习的两步框架的精度<i>acc</i><sub>0</sub>作为真实值 (式 (8) ) 。对于当前的状态<i>st</i>, 智能体选择的调整动作得到新状态<i>st</i>′, 产生的新精度<i>acc</i><sub>1</sub>, 如果大于真实值<i>acc</i><sub>0</sub>, 则会获得一个奖励 (1) , 反之则会获得一个惩罚 (-1) 。对于终止动作而言, 终止时最终的新精度<i>acc</i><sub>1</sub>若大于<i>acc</i><sub>0</sub>, 会获得一个比较大的奖励, 反之会获得一个大惩罚。而对于真实值大于<i>τ</i>的目标, 算法选择直接让智能体选择终止动作, 获得奖励, 这样做可以将训练时间缩短, 只调整精度较差的目标。考虑原始精度, 选择<i>τ</i>=0.5。经过调参, 在式 (10) 中<i>η</i>=3。</p>
                </div>
                <div class="p1">
                    <p id="94"><i>acc</i><sub>1</sub>=<i>Conv</i><sub>HAD</sub> (<i>bbox</i>′)      (7) </p>
                </div>
                <div class="p1">
                    <p id="95"><i>acc</i><sub>0</sub>=<i>Conv</i><sub>HAD</sub> (<i>bbox</i>)      (8) </p>
                </div>
                <div class="p1">
                    <p id="96"><i>R</i><sub><i>a</i></sub> (<i>st</i>, <i>st</i>′) =<i>sign</i> (<i>acc</i><sub>1</sub>-<i>acc</i><sub>0</sub>)      (9) </p>
                </div>
                <div class="area_img" id="97">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JYRJ201904030_09700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="99">在本文中, 采用了比较先进的网络densenet<citation id="224" type="reference"><link href="191" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>作为特征提取的方法。图4所示是深度强化学习网络的结构。</p>
                </div>
                <div class="area_img" id="101">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201904030_10100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 深度强化学习网络的结构" src="Detail/GetImg?filename=images/JYRJ201904030_10100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 深度强化学习网络的结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201904030_10100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="102">在深度强化学习网络运行前, 算法先把每个包围框裁剪好, 将尺度调整为256×256, 图像不发生形变, 多余的部分用0填充。算法将densenet的特征提取部分取出作为前置网络, 将该网络的输出重置成一个1 920维的向量, 与历史动作的36维向量组成了一个新的1 956维的向量。深度Q网络是由两个全连接层组成, 每层后都跟有一个ReLU激活函数和一个dropout层。最后, 输出层对应于智能体可能选择的动作, 在本文中数量为9个。</p>
                </div>
                <h4 class="anchor-tag" id="103" name="103"><b>2.2.4</b> 训 练</h4>
                <div class="p1">
                    <p id="104">• <b>探索-利用</b> 算法采用ε-greedy策略来训练深度Q网络。训练的时候会有<i>ε</i>的概率随机选择动作, 即探索过程;有1-<i>ε</i>的概率会通过深度Q网络选择一个动作, 即利用过程。初始化<i>ε</i>=1, 之后随着训练次数增加<i>ε</i>会逐渐减小, 直到<i>ε</i>=0.1为止。事实上, 随机选择动作会导致智能体学习终止动作更加困难, 为了使得智能体更好地学习终止动作, 当原始精度大于0.8时, 算法强制智能体选择终止动作。这样可以使得训练的速度更快一些。由于算法一直都在做探索的过程, 所以该方法不会被卡在局部最小值。</p>
                </div>
                <div class="p1">
                    <p id="105">• <b>训练参数</b> 深度Q网络的初始化权重采用的是正态分布初始化, densenet则采用预训练权重。为了训练, 本文采用了Adam优化器, 学习率设置为1e-6以避免梯度爆炸。本文在每个模型上训练50个epoch。</p>
                </div>
                <div class="p1">
                    <p id="106">• <b>经验回放</b> 上文提到, bellman方程 (式 (2) ) 从 (<i>st</i>, <i>a</i>, <i>r</i>, <i>st</i>′) 的迁移中进行学习, 这也被称为经验。深度Q网络中的连续经验是非常相关且重要的, 处理不当可能导致学习速率低下和学习的不稳定, 这是Q学习中的一个传统性问题。算法采用了一个回放存储器使网络收敛, 以解决这个问题。回放存储器在每次迭代后会收集经验保存下来, 每次训练时会从保存在回放存储器中的经验中随机挑选一些进行训练。在本文中, 最大可保存的经验数为1 000, 每次训练挑选的数量为100。</p>
                </div>
                <div class="p1">
                    <p id="107">• <b>折扣因子</b> 想要在长期的训练中表现出良好的效果, 只考虑当前的奖励是不够的, 所以算法将未来的奖励也加入训练。本文在式 (3) 中设置了折扣因子<i>γ</i>=0.90, 该数值可以很好地平衡当前的奖励和未来的奖励。</p>
                </div>
                <h4 class="anchor-tag" id="108" name="108"><b>2.3 姿态检测器</b></h4>
                <div class="p1">
                    <p id="109">算法采用了stacked hourglass<citation id="225" type="reference"><link href="153" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>模型作为人体姿态检测器。该模型以Residual<citation id="226" type="reference"><link href="181" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>模块为基础, 通过级联多个Residual模块和降采样层来获取不同尺度的信息, 随着阶数的增加, 级联的Residual模块的数量和特征图尺度的跨度也会增加, 最后将不同尺度的信息进行特征融合以预测人体姿态。</p>
                </div>
                <div class="p1">
                    <p id="110">本文以256×256的图片作为姿态检测器的输入, 模型的阶数为四, 在五个不同的尺度上采集图像特征, 并且最后跳级融合。</p>
                </div>
                <h3 id="111" name="111" class="anchor-tag"><b>3 实 验</b></h3>
                <div class="p1">
                    <p id="112">实验采用的数据集有两部分:人体检测器所使用的数据集为VOC0712;单人姿态检测器所采用的数据集来自MPII。目标精细模型所采用的数据集为原始的两步框架的结果。</p>
                </div>
                <h4 class="anchor-tag" id="113" name="113"><b>3.1 数据集</b></h4>
                <div class="p1">
                    <p id="114">MPII数据集包含了3 844张训练图片和1 758张验证图片, 其中1 758张验证图片官网不提供标签数据。在训练图片中包含了28 000个左右的训练样本可供单人姿态检测器所训练。本次实验采用了25 000个目标作为训练样本, 3 000个目标作为验证样本。</p>
                </div>
                <div class="p1">
                    <p id="115">目标精细模型所采用的数据集测试由两步框架的精度结果构成的。预先将人体检测器所检测到的目标一一做精度计算, 保存其结果, 做成一个由图片名、包围框、原始精度所构成的数据集。训练时读取包围框以供调整, 读取原始精度作为真实值。</p>
                </div>
                <h4 class="anchor-tag" id="116" name="116"><b>3.2 实验结果</b></h4>
                <div class="p1">
                    <p id="117">在MPII数据集上验证了本文算法。完整的精度结果参见表1。结果的演示图如图5所示。结果显示, 本文算法可以以很高的精度完成多人姿态检测任务。</p>
                </div>
                <div class="area_img" id="118">
                    <p class="img_tit"><b>表1 在MPII数据集上的验证结果 mAP</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="118" border="1"><tr><td><br />算法</td><td>头</td><td>肩</td><td>肘</td><td>腕</td></tr><tr><td><br />Iqbal&amp;Gall<sup>[22]</sup></td><td>58.4</td><td>53.9</td><td>44.5</td><td>35.0</td></tr><tr><td><br />DeeperCut<sup>[6]</sup></td><td>78.4</td><td>72.5</td><td>60.2</td><td>51.0</td></tr><tr><td><br />Levinkov et al.<sup>[23]</sup></td><td>89.8</td><td>85.2</td><td>71.8</td><td>59.6</td></tr><tr><td><br />本文算法</td><td>67.0</td><td>77.5</td><td>76.6</td><td>76.9</td></tr><tr><td><br />算法</td><td>臀</td><td>膝</td><td>踝</td><td>整体</td></tr><tr><td><br />Iqbal&amp;Gall<sup>[22]</sup></td><td>42.2</td><td>36.7</td><td>31.1</td><td>43.1</td></tr><tr><td><br />DeeperCut<sup>[6]</sup></td><td>57.2</td><td>52.0</td><td>45.4</td><td>59.5</td></tr><tr><td><br />Levinkov et al.<sup>[23]</sup></td><td>71.1</td><td>63.0</td><td>53.5</td><td>70.6</td></tr><tr><td><br />本文算法</td><td>72.8</td><td>71.8</td><td>73.2</td><td>73.7</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="119">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201904030_119.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 效果演示图一" src="Detail/GetImg?filename=images/JYRJ201904030_119.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 效果演示图一  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201904030_119.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="120">本文算法虽然在精度上有改进, 但是却存在计算量巨大的缺点。一张图片的预测需要经过目标检测器、目标精细模型和姿态检测器三个模型。其中, 在目标精细模型中, 每迭代调整一次包围框都需要经历一次网络前传, 这导致了计算量的陡增。同时, 由于一张图片可能存在多个目标, 每个目标需要多次迭代, 所以时间开销也更大。在GTX1080TI显卡上进行预测, 目标检测器与姿态检测器构成的两步检测算法单个目标耗时120 ms;加入目标精细模型后的基于强化学习的两步检测算法单个目标耗时220 ms。</p>
                </div>
                <div class="p1">
                    <p id="121">同时, 对于人体信息丢失严重的情况, 检测效果不够理想, 如图6所示。</p>
                </div>
                <div class="area_img" id="122">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201904030_122.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 效果演示图二" src="Detail/GetImg?filename=images/JYRJ201904030_122.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 效果演示图二  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201904030_122.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h3 id="123" name="123" class="anchor-tag"><b>4 结 语</b></h3>
                <div class="p1">
                    <p id="124">本文提出了一种新的多人姿态检测算法, 在其准确率方面优于以往的两步框架算法。算法加入了目标精细模型, 该模型可通过对包围框的调整使得信息更加精确, 使得两步框架的精度有了一定的提升, 证明该方法是可靠的。但由于它需要不断对包围框进行迭代调整, 对单个目标的处理速度偏高, 在GTX1080TI显卡上达到了约160 ms。在未来的工作中, 可以期待强化学习在更多的领域参与到其中去。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="153">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Stacked Hourglass Networks for Human Pose Estimation">

                                <b>[1]</b> Newell A, Yang K, Deng J. Stacked Hourglass Networks for Human Pose Estimation[C]//European Conference on Computer Vision, 2016:483-499.
                            </a>
                        </p>
                        <p id="155">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Articulated people detection and pose estimation: Reshaping the future">

                                <b>[2]</b> Jain A. Articulated people detection and pose estimation: Reshaping the future[C]//Computer Vision and Pattern Recognition. IEEE, 2012:3178-3185.
                            </a>
                        </p>
                        <p id="157">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Using k-poselets for detecting people and localizing their keypoints">

                                <b>[3]</b> Gkioxari G, Hariharan B, Girshick R, et al. Using k-Poselets for Detecting People and Localizing Their Keypoints[C]//IEEE Conference on Computer Vision and Pattern Recognition. IEEE Computer Society, 2014:3582-3589.
                            </a>
                        </p>
                        <p id="159">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Parsing occluded people by flexible compositions">

                                <b>[4]</b> Chen X, Yuille A. Parsing occluded people by flexible compositions[C]//Computer Vision and Pattern Recognition. IEEE, 2015:3945-3954.
                            </a>
                        </p>
                        <p id="161">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=DeepCut:Joint Subset Partition and Labeling for Multi Person Pose Estimation">

                                <b>[5]</b> Pishchulin L, Insafutdinov E, Tang S, et al. DeepCut: Joint Subset Partition and Labeling for Multi Person Pose Estimation[J]. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) .IEEE, 2016:4929-4937.
                            </a>
                        </p>
                        <p id="163">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=DeeperCut:A Deeper, Stronger, and Faster Multi-person Pose Estimation Model">

                                <b>[6]</b> Insafutdinov E, Pishchulin L, Andres B, et al. DeeperCut: A Deeper, Stronger, and Faster Multi-person Pose Estimation Model[C]//European Conference on Computer Vision. Springer International Publishing, 2016:34-50.
                            </a>
                        </p>
                        <p id="165">
                            <a id="bibliography_7" >
                                    <b>[7]</b>
                                 Liu W, Anguelov D, Erhan D, et al. SSD: Single Shot MultiBox Detector[C]//European Conference on Computer Vision. Springer International Publishing, 2016:21-37.
                            </a>
                        </p>
                        <p id="167">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Cascaded models for articulated pose es-timation">

                                <b>[8]</b> Sapp B, Toshev A, Taskar B. Cascaded models for articulated pose estimation[C]//European Conference on Computer Vision. Springer-Verlag, 2010:406-420.
                            </a>
                        </p>
                        <p id="169">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Human pose estima-tion using body parts dependent joint regressors">

                                <b>[9]</b> Dantone M, Gall J, Leistner C, et al. Human Pose Estimation Using Body Parts Dependent Joint Regressors[C]//IEEE Conference on Computer Vision and Pattern Recognition. IEEE, 2013:3041-3048.
                            </a>
                        </p>
                        <p id="171">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=An efficient branch-and-bound algorithm for optimal human pose estimation">

                                <b>[10]</b> Savarese S, Lee H, Telaprolu M, et al. An efficient branch-and-bound algorithm for optimal human pose estimation[C]//Computer Vision and Pattern Recognition.IEEE, 2012:1616-1623.
                            </a>
                        </p>
                        <p id="173">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Human Pose Estimation with Fields of Parts">

                                <b>[11]</b> Kiefel M, Gehler P V. Human Pose Estimation with Fields of Parts[M]//Computer Vision—ECCV 2014. Springer International Publishing, 2014:331-346.
                            </a>
                        </p>
                        <p id="175">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Computationally efficient regression on a dependency graph for human poseestimation">

                                <b>[12]</b> Hara K, Chellappa R. Computationally Efficient Regression on a Dependency Graph for Human Pose Estimation[C]//Computer Vision and Pattern Recognition.IEEE, 2013:3390-3397.
                            </a>
                        </p>
                        <p id="177">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deeppose:Human pose estimation via deep neural networks">

                                <b>[13]</b> Toshev A, Szegedy C. DeepPose: Human Pose Estimation via Deep Neural Networks[C]//Computer Vision and Pattern Recognition. IEEE, 2014:1653-1660.
                            </a>
                        </p>
                        <p id="179">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multi-source deep learning for human pose estimation">

                                <b>[14]</b> Ouyang W, Chu X, Wang X. Multi-source Deep Learning for Human Pose Estimation[C]//Computer Vision and Pattern Recognition. IEEE, 2014:2337-2344.
                            </a>
                        </p>
                        <p id="181">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep residual learning for image recognition">

                                <b>[15]</b> He K, Zhang X, Ren S, et al. Deep Residual Learning for Image Recognition[C]//2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) .IEEE, 2015:770-778.
                            </a>
                        </p>
                        <p id="183">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Faster R-CNN:towards real-time object detection with region proposal networks">

                                <b>[16]</b> Ren S, He K, Girshick R, et al. Faster R-CNN: towards real-time object detection with region proposal networks[C]//International Conference on Neural Information Processing Systems. MIT Press, 2015:91-99.
                            </a>
                        </p>
                        <p id="185">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Active object localization wife deep reinforcement learn-ing">

                                <b>[17]</b> Caicedo J C, Lazebnik S. Active Object Localization with Deep Reinforcement Learning[C]//Proceedings of the 2015 IEEE International Conference on Computer Vision (ICCV) . IEEE Computer Society, 2015: 2488-2496.
                            </a>
                        </p>
                        <p id="187">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Hierarchical Object Detection with Deep Reinforcement Learning">

                                <b>[18]</b> Bellver M, Giro-I-Nieto X, Marques F, et al. Hierarchical Object Detection with Deep Reinforcement Learning[J]. Advances in Parallel Computing, 2016, 31.
                            </a>
                        </p>
                        <p id="189">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Human-Level control through deep reinforcement learning">

                                <b>[19]</b> Mnih V, Kavukcuoglu K, Silver D, et al. Human-level control through deep reinforcement learning.[J]. Nature, 2015, 518 (7540) :529.
                            </a>
                        </p>
                        <p id="191">
                            <a id="bibliography_20" >
                                    <b>[20]</b>
                                 Huang G, Liu Z, Laurens V D M, et al. Densely Connected Convolutional Networks[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2017) .IEEE, 2016:2261-2269.
                            </a>
                        </p>
                        <p id="193">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Very deep convolutional networks for large-scale image recognition[EB]">

                                <b>[21]</b> Simonyan K, Zisserman A. Very deep convolutional networks for large-scale image recognition[EB]. arXiv preprint arXiv:1409.1556, 2014.
                            </a>
                        </p>
                        <p id="195">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multi-person Pose Estimation with Local Joint-to-Person Associations">

                                <b>[22]</b> Iqbal U, Gall J. Multi-Person Pose Estimation with Local Joint-to-Person Associations[C]//European Conference on Computer Vision, ECCV 2016 Workshops, 2016:627-642.
                            </a>
                        </p>
                        <p id="197">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Joint graph decomposition &amp;amp; node labeling Problem,algorithms,applications">

                                <b>[23]</b> Levinkov E, Uhrig J, Tang S, et al. Joint graph decomposition &amp; node labeling:Problem, algorithms, applications[C]//IEEE Conference on Computer Vision and Pattern Recognition (CVPR) . IEEE, 2017.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JYRJ201904030" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201904030&amp;v=MjQ5NDFyQ1VSN3FmWnVadEZ5em1VYjdOTHpUWlpMRzRIOWpNcTQ5R1pJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0Y=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
