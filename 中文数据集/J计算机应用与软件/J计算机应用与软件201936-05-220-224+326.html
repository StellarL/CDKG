<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637135741556065000%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJYRJ201905039%26RESULT%3d1%26SIGN%3dQveAGEr5SjQtg8tUtq%252fgCc4sqNk%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JYRJ201905039&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JYRJ201905039&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201905039&amp;v=MjMyNzlMTHpUWlpMRzRIOWpNcW85R2JZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVadEZ5emtVTDM=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#27" data-title="&lt;b&gt;0 引 言&lt;/b&gt; "><b>0 引 言</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#34" data-title="&lt;b&gt;1 Resnet网络&lt;/b&gt; "><b>1 Resnet网络</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#40" data-title="&lt;b&gt;2 多连接卷积网络&lt;/b&gt; "><b>2 多连接卷积网络</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#43" data-title="&lt;b&gt;2.1 多连接块&lt;/b&gt;"><b>2.1 多连接块</b></a></li>
                                                <li><a href="#47" data-title="&lt;b&gt;2.2 恒等映射&lt;/b&gt;"><b>2.2 恒等映射</b></a></li>
                                                <li><a href="#50" data-title="&lt;b&gt;2.3 多层次卷积特征融合&lt;/b&gt;"><b>2.3 多层次卷积特征融合</b></a></li>
                                                <li><a href="#52" data-title="&lt;b&gt;2.4 损失函数&lt;/b&gt;"><b>2.4 损失函数</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#56" data-title="&lt;b&gt;3 实验结果与分析&lt;/b&gt; "><b>3 实验结果与分析</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#58" data-title="&lt;b&gt;3.1 参数设置&lt;/b&gt;"><b>3.1 参数设置</b></a></li>
                                                <li><a href="#60" data-title="&lt;b&gt;3.2 实验结果对比&lt;/b&gt;"><b>3.2 实验结果对比</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#71" data-title="&lt;b&gt;4 结 语&lt;/b&gt; "><b>4 结 语</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#36" data-title="图1 Resnet网络中子块构造的示意图">图1 Resnet网络中子块构造的示意图</a></li>
                                                <li><a href="#42" data-title="图2 深度网络结构">图2 深度网络结构</a></li>
                                                <li><a href="#62" data-title="&lt;b&gt;表1 制冷型红外图像的超分辨重建&lt;/b&gt;SSIM&lt;b&gt;对比&lt;/b&gt;"><b>表1 制冷型红外图像的超分辨重建</b>SSIM<b>对比</b></a></li>
                                                <li><a href="#63" data-title="&lt;b&gt;表2 制冷型红外图像的超分辨重建&lt;/b&gt;PSNR&lt;b&gt;对比&lt;/b&gt;"><b>表2 制冷型红外图像的超分辨重建</b>PSNR<b>对比</b></a></li>
                                                <li><a href="#67" data-title="图3 非制冷红外图像超分辨重建对比图 (放大因子3) ">图3 非制冷红外图像超分辨重建对比图 (放大因子3) </a></li>
                                                <li><a href="#70" data-title="图4 制冷型红外图像超分辨重建对比图 (放大因子3) ">图4 制冷型红外图像超分辨重建对比图 (放大因子3) </a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" title=" 薛峰, 朱强, 林楠.基于低秩和邻域嵌入的单帧红外图像超分辨算法[J].红外技术, 2017, 39 (11) :1032-1037." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=HWJS201711011&amp;v=MTA0OTN5emtVTDNMTFRyQmZiRzRIOWJOcm85RVpZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVadEY=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         薛峰, 朱强, 林楠.基于低秩和邻域嵌入的单帧红外图像超分辨算法[J].红外技术, 2017, 39 (11) :1032-1037.
                                    </a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" title=" 李方彪, 何昕, 魏仲慧, 等.生成式对抗神经网络的多帧红外图像超分辨率重建[J].红外与激光工程, 2018, 47 (2) :26-33." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=HWYJ201802004&amp;v=MjM4MTBadEZ5emtVTDNMTFRyU1pMRzRIOW5Nclk5RllJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnU=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         李方彪, 何昕, 魏仲慧, 等.生成式对抗神经网络的多帧红外图像超分辨率重建[J].红外与激光工程, 2018, 47 (2) :26-33.
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title=" Dong C, Loy C C, He K, et al.Learning a Deep Convolutional Network for Image Super-Resolution[C]// European Conference on Computer Vision.Springer, Cham, 2014:184-199." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning a deep convolutional network for image super-resolution">
                                        <b>[3]</b>
                                         Dong C, Loy C C, He K, et al.Learning a Deep Convolutional Network for Image Super-Resolution[C]// European Conference on Computer Vision.Springer, Cham, 2014:184-199.
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" title=" Dong C, Loy C C, He K, et al.Image Super-Resolution Using Deep Convolutional Networks[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2016, 38 (2) :295-307." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image super-resolution using deep convolutional networks">
                                        <b>[4]</b>
                                         Dong C, Loy C C, He K, et al.Image Super-Resolution Using Deep Convolutional Networks[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2016, 38 (2) :295-307.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" title=" Shi W, Caballero J, Huszar F, et al.Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network[C]// 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) .IEEE Computer Society, 2016:1874-1883." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network">
                                        <b>[5]</b>
                                         Shi W, Caballero J, Huszar F, et al.Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network[C]// 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) .IEEE Computer Society, 2016:1874-1883.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" title=" 徐冉, 张俊格, 黄凯奇.利用双通道卷积神经网络的图像超分辨率算法[J].中国图象图形学报, 2016, 21 (5) :556-564." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZGTB201605003&amp;v=MzA1NTBmTXFvOUZaNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnRGeXprVUwzTFB5cmZiTEc0SDk=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                         徐冉, 张俊格, 黄凯奇.利用双通道卷积神经网络的图像超分辨率算法[J].中国图象图形学报, 2016, 21 (5) :556-564.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" title=" Kim J, Kwon Lee J, Mu Lee K.Accurate image super-resolution using very deep convolutional networks[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.2016:1646-1654." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Accurate image super-resolution using very deep convolutional networks">
                                        <b>[7]</b>
                                         Kim J, Kwon Lee J, Mu Lee K.Accurate image super-resolution using very deep convolutional networks[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.2016:1646-1654.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" title=" Ledig C, Theis L, Huszar F, et al.Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network[C]// 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) .IEEE, 2017:4681-4690." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Photo-realistic single image super-resolution using a genera-tive adversarial network">
                                        <b>[8]</b>
                                         Ledig C, Theis L, Huszar F, et al.Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network[C]// 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) .IEEE, 2017:4681-4690.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" title=" Lim B, Son S, Kim H, et al.Enhanced Deep Residual Networks for Single Image Super-Resolution[C]// 2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW) .IEEE Computer Society, 2017:136-144." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Enhanced Deep Residual Networks for Single Image Super-Resolution">
                                        <b>[9]</b>
                                         Lim B, Son S, Kim H, et al.Enhanced Deep Residual Networks for Single Image Super-Resolution[C]// 2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW) .IEEE Computer Society, 2017:136-144.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_10" title=" Szegedy C, Liu W, Jia Y Q, et al.Going deeper with convolutions[C]// 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) .IEEE, 2015." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Going Deeper with Convolutions">
                                        <b>[10]</b>
                                         Szegedy C, Liu W, Jia Y Q, et al.Going deeper with convolutions[C]// 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) .IEEE, 2015.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_11" title=" He K, Zhang X, Ren S, et al.Deep Residual Learning for Image Recognition[C]// IEEE Conference on Computer Vision and Pattern Recognition.IEEE Computer Society, 2016:770-778." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep residual learning for image recognition">
                                        <b>[11]</b>
                                         He K, Zhang X, Ren S, et al.Deep Residual Learning for Image Recognition[C]// IEEE Conference on Computer Vision and Pattern Recognition.IEEE Computer Society, 2016:770-778.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_12" title=" Yang C Y, Ma C, Yang M H.Single-image super-resolution:a benchmark[M]// Lecture Notes in Computer Science.Heidelberg:Springer, 2014, 8692:372-386." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Single-image super-resolution:a benchmark">
                                        <b>[12]</b>
                                         Yang C Y, Ma C, Yang M H.Single-image super-resolution:a benchmark[M]// Lecture Notes in Computer Science.Heidelberg:Springer, 2014, 8692:372-386.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JYRJ" target="_blank">计算机应用与软件</a>
                2019,36(05),220-224+326 DOI:10.3969/j.issn.1000-386x.2019.05.038            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于多层连接卷积神经网络的单帧图像超分辨重建</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%B4%BA%E7%91%9C%E9%A3%9E&amp;code=31234281&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">贺瑜飞</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%AB%98%E5%AE%8F%E4%BC%9F&amp;code=10680360&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">高宏伟</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%A6%86%E6%9E%97%E5%AD%A6%E9%99%A2%E6%95%B0%E5%AD%A6%E4%B8%8E%E7%BB%9F%E8%AE%A1%E5%AD%A6%E9%99%A2&amp;code=0083326&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">榆林学院数学与统计学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%A6%86%E6%9E%97%E5%AD%A6%E9%99%A2%E7%8E%B0%E4%BB%A3%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%85%88%E8%BF%9B%E5%88%B6%E9%80%A0%E6%8A%80%E6%9C%AF%E7%A0%94%E7%A9%B6%E4%B8%AD%E5%BF%83&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">榆林学院现代设计与先进制造技术研究中心</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>现有光电观瞄装备采取的电子变倍功能大都是采用线性插值进行放大重建, 存在细节不明显, 边缘模糊的现象。针对目前超分辨重建算法存在的问题, 提出一个多连接卷积网络。该网络构建出多连接网络结构, 通过一个较长的跳跃式策略进行恒等映射, 实现低层次特征和高级特征的级联, 能够同时表征各种复杂的重构场景。采用双参数损失函数来优化训练深度网络, 提高网路模型的泛化能力。仿真实验结果表明, 该方法能够生成具有丰富细节而且清晰的高分辨红外图像, 同时也在客观定量评价上都有很大提高。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%B6%85%E5%88%86%E8%BE%A8%E9%87%8D%E5%BB%BA&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">超分辨重建;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%BA%A2%E5%A4%96%E5%9B%BE%E5%83%8F&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">红外图像;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%89%B9%E5%BE%81%E7%BA%A7%E8%81%94&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">特征级联;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">损失函数;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%81%92%E7%AD%89%E6%98%A0%E5%B0%84&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">恒等映射;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    贺瑜飞, 高级讲师, 主研领域:模式识别, 光电设计, 深度学习, 红外图像预处理。;
                                </span>
                                <span>
                                    高宏伟, 教授。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-10-31</p>

                    <p>

                            <b>基金：</b>
                                                        <span>榆林学院高层次人才科研启动基金项目 (12GK43);</span>
                    </p>
            </div>
                    <h1><b>SUPER-RESOLUTION RECONSTRUCTION USING MULTILAYER CONNECTED CONVOLUTIONAL NEURAL NETWORK FOR SINGLE-FRAME IMAGE</b></h1>
                    <h2>
                    <span>He Yufei</span>
                    <span>Gao Hongwei</span>
            </h2>
                    <h2>
                    <span>School of Mathematics and Statistics, Yulin University</span>
                    <span>Research Center for Contemporary Design and Advanced Manufacturing Technology, Yulin University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Most of the electronic magnification functions adopted by existing photoelectric sight equipment are magnified and reconstructed by linear interpolation, and the details are not obvious and the edges are blurred. In this paper, we proposed a multi-connection convolutional network to solve the problems of the super-resolution reconstruction (SR) algorithm. The network constructed a multi-connection network structure, where a long skip strategy was used to obtain the identity mapping. It could concatenate the low-level features and high-level features, and simultaneously represented various complex reconstruction scenes. A two parameter loss function was applied to optimize and train the deep network and improve the generalization ability of the network model. The simulation results show that the proposed SR algorithm can generate high resolution infrared images with rich details and clearness, and the objective quantitative evaluation is also greatly improved.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Super-resolution%20reconstruction&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Super-resolution reconstruction;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Infrared%20image&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Infrared image;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Deep%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Deep learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Feature%20cascade&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Feature cascade;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Loss%20function&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Loss function;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Identity%20mapping&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Identity mapping;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-10-31</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="27" name="27" class="anchor-tag"><b>0 引 言</b></h3>
                <div class="p1">
                    <p id="28">红外图像是由红外探测器将入射的红外辐射信号转变成电信号而形成的图像, 不受环境光线的影响, 具有全天候, 全天时的能力, 在预警、监控等军事领域有着广泛的应用。然而, 由于红外探测器成像体制的局限以及硬件工艺的限制, 使得现有的红外图像分辨率较低, 纹理特征不明显以及灰度分布较集中<citation id="73" type="reference"><link href="3" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="29">为了得到纹理清晰的超分辨图像, 以便适应现代战争对精确打击与远程攻击能力的要求, 采用超分辨重建提升图像质量是目前最经济可行的方案<citation id="74" type="reference"><link href="5" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>。单帧图像超分辨重建算法是利用同一场景的低分辨率 (LR) 图像估计出其相应的高分辨率 (HR) 图像的过程, 大致可以分为插值法、重构法、学习法, 其中基于学习法的超分辨重建方法是目前研究最多、性能最好的方法。该方法从低分辨图像与对应的高分辨图像中学习出相应的特征映射关系, 从而实现超分辨重建, 其性能主要取决于于算法的特征学习能力, 采用超分辨重建提升图像质量是目前最经济可行的方案。</p>
                </div>
                <div class="p1">
                    <p id="30">随着高性能GPU的出现, 以卷积神经网络为代表的深度学习算法广泛的应用于图像处理领域。相比传统的低层次特征, 高特征层次可以实现更加复杂的函数逼近, 适合超分辨重建。近年来, 基于卷积神经网络的超分辨重建算法能够提供端到端的映射, 具有显著的重建质量。Dong等<citation id="75" type="reference"><link href="7" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>开创性地提出了SRCNN算法进行超分辨重建, 该算法使用双三次插值将LR图像放大到所需的大小作为输入, 这意味着超分辨重建是在一幅伪高分辨图像上进行, 增加了计算复杂度, 同时也会带来重建伪影问题。为了解决这个问题, Dong等<citation id="76" type="reference"><link href="9" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>在网络末端添加了一个解卷积层来代替上采样操作。Shi等<citation id="77" type="reference"><link href="11" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>设计了一个高效的亚像素卷积层将HR特征映射到LR空间, 从而提升了重建准确性和计算效率。由于SRCNN依赖于较小的感受野, 且三层网络结构一定程度上也限制对更加复杂特征的提取与表征能力, 徐冉等<citation id="78" type="reference"><link href="13" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>在VGG-net基础上提出了多层网络, 对复杂图像的重建效果较好, 但可能存在梯度消失等问题;Kim等<citation id="79" type="reference"><link href="15" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>引入了残差网络模型来解决较深层次网络梯度消失问题, 该模型理论上适用于任意深度的网络, 具有更大的感受野。残差网络使得深度模型利用更多的上下文信息并模拟更加复杂的函数, 获得更优的重建性能。</p>
                </div>
                <div class="p1">
                    <p id="31">随着深度模型研究的深入, 残余网络缓解了消失梯度问题, 但未能充分利用低层次特征。文献<citation id="80" type="reference">[<a class="sup">8</a>]</citation>通过跳跃式传递策略将第一层学习到的特征传递到特定的后续层, 而不是直接使用低层次特征。超分辨任务中, 图像的局部特征有时比全局特征更能获得准确的重构效果, 因此简单地将残余网络引入到后端并不能进一步提高重构精度。</p>
                </div>
                <div class="p1">
                    <p id="32">深度学习中损失函数是整个网络模型的调度中心, 一般是利用最小化损失函数实现参数的最优训练。目前大多数深度模型采用l<sub>2</sub>损失函数进行网络训练, 不可避免地存在过度平滑问题, 因此不能保证获得更好的性能。不同于以l<sub>2</sub>为损失函数训练网络, EDSR<citation id="81" type="reference"><link href="19" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation> 采用l<sub>1</sub>损失函数对网络进行训练, 能够改进网络质量。尽管如此, l<sub>1</sub>和l<sub>2</sub>的损失函数非常简单, 以至于无法适应复杂的重建函数。</p>
                </div>
                <div class="p1">
                    <p id="33">本文针对目前深度超分辨重建算法存在的问题, 提出了一个多连接卷积网络, 该网络是一个构建在多连接块上的结构, 用来组合低层次特征和高级特征, 能够同时提取各种复杂特征。这些特征通过一个较长的跳跃式连接进一步聚合。此外, 本文还采用强大而灵活的双参数损失函数来优化训练深度网络, 提高网路模型的泛化能力。仿真实验结果表明, 本文所提出的方法能够生成具有丰富细节而且清晰的高分辨红外图像。</p>
                </div>
                <h3 id="34" name="34" class="anchor-tag"><b>1 Resnet网络</b></h3>
                <div class="p1">
                    <p id="35">Resnet网络<citation id="82" type="reference"><link href="21" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>的关键核心点是引入了残差块, 在一个浅层网络基础上叠加恒等映射层进行残差学习, 提升深度特征提取的精度, 解决了梯度消失的问题。假定Resnet网络的原始输入样本为<i>x</i>, 经过多层网络映射后能够得到<i>F</i> (<i>x</i>) , 因此, 残差函数<i>H</i> (<i>x</i>) =<i>F</i> (<i>x</i>) -<i>x</i>, 如图1所示。可以看出, 经过一个恒等映射, 将输入叠加到卷积输出上, 形成能够跳过一层或多层的跳跃式连接, 消除梯度消失现象, 可以使网络深度做到成百上千层。</p>
                </div>
                <div class="area_img" id="36">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201905039_036.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 Resnet网络中子块构造的示意图" src="Detail/GetImg?filename=images/JYRJ201905039_036.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 Resnet网络中子块构造的示意图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201905039_036.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="37">恒等映射单地叠加在网络中, 即便增加网络的层数也不会降低网络的性能。图1的结构能够可以简单地使多个非线性层的权重趋向零以近似恒等映射, 其输出可以表示为:</p>
                </div>
                <div class="p1">
                    <p id="38"><i>y</i>=<i>H</i> (<i>x</i>, <i>W</i><sub><i>i</i></sub>) +<i>x</i>      (1) </p>
                </div>
                <div class="p1">
                    <p id="39">式中:<i>x</i>和<i>y</i>分别表示子块的输入与输出结果, <i>H</i> (<i>x</i>, <i>W</i><sub><i>i</i></sub>) 是残差映射。 所引入的<i>x</i>通路, 既没有引入额外参数也没有增加计算复杂度。仿真实验结果表明, Resnet网络比相同规模的简单网络更容易收敛, 能获得较好输出结果, 不受网络深度影响。</p>
                </div>
                <h3 id="40" name="40" class="anchor-tag"><b>2 多连接卷积网络</b></h3>
                <div class="p1">
                    <p id="41">现有的超分辨重建算法更注重提取图像的高层次特征, 包括全局特征, 而忽略低层次特征。低层次特征包含丰富的局部细节, 可获得令人满意的人类视觉效果, 这对超分辨任务具有实质性影响;高层次特征主要关注于大范围的感受野, 确保超分辨重建的图像准确性。为了将不同层次的特征进行结合, 本文设计了一种新颖的超分辨率多连接网络。如图2所示, 整个架构由<i>N</i>个多连接模块、一个上采样模块和一个较长跨连接模块组成。接下来, 我们详细分析介绍每个模块。</p>
                </div>
                <div class="area_img" id="42">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201905039_042.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 深度网络结构" src="Detail/GetImg?filename=images/JYRJ201905039_042.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 深度网络结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201905039_042.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="43" name="43"><b>2.1 多连接块</b></h4>
                <div class="p1">
                    <p id="44">级联操作是深度网络结构设计中很重要的一种操作, 经常用于将特征联合, 多个卷积特征提取框架提取的特征融合或者是将输出层的信息进行融合。本文采用的模型将低分辨图像输入到卷积层与 ReLU激活函数, 构建出多连接块结构, 其中多连接块包含一个子块和一个恒等映射。</p>
                </div>
                <div class="p1">
                    <p id="45">受到文献<citation id="83" type="reference">[<a class="sup">11</a>]</citation>的启发, 本文利用多个不同的子模块来获取最优特征表示。相比于文献<citation id="84" type="reference">[<a class="sup">8</a>]</citation>在残差模块中采用大小为3×3卷积核的两个卷积层, 而本文通过堆叠多连结模块, 特征映射的数量以一定的增长率增加, 其增长率等于子模块中最后卷积层的特征图的数量。因此, 为了降低维度的目标, 采用1×1卷积层。1×1卷积层主要有两个作用:一是通过卷积核通道数进行降维与升维;二是跨通道交互和信息整合。</p>
                </div>
                <div class="p1">
                    <p id="46">事实证明通过扩大感受野可以增加上下文信息, 这对恢复降质图像是有效的。一般来说, 通过增加网络的宽度或深度可以扩大感受野。为了合理扩大感受野, 我们在子模块的第二卷积层上采用一个较大的滤波器。起始模块 ( Inception module) 表明具有大于3×3的滤波器的卷积层总是可以被缩减为多个3×3卷积层, 能保持相同的感受野, 而不降低性能。同时, 多个3×3卷积层能增加网络的深度, 以提取更多的区域细节, 具有更好的非线性。基于这些理论, 本文通过额外引入一个3×3卷积层来增强深度模型的基本分支模块, 并形成本文的核心结构, 如图2所示。其中右侧是整个网络的子结构。</p>
                </div>
                <h4 class="anchor-tag" id="47" name="47"><b>2.2 恒等映射</b></h4>
                <div class="p1">
                    <p id="48">低层次特征主要由局部特征组成, 如边缘、轮廓和角点等, 这些都有利于超分辨重建任务。在网络继续加深时, 在训练集上的效果反而下降。如图2所示, 我们采用一个级联操作, 在每个多连接块中都具有恒等映射的子块。通过引入恒等映射, 网络如果不需要更深的深度, 可以走恒等映射的通道, 而将残差映射趋为0。恒等映射将先前的<i>i</i>-1个子模块的输出作为第<i>i</i>+1个子块的输入以及多连接块的堆叠, 其中<i>i</i>是子块的索引。第<i>i</i>个子块的输出直接用于第<i>i</i>+1个子块。恒等映射的形式确保了低级特征能用于超分辨重建, 从而使得网络完全融合了局部特征和全局特征。因此, 前面的子块中提取的特征使得后续层的特征具有更多丰富与复杂的特征, 最终可以获得更优越的特征表示。</p>
                </div>
                <div class="p1">
                    <p id="49">为了提取更多样化和复杂的特征, 多连接块被堆叠<i>N</i>次。然后, 应用1×1卷积层来减少维数, 从而以更少的信息损失进行更快的计算。新结构中的中间3×3的卷积层首先在一个1×1卷积层下降维, 然后在另一个1×1的卷积层下做了还原, 既保持了精度又减少了计算量。</p>
                </div>
                <h4 class="anchor-tag" id="50" name="50"><b>2.3 多层次卷积特征融合</b></h4>
                <div class="p1">
                    <p id="51">为了学习到图像中更丰富的深层次特征, 本文直接对原始低分辨图像进行多层卷积, 并采用一个跨连接操作将低层次特征与高层次特征以端到端用多层的方式级联起来, 然后输入到上采样模块中。上采样模块实现了多层次卷积特征图的融合。由于本文直接对原始低分辨图像进行重建, 经过多个卷积层后, 得到<i>r</i><sup>2</sup>个特征图, 其中<i>r</i>超分辨放大倍率。通过将每一个像素点对应的<i>r</i><sup>2</sup>通道的特征排列成<i>r</i>×<i>r</i>的子图像, 从而得到大小为<i>rH</i>×<i>rW</i>的高分辨图像。该策略不直接去多特征图进行上采样映射, 而是通过学习函数隐藏在各卷基层的学习过程中, 具有很高的效率, 同时多层次卷积可以学习到更加准确的高分辨特征。</p>
                </div>
                <h4 class="anchor-tag" id="52" name="52"><b>2.4 损失函数</b></h4>
                <div class="p1">
                    <p id="53">本文采用了一种广义的双参数损失函数, 该函数可以推广到目前许多流行的鲁棒损失函数。假定<i>I</i><sub>HR</sub>和<i>S</i><sub>HR</sub>之间的误差可以表示为<i>e</i>=<i>I</i><sub>HR</sub>-<i>S</i><sub>HR</sub>, 因此本文采用的损失函数可以表示为:</p>
                </div>
                <div class="p1">
                    <p id="54" class="code-formula">
                        <mathml id="54"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtext>L</mtext><mo stretchy="false"> (</mo><mi>e</mi><mo>, </mo><mi>α</mi><mo>, </mo><mi>β</mi><mo stretchy="false">) </mo><mo>=</mo><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mi>log</mi><mrow><mo> (</mo><mrow><mn>0</mn><mo>.</mo><mn>5</mn><mrow><mo> (</mo><mrow><mfrac><mi>e</mi><mi>β</mi></mfrac></mrow><mo>) </mo></mrow><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mn>1</mn></mrow><mo>) </mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mi>α</mi><mo>=</mo><mn>0</mn></mtd></mtr><mtr><mtd><mn>1</mn><mo>-</mo><mi>exp</mi><mrow><mo> (</mo><mrow><mn>1</mn><mo>-</mo><mn>0</mn><mo>.</mo><mn>5</mn><mrow><mo> (</mo><mrow><mfrac><mi>e</mi><mi>β</mi></mfrac></mrow><mo>) </mo></mrow><msup><mrow></mrow><mn>2</mn></msup></mrow><mo>) </mo></mrow><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mi>α</mi><mo>=</mo><mo>-</mo><mi>∞</mi></mtd></mtr><mtr><mtd><mfrac><mrow><mi>ρ</mi><mo stretchy="false"> (</mo><mi>α</mi><mo stretchy="false">) </mo></mrow><mi>α</mi></mfrac><mrow><mo> (</mo><mrow><mrow><mo> (</mo><mrow><mfrac><mn>1</mn><mrow><mi>ρ</mi><mo stretchy="false"> (</mo><mi>α</mi><mo stretchy="false">) </mo></mrow></mfrac><mrow><mo> (</mo><mrow><mfrac><mi>e</mi><mi>β</mi></mfrac></mrow><mo>) </mo></mrow><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mn>1</mn></mrow><mo>) </mo></mrow><msup><mrow></mrow><mrow><mfrac><mi>α</mi><mn>2</mn></mfrac></mrow></msup><mo>-</mo><mn>1</mn></mrow><mo>) </mo></mrow><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext></mrow><mtext>o</mtext><mtext>t</mtext><mtext>h</mtext><mtext>e</mtext><mtext>r</mtext><mtext>w</mtext><mtext>i</mtext><mtext>s</mtext><mtext>e</mtext></mtd></mtr></mtable></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="55">式中:<i>ρ</i> (<i>α</i>) =max (1, 2-<i>α</i>) ;<i>α</i>、<i>β</i>是具有连续值属性的参数, 可以通过不同的参数设置, 模拟出任意的损失函数, 如均方误差损失函数 (<i>l</i><sub>2</sub>) , 绝对值误差损失函数 (<i>l</i><sub>1</sub>) 等。相比于传统固定参数损失函数, 本文采用双参数损失函数可以通过微调<i>α</i>和<i>β</i>获得更优的损失函数, 具有很大的灵活性, 可以适应更复杂的场景。为了适应本文的提出的超分辨深度模型, <i>α</i>和<i>β</i>分别经验设置为1.12和0.05。</p>
                </div>
                <h3 id="56" name="56" class="anchor-tag"><b>3 实验结果与分析</b></h3>
                <div class="p1">
                    <p id="57">首先介绍提出的超分辨算法的实验仿真条件和参数设置, 接下来对比了本方法与其他方法在低成本非制冷红外探测器获得的红外图像重建结果的视觉效果和数值指标。由于目前大多数算法都是基于高分辨率的图像, 本文采用的FLIR的制冷型锑化铟红外探测器SC8400进行高分辨图像采集, 大小为1 280×1 024, 其对应的低分辨红外图像是经过高斯模糊采样获取。同时, 我们也采用低分辨探测器获取的320×240的低分辨图像进行验证。</p>
                </div>
                <h4 class="anchor-tag" id="58" name="58"><b>3.1 参数设置</b></h4>
                <div class="p1">
                    <p id="59">本文采用的图像被裁剪成具有相同大小的64×64的块进行训练, 其对应的高分辨与低分辨对应的图像作为训练数据。本文采用了16个子结构模块进行构造深度网络。在训练阶段, 最小批量设置为16;采用梯度下降算法进行优化时, 权重的更新规则中, 学习速率初始化设置为0.25, 然后在训练到第30个Epoch (Epoch是指对所有训练数据的一轮遍历) 时, 学习速率改为0.025;若100个Epoch后, 我们提出的损失函数没有改变则停止训练。为了提高优化效率, 本文采用Adam优化算法, Adam 是随机梯度下降算法的扩展式, 可以基于训练数据迭代地更新神经网络权重, 其参数设定为:alpha=0.001, beta1=0.9, beta2=0.999 和 epsilon=10<sup>-8</sup>。整个超分辨重建网络的放大因子设定为3。本文选择的软件平台是Windows下Keras框架, 硬件配置:CPU为Intel Core i7, GPU为Titan X, 内存为16 GB。</p>
                </div>
                <h4 class="anchor-tag" id="60" name="60"><b>3.2 实验结果对比</b></h4>
                <div class="p1">
                    <p id="61">本文所用的对比算法分别是基于稀疏表示的图像超分辨算法 (SC) <citation id="85" type="reference"><link href="25" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>, 基于卷积神经网络的超分辨算法 (SRCNN) <citation id="86" type="reference"><link href="9" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>, 亚像素超分辨重建算法 (SPSR) <citation id="87" type="reference"><link href="13" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>和深度网络超分辨 (VDSR) <citation id="88" type="reference"><link href="15" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>。所有对比方法的代码都是来自于网上公开的源码。为了便于分析本文提出的超分辨模型的性能, 采用传统损失函数的多层超分辨算法记为MSR_T, 采用2.4节损失函数的算法记为MSR_L。最后, 利用峰值信噪比 (PSNR) 和结构相似度 (SSIM) 对各算法对应的超分辨图像进行定量评价, 其中深黑体表示所有算法中最好的结果。</p>
                </div>
                <div class="area_img" id="62">
                    <p class="img_tit"><b>表1 制冷型红外图像的超分辨重建</b>SSIM<b>对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="62" border="1"><tr><td><br />算法</td><td>SC</td><td>SRCNN</td><td>VDSR</td><td>SPSR</td><td>MSR_T</td><td>MSR_L</td></tr><tr><td><br />1</td><td>0.835</td><td>0.837</td><td>0.842</td><td>0.842</td><td><b>0.846</b></td><td><b>0.847</b></td></tr><tr><td><br />2</td><td>0.905</td><td>0.922</td><td>0.939</td><td>0.942</td><td><b>0.943</b></td><td><b>0.944</b></td></tr><tr><td><br />3</td><td>0.825</td><td>0.858</td><td>0.861</td><td><b>0.872</b></td><td><b>0.872</b></td><td>0.871</td></tr><tr><td><br />4</td><td>0.757</td><td>0.753</td><td>0.784</td><td>0.786</td><td><b>0.785</b></td><td><b>0.790</b></td></tr><tr><td><br />5</td><td>0.757</td><td>0.780</td><td>0.798</td><td>0.802</td><td><b>0.808</b></td><td><b>0.810</b></td></tr><tr><td><br />6</td><td>0.822</td><td>0.836</td><td>0.838</td><td>0.835</td><td><b>0.840</b></td><td><b>0.843</b></td></tr><tr><td><br />平均</td><td>0.817</td><td>0.831</td><td>0.844</td><td>0.847</td><td><b>0.849</b></td><td><b>0.851</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="63">
                    <p class="img_tit"><b>表2 制冷型红外图像的超分辨重建</b>PSNR<b>对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="63" border="1"><tr><td><br />算法</td><td>SC</td><td>SRCNN</td><td>VDSR</td><td>SPSR</td><td>MSR_T</td><td>MSR_L</td></tr><tr><td><br />1</td><td>28.2</td><td>28.6</td><td>28.8</td><td>29.1</td><td>29.5</td><td><b>29.6</b></td></tr><tr><td><br />2</td><td>33.7</td><td>33.6</td><td>34.0</td><td>34.2</td><td><b>34.4</b></td><td><b>34.4</b></td></tr><tr><td><br />3</td><td>31.7</td><td>31.7</td><td>31.8</td><td>31.9</td><td>31.6</td><td><b>31.9</b></td></tr><tr><td><br />4</td><td>25.3</td><td>21.4</td><td>22.7</td><td>23.3</td><td>23.3</td><td><b>23.5</b></td></tr><tr><td><br />5</td><td>24.1</td><td>25.6</td><td>25.7</td><td>25.6</td><td>25.6</td><td><b>25.7</b></td></tr><tr><td><br />6</td><td>24.5</td><td>25.0</td><td>25.2</td><td>26.9</td><td>26.8</td><td><b>27.1</b></td></tr><tr><td><br />平均</td><td>27.9</td><td>27.7</td><td>28.0</td><td>28.5</td><td>28.5</td><td><b>28.7</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="64">从表1的数据可以看出, MSR_L和MSR_T算法明显优于稀疏SC算法, SRCNN和VDSR, 其中MSR_T的最低PSNR值都比SRCNN高了0.1 dB; SSIM值较ASDS提升了接近0.01, 较SRCNN提升了接近0.02。实验结果表明了本文方法的有效性。在SSIM和PSNR指标上, MSR_L较MSR_T方法略有提升, 其中MSR_L的平均PSNR比MSR_T的平均PSNR提升了接近0.2 dB, MSR_L的平均SSIM比MSR_T的平均SSIM提升了接近0.01。实验表明了本文方法的优越性, 充分说明采用2.4节损失函数的算法具有很大的灵活性, 可以适应更复杂的场景。</p>
                </div>
                <div class="p1">
                    <p id="65">由于低成本探测器获取红外图像没有基准高分辨图, 实验仅从重建的主观视觉效果进行分析。受限于篇幅, 本文只选取了1幅非制冷型红外图像与一幅制冷型红外图像进行定性对比, 如图3-图4所示。图3的图像是阿拉善戈壁的成像效果, 可以看出原始图像的分辨率较差, 不够清晰。图3 (a) 是利用冗余字典进行稀疏映射, 但重构出纹理不清晰, 细节比较模糊, 存在大量的伪影, 主要是因为红外图像纹理简单, 细节不够丰富, 重建易受噪声干扰。从重建结果可以看出来, 图3 (b) 的SRCNN算法得到的结果比较SC较好, 能够重建出工房屋檐, 阿拉善机场的塔楼边缘, 电线杆等明显边界的目标区域, 但对一些没有细节杂乱的区域, 重建质量模糊, 部分位置还存在平滑现象。SPSR和VDSR是两种基于深度残差学习的超分辨重建算法, 从实验结果可以看出, 这类算法得到的结果具有比较好的高频细节, 整体上恢复的具有明显的边缘, 但对一些精细的图像结构不能较好的重建。SPSR方法可以恢复许多精细的高频细节, 超分辨图像趋向于模糊边缘, 主要是直接采用高层次深度特征, 虽然能重建出整体结构, 但弱小细节表征不够。VDSR算法视觉效果有所提升, 在平滑区域存在一些划痕, 可能是恒定映射引入了一些噪声造成一些伪影现象。我们的算法重建结果具有较为锐利的边缘和比较清晰的纹理细度, 在视觉上具有最佳的视觉质量。MSR_L和MSR_T的算法类似, 恢复出来的图像视觉质量差异不大, 都比其他对比算法效果好, 仅仅在定性对比上, MSR_L比MSR_T的定量指标好一些。</p>
                </div>
                <div class="area_img" id="67">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201905039_06700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 非制冷红外图像超分辨重建对比图 (放大因子3)" src="Detail/GetImg?filename=images/JYRJ201905039_06700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 非制冷红外图像超分辨重建对比图 (放大因子3)   <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201905039_06700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="68">图4的红外图像是城市上空制冷型红外探测器的成像效果。从图3、图4对比可以看出, 中波制冷型红外图像比非制冷红外图像的细节丰富, 视觉效果好。因此, 超分辨算法对细节丰富的区域创建质量整体上较好, 在同等条件下也存在一些问题。图4 (a) 中楼顶区域存在一些盲元与噪声, 重建结果则放大了这种差异, 导致伪影严重。图4 (b) 、图4 (c) 和图4 (d) 三种超分辨算法的结果比稀疏重建较好, 图4 (b) 直接对插值后的图像重建, 存在梯度消失现象, 结果过于平滑。图4 (c) 和图4 (d) 引入了残差网络思想, 多达20层的网络使整个网络拥有更大的感受野, 根据更多的像素点去推测待重构像素点, 结果与真实更接近, 但弱小细节表征不够。本文提出的MSR_L, MSR_T方法通过一个较长的跳跃式策略进行恒等映射, 实现低层次特征和高级特征的级联, 能够同时表征各种复杂的重构场景, 能够生成具有丰富细节而清晰的高分辨红外图像, 同时也在客观定量评价上都有很大提高。</p>
                </div>
                <div class="area_img" id="70">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201905039_07000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 制冷型红外图像超分辨重建对比图 (放大因子3)" src="Detail/GetImg?filename=images/JYRJ201905039_07000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 制冷型红外图像超分辨重建对比图 (放大因子3)   <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201905039_07000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h3 id="71" name="71" class="anchor-tag"><b>4 结 语</b></h3>
                <div class="p1">
                    <p id="72">为了有效地对低成本探测器获取的红外图像进行超分辨重建, 提升红外图像重建质量, 增强边缘、纹理等细节信息, 进一步改善重建视觉效果, 本文提出一种基于跨连接的多层次深层网络的超分辨率重建算法。本文算法首先设计出了一个子结构模块;然后通过一个较长的跳跃式策略进行恒等映射, 实现低层次特征和高级特征的级联, 能够同时表征各种复杂的重构场景;最后采用强大而灵活的双参数损失函数来优化训练深度网络, 提高网路模型的泛化能力。仿真实验结果表明, 本文所提出的方法能够生成具有丰富细节而清晰的高分辨红外图像, 同时也在客观定量评价上都有很大提高。本文算法在GPU上运行, 大量减少了算法的运算时间, 但还不能实现实时的嵌入式处理。下一步将考虑对算法进行裁剪, 在不降低性能的条件下实现超分辨重建, 满足实时光电系统的需求。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=HWJS201711011&amp;v=MDQ3MDdUNmo1NE8zenFxQnRHRnJDVVI3cWZadVp0Rnl6a1VMM0xMVHJCZmJHNEg5Yk5ybzlFWllRS0RIODR2UjQ=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> 薛峰, 朱强, 林楠.基于低秩和邻域嵌入的单帧红外图像超分辨算法[J].红外技术, 2017, 39 (11) :1032-1037.
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=HWYJ201802004&amp;v=MDcwNTE0TzN6cXFCdEdGckNVUjdxZlp1WnRGeXprVUwzTExUclNaTEc0SDluTXJZOUZZSVFLREg4NHZSNFQ2ajU=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> 李方彪, 何昕, 魏仲慧, 等.生成式对抗神经网络的多帧红外图像超分辨率重建[J].红外与激光工程, 2018, 47 (2) :26-33.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning a deep convolutional network for image super-resolution">

                                <b>[3]</b> Dong C, Loy C C, He K, et al.Learning a Deep Convolutional Network for Image Super-Resolution[C]// European Conference on Computer Vision.Springer, Cham, 2014:184-199.
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image super-resolution using deep convolutional networks">

                                <b>[4]</b> Dong C, Loy C C, He K, et al.Image Super-Resolution Using Deep Convolutional Networks[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2016, 38 (2) :295-307.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network">

                                <b>[5]</b> Shi W, Caballero J, Huszar F, et al.Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network[C]// 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) .IEEE Computer Society, 2016:1874-1883.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZGTB201605003&amp;v=MjY1ODRiTEc0SDlmTXFvOUZaNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnRGeXprVUwzTFB5cmY=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b> 徐冉, 张俊格, 黄凯奇.利用双通道卷积神经网络的图像超分辨率算法[J].中国图象图形学报, 2016, 21 (5) :556-564.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Accurate image super-resolution using very deep convolutional networks">

                                <b>[7]</b> Kim J, Kwon Lee J, Mu Lee K.Accurate image super-resolution using very deep convolutional networks[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.2016:1646-1654.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Photo-realistic single image super-resolution using a genera-tive adversarial network">

                                <b>[8]</b> Ledig C, Theis L, Huszar F, et al.Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network[C]// 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) .IEEE, 2017:4681-4690.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Enhanced Deep Residual Networks for Single Image Super-Resolution">

                                <b>[9]</b> Lim B, Son S, Kim H, et al.Enhanced Deep Residual Networks for Single Image Super-Resolution[C]// 2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW) .IEEE Computer Society, 2017:136-144.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Going Deeper with Convolutions">

                                <b>[10]</b> Szegedy C, Liu W, Jia Y Q, et al.Going deeper with convolutions[C]// 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) .IEEE, 2015.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep residual learning for image recognition">

                                <b>[11]</b> He K, Zhang X, Ren S, et al.Deep Residual Learning for Image Recognition[C]// IEEE Conference on Computer Vision and Pattern Recognition.IEEE Computer Society, 2016:770-778.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Single-image super-resolution:a benchmark">

                                <b>[12]</b> Yang C Y, Ma C, Yang M H.Single-image super-resolution:a benchmark[M]// Lecture Notes in Computer Science.Heidelberg:Springer, 2014, 8692:372-386.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JYRJ201905039" />
        <input id="dpi" type="hidden" value="600" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201905039&amp;v=MjMyNzlMTHpUWlpMRzRIOWpNcW85R2JZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVadEZ5emtVTDM=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZKQ291S0FIT2hubWNrWXI0QkNHMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="1" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
