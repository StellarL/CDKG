<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637135556485315000%26DBCODE%3dCJFD%26TABLEName%3dCJFDTEMP%26FileName%3dJYRJ201910031%26RESULT%3d1%26SIGN%3dlz2G9NvsxV%252f3WxX3mZzii2r3Lb4%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JYRJ201910031&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JYRJ201910031&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201910031&amp;v=MTM0OTdyN0JMelRaWkxHNEg5ak5yNDlHWllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVp0RmlEaFY=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#45" data-title="&lt;b&gt;0 引 言&lt;/b&gt; "><b>0 引 言</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#51" data-title="&lt;b&gt;1&lt;/b&gt; β-&lt;b&gt;VAE工作原理&lt;/b&gt; "><b>1</b> β-<b>VAE工作原理</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#64" data-title="&lt;b&gt;2 注意力模块学习方法&lt;/b&gt; "><b>2 注意力模块学习方法</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#77" data-title="&lt;b&gt;3 VAE-ATTN小样本图像分类架构&lt;/b&gt; "><b>3 VAE-ATTN小样本图像分类架构</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#96" data-title="&lt;b&gt;4 实 验&lt;/b&gt; "><b>4 实 验</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#98" data-title="&lt;b&gt;4.1 实验设置&lt;/b&gt;"><b>4.1 实验设置</b></a></li>
                                                <li><a href="#102" data-title="&lt;b&gt;4.2 实验分析&lt;/b&gt;"><b>4.2 实验分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#121" data-title="&lt;b&gt;5 结 语&lt;/b&gt; "><b>5 结 语</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#53" data-title="图1 VAE的工作原理">图1 VAE的工作原理</a></li>
                                                <li><a href="#67" data-title="图2 注意力模型的网络结构以及相应特征的维度">图2 注意力模型的网络结构以及相应特征的维度</a></li>
                                                <li><a href="#82" data-title="图3 VAE-ATTN图像分类框架">图3 VAE-ATTN图像分类框架</a></li>
                                                <li><a href="#101" data-title="&lt;b&gt;表1 元学习参数表&lt;/b&gt;"><b>表1 元学习参数表</b></a></li>
                                                <li><a href="#149" data-title="图4 测试图像与重构图像">图4 测试图像与重构图像</a></li>
                                                <li><a href="#150" data-title="图5 特征通过t-SNE投影至2维空间的可视化结果">图5 特征通过t-SNE投影至2维空间的可视化结果</a></li>
                                                <li><a href="#114" data-title="&lt;b&gt;表2 Mini-ImageNet 小样本分类结果&lt;/b&gt;"><b>表2 Mini-ImageNet 小样本分类结果</b></a></li>
                                                <li><a href="#115" data-title="&lt;b&gt;表3 Omniglot小样本分类结果&lt;/b&gt;"><b>表3 Omniglot小样本分类结果</b></a></li>
                                                <li><a href="#117" data-title="续表3">续表3</a></li>
                                                <li><a href="#120" data-title="图6 Mini-ImageNet实验分类准确率对比">图6 Mini-ImageNet实验分类准确率对比</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" title=" Prodromidis A,Chan P,Stolfo S.Meta-learning in distributed data mining systems:Issues and approaches[J].Advances in distributed and parallel knowledge discovery,2000,3:81-114." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Meta-Learning in Distributed Data Mining Systems:Issues and Approaches">
                                        <b>[1]</b>
                                         Prodromidis A,Chan P,Stolfo S.Meta-learning in distributed data mining systems:Issues and approaches[J].Advances in distributed and parallel knowledge discovery,2000,3:81-114.
                                    </a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" title=" Santoro A,Bartunov S,Botvinick M,et al.One-shot learning with memory-augmented neural networks[EB].arXiv preprint arXiv:1605.06065,2016." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=One-shot learning with memory-augmented neural networks[EB]">
                                        <b>[2]</b>
                                         Santoro A,Bartunov S,Botvinick M,et al.One-shot learning with memory-augmented neural networks[EB].arXiv preprint arXiv:1605.06065,2016.
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title=" Graves A,Wayne G,Danihelka I.Neural turing machines[EB].arXiv preprint arXiv:1410.5401,2014." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Neural turing machines[EB]">
                                        <b>[3]</b>
                                         Graves A,Wayne G,Danihelka I.Neural turing machines[EB].arXiv preprint arXiv:1410.5401,2014.
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" title=" Andrychowicz M,Denil M,Gomez S,et al.Learning to learn by gradient descent by gradient descent[C]//Advances in Neural Information Processing Systems.2016:3981-3989." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning to learn by gradient descent by gradient descent">
                                        <b>[4]</b>
                                         Andrychowicz M,Denil M,Gomez S,et al.Learning to learn by gradient descent by gradient descent[C]//Advances in Neural Information Processing Systems.2016:3981-3989.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" title=" Ravi S,Larochelle H.Optimization as a Model for Few-Shot Learning[C]//ICLR 2017,2017." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=OPTIMIZATION AS A MODEL FOR FEW-SHOT LEARNING">
                                        <b>[5]</b>
                                         Ravi S,Larochelle H.Optimization as a Model for Few-Shot Learning[C]//ICLR 2017,2017.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" title=" Yang F S Y,Zhang L,Xiang T,et al.Learning to compare:Relation network for few-shot learning[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),Salt Lake City,UT,USA.2018." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning to compare:Relation network for few-shot learning">
                                        <b>[6]</b>
                                         Yang F S Y,Zhang L,Xiang T,et al.Learning to compare:Relation network for few-shot learning[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),Salt Lake City,UT,USA.2018.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" title=" Vinyals O,Blundell C,Lillicrap T,et al.Matching networks for one shot learning[C]//Advances in neural information processing systems.2016:3630-3638." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Matching networks for one shot learning">
                                        <b>[7]</b>
                                         Vinyals O,Blundell C,Lillicrap T,et al.Matching networks for one shot learning[C]//Advances in neural information processing systems.2016:3630-3638.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" title=" Finn C,Abbeel P,Levine S.Model-agnostic meta-learning for fast adaptation of deep networks[EB].arXiv preprint arXiv:1703.03400,2017." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Model-agnostic meta-learning for fast adaptation of deep networks[EB]">
                                        <b>[8]</b>
                                         Finn C,Abbeel P,Levine S.Model-agnostic meta-learning for fast adaptation of deep networks[EB].arXiv preprint arXiv:1703.03400,2017.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" title=" Munkhdalai T,Yu H.Meta networks[EB].arXiv preprint arXiv:1703.00837,2017." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Meta networks[EB]">
                                        <b>[9]</b>
                                         Munkhdalai T,Yu H.Meta networks[EB].arXiv preprint arXiv:1703.00837,2017.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_10" title=" Nichol A,Schulman J.Reptile:a Scalable Metalearning Algorithm[EB].arXiv preprint arXiv:1803.02999,2018." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Reptile:a Scalable Metalearning Algorithm[EB]">
                                        <b>[10]</b>
                                         Nichol A,Schulman J.Reptile:a Scalable Metalearning Algorithm[EB].arXiv preprint arXiv:1803.02999,2018.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_11" title=" Kingma D P,Welling M.Auto-encoding variational bayes[EB].arXiv preprint arXiv:1312.6114,2013." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Auto-encoding variational bayes[EB]">
                                        <b>[11]</b>
                                         Kingma D P,Welling M.Auto-encoding variational bayes[EB].arXiv preprint arXiv:1312.6114,2013.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_12" title=" Pu Y,Gan Z,Henao R,et al.Variational autoencoder for deep learning of images,labels and captions[C]//Advances in neural information processing systems.2016:2352-2360." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Variational autoencoder for deep learning of images,labels and captions">
                                        <b>[12]</b>
                                         Pu Y,Gan Z,Henao R,et al.Variational autoencoder for deep learning of images,labels and captions[C]//Advances in neural information processing systems.2016:2352-2360.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_13" title=" Higgins I,Matthey L,Glorot X,et al.Early visual concept learning with unsupervised deep learning[EB].arXiv preprint arXiv:1606.05579,2016." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Early visual concept learning with unsupervised deep learning[EB]">
                                        <b>[13]</b>
                                         Higgins I,Matthey L,Glorot X,et al.Early visual concept learning with unsupervised deep learning[EB].arXiv preprint arXiv:1606.05579,2016.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_14" title=" Burgess C P,Higgins I,Pal A,et al.Understanding disentangling in β-VAE[EB].arXiv preprint arXiv:1804.03599,2018." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Understanding disentangling in β-VAE[EB]">
                                        <b>[14]</b>
                                         Burgess C P,Higgins I,Pal A,et al.Understanding disentangling in β-VAE[EB].arXiv preprint arXiv:1804.03599,2018.
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_15" title=" Noh H,Kim T,Mun J,et al.Transfer learning via unsupervised task discovery for visual question answering[EB].arXiv preprint arXiv:1810.02358,2018." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Transfer learning via unsupervised task discovery for visual question answering[EB]">
                                        <b>[15]</b>
                                         Noh H,Kim T,Mun J,et al.Transfer learning via unsupervised task discovery for visual question answering[EB].arXiv preprint arXiv:1810.02358,2018.
                                    </a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_16" title=" Lake B,Salakhutdinov R,Gross J,et al.One shot learning of simple visual concepts[C]//Proceedings of the Annual Meeting of the Cognitive Science Society.2011." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=One shot learning of simple visual concepts">
                                        <b>[16]</b>
                                         Lake B,Salakhutdinov R,Gross J,et al.One shot learning of simple visual concepts[C]//Proceedings of the Annual Meeting of the Cognitive Science Society.2011.
                                    </a>
                                </li>
                                <li id="35">


                                    <a id="bibliography_17" title=" Bertinetto L,Valmadre J,Henriques J F,et al.Fully-convolutional siamese networks for object tracking[C]//European conference on computer vision.Springer,Cham,2016:850-865." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fully-convolutional siamese networks for object tracking">
                                        <b>[17]</b>
                                         Bertinetto L,Valmadre J,Henriques J F,et al.Fully-convolutional siamese networks for object tracking[C]//European conference on computer vision.Springer,Cham,2016:850-865.
                                    </a>
                                </li>
                                <li id="37">


                                    <a id="bibliography_18" title=" Mnih V,Heess N,Graves A,et al.Recurrent models of visual attention[C]//Proceedings of the 27th International Conference on Neural Information Processing Systems—Volume 2.MIT Press,2014:2204-2212." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Recurrent models of visual attention">
                                        <b>[18]</b>
                                         Mnih V,Heess N,Graves A,et al.Recurrent models of visual attention[C]//Proceedings of the 27th International Conference on Neural Information Processing Systems—Volume 2.MIT Press,2014:2204-2212.
                                    </a>
                                </li>
                                <li id="39">


                                    <a id="bibliography_19" title=" Vaswani A,Shazeer N,Parmar N,et al.Attention is all you need[EB].eprint arXiv:1706.03762,2017." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Attention is all you need[EB]">
                                        <b>[19]</b>
                                         Vaswani A,Shazeer N,Parmar N,et al.Attention is all you need[EB].eprint arXiv:1706.03762,2017.
                                    </a>
                                </li>
                                <li id="41">


                                    <a id="bibliography_20" title=" van der Maaten L J P,Hinton G E.Visualizing High-Dimensional Data Using t-SNE[J].Journal of Machine Learning Research,2008,9:2579-2605." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Visualizing high-dimensional data using t-sne">
                                        <b>[20]</b>
                                         van der Maaten L J P,Hinton G E.Visualizing High-Dimensional Data Using t-SNE[J].Journal of Machine Learning Research,2008,9:2579-2605.
                                    </a>
                                </li>
                                <li id="43">


                                    <a id="bibliography_21" title=" Bengio Y,Courville A,Vincent P.Representation Learning:A Review and New Perspectives[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2013,35(8):1798-1828." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Representation Learning A Review and New Perspectives">
                                        <b>[21]</b>
                                         Bengio Y,Courville A,Vincent P.Representation Learning:A Review and New Perspectives[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2013,35(8):1798-1828.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JYRJ" target="_blank">计算机应用与软件</a>
                2019,36(10),168-174 DOI:10.3969/j.issn.1000-386x.2019.10.030            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于VAE和注意力机制的小样本图像分类方法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%83%91%E6%AC%A3%E6%82%A6&amp;code=42911570&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">郑欣悦</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%BB%84%E6%B0%B8%E8%BE%89&amp;code=42911571&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">黄永辉</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E7%A7%91%E5%AD%A6%E9%99%A2%E5%9B%BD%E5%AE%B6%E7%A9%BA%E9%97%B4%E7%A7%91%E5%AD%A6%E4%B8%AD%E5%BF%83%E5%A4%8D%E6%9D%82%E8%88%AA%E5%A4%A9%E7%B3%BB%E7%BB%9F%E7%94%B5%E5%AD%90%E4%BF%A1%E6%81%AF%E6%8A%80%E6%9C%AF%E9%87%8D%E7%82%B9%E5%AE%9E%E9%AA%8C%E5%AE%A4&amp;code=1697354&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国科学院国家空间科学中心复杂航天系统电子信息技术重点实验室</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E7%A7%91%E5%AD%A6%E9%99%A2%E5%A4%A7%E5%AD%A6&amp;code=1698842&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国科学院大学</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>小样本图像识别是人工智能中具有挑战性的新兴领域。传统的深度学习方法无法解决样本匮乏带来的问题,模型易出现过拟合导致训练效果不佳的情况。针对以上问题,提出结合表征学习和注意力机制的小样本学习方法。通过预训练VAE(Variational Auto-encoder)从任务中学习丰富的隐特征;对提取出的隐特征构建注意力机制,使得元学习器能快速地注意到对当前任务重要的特征;将注意力模块增强之后的特征使用分类器进行图像分类。实验表明,该算法在Mini-ImageNet和Omniglot数据集上达到72.5%和98.8%的准确率,显著优于现有元学习算法的性能。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%B0%8F%E6%A0%B7%E6%9C%AC%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">小样本学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%85%83%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">元学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">注意力机制;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">图像分类;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    郑欣悦，硕士，主研领域:图像识别，神经网络，迁移学习。;
                                </span>
                                <span>
                                    黄永辉，研究员。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-02-26</p>

                    <p>

                            <b>基金：</b>
                                                        <span>中国科学院复杂航天系统电子信息技术重点实验室自主部署基金课题(Y42613A32S);</span>
                    </p>
            </div>
                    <h1><b>FEW-SHOT IMAGE CLASSIFICATION BASED ON VAE AND ATTENTION MECHANISM</b></h1>
                    <h2>
                    <span>Zheng Xinyue</span>
                    <span>Huang Yonghui</span>
            </h2>
                    <h2>
                    <span>Key Laboratory of Electronics and Information Technology for Space System,National Space Science Center,Chinese Academy of Science</span>
                    <span>University of Chinese Academy of Sciences</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Few-shot learning is a challenging emerging field of artificial intelligence. Traditional deep learning models cannot solve the problem caused by the lack of samples, and the model is prone to over-fitting which leads to poor performance. In order to solve this problem, this paper proposed a few-shot learning which combined representation learning with attention mechanism. We pre-trained the VAE(variational autoencoder) to learn the rich latent features from different tasks. Then, the attention mechanism was constructed for the extracted latent features so that the meta-learner could quickly notice the key features for current learning task. Finally, the feature augmented by attention model was used to classify the image using classifier. The experimental results show that the proposed method achieves 72.5% and 98.8% accuracy on the Mini-ImageNet and Omniglot datasets respectively, which significantly surpasses the existing meta-learning algorithms.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Few-shot%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Few-shot learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Meta-learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Meta-learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Attention%20mechanism&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Attention mechanism;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Image%20classification&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Image classification;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2019-02-26</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="45" name="45" class="anchor-tag"><b>0 引 言</b></h3>
                <div class="p1">
                    <p id="46">近年来,人工智能技术研究飞速发展,深度学习算法已在图像识别领域取得了突破性的进展,但算法也逐渐显露出泛化能力差、所需训练数据大等缺点。目前,以CNN为基础的图像识别方法通常需要海量的训练数据和充足的迭代次数,才可对特定的图像类别进行精准的分类。然而,实际应用中研究者常面临数据稀缺的情况,比如罕见物种图片、珍贵的医疗诊断图片、稀有遥感图像等,采集这些数据的难度大且成本高。而少量的样本通常不足以训练出一个较好的深度神经网络。因此,如何实现小样本图像识别成为了计算机视觉领域的重要研究方向。</p>
                </div>
                <div class="p1">
                    <p id="47">针对小样本学习问题,深度学习领域存在着许多不同的解决方案,其中元学习方法取得了尤为显著的成效。元学习(Meta-learning)是指导分类器学会如何学习的过程。元学习器在有限的样例中对结构基础层次和参数空间进行优化,以获得跨任务泛化性能<citation id="128" type="reference"><link href="3" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>,具备小样本学习的能力。训练完成的元学习器可以仅根据1至5个输入-输出样例对新的测试样本进行分类。</p>
                </div>
                <div class="p1">
                    <p id="48">目前元学习的方法可以归类为以下几种:基于记忆存储的方法<citation id="134" type="reference"><link href="5" rel="bibliography" /><link href="19" rel="bibliography" /><sup>[<a class="sup">2</a>,<a class="sup">9</a>]</sup></citation>通过权重更新来调整偏差,并不断地从记忆中学习。Santoro等<citation id="135" type="reference"><link href="5" rel="bibliography" /><link href="7" rel="bibliography" /><sup>[<a class="sup">2</a>,<a class="sup">3</a>]</sup></citation>利用神经图灵机引入的外部存储器来实现短期记忆并在标签和输入图像之间建立连接,使输入能够与存储器中的相关图像进行比较,以实现更好的预测。基于梯度的方法<citation id="136" type="reference"><link href="9" rel="bibliography" /><link href="11" rel="bibliography" /><sup>[<a class="sup">4</a>,<a class="sup">5</a>]</sup></citation>通常通过训练额外的网络来预测分类器更新策略,如Larochelle等<citation id="129" type="reference"><link href="11" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>提出训练LSTM优化器以学习分类器网络的参数优化规则。关系网络<citation id="130" type="reference"><link href="13" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>和匹配网络<citation id="131" type="reference"><link href="15" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>采用度量学习的思想,不再使用人工设计的指标,而是完全利用神经网络来学习深度距离度量。Finn等<citation id="132" type="reference"><link href="17" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>提出了一种称为MAML模型无关的元学习方法。该方法的基本思想是同时启动多个任务,然后获取不同学习任务的合成梯度方向来更新神经网络。这样的优化方式能找到最适合网络的初始化位置,这里的初始化位置被定义为:仅通过几个小样本的训练可以调整到最好表现。Reptile<citation id="133" type="reference"><link href="21" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>是OpenAI提出的简化版MAML算法,MAML需要在反向传播中计算二阶导数,而Reptile只需要计算一阶导数,消耗更少的计算资源且更易于实现。</p>
                </div>
                <div class="p1">
                    <p id="49">虽然上述方法取得了令人瞩目的成果,但普遍存在两个缺陷:一是算法引入人为设计的规则来约束学习;二是需要更多额外的存储空间对经验进行存储,并且没有提供将知识转移到其他任务的理论手段。因此本文提出一种结合表征学习和注意力机制<citation id="137" type="reference"><link href="37" rel="bibliography" /><link href="39" rel="bibliography" /><sup>[<a class="sup">18</a>,<a class="sup">19</a>]</sup></citation>的元学习方法VAE-ATTN。表征模块利用过去的知识,将高维图像数据表达为有意义的高级表征;注意力模块引导学习器关注关键特征,以快速适应新的学习任务。</p>
                </div>
                <div class="p1">
                    <p id="50">VAE-ATTN算法首先运用变分自编码器VAE<citation id="138" type="reference"><link href="23" rel="bibliography" /><link href="25" rel="bibliography" /><sup>[<a class="sup">11</a>,<a class="sup">12</a>]</sup></citation>通过无监督学习方法获取各个任务内部共享的特征。VAE的编码器保留预训练后的网络模型参数,将提取的低维高级表征迁移到不同的识别任务中。同时,在通道维度引入注意力机制,通过计算概率分布选择性加强对当前学习任务更重要的特征。本文使用Reptile元学习算法作为基准算法。实验结果表明,VAE-ATTN算法整体性能优于MAML、MatchingNets、Meta-LSTM等对比算法,验证了有效的表征学习和注意力机制的结合能获得更加精准的小样本分类结果。</p>
                </div>
                <h3 id="51" name="51" class="anchor-tag"><b>1</b> β-<b>VAE工作原理</b></h3>
                <div class="p1">
                    <p id="52">本文使用变分自编码器进行表征学习,表征学习的目标是从数据中自动学习到从原始数据到数据表征之间的映射。VAE作为深度神经网络,由编码器和解码器构成。如图1所示,VAE本质是提取数据的隐特征,构建从隐特征到生成目标的模型。编码器从原始数据中提取潜在的合理变量,再对编码结果加上高斯噪声加以约束,使之成为服从高斯分布的隐含特征。解码器构建的模型将隐特征映射到重新生成的概率分布中,重构的分布需尽量与原始分布相同。</p>
                </div>
                <div class="area_img" id="53">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201910031_053.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 VAE的工作原理" src="Detail/GetImg?filename=images/JYRJ201910031_053.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 VAE的工作原理  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201910031_053.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="54">网络有两个组件:具有参数ϕ的编码器网络<i>E</i>和具有参数<i>θ</i>的解码器<i>D</i>,其损失函数为:</p>
                </div>
                <div class="p1">
                    <p id="55"><i>L</i>(ϕ,<i>θ</i>,<i>x</i>)=<i>E</i><sub><i>q</i></sub><sub>ϕ(</sub><sub><i>z</i></sub><sub>|</sub><sub><i>x</i></sub><sub>)</sub>[log <i>p</i><sub><i>θ</i></sub>(<i>x</i>|<i>z</i>)]-<i>D</i><sub><i>KL</i></sub>(<i>q</i><sub>ϕ</sub>(<i>z</i>|<i>x</i>)‖<i>p</i><sub><i>θ</i></sub>(<i>z</i>))(1)</p>
                </div>
                <div class="p1">
                    <p id="57">式中:<i>q</i><sub>ϕ</sub>(<i>z</i>|<i>x</i>)表示从数据空间到隐含空间的编码器;<i>p</i><sub><i>θ</i></sub>(<i>x</i>|<i>z</i>)表示从隐含空间到数据空间的解码器。</p>
                </div>
                <div class="p1">
                    <p id="58">损失函数由两方面构成:式(1)第一项为重构误差,驱使重构的<i>p</i><sub><i>θ</i></sub>(<i>x</i>|<i>z</i>)分布更接近于输入分布<i>p</i><sub><i>θ</i></sub>(<i>x</i>);第二项旨在减小KL散度,驱使<i>q</i><sub>ϕ</sub>(<i>z</i>|<i>x</i>)更接近于先验分布<i>p</i><sub><i>θ</i></sub>(<i>z</i>)。为了实现这种重构,VAE将捕捉到可以代表原始输入数据的最重要的特征因素。</p>
                </div>
                <div class="p1">
                    <p id="59">特别地,我们尝试用VAE的变体β-VAE<citation id="140" type="reference"><link href="27" rel="bibliography" /><link href="29" rel="bibliography" /><link href="31" rel="bibliography" /><sup>[<a class="sup">13</a>,<a class="sup">14</a>,<a class="sup">15</a>]</sup></citation>进行实验。β-VAE引入解缠性先验<citation id="139" type="reference"><link href="43" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>,假设数据是基于互相独立的因素生成的,因此可以用表征中不同的独立变量表示这些因素。该解缠性先验可促进编码器学习数据简洁的抽象表示,从而用于各种下游任务并提升样本效率。</p>
                </div>
                <div class="p1">
                    <p id="60">如式(2)所示,β-VAE引入了一个可调节的超参数<i>β</i>,它可控制隐变量的维度以及重建精度之间的平衡,同时高斯先验的各向同性性质也给学习的后验带来了隐形的约束。<i>β</i>变化会改变训练期间学习程度,从而鼓励不同的学习表征,实验中需要调整的值以促进使用解缠后的表征。</p>
                </div>
                <div class="p1">
                    <p id="61"><i>L</i>(ϕ,<i>θ</i>,<i>x</i>)=<i>E</i><sub><i>q</i></sub><sub>ϕ(</sub><sub><i>z</i></sub><sub>|</sub><sub><i>x</i></sub><sub>)</sub>[log <i>p</i><sub><i>θ</i></sub>(<i>x</i>|<i>z</i>)]-<i>βD</i><sub><i>KL</i></sub>(<i>q</i><sub>ϕ</sub>(<i>z</i>|<i>x</i>)‖<i>p</i><sub><i>θ</i></sub>(<i>z</i>))(2)</p>
                </div>
                <div class="p1">
                    <p id="63">VAE的无监督学习阶段需要编码器对输入数据降维,并从中提取通用而高级的表征,以适用于小样本学习中不同图像类别的一系列任务分布。从元学习的角度处理这个问题,将目标定义为一个有效的学习过程,可以从无标记数据转移到少标记样本的任务。</p>
                </div>
                <h3 id="64" name="64" class="anchor-tag"><b>2 注意力模块学习方法</b></h3>
                <div class="p1">
                    <p id="65">Bengio等<citation id="141" type="reference"><link href="43" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>提出具有适合特定任务和数据域的表征可以显著提高训练模型的学习成功率和稳健性。因此,本文对VAE提取的高级表征构建注意力机制,使元学习器能在全局信息中关注更有利于当前学习任务的目标表征。自注意机制与人类视觉注意力机制起着类似的作用,从大量的信息中筛选出部分关键的信息,并聚焦到这些重要的信息上。</p>
                </div>
                <div class="p1">
                    <p id="66">图2阐述了注意力模型的内部结构。该模块通过分析输入数据的总特征,捕获通道间依赖关系,预测通道重要性,以此选择性地强调某些特征。</p>
                </div>
                <div class="area_img" id="67">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201910031_067.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 注意力模型的网络结构以及相应特征的维度" src="Detail/GetImg?filename=images/JYRJ201910031_067.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 注意力模型的网络结构以及相应特征的维度  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201910031_067.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="68">根据预训练过编码器产生的隐特征<i>γ</i>构建注意力模块的输入, <i>γ</i>∈<b>R</b><sup><i>b</i></sup><sup>×</sup><sup><i>h</i></sup><sup>×</sup><sup><i>w</i></sup><sup>×</sup><sup><i>c</i></sup>,其中<i>b</i>为批大小(Batch size),<i>h</i>和<i>w</i>为特征图的长和宽,<i>c</i>是通道数。由式(3)-式(6)所示,<i>Q</i>和<i>K</i>由输入特征<i>γ</i>通过1×1卷积的跨通道信息整合而得的新的特征图,并将维度变换为<b>R</b><sup><i>x</i></sup><sup>×</sup><sup><i>c</i></sup>,其中<i>x</i>=<i>h</i>×<i>w</i>,接着在<i>Q</i>和<i>K</i>的转置之间执行矩阵乘法,最后使用softmax函数进行归一化,得到维度为<i>c</i>×<i>c</i>注意力概率分布<i>α</i><sub><i>ji</i></sub>。这样设计的意义在于计算<i>γ</i>的每个通道数之间的影响力权重,可以突出关键特征图的作用,减少冗余特征对整体分类性能的影响。</p>
                </div>
                <div class="p1">
                    <p id="69"><i>Q</i>=<i>reshape</i>(<i>F</i><sub>CNN</sub>(<i>γ</i>;<i>θ</i><sub>1</sub>))      (3)</p>
                </div>
                <div class="p1">
                    <p id="70"><i>K</i>=<i>reshape</i>(<i>F</i><sub>CNN</sub>(<i>γ</i>;<i>θ</i><sub>2</sub>))      (4)</p>
                </div>
                <div class="p1">
                    <p id="71"><i>V</i>=<i>reshape</i>(<i>γ</i>)      (5)</p>
                </div>
                <div class="p1">
                    <p id="72"><mathml id="124"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>α</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mfrac><mrow><mi>exp</mi><mo stretchy="false">(</mo><mi>Q</mi><msub><mrow></mrow><mi>i</mi></msub><mo>⋅</mo><mi>Κ</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>C</mi></munderover><mtext>e</mtext></mstyle><mtext>x</mtext><mtext>p</mtext><mo stretchy="false">(</mo><mi>Q</mi><msub><mrow></mrow><mi>i</mi></msub><mo>⋅</mo><mi>Κ</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo></mrow></mfrac></mrow></math></mathml>      (6)</p>
                </div>
                <div class="p1">
                    <p id="73">最后,将权重系数<i>α</i><sub><i>ij</i></sub>与原始特征进行加权求和,再用尺度系数<i>β</i>加以调整,即可获得辨别性高的特征表达<i>O</i><sub><i>j</i></sub>:</p>
                </div>
                <div class="p1">
                    <p id="74"><mathml id="125"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ο</mi><msub><mrow></mrow><mi>j</mi></msub><mo>=</mo><mi>β</mi><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>C</mi></munderover><mo stretchy="false">(</mo></mstyle><mi>α</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>⋅</mo><mi>V</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo>+</mo><mi>γ</mi><msub><mrow></mrow><mi>j</mi></msub></mrow></math></mathml>      (7)</p>
                </div>
                <div class="p1">
                    <p id="75">其中:<i>β</i>初始化为0,在学习的过程中逐渐分配到更大的权重。</p>
                </div>
                <div class="p1">
                    <p id="76">该注意力模块能自适应地整合局部特征并明确全局依赖,使得元学习器能注意到更有用的特征,在样本匮乏的情况下出色地完成分类工作。</p>
                </div>
                <h3 id="77" name="77" class="anchor-tag"><b>3 VAE-ATTN小样本图像分类架构</b></h3>
                <div class="p1">
                    <p id="78">针对传统深度学习方法的局限性,VAE-ATTN提供了很好的解决方案。VAE-ATTN提出通过预训练VAE学习任务高级表征,混合使用注意力机制快速运用关键表征的方法,最大化从少量样本中获取的有效信息。</p>
                </div>
                <div class="p1">
                    <p id="79">方法分为两个阶段,第一阶段为表征模块的预训练。算法使用深度生成模型VAE构建一个提供数据嵌入或特征表征的模型。预训练集由大规模图像分类数据集ImageNet上随机抽取的150个类组成,这些类别和元数据集中的类别没有重叠。VAE从预训练集中学习各个图像类别共享的特征子集。特别地,实验尝试使用β-VAE作为表征模块,相比于线性嵌入或从常规变分自编码器获得的特征,β-VAE能够提取解缠的特征,具有更加有效的表征能力。</p>
                </div>
                <div class="p1">
                    <p id="80">第二阶段为元学习阶段。将预训练完成的VAE编码器,作为特征提取器迁移至新的识别任务中。VAE输出的通道响应彼此关联,每个通道映射可以被视作特定于类别的响应。因此对VAE的输出特征引入注意力机制,利用通道映射之间的相互依赖性,选择性地强调相互依赖的特征映射,并改进特定类别的特征表示。本文使用的基准元学习算法为模型无关的Reptile元学习方法,Reptile掌握任务分布规律,从特征空间和参数空间对元学习器进行联合优化。</p>
                </div>
                <div class="p1">
                    <p id="81">图3为基于VAE和注意力机制的元学习图像分类架构。编码器是深度为4的卷积网络,解码器由4层反卷积构成。对编码器提取的特征输入注意力模块,进行特征加强。最后通过由全连接层和Softmax层组成的分类器,得到图像分类成果。这样的结构即保留了抽象的图像特征,又为在面临新任务的学习时保留了调整的余地。算法运行的伪代码如算法1所示。</p>
                </div>
                <div class="area_img" id="82">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201910031_082.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 VAE-ATTN图像分类框架" src="Detail/GetImg?filename=images/JYRJ201910031_082.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 VAE-ATTN图像分类框架  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201910031_082.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="83"><b>算法1</b> VAE-ATTN元学习算法</p>
                </div>
                <div class="p1">
                    <p id="84">1 预训练VAE模型,重复步骤1)-步骤2)直至图像重构误差小于<i>σ</i>:</p>
                </div>
                <div class="p1">
                    <p id="85">1) 从预训练集中采样<i>n</i>张图片<i>P</i>(0)～<i>P</i>(<i>n</i>-1);</p>
                </div>
                <div class="p1">
                    <p id="86">2) 在每幅图像上执行随机梯度下降,优化网络编码器参数ϕ和解码器参数<i>θ</i>。</p>
                </div>
                <div class="p1">
                    <p id="87">2 将预训练好的编码器的参数值ϕ固定,连接Attention模块。</p>
                </div>
                <div class="p1">
                    <p id="88">3 Attention模块参数A在元数据集上通过Reptile算法进行训练以学会强调关键的特征图,步骤1)-步骤3)预定义的<i>J</i>次:</p>
                </div>
                <div class="p1">
                    <p id="89">1) 从元数据集中采样<i>n</i>个任务<i>τ</i>(0)～<i>τ</i>(<i>n</i>-1);</p>
                </div>
                <div class="p1">
                    <p id="90">2) 在每个任务<i>τ</i><sub><i>i</i></sub>上执行连续<i>k</i>步梯度下降,计算权值<i>W</i><sub><i>i</i></sub>=<i>SGD</i>(<i>L</i><sub><i>τi</i></sub><sub>,</sub><sub><i>k</i></sub><sub>,</sub><sub><i>A</i></sub>);</p>
                </div>
                <div class="p1">
                    <p id="91">3) 对所有任务更新后的权值取加权平均<mathml id="126"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mn>1</mn><mi>k</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mo stretchy="false">(</mo></mstyle><mi>W</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mi>A</mi><mo stretchy="false">)</mo></mrow></math></mathml>,优化注意力模块参数<i>A</i>。</p>
                </div>
                <div class="p1">
                    <p id="92">4 在测试集上验证模型,获得最终准确率。</p>
                </div>
                <div class="p1">
                    <p id="93">Reptile<citation id="142" type="reference"><link href="21" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>作为基准元学习算法,本质上是通过不断地采样不同类别的任务,在任务层面实现知识的泛化。算法的优化目标如下:</p>
                </div>
                <div class="p1">
                    <p id="94"><mathml id="127"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mi>ψ</mi></munder><mstyle displaystyle="true"><munder><mo>∑</mo><mi>τ</mi></munder><mo stretchy="false">(</mo></mstyle><msup><mi>D</mi><mo>′</mo></msup><msub><mrow></mrow><mi>τ</mi></msub><mo>,</mo><msup><mi>ψ</mi><mo>′</mo></msup><msub><mrow></mrow><mi>τ</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>τ</mi></munder><mi>L</mi></mstyle><mo stretchy="false">(</mo><msup><mi>D</mi><mo>′</mo></msup><msub><mrow></mrow><mi>τ</mi></msub><mo>,</mo><mi>Τ</mi><mo stretchy="false">(</mo><mi>D</mi><msub><mrow></mrow><mi>τ</mi></msub><mo>,</mo><mi>ψ</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow></math></mathml>      (8)</p>
                </div>
                <div class="p1">
                    <p id="95">式中:<i>D</i><sub><i>τ</i></sub>和<i>D</i>′<sub><i>τ</i></sub>分别代表任务<i>τ</i>上的训练集和测试集;<i>T</i>(<i>D</i><sub><i>τ</i></sub>,<i>ψ</i>)是学习器在<i>D</i><sub><i>τ</i></sub>上的训练过程;<i>L</i>(<i>D</i>′<sub><i>τ</i></sub>,<i>ψ</i>′<sub><i>τ</i></sub>)是在通过测试样本<i>D</i>′<sub><i>τ</i></sub>更新后网络参数<i>ψ</i>′<sub><i>τ</i></sub>上计算的交叉损失。这样的优化方式是将每个任务都视为一个独立的学习问题,在训练过程中不仅关注数据层面,而且学习任务分布规律,从而实现对新任务的快速适应。</p>
                </div>
                <h3 id="96" name="96" class="anchor-tag"><b>4 实 验</b></h3>
                <div class="p1">
                    <p id="97">为了验证基于VAE和注意力机制的元学习方法的有效性,实验选取两个重要的基准数据集Mini-ImageNet和Omniglot进行实验,并将测试结果与其他元学习方法进行比较。Omniglot<citation id="143" type="reference"><link href="33" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>是Lake等提出的语言文字数据集,该数据集包含50种文字,1 623类手写字符,每一类字符仅拥有20个样本,且这些样本均为不同的人绘制而成。Mini-ImageNet<citation id="144" type="reference"><link href="15" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>数据集由DeepMind于2016年提出,是计算机视觉领域的重要基准数据集,它通过从ImageNet随机抽样100个类并为每个类选择600个样本创建而成。其中:训练集包含64个类别,共计38 400幅图像;测试集包含20个类别,共计12 000幅图像;验证集包含16个类,9 600张图像。</p>
                </div>
                <h4 class="anchor-tag" id="98" name="98"><b>4.1 实验设置</b></h4>
                <div class="p1">
                    <p id="99">预训练阶段:变分自编码器从原始的,未标记的预训练集数据中进行学习。从ImageNet中随机抽取150类,每类600张图片组成预训练集。预训练集没有与Mini-ImageNet数据集中的类别重叠。在β-VAE训练阶段,本文采用Adam优化器,固定学习率为0.001。编码器模型运用4层CNN卷积层,每层使用64个大小为3×3的卷积核,输出为100维的隐变量。损失函数一方面通过交叉熵来度量图片的重构误差,另一方面,通过KL散度来度量隐变量的分布和单位高斯分布的差异。根据损失函数的收敛特性,本文选取的批大小为32,以获得随机性避免陷入局部最优化。</p>
                </div>
                <div class="p1">
                    <p id="100">元学习阶段:网络运用训练集中有标记的,训练集数据样本进行学习。在预训练阶段之后,β-VAE已经从预训练集中学习了低维的高级特征,元学习器只需要通过快速调整其注意力模块来学习如何适应新的学习任务。网络使用Reptile算法对注意力模块进行2万次的训练迭代,每次连续计算8步梯度下降来更新网络参数,详细超参设置见表1。</p>
                </div>
                <div class="area_img" id="101">
                    <p class="img_tit"><b>表1 元学习参数表</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="101" border="1"><tr><td><br />参数</td><td>5-shot 5-way<br />5-shot 20-way</td><td>1-shot 5-way<br />1-shot 20-way</td></tr><tr><td><br />Adam learning rate</td><td>0.005</td><td>0.003</td></tr><tr><td><br />Outer iterations</td><td>20 000</td><td>20 000</td></tr><tr><td><br />Outer step size</td><td>1</td><td>1</td></tr><tr><td><br />Meta batch size</td><td>5</td><td>5</td></tr><tr><td><br />Inner iterations</td><td>8</td><td>8</td></tr><tr><td><br />Inner batch size</td><td>10</td><td>10</td></tr><tr><td><br />Train shots</td><td>15</td><td>15</td></tr><tr><td><br />Eval.inner iterations</td><td>88</td><td>50</td></tr><tr><td><br />Eval.inner batch size</td><td>10</td><td>5</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="102" name="102"><b>4.2 实验分析</b></h4>
                <div class="p1">
                    <p id="103">实验考虑解决小样本分类中K-样本,N-类别<citation id="145" type="reference"><link href="15" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>学习问题。对于K-样本,N-类别(K-shot,N-way)分类的每个任务,学习器训练<i>N</i>个相关类,每个类都有<i>K</i>个例子,首先从元数据集中采样<i>N</i>个类,为每个类选择<i>K</i>+1个样本。然后,将这些示例拆分为训练和测试集,其中训练集包含每个类的<i>K</i>个示例,测试集包含剩余样本。以5-样本,5-类别分类为例,实验中共抽取30个样例,使用其中25个样本5(图像)×5(类)训练学习器并使用剩余的示例来测试模型。</p>
                </div>
                <h4 class="anchor-tag" id="104" name="104"><b>4.2.1</b> β-<b>VAE的重构分析</b></h4>
                <div class="p1">
                    <p id="105">对于无监督学习阶段,实验考察了<i>β</i>参数对提取解缠特征的影响。实验发现<i>β</i>=8是对于最终学习器进行小样本分类的最合适的参数值,实验中大约一半的隐变量已经收敛到单位高斯先验。如图4所示,(a)为测试图片,(b)为<i>β</i>=8时的β-VAE重构图像。从图像重建的质量上分析,由于隐变量的维度受限,良好的解缠表征可能会导致模糊的重建<citation id="146" type="reference"><link href="25" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>。但解缠表征例如旋转、大小、位置等有助于加速后期元学习阶段的学习,帮助注意力模块理解不同任务之间的共享特征,对提升小样本分类性能有更明显的成效。</p>
                </div>
                <div class="area_img" id="149">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201910031_14900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 测试图像与重构图像" src="Detail/GetImg?filename=images/JYRJ201910031_14900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 测试图像与重构图像  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201910031_14900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="108" name="108"><b>4.2.2 注意力影响可视化分析</b></h4>
                <div class="p1">
                    <p id="109">该部分实验成果可视化了注意力机制给小样本分类带来的影响。实验使用t-SNE算法<citation id="147" type="reference"><link href="41" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>将网络输出的特征值降维并投影至2维空间。图5是Mini-ImageNet实验中测试场景的特征可视化图,(a)为特征在进入注意力模块之前的前期特征,(b)为经过注意力机制增强之后的特征。为使图像表述更加清晰,t-SNE实验中共采样3种类别,每种类别200幅图像进行降维,图中的3种标记符号分别代表3个不同的类别。</p>
                </div>
                <div class="area_img" id="150">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201910031_15000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 特征通过t-SNE投影至2维空间的可视化结果" src="Detail/GetImg?filename=images/JYRJ201910031_15000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 特征通过t-SNE投影至2维空间的可视化结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201910031_15000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="111">可以看出,在经过注意力模块的特征改进之后,不同图像类别之间的分布差异更加明显,类内距离的标准差缩小,而类间距标准差增大。实验结果表明,注意力机制可以捕获高级特征里的关键特征,有助于元学习器更好地区分不同类别的图像。</p>
                </div>
                <h4 class="anchor-tag" id="112" name="112"><b>4.2.3 小样本图像分类结果</b></h4>
                <div class="p1">
                    <p id="113">将VAE-ATTN元学习方法与现有元学习方法相比较,表2及表3展示了基础设置和直推设置的实验成果。在直推模式中,元学习器允许同时拥有标签训练样本和无标签测试样本,训练后的模型一次性对测试集中的所有样本进行分类,因此允许信息通过批量标准化在测试样本之间共享<citation id="148" type="reference"><link href="19" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>。也就是说,测试样本的类标签预测过程会受到彼此的影响,不再是相互独立的。表2与表3中,Y表示运用了直推设置,N表示未运用直推设置。观察实验结果发现,使用直推设置的分类结果明显优于未使用该设置的结果。</p>
                </div>
                <div class="area_img" id="114">
                    <p class="img_tit"><b>表2 Mini-ImageNet 小样本分类结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"> %</p>
                    <table id="114" border="1"><tr><td>算法</td><td>直推设置</td><td>5-shot 5-way</td><td>1-shot 5-way</td></tr><tr><td><br />1<sup><i>st</i></sup> order MAML</td><td>Y</td><td>63.1±0.9</td><td>48.1±1.7</td></tr><tr><td><br />MAML</td><td>Y</td><td>63.1±0.9</td><td>48.7±1.8</td></tr><tr><td><br />Reptile</td><td>N</td><td>62.7±0.4</td><td>45.8±0.4</td></tr><tr><td><br />Reptile</td><td>Y</td><td>66.0±0.6</td><td>48.2±0.6</td></tr><tr><td><br />MatchingNets</td><td>N</td><td>55.3±0.7</td><td>43.5±0.8</td></tr><tr><td><br />Meta-LSTM</td><td>N</td><td>60.6±0.7</td><td>43.4±0.8</td></tr><tr><td><br />本文算法(<i>β</i>=8)</td><td>N</td><td>68.1±0.6</td><td>50.4±0.3</td></tr><tr><td><br /><b>本文算法(<i>β</i>=8)</b></td><td><b>Y</b></td><td><b>72.5±0.5</b></td><td><b>53.5±0.6</b></td></tr><tr><td><br />本文算法(<i>β</i>=1)</td><td>N</td><td>66.3±0.8</td><td>47.5±0.5</td></tr><tr><td><br />本文算法(<i>β</i>=1)</td><td>Y</td><td>69.2±0.4</td><td>51.0±0.4</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="115">
                    <p class="img_tit"><b>表3 Omniglot小样本分类结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note">%</p>
                    <table id="115" border="1"><tr><td><br />算法</td><td>直推设置</td><td>5-shot 20-way</td><td>1-shot 20-way</td></tr><tr><td><br />Siamese Nets<sup>[17]</sup></td><td>N</td><td>97.0</td><td>88.1</td></tr><tr><td><br />Matching Nets</td><td>N</td><td>98.7</td><td>93.5</td></tr><tr><td><br />1<sup><i>st</i></sup> order MAML</td><td>Y</td><td>97.0±0.1</td><td>89.4±0.5</td></tr><tr><td><br />MAML</td><td>Y</td><td>98.9±0.2</td><td>95.8±0.3</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="117">
                    <p class="img_tit">续表3 <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note">%</p>
                    <table id="117" border="1"><tr><td><br />算法</td><td>直推设置</td><td>5-shot 20-way</td><td>1-shot 20-way</td></tr><tr><td><br />Reptile</td><td>N</td><td>96.7±0.3</td><td>88.1±0.2</td></tr><tr><td><br />Reptile</td><td>Y</td><td>97.1±0.3</td><td>89.4±0.1</td></tr><tr><td><br />本文算法(<i>β</i>=8)</td><td>N</td><td>97.6±0.3</td><td>93.5±0.2</td></tr><tr><td><br /><b>本文算法(<i>β</i>=8)</b></td><td><b>Y</b></td><td><b>98.8±0.1</b></td><td><b>96.5±0.2</b></td></tr><tr><td><br />本文算法(<i>β</i>=1)</td><td>N</td><td>97.1±0.3</td><td>89.4±0.1</td></tr><tr><td><br />本文算法(<i>β</i>=1)</td><td>Y</td><td>97.1±0.3</td><td>89.4±0.1</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="118">从表2中可以看出,在Mini-ImageNet上,本文提出的算法超过了当前性能优异的元学习算法,如MAML、MatchingNets、Meta-LSTM等。在5-样本,5-类别以及1-样本,5-类别的测试场景中分别获得72.5%和53.5%的准确率,显著超越原始Reptile算法的分类性能。由表3可知,在Omniglot数据集上,β-VAE在5-样本,20-类别以及1-样本,20-类别的测试场景中,取得了98.8%和96.5%的高分类准确率。实验结果说明基于表征学习和注意力机制的方法改善了元学习器,证明了VAE-ATTN算法的合理性。</p>
                </div>
                <div class="p1">
                    <p id="119">图6是Mini-ImageNet中5-样本,5-类别的直推实验的分类准确率曲线图。可以看出,VAE-ATTN算法均超出Reptile基准元学习算法,且运用β-VAE进行预训练的分类效果也优于常规VAE的训练效果。这一结果说明β-VAE提取的解缠表征加速元学习器结构化地理解多样的任务,实现更高的小样本分类准确率。</p>
                </div>
                <div class="area_img" id="120">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201910031_120.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 Mini-ImageNet实验分类准确率对比" src="Detail/GetImg?filename=images/JYRJ201910031_120.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 Mini-ImageNet实验分类准确率对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201910031_120.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h3 id="121" name="121" class="anchor-tag"><b>5 结 语</b></h3>
                <div class="p1">
                    <p id="122">小样本图像识别在人工智能领域是复杂且具有挑战性的研究方向,极具探索价值和意义。本文通过分析以往元学习方法存在的问题,提出结合表征学习和注意力机制的新元学习方法VAE-ATTN。算法运用β-VAE学习的高级的解缠表征,并通过注意力机制增强重要的信息并抑制冗余的信息,从而引导元学习器进行小样本学习。本文算法在Mini-ImageNet和Omniglot数据集上的小样本学习测试中均展现了良好的性能,表明了算法的有效性和可行性。</p>
                </div>
                <div class="p1">
                    <p id="123">在后续工作中,我们将考虑更具泛化性的元学习方法,目标是提取可跨任务或远距离迁移的特征,使得小样本学习能根据更充分的先验知识进行新任务的快速学习。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Meta-Learning in Distributed Data Mining Systems:Issues and Approaches">

                                <b>[1]</b> Prodromidis A,Chan P,Stolfo S.Meta-learning in distributed data mining systems:Issues and approaches[J].Advances in distributed and parallel knowledge discovery,2000,3:81-114.
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=One-shot learning with memory-augmented neural networks[EB]">

                                <b>[2]</b> Santoro A,Bartunov S,Botvinick M,et al.One-shot learning with memory-augmented neural networks[EB].arXiv preprint arXiv:1605.06065,2016.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Neural turing machines[EB]">

                                <b>[3]</b> Graves A,Wayne G,Danihelka I.Neural turing machines[EB].arXiv preprint arXiv:1410.5401,2014.
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning to learn by gradient descent by gradient descent">

                                <b>[4]</b> Andrychowicz M,Denil M,Gomez S,et al.Learning to learn by gradient descent by gradient descent[C]//Advances in Neural Information Processing Systems.2016:3981-3989.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=OPTIMIZATION AS A MODEL FOR FEW-SHOT LEARNING">

                                <b>[5]</b> Ravi S,Larochelle H.Optimization as a Model for Few-Shot Learning[C]//ICLR 2017,2017.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning to compare:Relation network for few-shot learning">

                                <b>[6]</b> Yang F S Y,Zhang L,Xiang T,et al.Learning to compare:Relation network for few-shot learning[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),Salt Lake City,UT,USA.2018.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Matching networks for one shot learning">

                                <b>[7]</b> Vinyals O,Blundell C,Lillicrap T,et al.Matching networks for one shot learning[C]//Advances in neural information processing systems.2016:3630-3638.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Model-agnostic meta-learning for fast adaptation of deep networks[EB]">

                                <b>[8]</b> Finn C,Abbeel P,Levine S.Model-agnostic meta-learning for fast adaptation of deep networks[EB].arXiv preprint arXiv:1703.03400,2017.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Meta networks[EB]">

                                <b>[9]</b> Munkhdalai T,Yu H.Meta networks[EB].arXiv preprint arXiv:1703.00837,2017.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Reptile:a Scalable Metalearning Algorithm[EB]">

                                <b>[10]</b> Nichol A,Schulman J.Reptile:a Scalable Metalearning Algorithm[EB].arXiv preprint arXiv:1803.02999,2018.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Auto-encoding variational bayes[EB]">

                                <b>[11]</b> Kingma D P,Welling M.Auto-encoding variational bayes[EB].arXiv preprint arXiv:1312.6114,2013.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Variational autoencoder for deep learning of images,labels and captions">

                                <b>[12]</b> Pu Y,Gan Z,Henao R,et al.Variational autoencoder for deep learning of images,labels and captions[C]//Advances in neural information processing systems.2016:2352-2360.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Early visual concept learning with unsupervised deep learning[EB]">

                                <b>[13]</b> Higgins I,Matthey L,Glorot X,et al.Early visual concept learning with unsupervised deep learning[EB].arXiv preprint arXiv:1606.05579,2016.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Understanding disentangling in β-VAE[EB]">

                                <b>[14]</b> Burgess C P,Higgins I,Pal A,et al.Understanding disentangling in β-VAE[EB].arXiv preprint arXiv:1804.03599,2018.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Transfer learning via unsupervised task discovery for visual question answering[EB]">

                                <b>[15]</b> Noh H,Kim T,Mun J,et al.Transfer learning via unsupervised task discovery for visual question answering[EB].arXiv preprint arXiv:1810.02358,2018.
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=One shot learning of simple visual concepts">

                                <b>[16]</b> Lake B,Salakhutdinov R,Gross J,et al.One shot learning of simple visual concepts[C]//Proceedings of the Annual Meeting of the Cognitive Science Society.2011.
                            </a>
                        </p>
                        <p id="35">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fully-convolutional siamese networks for object tracking">

                                <b>[17]</b> Bertinetto L,Valmadre J,Henriques J F,et al.Fully-convolutional siamese networks for object tracking[C]//European conference on computer vision.Springer,Cham,2016:850-865.
                            </a>
                        </p>
                        <p id="37">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Recurrent models of visual attention">

                                <b>[18]</b> Mnih V,Heess N,Graves A,et al.Recurrent models of visual attention[C]//Proceedings of the 27th International Conference on Neural Information Processing Systems—Volume 2.MIT Press,2014:2204-2212.
                            </a>
                        </p>
                        <p id="39">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Attention is all you need[EB]">

                                <b>[19]</b> Vaswani A,Shazeer N,Parmar N,et al.Attention is all you need[EB].eprint arXiv:1706.03762,2017.
                            </a>
                        </p>
                        <p id="41">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Visualizing high-dimensional data using t-sne">

                                <b>[20]</b> van der Maaten L J P,Hinton G E.Visualizing High-Dimensional Data Using t-SNE[J].Journal of Machine Learning Research,2008,9:2579-2605.
                            </a>
                        </p>
                        <p id="43">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Representation Learning A Review and New Perspectives">

                                <b>[21]</b> Bengio Y,Courville A,Vincent P.Representation Learning:A Review and New Perspectives[J].IEEE Transactions on Pattern Analysis and Machine Intelligence,2013,35(8):1798-1828.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JYRJ201910031" />
        <input id="dpi" type="hidden" value="600" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201910031&amp;v=MTM0OTdyN0JMelRaWkxHNEg5ak5yNDlHWllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVp0RmlEaFY=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="0" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
