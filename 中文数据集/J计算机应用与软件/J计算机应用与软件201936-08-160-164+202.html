<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637135610692346250%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJYRJ201908029%26RESULT%3d1%26SIGN%3dRQhpDTisNAQWJnUF57I1qCJSkdI%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JYRJ201908029&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JYRJ201908029&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201908029&amp;v=MTYxNTQ5SGJZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVadEZ5am1WcnpBTHpUWlpMRzRIOWpNcDQ=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#35" data-title="&lt;b&gt;0 引 言&lt;/b&gt; "><b>0 引 言</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#38" data-title="&lt;b&gt;1 算法设计&lt;/b&gt; "><b>1 算法设计</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#39" data-title="&lt;b&gt;1.1 融合周期性学习率&lt;/b&gt;"><b>1.1 融合周期性学习率</b></a></li>
                                                <li><a href="#46" data-title="&lt;b&gt;1.2 构建基于深度残差网络和U-net的基本结构&lt;/b&gt;"><b>1.2 构建基于深度残差网络和U-net的基本结构</b></a></li>
                                                <li><a href="#49" data-title="&lt;b&gt;1.3 双阶段视频显著性检测&lt;/b&gt;"><b>1.3 双阶段视频显著性检测</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#56" data-title="&lt;b&gt;2 实验及结果分析&lt;/b&gt; "><b>2 实验及结果分析</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#57" data-title="&lt;b&gt;2.1 实验设置&lt;/b&gt;"><b>2.1 实验设置</b></a></li>
                                                <li><a href="#59" data-title="&lt;b&gt;2.2 方法对比&lt;/b&gt;"><b>2.2 方法对比</b></a></li>
                                                <li><a href="#76" data-title="&lt;b&gt;2.3 必要性分析&lt;/b&gt;"><b>2.3 必要性分析</b></a></li>
                                                <li><a href="#81" data-title="&lt;b&gt;2.4 运行时间分析&lt;/b&gt;"><b>2.4 运行时间分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#85" data-title="&lt;b&gt;3 结 语&lt;/b&gt; "><b>3 结 语</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#45" data-title="图1 学习率周期性减半规则示意图">图1 学习率周期性减半规则示意图</a></li>
                                                <li><a href="#51" data-title="图2 基于残差网络的双阶段训练模型结构图">图2 基于残差网络的双阶段训练模型结构图</a></li>
                                                <li><a href="#61" data-title="图3 DAVIS数据集上的视觉效果对比">图3 DAVIS数据集上的视觉效果对比</a></li>
                                                <li><a href="#62" data-title="图4 SegTrackV2数据集上的视觉效果对比">图4 SegTrackV2数据集上的视觉效果对比</a></li>
                                                <li><a href="#69" data-title="&lt;b&gt;表1 两个公开数据集上F-measure值的对比&lt;/b&gt;"><b>表1 两个公开数据集上F-measure值的对比</b></a></li>
                                                <li><a href="#74" data-title="&lt;b&gt;表2 两个公开数据集上MAE值的对比&lt;/b&gt;"><b>表2 两个公开数据集上MAE值的对比</b></a></li>
                                                <li><a href="#78" data-title="&lt;b&gt;表3 单双阶段的MAE值对比&lt;/b&gt;"><b>表3 单双阶段的MAE值对比</b></a></li>
                                                <li><a href="#79" data-title="&lt;b&gt;表4 单双阶段的F-measure值对比&lt;/b&gt;"><b>表4 单双阶段的F-measure值对比</b></a></li>
                                                <li><a href="#83" data-title="&lt;b&gt;表5 各个方法处理同一视频的平均速率对比&lt;/b&gt;"><b>表5 各个方法处理同一视频的平均速率对比</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" title=" 谢更新, 张恒, 罗正平.EAST 上基于图像显著性的快速边缘提取算法[J].计算机应用与软件, 2018, 35 (10) :189-193." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201810034&amp;v=MTAwMzJyNDlHWUlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVp0RnlqbVZyekFMelRaWkxHNEg5bk4=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         谢更新, 张恒, 罗正平.EAST 上基于图像显著性的快速边缘提取算法[J].计算机应用与软件, 2018, 35 (10) :189-193.
                                    </a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" title=" Ma C, Miao Z, Zhang X P, et al.A Saliency Prior Context Model for Real-Time Object Tracking[J].IEEE Transactions on Multimedia, 2017, 19 (11) :2415-2424." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Saliency Prior Context Model for RealTime Object Tracking">
                                        <b>[2]</b>
                                         Ma C, Miao Z, Zhang X P, et al.A Saliency Prior Context Model for Real-Time Object Tracking[J].IEEE Transactions on Multimedia, 2017, 19 (11) :2415-2424.
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title=" Wang J, Jiang H, Yuan Z, et al.Salient Object Detection:A Discriminative Regional Feature Integration Approach[J].International Journal of Computer Vision, 2017, 123 (2) :251-268." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJDD1685ADBA676AE80B01F9179C203D676&amp;v=Mjc1OThBPU5qN0Jhc2U1R05uSjN2czNGZTBJQ2cxTXh4Wmg2ajRMUVg3bHBXRTNlYkhnUTcyWkNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkJyTFUwNXR0aHhMdTZ3Ng==&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         Wang J, Jiang H, Yuan Z, et al.Salient Object Detection:A Discriminative Regional Feature Integration Approach[J].International Journal of Computer Vision, 2017, 123 (2) :251-268.
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" title=" Cheng M M, Zhang G X, Mitra N J, et al.Global Contrast Based Salient Region Detection[C]//2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) .IEEE, 2011." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Global contrast based salient region detection">
                                        <b>[4]</b>
                                         Cheng M M, Zhang G X, Mitra N J, et al.Global Contrast Based Salient Region Detection[C]//2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) .IEEE, 2011.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" title=" Liu Z, Li J, Ye L, et al.Saliency Detection for Unconstrained Videos Using Superpixel-level Graph and Spatiotemporal Propagation[J].IEEE Transactions on Circuits and Systems for Video Technology, 2017, 27 (12) :2527-2542." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Saliency detection for unconstrained videos using superpixel-level graph and spatiotemporal propagation">
                                        <b>[5]</b>
                                         Liu Z, Li J, Ye L, et al.Saliency Detection for Unconstrained Videos Using Superpixel-level Graph and Spatiotemporal Propagation[J].IEEE Transactions on Circuits and Systems for Video Technology, 2017, 27 (12) :2527-2542.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" title=" Wang W, Shen J, Shao L.Consistent Video Saliency Using Local Gradient Flow Optimization and Global Refinement[J].IEEE Transactions on Image Processing, 2015, 24 (11) :4185-4196." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Consistent video saliency using local gradient flow optimization and global refinement">
                                        <b>[6]</b>
                                         Wang W, Shen J, Shao L.Consistent Video Saliency Using Local Gradient Flow Optimization and Global Refinement[J].IEEE Transactions on Image Processing, 2015, 24 (11) :4185-4196.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" title=" Smith L N.Cyclical Learning Rates for Training Neural Networks[EB].arXiv:1506.01186, 2015." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Cyclical Learning Rates for Training Neural Networks[EB]">
                                        <b>[7]</b>
                                         Smith L N.Cyclical Learning Rates for Training Neural Networks[EB].arXiv:1506.01186, 2015.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" title=" He K, Zhang X, Ren S, et al.Identity Mappings in Deep Residual Networks[C]//European Conference on Computer Vision.2016:630-645." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Identity mappings in deep residual networks">
                                        <b>[8]</b>
                                         He K, Zhang X, Ren S, et al.Identity Mappings in Deep Residual Networks[C]//European Conference on Computer Vision.2016:630-645.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" title=" Ronneberger O, Fischer P, Brox T.U-Net:Convolutional Networks for Biomedical Image Segmentation[C]//International Conference on Medical Image Computing and Computer-Assisted Intervention, Springer, Cham, 2015:234-241." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=U-Net:Convolutional Networks for Biomedical Image Segmentation">
                                        <b>[9]</b>
                                         Ronneberger O, Fischer P, Brox T.U-Net:Convolutional Networks for Biomedical Image Segmentation[C]//International Conference on Medical Image Computing and Computer-Assisted Intervention, Springer, Cham, 2015:234-241.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_10" title=" Yang C, Zhang L, Lu H, et al.Saliency Detection via Graph-Based Manifold Ranking[C]//Computer Vision and Pattern Recognition.IEEE, 2013:3166-3173." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Saliency detection via graph-based manifold ranking">
                                        <b>[10]</b>
                                         Yang C, Zhang L, Lu H, et al.Saliency Detection via Graph-Based Manifold Ranking[C]//Computer Vision and Pattern Recognition.IEEE, 2013:3166-3173.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_11" title=" Shi J, Yan Q, Xu L, et al.Hierarchical Saliency Detection on Extended CSSD[J].IEEE Transactions on Pattern Analysis &amp;amp; Machine Intelligence, 2014, 38 (4) :717." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Hierarchical Image Saliency Detection on Extended CSSD">
                                        <b>[11]</b>
                                         Shi J, Yan Q, Xu L, et al.Hierarchical Saliency Detection on Extended CSSD[J].IEEE Transactions on Pattern Analysis &amp;amp; Machine Intelligence, 2014, 38 (4) :717.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_12" title=" Li G, Yu Y.Visual Saliency Detection Based on Multiscale Deep CNN Features[J].IEEE Trans.on Image Processing, 2016, 25 (11) :5012-5024." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Visual saliency detection based on multiscale deep CNN features">
                                        <b>[12]</b>
                                         Li G, Yu Y.Visual Saliency Detection Based on Multiscale Deep CNN Features[J].IEEE Trans.on Image Processing, 2016, 25 (11) :5012-5024.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_13" title=" Friedman I, Chemla I, Smolyansky E, et al.GyGO:an E-commerce Video Object Segmentation Dataset by Visualead[DB/OL].[2017-09-10].https://github.com/ilchemla/gygo-dataset." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=GyGO:an E-commerce Video Object Segmentation Dataset by Visualead">
                                        <b>[13]</b>
                                         Friedman I, Chemla I, Smolyansky E, et al.GyGO:an E-commerce Video Object Segmentation Dataset by Visualead[DB/OL].[2017-09-10].https://github.com/ilchemla/gygo-dataset.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_14" title=" Perazzi F, Ponttuset J, Mcwilliams B, et al.A Benchmark Dataset and Evaluation Methodology for Video Object Segmentation[C]//Computer Vision and Pattern Recognition, IEEE, 2016:724-732." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A benchmark dataset and evaluation methodology for video object segmentation">
                                        <b>[14]</b>
                                         Perazzi F, Ponttuset J, Mcwilliams B, et al.A Benchmark Dataset and Evaluation Methodology for Video Object Segmentation[C]//Computer Vision and Pattern Recognition, IEEE, 2016:724-732.
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_15" title=" Li F, Kim T, Humayun A, et al.Video Segmentation by Tracking Many Figure-Ground Segments[C]//IEEE International Conference on Computer Vision.IEEE Computer Society, 2013:2192-2199." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Video Segmentation by Tracking Many Figure-Ground Segments">
                                        <b>[15]</b>
                                         Li F, Kim T, Humayun A, et al.Video Segmentation by Tracking Many Figure-Ground Segments[C]//IEEE International Conference on Computer Vision.IEEE Computer Society, 2013:2192-2199.
                                    </a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_16" title=" Wang W, Shen J, Yang R, et al.Saliency-Aware Video Object Segmentation[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2018, 40 (1) :20-33." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Saliency-aware video object segmentation">
                                        <b>[16]</b>
                                         Wang W, Shen J, Yang R, et al.Saliency-Aware Video Object Segmentation[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2018, 40 (1) :20-33.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JYRJ" target="_blank">计算机应用与软件</a>
                2019,36(08),160-164+202 DOI:10.3969/j.issn.1000-386x.2019.08.028            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于深度残差网络的双阶段视频显著性检测</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BC%A0%E4%BA%AE&amp;code=25405328&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">张亮</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%AE%B5%E5%90%91%E6%AC%A2&amp;code=39700579&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">段向欢</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%8E%E5%BB%BA%E4%BC%9F&amp;code=07077719&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">李建伟</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%B9%BF%E5%B7%9E%E6%B0%91%E8%88%AA%E8%81%8C%E4%B8%9A%E6%8A%80%E6%9C%AF%E5%AD%A6%E9%99%A2%E8%88%AA%E7%A9%BA%E6%B8%AF%E7%AE%A1%E7%90%86%E5%AD%A6%E9%99%A2&amp;code=0020548&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">广州民航职业技术学院航空港管理学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%B2%B3%E5%8C%97%E5%B7%A5%E4%B8%9A%E5%A4%A7%E5%AD%A6%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E5%AD%A6%E9%99%A2&amp;code=0149979&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">河北工业大学人工智能与数据科学学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>为了进一步推进视频显著性检测的研究, 提出一种以深度残差网络和U-net为基本结构的双阶段视频显著性检测方法。用静态图像和视频序列训练第一阶段模型来分别提高模型对于空间特征和时序特征的学习能力;通过调整基本结构的输入端, 融合连续三帧视频序列以及第一阶段得到的显著图作为每次的输入来训练第二阶段的模型, 增强模型学习的时序特征;融合周期性学习率, 使得学习率周期性变化, 保证模型在训练的每个阶段可以利用到最佳学习率, 以此更好更快地达到收敛。实验表明, 该方法在两个公开视频数据集上的检测效果均超过了当前主流的方法, 检测精度更高, 鲁棒性更好。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%98%BE%E8%91%97%E6%80%A7%E6%A3%80%E6%B5%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">显著性检测;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%A7%86%E9%A2%91%E6%98%BE%E8%91%97%E6%80%A7%E6%A3%80%E6%B5%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">视频显著性检测;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E6%AE%8B%E5%B7%AE%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度残差网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%91%A8%E6%9C%9F%E6%80%A7%E5%AD%A6%E4%B9%A0%E7%8E%87&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">周期性学习率;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=U-net&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">U-net;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    张亮, 副教授, 主研领域:显著性检测, 无人机图像处理。;
                                </span>
                                <span>
                                    段向欢, 硕士生。;
                                </span>
                                <span>
                                    李建伟, 教授。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-03-16</p>

                    <p>

                            <b>基金：</b>
                                                        <span>河北省自然科学基金项目 (C2018202083);</span>
                    </p>
            </div>
                    <h1><b>TWO-STAGE VIDEO SALIENCY DETECTION BASED ON DEEP RESIDUAL NETWORK</b></h1>
                    <h2>
                    <span>Zhang Liang</span>
                    <span>Duan Xianghuan</span>
                    <span>Li Jianwei</span>
            </h2>
                    <h2>
                    <span>Airport Management College, Guangzhou Civil Aviation College</span>
                    <span>School of Artificial Inteligence, Hebei University of Technology</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>In order to further promote the research of video saliency detection, we proposed two-stage video saliency detection based on deep residual network and U-net. The static image and video sequence were used to train the model in the first stage to improve the learning ability of the model for spatial and temporal features. By adjusting the input end of the basic structure, fusing three consecutive video sequences and saliency images obtained in the first stage as input, the second stage model was trained to enhance the temporal characteristics of model learning. In addition, the cyclical learning rate was fused to make the learning rate change periodically, which ensured that the model could use the optimal learning rate in each stage of the training process to achieve better and faster convergence. Experiments show that the proposed method is better than the current mainstream methods on both open video datasets, and has higher detection accuracy and better robustness.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Saliency%20detection&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Saliency detection;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Video%20saliency%20detection&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Video saliency detection;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Deep%20residual%20network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Deep residual network;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Cyclical%20learning%20rate&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Cyclical learning rate;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=U-net&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">U-net;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2019-03-16</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="35" name="35" class="anchor-tag"><b>0 引 言</b></h3>
                <div class="p1">
                    <p id="36">显著性检测一直是计算机视觉领域火热的研究方向, 它在边缘提取<citation id="87" type="reference"><link href="3" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>、目标跟踪<citation id="88" type="reference"><link href="5" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>等诸多领域有着非常重要的应用价值。随着图像显著性研究不断取得突破性的成果<citation id="89" type="reference"><link href="7" rel="bibliography" /><link href="9" rel="bibliography" /><sup>[<a class="sup">3</a>,<a class="sup">4</a>]</sup></citation>, 处理大量视频数据的要求不断涌现, 越来越多的研究人员加入到了视频显著性检测的研究行列。</p>
                </div>
                <div class="p1">
                    <p id="37">视频显著性检测通过设计算法让计算机模拟人眼自动提取视频序列中人类感兴趣的区域。它常作为视频处理的预处理操作, 能有效提升任务的处理效率。相比图像显著性检测, 视频显著性目标检测不仅要考虑空间特征, 还需考虑时间特征或者运动特征, 所以更具挑战性。随着深度学习理论在图像领域中的不断应用, 深度神经网络在视频显著性检测中的潜力也愈发凸显。在背景复杂或者运动信息多样的情况下, 已知的视频显著性检测算法仍然没有实现高准确率、高运行效率和高鲁棒性等问题。如何更好地提高显著目标检测性能以及减少帧间损失等仍是一个研究热点。以往的方法常常分为两个阶段:分别处理视频的空间特征 (或运动特征) 与时序特征。再通过加权函数融合两个阶段得到的显著图或者利用能量函数进一步优化两个阶段得到的结果<citation id="93" type="reference"><link href="11" rel="bibliography" /><link href="13" rel="bibliography" /><sup>[<a class="sup">5</a>,<a class="sup">6</a>]</sup></citation>。为了加深神经网络模型的层数、挖掘更深层次的特征, 同时避免常见的梯度消失、梯度爆炸和网络退化等问题, 本文融合周期性学习率<citation id="90" type="reference"><link href="15" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>, 提出一种以深度残差网络<citation id="91" type="reference"><link href="17" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>和U-net<citation id="92" type="reference"><link href="19" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>为基本结构的双阶段视频显著性检测方法。不同于以往方法需要通过先验手动提取空间、时序维度的特征, 该方法分为两个训练阶段, 利用深度学习模型自动挖掘各个维度深层特征的优势。第一阶段初步获取样本的时空特征;第二个阶段通过融合连续三帧视频序列信息进一步加强模型学习到的时序特征, 以此来提高视频显著性检测的效果。</p>
                </div>
                <h3 id="38" name="38" class="anchor-tag"><b>1 算法设计</b></h3>
                <h4 class="anchor-tag" id="39" name="39"><b>1.1 融合周期性学习率</b></h4>
                <div class="p1">
                    <p id="40">模型调优是训练深度学习模型的重要一环, 超参数的设置和优化起着关键性的作用。学习率就是非常重要的超参数之一, 它决定着每一次损失函数的梯度更新程度, 其选取是否恰当将直接影响训练模型的进度和最终效果。训练模型的权重如下:</p>
                </div>
                <div class="p1">
                    <p id="41"><mathml id="42"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>θ</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mi>θ</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mi>α</mi><mo>⋅</mo><mfrac><mo>∂</mo><mrow><mo>∂</mo><mi>θ</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></mfrac><mi>J</mi><mo stretchy="false"> (</mo><mi>θ</mi><mo stretchy="false">) </mo></mrow></math></mathml>      (1) </p>
                </div>
                <div class="p1">
                    <p id="43">式中:θ表示模型的权重;θ<sub>i</sub>表示其中某个权重;J为代价函数;α为学习率的值。如采用固定的学习率, 很难依据先验知识确定最佳初始值。</p>
                </div>
                <div class="p1">
                    <p id="44">文献<citation id="94" type="reference">[<a class="sup">7</a>]</citation>提出周期性学习率的概念, 让学习率在最大边界和最小边界内循环变化而不是单一地按着一定规则衰减。周期性学习率可以有不同的模式, 常见的有三角形循环规则、周期性减半规则和周期性指数级衰减策略, 它们的基本策略在于每个周期结束后, 学习率的最大值不变或按着半数、指数级进行衰减。本文在融合周期性学习率时, 选用的模式是周期性减半规则, 如图1所示, 每个新的周期, 最大学习率会是上个周期的半数值。步长采用周期的一半, 学习率的上下边界采用的默认值为0.001和0.006。</p>
                </div>
                <div class="area_img" id="45">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201908029_045.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 学习率周期性减半规则示意图" src="Detail/GetImg?filename=images/JYRJ201908029_045.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 学习率周期性减半规则示意图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201908029_045.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="46" name="46"><b>1.2 构建基于深度残差网络和U-net的基本结构</b></h4>
                <div class="p1">
                    <p id="47">深度残差网络通过添加经典的捷径连接 (shortcut connections) , 解决了网络层数加深时常会遇到的梯度爆炸、梯度消失、网络退化等问题。残差网络经过不断地改进和发展, 已有多个变体, 其中常用的是ResNet-50、ResNet-101、ResNet-152等, 名称中的数字代表神经网络的深度。为了加深网络层数, 学习更深层次的对象特征, 本文提出融合深度残差网络和U-net作为基本结构进行双阶段的视频显著性检测方法。为了减少计算量, 本文选用ResNet-50, 其基本结构由残差模块堆叠而成, 残差模块包含卷积层、批标准化层、激活层等。此外, 为使残差模块更加有效堆叠, 每个残差模块后面都添加了非线性的激活函数。</p>
                </div>
                <div class="p1">
                    <p id="48">U-net采用编码-解码结构, 并且网络模型左右严格对称, 呈“U”形。本文将残差网络和U-net相融合, 即把ResNet-50网络模型作为U-net的编码端, 负责对图像或者视频帧进行上下文特征信息的学习。同时, 为了更好地与后面的网络进行衔接, 对ResNet-50尾部结构进行了调整, 并增加了两个残差模块。得益于更深层的压缩路径, U-net新的编码器可以提取更多维度的信息和更高级的特征, 更好地区分目标图像中的显著性信息和冗余信息, 为后续解码工作奠定良好基础。为了按照下采样的节奏来逐步将特征图恢复至原始输入图片的大小, 针对编码器进行的五次压缩操作, 新的U-net解码端在相对应的层次也进行了五次上采样工作, 最后输出的检测图即为最终结果。新的网络模型是一个端到端的结构, 一定程度上可以认为解码器和编码器的结构呈镜面对称, 最大的不同是跳跃连接带来的差异。整个网络结构一共包含了四个跳跃连接, 除去第一次下采样和最后一次升采样, 跳跃连接将对应的下采样和升采样区域联系起来。通过高层特征和底层特征的结合, 减少了压缩路径下采样过程中重要特征信息的丢失, 为检测结果增加了更多细节信息。</p>
                </div>
                <h4 class="anchor-tag" id="49" name="49"><b>1.3 双阶段视频显著性检测</b></h4>
                <div class="p1">
                    <p id="50">前面提到的残差网络ResNet-50和U-net的结构, 可记作BaseNet。利用该结构训练图像或者视频序列, 得到的模型能够学习到不错的空间特征和时序特征。但是, 为了进一步增强模型对视频序列时序特征的学习能力, 减少检测结果中视频帧间的特征损失, 本文又提出了基于BaseNet的双阶段视频显著性检测方法。整体模型的结构如图2所示, 其中残差块指的是残差网络中的基本残差单元, ResNet-50-v1是对ResNet-50网络的末尾层进行调整得到的, ResNet-50-v2是为了构成类似U-net的整体结构对ResNet-50网络进行了修改, ResNet-50-v3则因为修改了输入端的结构而与ResNet-50-v1稍有不同。</p>
                </div>
                <div class="area_img" id="51">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201908029_051.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 基于残差网络的双阶段训练模型结构图" src="Detail/GetImg?filename=images/JYRJ201908029_051.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 基于残差网络的双阶段训练模型结构图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201908029_051.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="52">该方法共分为两个阶段:第一阶段采用BaseNet网络模型, 采用图像数据集和视频数据集训练模型, 以期提升模型捕获时空特征的能力;第二阶段将第一阶段通过训练达到收敛的模型锁定, 即它的模型权重不再被训练和改变, 称为BaseModel, 然后对BaseNet的输入端进行调整和改变, 新网络模型将接收的连续三帧视频序列和BaseModel对于第二帧预测的显著图作为输入进行训练, 直到模型收敛为止。每个连续的三帧视频序列中, 第二帧是求解目标, 它所对应的前后帧以及BaseModel模型预测的显著图都是参考信息, 以此更好地指导网络模型进行训练。整个方法可以通过下式抽象表示:</p>
                </div>
                <div class="p1">
                    <p id="53"><i>Y</i>=<i>F</i><sub>2</sub> (<i>I</i><sub><i>t</i>-1</sub>, <i>I</i><sub><i>t</i></sub>, <i>I</i><sub><i>t</i>+1</sub>, <i>F</i><sub>1</sub> (<i>I</i><sub><i>t</i></sub>;<i>θ</i><sub>1</sub>) ;<i>θ</i><sub>2</sub>)      (2) </p>
                </div>
                <div class="p1">
                    <p id="54">式中:<i>Y</i>表示整个模型的输出结果, 即所需要的对应的最终显著图;<i>I</i>表示视频帧或者静态图像, <i>t</i>表示视频序列在时间维度上的刻度;<i>F</i><sub>1</sub>表示第一阶段训练好的模型, 即BaseModel, <i>θ</i><sub>1</sub>表示模型的权重或者参数;<i>I</i><sub><i>t</i>-1</sub>、<i>I</i><sub><i>t</i></sub>、<i>I</i><sub><i>t</i>+1</sub>分别表示连续的三帧视频序列;<i>F</i><sub>1</sub> (<i>I</i><sub><i>t</i></sub>;<i>θ</i><sub>1</sub>) 为第二帧视频序列在模型BaseModel下得到的显著图;<i>F</i><sub>2</sub>表示第二个阶段训练好的模型;<i>θ</i><sub>2</sub>表示该模型训练好的参数和权重。</p>
                </div>
                <div class="p1">
                    <p id="55">对于双阶段模型的初始化、超参数设计以及损失函数等训练细节会在2.1节进行描述。通过引入周期性学习率、融合残差网络和U-net网络以及构造双阶段网络结构, 既使模型得到更好的训练, 也进一步提高模型对视频时序特征的学习能力, 得到的最终模型在对视频序列检测时, 检测结果各帧中的显著目标均能更为完整、清晰和准确。</p>
                </div>
                <h3 id="56" name="56" class="anchor-tag"><b>2 实验及结果分析</b></h3>
                <h4 class="anchor-tag" id="57" name="57"><b>2.1 实验设置</b></h4>
                <div class="p1">
                    <p id="58">本文采用深度学习框架Keras来进行测试和实验。在训练模型的第一阶段, 使用DUT-OMRON<citation id="95" type="reference"><link href="21" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>、ECSSD<citation id="96" type="reference"><link href="23" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>、HKU-IS<citation id="97" type="reference"><link href="25" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>和MSRA10K<citation id="98" type="reference"><link href="9" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>等图像数据集以及UVSD<citation id="99" type="reference"><link href="11" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>、Gygo-dataset<citation id="100" type="reference"><link href="27" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>和DAVIS<citation id="101" type="reference"><link href="29" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation> (仅训练集) 等视频数据集训练模型。这些数据集是显著性检测领域比较知名和权威的数据集, 一方面对应的真实值标注比较可靠, 前人的工作也经常用到这些数据集;另一方面这些数据集包含了不同的场景和目标类别, 用它们一起训练进一步提升模型的泛化能力。在训练阶段, 将这些数据汇合并打乱重排, 训练集、验证集都是按着4∶1的比例随机划分, 保证验证集样本有着丰富的类型, 每轮的训练都是根据验证集样本的表现调整模型参数。这样通过多轮的训练, 提升模型对于不同场景和目标的适应性。在实验中对于模型的编码端, 采用ImageNet数据集上训练好的ResNet-50预训练模型进行迁移学习, 采用均方误差 (MSE) 作为该阶段的损失函数。类似地, 在训练模型的第二阶段, 使用第一阶段的3个视频数据集作为训练集, 进一步增强模型对于训练集时序特征的学习。由于两个阶段输入通道和训练模型不一致, 所以从零开始训练调整后的网络模型, 并采用平均绝对误差 (MAE) 作为惩罚函数来训练模型。模型的输入图片可以是任意大小, 批量大小为32, 训练轮数设置为200。在测试阶段, 为了更好地评估提出的方法, 使用的是SegTrackV2<citation id="102" type="reference"><link href="31" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>和DAVIS (测试集部分) 等常用的、具有挑战性的数据集。</p>
                </div>
                <h4 class="anchor-tag" id="59" name="59"><b>2.2 方法对比</b></h4>
                <div class="p1">
                    <p id="60">为了更全面地评估文本所提方法, 从定性和定量两个方面将其和目前主流方法在相同测试集上进行了对比。对比的方法包括:基于超像素级图和时空传播的显著性 (SGSP) <citation id="103" type="reference"><link href="11" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>、融合显著区域特征的显著性 (DRFI) <citation id="104" type="reference"><link href="7" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>、基于梯度流域计算的显著性 (GAFL) <citation id="105" type="reference"><link href="13" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>、基于测地距离的视频显著性 (SAGE) <citation id="106" type="reference"><link href="33" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>。其中, 第二个方法是用于图像显著性检测的, 其他的方法都是针对视频显著性检测的。在DAVIS测试数据集和SegTrackV2数据集上的视觉效果对比分别如图3和图4所示。</p>
                </div>
                <div class="area_img" id="61">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201908029_061.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 DAVIS数据集上的视觉效果对比" src="Detail/GetImg?filename=images/JYRJ201908029_061.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 DAVIS数据集上的视觉效果对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201908029_061.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="62">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201908029_062.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 SegTrackV2数据集上的视觉效果对比" src="Detail/GetImg?filename=images/JYRJ201908029_062.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 SegTrackV2数据集上的视觉效果对比  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201908029_062.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="63">图中每一行表示几种方法对于同一视频帧的检测结果。图中前两列表示原始视频帧 (Input) 及对应的真实值 (GT) , 自第一行起, 每三行取自同一视频序列, 每个数据集随机选取了三个视频样本。通过分析图3和图4可知, DRFI方法由于没有考虑时序信息, 所以处理的结果会附带很多显著目标周围的背景, 造成显著目标在结果中不易辨识, 目标的边缘也不易识别。在某些情况下, 该方法会造成目标不完整, 在多目标视频中很难把握全部目标信息。其他用于视频的方法融合了空间和时序信息, 故均可以更好地处理帧间信息, 显著目标周围的背景也处理得较为彻底。但SGSP方法由于过度依赖运动特征, 对于帧间运动不是很明显的视频往往效果不是很理想, 得到的结果中显著目标周围往往附带一些冗余的像素, 很难确定显著目标的边缘, 有些场景背景抑制能力不高, 如图3中第四、五行和图4中第一行和最后一行。GAFL和SAGE两种方法都基于超像素和光流法, 主要依赖视频帧的边缘特征和运动特征, 其结果相对清晰, 但光流的计算比较耗时, 检测结果也需要进一步改善。相比而言, 本文方法通过深度模型自动挖掘各层次的关键特征, 获得的结果保留了显著目标的准确信息, 目标背景处理更彻底, 各帧的预测结果显著目标都更完整, 更接近人工标注的真实值。</p>
                </div>
                <div class="p1">
                    <p id="64">在定量角度, 采用常见的评价标准F-measure和MAE (平均绝对误差) 对以上方法进行客观评价。F-measure (<i>F</i><sub>m</sub>) 代表查准率 (<i>P</i><sub>m</sub>) 和查全率 (<i>R</i>) 的统计加权调和平均值, 可表示为:</p>
                </div>
                <div class="p1">
                    <p id="65"><mathml id="66"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>F</mi><msub><mrow></mrow><mi>m</mi></msub><mo>=</mo><mfrac><mrow><mo stretchy="false"> (</mo><mn>1</mn><mo>+</mo><mi>β</mi><msup><mrow></mrow><mn>2</mn></msup><mo stretchy="false">) </mo><mo>×</mo><mi>Ρ</mi><msub><mrow></mrow><mi>m</mi></msub><mo>×</mo><mi>R</mi></mrow><mrow><mi>β</mi><msup><mrow></mrow><mn>2</mn></msup><mo>×</mo><mi>Ρ</mi><msub><mrow></mrow><mi>m</mi></msub><mo>×</mo><mi>R</mi></mrow></mfrac></mrow></math></mathml>      (3) </p>
                </div>
                <div class="p1">
                    <p id="67">基于前人工作的推荐, 这里设置<i>β</i>为0.3, 以此来突出查准率的重要性。</p>
                </div>
                <div class="p1">
                    <p id="68">各个方法在公开数据集DAVIS和SegTrackV2上的F-measure值如表1所示。</p>
                </div>
                <div class="area_img" id="69">
                    <p class="img_tit"><b>表1 两个公开数据集上F-measure值的对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="69" border="1"><tr><td><br />方法</td><td>DAVIS</td><td>SegTrackV2</td></tr><tr><td><br />DRFI</td><td>0.49</td><td>0.492</td></tr><tr><td><br />GAFL</td><td>0.517</td><td>0.426</td></tr><tr><td><br />SAGE</td><td>0.482</td><td>0.404</td></tr><tr><td><br />SGSP</td><td>0.578</td><td>0.462</td></tr><tr><td><br />Ours</td><td>0.790</td><td>0.766</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="70">MAE (<i>M</i>) 表示待评估模型预测出的显著概率图<i>P</i>与对应的真实值<i>G</i>之间的逐像素平均差异。它的公式表示为:</p>
                </div>
                <div class="p1">
                    <p id="71"><mathml id="72"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Μ</mi><mo>=</mo><mfrac><mn>1</mn><mrow><mi>h</mi><mo>×</mo><mi>w</mi></mrow></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>h</mi><mo>×</mo><mi>w</mi></mrow></munderover><mrow><mrow><mo>|</mo><mrow><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>-</mo><mi>G</mi><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo></mrow><mo>|</mo></mrow></mrow></mstyle></mrow></math></mathml>      (4) </p>
                </div>
                <div class="p1">
                    <p id="73">式中:<i>h</i>和<i>w</i>分别代表输入视频帧的高度和宽度。所有方法在DAVIS测试集和SegTrackV2数据集上的MAE的值如表2所示。</p>
                </div>
                <div class="area_img" id="74">
                    <p class="img_tit"><b>表2 两个公开数据集上MAE值的对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="74" border="1"><tr><td><br />方法</td><td>DAVIS</td><td>SegTrackV2</td></tr><tr><td><br />DRFI</td><td>0.153</td><td>0.097</td></tr><tr><td><br />GAFL</td><td>0.130</td><td>0.101</td></tr><tr><td><br />SAGE</td><td>0.125</td><td>0.100</td></tr><tr><td><br />SGSP</td><td>0.152</td><td>0.130</td></tr><tr><td><br />Ours</td><td>0.044</td><td>0.032</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="75">由表1、表2可知, 本文所提方法的MAE (值越小效果越好) 和F-measure (值越大效果越好) 两个评价指标均优于对比的其他方法, 充分说明了该方法的优越性和有效性。</p>
                </div>
                <h4 class="anchor-tag" id="76" name="76"><b>2.3 必要性分析</b></h4>
                <div class="p1">
                    <p id="77">为了更好地突出所提方法中一些设计思想和细节的必要性, 我们通过实验进行了对比分析。对于双阶段训练的必要性, 对比实验的结果如表3和表4所示。</p>
                </div>
                <div class="area_img" id="78">
                    <p class="img_tit"><b>表3 单双阶段的MAE值对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="78" border="1"><tr><td><br />阶段情况</td><td>DAVIS</td><td>SegTrackV2</td></tr><tr><td><br />单阶段</td><td>0.050</td><td>0.036</td></tr><tr><td><br />双阶段</td><td>0.044</td><td>0.032</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="79">
                    <p class="img_tit"><b>表4 单双阶段的F-measure值对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="79" border="1"><tr><td><br />阶段情况</td><td>DAVIS</td><td>SegTrackV2</td></tr><tr><td><br />单阶段</td><td>0.769</td><td>0.762</td></tr><tr><td><br />双阶段</td><td>0.790</td><td>0.766</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="80">通过对比数据可知, 双阶段得到的模型在两个测试集上的MAE、F-measure的值都得到了进一步的提升, 突出了双阶段训练的必要性。此外, 通过观察表1和表4、表2和表3, 可以发现本文单阶段得到的模型在公开测试集上的MAE、F-measure值都超过了对比的四个模型。可见这些模型利用先验知识手动提取相关特征普遍容易造成获取特征不足的问题, 对比数据也再次证明通过深度学习模型自动获取各个维度特征会得到更好的结果。综合考虑检测效果提升幅度、计算量成本、整体模型复杂度以及避免模型对于训练数据过拟合, 本文采用了双阶段的训练。</p>
                </div>
                <h4 class="anchor-tag" id="81" name="81"><b>2.4 运行时间分析</b></h4>
                <div class="p1">
                    <p id="82">针对运行效率的问题, 本文分别从DAVIS测试集和SegTrackV2数据集随机选择出一个视频序列, 各个方法处理这两个视频的效率对比如表5所示。其中:Cows来自DAVIS数据集;Frog来自SegTrackV2数据集。实验的计算资源包括Intel (R) Core (TM) i5-7300HQ CPU (4核) , 计算机内存8 GB。</p>
                </div>
                <div class="area_img" id="83">
                    <p class="img_tit"><b>表5 各个方法处理同一视频的平均速率对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="83" border="1"><tr><td><br />方法</td><td>Cows/fps</td><td>Frog/fps</td><td>代码</td></tr><tr><td><br />DRFI</td><td>114.40</td><td>6.13</td><td>MATLAB</td></tr><tr><td><br />GAFL</td><td>40.66</td><td>7.74</td><td>MATLAB</td></tr><tr><td><br />SAGE</td><td>52.56</td><td>8.25</td><td>MATLAB</td></tr><tr><td><br />SGSP</td><td>55.93</td><td>8.09</td><td>MATLAB</td></tr><tr><td><br />Ours</td><td>1.75</td><td>1.65</td><td>Python</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="84">从表5中数据可知, 本文方法不需要超像素分割、平滑滤波等预处理以及光流计算, 在处理视频时省去了很大的时间开销, 具有很高的运行效率, 相比其他的方法有数量级的提升。</p>
                </div>
                <h3 id="85" name="85" class="anchor-tag"><b>3 结 语</b></h3>
                <div class="p1">
                    <p id="86">本文提出一种基于深度残差网络的双阶段视频显著性检测方法。该方法一方面通过融合周期性学习率, 使得学习率周期性变化, 保证了模型在训练过程中能够使用最优或者接近最优的学习率, 实现更好、更快的收敛。另一方面, 通过融合残差网络和U-net, 实现了网络模型更深的层次, 进一步丰富了模型提取的不同维度和深度的特征。此外, 该方法又提出了双阶段的训练结构, 通过结合前后帧的信息以及第一阶段得到的显著图, 进一步增强模型对于视频序列时序特征的学习能力。实验结果表明, 本文所提方法在定量和定性的评价标准上都比主流的几个方法更具优势。如何进一步简化模型、降低训练成本以及实现无监督训练模式等, 是未来工作中值得进一步研究的内容。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201810034&amp;v=MDQ2ODF6VFpaTEc0SDluTnI0OUdZSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnRGeWptVnJ6QUw=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> 谢更新, 张恒, 罗正平.EAST 上基于图像显著性的快速边缘提取算法[J].计算机应用与软件, 2018, 35 (10) :189-193.
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Saliency Prior Context Model for RealTime Object Tracking">

                                <b>[2]</b> Ma C, Miao Z, Zhang X P, et al.A Saliency Prior Context Model for Real-Time Object Tracking[J].IEEE Transactions on Multimedia, 2017, 19 (11) :2415-2424.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJDD1685ADBA676AE80B01F9179C203D676&amp;v=MTY2NDR4THU2dzZBPU5qN0Jhc2U1R05uSjN2czNGZTBJQ2cxTXh4Wmg2ajRMUVg3bHBXRTNlYkhnUTcyWkNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkJyTFUwNXR0aA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> Wang J, Jiang H, Yuan Z, et al.Salient Object Detection:A Discriminative Regional Feature Integration Approach[J].International Journal of Computer Vision, 2017, 123 (2) :251-268.
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Global contrast based salient region detection">

                                <b>[4]</b> Cheng M M, Zhang G X, Mitra N J, et al.Global Contrast Based Salient Region Detection[C]//2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) .IEEE, 2011.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Saliency detection for unconstrained videos using superpixel-level graph and spatiotemporal propagation">

                                <b>[5]</b> Liu Z, Li J, Ye L, et al.Saliency Detection for Unconstrained Videos Using Superpixel-level Graph and Spatiotemporal Propagation[J].IEEE Transactions on Circuits and Systems for Video Technology, 2017, 27 (12) :2527-2542.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Consistent video saliency using local gradient flow optimization and global refinement">

                                <b>[6]</b> Wang W, Shen J, Shao L.Consistent Video Saliency Using Local Gradient Flow Optimization and Global Refinement[J].IEEE Transactions on Image Processing, 2015, 24 (11) :4185-4196.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Cyclical Learning Rates for Training Neural Networks[EB]">

                                <b>[7]</b> Smith L N.Cyclical Learning Rates for Training Neural Networks[EB].arXiv:1506.01186, 2015.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Identity mappings in deep residual networks">

                                <b>[8]</b> He K, Zhang X, Ren S, et al.Identity Mappings in Deep Residual Networks[C]//European Conference on Computer Vision.2016:630-645.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=U-Net:Convolutional Networks for Biomedical Image Segmentation">

                                <b>[9]</b> Ronneberger O, Fischer P, Brox T.U-Net:Convolutional Networks for Biomedical Image Segmentation[C]//International Conference on Medical Image Computing and Computer-Assisted Intervention, Springer, Cham, 2015:234-241.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Saliency detection via graph-based manifold ranking">

                                <b>[10]</b> Yang C, Zhang L, Lu H, et al.Saliency Detection via Graph-Based Manifold Ranking[C]//Computer Vision and Pattern Recognition.IEEE, 2013:3166-3173.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Hierarchical Image Saliency Detection on Extended CSSD">

                                <b>[11]</b> Shi J, Yan Q, Xu L, et al.Hierarchical Saliency Detection on Extended CSSD[J].IEEE Transactions on Pattern Analysis &amp; Machine Intelligence, 2014, 38 (4) :717.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Visual saliency detection based on multiscale deep CNN features">

                                <b>[12]</b> Li G, Yu Y.Visual Saliency Detection Based on Multiscale Deep CNN Features[J].IEEE Trans.on Image Processing, 2016, 25 (11) :5012-5024.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=GyGO:an E-commerce Video Object Segmentation Dataset by Visualead">

                                <b>[13]</b> Friedman I, Chemla I, Smolyansky E, et al.GyGO:an E-commerce Video Object Segmentation Dataset by Visualead[DB/OL].[2017-09-10].https://github.com/ilchemla/gygo-dataset.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A benchmark dataset and evaluation methodology for video object segmentation">

                                <b>[14]</b> Perazzi F, Ponttuset J, Mcwilliams B, et al.A Benchmark Dataset and Evaluation Methodology for Video Object Segmentation[C]//Computer Vision and Pattern Recognition, IEEE, 2016:724-732.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Video Segmentation by Tracking Many Figure-Ground Segments">

                                <b>[15]</b> Li F, Kim T, Humayun A, et al.Video Segmentation by Tracking Many Figure-Ground Segments[C]//IEEE International Conference on Computer Vision.IEEE Computer Society, 2013:2192-2199.
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Saliency-aware video object segmentation">

                                <b>[16]</b> Wang W, Shen J, Yang R, et al.Saliency-Aware Video Object Segmentation[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2018, 40 (1) :20-33.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JYRJ201908029" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201908029&amp;v=MTYxNTQ5SGJZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVadEZ5am1WcnpBTHpUWlpMRzRIOWpNcDQ=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hKZG9XL1JrZXNhdGVWVEdvM0tSMD0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
