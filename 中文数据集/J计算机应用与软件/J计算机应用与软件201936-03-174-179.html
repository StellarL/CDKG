<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637136396953721250%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJYRJ201903033%26RESULT%3d1%26SIGN%3dpo4SCfm1EuH1D26fuyYInZRAJ3M%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JYRJ201903033&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JYRJ201903033&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201903033&amp;v=MjQ1OTBac0ZpRGxWcnZKTHpUWlpMRzRIOWpNckk5R1o0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnU=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#51" data-title="&lt;b&gt;0 引 言&lt;/b&gt; "><b>0 引 言</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#60" data-title="&lt;b&gt;1 极限学习机&lt;/b&gt; "><b>1 极限学习机</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#83" data-title="&lt;b&gt;2 实验和结果&lt;/b&gt; "><b>2 实验和结果</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#84" data-title="&lt;b&gt;2.1 实 验&lt;/b&gt;"><b>2.1 实 验</b></a></li>
                                                <li><a href="#93" data-title="&lt;b&gt;2.2 结 果&lt;/b&gt;"><b>2.2 结 果</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#97" data-title="&lt;b&gt;3 结 语&lt;/b&gt; "><b>3 结 语</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#53" data-title="图1 来自不同类别的Tobacco-3482数据集的样本图像">图1 来自不同类别的Tobacco-3482数据集的样本图像</a></li>
                                                <li><a href="#55" data-title="图2 Tobacco-3482数据集广告类的文档, 显示了较高的类内差异">图2 Tobacco-3482数据集广告类的文档, 显示了较高的类内差异</a></li>
                                                <li><a href="#56" data-title="图3 不同类别数据集的类间差异">图3 不同类别数据集的类间差异</a></li>
                                                <li><a href="#80" data-title="图4 CNN与ELM结合网络图">图4 CNN与ELM结合网络图</a></li>
                                                <li><a href="#87" data-title="&lt;b&gt;表1 在数据集Tobacco-3482上采用不同预训练方法得到的准确率比较&lt;/b&gt;"><b>表1 在数据集Tobacco-3482上采用不同预训练方法得到的准确率比较</b></a></li>
                                                <li><a href="#89" data-title="&lt;b&gt;表2 图像分类所需的时间对比&lt;/b&gt;"><b>表2 图像分类所需的时间对比</b></a></li>
                                                <li><a href="#92" data-title="图5 不同ELM分类器与原始网络的平均精度对比图">图5 不同ELM分类器与原始网络的平均精度对比图</a></li>
                                                <li><a href="#96" data-title="图6 ELM的混淆矩阵">图6 ELM的混淆矩阵</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" title=" Afzal M Z, Capobianco S, Malik M I, et al. Deepdocclassifier: document classification with deep convolutional neural network[C]//2015 13th International Conference on Document Analysis and Recognition (ICDAR) . IEEE, 2015:1111-1115." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deepdocclassifier:Document classification with deep Convolutional Neural Network">
                                        <b>[1]</b>
                                         Afzal M Z, Capobianco S, Malik M I, et al. Deepdocclassifier: document classification with deep convolutional neural network[C]//2015 13th International Conference on Document Analysis and Recognition (ICDAR) . IEEE, 2015:1111-1115.
                                    </a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" title=" Kang L, Kumar J, Ye P, et al. Convolutional Neural Networks for Document Image Classification[C]//International Conference on Pattern Recognition. IEEE Computer Society, 2014." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Convolutional Neural Networks for Document Image Classification">
                                        <b>[2]</b>
                                         Kang L, Kumar J, Ye P, et al. Convolutional Neural Networks for Document Image Classification[C]//International Conference on Pattern Recognition. IEEE Computer Society, 2014.
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title=" Harley A W, Ufkes A, Derpanis K G. Evaluation of deep convolutional nets for document image classification and retrieval[C]//13th International Conference on Document Analysis and Recognition (ICDAR) . IEEE, 2015:991-995." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Derpanis.Evaluation of deep convolutional nets for document image classification and retrieval">
                                        <b>[3]</b>
                                         Harley A W, Ufkes A, Derpanis K G. Evaluation of deep convolutional nets for document image classification and retrieval[C]//13th International Conference on Document Analysis and Recognition (ICDAR) . IEEE, 2015:991-995.
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" title=" Kumar J, Ye P, Doermann D. Learning document structure for retrieval and classification[C]//Proceedings of the 21st International Conference on Pattern Recognition (ICPR2012) . IEEE, 2012: 1558-561." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning document structure for retrieval and classification">
                                        <b>[4]</b>
                                         Kumar J, Ye P, Doermann D. Learning document structure for retrieval and classification[C]//Proceedings of the 21st International Conference on Pattern Recognition (ICPR2012) . IEEE, 2012: 1558-561.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" title=" Chen S, He Y, Sun J, et al. Structured document classification by matching local salient features[C]//Proceedings of the 21st International Conference on Pattern Recognition (ICPR2012) . IEEE, 2012:653-656." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Structured document classification by matching local salient features">
                                        <b>[5]</b>
                                         Chen S, He Y, Sun J, et al. Structured document classification by matching local salient features[C]//Proceedings of the 21st International Conference on Pattern Recognition (ICPR2012) . IEEE, 2012:653-656.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" title=" Reddy K V U. Form classification[C]//Proceedings of SPIE—The International Society for Optical Engineering, 2008, 6815." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Form classification">
                                        <b>[6]</b>
                                         Reddy K V U. Form classification[C]//Proceedings of SPIE—The International Society for Optical Engineering, 2008, 6815.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" title=" Tang B, He H, Baggenstoss P, et al. A Bayesian Classification Approach Using Class-Specific Features for Text Categorization[J]. IEEE Transactions on Knowledge and Data Engineering, 2016, 28 (6) :1602-1606." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Bayesian classification approach using class-specific features for text categorization">
                                        <b>[7]</b>
                                         Tang B, He H, Baggenstoss P, et al. A Bayesian Classification Approach Using Class-Specific Features for Text Categorization[J]. IEEE Transactions on Knowledge and Data Engineering, 2016, 28 (6) :1602-1606.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" title=" Diab D M, El Hindi K M. Using differential evolution for fine tuning naive Bayesian classifiers and its application for text classification[J]. Applied Soft Computing, 2017, 54:183-199." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES585765EE04FD1F9E7AE77FBE8E4D3EF8&amp;v=MTkyMTUwNXRwZ3pMaTZ4S2c9TmlmT2ZiYXdHOWJLcXZvd1pPOTVlSDFQeG1NVW0wcDZUd21RMlJwQWZjYVhNTXlYQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQnJMVQ==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                         Diab D M, El Hindi K M. Using differential evolution for fine tuning naive Bayesian classifiers and its application for text classification[J]. Applied Soft Computing, 2017, 54:183-199.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" title=" Shin C, Doermann D S. Document Image Retrieval Based on Layout Structural Similarity[C]//Proceedings of the 2006 International Conference on Image Processing, Computer Vision, &amp;amp; Pattern Recognition, Las Vegas, Nevada, USA, June 26-29, 2006, Volume 2. DBLP, 2006." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Document image retrieval based on layout structural similarity">
                                        <b>[9]</b>
                                         Shin C, Doermann D S. Document Image Retrieval Based on Layout Structural Similarity[C]//Proceedings of the 2006 International Conference on Image Processing, Computer Vision, &amp;amp; Pattern Recognition, Las Vegas, Nevada, USA, June 26-29, 2006, Volume 2. DBLP, 2006.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_10" title=" Collins-Thompson K, Nickolov R. A clustering-based algorithm for automatic document separation[C]//Research &amp;amp; Development in Information Retrieval. 2007." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A clustering-based algorithm for automatic document separation">
                                        <b>[10]</b>
                                         Collins-Thompson K, Nickolov R. A clustering-based algorithm for automatic document separation[C]//Research &amp;amp; Development in Information Retrieval. 2007.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_11" title=" Kumar J, Ye P, Doermann D. Structural similarity for document image classification and retrieval[J]. Pattern Recognition Letters, 2014, 43:119-126." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14010600191904&amp;v=MjQxNzZyUmRHZXJxUVRNbndaZVp0RmlubFVyM0pLRjRSYmhJPU5pZk9mYks4SHRETXFZOUZaZUlPQlh3OW9CTVQ2VDRQUUgvaQ==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[11]</b>
                                         Kumar J, Ye P, Doermann D. Structural similarity for document image classification and retrieval[J]. Pattern Recognition Letters, 2014, 43:119-126.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_12" title=" Shunsuke K, Ryunosuke K, Donahue I. End-to-end text classification via image-based embedding using character-level networks[EB].arXiv preprint arXiv:1810.03595v2, 2018." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=End-to-end text classification via image-based embedding using character-level networks[EB]">
                                        <b>[12]</b>
                                         Shunsuke K, Ryunosuke K, Donahue I. End-to-end text classification via image-based embedding using character-level networks[EB].arXiv preprint arXiv:1810.03595v2, 2018.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_13" title=" Sharad J, Suraj S, Nitin K. First steps toward CNN based source classification of document images shared over messaging app[EB]. arXiv preprint arXiv:1808.05941v1, 2018." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=First steps toward CNN based source classification of document images shared over messaging app[EB]">
                                        <b>[13]</b>
                                         Sharad J, Suraj S, Nitin K. First steps toward CNN based source classification of document images shared over messaging app[EB]. arXiv preprint arXiv:1808.05941v1, 2018.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_14" title=" Wang H, Feng L, Kong A, et al. Multi-view reconstructive preserving embedding for dimension reduction[EB]. arXiv preprint arXiv:1807.10614v1, 2018." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multi-view reconstructive preserving embedding for dimension reduction[EB]">
                                        <b>[14]</b>
                                         Wang H, Feng L, Kong A, et al. Multi-view reconstructive preserving embedding for dimension reduction[EB]. arXiv preprint arXiv:1807.10614v1, 2018.
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_15" title=" Praveen K, Jawahar C V. HWNet v2: An efficient word image representation for handwritten documents[EB]. arXiv preprint arXiv:1802.06194v1, 2018." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=HWNet v2: An efficient word image representation for handwritten documents[EB]">
                                        <b>[15]</b>
                                         Praveen K, Jawahar C V. HWNet v2: An efficient word image representation for handwritten documents[EB]. arXiv preprint arXiv:1802.06194v1, 2018.
                                    </a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_16" title=" Das A, Roy S, Bhattacharya U, et al. Document Image Classification with Intra-Domain Transfer Learning and Stacked Generalization of Deep Convolutional Neural Networks[EB]. arXiv preprint arXiv:1801.09321v3, 2018." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Document Image Classification with Intra-Domain Transfer Learning and Stacked Generalization of Deep Convolutional Neural Networks[EB]">
                                        <b>[16]</b>
                                         Das A, Roy S, Bhattacharya U, et al. Document Image Classification with Intra-Domain Transfer Learning and Stacked Generalization of Deep Convolutional Neural Networks[EB]. arXiv preprint arXiv:1801.09321v3, 2018.
                                    </a>
                                </li>
                                <li id="35">


                                    <a id="bibliography_17" title=" Roy S, Das A, Bhattacharya U. Generalized stacking of layerwise-trained Deep Convolutional Neural Networks for document image classification[C]//2016 23rd International Conference on Pattern Recognition (ICPR) . IEEE, 2016:1273-1278." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Generalized stacking of layerwise-trained Deep Convolutional Neural Networks for document image classification">
                                        <b>[17]</b>
                                         Roy S, Das A, Bhattacharya U. Generalized stacking of layerwise-trained Deep Convolutional Neural Networks for document image classification[C]//2016 23rd International Conference on Pattern Recognition (ICPR) . IEEE, 2016:1273-1278.
                                    </a>
                                </li>
                                <li id="37">


                                    <a id="bibliography_18" title=" Csurka G, Larlus D, Gordo A, et al. What is the right way to represent document images?[EB]. arXiv preprint arXiv:1603.01076, 2016." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=What is the right way to represent document images?[EB]">
                                        <b>[18]</b>
                                         Csurka G, Larlus D, Gordo A, et al. What is the right way to represent document images?[EB]. arXiv preprint arXiv:1603.01076, 2016.
                                    </a>
                                </li>
                                <li id="39">


                                    <a id="bibliography_19" title=" Afzal M Z, Andreas K&#246;lsch, Ahmed S, et al. Cutting the error by half: investigation of very deep CNN and advanced training strategies for document image classification[C]//Iapr International Conference on Document Analysis &amp;amp; Recognition. IEEE Computer Society, 2017." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Cutting the error by half: investigation of very deep CNN and advanced training strategies for document image classification">
                                        <b>[19]</b>
                                         Afzal M Z, Andreas K&#246;lsch, Ahmed S, et al. Cutting the error by half: investigation of very deep CNN and advanced training strategies for document image classification[C]//Iapr International Conference on Document Analysis &amp;amp; Recognition. IEEE Computer Society, 2017.
                                    </a>
                                </li>
                                <li id="41">


                                    <a id="bibliography_20" title=" Krizhevsky A, Sutskever I, Hinton G. Imagenet classification with deep convolutional neural networks[C]//Proceedings of the 25th International Conference on Neural Information Processing Systems—Volume 1. Curran Associates Inc., 2012:1097-1105." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Image Net Classification with Deep Convolutional Neural Networks">
                                        <b>[20]</b>
                                         Krizhevsky A, Sutskever I, Hinton G. Imagenet classification with deep convolutional neural networks[C]//Proceedings of the 25th International Conference on Neural Information Processing Systems—Volume 1. Curran Associates Inc., 2012:1097-1105.
                                    </a>
                                </li>
                                <li id="43">


                                    <a id="bibliography_21" title=" Huang G B, Zhu Q Y, Siew C K. Extreme learning machine: A new learning scheme of feedforward neural networks[C]//Neural Networks, 2004. Proceedings. 2004 IEEE International Joint Conference on. IEEE, 2004:985-990." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Extreme learning machine:a new learning scheme of feedforward neural networks">
                                        <b>[21]</b>
                                         Huang G B, Zhu Q Y, Siew C K. Extreme learning machine: A new learning scheme of feedforward neural networks[C]//Neural Networks, 2004. Proceedings. 2004 IEEE International Joint Conference on. IEEE, 2004:985-990.
                                    </a>
                                </li>
                                <li id="45">


                                    <a id="bibliography_22" title=" Huang G B, Zhu Q Y, Siew C K. Extreme learning machine: Theory and applications[J]. Neurocomputing, 2006, 70 (1-3) :489-501." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501913101&amp;v=MDIxMjI5RWJlb01EWHc0b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcjNKS0Y0UmJoST1OaWZPZmJLN0h0RE5xbw==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[22]</b>
                                         Huang G B, Zhu Q Y, Siew C K. Extreme learning machine: Theory and applications[J]. Neurocomputing, 2006, 70 (1-3) :489-501.
                                    </a>
                                </li>
                                <li id="47">


                                    <a id="bibliography_23" title=" Kotsiantis S B. Supervised machine learning: A review of classification techniques[J]. Informatica, 2007, 31 (3) :249-268." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Supervised machine learning: a review of classification techniques">
                                        <b>[23]</b>
                                         Kotsiantis S B. Supervised machine learning: A review of classification techniques[J]. Informatica, 2007, 31 (3) :249-268.
                                    </a>
                                </li>
                                <li id="49">


                                    <a id="bibliography_24" title=" Jia Y, Shelhamer E, Donahue J, et al. Caffe: convolutional architecture for fast feature embedding[EB]. arXiv preprint arXiv:1408.5093, 2014." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Caffe: convolutional architecture for fast feature embedding[EB]">
                                        <b>[24]</b>
                                         Jia Y, Shelhamer E, Donahue J, et al. Caffe: convolutional architecture for fast feature embedding[EB]. arXiv preprint arXiv:1408.5093, 2014.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JYRJ" target="_blank">计算机应用与软件</a>
                2019,36(03),174-179 DOI:10.3969/j.issn.1000-386x.2019.03.032            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于深度CNN和极限学习机相结合的实时文档分类</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%97%AB%E6%B2%B3&amp;code=23290520&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">闫河</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E9%B9%8F&amp;code=22545900&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王鹏</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%91%A3%E8%8E%BA%E8%89%B3&amp;code=39481741&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">董莺艳</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%BD%97%E6%88%90&amp;code=40711088&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">罗成</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%8E%E7%84%95&amp;code=40711089&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">李焕</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E9%87%8D%E5%BA%86%E7%90%86%E5%B7%A5%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0046910&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">重庆理工大学计算机科学与工程学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E9%87%8D%E5%BA%86%E7%90%86%E5%B7%A5%E5%A4%A7%E5%AD%A6%E4%B8%A4%E6%B1%9F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AD%A6%E9%99%A2&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">重庆理工大学两江人工智能学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>提出一种文档图像实时分类训练和测试的方法。在实际应用中, 数据训练的精确性和高效性在文档图像识别中起着关键的作用。现有的深度学习方法不能满足此要求, 因为需要大量的时间用于训练和微调深层次的网络架构。针对此问题, 提出一种基于计算机视觉的新方法:第一阶段训练深度网络, 作为特征提取器;第二阶段用极限学习机 (ELM) 用于分类。该方法的性能优于目前最先进的基于深度学习的相关方法, 在Tobacco-3482数据集上的最终准确率为83.45%。与之前基于卷积神经网络 (CNN) 的方法相比, 相对误差降低了26%。ELM的训练时间仅为1.156秒, 对2 482张图像的整体预测时间是3.083秒。因此, 该文档分类方法适合于大规模实时应用。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%96%87%E6%A1%A3%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">文档图像分类;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=CNN&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">CNN;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">迁移学习;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    闫河, 教授, 主研领域:深度学习, 图像识别。;
                                </span>
                                <span>
                                    王鹏, 硕士生。;
                                </span>
                                <span>
                                    董莺艳, 硕士生。;
                                </span>
                                <span>
                                    罗成, 硕士生。;
                                </span>
                                <span>
                                    李焕, 硕士生。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-10-09</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金面上项目 (61173184);</span>
                                <span>重庆市自然科学基金项目 (cstc2018jcyjAX0694);</span>
                    </p>
            </div>
                    <h1><b>REAL-TIME DOCUMENT CLASSIFICATION BASED ON DEEP CNN AND EXTREME LEARNING MACHINE</b></h1>
                    <h2>
                    <span>Yan He</span>
                    <span>Wang Peng</span>
                    <span>Dong Yingyan</span>
                    <span>Luo Cheng</span>
                    <span>Li Huan</span>
            </h2>
                    <h2>
                    <span>College of Computer Science, Chongqing University of Teachnology</span>
                    <span>Artificial Intelligence College, Chongqing University of Teachnology</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>This paper presented a real-time training and testing method for document image classification. In practical applications, the accuracy and efficiency of data training play a key role in document image recognition. The existing deep learning methods cannot meet this requirement, because they need a lot of time to train and fine-tune the deep network architecture. To solve this problem, we proposed a new method based on computer vision. The method was divided into two steps: the depth network was trained as a feature extractor; we used the extreme learning machine (ELM) for classification. The performance of this method is superior to the advanced methods based on deep learning. The final accuracy of this method on Tobacco-3482 dataset is 83.45%. Compared with the method based on convolution neural network, the relative error is reduced by 26%. The training time of ELM is only 1.156 s, and the overall prediction time of 2 482 images is 3.083 s. Therefore, the method is suitable for large-scale real-time applications.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Document%20image%20classification&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Document image classification;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=CNN&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">CNN;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Migration%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Migration learning;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-10-09</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="51" name="51" class="anchor-tag"><b>0 引 言</b></h3>
                <div class="p1">
                    <p id="52">如今, 商业文件 (见图1) 通常由文档分析系统 (DAS) 进行处理, 以减少工作人员的工作量。DAS的一项重要任务是对文档进行分类, 即确定文档所指的业务流程的类型。典型的文档类是发票、地址变更或索赔等。文档分类方法可分为基于图像<citation id="100" type="reference"><link href="3" rel="bibliography" /><link href="5" rel="bibliography" /><link href="7" rel="bibliography" /><link href="9" rel="bibliography" /><link href="11" rel="bibliography" /><link href="13" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>,<a class="sup">3</a>,<a class="sup">4</a>,<a class="sup">5</a>,<a class="sup">6</a>]</sup></citation>和基于内容的方法<citation id="101" type="reference"><link href="15" rel="bibliography" /><link href="17" rel="bibliography" /><sup>[<a class="sup">7</a>,<a class="sup">8</a>]</sup></citation>。DAS选取哪一种方法更合适, 通常取决于用户处理的文档。像通常的字母一样, 自由格式的文档通常需要基于内容的分类, 而在不同布局中包含相同文本的表单则可以通过基于图像的方法来区分。</p>
                </div>
                <div class="area_img" id="53">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201903033_053.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 来自不同类别的Tobacco-3482数据集的样本图像" src="Detail/GetImg?filename=images/JYRJ201903033_053.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 来自不同类别的Tobacco-3482数据集的样本图像  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201903033_053.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="54">然而, 并不总是事先知道文档属于什么类别, 这就是为什么在基于图像的方法和基于内容的方法之间很难选择的原因。一般来说, 基于图像的方法是大多数学者首选的方法, 因为它直接工作在数字图像上。由于文档图像类的多样性, 存在高类内方差和低类间方差的类, 分别如图2和图3所示。因此, 很难找到用于文档图像分类的人工特征提取方法。</p>
                </div>
                <div class="area_img" id="55">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201903033_055.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 Tobacco-3482数据集广告类的文档, 显示了较高的类内差异" src="Detail/GetImg?filename=images/JYRJ201903033_055.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 Tobacco-3482数据集广告类的文档, 显示了较高的类内差异  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201903033_055.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="56">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201903033_056.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 不同类别数据集的类间差异" src="Detail/GetImg?filename=images/JYRJ201903033_056.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 不同类别数据集的类间差异  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201903033_056.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="57">近几年, 随着深度学习的发展, 该技术已经应用到众多领域。众多学者将深度学习应用于文档结构学习中, 使用CNN自动学习并提取文档图像中的特征, 然后对文档图像进行分类。然而, 同样在这种方法中, 即使使用GPU训练此过程也非常耗时。通过以上分析, 以下简要概述相关研究发展历程。</p>
                </div>
                <div class="p1">
                    <p id="58">文献<citation id="102" type="reference">[<a class="sup">9</a>]</citation>使用布局和结构相似性方法进行文档匹配, 而文献<citation id="103" type="reference">[<a class="sup">10</a>]</citation>将基于文本和布局的特征结合起来。2012年, 文献<citation id="104" type="reference">[<a class="sup">4</a>]</citation>中提出了一种文档分类的方法, 该方法依赖于从文档图像的图像块中派生出的编码码字符。在文档学习中编码字典是以一种无监督的学习方式。为此, 该方法递归地将图像划分为块, 并使用图像块中字的直方图来建模图像块之间的空间联系。两年后, 同一位作者还提出了另一种方法, 即建立文档图像SURF描述符的编码记录<citation id="105" type="reference"><link href="23" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>, 并用之前所提方法, 运用这些特征用于文档分类。Chen等<citation id="106" type="reference"><link href="11" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>提出了一种利用低层图像特征对文档进行分类的方法。然而, 它们的方法仅限于结构化文档。以上方法大部分都局限于结构化文件。文献<citation id="107" type="reference">[<a class="sup">6</a>]</citation>中使用二进制图像中的像素信息对表单文档进行分类。该方法利用k均值算法对图像进行像素密度分类。在文献<citation id="108" type="reference">[<a class="sup">12</a>]</citation>中, 为了准确地识别出中文、日语、泰语等文档, 本文提出了CE-CLCNN的深度卷积网络结构。这种结构是基于端到端的学习模型, 并通过对文档的每个字符作为文档处理。实验表明, 该方法取得了不错的识别分类效果。文献<citation id="109" type="reference">[<a class="sup">13</a>]</citation>中运用卷积神经网络识别文档图像, 并运用智能手机的相机提取文档字符, 解决了文档因权限不能下载等问题, 通过在文档图片上的对比实验取得了较好的效果。文献<citation id="110" type="reference">[<a class="sup">14</a>]</citation>中提出了一种多视角重构方法, 将高维数据映射到低维空间, 通过降维处理, 并应用于文档分类识别, 通过实验验证了此方法有效。文献<citation id="111" type="reference">[<a class="sup">15</a>]</citation>提出预训练网络结构, 并通过训练学习不同大小的文档图像, 以增加训练的数据量, 在英文和印度文上的实验结果表明, 此方法具有更好的识别效果。文献<citation id="112" type="reference">[<a class="sup">16</a>]</citation>提出了基于区域的深度卷积神经网络框架, 用于文档学习, 在ImageNet数据集中, 通过预训练vgg-16网络结构中导出权重来训练文档分类器, 从而实现“域间”转移学习。文献<citation id="113" type="reference">[<a class="sup">17</a>]</citation>运用了轻量级的神经网络训练Tobacco-3482数据集, 在没有使用迁移学习的条件下, 取得了不错的效果。在文献<citation id="114" type="reference">[<a class="sup">18</a>]</citation>中, 比较了RVL-CDIP数据集上的使用AlexNet和Google网络架构的性能, 显示出比常规方法更高的鲁棒性。同时, 在文献<citation id="115" type="reference">[<a class="sup">19</a>]</citation>分别用AlexNet、VGG-16、Google和ResNet-50模型对RVL-CDIP和Tobacco3482数据集进行了迁移学习测试。尽管上述基于CNN的深度学习方法在鲁棒性等方面有了很大的提升, 但是大部分网络的训练非常耗时。为了使深度神经网络CNN表现出最佳性能同时满足实时训练要求, 本文提出使用CNNs<citation id="116" type="reference"><link href="41" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>和极限学习机 (ELM) <citation id="117" type="reference"><link href="43" rel="bibliography" /><link href="45" rel="bibliography" /><sup>[<a class="sup">21</a>,<a class="sup">22</a>]</sup></citation>相结合的方法。</p>
                </div>
                <div class="p1">
                    <p id="59">在本文中, 我们提出使用极限学习机 (ELM) 的方法完成实时训练。为了克服人工特征提取和长时间训练的困难, 我们设计了一种将深度CNN的自动特征学习与高效的极限学习机相结合的方法。此方法共有两阶段:第一阶段是深度神经网络的训练并将其用作特征提取器;第二阶段用ELM进行分类。ELM的本质不同于其他神经网络, 具有高效迅速等特点。结果表明, 在一幅图像上平均训练时间仅需1 ms, 因此显示出对于实时性能的要求。同时, 该方法使得神经网络非常适合在增量学习框架中使用。</p>
                </div>
                <h3 id="60" name="60" class="anchor-tag"><b>1 极限学习机</b></h3>
                <div class="p1">
                    <p id="61">ELM是一种单隐层前馈网络 (SLFN) <citation id="119" type="reference"><link href="43" rel="bibliography" /><link href="45" rel="bibliography" /><sup>[<a class="sup">21</a>,<a class="sup">22</a>]</sup></citation>的算法。ELM主要的思想是模仿生物行为, 而一般的神经网络训练使用反向传播调整权值, ELM不需要该步骤。一个ELM是通过在两个不同但连续的阶段更新权重来学习, 利用了随机特征映射和最小二乘拟合。在第一阶段, 输入层和隐藏层之间的权重被随机初始化;在第二阶段, 运用线性最小二乘优化的方法, 因此不需要反向传播。ELM与其他学习算法的区别在于将输入特征映射到一个随机空间, 然后在该阶段进行学习。在监督学习中设置的每一个输入样本都有一个相关的类别标签, <i>x</i>和<i>t</i>分别代表输入样本和类别标签。让<i>X</i>和<i>T</i>代表<i>N</i>个样本的集合并表示为{<i>X</i>, <i>T</i>}={<i>x</i><sub><i>k</i></sub>, <i>t</i><sub><i>k</i></sub>}<mathml id="62"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup></mrow></math></mathml>, 其中<i>x</i><sub><i>k</i></sub>∈ℝ<sub><i>d</i></sub>和<i>t</i><sub><i>k</i></sub>∈ℝ<sup><i>m</i></sup>是第<i>K</i>个输入的<i>d</i>维和<i>m</i>维的目标向量, 并寻找功能函数把输入向量映射到目标向量。虽然这类函数有许多复杂的形式<citation id="118" type="reference"><link href="41" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>, 但其中一个简单而有效的函数是单隐层前馈网络 (SLFN) 。关于上述设置, 具有<i>N</i>个隐藏节点的单层网络可以描述如下:</p>
                </div>
                <div class="p1">
                    <p id="63"><mathml id="64"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>o</mi><msub><mrow></mrow><mi>j</mi></msub><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mi>β</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mi>g</mi><mo stretchy="false"> (</mo><mi>w</mi><msubsup><mrow></mrow><mi>i</mi><mtext>Τ</mtext></msubsup><mi>x</mi><msub><mrow></mrow><mi>j</mi></msub><mo>+</mo><mi>b</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo></mrow></math></mathml>      (1) </p>
                </div>
                <div class="p1">
                    <p id="65">式中:<i>w</i><sub><i>i</i></sub>是连接第<i>i</i>个隐含节点和输出节点的权值矩阵;<i>β</i><sub><i>i</i></sub>是第<i>i</i>个节点的输出权值向量;<i>b</i><sub><i>i</i></sub>是偏置;函数<i>g</i>为relu、sigmoid等激活函数。</p>
                </div>
                <div class="p1">
                    <p id="66">以上是前馈神经网络的描述, 对于极限学习机的输入和输出权值{<i>w</i><sub><i>i</i></sub>, <i>b</i><sub><i>i</i></sub>}<mathml id="67"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></msubsup></mrow></math></mathml>是随机产生的。第二阶段, 利用正则化线性最小二乘法对隐层和输出层的连接参数进行优化。假设<i>ψ</i> (<i>x</i><sub><i>j</i></sub>) 是从隐藏层到输入<i>x</i><sub><i>i</i></sub>的响应向量, <i>B</i>是连接隐藏层和输出层的输出参数。ELM最小化平方损失之和为:</p>
                </div>
                <div class="area_img" id="129">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JYRJ201903033_12900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="70">式中:正则化是为了避免过拟合, 其中<i>C</i>是调整系数, 通过计算<i>H</i>=[[<i>ψ</i> (<i>x</i><sub>1</sub>) ]<sup>T</sup>, [<i>ψ</i> (<i>x</i><sub>2</sub>) ]<sup>T</sup>, …, [<i>ψ</i> (<i>x</i><sub><i>N</i></sub>) ]<sup>T</sup>]<sup>T</sup>和<i>T</i>=[<i>t</i><sub>1</sub>, <i>t</i><sub>2</sub>, …, <i>t</i><sub><i>N</i></sub>]可以得到如下的最优化问题, 并称为岭回归:</p>
                </div>
                <div class="area_img" id="130">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JYRJ201903033_13000.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="73">上述问题是凸优化问题, 并受下列线性条件的约束。</p>
                </div>
                <div class="p1">
                    <p id="74"><i>B</i>+<i>CH</i><sup>T</sup> (<i>T</i>-<i>HB</i>) =0      (4) </p>
                </div>
                <div class="p1">
                    <p id="75">该线性系统可以用数值方法求解, 从而得到最优解<i>B</i><sup>*</sup>。</p>
                </div>
                <div class="p1">
                    <p id="76"><mathml id="77"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>B</mi><msup><mrow></mrow><mo>*</mo></msup><mo>=</mo><mrow><mo> (</mo><mrow><mi>Η</mi><msup><mrow></mrow><mtext>Τ</mtext></msup><mi>Η</mi><mo>+</mo><mfrac><mrow><mi>Ι</mi><msub><mrow></mrow><mi>Ν</mi></msub></mrow><mi>C</mi></mfrac></mrow><mo>) </mo></mrow><msup><mrow></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mi>Η</mi><mi>Τ</mi></mrow></math></mathml>      (5) </p>
                </div>
                <div class="p1">
                    <p id="78">本文方法不需要高分辨率的文档特征, 如光学字符识别。相反, 它完全依赖于文档的结构和布局来对它们进行分类。</p>
                </div>
                <div class="p1">
                    <p id="79">本文提出的网络体系结构是基于AlexNet<citation id="120" type="reference"><link href="43" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>的CNN网络。它由五个卷积层和一个极限学习机组成。与原有的AlexNet体系结构一样, 在最后一个最大池层之后, 我们得到了256个大小为6×6的特征映射 (如图4所示) 。虽然AlexNet使用多个完全连接的层来对生成的特征映射进行分类, 但我们建议使用单层ELM。卷积层的权重是在一个大型数据集上预先训练成一个完整的AlexNet网络, 此网络有三个全连接层和标准反向传播机制。在训练结束后, 全连接层被丢弃, 卷积层被固定, 并作为特征提取器。然后, 由CNN提取的特征向量作为ELM训练和测试的输入向量。该体系结构中使用的ELM是一种单层前馈神经网络。当目标数据集有10类时, 我们用隐藏层中的2 000个神经元和10个输出神经元对极限学习机进行测试。隐含层神经元以sigmiod作为激活函数。</p>
                </div>
                <div class="area_img" id="80">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201903033_080.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 CNN与ELM结合网络图" src="Detail/GetImg?filename=images/JYRJ201903033_080.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 CNN与ELM结合网络图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201903033_080.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="81">在一个大型数据集上训练一个完整的AlexNet, 为ELM提供一个有效的特征提取器, 然后在目标数据集上对ELM进行训练。具体来说, 在数据集上训练AlexNet网络, 其中数据中包含16个类。因此, AlexNet最后一个全连接层中的神经元数目从1 000变为16。除了最后一个网络层之外, 所有的网络层都是使用在ImageNet上预先训练过的AlexNet网络模型, 并保留此模型的初始化条件。训练使用随机梯度下降, 批量大小为25, 初始学习速率为0.001, 动量为0.9, 重量衰减为0.000 5。为了防止过拟合, 第六层和第七层配置dropout ratio为0.5。经过40次迭代的训练后, 完成了整个训练过程。本文用Caffe框架<citation id="121" type="reference"><link href="49" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>来训练这个模型。</p>
                </div>
                <div class="p1">
                    <p id="82">极限学习机被用来训练和评估包含10个类别的图像Tobacco-3482数据集<citation id="122" type="reference"><link href="23" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>。这些图像通过CNN训练后并将第五个池化层的激活值传递给ELM (全连接层) 。</p>
                </div>
                <h3 id="83" name="83" class="anchor-tag"><b>2 实验和结果</b></h3>
                <h4 class="anchor-tag" id="84" name="84"><b>2.1 实 验</b></h4>
                <div class="p1">
                    <p id="85">在本文中, 使用了两个数据集。首先, 我们使用Ryerson视觉实验室复杂文档信息处理 (RVL-CDIP) 数据集<citation id="123" type="reference"><link href="7" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>来训练一个完整的AlexNet。此数据集包含400 000幅图像, 其中分布着16个类, 320 000幅用于训练, 40 000幅用于验证和测试。</p>
                </div>
                <div class="p1">
                    <p id="86">其次, 我们使用Tobacco-3482数据集<citation id="124" type="reference"><link href="23" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>对提出的ELM进行训练, 并对其性能进行评价。训练结果如表1所示。此数据集包含来自10个文档类的3 482幅图像。由于两个数据集之间存在一些重叠, 因此我们将包含在大数据集中的两个数据集排除。AlexNet并不是对32万幅图像进行训练, 而是只对319 784幅图像进行训练。</p>
                </div>
                <div class="area_img" id="87">
                    <p class="img_tit"><b>表1 在数据集Tobacco-3482上采用不同预训练方法得到的准确率比较</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="87" border="1"><tr><td><br />网络结构</td><td>准确率/%</td></tr><tr><td><br />Structural methods<sup>[1]</sup></td><td>40.30</td></tr><tr><td><br />AlexNet (Image Net) </td><td>75.73</td></tr><tr><td><br />Alex Net-ELM (ImageNet) </td><td>73.77</td></tr><tr><td><br />Alex Net (RVL-CDIP) </td><td>90.05</td></tr><tr><td><br />Alex Net-ELM (RVL-CDIP) </td><td>83.24</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="88">为了与Tobacco-3482数据集上的其他方法进行公正的比较, 我们使用了与Kang<citation id="125" type="reference"><link href="5" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>和Harley<citation id="126" type="reference"><link href="7" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>类似的评估标准。具体来说, 我们只使用Tobacco-3482数据集的子集进行训练, 从每类10幅图像到每类100幅图像不等, 剩下的图像用于测试。由于数据集非常小, 因此对于每个数据集分为10个不同的数据集来训练和评估分类器, 并得到评估的性能。训练时间对比如表2所示。</p>
                </div>
                <div class="area_img" id="89">
                    <p class="img_tit"><b>表2 图像分类所需的时间对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="89" border="1"><tr><td><br />网络结构</td><td>训练</td><td>测试</td></tr><tr><td><br />AlexNet (GPU) </td><td>10 min 34 s</td><td>3 480 ms</td></tr><tr><td><br />AlexNet-ELM (GPU) </td><td>1 176 ms</td><td>3 066 ms</td></tr><tr><td><br />AlexNet (CUP) </td><td>6 h 44 min 8 s</td><td>4 min 30 s</td></tr><tr><td><br />AlexNet-ELM (CPU) </td><td>1 min 26 s</td><td>4 min 19 s</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="90">由于本实验需要最初的网络结构AlexNet, 因而需要对AlexNet进行训练, 通过运用ImageNet预先训练好完整的网络结构, 与Afzal等<citation id="127" type="reference"><link href="3" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>使用Tobacco-3482数据集训练网络一样。前面我们训练了多个版本的网络, 每个训练数据的大小划分为10个不同的区间, 即每类图像分别为10, 20, …, 100, 共训练了100个网络。这些实验的训练数据集进一步细分为用于实际训练的数据集 (80%) 和用于验证的数据集 (20%) 。</p>
                </div>
                <div class="p1">
                    <p id="91">我们对319 784幅RVL-CDIP语料库的图像进行了初始化AlexNet的训练, 并丢弃了网络中全连接的部分。保留的网络结构被用作特征提取器来训练和测试极限学习机。极限学习机在Tobacco-3482数据集上训练。由于这些网络初始权值都是随机初始化, 我们为100个分区中的每一类训练10个ELM, 并得到每个分区的平均精度。训练对比如图5所示。</p>
                </div>
                <div class="area_img" id="92">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201903033_092.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 不同ELM分类器与原始网络的平均精度对比图" src="Detail/GetImg?filename=images/JYRJ201903033_092.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 不同ELM分类器与原始网络的平均精度对比图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201903033_092.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="93" name="93"><b>2.2 结 果</b></h4>
                <div class="p1">
                    <p id="94">本文提出的分类器性能与图5中描述的Artis状态相比较, 带有文档预训练的ELM分类器的性能已经优于当前最先进的技术水平。每类100个训练样本, 测试准确率由75.73%提高到83.24%, 相比较减少30%以上的误差。随着识别性能的提高, 训练和测试所需的运行时间也减少了。特别是在GPU加速训练的情况下, 本文方法比当前先进水平快500多倍。对于训练和测试, CNN与ELM相结合的方法每幅图像识别只需要约1 ms, 从而实现了实时性。超过90%运行耗时用于特征提取, 使用不同的CNN架构可以进一步加快速度。采用ImageNet预训练的ELM分类器达到了与目前最先进水平相当的精度, 其计算成本仅为计算量的一小部分。</p>
                </div>
                <div class="p1">
                    <p id="95">图6显示了对每类100幅图像进行训练的示例性ELM分类器的混淆矩阵。可以看出, 科学这一类文本是迄今为止最难辨认的。这个结果是与Afzal等<citation id="128" type="reference"><link href="3" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>的实验结果一致, 同时也可以解释科学类与报告类之间的低类间差异。</p>
                </div>
                <div class="area_img" id="96">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201903033_096.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 ELM的混淆矩阵" src="Detail/GetImg?filename=images/JYRJ201903033_096.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 ELM的混淆矩阵  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201903033_096.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h3 id="97" name="97" class="anchor-tag"><b>3 结 语</b></h3>
                <div class="p1">
                    <p id="98">本文主要解决了文档分类和实时训练的问题, 提出了针对文档分类的实时训练方法。本文方法主要分为两个步骤:首先选用深度神经网络完成对特征的有效提取;然后运用极限学习机对数据的高效训练。使用极限学习机后, 数据的训练效率和训练的时间明显提升。通过在众多评定标准和对比实验下证明了本文方法的有效性和鲁棒性。对于文档分类识别领域是一次重大的突破。</p>
                </div>
                <div class="p1">
                    <p id="99">下一步研究方向是如何快速提取图像特征, 因为在本文方法中, 超过90%的时间用于从深层神经网络中提取特征。另一个的研究方向是在一个高性能集群中对googlenet和Resnet-50与ELM结合的分类器进行进一步性能测试。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deepdocclassifier:Document classification with deep Convolutional Neural Network">

                                <b>[1]</b> Afzal M Z, Capobianco S, Malik M I, et al. Deepdocclassifier: document classification with deep convolutional neural network[C]//2015 13th International Conference on Document Analysis and Recognition (ICDAR) . IEEE, 2015:1111-1115.
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Convolutional Neural Networks for Document Image Classification">

                                <b>[2]</b> Kang L, Kumar J, Ye P, et al. Convolutional Neural Networks for Document Image Classification[C]//International Conference on Pattern Recognition. IEEE Computer Society, 2014.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Derpanis.Evaluation of deep convolutional nets for document image classification and retrieval">

                                <b>[3]</b> Harley A W, Ufkes A, Derpanis K G. Evaluation of deep convolutional nets for document image classification and retrieval[C]//13th International Conference on Document Analysis and Recognition (ICDAR) . IEEE, 2015:991-995.
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning document structure for retrieval and classification">

                                <b>[4]</b> Kumar J, Ye P, Doermann D. Learning document structure for retrieval and classification[C]//Proceedings of the 21st International Conference on Pattern Recognition (ICPR2012) . IEEE, 2012: 1558-561.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Structured document classification by matching local salient features">

                                <b>[5]</b> Chen S, He Y, Sun J, et al. Structured document classification by matching local salient features[C]//Proceedings of the 21st International Conference on Pattern Recognition (ICPR2012) . IEEE, 2012:653-656.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Form classification">

                                <b>[6]</b> Reddy K V U. Form classification[C]//Proceedings of SPIE—The International Society for Optical Engineering, 2008, 6815.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Bayesian classification approach using class-specific features for text categorization">

                                <b>[7]</b> Tang B, He H, Baggenstoss P, et al. A Bayesian Classification Approach Using Class-Specific Features for Text Categorization[J]. IEEE Transactions on Knowledge and Data Engineering, 2016, 28 (6) :1602-1606.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES585765EE04FD1F9E7AE77FBE8E4D3EF8&amp;v=MzE1NDMwNXRwZ3pMaTZ4S2c9TmlmT2ZiYXdHOWJLcXZvd1pPOTVlSDFQeG1NVW0wcDZUd21RMlJwQWZjYVhNTXlYQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQnJMVQ==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b> Diab D M, El Hindi K M. Using differential evolution for fine tuning naive Bayesian classifiers and its application for text classification[J]. Applied Soft Computing, 2017, 54:183-199.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Document image retrieval based on layout structural similarity">

                                <b>[9]</b> Shin C, Doermann D S. Document Image Retrieval Based on Layout Structural Similarity[C]//Proceedings of the 2006 International Conference on Image Processing, Computer Vision, &amp; Pattern Recognition, Las Vegas, Nevada, USA, June 26-29, 2006, Volume 2. DBLP, 2006.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A clustering-based algorithm for automatic document separation">

                                <b>[10]</b> Collins-Thompson K, Nickolov R. A clustering-based algorithm for automatic document separation[C]//Research &amp; Development in Information Retrieval. 2007.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_11" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14010600191904&amp;v=MTUyODFlWnRGaW5sVXIzSktGNFJiaEk9TmlmT2ZiSzhIdERNcVk5RlplSU9CWHc5b0JNVDZUNFBRSC9pclJkR2VycVFUTW53Wg==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[11]</b> Kumar J, Ye P, Doermann D. Structural similarity for document image classification and retrieval[J]. Pattern Recognition Letters, 2014, 43:119-126.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=End-to-end text classification via image-based embedding using character-level networks[EB]">

                                <b>[12]</b> Shunsuke K, Ryunosuke K, Donahue I. End-to-end text classification via image-based embedding using character-level networks[EB].arXiv preprint arXiv:1810.03595v2, 2018.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=First steps toward CNN based source classification of document images shared over messaging app[EB]">

                                <b>[13]</b> Sharad J, Suraj S, Nitin K. First steps toward CNN based source classification of document images shared over messaging app[EB]. arXiv preprint arXiv:1808.05941v1, 2018.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multi-view reconstructive preserving embedding for dimension reduction[EB]">

                                <b>[14]</b> Wang H, Feng L, Kong A, et al. Multi-view reconstructive preserving embedding for dimension reduction[EB]. arXiv preprint arXiv:1807.10614v1, 2018.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=HWNet v2: An efficient word image representation for handwritten documents[EB]">

                                <b>[15]</b> Praveen K, Jawahar C V. HWNet v2: An efficient word image representation for handwritten documents[EB]. arXiv preprint arXiv:1802.06194v1, 2018.
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Document Image Classification with Intra-Domain Transfer Learning and Stacked Generalization of Deep Convolutional Neural Networks[EB]">

                                <b>[16]</b> Das A, Roy S, Bhattacharya U, et al. Document Image Classification with Intra-Domain Transfer Learning and Stacked Generalization of Deep Convolutional Neural Networks[EB]. arXiv preprint arXiv:1801.09321v3, 2018.
                            </a>
                        </p>
                        <p id="35">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Generalized stacking of layerwise-trained Deep Convolutional Neural Networks for document image classification">

                                <b>[17]</b> Roy S, Das A, Bhattacharya U. Generalized stacking of layerwise-trained Deep Convolutional Neural Networks for document image classification[C]//2016 23rd International Conference on Pattern Recognition (ICPR) . IEEE, 2016:1273-1278.
                            </a>
                        </p>
                        <p id="37">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=What is the right way to represent document images?[EB]">

                                <b>[18]</b> Csurka G, Larlus D, Gordo A, et al. What is the right way to represent document images?[EB]. arXiv preprint arXiv:1603.01076, 2016.
                            </a>
                        </p>
                        <p id="39">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Cutting the error by half: investigation of very deep CNN and advanced training strategies for document image classification">

                                <b>[19]</b> Afzal M Z, Andreas Kölsch, Ahmed S, et al. Cutting the error by half: investigation of very deep CNN and advanced training strategies for document image classification[C]//Iapr International Conference on Document Analysis &amp; Recognition. IEEE Computer Society, 2017.
                            </a>
                        </p>
                        <p id="41">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Image Net Classification with Deep Convolutional Neural Networks">

                                <b>[20]</b> Krizhevsky A, Sutskever I, Hinton G. Imagenet classification with deep convolutional neural networks[C]//Proceedings of the 25th International Conference on Neural Information Processing Systems—Volume 1. Curran Associates Inc., 2012:1097-1105.
                            </a>
                        </p>
                        <p id="43">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Extreme learning machine:a new learning scheme of feedforward neural networks">

                                <b>[21]</b> Huang G B, Zhu Q Y, Siew C K. Extreme learning machine: A new learning scheme of feedforward neural networks[C]//Neural Networks, 2004. Proceedings. 2004 IEEE International Joint Conference on. IEEE, 2004:985-990.
                            </a>
                        </p>
                        <p id="45">
                            <a id="bibliography_22" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501913101&amp;v=MTE4OTNGNFJiaEk9TmlmT2ZiSzdIdEROcW85RWJlb01EWHc0b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcjNKSw==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[22]</b> Huang G B, Zhu Q Y, Siew C K. Extreme learning machine: Theory and applications[J]. Neurocomputing, 2006, 70 (1-3) :489-501.
                            </a>
                        </p>
                        <p id="47">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Supervised machine learning: a review of classification techniques">

                                <b>[23]</b> Kotsiantis S B. Supervised machine learning: A review of classification techniques[J]. Informatica, 2007, 31 (3) :249-268.
                            </a>
                        </p>
                        <p id="49">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Caffe: convolutional architecture for fast feature embedding[EB]">

                                <b>[24]</b> Jia Y, Shelhamer E, Donahue J, et al. Caffe: convolutional architecture for fast feature embedding[EB]. arXiv preprint arXiv:1408.5093, 2014.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JYRJ201903033" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201903033&amp;v=MjQ1OTBac0ZpRGxWcnZKTHpUWlpMRzRIOWpNckk5R1o0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnU=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDN5c2Q1VWdqSmZRZ2l1bjJRM2RRZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
