<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637135556053752500%26DBCODE%3dCJFD%26TABLEName%3dCJFDTEMP%26FileName%3dJYRJ201910022%26RESULT%3d1%26SIGN%3dYUBA5bqBhxPgru02BnATTuxf8BA%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JYRJ201910022&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JYRJ201910022&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201910022&amp;v=MjgyODY0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVadEZpRGhVYnpNTHpUWlpMRzRIOWpOcjQ5SFpvUUtESDg0dlI=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#49" data-title="&lt;b&gt;0 引 言&lt;/b&gt; "><b>0 引 言</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#56" data-title="&lt;b&gt;1 抑郁症识别算法&lt;/b&gt; "><b>1 抑郁症识别算法</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#59" data-title="&lt;b&gt;1.1 全局部分&lt;/b&gt;"><b>1.1 全局部分</b></a></li>
                                                <li><a href="#67" data-title="&lt;b&gt;1.2 局部部分&lt;/b&gt;"><b>1.2 局部部分</b></a></li>
                                                <li><a href="#88" data-title="&lt;b&gt;1.3 网络级联&lt;/b&gt;"><b>1.3 网络级联</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#94" data-title="&lt;b&gt;2 实 验&lt;/b&gt; "><b>2 实 验</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#96" data-title="&lt;b&gt;2.1 抑郁症数据集&lt;/b&gt;"><b>2.1 抑郁症数据集</b></a></li>
                                                <li><a href="#101" data-title="&lt;b&gt;2.2 实验设置&lt;/b&gt;"><b>2.2 实验设置</b></a></li>
                                                <li><a href="#106" data-title="&lt;b&gt;2.3 结果与分析&lt;/b&gt;"><b>2.3 结果与分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#118" data-title="&lt;b&gt;3 结 语&lt;/b&gt; "><b>3 结 语</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#58" data-title="图1 抑郁症识别算法流程图">图1 抑郁症识别算法流程图</a></li>
                                                <li><a href="#62" data-title="图2 DNN的结构">图2 DNN的结构</a></li>
                                                <li><a href="#83" data-title="图3 AR-LGC的子块">图3 AR-LGC的子块</a></li>
                                                <li><a href="#87" data-title="图4 眼睛区域局部特征提取流程图">图4 眼睛区域局部特征提取流程图</a></li>
                                                <li><a href="#158" data-title="图5 抑郁症数据集样本">图5 抑郁症数据集样本</a></li>
                                                <li><a href="#158" data-title="图5 抑郁症数据集样本">图5 抑郁症数据集样本</a></li>
                                                <li><a href="#108" data-title="图6 不同邻域大小和Batch size在AVEC2013的
抑郁症识别结果">图6 不同邻域大小和Batch size在AVEC2013的
抑郁症识别结果</a></li>
                                                <li><a href="#109" data-title="图7 不同邻域大小和Batch size在AVEC2014的
抑郁症识别结">图7 不同邻域大小和Batch size在AVEC2014的
抑郁症识别结</a></li>
                                                <li><a href="#111" data-title="&lt;b&gt;表1 使用全局和局部特征的抑郁症识别结果&lt;/b&gt;"><b>表1 使用全局和局部特征的抑郁症识别结果</b></a></li>
                                                <li><a href="#114" data-title="&lt;b&gt;表2 使用不同的局部特征的抑郁症识别结果&lt;/b&gt;"><b>表2 使用不同的局部特征的抑郁症识别结果</b></a></li>
                                                <li><a href="#116" data-title="&lt;b&gt;表3 与在AVEC2013的其他识别方法的比较&lt;/b&gt;"><b>表3 与在AVEC2013的其他识别方法的比较</b></a></li>
                                                <li><a href="#117" data-title="&lt;b&gt;表4 与在AVEC2014的其他识别方法的比较&lt;/b&gt;"><b>表4 与在AVEC2014的其他识别方法的比较</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" title=" World Health Organization[OL].https://www.who.int/mental_health/management /depression/en." target="_blank"
                                       href="">
                                        <b>[1]</b>
                                         World Health Organization[OL].https://www.who.int/mental_health/management /depression/en.
                                    </a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" title=" Mathers C,Boerma J,Fat D.The global burden of disease:2004 update[M].Geneva,Switzerland:WHO,2008." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The global burden of disease:2004 update">
                                        <b>[2]</b>
                                         Mathers C,Boerma J,Fat D.The global burden of disease:2004 update[M].Geneva,Switzerland:WHO,2008.
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title=" First M B.Structured clinical interview for DSM-IV-TR axis I disorders:patient edition[M].Biometrics Research Department,Columbia University,2005." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Structured clinical interview for DSM-IV-TR axis I disorders:patient edition">
                                        <b>[3]</b>
                                         First M B.Structured clinical interview for DSM-IV-TR axis I disorders:patient edition[M].Biometrics Research Department,Columbia University,2005.
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" title=" Beck A T,Steer R A,Ball R,et al.Comparison of Beck Depression Inventories-IA and-II in psychiatric outpatients[J].Journal of personality assessment,1996,67(3):588-597." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=STJD&amp;filename=STJD785038960&amp;v=MTYxMDVSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnRGaURoVWJ6TU5qbkJhclN3RzlIUHA0WkRaSVFLREg4NHY=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                         Beck A T,Steer R A,Ball R,et al.Comparison of Beck Depression Inventories-IA and-II in psychiatric outpatients[J].Journal of personality assessment,1996,67(3):588-597.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" title=" Birdwhistell R L.Toward analyzing American movement[M]//Weitz S.Nonverbal Communication:Readings With Commentary.New York,NY,USA:Oxford Univ.Press,1974:134-143." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Toward analyzing American movement">
                                        <b>[5]</b>
                                         Birdwhistell R L.Toward analyzing American movement[M]//Weitz S.Nonverbal Communication:Readings With Commentary.New York,NY,USA:Oxford Univ.Press,1974:134-143.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" >
                                        <b>[6]</b>
                                     Esteva A,Kuprel B,Novoa R A,et al.Dermatologist-level classification of skin cancer with deep neural networks[J].Nature,2017,542(7639):115.</a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" title=" Yang Y,Fairbairn C,Cohn J F.Detecting depression severity from vocal prosody[J].IEEE Transactions on Affective Computing,2013,4(2):142-150." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Detecting Depression Severity from Vocal Prosody">
                                        <b>[7]</b>
                                         Yang Y,Fairbairn C,Cohn J F.Detecting depression severity from vocal prosody[J].IEEE Transactions on Affective Computing,2013,4(2):142-150.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" title=" Valstar M,Schuller B,Smith K,et al.AVEC 2013:the continuous audio/visual emotion and depression recognition challenge[C]//3rd ACM international workshop on Audio/visual emotion challenge,2013:3-10." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=AVEC2013:the continuous audio/visual emotion and depression recognition challenge">
                                        <b>[8]</b>
                                         Valstar M,Schuller B,Smith K,et al.AVEC 2013:the continuous audio/visual emotion and depression recognition challenge[C]//3rd ACM international workshop on Audio/visual emotion challenge,2013:3-10.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" title=" Valstar M,Schuller B,Smith K,et al.Avec 2014:3d dimensional affect and depression recognition challenge[C]//4th International Workshop on Audio/Visual Emotion Challenge,2014:3-10." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Avec 2014:3d dimensional affect and depression recognition challenge">
                                        <b>[9]</b>
                                         Valstar M,Schuller B,Smith K,et al.Avec 2014:3d dimensional affect and depression recognition challenge[C]//4th International Workshop on Audio/Visual Emotion Challenge,2014:3-10.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_10" title=" Cummins N,Joshi J,Dhall A,et al.Diagnosis of depression by behavioural signals:a multimodal approach[C]//3rd ACM international workshop on Audio/visual emotion challenge,2013:11-20." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Diagnosis of depression by behavioural signals:a multimodal approach">
                                        <b>[10]</b>
                                         Cummins N,Joshi J,Dhall A,et al.Diagnosis of depression by behavioural signals:a multimodal approach[C]//3rd ACM international workshop on Audio/visual emotion challenge,2013:11-20.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_11" title=" Meng H,Huang D,Wang H,et al.Depression recognition based on dynamic facial and vocal expression features using partial least square regression[C]//3rd ACM international workshop on Audio/visual emotion challenge,2013:21-30." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Depression recognition based on dynamic facial and vocal expression features using partial least square regression">
                                        <b>[11]</b>
                                         Meng H,Huang D,Wang H,et al.Depression recognition based on dynamic facial and vocal expression features using partial least square regression[C]//3rd ACM international workshop on Audio/visual emotion challenge,2013:21-30.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_12" title=" Wen L,Li X,Guo G,et al.Automated depression diagnosis based on facial dynamic analysis and sparse coding[J].IEEE Transactions on Information Forensics and Security,2015,10(7):1432-1441." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Automated depression diagnosis based on facial dynamic analysis and sparse coding">
                                        <b>[12]</b>
                                         Wen L,Li X,Guo G,et al.Automated depression diagnosis based on facial dynamic analysis and sparse coding[J].IEEE Transactions on Information Forensics and Security,2015,10(7):1432-1441.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_13" title=" Kaya H,&#199;illi F,Salah A A.Ensemble CCA for continuous emotion prediction[C]//4th International Workshop on Audio/Visual Emotion Challenge,2014:19-26." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Ensemble CCA for continuous emotion prediction">
                                        <b>[13]</b>
                                         Kaya H,&#199;illi F,Salah A A.Ensemble CCA for continuous emotion prediction[C]//4th International Workshop on Audio/Visual Emotion Challenge,2014:19-26.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_14" title=" Zhu Y,Shang Y,Shao Z,et al.Automated depression diagnosis based on deep networks to encode facial appearance and dynamics[J].IEEE Transactions on Affective Computing,2018,9(4):578-584." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Automated depression diagnosis based on deep networks to encode facial appearance and dynamics">
                                        <b>[14]</b>
                                         Zhu Y,Shang Y,Shao Z,et al.Automated depression diagnosis based on deep networks to encode facial appearance and dynamics[J].IEEE Transactions on Affective Computing,2018,9(4):578-584.
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_15" title=" Kang Y,Jiang X,Yin Y,et al.Deep Transformation Learning for Depression Diagnosis from Facial Images[C]//Chinese Conference on Biometric Recognition,2017:13-22." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep transformation learning for depression diagnosis from facial images">
                                        <b>[15]</b>
                                         Kang Y,Jiang X,Yin Y,et al.Deep Transformation Learning for Depression Diagnosis from Facial Images[C]//Chinese Conference on Biometric Recognition,2017:13-22.
                                    </a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_16" title=" Pampouchidou A,Pediaditis M,Maridaki A,et al.Quantitative comparison of motion history image variants for video-based depression assessment[J].EURASIP Journal on Image and Video Processing,2017,2017(1):64." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Quantitative comparison of motion history image variants for video-based depression assessment">
                                        <b>[16]</b>
                                         Pampouchidou A,Pediaditis M,Maridaki A,et al.Quantitative comparison of motion history image variants for video-based depression assessment[J].EURASIP Journal on Image and Video Processing,2017,2017(1):64.
                                    </a>
                                </li>
                                <li id="35">


                                    <a id="bibliography_17" title=" Schroff F,Kalenichenko D,Philbin J.Facenet:A unified embedding for face recognition and clustering[C]//IEEE conference on computer vision and pattern recognition,2015:815-823." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Facenet:A unified embedding for face recognition and clustering">
                                        <b>[17]</b>
                                         Schroff F,Kalenichenko D,Philbin J.Facenet:A unified embedding for face recognition and clustering[C]//IEEE conference on computer vision and pattern recognition,2015:815-823.
                                    </a>
                                </li>
                                <li id="37">


                                    <a id="bibliography_18" title=" Shao Z,Shu H,Wu J,et al.Quaternion Bessel–Fourier moments and their invariant descriptors for object reconstruction and recognition[J].Pattern Recognition,2014,47(2):603-611." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES15122300161997&amp;v=MjQ0ODhIOVBPckk5RlplME9CWFUrb0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcnpKS0ZvV2FSWT1OaWZPZmJLOQ==&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[18]</b>
                                         Shao Z,Shu H,Wu J,et al.Quaternion Bessel–Fourier moments and their invariant descriptors for object reconstruction and recognition[J].Pattern Recognition,2014,47(2):603-611.
                                    </a>
                                </li>
                                <li id="39">


                                    <a id="bibliography_19" title=" Lan R,Zhou Y,Tang Y Y.Quaternionic local ranking binary pattern:a local descriptor of color images[J].IEEE Transactions on Image Processing,2016,25(2):566-579." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Quaternionic local ranking binary pattern:A local descriptor of color images">
                                        <b>[19]</b>
                                         Lan R,Zhou Y,Tang Y Y.Quaternionic local ranking binary pattern:a local descriptor of color images[J].IEEE Transactions on Image Processing,2016,25(2):566-579.
                                    </a>
                                </li>
                                <li id="41">


                                    <a id="bibliography_20" title=" Chen B,Yang J,Jeon B,et al.Kernel quaternion principal component analysis and its application in RGB-D object recognition[J].Neurocomputing,2017,266:293-303." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES06F52878856D908523BA2F3BD6018735&amp;v=MTYyODJ3PU5pZk9mYk8rYU5UT3A0aE5iTzRKZUhVNXh4TVI2VTBNU2duaDNtWXplYk9jUXJtYUNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkJyTFUwNXR0Z3pMeTl3Ng==&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[20]</b>
                                         Chen B,Yang J,Jeon B,et al.Kernel quaternion principal component analysis and its application in RGB-D object recognition[J].Neurocomputing,2017,266:293-303.
                                    </a>
                                </li>
                                <li id="43">


                                    <a id="bibliography_21" title=" Zhu Z,Jin L,Song E,et al.Quaternion switching vector median filter based on local reachability density[J].IEEE Signal Processing Letters,2018,25(6):843-847." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Quaternion switching vector median filter based on local reachability density">
                                        <b>[21]</b>
                                         Zhu Z,Jin L,Song E,et al.Quaternion switching vector median filter based on local reachability density[J].IEEE Signal Processing Letters,2018,25(6):843-847.
                                    </a>
                                </li>
                                <li id="45">


                                    <a id="bibliography_22" title=" Jan A,Meng H,Gaus Y F A,et al.Automatic depression scale prediction using facial expression dynamics and regression[C]//4th International Workshop on Audio/Visual Emotion Challenge,2014:73-80." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Automatic depression scale prediction using facial expression dynamics and regression">
                                        <b>[22]</b>
                                         Jan A,Meng H,Gaus Y F A,et al.Automatic depression scale prediction using facial expression dynamics and regression[C]//4th International Workshop on Audio/Visual Emotion Challenge,2014:73-80.
                                    </a>
                                </li>
                                <li id="47">


                                    <a id="bibliography_23" title=" Espinosa H P,Escalante H J,Villaseňor-Pineda L,et al.Fusing affective dimensions and audio-visual features from segmented video for depression recognition:INAOE-BUAP&#39;s participation at AVEC&#39;14 challenge[C]//Proceedings of the 4th International Workshop on Audio/Visual Emotion Challenge.ACM,2014:49-55." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fusing affective dimensions and audio-visual features from segmented video for depression recognition:INAOE-BUAP&amp;#39;&amp;#39;s participation at AVEC&amp;#39;&amp;#39;14 challenge">
                                        <b>[23]</b>
                                         Espinosa H P,Escalante H J,Villaseňor-Pineda L,et al.Fusing affective dimensions and audio-visual features from segmented video for depression recognition:INAOE-BUAP&#39;s participation at AVEC&#39;14 challenge[C]//Proceedings of the 4th International Workshop on Audio/Visual Emotion Challenge.ACM,2014:49-55.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JYRJ" target="_blank">计算机应用与软件</a>
                2019,36(10),117-122+150 DOI:10.3969/j.issn.1000-386x.2019.10.021            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于级联深度神经网络的抑郁症识别</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%B1%9F%E7%AD%B1&amp;code=38569710&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">江筱</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%82%B5%E7%8F%A0%E5%AE%8F&amp;code=34964239&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">邵珠宏</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%B0%9A%E5%AA%9B%E5%9B%AD&amp;code=21887500&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">尚媛园</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E4%B8%81%E8%BE%89&amp;code=23740820&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">丁辉</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E9%A6%96%E9%83%BD%E5%B8%88%E8%8C%83%E5%A4%A7%E5%AD%A6%E4%BF%A1%E6%81%AF%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0044636&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">首都师范大学信息工程学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E7%94%B5%E5%AD%90%E7%B3%BB%E7%BB%9F%E5%8F%AF%E9%9D%A0%E6%80%A7%E6%8A%80%E6%9C%AF%E5%8C%97%E4%BA%AC%E5%B8%82%E9%87%8D%E7%82%B9%E5%AE%9E%E9%AA%8C%E5%AE%A4&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">电子系统可靠性技术北京市重点实验室</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%8C%97%E4%BA%AC%E6%88%90%E5%83%8F%E6%8A%80%E6%9C%AF%E9%AB%98%E7%B2%BE%E5%B0%96%E5%88%9B%E6%96%B0%E4%B8%AD%E5%BF%83&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">北京成像技术高精尖创新中心</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>抑郁症是最常见的心理障碍之一,严重困扰患者的工作和生活。随着情感感知技术的发展,开发抑郁症自动识别系统具有广阔的前景。基于视频人脸图像,结合级联深度神经网络和多特征(全局特征和局部特征)对抑郁症BDI-II分值进行预测。设计全局特征网络、局部特征网络(眼部,嘴部)三个分支,利用FaceNet网络和深度神经网络提取全局特征,利用基于四元数的局部二进制编码和深度神经网络提取局部特征。在融合层将全局特征向量和局部特征向量拼接,接入第三个深度神经网络对抑郁程度进行预测。在AVEC2013和AVEC 2014抑郁症数据库上进行测试,实验结果表明,与其他基于视觉的方法相比,该方法取得了更小的平均绝对误差和均方根误差。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%8A%91%E9%83%81%E7%97%87%E8%AF%86%E5%88%AB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">抑郁症识别;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深层神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%9B%9B%E5%85%83%E6%95%B0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">四元数;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%B1%80%E9%83%A8%E4%BA%8C%E5%80%BC%E6%A8%A1%E5%BC%8F&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">局部二值模式;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%BC%82%E6%88%96%E9%9D%9E%E5%AF%B9%E7%A7%B0%E5%8C%BA%E5%9F%9F%E5%B1%80%E9%83%A8%E6%A2%AF%E5%BA%A6%E7%BC%96%E7%A0%81&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">异或非对称区域局部梯度编码;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    江筱，硕士生，主研领域:计算机视觉。;
                                </span>
                                <span>
                                    邵珠宏，副教授。;
                                </span>
                                <span>
                                    尚媛园，教授。;
                                </span>
                                <span>
                                    丁辉，副教授。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-01-07</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金项目(61876112);</span>
                                <span>北京市属高校高水平教师队伍建设支持计划项目(CIT&amp;TCD20170322);</span>
                                <span>首都师范大学青年科研创新团队项目;</span>
                    </p>
            </div>
                    <h1><b>DEPRESSION RECOGNITION BASED ON CASCADED DEEP NEURAL NETWORKS</b></h1>
                    <h2>
                    <span>Jiang Xiao</span>
                    <span>Shao Zhuhong</span>
                    <span>Shang Yuanyuan</span>
                    <span>Ding Hui</span>
            </h2>
                    <h2>
                    <span>College of Information Engineering, Capital Normal University</span>
                    <span>Beijing Key Laboratory of Electronic System Reliability Technology</span>
                    <span>Beijing Advanced Innovation Center for Imaging Technology</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Depression is one of the most common psychological disorders, which seriously disturbs the work and life of patients. With the development of affective sensing technology, the development of automatic depression recognition system has broad prospects. In this paper, based on video face images, we combined cascaded depth neural networks with multi-features(global and local features) to predict the BDI-II score of depression. We designed three branches in this framework: global feature network, local feature network(eye, mouth). FaceNet network and depth neural network were used to extract global features, and quaternion-based local binary coding and depth neural network were used to extract local features. In the merger layer, the global eigenvectors and local eigenvectors were joined together, and the third deep neural network was used to predict the degree of depression. The proposed method has been tested on AVEC2013 and AVEC 2014 depression databases. The experimental results show that compared with other vision-based methods, this method achieves smaller mean absolute error and root mean square error.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Depression%20recognition&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Depression recognition;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Deep%20neural%20networks&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Deep neural networks;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Quaternion&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Quaternion;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Local%20binary%20patterns&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Local binary patterns;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=XOR%20asymmetric%20region%20local%20gradient%20coding&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">XOR asymmetric region local gradient coding;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2019-01-07</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="49" name="49" class="anchor-tag"><b>0 引 言</b></h3>
                <div class="p1">
                    <p id="50">抑郁症是世界范围内最常见的精神疾病之一,其主要特点是情绪低落持续时间长,严重影响个人的日常生活和工作。据报道,重度抑郁症(MDD)是导致残疾的主要原因<citation id="134" type="reference"><link href="3" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>。根据世界卫生组织(WHO)的一项调查,抑郁症是全球第四大致残原因,并将在2020年成为头号致残原因<citation id="135" type="reference"><link href="5" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="51">目前,对抑郁症的筛查和诊断主要是根据患者在临床访谈中的自我报告。在临床访谈中,由一名经过专门培训且经验丰富的精神卫生专业人员利用《精神障碍诊断与统计手册》(DSM-IV)中规定的标准<citation id="136" type="reference"><link href="7" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>,来评估患者是否患有抑郁症以及严重程度。除此之外,临床实践中的贝克抑郁量表(Beck Depression Inventory,BDI)<citation id="137" type="reference"><link href="9" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>等自我报告量表也可作为诊断依据。贝克抑郁症量表中包含21个问题,每一个答案的分值为0～3,总分数范围为0～63。根据BDI-II<citation id="138" type="reference"><link href="9" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>,总分小于13分为“无”抑郁,14～19分为“轻度”抑郁,20～28分为“中度”抑郁,总分大于29分为“重度”抑郁。</p>
                </div>
                <div class="p1">
                    <p id="52">研究表明,在人类的非语言行为中,人脸面部区域表情包含了大部分信息<citation id="139" type="reference"><link href="11" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>,利用人脸面部信息进行临床心理健康分析的研究引起了广泛关注<citation id="140" type="reference"><link href="15" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>。随着情感感知和深度学习等技术的进步<citation id="141" type="reference"><link href="13" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>,用计算机辅助抑郁症自动识别将有利于客观的诊断。</p>
                </div>
                <div class="p1">
                    <p id="53">音频/视觉情感挑战(AVEC2013)<citation id="142" type="reference"><link href="17" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>和AVEC2014<citation id="143" type="reference"><link href="19" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>的抑郁症识别竞赛关注的是对抑郁症自动预测方法的研究,预测患者贝克抑郁症量表II(BDI-II)分值。</p>
                </div>
                <div class="p1">
                    <p id="54">在AVEC2013抑郁症识别中,首先对人脸图像做检测与对齐的预处理,然后提取局部相位量化(LPQ)特征,最后采用支持向量回归(SVR)来预测抑郁得分。Cummins等<citation id="144" type="reference"><link href="21" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>在他们的抑郁症识别系统中,对时间兴趣点(SITPs)和梯度直方图金字塔(PHOG)做了对比,实验表明,梯度直方图金字塔方法取得了较好的效果。Meng等<citation id="145" type="reference"><link href="23" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>利用基于动态特征图(MMH)来描述视频的运动信息,然后提取边缘方向直方图(EOH)和局部二值模式(LBP)特征,最后利用最小二乘法(PLS)进行回归分析。Wen等<citation id="146" type="reference"><link href="25" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation> 首先研究了从面部区域子空间中提取的三个正交平面的动态描述子TOP-LPQ特征,利用sparse编码计算行为模式字典,最后应用判别映射和决策融合来提高诊断精度。</p>
                </div>
                <div class="p1">
                    <p id="55">在AVEC2014抑郁症识别中,首先提取局部动态外观描述符(LGBP-TOP),然后采用支持向量回归预测得分。Kaya等<citation id="147" type="reference"><link href="27" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>通过典型相关分析(CCA)和Moore-Penrose广义逆(MPGI)引入了一个区域线性回归集合。为了提高抑郁预测性能,Zhu等<citation id="148" type="reference"><link href="29" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>提出了一种双流框架,分别命名为外观DCNN和动态DCNN。Kang等<citation id="149" type="reference"><link href="31" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>引入了一种基于视频数据的深度转换学习(DTL)方法用于抑郁症的识别,该方法可以捕获抑郁症数据的非线性映射。Pampouchidou 等<citation id="150" type="reference"><link href="33" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>通过使用来自Gabor抑制滤波数据的运动历史图像(MHI)的变体来代替原始图像,研究了抑郁症的分类评估。但以上方法只关注人脸全局信息,没有考虑到局部特征对抑郁症识别精度的影响。本文结合了全局特征和局部特征以及深度神经网络实现了对抑郁症自动识别,实现了全局信息和局部信息的融合互补。实验结果表明,该方法具有较好的综合性能,大大提高了人脸图像预测抑郁症BDI-II分值的精确度。同时,本文采用了级联深度神经网络提取图像特征,加强了面部重点区域对抑郁症识别的影响,表达能力与针对性更强,对抑郁症的识别效果比一般的CNN神经网络算法<citation id="151" type="reference"><link href="29" rel="bibliography" /><link href="31" rel="bibliography" /><sup>[<a class="sup">14</a>,<a class="sup">15</a>]</sup></citation>效果好。</p>
                </div>
                <h3 id="56" name="56" class="anchor-tag"><b>1 抑郁症识别算法</b></h3>
                <div class="p1">
                    <p id="57">为了充分利用图像的全局信息和人脸区域的局部信息,使两种信息达到互补的效果,本文提取全局特征、眼部局部特征和嘴部局部特征后分别输入gloablDNN、localDNN1和localDNN2,经过深度变换后在融合层拼接,最后输入到DNN网络预测抑郁症评分。算法流程图如图1所示。</p>
                </div>
                <div class="area_img" id="58">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201910022_058.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 抑郁症识别算法流程图" src="Detail/GetImg?filename=images/JYRJ201910022_058.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 抑郁症识别算法流程图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201910022_058.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="59" name="59"><b>1.1 全局部分</b></h4>
                <div class="p1">
                    <p id="60">在全局部分中,全局特征由FaceNet<citation id="152" type="reference"><link href="35" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>提取。FaceNet是一个已经在包含453 453幅人脸图像的数据库上预训练好的提取人脸特征的网络。把从FaceNet中提取出的全局特征输入globalDNN以获得一个更紧致的特征表达。</p>
                </div>
                <div class="p1">
                    <p id="61">DNN结构如图2所示。</p>
                </div>
                <div class="area_img" id="62">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201910022_062.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 DNN的结构" src="Detail/GetImg?filename=images/JYRJ201910022_062.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 DNN的结构  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201910022_062.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="63">假设DNN共有<i>N</i>层,第一层的输出表示为:</p>
                </div>
                <div class="p1">
                    <p id="64"><i>y</i><sup>(1)</sup>=<i>f</i>(<i><b>W</b></i><sup>(1)</sup><i>x</i>+<i><b>b</b></i><sup>(1)</sup>)      (1)</p>
                </div>
                <div class="p1">
                    <p id="65">式中:<i><b>W</b></i><sup>(1)</sup>是权重矩阵,<i><b>b</b></i><sup>(1)</sup>是偏移矩阵,<i>f</i>是非线性激活函数,输入<i>y</i><sup>(1)</sup>同时是第二层的输入。在DNN模型中的第<i>m</i>层的输出表示为:</p>
                </div>
                <div class="p1">
                    <p id="66"><i>y</i><sup>(</sup><sup><i>m</i></sup><sup>)</sup>=<i>f</i>(<i><b>W</b></i><sup>(</sup><sup><i>m</i></sup><sup>)</sup><i>y</i><sup>(</sup><sup><i>m</i></sup><sup>-1)</sup>+<i><b>b</b></i><sup>(</sup><sup><i>m</i></sup><sup>)</sup>)      (2)</p>
                </div>
                <h4 class="anchor-tag" id="67" name="67"><b>1.2 局部部分</b></h4>
                <div class="p1">
                    <p id="68">在局部特征部分,本文结合了局部二值编码和异或非对称区域局部梯度编码,对人脸图像中的眼睛和嘴巴区域提取手工特征,分别将特征输入局部网络进行处理。基于彩色图像的四元数处理和分析方法得到了广泛的研究,其目标是将彩色图像的各个通道作为整体进行编码,如四元数贝塞尔傅里叶矩<citation id="153" type="reference"><link href="37" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>、四元数局部排序二值模式<citation id="154" type="reference"><link href="39" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>、核四元数主成分分析<citation id="155" type="reference"><link href="41" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>、四元数中值滤波转换矢量<citation id="156" type="reference"><link href="43" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="69"><i>f</i><sub>e</sub>(<i>x</i>,<i>y</i>)代表图像块的眼睛区域,可以表示为四元数矩阵:</p>
                </div>
                <div class="p1">
                    <p id="70"><i>f</i><sub>q</sub>(<i>x</i>,<i>y</i>)=<i>if</i><sub>e,R</sub>(<i>x</i>,<i>y</i>)+<i>jf</i><sub>e,G</sub>(<i>x</i>,<i>y</i>)+<i>kf</i><sub>e,B</sub>(<i>x</i>,<i>y</i>)      (3)</p>
                </div>
                <div class="p1">
                    <p id="71">式中:<i>f</i><sub>e,R</sub>(<i>x</i>,<i>y</i>) 、 <i>f</i><sub>e,G</sub>(<i>x</i>,<i>y</i>) 、 <i>f</i><sub>e,B</sub>(<i>x</i>,<i>y</i>) 分别是彩色图像<i>f</i><sub>e</sub>(<i>x</i>,<i>y</i>)的红、绿、蓝三个通道。 为了获得相位 L1,首先将式(3)进行Clifford变换,即用<i>f</i><sub>q</sub>(<i>x</i>,<i>y</i>)乘以纯单位四元数<i>p</i>, 即可得到完全四元数矩阵:</p>
                </div>
                <div class="p1">
                    <p id="72"><i>f</i>′<sub>q</sub>(<i>x</i>,<i>y</i>)=<i>f</i><sub>a</sub>(<i>x</i>,<i>y</i>)+<i>if</i><sub>b</sub>(<i>x</i>,<i>y</i>)+<i>jf</i><sub>c</sub>(<i>x</i>,<i>y</i>)+<i>kf</i><sub>d</sub>(<i>x</i>,<i>y</i>)      (4)</p>
                </div>
                <div class="p1">
                    <p id="73">四元数矩阵<i>f</i>′q(<i>x</i>,<i>y</i>)的L1相位公式为:</p>
                </div>
                <div class="p1">
                    <p id="74" class="code-formula">
                        <mathml id="74"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>Ι</mi><msub><mrow></mrow><mtext>Ρ</mtext></msub><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>=</mo><mrow><mi>tan</mi></mrow><msup><mrow></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mfrac><mrow><mi>α</mi><msub><mrow></mrow><mn>1</mn></msub><mrow><mo>|</mo><mrow><mi>f</mi><msub><mrow></mrow><mtext>b</mtext></msub><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo></mrow><mo>|</mo></mrow><mo>+</mo><mi>α</mi><msub><mrow></mrow><mn>2</mn></msub><mrow><mo>|</mo><mrow><mi>f</mi><msub><mrow></mrow><mtext>c</mtext></msub><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo></mrow><mo>|</mo></mrow><mo>+</mo><mi>α</mi><msub><mrow></mrow><mn>3</mn></msub><mrow><mo>|</mo><mrow><mi>f</mi><msub><mrow></mrow><mtext>d</mtext></msub><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo></mrow><mo>|</mo></mrow></mrow><mrow><mi>f</mi><msub><mrow></mrow><mtext>a</mtext></msub><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo></mrow></mfrac></mtd></mtr><mtr><mtd><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>5</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="75">式中:{<i>α</i><sub>1</sub><i>α</i><sub>2</sub><i>α</i><sub>3</sub>}表示权重向量。LBP算子在一个3×3的图像块中计算,图像块<i>S</i><sub><i>m</i></sub>的中心点的值计算为:</p>
                </div>
                <div class="area_img" id="77">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JYRJ201910022_07700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="78">式中:<mathml id="121"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>|</mo><mrow><mi>S</mi><msub><mrow></mrow><mi>m</mi></msub></mrow><mo>|</mo></mrow></mrow></math></mathml>表示<i>S</i><sub><i>m</i></sub>中的像素总个数,<i>x</i><sub>c</sub>和<i>x</i><sub><i>n</i></sub>分别是中心和邻近像素,考虑到RGB格式的图像有三个色彩通道,所以选择了三个单位四元数分别计算LBP特征。四元数矩阵<i>f</i>′q(<i>x</i>,<i>y</i>)的幅值定义为:</p>
                </div>
                <div class="p1">
                    <p id="79" class="code-formula">
                        <mathml id="79"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>Ι</mi><msub><mrow></mrow><mtext>A</mtext></msub><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>=</mo><msqrt><mrow><mi>f</mi><msup><mrow></mrow><mspace width="0.25em" /></msup><mn>2</mn><msub><mrow></mrow><mtext>a</mtext></msub><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>+</mo><mi>f</mi><msup><mrow></mrow><mspace width="0.25em" /></msup><mn>2</mn><msub><mrow></mrow><mtext>b</mtext></msub><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>+</mo><mi>f</mi><msup><mrow></mrow><mspace width="0.25em" /></msup><mn>2</mn><msub><mrow></mrow><mtext>c</mtext></msub><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>+</mo><mi>f</mi><msup><mrow></mrow><mspace width="0.25em" /></msup><mn>2</mn><msub><mrow></mrow><mtext>d</mtext></msub><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo></mrow></msqrt></mtd></mtr><mtr><mtd><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>7</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="80">根据运算法则,经过clifford变换不改变幅值,因此,可以用任意一个clifford变换的结果来计算幅值。异或非对称区域局部梯度编码考虑到中心像素值与邻近像素之间的强度关系对局部区域的纹理特征进行编码。</p>
                </div>
                <div class="p1">
                    <p id="81">选择图像块大小为(2<i>m</i>+1)×(2<i>n</i>+1),把图像块分成如图3所示的9个子块,<i>R</i><sub><i>t</i></sub>表示子块的中心像素值,编码方式为:</p>
                </div>
                <div class="p1">
                    <p id="82" class="code-formula">
                        <mathml id="82"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtable columnalign="left"><mtr><mtd><mi>Ρ</mi><msub><mrow></mrow><mn>0</mn></msub><mo>:</mo><mi>h</mi><mo stretchy="false">(</mo><mi>R</mi><msub><mrow></mrow><mn>1</mn></msub><mo>-</mo><mi>R</mi><msub><mrow></mrow><mn>3</mn></msub><mo stretchy="false">)</mo><mo>,</mo><mi>Ρ</mi><msub><mrow></mrow><mn>1</mn></msub><mo>:</mo><mi>h</mi><mo stretchy="false">(</mo><mi>R</mi><msub><mrow></mrow><mn>8</mn></msub><mo>-</mo><mi>R</mi><msub><mrow></mrow><mn>4</mn></msub><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mi>Ρ</mi><msub><mrow></mrow><mn>2</mn></msub><mo>:</mo><mi>h</mi><mo stretchy="false">(</mo><mi>R</mi><msub><mrow></mrow><mn>7</mn></msub><mo>-</mo><mi>R</mi><msub><mrow></mrow><mn>5</mn></msub><mo stretchy="false">)</mo><mo>,</mo><mi>Ρ</mi><msub><mrow></mrow><mn>3</mn></msub><mo>:</mo><mi>h</mi><mo stretchy="false">(</mo><mi>R</mi><msub><mrow></mrow><mn>1</mn></msub><mo>-</mo><mi>R</mi><msub><mrow></mrow><mn>7</mn></msub><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mi>Ρ</mi><msub><mrow></mrow><mn>4</mn></msub><mo>:</mo><mi>h</mi><mo stretchy="false">(</mo><mi>R</mi><msub><mrow></mrow><mn>2</mn></msub><mo>-</mo><mi>R</mi><msub><mrow></mrow><mn>6</mn></msub><mo stretchy="false">)</mo><mo>,</mo><mi>Ρ</mi><msub><mrow></mrow><mn>5</mn></msub><mo>:</mo><mi>h</mi><mo stretchy="false">(</mo><mi>R</mi><msub><mrow></mrow><mn>3</mn></msub><mo>-</mo><mi>R</mi><msub><mrow></mrow><mn>5</mn></msub><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mi>Ρ</mi><msub><mrow></mrow><mn>6</mn></msub><mo>:</mo><mi>h</mi><mo stretchy="false">(</mo><mi>R</mi><msub><mrow></mrow><mn>1</mn></msub><mo>-</mo><mi>R</mi><msub><mrow></mrow><mn>5</mn></msub><mo stretchy="false">)</mo><mo>,</mo><mi>Ρ</mi><msub><mrow></mrow><mn>7</mn></msub><mo>:</mo><mi>h</mi><mo stretchy="false">(</mo><mi>R</mi><msub><mrow></mrow><mn>3</mn></msub><mo>-</mo><mi>R</mi><msub><mrow></mrow><mn>7</mn></msub><mo stretchy="false">)</mo></mtd></mtr></mtable><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>8</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="area_img" id="83">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201910022_083.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 AR-LGC的子块" src="Detail/GetImg?filename=images/JYRJ201910022_083.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 AR-LGC的子块  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201910022_083.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="84">当作用于中心像素点的邻域窗口大小不一样时,可以得到两个不同的二值序列<i>PA</i>和<i>PB</i>,则异或非对称区域局部梯度编码为:</p>
                </div>
                <div class="p1">
                    <p id="85"><mathml id="122"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>f</mi><msub><mrow></mrow><mrow><mtext>X</mtext><mtext>Ο</mtext><mtext>R</mtext><mo>-</mo><mtext> </mtext><mtext>A</mtext><mtext>R</mtext><mtext>L</mtext><mtext>G</mtext><mtext>C</mtext></mrow></msub><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>n</mi><mo>=</mo><mn>0</mn></mrow><mn>7</mn></munderover><mo stretchy="false">(</mo></mstyle><mi>Ρ</mi><mi>A</mi><mo>⊕</mo><mi>Ρ</mi><mi>B</mi><mo stretchy="false">)</mo><mn>2</mn><msup><mrow></mrow><mi>n</mi></msup></mrow></math></mathml>      (9)</p>
                </div>
                <div class="p1">
                    <p id="86">图4显示了从眼睛区域提取特征的流程,对嘴巴区域提取特征的操作类似,这种编码方法基于中心像素和周围邻域之间的灰度关系,结合了来自水平、垂直和对角方向的不同尺度的信息。</p>
                </div>
                <div class="area_img" id="87">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201910022_087.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 眼睛区域局部特征提取流程图" src="Detail/GetImg?filename=images/JYRJ201910022_087.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 眼睛区域局部特征提取流程图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201910022_087.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="88" name="88"><b>1.3 网络级联</b></h4>
                <div class="p1">
                    <p id="89">将全局特征、眼部局部特征和嘴部局部特征分别输入globalDNN、localDNN1和localDNN2,在融合层将输出拼接在一起。混合特征向量<i>y</i><sub>mix</sub>表示为:</p>
                </div>
                <div class="p1">
                    <p id="90"><i>y</i><sub>mix</sub>(<i>x</i>,<i>y</i>)=[<i>y</i><mathml id="123"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>1</mn><mrow><mi>Μ</mi><msub><mrow></mrow><mn>1</mn></msub></mrow></msubsup></mrow></math></mathml>,<i>y</i><mathml id="124"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>1</mn><mrow><mi>Μ</mi><msub><mrow></mrow><mn>2</mn></msub></mrow></msubsup></mrow></math></mathml>,<i>y</i><mathml id="125"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>1</mn><mrow><mi>Μ</mi><msub><mrow></mrow><mn>3</mn></msub></mrow></msubsup></mrow></math></mathml>]      (10)</p>
                </div>
                <div class="p1">
                    <p id="91">式中:<i>y</i><mathml id="126"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>1</mn><mrow><mi>Μ</mi><msub><mrow></mrow><mn>1</mn></msub></mrow></msubsup></mrow></math></mathml>、<i>y</i><mathml id="127"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>1</mn><mrow><mi>Μ</mi><msub><mrow></mrow><mn>2</mn></msub></mrow></msubsup></mrow></math></mathml>、<i>y</i><mathml id="128"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>1</mn><mrow><mi>Μ</mi><msub><mrow></mrow><mn>3</mn></msub></mrow></msubsup></mrow></math></mathml>分别代表globalDNN、localDNN1、localDNN2的输出。融合层后接入一个新的DNN网络,作用是把融合特征映射到一个新的特征空间以学习一个更紧致的表示,从而预测输入抑郁症人脸的BDI-II分值。最终的损失函数表示为:</p>
                </div>
                <div class="p1">
                    <p id="92"><mathml id="129"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><msub><mrow></mrow><mrow><mtext>s</mtext><mtext>u</mtext><mtext>m</mtext></mrow></msub><mo>=</mo><mfrac><mn>1</mn><mi>Ν</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mo stretchy="false">(</mo></mstyle><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mover accent="true"><mi>y</mi><mo>^</mo></mover><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><msup><mrow></mrow><mn>2</mn></msup></mrow></math></mathml>      (11)</p>
                </div>
                <div class="p1">
                    <p id="93">式中:<i>N</i>表示Batch size大小,<i>y</i><sub><i>i</i></sub>表示最后的预测值,<mathml id="130"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>y</mi><mo>^</mo></mover></math></mathml><sub><i>i</i></sub>是第<i>i</i>幅输入抑郁样本的标签。正则化可以防止过拟合现象的产生,降低泛化误差。由于本文在模型训练和测试过程中,对抑郁症的预测结果都比较好,没有出现过拟合的情况。因此,本文在损失函数中,放弃使用正则化对网络进一步调节。</p>
                </div>
                <h3 id="94" name="94" class="anchor-tag"><b>2 实 验</b></h3>
                <div class="p1">
                    <p id="95">为了验证所提出方法的有效性,本文在AVEC 2013和AVEC2014抑郁症竞赛数据集上进行实验。</p>
                </div>
                <h4 class="anchor-tag" id="96" name="96"><b>2.1 抑郁症数据集</b></h4>
                <div class="p1">
                    <p id="97">AVEC2013和AVEC2014抑郁症数据集都是语音-视觉抑郁语言库(AVid-Corpus)的子集,用于探究抑郁症的严重程度。AVEC2013包含在自然条件下采集的82个人的150个视频。这些视频记录了被试者完成给定任务的人机交互过程。视频的长度在20～50分钟不等,平均是25分钟。受试者的年龄从18～63岁不等。这些视频随机分为测试集、和训练集,比例为1∶1<citation id="157" type="reference"><link href="29" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>,且每个集合中抑郁分值样本分布均衡。训练集用来训练深度模型,测试集用来测试模型识别抑郁症的效果。抑郁症的严重程度可根据贝克量表进行评估。AVEC2013数据库中的一些被试者如图5(a)所示,他们在录制时可以自由移动,包括说话、移动头部和变换表情等。AVEC2014包含来自AViD-Corpus12个任务中的其中两个,分别被称之为Freedom和Northwind。</p>
                </div>
                <div class="area_img" id="158">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201910022_15800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 抑郁症数据集样本" src="Detail/GetImg?filename=images/JYRJ201910022_15800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 抑郁症数据集样本  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201910022_15800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="158">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201910022_15801.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 抑郁症数据集样本" src="Detail/GetImg?filename=images/JYRJ201910022_15801.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 抑郁症数据集样本  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201910022_15801.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="100">本文采用了第一个任务中的视频。同样地,训练集用来训练深度模型,测试集用来测试模型识别抑郁症的效果。AVEC2014数据库中的一些被试者样本如图5(b)所示。</p>
                </div>
                <h4 class="anchor-tag" id="101" name="101"><b>2.2 实验设置</b></h4>
                <div class="p1">
                    <p id="102">为了减少视频中视频帧冗余量,本文依据经验值每十帧取一帧,在每个数据集上个提取了15 000帧图像。并用Dlib进行关键点检测和人脸五官定位,将检测后的图像裁剪为256×256大小,用于实验。对于全局DNN的输入,本文选取的是Facenet的映射层输出特征向量<i><b>x</b></i><sub><i>G</i></sub>,其维度是128维。globalDNN有四个全连接层,每层设置为128、56、16、1个输出。对于局部特征的提取,参考四元数为<i>q</i><sub>1</sub>= 0.992<i>i</i>+0.085 7<i>j</i>+0.090 7<i>k</i>、<i>q</i><sub>2</sub> =0.091 2<i>i</i>+0.990 8<i>j</i>+0.099 9<i>k</i>、<i>q</i><sub>3</sub>=0.085 2<i>i</i>+0.085 5<i>j</i>+0.992 7<i>k</i>。权重分别为{0.4,0.5,0.6}。在异或非对称区域局部梯度编码的提取过程中,其中一个邻域大小被固定为3×3,另外一个邻域从{5×5, 7×7, 9×9, 11×11}中取值。从眼睛区域提取特征为<i>x</i><sub><i>L</i></sub><sub>1</sub>,维度为2 048维,从嘴巴区域提取出的特征为<i>x</i><sub><i>L</i></sub><sub>2</sub>,1 024维。将localDNN1设置为5层,并且每一层的输出分别为2 048、1 024、512、128、1。localDNN2设置为四层,每一层的神经元个数为1 024、512、128、1。在训练过程中,使用的优化器是Adam,学习率决定了找到网络最优参数的速度快慢,如果学习率太大,很可能会跳过网络的最优参数,反之如果学习率过小,可能导致算法长时间无法收敛。实验比较了不同的学习率对损失函数和对抑郁症预测的影响,结果表明,当初始学习率学习率被设置为10e<sup>-2</sup>,时,RMSE和MAE的值最小,检测性能最好。迭代次数为30 000,Batch size从{30,40,50}中取值。对于每一个测试视频,对视频中提取的每个帧的预测值取平均来计算预测的抑郁症评分。选择两个客观标准,均值绝对误差(MAE)和根均方误差(RMSE)来测量总体性能,计算如下:</p>
                </div>
                <div class="p1">
                    <p id="103"><mathml id="131"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Μ</mi><mi>A</mi><mi>E</mi><mo>=</mo><mfrac><mn>1</mn><mi>Ν</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mrow><mrow><mo>|</mo><mrow><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mover accent="true"><mi>y</mi><mo>^</mo></mover><msub><mrow></mrow><mi>i</mi></msub></mrow><mo>|</mo></mrow></mrow></mstyle></mrow></math></mathml>      (12)</p>
                </div>
                <div class="p1">
                    <p id="104"><mathml id="132"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>R</mi><mi>Μ</mi><mi>S</mi><mi>E</mi><mo>=</mo><msqrt><mrow><mfrac><mn>1</mn><mi>Ν</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mo stretchy="false">(</mo></mstyle><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mover accent="true"><mi>y</mi><mo>^</mo></mover><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><msup><mrow></mrow><mn>2</mn></msup></mrow></msqrt></mrow></math></mathml>      (13)</p>
                </div>
                <div class="p1">
                    <p id="105">式中:<i>N</i>是测试集中的样本总数,<i>y</i><sub><i>i</i></sub>代表的是第<i>i</i>个样本的预测值,<mathml id="133"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>y</mi><mo>^</mo></mover></math></mathml><sub><i>i</i></sub> 是第<i>i</i>个样本的真值。</p>
                </div>
                <h4 class="anchor-tag" id="106" name="106"><b>2.3 结果与分析</b></h4>
                <div class="p1">
                    <p id="107">首先进行实验以选择最佳的邻域和训练批次大小,图6和图7是MAE和RMSE随着邻域和训练批次大小的变化而变化的曲线图。可观察到两个指标在垂直方向上变化显著,这表明邻域大小对抑郁评分的影响更大。对于两个数据库,异或非对称区域局部梯度编码的最佳邻域大小设置为5×5。</p>
                </div>
                <div class="area_img" id="108">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201910022_108.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 不同邻域大小和Batch size在AVEC2013的
抑郁症识别结果" src="Detail/GetImg?filename=images/JYRJ201910022_108.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 不同邻域大小和Batch size在AVEC2013的
抑郁症识别结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201910022_108.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="109">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JYRJ201910022_109.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 不同邻域大小和Batch size在AVEC2014的
抑郁症识别结" src="Detail/GetImg?filename=images/JYRJ201910022_109.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 不同邻域大小和Batch size在AVEC2014的
抑郁症识别结  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JYRJ201910022_109.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="110">接下来的实验评估使用全局和局部特征进行抑郁症识别的性能,表1列出了在两个数据库上的MAE和RMSE值。</p>
                </div>
                <div class="area_img" id="111">
                    <p class="img_tit"><b>表1 使用全局和局部特征的抑郁症识别结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="111" border="1"><tr><td rowspan="2"><br />方法</td><td colspan="2"><br />AVEC 2013</td><td colspan="2">AVEC 2014</td></tr><tr><td><br />MAE</td><td>RMSE</td><td>MAE</td><td>RMSE</td></tr><tr><td><br />全局特征</td><td>8.86</td><td>10.68</td><td>8.66</td><td>10.62</td></tr><tr><td><br />局部特征</td><td>7.22</td><td>8.86</td><td>7.18</td><td>8.80</td></tr><tr><td><br />本文</td><td>7.09</td><td>8.69</td><td>7.07</td><td>8.69</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="112">由表1可以看出,使用全局特征或局部特征的MAE和RMSE值大于使用多特征的MAE和RMSE值。这主要是因为局部区域尤其是眼部传达了更为突出的信息,这也与临床医生的经验是一致的。</p>
                </div>
                <div class="p1">
                    <p id="113">为了证明所提出方法中使用的局部特征的优势,表2给出了使用不同局部特征的抑郁症识别结果。</p>
                </div>
                <div class="area_img" id="114">
                    <p class="img_tit"><b>表2 使用不同的局部特征的抑郁症识别结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="114" border="1"><tr><td rowspan="2"><br />局部特征</td><td colspan="2"><br />AVEC 2013</td><td colspan="2">AVEC 2014</td></tr><tr><td><br />MAE</td><td>RMSE</td><td>MAE</td><td>RMSE</td></tr><tr><td>LBP</td><td>8.01</td><td>9.94</td><td>8.15</td><td>10.21</td></tr><tr><td><br />QLBP</td><td>7.92</td><td>9.77</td><td>7.74</td><td>9.55</td></tr><tr><td><br />XOR-AR-LGC</td><td>7.55</td><td>9.36</td><td>7.57</td><td>9.47</td></tr><tr><td><br />QLBP+XOR-AR-LGC</td><td>7.22</td><td>8.86</td><td>7.18</td><td>8.80</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="115">由表2可以看出,四元数局部二值编码优于传统的局部二值编码,这是由于充分利用了颜色信息。将四元数局部二值编码与异或非对称区域局部梯度编码结合作为混合特征,即同时考虑相位和幅度信息,可以获得更好的性能。除此之外,本文结合了全局特征和局部特征以及深度神经网络实现了对抑郁症自动识别,进而实现了全局信息和局部信息的融合互补。与其他现有方法的性能做对比,具体结果如表3和表4所示。可以看出,本文所提出的级联DNN方法在抑郁症识别任务中具有最小的MAE和RMSE值。</p>
                </div>
                <div class="area_img" id="116">
                    <p class="img_tit"><b>表3 与在AVEC2013的其他识别方法的比较</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="116" border="1"><tr><td><br />方法</td><td>MAE</td><td>RMSE</td></tr><tr><td><br />文献[8]<br />(LPQ+SVR)</td><td>10.88</td><td>13.61</td></tr><tr><td><br />文献[11]<br />(MMH+PLS)</td><td>9.14</td><td>11.19</td></tr><tr><td><br />文献[12]<br />(TOP-LPQ+Sparse Coding)</td><td>8.22</td><td>10.27</td></tr><tr><td><br />文献[14]<br />(Facial DCNN +Dynamics DCNN)</td><td>7.58</td><td>9.82</td></tr><tr><td><br />本文<br />(GlobalDNN+LocalDNN)</td><td>7.09</td><td>8.69</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="117">
                    <p class="img_tit"><b>表4 与在AVEC2014的其他识别方法的比较</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="117" border="1"><tr><td><br />方法</td><td>MAE</td><td>RMSE</td></tr><tr><td><br />文献[9]<br />(LGBP-TOP+SVR)</td><td>8.86</td><td>10.86</td></tr><tr><td><br />文献[23]<br />(DSM-IV-TR)</td><td>9.35</td><td>11.91</td></tr><tr><td><br />文献[22]<br />(MHH+PLS)</td><td>8.44</td><td>10.50</td></tr><tr><td><br />文献[13]<br />(CAA+MPGI)</td><td>7.96</td><td>9.97</td></tr><tr><td><br />文献[14]<br />(Facial DCNN +Dynamics DCNN)</td><td>7.47</td><td>9.55</td></tr><tr><td><br />文献[15]<br />(Deep Feature +DTL)</td><td>7.74</td><td>9.43</td></tr><tr><td><br />本文<br />(GlobalDNN+LocalDNN)</td><td>7.07</td><td>8.69</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h3 id="118" name="118" class="anchor-tag"><b>3 结 语</b></h3>
                <div class="p1">
                    <p id="119">为了促进在临床诊断中抑郁症的自动识别和监测,本文研究了一种基于级联深层神经网络和多特征的抑郁症识别方法。多特征包括全局特征和局部特征,全局特征使用FaceNet提取之后送入DNN网络,QLBP-XOR-AR-LGC特征从眼睛和嘴巴区域提取之后分别送入localDNN1和localDNN2。最后将三个DNN输出的特征向量在融合层拼接,送入最后一个DNN网络以预测抑郁评分。全局特征和局部特征通过级联深层神经网络有效实现了互补从而获得了良好的性能。本文在AEVC2013和AVEC2014两个常用数据库上进行的实验验证了该方法的可行性和有效性。今后,我们将结合多模态融合的要求,进一步提高抑郁症识别性能。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" target="_blank" href="">

                                <b>[1]</b> World Health Organization[OL].https://www.who.int/mental_health/management /depression/en.
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The global burden of disease:2004 update">

                                <b>[2]</b> Mathers C,Boerma J,Fat D.The global burden of disease:2004 update[M].Geneva,Switzerland:WHO,2008.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Structured clinical interview for DSM-IV-TR axis I disorders:patient edition">

                                <b>[3]</b> First M B.Structured clinical interview for DSM-IV-TR axis I disorders:patient edition[M].Biometrics Research Department,Columbia University,2005.
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=STJD&amp;filename=STJD785038960&amp;v=MTI0NTVxcUJ0R0ZyQ1VSN3FmWnVadEZpRGhVYnpNTmpuQmFyU3dHOUhQcDRaRFpJUUtESDg0dlI0VDZqNTRPM3o=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b> Beck A T,Steer R A,Ball R,et al.Comparison of Beck Depression Inventories-IA and-II in psychiatric outpatients[J].Journal of personality assessment,1996,67(3):588-597.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Toward analyzing American movement">

                                <b>[5]</b> Birdwhistell R L.Toward analyzing American movement[M]//Weitz S.Nonverbal Communication:Readings With Commentary.New York,NY,USA:Oxford Univ.Press,1974:134-143.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" >
                                    <b>[6]</b>
                                 Esteva A,Kuprel B,Novoa R A,et al.Dermatologist-level classification of skin cancer with deep neural networks[J].Nature,2017,542(7639):115.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Detecting Depression Severity from Vocal Prosody">

                                <b>[7]</b> Yang Y,Fairbairn C,Cohn J F.Detecting depression severity from vocal prosody[J].IEEE Transactions on Affective Computing,2013,4(2):142-150.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=AVEC2013:the continuous audio/visual emotion and depression recognition challenge">

                                <b>[8]</b> Valstar M,Schuller B,Smith K,et al.AVEC 2013:the continuous audio/visual emotion and depression recognition challenge[C]//3rd ACM international workshop on Audio/visual emotion challenge,2013:3-10.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Avec 2014:3d dimensional affect and depression recognition challenge">

                                <b>[9]</b> Valstar M,Schuller B,Smith K,et al.Avec 2014:3d dimensional affect and depression recognition challenge[C]//4th International Workshop on Audio/Visual Emotion Challenge,2014:3-10.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Diagnosis of depression by behavioural signals:a multimodal approach">

                                <b>[10]</b> Cummins N,Joshi J,Dhall A,et al.Diagnosis of depression by behavioural signals:a multimodal approach[C]//3rd ACM international workshop on Audio/visual emotion challenge,2013:11-20.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Depression recognition based on dynamic facial and vocal expression features using partial least square regression">

                                <b>[11]</b> Meng H,Huang D,Wang H,et al.Depression recognition based on dynamic facial and vocal expression features using partial least square regression[C]//3rd ACM international workshop on Audio/visual emotion challenge,2013:21-30.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Automated depression diagnosis based on facial dynamic analysis and sparse coding">

                                <b>[12]</b> Wen L,Li X,Guo G,et al.Automated depression diagnosis based on facial dynamic analysis and sparse coding[J].IEEE Transactions on Information Forensics and Security,2015,10(7):1432-1441.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Ensemble CCA for continuous emotion prediction">

                                <b>[13]</b> Kaya H,Çilli F,Salah A A.Ensemble CCA for continuous emotion prediction[C]//4th International Workshop on Audio/Visual Emotion Challenge,2014:19-26.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Automated depression diagnosis based on deep networks to encode facial appearance and dynamics">

                                <b>[14]</b> Zhu Y,Shang Y,Shao Z,et al.Automated depression diagnosis based on deep networks to encode facial appearance and dynamics[J].IEEE Transactions on Affective Computing,2018,9(4):578-584.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep transformation learning for depression diagnosis from facial images">

                                <b>[15]</b> Kang Y,Jiang X,Yin Y,et al.Deep Transformation Learning for Depression Diagnosis from Facial Images[C]//Chinese Conference on Biometric Recognition,2017:13-22.
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Quantitative comparison of motion history image variants for video-based depression assessment">

                                <b>[16]</b> Pampouchidou A,Pediaditis M,Maridaki A,et al.Quantitative comparison of motion history image variants for video-based depression assessment[J].EURASIP Journal on Image and Video Processing,2017,2017(1):64.
                            </a>
                        </p>
                        <p id="35">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Facenet:A unified embedding for face recognition and clustering">

                                <b>[17]</b> Schroff F,Kalenichenko D,Philbin J.Facenet:A unified embedding for face recognition and clustering[C]//IEEE conference on computer vision and pattern recognition,2015:815-823.
                            </a>
                        </p>
                        <p id="37">
                            <a id="bibliography_18" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES15122300161997&amp;v=MjU3MzQ0UFFIL2lyUmRHZXJxUVRNbndaZVp0RmlubFVyekpLRm9XYVJZPU5pZk9mYks5SDlQT3JJOUZaZTBPQlhVK29CTVQ2VA==&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[18]</b> Shao Z,Shu H,Wu J,et al.Quaternion Bessel–Fourier moments and their invariant descriptors for object reconstruction and recognition[J].Pattern Recognition,2014,47(2):603-611.
                            </a>
                        </p>
                        <p id="39">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Quaternionic local ranking binary pattern:A local descriptor of color images">

                                <b>[19]</b> Lan R,Zhou Y,Tang Y Y.Quaternionic local ranking binary pattern:a local descriptor of color images[J].IEEE Transactions on Image Processing,2016,25(2):566-579.
                            </a>
                        </p>
                        <p id="41">
                            <a id="bibliography_20" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES06F52878856D908523BA2F3BD6018735&amp;v=MTgxMTR0Z3pMeTl3Nnc9TmlmT2ZiTythTlRPcDRoTmJPNEplSFU1eHhNUjZVME1TZ25oM21ZemViT2NRcm1hQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQnJMVTA1dA==&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[20]</b> Chen B,Yang J,Jeon B,et al.Kernel quaternion principal component analysis and its application in RGB-D object recognition[J].Neurocomputing,2017,266:293-303.
                            </a>
                        </p>
                        <p id="43">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Quaternion switching vector median filter based on local reachability density">

                                <b>[21]</b> Zhu Z,Jin L,Song E,et al.Quaternion switching vector median filter based on local reachability density[J].IEEE Signal Processing Letters,2018,25(6):843-847.
                            </a>
                        </p>
                        <p id="45">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Automatic depression scale prediction using facial expression dynamics and regression">

                                <b>[22]</b> Jan A,Meng H,Gaus Y F A,et al.Automatic depression scale prediction using facial expression dynamics and regression[C]//4th International Workshop on Audio/Visual Emotion Challenge,2014:73-80.
                            </a>
                        </p>
                        <p id="47">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fusing affective dimensions and audio-visual features from segmented video for depression recognition:INAOE-BUAP&amp;#39;&amp;#39;s participation at AVEC&amp;#39;&amp;#39;14 challenge">

                                <b>[23]</b> Espinosa H P,Escalante H J,Villaseňor-Pineda L,et al.Fusing affective dimensions and audio-visual features from segmented video for depression recognition:INAOE-BUAP's participation at AVEC'14 challenge[C]//Proceedings of the 4th International Workshop on Audio/Visual Emotion Challenge.ACM,2014:49-55.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JYRJ201910022" />
        <input id="dpi" type="hidden" value="600" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JYRJ201910022&amp;v=MjgyODY0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVadEZpRGhVYnpNTHpUWlpMRzRIOWpOcjQ5SFpvUUtESDg0dlI=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVR2ZlpVSUdIOHYva21ub0pKOEU0cz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="0" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
