<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637127958761056250%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dMUTE201902009%26RESULT%3d1%26SIGN%3d4c83f9Lz%252b1msWw10JAg%252fvo6JPyg%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=MUTE201902009&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=MUTE201902009&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MUTE201902009&amp;v=MzEzMDFyQ1VSTE9lWmVSckZ5M2hVNy9NS0RqZmE3RzRIOWpNclk5RmJZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0Y=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVjblNCVTU2UWRrTGIyZlBkcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#67" data-title="1 从Shannon信道看传统的贝叶斯预测和最大似然估计 ">1 从Shannon信道看传统的贝叶斯预测和最大似然估计</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#68" data-title="&lt;b&gt;1.1&lt;/b&gt;&lt;i&gt;Shannon&lt;/i&gt;信道和转移概率函数"><b>1.1</b><i>Shannon</i>信道和转移概率函数</a></li>
                                                <li><a href="#81" data-title="&lt;b&gt;1.2&lt;/b&gt; 信源可变时传统的贝叶斯预测和最大似然估计"><b>1.2</b> 信源可变时传统的贝叶斯预测和最大似然估计</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#95" data-title="2 基于隶属函数的语义信息理论 ">2 基于隶属函数的语义信息理论</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#96" data-title="&lt;b&gt;2.1&lt;/b&gt;&lt;i&gt;Shannon&lt;/i&gt;信道和语义信道"><b>2.1</b><i>Shannon</i>信道和语义信道</a></li>
                                                <li><a href="#107" data-title="&lt;b&gt;2.2&lt;/b&gt; 理解&lt;i&gt;GPS&lt;/i&gt;定位——似然函数还是隶属函数?"><b>2.2</b> 理解<i>GPS</i>定位——似然函数还是隶属函数?</a></li>
                                                <li><a href="#116" data-title="&lt;b&gt;2.3&lt;/b&gt; 用标准 (normalized) 似然度定义语义信息测度"><b>2.3</b> 用标准 (normalized) 似然度定义语义信息测度</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#132" data-title="3 最大语义信息估计和语义信道优化 ">3 最大语义信息估计和语义信道优化</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#133" data-title="&lt;b&gt;3.1&lt;/b&gt; 似然度和语义信息之间的关系"><b>3.1</b> 似然度和语义信息之间的关系</a></li>
                                                <li><a href="#139" data-title="&lt;b&gt;3.2&lt;/b&gt; 预测模型或隶属函数的优化"><b>3.2</b> 预测模型或隶属函数的优化</a></li>
                                                <li><a href="#151" data-title="&lt;b&gt;3.3&lt;/b&gt; 医学检验和&lt;i&gt;GPS&lt;/i&gt;的语义信道优化"><b>3.3</b> 医学检验和<i>GPS</i>的语义信道优化</a></li>
                                                <li><a href="#171" data-title="&lt;b&gt;3.4&lt;/b&gt; 从因素空间看模糊分类"><b>3.4</b> 从因素空间看模糊分类</a></li>
                                                <li><a href="#175" data-title="&lt;b&gt;3.5&lt;/b&gt; 用最大语义信息 (或最大标准似然度) 准则选择假设"><b>3.5</b> 用最大语义信息 (或最大标准似然度) 准则选择假设</a></li>
                                                <li><a href="#178" data-title="&lt;b&gt;3.6&lt;/b&gt; 多标签逻辑分类和单标签选择分类"><b>3.6</b> 多标签逻辑分类和单标签选择分类</a></li>
                                                <li><a href="#182" data-title="&lt;b&gt;3.7&lt;/b&gt; 语义贝叶斯决策和翻译"><b>3.7</b> 语义贝叶斯决策和翻译</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#188" data-title="4 两种信道相互匹配的迭代算法 (信道匹配算法或&lt;i&gt;CM&lt;/i&gt;算法)  ">4 两种信道相互匹配的迭代算法 (信道匹配算法或<i>CM</i>算法) </a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#189" data-title="&lt;b&gt;4.1&lt;/b&gt; 用&lt;i&gt;CM&lt;/i&gt;算法求检验, 估计和预测的最大互信息和最大似然度"><b>4.1</b> 用<i>CM</i>算法求检验, 估计和预测的最大互信息和最大似然度</a></li>
                                                <li><a href="#216" data-title="&lt;b&gt;4.2&lt;/b&gt; CM算法用于混合模型"><b>4.2</b> CM算法用于混合模型</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#232" data-title="5 总结 ">5 总结</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#111" data-title="图1 GPS定位图解">图1 GPS定位图解</a></li>
                                                <li><a href="#120" data-title="图2 语义信息量图解">图2 语义信息量图解</a></li>
                                                <li><a href="#154" data-title="图3 医学检验图解 (二元有噪声Shannon信道, 互信息随判决分界点&lt;i&gt;z&lt;/i&gt;’改变) ">图3 医学检验图解 (二元有噪声Shannon信道, 互信息随判决分界点<i>z</i>’改变) </a></li>
                                                <li><a href="#156" data-title="&lt;b&gt;表&lt;/b&gt;&lt;b&gt;1 医学检验的敏感性和特异性构成&lt;/b&gt;Shannon&lt;b&gt;信道&lt;/b&gt;&lt;i&gt;P&lt;/i&gt; (&lt;i&gt;Y&lt;/i&gt;|&lt;i&gt;X&lt;/i&gt;) "><b>表</b><b>1 医学检验的敏感性和特异性构成</b>Shannon<b>信道</b><i>P</i> (<i>Y</i>|<i>X</i>) </a></li>
                                                <li><a href="#160" data-title="表2 医学检验的语义信道——含有两个不信度b&lt;sub&gt;1&lt;/sub&gt;′和b&lt;sub&gt;0&lt;/sub&gt;′">表2 医学检验的语义信道——含有两个不信度b<sub>1</sub>′和b<sub>0</sub>′</a></li>
                                                <li><a href="#215" data-title="图4 分界起点很差时的迭代">图4 分界起点很差时的迭代</a></li>
                                                <li><a href="#228" data-title="表3 R&amp;lt;R*时的模型参数和迭代结果 (迭代次数是 5) ">表3 R&lt;R*时的模型参数和迭代结果 (迭代次数是 5) </a></li>
                                                <li><a href="#229" data-title="图5 &lt;i&gt;R&lt;/i&gt;&amp;lt;&lt;i&gt;R&lt;/i&gt;*时的迭代过程。&lt;i&gt;R&lt;/i&gt;和&lt;i&gt;G&lt;/i&gt;不断增加并逐渐接近&lt;i&gt;G&lt;/i&gt;*, 使得&lt;i&gt;H&lt;/i&gt; (&lt;i&gt;P&lt;/i&gt;‖&lt;i&gt;Q&lt;/i&gt;) 接近0。">图5 <i>R</i>&lt;<i>R</i>*时的迭代过程。<i>R</i>和<i>G</i>不断增加并逐渐接近<i>G</i>*, 使得<i>H</i> (<i>P</i>‖<i>Q</i>) 接近0。</a></li>
                                                <li><a href="#230" data-title="图6 样本概率分布&lt;i&gt;P&lt;/i&gt; (&lt;i&gt;X&lt;/i&gt;) 和模型预测的概率分布&lt;i&gt;Q&lt;/i&gt; (&lt;i&gt;X&lt;/i&gt;) (5次迭代后) ">图6 样本概率分布<i>P</i> (<i>X</i>) 和模型预测的概率分布<i>Q</i> (<i>X</i>) (5次迭代后) </a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>
                                    <dd class="subnode">
                                        <h6>
                                            <a href="#a_footnote">注释</a>

                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="4">


                                    <a id="bibliography_1" title=" Zadeh L A.Fuzzy sets[J].Information and Control, 1965, 8 (3) :338～353." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fuzzy Set">
                                        <b>[1]</b>
                                         Zadeh L A.Fuzzy sets[J].Information and Control, 1965, 8 (3) :338～353.
                                    </a>
                                </li>
                                <li id="6">


                                    <a id="bibliography_2" title=" Dubois D, Moral S, Prade H.A semantics for possibility theory based on likelihoods[J].Journal of Mathematical Analysis and Applications, 1997, 205 (20) :359～380." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011601171954&amp;v=MjAzMjdVYWhZPU5pZk9mYks3SHRETnFZOUVaZXdPQlhrOW9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVUxySUpWbw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVjblNCVTU2UWRrTGIyZlBkcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         Dubois D, Moral S, Prade H.A semantics for possibility theory based on likelihoods[J].Journal of Mathematical Analysis and Applications, 1997, 205 (20) :359～380.
                                    </a>
                                </li>
                                <li id="8">


                                    <a id="bibliography_3" title=" Cattaneo M E G V.The likelihood interpretation as the foundation of fuzzy set theory[J].International Journal of Approximate Reasoning, Available online 22 August 2017." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESD28EA408629E19534DC50500BBD237D8&amp;v=MjEyNDlLdz1OaWZPZmNlNkZxUzlxNDlOWXVrR2VYMHd5aFVYbmt4NFNIcmlyR0JIRGJDWFFzNlhDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZDcGJRMzVOMWh3Ynkvdw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVjblNCVTU2UWRrTGIyZlBkcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         Cattaneo M E G V.The likelihood interpretation as the foundation of fuzzy set theory[J].International Journal of Approximate Reasoning, Available online 22 August 2017.
                                    </a>
                                </li>
                                <li id="10">


                                    <a id="bibliography_4" title=" Yang C C.Fuzzy Bayesian inference[C]//Computational Cybernetics and Simulation, IEEE International Conference on Systems, Man, and Cybernetics Conference Proceedings, Orlando, 1997, Vol.3:2707～2712." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fuzzy Bayesian inference">
                                        <b>[4]</b>
                                         Yang C C.Fuzzy Bayesian inference[C]//Computational Cybernetics and Simulation, IEEE International Conference on Systems, Man, and Cybernetics Conference Proceedings, Orlando, 1997, Vol.3:2707～2712.
                                    </a>
                                </li>
                                <li id="12">


                                    <a id="bibliography_5" title=" Viertl R.Foundations of fuzzy bayesian inference[J].Journal of Uncertain Systems, 2008, 2 (3) :187～191." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Foundations of fuzzy bayesian inference">
                                        <b>[5]</b>
                                         Viertl R.Foundations of fuzzy bayesian inference[J].Journal of Uncertain Systems, 2008, 2 (3) :187～191.
                                    </a>
                                </li>
                                <li id="14">


                                    <a id="bibliography_6" title=" Thomas S F.Possibilistic uncertainty and statistical inference[C]//ORSA/TIMS Meeting, Houston, 1981." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Possibilistic uncertainty and statistical inference">
                                        <b>[6]</b>
                                         Thomas S F.Possibilistic uncertainty and statistical inference[C]//ORSA/TIMS Meeting, Houston, 1981.
                                    </a>
                                </li>
                                <li id="16">


                                    <a id="bibliography_7" title=" 鲁晨光.B-模糊集合代数和广义交互熵公式[J].模糊系统和数学, 1991, 5 (1) :76～80." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MUTE199101011&amp;v=MzAyNDFEamZhN0t4RjlETXJvOUVaWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUnJGeTNoVTcvTUs=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVjblNCVTU2UWRrTGIyZlBkcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                         鲁晨光.B-模糊集合代数和广义交互熵公式[J].模糊系统和数学, 1991, 5 (1) :76～80.
                                    </a>
                                </li>
                                <li id="18">


                                    <a id="bibliography_8" title=" Zadeh L A.Probability measures of fuzzy events[J].Journal of Mathematical Analysis and Applications, 1968, 23 (2) :421～427." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Probability measures of fuzzy events">
                                        <b>[8]</b>
                                         Zadeh L A.Probability measures of fuzzy events[J].Journal of Mathematical Analysis and Applications, 1968, 23 (2) :421～427.
                                    </a>
                                </li>
                                <li id="20">


                                    <a id="bibliography_9" title=" Davidson D.Truth and meaning[J].Synthese, 1967, 17:304～323." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002352349&amp;v=MDc3MjFNSDdSN3FlYnVkdEZDM2xWcnZJSUZvPU5qN0Jhck80SHRIT3JJcEhaKzhHWTNrNXpCZGg0ajk5U1hxUnJ4b3hj&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVjblNCVTU2UWRrTGIyZlBkcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                         Davidson D.Truth and meaning[J].Synthese, 1967, 17:304～323.
                                    </a>
                                </li>
                                <li id="22">


                                    <a id="bibliography_10" title=" 汪培庄.模糊集和随机集落影[M].北京师范大学出版社, 1985." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CBBD&amp;filename=0000000000348776&amp;v=MjQ2NDZPNEh0SE1yNHhCYk93SUNoTTh6eFVTbURkOVNIN24zeEU5ZmJ2bktyaWZadTl1RkN2Z1U3ck1JRjRRVlYyN0hy&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVjblNCVTU2UWRrTGIyZlBkcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[10]</b>
                                         汪培庄.模糊集和随机集落影[M].北京师范大学出版社, 1985.
                                    </a>
                                </li>
                                <li id="24">


                                    <a id="bibliography_11" title=" 鲁晨光.广义信息论[M].中国科学技术大学出版社, 1993." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CBBD&amp;filename=0007312005012999&amp;v=MjQ1NzZWVjI3R2JDNUhOSE1xbzlFWnVJR0JSTTh6eFVTbURkOVNIN24zeEU5ZmJ2bktyaWZadTl1RkN2Z1U3ck1JRjRR&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVjblNCVTU2UWRrTGIyZlBkcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[11]</b>
                                         鲁晨光.广义信息论[M].中国科学技术大学出版社, 1993.
                                    </a>
                                </li>
                                <li id="26">


                                    <a id="bibliography_12" title=" Shannon C E.A mathematical theory of communication[J].Bell System Technical Journal, 1948, 27 (3) :379～429;623～656." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A mathematical theory of communication">
                                        <b>[12]</b>
                                         Shannon C E.A mathematical theory of communication[J].Bell System Technical Journal, 1948, 27 (3) :379～429;623～656.
                                    </a>
                                </li>
                                <li id="28">


                                    <a id="bibliography_13" title=" Lu C.A generalization of Shannon’s information theory[J].Int.J.of General Systems, 1999, 28 (6) :453～490." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Generalization of Shannon&amp;#39;s information theory">
                                        <b>[13]</b>
                                         Lu C.A generalization of Shannon’s information theory[J].Int.J.of General Systems, 1999, 28 (6) :453～490.
                                    </a>
                                </li>
                                <li id="30">


                                    <a id="bibliography_14" title=" Fisher R A.On the mathematical foundations of theoretical statistics[J].Philo.Trans.Roy.Soc., 1922, A222:309～368." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJRS&amp;filename=SJRS121022008962&amp;v=Mjg4MThJSkRoTTh6eFVTbURkOVNIN24zeEU5ZmJ2bktyaWZadTl1RkN2Z1U3ck1JRjRRTmlmWmZiSzZIOUhPclk5RmJP&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVjblNCVTU2UWRrTGIyZlBkcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[14]</b>
                                         Fisher R A.On the mathematical foundations of theoretical statistics[J].Philo.Trans.Roy.Soc., 1922, A222:309～368.
                                    </a>
                                </li>
                                <li id="32">


                                    <a id="bibliography_15" title=" Cover T M, Thomas J A.Elements of information theory[M] 2nd Edition.New York:John Wiley &amp;amp; Sons, 2006." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Elements of information theory">
                                        <b>[15]</b>
                                         Cover T M, Thomas J A.Elements of information theory[M] 2nd Edition.New York:John Wiley &amp;amp; Sons, 2006.
                                    </a>
                                </li>
                                <li id="34">


                                    <a id="bibliography_16" title=" Popper K.Conjectures and refutations[M].Repr.London and New York:Routledge, 1963/2005." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Conjectures and refutations">
                                        <b>[16]</b>
                                         Popper K.Conjectures and refutations[M].Repr.London and New York:Routledge, 1963/2005.
                                    </a>
                                </li>
                                <li id="36">


                                    <a id="bibliography_17" title=" Carnap R, Bar-Hillel Y.An outline of a theory of semantic information[R].Tech.Rep.No.247, Research Lab.of Electronics, MIT, 1952." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=An outline of a theory of semantic information">
                                        <b>[17]</b>
                                         Carnap R, Bar-Hillel Y.An outline of a theory of semantic information[R].Tech.Rep.No.247, Research Lab.of Electronics, MIT, 1952.
                                    </a>
                                </li>
                                <li id="38">


                                    <a id="bibliography_18" >
                                        <b>[18]</b>
                                     Akaike H.A new look at the statistical model identification[J].IEEE Transactions on Automatic Control, 1974, 19:716～723.</a>
                                </li>
                                <li id="40">


                                    <a id="bibliography_19" title=" 汪培庄.因素空间和数据科学[J].辽宁工程技术大学学报, 2015, 34 (2) :273～280." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=FXKY201502028&amp;v=MTA2OTlLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplUnJGeTNoVTcvTUl6WEFkN0c0SDlUTXJZOUhiSVE=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVjblNCVTU2UWRrTGIyZlBkcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[19]</b>
                                         汪培庄.因素空间和数据科学[J].辽宁工程技术大学学报, 2015, 34 (2) :273～280.
                                    </a>
                                </li>
                                <li id="42">


                                    <a id="bibliography_20" title=" Thornbury J R, Fryback D G, Edwards W.Likelihood ratios as a measure of the diagnostic usefulness of excretory urogram information[J].Radiology, 1975, 114 (3) :561～565." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Likelihood ratios as a measure of the diagnostic usefulness of excretory urogram information">
                                        <b>[20]</b>
                                         Thornbury J R, Fryback D G, Edwards W.Likelihood ratios as a measure of the diagnostic usefulness of excretory urogram information[J].Radiology, 1975, 114 (3) :561～565.
                                    </a>
                                </li>
                                <li id="44">


                                    <a id="bibliography_21" title=" Kok M, Dahlin J, Schon B, Wills T B.A Newton-based maximum likelihood estimation in nonlinear state space models[OL].IFAC-PapersOnLine, 2015, 48:398～403." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Newton-based maximum likelihood estimation in nonlinear state space models">
                                        <b>[21]</b>
                                         Kok M, Dahlin J, Schon B, Wills T B.A Newton-based maximum likelihood estimation in nonlinear state space models[OL].IFAC-PapersOnLine, 2015, 48:398～403.
                                    </a>
                                </li>
                                <li id="46">


                                    <a id="bibliography_22" title=" Zhang M L, Zhou Z H.A review on multilabel learning algorithms[J].IEEE Trans.Knowledge and Data Engineering, 2014, 26 (8) :1819～1837." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A review on multi-label learning algorithms">
                                        <b>[22]</b>
                                         Zhang M L, Zhou Z H.A review on multilabel learning algorithms[J].IEEE Trans.Knowledge and Data Engineering, 2014, 26 (8) :1819～1837.
                                    </a>
                                </li>
                                <li id="48">


                                    <a id="bibliography_23" title=" Dempster A P, Laird N M, Rubin D B.Maximum likelihood from incomplete data via the EM algorithm[J].Journal of the Royal Statistical Society, Series B, 1977, 39 (1) :1～38." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJST&amp;filename=SJST14120603923799&amp;v=MTE5NzNySzhIOVBNcVk5R2Jla01DM1V3b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VTHJJSlZvVWFoWT1OaWZZZQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVjblNCVTU2UWRrTGIyZlBkcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[23]</b>
                                         Dempster A P, Laird N M, Rubin D B.Maximum likelihood from incomplete data via the EM algorithm[J].Journal of the Royal Statistical Society, Series B, 1977, 39 (1) :1～38.
                                    </a>
                                </li>
                                <li id="50">


                                    <a id="bibliography_24" title=" Barron A, Roos T, Watanabe K.Bayesian properties of normalized maximum likelihood and its fast computation[C]//IEEE IT Symposium on Information Theory, 2014:1667～1671." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Bayesian properties of normalized maximum likelihood and its fast computation">
                                        <b>[24]</b>
                                         Barron A, Roos T, Watanabe K.Bayesian properties of normalized maximum likelihood and its fast computation[C]//IEEE IT Symposium on Information Theory, 2014:1667～1671.
                                    </a>
                                </li>
                                <li id="52">


                                    <a id="bibliography_25" title=" Lu C.Semantic channel and Shannon channel mutually match and iterate for tests and estimations with maximum mutual information and maximum likelihood[C]//Proceedings of International Conference on Big Data and Smart Computing, Shanghai, 2018." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Semantic channel and Shannon channel mutually match and iterate for tests and estimations with maximum mutual information and maximum likelihood">
                                        <b>[25]</b>
                                         Lu C.Semantic channel and Shannon channel mutually match and iterate for tests and estimations with maximum mutual information and maximum likelihood[C]//Proceedings of International Conference on Big Data and Smart Computing, Shanghai, 2018.
                                    </a>
                                </li>
                                <li id="54">


                                    <a id="bibliography_26" title=" Lu C.Channels’ matching algorithm for mixture models[C]//Proceedings of International Conference on Intelligence Science, Shanghai, 2017." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Channels&amp;#39;&amp;#39; matching algorithm for mixture models">
                                        <b>[26]</b>
                                         Lu C.Channels’ matching algorithm for mixture models[C]//Proceedings of International Conference on Intelligence Science, Shanghai, 2017.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=MUTE" target="_blank">模糊系统与数学</a>
                2019,33(02),56-69             </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm">Zadeh的隶属函数对似然方法、语义通信和统计学习的意义</span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%B2%81%E6%99%A8%E5%85%89&amp;code=41742867&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVjblNCVTU2UWRrTGIyZlBkcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">鲁晨光</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%B1%AA%E5%9F%B9%E5%BA%84&amp;code=30463150&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVjblNCVTU2UWRrTGIyZlBkcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">汪培庄</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E8%BE%BD%E5%AE%81%E5%B7%A5%E7%A8%8B%E6%8A%80%E6%9C%AF%E5%A4%A7%E5%AD%A6%E6%99%BA%E8%83%BD%E5%B7%A5%E7%A8%8B%E4%B8%8E%E6%95%B0%E5%AD%A6%E7%A0%94%E7%A9%B6%E9%99%A2&amp;code=0034851&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVjblNCVTU2UWRrTGIyZlBkcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">辽宁工程技术大学智能工程与数学研究院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>流行的似然方法不合适数据先验分布 (即信源) 可变场合。为此, 我们把Zadeh的隶属函数看做预测模型, 用隶属函数和可变信源产生似然函数, 用平均对数标准 (normalized) 似然度定义语义信息测度。这样可以保证: (1) 坚持使用最大似然准则; (2) 预测模型适合信源可变场合; (3) 得到的语义贝叶斯预测兼容贝叶斯定理; (4) 预测模型能表达语义, 便于理解。一组隶属函数构成一个语义信道, 优化隶属函数就是使语义信道匹配Shannon信道, 产生多标签模糊分类。文中介绍了通过两种信道相互匹配求解最大似然度的迭代算法。几个例子显示这种算法用于检验、估计和混合模型时, 收敛快速且可靠。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%A8%A1%E7%B3%8A%E9%9B%86%E5%90%88&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVjblNCVTU2UWRrTGIyZlBkcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">模糊集合;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%9A%B6%E5%B1%9E%E5%87%BD%E6%95%B0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVjblNCVTU2UWRrTGIyZlBkcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">隶属函数;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Shannon%E4%BF%A1%E6%81%AF%E8%AE%BA&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVjblNCVTU2UWRrTGIyZlBkcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Shannon信息论;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%AF%AD%E4%B9%89%E4%BF%A1%E6%81%AF&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVjblNCVTU2UWRrTGIyZlBkcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">语义信息;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E5%BA%A6&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVjblNCVTU2UWRrTGIyZlBkcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">最大似然度;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%A4%9A%E6%A0%87%E7%AD%BE%E5%88%86%E7%B1%BB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVjblNCVTU2UWRrTGIyZlBkcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">多标签分类;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%BC%B0%E8%AE%A1&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVjblNCVTU2UWRrTGIyZlBkcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">估计;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVjblNCVTU2UWRrTGIyZlBkcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">混合模型;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    鲁晨光 (1955-) , 男, 安徽含山人, 辽宁工程技术大学智能工程与数学研究院客座教授 (原长沙大学副教授) , 研究方向:语义信息论, 模糊逻辑, 统计学习, 色觉机制和哲学;;
                                </span>
                                <span>
                                    汪培庄 (1936-) , 湖北黄冈人, 辽宁工程技术大学智能工程与数学研究院特聘教授 (原北师大数学系教授) , 博士生导师, 研究方向:人工智能, 模糊数学, 概率论和随机过程等。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2017-10-03</p>

            </div>
                    <h1><b>Significance of Zadeh’s Membership Functions to Likelihood Method</b>, <b>Semantic Communication, and Statistical Learning</b></h1>
                    <h2>
                    <span>LU Chen-guang</span>
                    <span>WANG Pei-zhuang</span>
            </h2>
                    <h2>
                    <span>College of Intelligence Engineering and Mathematics, Liaoning Engineering and Technology University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>The popular likelihood method cannot be properly used in cases where the prior distribution of data (or sources) are variable.Hence, we use Zadeh's membership function as the predictive model, use this function with a changeable source to produce a likelihood function, and define the semantic information measure with average log-normalized-likelihood. Then we can ensure that (1) the maximum likelihood criterion is always adopted; (2) the predictive model may be used in cases where sources are changeable; (3) the probability prediction is compatible with the Bayes' theorem; (4) a predictive model may indicate the semantic meaning of a hypothesis and may be more understandable. A group of membership functions form a semantic channel. To optimize a group of membership functions is to let a semantic channel match a Shannon's channel to make a multi-class and multi-label fuzzy classification. Through two channels' mutual matching, we can obtain an iterative algorithm for maximum mutual information and maximum likelihood. Several examples show that this algorithm for tests, estimations, and mixture models is fast and reliable.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Fuzzy%20Set&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVjblNCVTU2UWRrTGIyZlBkcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Fuzzy Set;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Membership%20Function&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVjblNCVTU2UWRrTGIyZlBkcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Membership Function;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Shannon%20Information%20Theory&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVjblNCVTU2UWRrTGIyZlBkcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Shannon Information Theory;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Semantic%20Information%20Theory&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVjblNCVTU2UWRrTGIyZlBkcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Semantic Information Theory;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Maximum%20Likelihood%20Estimation&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVjblNCVTU2UWRrTGIyZlBkcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Maximum Likelihood Estimation;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Multi-label%20Classification&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVjblNCVTU2UWRrTGIyZlBkcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Multi-label Classification;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Mixture%20Models&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVjblNCVTU2UWRrTGIyZlBkcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Mixture Models;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Statistical%20Learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVjblNCVTU2UWRrTGIyZlBkcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Statistical Learning;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2017-10-03</p>
                            </div>


        <!--brief start-->
                        <div class="p1">
                    <p id="56">Zadeh提出模糊集合理论, 并且用隶属函数定义模糊集合<citation id="234" type="reference"><link href="4" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>。 按照Zadeh的定义, 论域<i>A</i>中的元素<i>X</i>在模糊集合<i>A</i><sub><i>j</i></sub>上的隶属函数<i>m</i><sub><i>Aj</i></sub> (<i>X</i>) 也就是假设“<i>X</i> 在 <i>A</i><sub><i>j</i></sub>中”的真值函数。关于隶属函数和似然度的关系, 已有很多讨论<citation id="235" type="reference"><link href="6" rel="bibliography" /><link href="8" rel="bibliography" /><link href="10" rel="bibliography" /><link href="12" rel="bibliography" /><link href="14" rel="bibliography" /><link href="16" rel="bibliography" /><sup>[<a class="sup">2</a>,<a class="sup">3</a>,<a class="sup">4</a>,<a class="sup">5</a>,<a class="sup">6</a>,<a class="sup">7</a>]</sup></citation>, 有人认为隶属函数可以解释为似然函数<citation id="236" type="reference"><link href="8" rel="bibliography" /><link href="10" rel="bibliography" /><link href="12" rel="bibliography" /><sup>[<a class="sup">3</a>,<a class="sup">4</a>,<a class="sup">5</a>]</sup></citation>;也有人认为隶属函数和数据的先验分布<i>P</i> (<i>X</i>) 一起, 可以产生似然函数<citation id="237" type="reference"><link href="14" rel="bibliography" /><link href="16" rel="bibliography" /><sup>[<a class="sup">6</a>,<a class="sup">7</a>]</sup></citation>:</p>
                </div>
                <div class="p1">
                    <p id="57" class="code-formula">
                        <mathml id="57"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>X</mi><mo stretchy="false">|</mo><mi>A</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>X</mi><mo stretchy="false">) </mo><mi>m</mi><msub><mrow></mrow><mrow><mi>A</mi><msub><mrow></mrow><mi>j</mi></msub></mrow></msub><mo stretchy="false"> (</mo><mi>X</mi><mo stretchy="false">) </mo><mo>/</mo><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>A</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="58">其中<mathml id="59"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>A</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>i</mi><mspace width="0.25em" /></mrow></munder><mi>Ρ</mi></mstyle><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mspace width="0.25em" /><mi>m</mi><msub><mrow></mrow><mrow><mi>A</mi><msub><mrow></mrow><mi>j</mi></msub></mrow></msub><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo></mrow></math></mathml>, 就是Zadeh提出的模糊事件的概率<citation id="238" type="reference"><link href="18" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>。 根据Davidson的真值条件语义学<citation id="239" type="reference"><link href="20" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>, 一个真值函数确定了一个假设的语义。所以我们称式 (1) 所示似然函数是语义似然函数, 或语义贝叶斯预测。模糊事件的概率就是假设<i>Y</i>的逻辑概率。</p>
                </div>
                <div class="p1">
                    <p id="60">汪培庄<citation id="240" type="reference"><link href="22" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>用随机集合落影定义隶属函数, 鲁晨光根据汪培庄的定义推导出了式 (1) <citation id="241" type="reference"><link href="24" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>, 并且推广Shannon信息论<citation id="242" type="reference"><link href="26" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>, 用对数标准似然度 (log-normalized likelihood) 定义语义信息测度, 得出了一系列关于优化语义通信的结论<citation id="243" type="reference"><link href="24" rel="bibliography" /><link href="28" rel="bibliography" /><sup>[<a class="sup">11</a>,<a class="sup">13</a>]</sup></citation>。我们最近的研究表明, 用语义似然函数和鲁晨光的语义信息方法, 可以得到一种适应性更广的假设检验和统计学习方法。</p>
                </div>
                <div class="p1">
                    <p id="61">最大似然方法要解决的问题是:使用最大似然准则, 用样本优化预测模型, 或用<i>n</i>个条件样本优化<i>n</i>个子模型。当一个新的条件样本或样本点出现时, 我们选择一个假设 (即模型标签) 作为预测, 使得似然度最大<citation id="244" type="reference"><link href="30" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>。然而这个方法不考虑数据<i>X</i>的先验概率分布, 即信源<i>P</i> (<i>X</i>) , 也不考虑假设 (或模型标签) <i>Y</i>或相应模型<i>θ</i>的先验概率分布<i>P</i> (<i>Y</i>) 或<i>P</i> (<i>θ</i>) 。当信源变化时, 原先优化的模型就会失效。比如对于HIV (艾滋病毒) 检验, 用似然方法优化预测模型时 (相应的假设是阳性和阴性) , 由普通人群得到的模型只能用于普通人群, 应用到高危人群 (比如同性恋人群) 就会失效。再比如, 当我们用似然方法预测小车相对于GPS指针的位置时 (做贝叶斯预测) , 需要考虑路况。带有GPS设备的汽车行驶在草原上和高速公路上, 我们根据GPS箭头预测小车相对指针的位置的概率分布 (似然函数) 是不同的 (参看图1) 。从Shannon信息论的角度看, GPS和医学检验都提供信道, 根据观察条件选择假设的规则也是提供信道。信道通常是稳定的。数据的先验分布是信源, 信源是经常变化的。所以, 我们希望得到一种预测模型, 它反映信道特性;当信源变化时, 模型仍然适用。后面将证明, Zadeh的隶属函数就是这样的模型!</p>
                </div>
                <div class="p1">
                    <p id="62">为了改进似然方法和贝叶斯推理, 最大后验估计——即MAP (Maximum A-Posterior) 估计——兼用似然度和模型的先验分布P (θ) 优化预测模型。这种方法不再严格坚持最大似然准则。根据MAP, 如果我们根据Y和或相应模型θ的先验概率分布做贝叶斯测, 得到的结果也不能保证和传统的贝叶斯预测 (根据贝叶斯定理) 得到的结果一致。</p>
                </div>
                <div class="p1">
                    <p id="63">本研究把Zadeh的隶属函数解释为假设或预测模型的真值函数;把一个模糊集合看作是一个预测子模型;用真值函数产生似然函数;然后用标准 (normalized) 似然度定义语义信息测度;用最大语义信息准则优化检验, 估计和预测。这种方法将能满足下面四项要求: (1) 坚持使用最大似然准则; (2) 预测模型适合信源可变场合; (3) 用优化的隶属函数做贝叶斯预测, 这样的预测和传统的贝叶斯预测兼容; (4) 模型反映语义, 便于理解。</p>
                </div>
                <div class="p1">
                    <p id="64">本文首先假设Shannon信道不变而信源可变。在这种情况下, 我们先考虑样本足够大时的非参数估计, 讨论如何结合传统的贝叶斯预测和最大似然准则优化和选择假设。</p>
                </div>
                <div class="p1">
                    <p id="65">然后, 我们考虑样本不够大时的参数估计, 讨论如何用隶属函数作为预测模型和如何用对数标准似然度定义语义信息, 如何用语义信息方法改进似然方法, 用于模型优化和假设选择 (即Bayesian决策) 。同时我们将讨论信道匹配方法如何用于逻辑分类 (分类是模糊的, 即多标签学习) 和选择分类 (分类是清晰的, 标签是一个简单或复合标签) 。</p>
                </div>
                <div class="p1">
                    <p id="66">最后, 我们简要讨论在Shannon信道需要优化时, 或者说样本标签不确定时, 如何通过两种信道相互匹配得到一种新的迭代算法 (信道匹配算法或CM算法) , 用CM算法优化Shannon信道, 产生具有最大平均对数似然度的检验和估计, 以及产生最小相对熵的混合模型。</p>
                </div>
                <h3 id="67" name="67" class="anchor-tag">1 从Shannon信道看传统的贝叶斯预测和最大似然估计</h3>
                <h4 class="anchor-tag" id="68" name="68"><b>1.1</b><i>Shannon</i>信道和转移概率函数</h4>
                <div class="p1">
                    <p id="69">我们先从<i>Shannon</i>信道<citation id="245" type="reference"><link href="26" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>谈起。</p>
                </div>
                <div class="p1">
                    <p id="70">设X是表示信源 (或数据) 的离散随机变量, 取值于集合A={x<sub>1</sub>, x<sub>2</sub>, …, x<sub>m</sub>};Y是表示信宿 (即假设) 的离散随机变量, 取值于集合B={y<sub>1</sub>, y<sub>2</sub>, …, y<sub>n</sub>};Z是表示观察条件的离散随机变量, 取值于集合C={z<sub>1</sub>, z<sub>2</sub>, …, z<sub>w</sub>}。我们根据Z, 选择Y, 预测X.从假设-检验的角度看, X是证据或样本点, Y是假设;我们用样本或样本的概率分布评价和检验一个假设。比如对于天气预报, X是日降雨量, Y是预报语句 (比如“明天有小到中雨”) , Z是预测依据的气象数据;对于医学检验, X是真有病或真没病的测试者, Y是阳性或阴性, Z是化验数据。对于<i>GPS</i>, X是带有<i>GPS</i>装置的小车的实际位置, Y是<i>GPS</i>箭头所指位置, Z是<i>GPS</i>设备到三个卫星的距离。</p>
                </div>
                <div class="p1">
                    <p id="71">我们用P (X) 表示X的概率分布, P (X) 又叫信源;用P (Y) 表示Y的概率分布, P (Y) 又叫信宿。<i>Shannon</i>用下面转移概率矩阵定义<i>Shannon</i>信道:</p>
                </div>
                <div class="p1">
                    <p id="72" class="code-formula">
                        <mathml id="72"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>Y</mi><mo stretchy="false">|</mo><mi>X</mi><mo stretchy="false">) </mo><mo>⇔</mo><mrow><mo>[</mo><mrow><mtable><mtr><mtd><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>y</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">|</mo><mi>x</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">) </mo></mtd><mtd><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>y</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">|</mo><mi>x</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">) </mo></mtd><mtd><mo>⋯</mo></mtd><mtd><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>y</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">|</mo><mi>x</mi><msub><mrow></mrow><mi>m</mi></msub><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>y</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">|</mo><mi>x</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">) </mo></mtd><mtd><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>y</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">|</mo><mi>x</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">) </mo></mtd><mtd><mo>⋯</mo></mtd><mtd><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>y</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">|</mo><mi>x</mi><msub><mrow></mrow><mi>m</mi></msub><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mo>⋯</mo></mtd><mtd><mo>⋯</mo></mtd><mtd><mo>⋯</mo></mtd><mtd><mo>⋯</mo></mtd></mtr><mtr><mtd><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>y</mi><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false">|</mo><mi>x</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">) </mo></mtd><mtd><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>y</mi><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false">|</mo><mi>x</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">) </mo></mtd><mtd><mo>⋯</mo></mtd><mtd><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>y</mi><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false">|</mo><mi>x</mi><msub><mrow></mrow><mi>m</mi></msub><mo stretchy="false">) </mo></mtd></mtr></mtable></mrow><mo>]</mo></mrow><mo>⇔</mo><mrow><mo>[</mo><mrow><mtable><mtr><mtd><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>y</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">|</mo><mi>X</mi><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>y</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">|</mo><mi>X</mi><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mo>⋯</mo></mtd></mtr><mtr><mtd><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>y</mi><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false">|</mo><mi>X</mi><mo stretchy="false">) </mo></mtd></mtr></mtable></mrow><mo>]</mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="73">其中双向箭头表示等价。其中一行P (y<sub>j</sub>|X) (注意:y<sub>j</sub>不变X变) 可被称作y<sub>j</sub>的转移概率函数 (机器学习中也被称之为学习函数) 。于是, 一组转移概率函数构成一个<i>Shannon</i>信道。转移概率函数有两个重要性质:</p>
                </div>
                <div class="p1">
                    <p id="74"> (1) P (y<sub>j</sub>|X) 和条件概率函数P (Y|x<sub>i</sub>) (或P (X|y<sub>j</sub>) ) 不同, 后者是归一化的, 而转移概率函数不是归一化的, 即一般情况下, <mathml id="75"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mi>i</mi></munder><mi>Ρ</mi></mstyle><mo stretchy="false"> (</mo><mi>y</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">|</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>≠</mo><mn>1</mn></mrow></math></mathml>。</p>
                </div>
                <div class="p1">
                    <p id="76"> (2) 可以用<i>P</i> (<i>y</i><sub><i>j</i></sub>|<i>X</i>) 和<i>P</i> (<i>X</i>) 做贝叶斯预测, 得到<i>X</i>的后验概率分布<i>P</i> (<i>X</i>|<i>y</i><sub><i>j</i></sub>) , 而且<i>P</i> (<i>y</i><sub><i>j</i></sub>|<i>X</i>) 乘上一个系数<i>k</i>, 预测不变, 即</p>
                </div>
                <div class="p1">
                    <p id="77" class="code-formula">
                        <mathml id="77"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>X</mi><mo stretchy="false">) </mo><mi>k</mi><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>y</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">|</mo><mi>X</mi><mo stretchy="false">) </mo></mrow><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mi>i</mi></munder><mi>Ρ</mi></mstyle><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mi>k</mi><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>y</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">|</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo></mrow></mfrac><mo>=</mo><mfrac><mrow><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>X</mi><mo stretchy="false">) </mo><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>y</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">|</mo><mi>X</mi><mo stretchy="false">) </mo></mrow><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mi>i</mi></munder><mi>Ρ</mi></mstyle><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>y</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">|</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo></mrow></mfrac><mo>=</mo><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>X</mi><mo stretchy="false">|</mo><mi>y</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="78">Shannon互信息被定义为<citation id="246" type="reference"><link href="26" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation><sup></sup></p>
                </div>
                <div class="p1">
                    <p id="79" class="code-formula">
                        <mathml id="79"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ι</mi><mo stretchy="false"> (</mo><mi>X</mi><mo>;</mo><mi>Y</mi><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>j</mi></munder><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mi>i</mi></munder><mi>Ρ</mi></mstyle></mrow></mstyle><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>y</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mi>log</mi><mfrac><mrow><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">|</mo><mi>y</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo></mrow><mrow><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="80">它和传统的贝叶斯预测之间的关系是显然的。</p>
                </div>
                <h4 class="anchor-tag" id="81" name="81"><b>1.2</b> 信源可变时传统的贝叶斯预测和最大似然估计</h4>
                <div class="p1">
                    <p id="82">在模型预测方法中, 我们之所以采用参数构造似然函数, 是因为样本不够大时, 我们得不到完整的转移概率函数或<i>Shannon</i> 信道。如果样本足够大, 我们就可以通过统计得到转移概率函数P (y<sub>j</sub>|X) , 然后根据贝叶斯定理, 得到贝叶斯预测:</p>
                </div>
                <div class="p1">
                    <p id="83" class="code-formula">
                        <mathml id="83"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>X</mi><mo stretchy="false">|</mo><mi>θ</mi><msub><mrow></mrow><mi>j</mi></msub><mspace width="0.25em" /><mo stretchy="false">) </mo><mo>=</mo><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>X</mi><mo stretchy="false">|</mo><mi>y</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>X</mi><mo stretchy="false">) </mo><mspace width="0.25em" /><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>y</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">|</mo><mi>X</mi><mo stretchy="false">) </mo><mo>/</mo><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>y</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>5</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="84">式中θ<sub>j</sub> 只表示相应y<sub>j</sub>的预测子模型, 并不表示模型参数。当信源由P (X) 变P' (X) 时, 预测仍然有效, 变为</p>
                </div>
                <div class="p1">
                    <p id="85" class="code-formula">
                        <mathml id="85"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><mo>’</mo><mo stretchy="false"> (</mo><mi>X</mi><mo stretchy="false">|</mo><mi>θ</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mspace width="0.25em" /><mi>Ρ</mi><mo>’</mo><mo stretchy="false"> (</mo><mi>X</mi><mo stretchy="false">) </mo><mspace width="0.25em" /><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>y</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">|</mo><mi>X</mi><mo stretchy="false">) </mo><mo>/</mo><mi>Ρ</mi><mo>’</mo><mo stretchy="false"> (</mo><mi>y</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>6</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="86">其中<i>P</i>’<mathml id="87"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false"> (</mo><mi>y</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>i</mi></munder><mspace width="0.25em" /></mstyle><mi>Ρ</mi></mrow></math></mathml>’ (<i>x</i><sub><i>i</i></sub>) <i>P</i> (<i>y</i><sub><i>j</i></sub>|<i>x</i><sub><i>i</i></sub>) 。</p>
                </div>
                <div class="p1">
                    <p id="88">假设样本大小是<i>N</i>, <i>N</i>个样本点是<i>x</i> (1) , <i>x</i> (2) , …, <i>x</i> (<i>N</i>) 。则根据模型<i>θ</i><sub><i>j</i></sub>预测的样本发生的概率是<i>P</i> (<i>x</i> (1) , <i>x</i> (2) , …, <i>x</i> (<i>N</i>) |<i>θ</i><sub><i>j</i></sub>) 。假设<i>N</i>个样本点来自<i>N</i>个独立同分布随机变量, 则<i>P</i> (<i>X</i>|<i>θ</i><sub><i>j</i></sub>) 就是似然函数, 从而有样本和模型之间的对数似然度 (参看<citation id="247" type="reference">[<a class="sup">15</a>]</citation>的11.7节) :</p>
                </div>
                <div class="p1">
                    <p id="89" class="code-formula">
                        <mathml id="89"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>log</mi><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>X</mi><msup><mrow></mrow><mi>Ν</mi></msup><mo stretchy="false">|</mo><mi>θ</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mi>log</mi><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo><mo>, </mo><mi>x</mi><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo><mo>⋯</mo><mi>x</mi><mo stretchy="false"> (</mo><mi>Ν</mi><mo stretchy="false">) </mo><mo stretchy="false">|</mo><mi>θ</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mo>=</mo><mi>log</mi><mstyle displaystyle="true"><munderover><mo>∏</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mi>Ρ</mi></mstyle><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo><mo stretchy="false">|</mo><mi>θ</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mi>Ν</mi><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mi>Ρ</mi></mstyle><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">|</mo><mi>y</mi><msub><mrow></mrow><mo>?</mo></msub><mo stretchy="false">) </mo><mi>log</mi><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">|</mo><mi>θ</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>7</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="90">其中<i>y</i><sub>?</sub>是未知假设, <i>P</i> (<i>x</i><sub><i>i</i></sub>|<i>y</i><sub>?</sub>) , <i>i</i>=1, 2, …, <i>m</i>是<i>N</i>个样本点在<i>A</i>上的频率分布。改变<i>θ</i><sub><i>j</i></sub>中的<i>j</i>, 可以最大化上面似然度。如果使似然度达最大的<i>j</i>是<i>j</i><sup>*</sup>, 那么<i>θ</i><sub><i>j</i><sup>*</sup></sub>就是最优估计。这样的估计可以保证, 当<i>P</i> (<i>X</i>|<i>θ</i><sub>1</sub>) =<i>P</i> (<i>X</i>|<i>y</i><sub>?</sub>) 时, <i>y</i><sub><i>j</i><sup>*</sup></sub>=<i>y</i><sub>1</sub>.而流行的贝叶斯推理不能保证<citation id="270" type="note"><link href="2" rel="footnote" /><sup> (1) </sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="91">发信人发送<i>y</i><sub><i>j</i><sup>*</sup></sub>时, 收信者结合<i>P</i> (<i>y</i><sub><i>j</i><sup>*</sup></sub>|<i>X</i>) 和<i>P</i> (<i>X</i>) (或<i>P</i>′ (<i>X</i>) ) 就可以做贝叶斯预测, 得到<i>P</i> (<i>X</i>|<i>θ</i><sub><i>j</i><sup>*</sup></sub>) 。</p>
                </div>
                <div class="p1">
                    <p id="92">然而上述方法有下面缺点:</p>
                </div>
                <div class="p1">
                    <p id="93"> (1) 样本不够大时, 得不到完整的且有规律的转移概率函数<i>P</i> (<i>y</i><sub><i>j</i></sub>|<i>X</i>) ;</p>
                </div>
                <div class="p1">
                    <p id="94"> (2) 求<i>P</i> (<i>y</i><sub><i>j</i></sub>|<i>X</i>) 需要<i>P</i> (<i>y</i><sub><i>j</i></sub>) , 而<i>P</i> (<i>y</i><sub><i>j</i></sub>) 常常是得不到的。比如对于天气预报, 我们只记录和统计了不同降水量的先验概率分布和预报<i>y</i><sub>1</sub>=“有暴雨”后的降水量概率分布, 并不知道不同预报的概率分布<i>P</i> (<i>Y</i>) 。如何建立<i>y</i><sub>1</sub>的概率预测模型 (子模型) , 使得先验概率分布变化时仍然适用, 这是个问题。下面我们将能看到, Zadeh的隶属函数就能克服转移概率函数的上述缺点!</p>
                </div>
                <h3 id="95" name="95" class="anchor-tag">2 基于隶属函数的语义信息理论</h3>
                <h4 class="anchor-tag" id="96" name="96"><b>2.1</b><i>Shannon</i>信道和语义信道</h4>
                <div class="p1">
                    <p id="97">现在我们用随机变量Ѳ表示一个模糊集合, Ѳ同时也是一个预测模型或一组模型参数;用<i>θ</i><sub><i>j</i></sub> 表示Ѳ的一个取值, 并且<i>y</i><sub><i>j</i></sub>等于模糊假设“<i>X</i>在<i>θ</i><sub><i>j</i></sub>中”。我们用<i>y</i><sub><i>j</i></sub> (<i>X</i>) 表示一个谓词, 其真值函数就是<i>X</i>在模糊集合<i>θ</i><sub><i>j</i></sub>上的隶属函数。为了强调它作为真值的含义并区别统计概率<i>P</i>, 后面我们用<i>T</i> (<i>θ</i><sub><i>j</i></sub>|<i>X</i>) 表示<i>y</i><sub><i>j</i></sub>真值函数和<i>θ</i><sub><i>j</i></sub>的隶属函数。即<i>T</i> (<i>θ</i><sub><i>j</i></sub>|<i>X</i>) =<i>m</i><sub><i>θ</i><sub><i>j</i></sub></sub> (<i>X</i>) ∈[0, 1]。</p>
                </div>
                <div class="p1">
                    <p id="98">对比流行的似然度方法, 上述方法使用子模型<i>θ</i><sub>1</sub> , <i>θ</i><sub>2</sub>, …, <i>θ</i><sub><i>n</i></sub>而不是一个模型<i>θ</i> or Ѳ. <i>P</i> (<i>X</i>|<i>θ</i><sub><i>j</i></sub>) 等价于流行的似然方法中的<i>P</i> (<i>X</i>|<i>y</i><sub><i>j</i></sub>, <i>θ</i>) 。用来检验<i>y</i><sub><i>j</i></sub>的样本也是一个子样本, 或条件样本。这些改变将使新的似然方法 (即语义信息方法) 更加灵活, 更加兼容Shannon信息论。</p>
                </div>
                <div class="p1">
                    <p id="99">当<i>X</i>=<i>x</i><sub><i>i</i></sub>时, <i>y</i><sub><i>j</i></sub> (<i>X</i>) 变成命题<i>y</i><sub><i>j</i></sub> (<i>x</i><sub><i>i</i></sub>) , 其真值是<i>T</i> (<i>θ</i><sub><i>j</i></sub>|<i>x</i><sub><i>i</i></sub>) 。于是, 一个语义信道由若干真值或真值函数 (即隶属函数) 构成:</p>
                </div>
                <div class="p1">
                    <p id="100" class="code-formula">
                        <mathml id="100"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Τ</mi><mo stretchy="false"> (</mo><mi>Θ</mi><mo stretchy="false">|</mo><mi>X</mi><mo stretchy="false">) </mo><mo>⇔</mo><mrow><mo>[</mo><mrow><mtable><mtr><mtd><mi>Τ</mi><mo stretchy="false"> (</mo><mi>θ</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">|</mo><mi>x</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">) </mo></mtd><mtd><mi>Τ</mi><mo stretchy="false"> (</mo><mi>θ</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">|</mo><mi>x</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">) </mo></mtd><mtd><mo>⋯</mo></mtd><mtd><mi>Τ</mi><mo stretchy="false"> (</mo><mi>θ</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">|</mo><mi>x</mi><msub><mrow></mrow><mi>m</mi></msub><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi>Τ</mi><mo stretchy="false"> (</mo><mi>θ</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">|</mo><mi>x</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">) </mo></mtd><mtd><mi>Τ</mi><mo stretchy="false"> (</mo><mi>θ</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">|</mo><mi>x</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">) </mo></mtd><mtd><mo>⋯</mo></mtd><mtd><mi>Τ</mi><mo stretchy="false"> (</mo><mi>θ</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">|</mo><mi>x</mi><msub><mrow></mrow><mi>m</mi></msub><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mo>⋯</mo></mtd><mtd><mo>⋯</mo></mtd><mtd><mo>⋯</mo></mtd><mtd><mo>⋯</mo></mtd></mtr><mtr><mtd><mi>Τ</mi><mo stretchy="false"> (</mo><mi>θ</mi><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false">|</mo><mi>x</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">) </mo></mtd><mtd><mi>Τ</mi><mo stretchy="false"> (</mo><mi>θ</mi><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false">|</mo><mi>x</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">) </mo></mtd><mtd><mo>⋯</mo></mtd><mtd><mi>Τ</mi><mo stretchy="false"> (</mo><mi>θ</mi><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false">|</mo><mi>x</mi><msub><mrow></mrow><mi>m</mi></msub><mo stretchy="false">) </mo></mtd></mtr></mtable></mrow><mo>]</mo></mrow><mo>⇔</mo><mrow><mo>[</mo><mrow><mtable><mtr><mtd><mi>Τ</mi><mo stretchy="false"> (</mo><mi>θ</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">|</mo><mi>X</mi><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi>Τ</mi><mo stretchy="false"> (</mo><mi>θ</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">|</mo><mi>X</mi><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mo>⋯</mo></mtd></mtr><mtr><mtd><mi>Τ</mi><mo stretchy="false"> (</mo><mi>θ</mi><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false">|</mo><mi>X</mi><mo stretchy="false">) </mo></mtd></mtr></mtable></mrow><mo>]</mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>8</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="101">语义信道和Shannon信道一样可以用作贝叶斯预测——语义贝叶斯预测, 产生似然函数:</p>
                </div>
                <div class="p1">
                    <p id="102" class="code-formula">
                        <mathml id="102"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>X</mi><mo stretchy="false">|</mo><mi>θ</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>X</mi><mo stretchy="false">) </mo><mi>Τ</mi><mo stretchy="false"> (</mo><mi>θ</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">|</mo><mi>X</mi><mo stretchy="false">) </mo><mo>/</mo><mi>Τ</mi><mo stretchy="false"> (</mo><mi>θ</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mo>, </mo><mspace width="0.25em" /><mi>Τ</mi><mo stretchy="false"> (</mo><mi>θ</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>i</mi></munder><mi>Ρ</mi></mstyle><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mi>Τ</mi><mo stretchy="false"> (</mo><mi>θ</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">|</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>9</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="103">其中<i>T</i> (<i>θ</i><sub><i>j</i></sub>) 就是<i>y</i><sub><i>j</i></sub>的逻辑概率。由式 (3) 可知, 当真值函数和转移概率函数成正比时, 语义贝叶斯预测和传统的贝叶斯预测等价。</p>
                </div>
                <div class="p1">
                    <p id="104"><i>y</i><sub><i>j</i></sub>的逻辑概率<i>T</i> (<i>θ</i><sub><i>j</i></sub>) 和被选择的概率<i>P</i> (<i>y</i><sub><i>j</i></sub>) 非常不同, 这是被经常忽视的。<i>T</i> (Ѳ) 也不是归一化的, 一般有<i>T</i> (<i>θ</i><sub>1</sub>) +<i>T</i> (<i>θ</i><sub>2</sub>) …+ <i>T</i> (<i>θn</i>) &gt;1。设<i>y</i><sub>1</sub>=“小雨” (“明天有小雨”的简写, 后面同理) , <i>y</i><sub>2</sub>=“中雨”, <i>y</i><sub>3</sub>=“小到中雨”。一定有<i>T</i> (<i>θ</i><sub>3</sub>) ≈<i>T</i> (<i>θ</i><sub>1</sub>∪<i>θ</i><sub>2</sub>) &gt;<i>T</i> (<i>θ</i><sub>1</sub>) , 但是可能<i>P</i> (<i>y</i><sub>3</sub>) &lt;<i>P</i> (<i>y</i><sub>1</sub>) 或<i>P</i> (<i>y</i><sub>3</sub>) =0。一个永真句, 比如“明天有雨或无雨”, 的逻辑概率是1, 但是它被选择的概率几乎是0。</p>
                </div>
                <div class="p1">
                    <p id="105"><i>P</i> (<i>X</i>|<i>θ</i><sub><i>j</i></sub>) 和<i>P</i> (<i>X</i>|<i>y</i><sub><i>j</i></sub>) 不同, <i>P</i> (<i>X</i>|<i>y</i><sub><i>j</i></sub>) 是样本的概率分布 (注意样本也是有条件的) 。<i>P</i> (<i>X</i>|<i>y</i><sub><i>j</i></sub>) 是<i>y</i><sub><i>j</i></sub>被选择后<i>X</i>发生的概率或频率。而<i>P</i> (<i>X</i>|<i>θ</i><sub><i>j</i></sub>) 则来自预测。</p>
                </div>
                <div class="p1">
                    <p id="106">一个语义信道后面总有一个Shannon信道。以天气预报为例, 转移概率函数<i>P</i> (<i>y</i><sub><i>j</i></sub>|<i>X</i>) 反映预报语句<i>y</i><sub><i>j</i></sub>的选择规律, 因预报员而异——有人预报对的多, 有人预报错的多。而<i>T</i> (<i>θ</i><sub><i>j</i></sub>|<i>X</i>) 反映<i>θ</i><sub><i>j</i></sub> 的语义, 主要取决于语言的定义, 但是也受过去的预报规则的影响 (后面详谈) 。不同的人理解的语义<i>T</i> (<i>θ</i><sub><i>j</i></sub>|<i>X</i>) 是大体相同的。</p>
                </div>
                <h4 class="anchor-tag" id="107" name="107"><b>2.2</b> 理解<i>GPS</i>定位——似然函数还是隶属函数?</h4>
                <div class="p1">
                    <p id="108">考虑全球定位系统 (GPS) 显示屏上的定位 (小圆圈或箭头) 的语义。它表示实际位置大概在某处。即<i>y</i><sub><i>j</i></sub>=“<i>X</i>≈<i>x</i><sub><i>j</i></sub>”。 一个时钟, 一个秤, 一个温度表, 含义类似。具有这样含义的<i>y</i><sub><i>j</i></sub>可谓无偏估计。设观察条件<i>Z</i>∈<i>C</i><sub><i>j</i></sub>时, 我们选择<i>y</i><sub><i>j</i></sub>=<i>f</i> (<i>Z</i>|<i>Z</i>∈<i>C</i><sub><i>j</i></sub>) , 则<i>P</i> (<i>y</i><sub><i>j</i></sub>|<i>X</i>) =<i>P</i> (<i>C</i><sub><i>j</i></sub>|<i>X</i>) , <i>j</i>=1, 2, …, <i>n</i>构成一个Shannon信道, 反映估计的选择规则。而一个无偏估计的语义信道可以表示为:</p>
                </div>
                <div class="p1">
                    <p id="109" class="code-formula">
                        <mathml id="109"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Τ</mi><mo stretchy="false"> (</mo><mi>θ</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">|</mo><mi>X</mi><mo stretchy="false">) </mo><mo>=</mo><mi>exp</mi><mo stretchy="false">[</mo><mo>-</mo><mo stretchy="false">|</mo><mi>X</mi><mo>-</mo><mi>x</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">|</mo><msup><mrow></mrow><mn>2</mn></msup><mo>/</mo><mo stretchy="false"> (</mo><mn>2</mn><mi>d</mi><msup><mrow></mrow><mn>2</mn></msup><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mo>, </mo><mtext> </mtext><mi>j</mi><mo>=</mo><mn>1</mn><mo>, </mo><mn>2</mn><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>n</mi><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>0</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="110">其中<i>d</i>是标准差。考虑GPS定位的特殊环境如图1所示。其中定位指在高楼上, 楼的左边是高速公路, 右边是普通公路。请问小车在哪里可能性最大?</p>
                </div>
                <div class="area_img" id="111">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MUTE201902009_111.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVjblNCVTU2UWRrTGIyZlBkcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 GPS定位图解" src="Detail/GetImg?filename=images/MUTE201902009_111.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVjblNCVTU2UWRrTGIyZlBkcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 GPS定位图解  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MUTE201902009_111.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVjblNCVTU2UWRrTGIyZlBkcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>
                                <p class="img_note">先验概率分布不均且不断变化时, 用山形分布的真值函数 (最大值是1) 表示定位的语义, 可以得到和常识一致的概率预测, 而用似然函数表示定位的语义不行。</p>

                </div>
                <div class="p1">
                    <p id="113">根据常识, 小车在高速公路上概率最大。如果认为GPS直接用似然函数预测小车位置, 则小车在楼顶上的概率最大——这是不对的。根据语义贝叶斯预测, 小车的后验概率分布或似然函数是</p>
                </div>
                <div class="p1">
                    <p id="114" class="code-formula">
                        <mathml id="114"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>X</mi><mo stretchy="false">|</mo><mi>θ</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mfrac><mrow><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>X</mi><mo stretchy="false">) </mo><mi>exp</mi><mo stretchy="false">[</mo><mo>-</mo><mo stretchy="false"> (</mo><mi>X</mi><mo>-</mo><mi>x</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><msup><mrow></mrow><mn>2</mn></msup><mo>/</mo><mo stretchy="false"> (</mo><mn>2</mn><mi>d</mi><msup><mrow></mrow><mn>2</mn></msup><mo stretchy="false">) </mo><mo stretchy="false">]</mo></mrow><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mi>i</mi></munder><mi>Ρ</mi></mstyle><mo stretchy="false"> (</mo><mi>X</mi><mo stretchy="false">) </mo><mi>exp</mi><mo stretchy="false">[</mo><mo>-</mo><mo stretchy="false"> (</mo><mi>X</mi><mo>-</mo><mi>x</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><msup><mrow></mrow><mn>2</mn></msup><mo>/</mo><mo stretchy="false"> (</mo><mn>2</mn><mi>d</mi><msup><mrow></mrow><mn>2</mn></msup><mo stretchy="false">) </mo><mo stretchy="false">]</mo></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="115">它和先验分布及真值函数的乘积成正比。上述结果和传统的贝叶斯预测兼容, 也和人脑推理结论一致。从GPS的例子可以看出, 语义信道比Shannon信道简单, 更易于理解。</p>
                </div>
                <h4 class="anchor-tag" id="116" name="116"><b>2.3</b> 用标准 (normalized) 似然度定义语义信息测度</h4>
                <div class="p1">
                    <p id="117">在<i>Shannon</i>信息论中, 只有统计概率, 没有逻辑概率, 也没有预测的概率 (似然度) 。鲁晨光提供的语义信息测度同时用到这三种概率<citation id="248" type="reference"><link href="24" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>, y<sub>j</sub>提供关于x<sub>i</sub>的信息量就是标准似然度的对数:</p>
                </div>
                <div class="p1">
                    <p id="118" class="code-formula">
                        <mathml id="118"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ι</mi><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>;</mo><mi>θ</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mi>log</mi><mfrac><mrow><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">|</mo><mi>θ</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo></mrow><mrow><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo></mrow></mfrac><mo>=</mo><mi>log</mi><mfrac><mrow><mi>Τ</mi><mo stretchy="false"> (</mo><mi>θ</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">|</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo></mrow><mrow><mi>Τ</mi><mo stretchy="false"> (</mo><mi>θ</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="119">其中用到语义贝叶斯预测, 并假设先验似然度等于先验概率P (x<sub>i</sub>) 。对于无偏估计, 真值函数和信息之间的关系如图2所示。</p>
                </div>
                <div class="area_img" id="120">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MUTE201902009_120.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVjblNCVTU2UWRrTGIyZlBkcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 语义信息量图解" src="Detail/GetImg?filename=images/MUTE201902009_120.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVjblNCVTU2UWRrTGIyZlBkcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 语义信息量图解  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MUTE201902009_120.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVjblNCVTU2UWRrTGIyZlBkcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>
                                <p class="img_note">偏差越大, 信息越少;逻辑概率越小, 信息量越大;错误预测提供负的信息。</p>

                </div>
                <div class="p1">
                    <p id="121">这个公式就能反映<i>Popper</i>的思想<citation id="249" type="reference"><link href="34" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>: (先验) 逻辑概率越小, 并能经得起检验 (后验逻辑概率越大) 信息量就越大; 永真句在逻辑上不能被证伪, 因而不含有信息。</p>
                </div>
                <div class="p1">
                    <p id="122">把式 (10) 中的T (θ<sub>j</sub>|X) 代入式 (12) , 就得到</p>
                </div>
                <div class="p1">
                    <p id="123" class="code-formula">
                        <mathml id="123"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ι</mi><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>;</mo><mi>θ</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mi>log</mi><mo stretchy="false"> (</mo><mn>1</mn><mo>/</mo><mi>Τ</mi><mo stretchy="false"> (</mo><mi>θ</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo>-</mo><mo stretchy="false">|</mo><mi>X</mi><mo>-</mo><mi>x</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">|</mo><msup><mrow></mrow><mn>2</mn></msup><mo>/</mo><mo stretchy="false"> (</mo><mn>2</mn><mi>d</mi><msup><mrow></mrow><mn>2</mn></msup><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="124">其中<i>log</i> (1/T (θ<sub>j</sub>) ) 就是<i>Bar</i>-<i>Hillel</i>和<i>Carnap</i>定义的语义信息测度<citation id="250" type="reference"><link href="36" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>。上述语义信息测度还考虑了偏差——语义信息量随偏差增大而减小。</p>
                </div>
                <div class="p1">
                    <p id="125">对I (x<sub>i</sub>; θ<sub>j</sub>) 求平均, 就得到广义<i>Kullback</i>-<i>Leibler</i> (<i>KL</i>) 信息:</p>
                </div>
                <div class="p1">
                    <p id="126" class="code-formula">
                        <mathml id="126"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ι</mi><mo stretchy="false"> (</mo><mi>X</mi><mo>;</mo><mi>θ</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>i</mi></munder><mi>Ρ</mi></mstyle><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">|</mo><mi>y</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mi>log</mi><mfrac><mrow><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">|</mo><mi>θ</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo></mrow><mrow><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo></mrow></mfrac><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>i</mi></munder><mi>Ρ</mi></mstyle><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">|</mo><mi>y</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mi>log</mi><mfrac><mrow><mi>Τ</mi><mo stretchy="false"> (</mo><mi>θ</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">|</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo></mrow><mrow><mi>Τ</mi><mo stretchy="false"> (</mo><mi>θ</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="127">其中对数左边是统计概率P (x<sub>i</sub>|y<sub>j</sub>) , i=1, 2, …, 它们构成样本概率分布P (X|y<sub>j</sub>) , 是用以检验θ<sub>j</sub>的。这个公式能反映<i>Popper</i>的观点:一个反例足以证伪一个普遍必然假设 (因为信息量负无穷大) 。</p>
                </div>
                <div class="p1">
                    <p id="128"><i>Akaike</i>揭示了似然度和<i>KL</i>信息之间的联系<citation id="251" type="reference"><link href="38" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>, 这一联系已经得到广泛关注 (见<citation id="252" type="reference">[<a class="sup">15</a>]</citation>中11.7节) 。然而, 上述广义<i>KL</i>信息和似然度之间的关于更加简单。</p>
                </div>
                <div class="p1">
                    <p id="129">对I (X; θ<sub>j</sub>) 求平均, 就得到广义或语义互信息公式:</p>
                </div>
                <div class="p1">
                    <p id="130" class="code-formula">
                        <mathml id="130"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ι</mi><mo stretchy="false"> (</mo><mi>X</mi><mo>;</mo><mi>Θ</mi><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>j</mi></munder><mi>Ρ</mi></mstyle><mo stretchy="false"> (</mo><mi>y</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>i</mi></munder><mi>Ρ</mi></mstyle><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">|</mo><mi>h</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mi>log</mi><mfrac><mrow><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">|</mo><mi>θ</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo></mrow><mrow><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo></mrow></mfrac><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>j</mi></munder><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mi>i</mi></munder><mi>Ρ</mi></mstyle></mrow></mstyle><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>y</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mi>log</mi><mfrac><mrow><mi>Τ</mi><mo stretchy="false"> (</mo><mi>θ</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">|</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo></mrow><mrow><mi>Τ</mi><mo stretchy="false"> (</mo><mi>θ</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>4</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="131">容易证明, 在语义贝叶斯预测和样本分布一致时, 即P (x<sub>i</sub>|θ<sub>j</sub>) =P (x<sub>i</sub>|y<sub>j</sub>) (对于所有i, j) 时, 上述语义互信息达到其上限, 等于<i>Shannon</i>互信息。</p>
                </div>
                <h3 id="132" name="132" class="anchor-tag">3 最大语义信息估计和语义信道优化</h3>
                <h4 class="anchor-tag" id="133" name="133"><b>3.1</b> 似然度和语义信息之间的关系</h4>
                <div class="p1">
                    <p id="134">假设相应<i>y</i><sub><i>j</i></sub>有<i>N</i><sub><i>j</i></sub>个样本, 它们来自<i>N</i><sub><i>j</i></sub>个独立同分布随机变量, 其中<i>x</i><sub><i>i</i></sub>有<i>N</i><sub><i>ij</i></sub>个; 当<i>N</i><sub><i>j</i></sub>无穷大时, 就有<i>P</i> (<i>X</i>|<i>y</i><sub><i>j</i></sub>) = <i>N</i><sub><i>ij</i></sub>/<i>N</i><sub><i>j</i></sub>.因此就有log (标准似然度) 和广义KL信息之间关系:</p>
                </div>
                <div class="p1">
                    <p id="135" class="code-formula">
                        <mathml id="135"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>log</mi><mstyle displaystyle="true"><munder><mo>∏</mo><mi>i</mi></munder><mrow><mrow><mo>[</mo><mrow><mfrac><mrow><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">|</mo><mi>θ</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo></mrow><mrow><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo></mrow></mfrac></mrow><mo>]</mo></mrow></mrow></mstyle><msup><mrow></mrow><mrow><mi>Ν</mi><msub><mrow></mrow><mrow><mi>j</mi><mi>i</mi></mrow></msub></mrow></msup><mo>=</mo><mi>Ν</mi><msub><mrow></mrow><mi>j</mi></msub><mstyle displaystyle="true"><munder><mo>∑</mo><mi>i</mi></munder><mi>Ρ</mi></mstyle><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">|</mo><mi>y</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mi>log</mi><mfrac><mrow><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">|</mo><mi>θ</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo></mrow><mrow><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo></mrow></mfrac><mo>=</mo><mi>Ν</mi><msub><mrow></mrow><mi>j</mi></msub><mi>Ι</mi><mo stretchy="false"> (</mo><mi>X</mi><mo>;</mo><mi>θ</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>5</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="136">对不同的<i>y</i><sub><i>j</i></sub>, <i>j</i>=1, 2, …, <i>n</i>求平均, 就得到平均log (标准似然度) , 它和语义互信息的关系是:</p>
                </div>
                <div class="p1">
                    <p id="137" class="code-formula">
                        <mathml id="137"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtable columnalign="left"><mtr><mtd><mtext> </mtext><mstyle displaystyle="true"><munder><mo>∑</mo><mi>j</mi></munder><mrow><mfrac><mrow><mi>Ν</mi><msub><mrow></mrow><mi>j</mi></msub></mrow><mi>Ν</mi></mfrac></mrow></mstyle><mi>log</mi><mstyle displaystyle="true"><munder><mo>∏</mo><mi>i</mi></munder><mrow><mrow><mo>[</mo><mrow><mfrac><mrow><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">|</mo><mi>θ</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo></mrow><mrow><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo></mrow></mfrac></mrow><mo>]</mo></mrow></mrow></mstyle><msup><mrow></mrow><mrow><mi>Ν</mi><msub><mrow></mrow><mrow><mi>j</mi><mi>i</mi></mrow></msub></mrow></msup></mtd></mtr><mtr><mtd><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>j</mi></munder><mi>Ρ</mi></mstyle><mo stretchy="false"> (</mo><mi>y</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>i</mi></munder><mi>Ρ</mi></mstyle><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">|</mo><mi>y</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mi>log</mi><mfrac><mrow><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">|</mo><mi>θ</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo></mrow><mrow><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo></mrow></mfrac><mo>=</mo><mi>Ι</mi><mo stretchy="false"> (</mo><mi>X</mi><mo>;</mo><mi>Θ</mi><mo stretchy="false">) </mo><mo>=</mo><mi>Η</mi><mo stretchy="false"> (</mo><mi>X</mi><mo stretchy="false">) </mo><mo>-</mo><mi>Η</mi><mo stretchy="false"> (</mo><mi>X</mi><mo stretchy="false">|</mo><mi>Θ</mi><mo stretchy="false">) </mo></mtd></mtr></mtable><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>6</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="138">因为优化模型<i>θ</i><sub><i>j</i></sub>或Ѳ时<i>P</i> (<i>X</i>) 不变, 最大似然准则等价于最大语义信息准则。容易证明:Shannon互信息是语义互信息的上限;似然函数和样本分布符合时, 两者相等。</p>
                </div>
                <h4 class="anchor-tag" id="139" name="139"><b>3.2</b> 预测模型或隶属函数的优化</h4>
                <div class="p1">
                    <p id="140">优化一个语义信道等价于优化一个模型Ѳ或一组子模型 (<i>θ</i><sub>1</sub>, <i>θ</i><sub>2</sub>, …, <i>θ</i><sub><i>n</i></sub>) 。给定Shannon信道时优化子模型<i>θ</i><sub><i>j</i></sub>, 也就是优化隶属函数<i>T</i> (<i>θ</i><sub><i>j</i></sub> |<i>X</i>) 。于是有</p>
                </div>
                <div class="p1">
                    <p id="141" class="code-formula">
                        <mathml id="141"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Τ</mi><msup><mrow></mrow><mo>*</mo></msup><mo stretchy="false"> (</mo><mi>θ</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">|</mo><mi>X</mi><mo stretchy="false">) </mo><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>arg</mi></mrow></mstyle><mrow><mi>Τ</mi><mo stretchy="false"> (</mo><mi>θ</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">|</mo><mi>X</mi><mo stretchy="false">) </mo></mrow></munder><mi>max</mi><mi>Ι</mi><mo stretchy="false"> (</mo><mi>X</mi><mo>;</mo><mi>θ</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>7</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="142"><i>I</i> (<i>X</i><sub>;</sub><i>θ</i><sub><i>j</i></sub>) 可以写成两个KL距离的差, </p>
                </div>
                <div class="p1">
                    <p id="143" class="code-formula">
                        <mathml id="143"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ι</mi><mo stretchy="false"> (</mo><mi>X</mi><mo>;</mo><mi>θ</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>i</mi></munder><mi>Ρ</mi></mstyle><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">|</mo><mi>y</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mi>log</mi><mfrac><mrow><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">|</mo><mi>y</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo></mrow><mrow><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo></mrow></mfrac><mo>-</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>i</mi></munder><mi>Ρ</mi></mstyle><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">|</mo><mi>y</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mi>log</mi><mfrac><mrow><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">|</mo><mi>y</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo></mrow><mrow><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">|</mo><mi>θ</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo></mrow></mfrac></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="144">因为当<i>P</i> (<i>X</i>|<i>θ</i><sub><i>j</i></sub>) =<i>P</i> (<i>X</i>|<i>y</i><sub><i>j</i></sub>) 时, 后一项为0, <i>I</i> (<i>X</i>;<i>θ</i><sub><i>j</i></sub>) 最大, 等于KL信息<i>I</i> (<i>X</i>; <i>y</i><sub><i>j</i></sub>) 。 两边除以<i>P</i> (<i>X</i>) 得到</p>
                </div>
                <div class="p1">
                    <p id="145" class="code-formula">
                        <mathml id="145"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mi>Τ</mi><mo stretchy="false"> (</mo><mi>θ</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">|</mo><mi>X</mi><mo stretchy="false">) </mo></mrow><mrow><mi>Τ</mi><mo stretchy="false"> (</mo><mi>θ</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo></mrow></mfrac><mo>=</mo><mfrac><mrow><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>y</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">|</mo><mi>X</mi><mo stretchy="false">) </mo></mrow><mrow><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>y</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo></mrow></mfrac></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="146">令<i>T</i> (<i>θ</i><sub><i>j</i></sub>|<i>X</i>) 的最大值等于1, 可以得到</p>
                </div>
                <div class="p1">
                    <p id="147" class="code-formula">
                        <mathml id="147"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Τ</mi><msup><mrow></mrow><mo>*</mo></msup><mo stretchy="false"> (</mo><mi>θ</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">|</mo><mi>X</mi><mo stretchy="false">) </mo><mo>=</mo><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>y</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">|</mo><mi>X</mi><mo stretchy="false">) </mo><mo>/</mo><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>y</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">|</mo><mi>x</mi><msubsup><mrow></mrow><mi>j</mi><mo>*</mo></msubsup><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>8</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="148">其中<i>x</i><sup>*</sup><sub><i>j</i></sub>是使<i>P</i> (<i>y</i><sub><i>j</i></sub>|<i>X</i>) 达最大的<i>x</i><sub><i>j</i></sub>.<i>P</i> (<i>y</i><sub><i>j</i></sub>|<i>X</i>) 通常很难求解, 但是有了样本分布<i>P</i> (<i>X</i>|<i>y</i><sub><i>j</i></sub>) 和<i>P</i> (<i>X</i>) , 求解<i>T</i> (<i>θ</i><sub><i>j</i></sub>|<i>X</i>) 反而比求解<i>P</i> (<i>y</i><sub><i>j</i></sub>|<i>X</i>) 容易, 因为通过贝叶斯公式可以得到:</p>
                </div>
                <div class="p1">
                    <p id="149" class="code-formula">
                        <mathml id="149"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Τ</mi><msup><mrow></mrow><mo>*</mo></msup><mo stretchy="false"> (</mo><mi>θ</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">|</mo><mi>X</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mrow><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>X</mi><mo stretchy="false">|</mo><mi>y</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mo>/</mo><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>X</mi><mo stretchy="false">) </mo></mrow><mrow><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>x</mi><msubsup><mrow></mrow><mi>j</mi><mo>*</mo></msubsup><mo stretchy="false">|</mo><mi>y</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mo>/</mo><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>x</mi><msubsup><mrow></mrow><mi>j</mi><mo>*</mo></msubsup><mo stretchy="false">) </mo></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>9</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="150">式 (13) 和 (17) 适合只有小样本时的参数估计; 而等式 (18) 和 (19) 适合有大样本时的非参数估计。从式 (18) 和 (3) 可见当样本足够大时, 用<i>T</i><sup>*</sup> (<i>θ</i><sub><i>j</i></sub>|<i>X</i>) 做语义贝叶斯预测和用<i>P</i> (<i>y</i><sub><i>j</i></sub>|<i>X</i>) 做传统的贝叶斯预测, 结果相同。</p>
                </div>
                <h4 class="anchor-tag" id="151" name="151"><b>3.3</b> 医学检验和<i>GPS</i>的语义信道优化</h4>
                <div class="p1">
                    <p id="152">我们先以医学检验为例说明真值函数和语义信道的优化。</p>
                </div>
                <div class="p1">
                    <p id="153">对于医学检验 (参看图3) , 这时<i>A</i>={<i>x</i><sub>0</sub>, <i>x</i><sub>1</sub>}, <i>B</i>={<i>y</i><sub>0</sub>, <i>y</i><sub>1</sub>}。其中<i>x</i><sub>0</sub>是真没病者, <i>x</i><sub>1</sub>是真有病者;<i>y</i><sub>0</sub>=检验呈阴性, <i>y</i><sub>1</sub>=检验呈阳性。<i>Z</i>是观察条件, 或因素值, <i>C</i>就是因素空间<citation id="253" type="reference"><link href="40" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>。{<i>C</i><sub>1</sub>, <i>C</i><sub>2</sub>}构成因素空间一个划分, 它和判决函数<i>Y</i>=<i>f</i> (<i>Z</i>) 相互确定。</p>
                </div>
                <div class="area_img" id="154">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MUTE201902009_154.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVjblNCVTU2UWRrTGIyZlBkcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 医学检验图解 (二元有噪声Shannon信道, 互信息随判决分界点z’改变)" src="Detail/GetImg?filename=images/MUTE201902009_154.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVjblNCVTU2UWRrTGIyZlBkcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 医学检验图解 (二元有噪声Shannon信道, 互信息随判决分界点<i>z</i>’改变)   <a class="btn-zoomin" href="Detail/GetImg?filename=images/MUTE201902009_154.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVjblNCVTU2UWRrTGIyZlBkcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="155">医学检验中把<i>x</i><sub>1</sub>检验为阳性的概率叫做敏感性 (sensitivity) , 把<i>x</i><sub>0</sub>检验为阴性的概率叫做特异性 (specificity) <citation id="254" type="reference"><link href="42" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>。检验的敏感性和特异性构成Shannon信道, 如表1所示。</p>
                </div>
                <div class="area_img" id="156">
                    <p class="img_tit"><b>表</b><b>1 医学检验的敏感性和特异性构成</b>Shannon<b>信道</b><i>P</i> (<i>Y</i>|<i>X</i>)  <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="156" border="1"><tr><td><br /></td><td>真有病<i>x</i><sub>1</sub></td><td>真没病<i>x</i><sub>0</sub></td></tr><tr><td><br />检验是阳性<i>y</i><sub>1</sub></td><td><i>P</i> (<i>y</i><sub>1</sub>|<i>x</i><sub>1</sub>) =敏感性</td><td><i>P</i> (<i>y</i><sub>1</sub>|<i>x</i><sub>0</sub>) =1-特异性</td></tr><tr><td><br />检验是阴性<i>y</i><sub>0</sub></td><td><i>P</i> (<i>y</i><sub>0</sub>|<i>x</i><sub>1</sub>) =1-敏感性</td><td><i>P</i> (<i>y</i><sub>0</sub>|<i>x</i><sub>0</sub>) =特异性</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="157">如果我们相信阳性表示绝对有病, 阴性表示绝对无病, 那么就有非模糊命题的真值<i>T</i> (<i>y</i><sub>1</sub>|<i>x</i><sub>1</sub>) =<i>T</i> (<i>y</i><sub>0</sub>|<i>x</i><sub>0</sub>) =1, <i>T</i> (<i>y</i><sub>1</sub>|<i>x</i><sub>0</sub>) = <i>T</i> (<i>y</i><sub>0</sub>|<i>x</i><sub>1</sub>) =0。但是采用这样的语义信道, 有一个反例存在, 就会有负无穷大信息。为此, 我们需要考虑预测和检验的可信度——用<i>b</i>表示, 并且用<i>b</i>′=1-|<i>b</i>|表示不信度。<i>y</i><sub><i>j</i></sub>的真值函数可定义为:</p>
                </div>
                <div class="p1">
                    <p id="158" class="code-formula">
                        <mathml id="158"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Τ</mi><mo stretchy="false"> (</mo><mi>θ</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">|</mo><mi>X</mi><mo stretchy="false">) </mo><mo>=</mo><mspace width="0.25em" /><msup><mi>b</mi><mo>′</mo></msup><mspace width="0.25em" /><mo>+</mo><mi>b</mi><mi>Τ</mi><mo stretchy="false"> (</mo><mi>y</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">|</mo><mi>X</mi><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mn>0</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="159">设阳性<i>y</i><sub>1</sub>的可信度<i>b</i><sub>1</sub>, 不信度是<i>b</i><sub>1</sub>′;阴性<i>y</i><sub>0</sub>的可信度是<i>b</i><sub>0</sub>, 不信度是<i>b</i><sub>0</sub>′.则医学检验的语义信道如表2所示。</p>
                </div>
                <div class="area_img" id="160">
                    <p class="img_tit">表2 医学检验的语义信道——含有两个不信度b<sub>1</sub>′和b<sub>0</sub>′ <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="160" border="1"><tr><td><br /></td><td>真有病<i>x</i><sub>1</sub></td><td>真没病<i>x</i><sub>0</sub></td></tr><tr><td><br />检验是阳性<i>y</i><sub>1</sub></td><td><i>T</i> (<i>θ</i><sub>1</sub>|<i>x</i><sub>1</sub>) =1</td><td><i>T</i> (<i>θ</i><sub>1</sub>|<i>x</i><sub>0</sub>) = <i>b</i><sub>1</sub>′</td></tr><tr><td><br />检验是阴性<i>y</i><sub>0</sub></td><td><i>T</i> (<i>θ</i><sub>0</sub>|<i>x</i><sub>1</sub>) = <i>b</i><sub>0</sub>′</td><td><i>T</i> (<i>θ</i><sub>0</sub>|<i>x</i><sub>0</sub>) =1</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="161">根据式 (18) , 两个优化的不信度是</p>
                </div>
                <div class="p1">
                    <p id="162" class="code-formula">
                        <mathml id="162"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>b</mi><msub><mrow></mrow><mn>1</mn></msub><mo>´</mo><msup><mrow></mrow><mo>*</mo></msup><mo>=</mo><mspace width="0.25em" /><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>y</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">|</mo><mi>x</mi><msub><mrow></mrow><mn>0</mn></msub><mo stretchy="false">) </mo><mo>/</mo><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>y</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">|</mo><mi>x</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi>b</mi><msub><mrow></mrow><mn>0</mn></msub><mo>´</mo><msup><mrow></mrow><mo>*</mo></msup><mo>=</mo><mspace width="0.25em" /><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>y</mi><msub><mrow></mrow><mn>0</mn></msub><mo stretchy="false">|</mo><mi>x</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">) </mo><mo>/</mo><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>y</mi><msub><mrow></mrow><mn>0</mn></msub><mo stretchy="false">|</mo><mi>x</mi><msub><mrow></mrow><mn>0</mn></msub><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mn>1</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="163">医学界用似然比<i>LR</i><sup>+</sup>和<i>LR</i><sup>-</sup>表示检验有多好<citation id="255" type="reference"><link href="42" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>。式 (21) 来自最大语义信息检验, 它和最大似然比检验是一一对应的, 因为:</p>
                </div>
                <div class="p1">
                    <p id="164" class="code-formula">
                        <mathml id="164"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>L</mi><mi>R</mi><msup><mrow></mrow><mo>+</mo></msup><mo>=</mo><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>y</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">|</mo><mi>x</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">) </mo><mo>/</mo><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>y</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">|</mo><mi>x</mi><msub><mrow></mrow><mn>0</mn></msub><mo stretchy="false">) </mo><mo>=</mo><mn>1</mn><mo>/</mo><mi>b</mi><msub><mrow></mrow><mn>1</mn></msub><mo>´</mo><msup><mrow></mrow><mo>*</mo></msup><mo>=</mo><mn>1</mn><mo>/</mo><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>b</mi><msubsup><mrow></mrow><mn>1</mn><mo>*</mo></msubsup><mo>*</mo><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi>L</mi><mi>R</mi><msup><mrow></mrow><mrow><mo>-</mo><mspace width="0.25em" /></mrow></msup><mo>=</mo><mspace width="0.25em" /><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>y</mi><msub><mrow></mrow><mn>0</mn></msub><mo stretchy="false">|</mo><mi>x</mi><msub><mrow></mrow><mn>0</mn></msub><mo stretchy="false">) </mo><mo>/</mo><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>y</mi><msub><mrow></mrow><mn>0</mn></msub><mo stretchy="false">|</mo><mi>x</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">) </mo><mo>=</mo><mn>1</mn><mo>/</mo><mi>b</mi><msub><mrow></mrow><mn>0</mn></msub><mo>´</mo><msup><mrow></mrow><mo>*</mo></msup><mo>=</mo><mn>1</mn><mo>/</mo><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>b</mi><msub><mrow></mrow><mn>0</mn></msub><mo>*</mo><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mn>2</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="165">其中<i>b</i><sup>*</sup><sub>1</sub>是优化的<i>y</i><sub>1</sub>的可信度, 在0和1之间变化, 和<i>LR</i><sup>+</sup>相比更加易于理解。<i>b</i><sup>*</sup><sub>0</sub>同理。</p>
                </div>
                <div class="p1">
                    <p id="166">对于GPS, 观察数据主要是GPS设备到三个卫星的距离。使用语义信息方法, 我们可以得到隶属函数——它可以消除来自其他因素的系统误差。假定GPS的Shannon信道是</p>
                </div>
                <div class="p1">
                    <p id="167" class="code-formula">
                        <mathml id="167"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>y</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">|</mo><mi>X</mi><mo stretchy="false">) </mo><mo>=</mo><mi>Κ</mi><mi>exp</mi><mo stretchy="false">[</mo><mo>-</mo><mo stretchy="false">|</mo><mi>X</mi><mo>-</mo><mi>x</mi><msub><mrow></mrow><mi>j</mi></msub><mo>-</mo><mi>Δ</mi><mi>x</mi><mo stretchy="false">|</mo><msup><mrow></mrow><mn>2</mn></msup><mo>/</mo><mo stretchy="false"> (</mo><mn>2</mn><mi>d</mi><msup><mrow></mrow><mn>2</mn></msup><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mo>, </mo><mtext> </mtext><mi>j</mi><mo>=</mo><mn>1</mn><mo>, </mo><mn>2</mn><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>n</mi><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="168">其中<i>x</i><sub><i>j</i></sub>是所指的位置, 即<i>y</i><sub><i>j</i></sub>=“<i>X</i>=<i>x</i><sub><i>j</i></sub>”中的<i>x</i><sub><i>j</i></sub>; <i>K</i>是一个常数, <i>Δx</i>是系统偏差, <i>d</i>是标准偏差, 那么根据式 (18) 就有优化的语义信道</p>
                </div>
                <div class="p1">
                    <p id="169" class="code-formula">
                        <mathml id="169"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Τ</mi><msup><mrow></mrow><mo>*</mo></msup><mo stretchy="false"> (</mo><mi>θ</mi><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false">|</mo><mi>X</mi><mo stretchy="false">) </mo><mo>=</mo><mi>exp</mi><mo stretchy="false">[</mo><mo>-</mo><mo stretchy="false">|</mo><mi>X</mi><mo>-</mo><mi>x</mi><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false">|</mo><msup><mrow></mrow><mn>2</mn></msup><mo>/</mo><mo stretchy="false"> (</mo><mn>2</mn><mi>d</mi><msup><mrow></mrow><mn>2</mn></msup><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mo>, </mo><mtext> </mtext><mi>k</mi><mo>=</mo><mn>1</mn><mo>, </mo><mn>2</mn><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>n</mi><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mn>4</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="170">其中<i>x</i><sub><i>k</i></sub>=<i>x</i><sub><i>j</i></sub>+<i>Δx</i>. 温度表, 秤, 指数预测…提供的语义信道优化类似。</p>
                </div>
                <h4 class="anchor-tag" id="171" name="171"><b>3.4</b> 从因素空间看模糊分类</h4>
                <div class="p1">
                    <p id="172">汪培庄提出的因素空间理论<citation id="256" type="reference"><link href="22" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>是知识表达和推理的合适框架, 其后续研究研究取得了许多有意义结果<citation id="257" type="reference"><link href="42" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>。但是在这些研究中, 背景分布不是概率分布, 隶属函数也没有和统计概率相联系。现在我们把背景分布改为因素空间客观事实的概率分布, 就能建立隶属函数和概率分布的联系。</p>
                </div>
                <div class="p1">
                    <p id="173">我们用颜色分类作为例子。因素空间就是边长为1的R-G-B立方空间, 一个颜色的因素值是一个三原色矢量 (r, g, b) 。矢量 (0, 0, 0) , (1, 0, 0) , (1, 1, 0) , (0, 1, 0) , (0, 1, 1) , (0, 0, 1) , (1, 0, 1) , (1, 1, 1) 表示典型的黑, 红, 黄, 绿, 青, 蓝, 绛, 白。假设所有可能的颜色在R-G-B空间的分布是<i>P</i> (<i>X</i>) , 给定<i>y</i><sub>1</sub>=“<i>X</i> 是红的”, <i>X</i>的条件分布是<i>P</i> (<i>X</i>|<i>y</i><sub>1</sub>) 。 如果样本足够大, 存在连续的<i>P</i> (<i>X</i>| <i>y</i><sub>1</sub>) 和 <i>P</i> (<i>X</i>) , 那么我们就可以利用式 (19) 得到优化的模糊集合{红色}的隶属函数。如果样本不足够大, 我们可用式 (17) 得到其参数形式。</p>
                </div>
                <div class="p1">
                    <p id="174">如果我们分类人群用模糊子集{小孩}, {幼儿}, {青年}, {成年}, {中年}…或者分类天气用模糊子集{无雨}, {小雨}, {中雨}, {中到大雨}, {大雨}, … , 我们可用类似方法得到隶属函数。这种方法并不要求这些子集构成论域<i>A</i>的一个划分, 这意味着, 我们可以允许{成年人}和{中年人}, 或者{中雨}和{中到大雨}同时存在。</p>
                </div>
                <h4 class="anchor-tag" id="175" name="175"><b>3.5</b> 用最大语义信息 (或最大标准似然度) 准则选择假设</h4>
                <div class="p1">
                    <p id="176">隶属函数优化好后, 我们需要根据<i>X</i>或<i>X</i>的条件概率分布<i>P</i> (<i>X</i>|<i>y</i><sub>?</sub>) 选择<i>Y</i>作为估计。如果只有一个样本点<i>X</i>=<i>x</i><sub><i>i</i></sub>, 那么我们把<i>x</i><sub><i>i</i></sub>和不同的<i>θ</i><sub><i>j</i></sub> 代入式 (12) 求<i>I</i> (<i>x</i><sub><i>i</i></sub>; <i>θ</i><sub><i>j</i></sub>) , 使<i>I</i> (<i>x</i><sub><i>i</i></sub>; <i>θ</i><sub><i>j</i></sub>) 达最大的<i>θ</i><sub><i>j</i></sub>记为<i>θ</i><sup>*</sup><sub><i>j</i></sub>, 相应的<i>θ</i><sup>*</sup><sub><i>j</i></sub>就是最大语义信息估计。给定<i>x</i><sub><i>i</i></sub>, 我们也可以求出隶属度或真值<i>T</i> (<i>θ</i><sub>1</sub>|<i>x</i><sub><i>i</i></sub>) , <i>T</i> (<i>θ</i><sub>2</sub>|<i>x</i><sub><i>i</i></sub>) , …, <i>T</i> (<i>θ</i><sub><i>n</i></sub>|<i>x</i><sub><i>i</i></sub>) 。和流行的MAP估计不同, 我们并不选择真值最大的<i>y</i><sub><i>j</i></sub>作为估计, 是因为真值函数覆盖范围大的假设逻辑概率大, 通常提供的信息量少<citation id="258" type="reference"><link href="34" rel="bibliography" /><link href="36" rel="bibliography" /><sup>[<a class="sup">16</a>,<a class="sup">17</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="177">如果样本分布是<i>P</i> (<i>X</i>|<i>y</i><sub>?</sub>) 或<i>P</i> (<i>X</i>|<i>z</i><sub><i>k</i></sub>) , 我们把它代入式 (13) , 改变<i>θ</i><sub><i>j</i></sub>, 求使广义KL信息<i>I</i> (<i>X</i>; <i>θ</i><sub><i>j</i></sub>) 达最大的<i>θ</i><sub><i>j</i></sub> , 记为<i>θ</i><sup>*</sup><sub><i>j</i></sub>, <i>θ</i><sup>*</sup><sub><i>j</i></sub>就是最大语义信息估计, 它和最大似然估计等价。</p>
                </div>
                <h4 class="anchor-tag" id="178" name="178"><b>3.6</b> 多标签逻辑分类和单标签选择分类</h4>
                <div class="p1">
                    <p id="179">从语义信息论角度看, 收信人分类和发信人分类是不同的。</p>
                </div>
                <div class="p1">
                    <p id="180">收信人通过样本得到语义信道, 这是逻辑分类即标签学习。 比如我们由天气预报历史数据得到带标签样本, 根据这些样本在降水量<i>X</i>上分类, 得到类别“有雨”, “无雨”, “小雨”, “小到中雨”…逻辑分类通常是多类别、多标签分类。利用式 (17) 和 (18) 就可以得到分类函数——即隶属函数或真值函数<i>T</i><sup>*</sup> (<i>θ</i><sub><i>j</i></sub>|<i>X</i>) , <i>j</i>=1, 2, … 这样的逻辑分类要比流行的多类别多标签学习简单很多<citation id="259" type="reference"><link href="46" rel="bibliography" /><sup>[<a class="sup">22</a>]</sup></citation>。另外我们还可以判别类别之间的蕴含关系。 比如对于所有<i>X</i>, <i>T</i><sup>*</sup> (<i>θ</i><sub>1</sub>|<i>X</i>) ≤<i>T</i><sup>*</sup> (<i>θ</i><sub>2</sub>|<i>X</i>) , 则<i>y</i><sub>1</sub>蕴含<i>y</i><sub>2</sub>, 集合<i>θ</i><sub>1</sub>是<i>θ</i><sub>2</sub>的子集。而发信人从多种假设即标签中选择一种发送出去, 比如气象台所做的。这是选择分类。选择分类通常是多类别单标签分类。对于不同的<i>x</i><sub><i>i</i></sub>, 把它划为哪个类别<i>y</i><sub><i>j</i></sub>就看相应的哪个<i>θ</i><sub><i>j</i></sub>使得<i>I</i> (<i>x</i><sub><i>i</i></sub>;<i>θ</i><sub><i>j</i></sub>) 达最大。根据式 (12) (计算<i>I</i> (<i>x</i><sub><i>i</i></sub>;<i>θ</i><sub><i>j</i></sub>) ) , 可以对<i>X</i>做一个划分, 这个划分就是单标签选择分类——不过这个单标签可能是一个复合标签, 比如“年轻男子”。</p>
                </div>
                <div class="p1">
                    <p id="181">上述逻辑分类和<i>P</i> (<i>X</i>) 无关, 而选择分类和<i>P</i> (<i>X</i>) 有关。两者都用语义信息准则或最大似然准则。到此, 我们可以得出结论, 用优化的隶属函数<i>T</i><sup>*</sup> (<i>θ</i><sub><i>j</i></sub>|<i>X</i>) 作为预测模型, 可以满足引言中的四项要求。</p>
                </div>
                <h4 class="anchor-tag" id="182" name="182"><b>3.7</b> 语义贝叶斯决策和翻译</h4>
                <div class="p1">
                    <p id="183">在图1所示GPS显示情况下, 我们需要判断小车具体在哪里?高速公路上, 普通道路上, 还是楼顶上。假设<i>y</i><sub>1</sub>=“小车在高速公路上”和<i>y</i><sub>2</sub>=“小车在普通路上”的真值函数是<i>T</i> (<i>θ</i><sub>1</sub>|<i>X</i>) 和<i>T</i> (<i>θ</i><sub>2</sub>|<i>X</i>) (覆盖范围更小) , 我们就可以把<i>I</i> (<i>x</i><sub><i>i</i></sub>; <i>θ</i><sub><i>k</i></sub>) , <i>k</i>=1, 2当做奖惩函数 (即负的损失函数, 用于贝叶斯决策) , 就得到<i>y</i><sub>1</sub>和<i>y</i><sub>2</sub>的平均语义信息:</p>
                </div>
                <div class="p1">
                    <p id="184" class="code-formula">
                        <mathml id="184"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ι</mi><mo stretchy="false"> (</mo><mi>X</mi><mo>;</mo><mi>θ</mi><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false">|</mo><mi>y</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>i</mi></munder><mi>Ρ</mi></mstyle><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">|</mo><mi>θ</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mi>log</mi><mfrac><mrow><mi>Τ</mi><mo stretchy="false"> (</mo><mi>θ</mi><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false">|</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo></mrow><mrow><mi>Τ</mi><mo stretchy="false"> (</mo><mi>θ</mi><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false">) </mo></mrow></mfrac><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>i</mi></munder><mi>Ρ</mi></mstyle><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">|</mo><mi>θ</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mi>log</mi><mfrac><mrow><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">|</mo><mi>θ</mi><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false">) </mo></mrow><mrow><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo></mrow></mfrac><mo>, </mo><mtext> </mtext><mi>j</mi><mo>=</mo><mn>1</mn><mo>, </mo><mn>2</mn><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mn>5</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="185">其中<i>P</i> (<i>x</i><sub><i>i</i></sub>|<i>θ</i><sub><i>j</i></sub>) 来自语义贝叶斯预测 (根据式 (1) ) 。语义贝叶斯决策在这里是选择使<i>I</i> (<i>X</i>;<i>θ</i><sub><i>k</i></sub>|<i>y</i><sub><i>j</i></sub>) 达最大的<i>y</i><sub><i>k</i></sub>.</p>
                </div>
                <div class="p1">
                    <p id="186">自然语言翻译是类似的。<i>y</i><sub><i>j</i></sub>就源语句, <i>y</i><sub><i>k</i></sub>, <i>k</i>=1, 2 就是目标语句。如果最为接近<i>P</i> (<i>X</i>|<i>θ</i><sub><i>j</i></sub>) 的是<i>P</i> (<i>X</i>|<i>θ</i><sub><i>k</i>*</sub>) , 则相应的<i>y</i><sub><i>k</i>*</sub>提供信息量最大, 就是最优翻译。</p>
                </div>
                <div class="p1">
                    <p id="187">以上讨论的Shannon信道都是给定的, 也就是说, 我们根据<i>Z</i>做出假设<i>y</i><sub><i>j</i></sub>=<i>f</i> (<i>Z</i>|<i>Z</i>∈<i>C</i><sub><i>j</i></sub>) 时, {<i>C</i><sub>1</sub>, <i>C</i><sub>2</sub>, …, <i>C</i><sub><i>n</i></sub>} (构成<i>C</i>的一个划分) 是给定的。下面考虑Shannon信道或<i>C</i>的划分可变时, 如何用迭代方法求出使平均对数似然度达最大的Shannon信道。</p>
                </div>
                <h3 id="188" name="188" class="anchor-tag">4 两种信道相互匹配的迭代算法 (信道匹配算法或<i>CM</i>算法) </h3>
                <h4 class="anchor-tag" id="189" name="189"><b>4.1</b> 用<i>CM</i>算法求检验, 估计和预测的最大互信息和最大似然度</h4>
                <div class="p1">
                    <p id="190">在语义互信息公式 (14) 中, 我们固定<i>P</i> (<i>Y</i>|<i>X</i>) , 改变<i>T</i> (<i>X</i>|Ѳ) , 最大化<i>I</i> (<i>X</i>;Ѳ) 。这一过程可谓“语义信道匹配Shannon信道” (匹配<i>I</i>) 。在<i>P</i> (<i>X</i>|<i>θ</i><sub><i>j</i></sub>) =<i>P</i> (<i>X</i>|<i>y</i><sub><i>j</i></sub>) 或<i>T</i> (<i>X</i>|Ѳ) ∝<i>P</i> (<i>Y</i>|<i>X</i>) 对所有<i>j</i>成立时, 语义互信息<i>I</i> (<i>X</i>; Ѳ) 达到其最大值, 并且等于其上限:Shannon互信息<i>I</i> (<i>X</i>;<i>Y</i>) 。但是反过来, 令<i>P</i> (<i>y</i><sub><i>j</i></sub>|<i>X</i>) ∝<i>T</i> (<i>θ</i><sub><i>j</i></sub>|<i>X</i>) (对所有<i>j</i>) 未必能增加Shannon互信息或语义互信息。给定语义信道, 可能存在更好的Shannon信道 (比如噪声更少) , 传递更多的语义信息。寻找这样Shannon信道可谓让Shannon信道匹配语义信道 (匹配II) 。</p>
                </div>
                <div class="p1">
                    <p id="191">鲁晨光曾推广Shannon的信息率失真函数<i>R</i> (<i>D</i>) 得到<i>R</i> (<i>G</i>) 函数<citation id="261" type="reference"><link href="24" rel="bibliography" /><link href="28" rel="bibliography" /><sup>[<a class="sup">11</a>,<a class="sup">13</a>]</sup></citation>, 其中<i>G</i>是语义互信息的下限, <i>R</i>是给定<i>G</i>时的最小Shannon互信息。 我们最近的研究表明:通过语义信道和Shannon 信道相互匹配和迭代可以找到使Shannon 互信息最大化的Shannon信道 (它决定了语义互信息和似然度的上限) , 从而求出使似然度达最大的语义信道。迭代收敛可以通过<i>R</i> (<i>G</i>) 函数得到证明 (参看<citation id="260" type="reference">[<a class="sup">25</a>]</citation>中图4) 。</p>
                </div>
                <div class="p1">
                    <p id="192">语义信道匹配Shannon方法如式 (17) 和 (18) 所示。Shannon匹配语义信道方法是, 令</p>
                </div>
                <div class="p1">
                    <p id="193" class="code-formula">
                        <mathml id="193"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>y</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">|</mo><mi>Ζ</mi><mo stretchy="false">) </mo><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>lim</mi></mrow></mstyle><mrow><mi>s</mi><mo>→</mo><mi>∞</mi></mrow></munder><mfrac><mrow><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>y</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mo stretchy="false">[</mo><mi>exp</mi><mo stretchy="false"> (</mo><mi>Ι</mi><mo stretchy="false"> (</mo><mi>X</mi><mo>;</mo><mi>θ</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">|</mo><mi>Ζ</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo stretchy="false">]</mo><msup><mrow></mrow><mi>s</mi></msup></mrow><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><msup><mi>j</mi><mo>′</mo></msup></munder><mi>Ρ</mi></mstyle><mo stretchy="false"> (</mo><mi>y</mi><msub><mrow></mrow><msup><mi>j</mi><mo>′</mo></msup></msub><mo stretchy="false">) </mo><mo stretchy="false">[</mo><mi>exp</mi><mo stretchy="false"> (</mo><mi>Ι</mi><mo stretchy="false"> (</mo><mi>X</mi><mo>;</mo><mi>θ</mi><msub><mrow></mrow><msup><mi>j</mi><mo>′</mo></msup></msub><mo stretchy="false">|</mo><mi>Ζ</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo stretchy="false">]</mo><msup><mrow></mrow><mi>s</mi></msup></mrow></mfrac><mo>, </mo><mtext> </mtext><mspace width="0.25em" /><mi>j</mi><mo>=</mo><mn>1</mn><mo>, </mo><mn>2</mn><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>n</mi><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mn>6</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="194">其中<i>I</i> (<i>X</i>;<mathml id="195"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>θ</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">|</mo><mi>Ζ</mi><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>i</mi></munder><mi>Ρ</mi></mstyle><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">|</mo><mi>Ζ</mi><mo stretchy="false">) </mo><mi>Ι</mi><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></math></mathml>;<i>θ</i><sub><i>j</i></sub>) , <i>j</i>=1, 2, …, <i>n</i>.当<i>s</i>→∞, <i>P</i> (<i>y</i><sub><i>j</i></sub>|<i>Z</i>) 变成集合<i>C</i><sub><i>j</i></sub>的特征函数, 取值于{0, 1}。上面公式的直观解释就是, 给定<i>Z</i>, 哪个<i>Y</i>使<i>I</i> (<i>X</i>;<i>θ</i><sub><i>j</i></sub>|<i>Z</i>) 最大, 我们就把<i>Z</i>划到相应的<i>C</i><sub><i>j</i></sub>, 做出假设<i>y</i><sub><i>j</i></sub>=<i>f</i> (<i>Z</i>|<i>Z</i>∈<i>C</i><sub><i>j</i></sub>) 。</p>
                </div>
                <div class="p1">
                    <p id="196">流行的求解最大互信息和最大似然度方法是梯度下降法、牛顿法<citation id="262" type="reference"><link href="44" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>、EM算法<citation id="263" type="reference"><link href="48" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>和最小最大法<citation id="264" type="reference"><link href="50" rel="bibliography" /><sup>[<a class="sup">24</a>]</sup></citation>。和这些方法相比, 计算实验表明, 上述算法 (信道匹配算法或CM算法) 看起来更加快速, 收敛更加可靠。</p>
                </div>
                <div class="p1">
                    <p id="197">对于图3所示检验, 优化Shannon信道就是优化判决分界点<i>z</i>’, 当<i>Z</i>&gt;<i>z</i>’时, 我们判断<i>Y</i>=<i>y</i><sub>1</sub>=阳性, 否则判断<i>Y</i>=<i>y</i><sub>0</sub>=阴性。</p>
                </div>
                <div class="p1">
                    <p id="198">举例说, 假定<i>Z</i>∈<i>C</i>={1, 2, …, 100}, 给定<i>x</i><sub>1</sub>和<i>x</i><sub>0</sub>时, <i>P</i> (<i>Z</i>|<i>X</i>) 是高斯分布:</p>
                </div>
                <div class="p1">
                    <p id="199" class="code-formula">
                        <mathml id="199"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>Ζ</mi><mo stretchy="false">|</mo><mi>x</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">) </mo><mo>=</mo><mi>Κ</mi><msub><mrow></mrow><mn>1</mn></msub><mi>exp</mi><mo stretchy="false">[</mo><mo>-</mo><mo stretchy="false"> (</mo><mi>Ζ</mi><mo>-</mo><mi>c</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">) </mo><msup><mrow></mrow><mn>2</mn></msup><mo>/</mo><mo stretchy="false"> (</mo><mn>2</mn><mi>d</mi><msubsup><mrow></mrow><mn>1</mn><mn>2</mn></msubsup><mo stretchy="false">) </mo><mo stretchy="false">]</mo></mtd></mtr><mtr><mtd><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>Ζ</mi><mo stretchy="false">|</mo><mi>x</mi><msub><mrow></mrow><mn>0</mn></msub><mo stretchy="false">) </mo><mo>=</mo><mi>Κ</mi><msub><mrow></mrow><mn>0</mn></msub><mi>exp</mi><mo stretchy="false">[</mo><mo>-</mo><mo stretchy="false"> (</mo><mi>Ζ</mi><mo>-</mo><mi>c</mi><msub><mrow></mrow><mn>0</mn></msub><mo stretchy="false">) </mo><msup><mrow></mrow><mn>2</mn></msup><mo>/</mo><mo stretchy="false"> (</mo><mn>2</mn><mi>d</mi><msubsup><mrow></mrow><mn>0</mn><mn>2</mn></msubsup><mo stretchy="false">) </mo><mo stretchy="false">]</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="200">其中<i>K</i><sub>1</sub> 和<i>K</i><sub>0</sub>是归一化常数。从<i>P</i> (<i>X</i>) 和<i>P</i> (<i>Z</i>|<i>X</i>) , 可以算出样本分布<i>P</i> (<i>X</i>|<i>Z</i>) 。假定开始的划分点是<i>z</i>′, 比方说<i>z</i>′=50, 做下面迭代运算:</p>
                </div>
                <div class="p1">
                    <p id="201">匹配I:计算下面各项</p>
                </div>
                <div class="p1">
                    <p id="202">构成Shannon信道的4个转移概率, 两个不信度<i>b</i><sub>1</sub>′<sup>*</sup>和<i>b</i><sub>0</sub>′<sup>*</sup> (根据式 (18) ) 和两个逻辑概率<i>T</i> (<i>θ</i><sub>1</sub>) 和<i>T</i> (<i>θ</i><sub>2</sub>) (根据式 (9) ) ;</p>
                </div>
                <div class="p1">
                    <p id="203">四个信息量<i>I</i><sub><i>ij</i></sub>=<i>I</i> (<i>x</i><sub><i>i</i></sub>;<i>θ</i><sub><i>j</i></sub>) (根据式 (12) 计算) , <i>i</i>=0, 1; <i>j</i>=0, 1;</p>
                </div>
                <div class="p1">
                    <p id="204">给定不同<i>Z</i>时的平均语义信息<i>I</i> (<i>X</i>;<i>θ</i><sub>1</sub>|<i>Z</i>) 和<i>I</i> (<i>X</i>;<i>θ</i><sub>0</sub>|<i>Z</i>) (显示为两条曲线) :</p>
                </div>
                <div class="p1">
                    <p id="205" class="code-formula">
                        <mathml id="205"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ι</mi><mo stretchy="false"> (</mo><mi>X</mi><mo>;</mo><mi>θ</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">|</mo><mi>z</mi><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>i</mi></munder><mi>Ρ</mi></mstyle><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">|</mo><mi>z</mi><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false">) </mo><mi>Ι</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>, </mo><mtext> </mtext><mi>j</mi><mo>=</mo><mn>0</mn><mo>, </mo><mn>1</mn><mo>;</mo><mspace width="0.25em" /><mi>k</mi><mo>=</mo><mn>1</mn><mo>, </mo><mn>2</mn><mo>, </mo><mo>⋯</mo><mo>, </mo><mn>1</mn><mn>0</mn><mn>0</mn><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mn>7</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="206">匹配II:利用式 (26) 改进<i>C</i>的划分。如果划分点和上个<i>z</i>’相同, 则令最优分界点<i>z</i><sup>*</sup>=<i>z</i>′, 迭代结束;否则转到匹配I.</p>
                </div>
                <div class="p1">
                    <p id="207"><b>例</b><b>4.1</b> 一个2×2 Shannon信道, <i>P</i> (<i>x</i><sub>0</sub>) =0.8; <i>c</i><sub>0</sub>=30, <i>c</i><sub>1</sub>=70; <i>d</i><sub>0</sub>=15, <i>d</i><sub>1</sub>=10。设起始<i>z</i>′=50。一次迭代后<i>z</i>′=53; 两次迭代后<i>z</i>′=54; 三次迭代后得到收敛点<i>z</i><sup>*</sup>=54。</p>
                </div>
                <div class="p1">
                    <p id="208"><b>例</b><b>4.2</b> 一个3×3 Shannon信道用作简化的估计模型 (和检验不同, 迭代方法同理) , <i>P</i> (<i>x</i><sub>0</sub>) =0.5, <i>P</i> (<i>x</i><sub>1</sub>) =0.35, <i>P</i> (<i>x</i><sub>2</sub>) =0.15; <i>c</i><sub>0</sub>=20, <i>c</i><sub>1</sub>=50, <i>c</i><sub>2</sub>=80; <i>d</i><sub>0</sub>=15, <i>d</i><sub>1</sub>=10, <i>d</i><sub>2</sub>=10。</p>
                </div>
                <div class="p1">
                    <p id="209"> (a) 用两个起始分界点<i>z</i><sub>1</sub>′=50和<i>z</i><sub>2</sub>′=60, 迭代4次收敛, 得到<i>z</i><sup>*</sup><sub>1</sub>=35和<i>z</i><sup>*</sup><sub>2</sub>=66。</p>
                </div>
                <div class="p1">
                    <p id="210"> (b) 用一对很差的起点<i>z</i><sub>1</sub>′=9和<i>z</i><sub>2</sub>′=20, 迭代11次收敛, 同样得到<i>z</i><sup>*</sup><sub>1</sub>=35和<i>z</i><sup>*</sup><sub>2</sub>=66。迭代前后的三条信息曲线如图4所示。可见迭代收敛可靠。</p>
                </div>
                <div class="p1">
                    <p id="211">由这两个例子可见, 用CM算法求解信道不确定时的最大似然检验和估计, 快速且可靠。</p>
                </div>
                <div class="p1">
                    <p id="212">我们可以把CM算法用到一般预测, 比如天气预报。这时候就可以用CM算法解释语义进化。Shannon信道反映语言用法, 而语义信道反映听众理解方式。语义信道匹配Shannon信道 (匹配I) 就是理解匹配用法;Shannon信道匹配语义信道 (匹配II) 就是用法匹配理解。语义信道和Shannon 信道相互匹配和迭代, 就是演讲者用法和听众理解相互匹配, 相互促进。自然语言应该就是通过这种方式进化的。</p>
                </div>
                <div class="area_img" id="215">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MUTE201902009_21500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVjblNCVTU2UWRrTGIyZlBkcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 分界起点很差时的迭代" src="Detail/GetImg?filename=images/MUTE201902009_21500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVjblNCVTU2UWRrTGIyZlBkcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 分界起点很差时的迭代  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MUTE201902009_21500.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVjblNCVTU2UWRrTGIyZlBkcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="216" name="216"><b>4.2</b> CM算法用于混合模型</h4>
                <div class="p1">
                    <p id="217">CM算法和EM算法<citation id="265" type="reference"><link href="48" rel="bibliography" /><sup>[<a class="sup">23</a>]</sup></citation>类似, 也可以用于求解混合模型。假设样本分布<i>P</i> (<i>X</i>) 是某种分布 (比如高斯分布) 条件概率函数<i>P</i><sup>*</sup> (<i>X</i>|<i>Y</i>) 和<i>P</i><sup>*</sup> (<i>Y</i>) 产生的。我们只知道模型构件是<i>n</i>个, 并不知道<i>P</i><sup>*</sup> (<i>Y</i>) 和模型参数。要求的是<i>P</i><sup>*</sup> (<i>Y</i>) 和模型参数<i>Θ</i>.和检验不同, 预测不再有对错, 但是要求预测的样本分布——记为<i>Q</i> (<i>X</i>) ——和<i>P</i> (<i>X</i>) 尽可能接近, 即相对熵<i>H</i> (<i>P</i>‖<i>Q</i>) 尽可能小。根据语义信息理论, 我们得到下面重要公式<citation id="266" type="reference"><link href="54" rel="bibliography" /><sup>[<a class="sup">26</a>]</sup></citation><sup>:</sup></p>
                </div>
                <div class="p1">
                    <p id="218" class="code-formula">
                        <mathml id="218"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Η</mi><mo stretchy="false"> (</mo><mi>Ρ</mi><mo stretchy="false">∥</mo><mi>Q</mi><mo stretchy="false">) </mo><mo>=</mo><mi>R</mi><mo>+</mo><mi>Η</mi><mo stretchy="false"> (</mo><mi>Y</mi><msup><mrow></mrow><mrow><mo>+</mo><mn>1</mn></mrow></msup><mo stretchy="false">∥</mo><mi>Y</mi><mo stretchy="false">) </mo><mo>-</mo><mi>G</mi><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mn>8</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="219">其中<i>G</i>是语义互信息, <i>R</i>是Shannon互信息, <mathml id="220"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Η</mi><mo stretchy="false"> (</mo><mi>Y</mi><msup><mrow></mrow><mrow><mo>+</mo><mn>1</mn></mrow></msup><mo stretchy="false">∥</mo><mi>Y</mi><mo stretchy="false">) </mo><mo>=</mo><mo>-</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>j</mi></munder><mi>Ρ</mi></mstyle><msup><mrow></mrow><mrow><mo>+</mo><mn>1</mn></mrow></msup><mo stretchy="false"> (</mo><mi>y</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mrow><mi>log</mi></mrow><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>y</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo></mrow></math></mathml>;其中<i>P</i> (<i>y</i><sub><i>j</i></sub>) 是猜测的<i>y</i><sub><i>j</i></sub>的概率, 而<i>P</i><sup>+1</sup> (<i>y</i><sub><i>j</i></sub>) 是根据<i>P</i> (<i>X</i>) 和<i>P</i> (<i>y</i><sub><i>j</i></sub>|<i>X</i>) 计算出的<i>y</i><sub><i>j</i></sub>的概率。</p>
                </div>
                <div class="p1">
                    <p id="221">我们以两个高斯分布函数的混合为例做了迭代运算实验。迭代之前初始化<i>P</i> (<i>Y</i>) (比如假设等概率) , 初始化两个高斯分布函数 (<i>P</i> (<i>X</i>|<i>θ</i><sub><i>j</i></sub>) =<i>k</i><sub><i>j</i></sub>exp[- (<i>X</i>-<i>c</i><sub><i>j</i></sub>) <sup>2</sup>/ (2<i>d</i><sub><i>j</i></sub>) <sup>2</sup>], <i>j</i>=1, 2, …, <i>n</i>) 中的四个参数 (<i>c</i><sub>1</sub>, <i>c</i><sub>2</sub>, <i>d</i><sub>1</sub>, <i>d</i><sub>2</sub>) 。 然后开始迭代运算。每次迭代分为三步<citation id="267" type="reference"><link href="54" rel="bibliography" /><sup>[<a class="sup">26</a>]</sup></citation><sup>:</sup></p>
                </div>
                <div class="p1">
                    <p id="222">匹配II a:令转移概率函数是通过似然函数<i>P</i> (<i>X</i>|<i>Θ</i>) 和<i>P</i> (<i>Y</i>) 产生的, 即</p>
                </div>
                <div class="p1">
                    <p id="223" class="code-formula">
                        <mathml id="223"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>y</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">|</mo><mi>X</mi><mo stretchy="false">) </mo><mo>=</mo><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>y</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>X</mi><mo stretchy="false">|</mo><mi>θ</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mo>/</mo><mi>Q</mi><mo stretchy="false"> (</mo><mi>X</mi><mo stretchy="false">) </mo><mo>, </mo><mspace width="0.25em" /><mi>Q</mi><mo stretchy="false"> (</mo><mi>X</mi><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>j</mi></munder><mi>Ρ</mi></mstyle><mo stretchy="false"> (</mo><mi>y</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mo stretchy="false">|</mo><mo stretchy="false"> (</mo><mi>X</mi><mo stretchy="false">|</mo><mi>θ</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mo>, </mo><mtext> </mtext><mi>j</mi><mo>=</mo><mn>1</mn><mo>, </mo><mn>2</mn><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>n</mi></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="224">匹配II b:改变<i>P</i> (<i>y</i><sub><i>j</i></sub>) , 使得<mathml id="225"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><msup><mrow></mrow><mrow><mo>+</mo><mn>1</mn></mrow></msup><mo stretchy="false"> (</mo><mi>y</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>i</mi></munder><mi>Ρ</mi></mstyle><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>y</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">|</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>≈</mo><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>y</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo></mrow></math></mathml>, 即<i>H</i> (<i>Y</i>‖<i>Y</i><sup>+1</sup>) 接近0。如果<i>H</i> (<i>P</i>‖<i>Q</i>) &lt;0.001则迭代结束。</p>
                </div>
                <div class="p1">
                    <p id="226">匹配I:改变四个参数最大化<i>G</i>. 然后转到匹配II a.</p>
                </div>
                <div class="p1">
                    <p id="227">表3显示了一个迭代实例。其中<i>R</i><sup>*</sup>是真实的Shannon互信息, <i>G</i><sup>*</sup>是与之相等语义互信息。 </p>
                </div>
                <div class="area_img" id="228">
                    <p class="img_tit">表3 R&lt;R*时的模型参数和迭代结果 (迭代次数是 5)  <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="228" border="1"><tr><td><i>Y</i></td><td colspan="3">真实<i>P</i><sup>*</sup> (<i>X</i>|<i>Y</i>) 和<i>P</i><sup>*</sup> (<i>Y</i>) </td><td colspan="3">初始参数<br /><i>H</i> (<i>P</i>‖<i>Q</i>) =0.410bit</td><td colspan="3">收敛后参数<br /><i>H</i> (<i>P</i>‖<i>Q</i>) =0.00088bit</td></tr><tr><td></td><td><i>c</i></td><td><i>d</i></td><td><i>P</i><sup>*</sup> (<i>Y</i>) </td><td><i>c</i></td><td><i>d</i></td><td><i>P</i> (<i>Y</i>) </td><td><i>c</i></td><td><i>d</i></td><td><i>P</i> (<i>Y</i>) </td></tr><tr><td><br /><i>y</i><sub>1</sub></td><td>35</td><td>8</td><td>0.7</td><td>30</td><td>15</td><td>0.5</td><td>35.4</td><td>8.3</td><td>0.720</td></tr><tr><td><br /><i>y</i><sub>2</sub></td><td>65</td><td>12</td><td>0.3</td><td>70</td><td>10</td><td>0.5</td><td>66.2</td><td>11.4</td><td>0.280</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="229">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MUTE201902009_229.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVjblNCVTU2UWRrTGIyZlBkcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 R&lt;R*时的迭代过程。R和G不断增加并逐渐接近G*, 使得H (P‖Q) 接近0。" src="Detail/GetImg?filename=images/MUTE201902009_229.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVjblNCVTU2UWRrTGIyZlBkcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 <i>R</i>&lt;<i>R</i>*时的迭代过程。<i>R</i>和<i>G</i>不断增加并逐渐接近<i>G</i>*, 使得<i>H</i> (<i>P</i>‖<i>Q</i>) 接近0。  <a class="btn-zoomin" href="Detail/GetImg?filename=images/MUTE201902009_229.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVjblNCVTU2UWRrTGIyZlBkcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="230">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/MUTE201902009_230.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVjblNCVTU2UWRrTGIyZlBkcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 样本概率分布P (X) 和模型预测的概率分布Q (X) (5次迭代后)" src="Detail/GetImg?filename=images/MUTE201902009_230.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVjblNCVTU2UWRrTGIyZlBkcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 样本概率分布<i>P</i> (<i>X</i>) 和模型预测的概率分布<i>Q</i> (<i>X</i>) (5次迭代后)   <a class="btn-zoomin" href="Detail/GetImg?filename=images/MUTE201902009_230.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVjblNCVTU2UWRrTGIyZlBkcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="231">我们用几组不同真实参数和<i>P</i><sup>*</sup> (<i>Y</i>) 检验了CM算法。 结果表明, CM算法迭代次数大多在5～10次, 而EM算法收敛大多在17次左右<citation id="268" type="reference"><link href="54" rel="bibliography" /><sup>[<a class="sup">26</a>]</sup></citation>。我们还找到迭代过程中<i>R</i>和<i>G</i>并不是单调增加的例子, 这时式 (28) 依然成立, 迭代收敛依然可靠。而这些例子是对EM算法及其收敛证明的严峻挑战<citation id="269" type="reference"><link href="54" rel="bibliography" /><sup>[<a class="sup">26</a>]</sup></citation>。</p>
                </div>
                <h3 id="232" name="232" class="anchor-tag">5 总结</h3>
                <div class="p1">
                    <p id="233">本文首先从<i>Shannon</i>信道的角度讨论了如何利用转移概率函数改进有大样本时的概率预测, 使之适合于信源可变场合。然后讨论, 对于样本不足够大时, 我们可以用<i>Zadeh</i>的隶属函数作为预测模型, 使得:模型反映语义; 最大语义信息估计兼容最大似然估计; 模型适合信源可变场合; 并且使得语义贝叶斯预测兼容传统的贝叶斯预测。文中介绍了如何优化语义信道, 简化多类别多标签分类, 以及在<i>Shannon</i>信道不确定时 (即标签不确定时) , 通过<i>CM</i>迭代算法求解具有最大互信息和最大似然度的检验和估计, 和求解具有最小相对熵的混合模型。研究显示, 和流行的方法比, <i>CM</i>算法速度看来更快, 收敛更可靠。总之, <i>Zadeh</i>的模糊集合和隶属函数对于改进假设检验、语义通信和统计学习有重要意义。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="4">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fuzzy Set">

                                <b>[1]</b> Zadeh L A.Fuzzy sets[J].Information and Control, 1965, 8 (3) :338～353.
                            </a>
                        </p>
                        <p id="6">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011601171954&amp;v=MzExMjJud1plWnVIeWptVUxySUpWb1VhaFk9TmlmT2ZiSzdIdEROcVk5RVpld09CWGs5b0JNVDZUNFBRSC9pclJkR2VycVFUTQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVjblNCVTU2UWRrTGIyZlBkcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> Dubois D, Moral S, Prade H.A semantics for possibility theory based on likelihoods[J].Journal of Mathematical Analysis and Applications, 1997, 205 (20) :359～380.
                            </a>
                        </p>
                        <p id="8">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESD28EA408629E19534DC50500BBD237D8&amp;v=MDIyODRwYlEzNU4xaHdieS93S3c9TmlmT2ZjZTZGcVM5cTQ5Tll1a0dlWDB3eWhVWG5reDRTSHJpckdCSERiQ1hRczZYQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQw==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVjblNCVTU2UWRrTGIyZlBkcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> Cattaneo M E G V.The likelihood interpretation as the foundation of fuzzy set theory[J].International Journal of Approximate Reasoning, Available online 22 August 2017.
                            </a>
                        </p>
                        <p id="10">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fuzzy Bayesian inference">

                                <b>[4]</b> Yang C C.Fuzzy Bayesian inference[C]//Computational Cybernetics and Simulation, IEEE International Conference on Systems, Man, and Cybernetics Conference Proceedings, Orlando, 1997, Vol.3:2707～2712.
                            </a>
                        </p>
                        <p id="12">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Foundations of fuzzy bayesian inference">

                                <b>[5]</b> Viertl R.Foundations of fuzzy bayesian inference[J].Journal of Uncertain Systems, 2008, 2 (3) :187～191.
                            </a>
                        </p>
                        <p id="14">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Possibilistic uncertainty and statistical inference">

                                <b>[6]</b> Thomas S F.Possibilistic uncertainty and statistical inference[C]//ORSA/TIMS Meeting, Houston, 1981.
                            </a>
                        </p>
                        <p id="16">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MUTE199101011&amp;v=MjE3MDllUnJGeTNoVTcvTUtEamZhN0t4RjlETXJvOUVaWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVo=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVjblNCVTU2UWRrTGIyZlBkcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b> 鲁晨光.B-模糊集合代数和广义交互熵公式[J].模糊系统和数学, 1991, 5 (1) :76～80.
                            </a>
                        </p>
                        <p id="18">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Probability measures of fuzzy events">

                                <b>[8]</b> Zadeh L A.Probability measures of fuzzy events[J].Journal of Mathematical Analysis and Applications, 1968, 23 (2) :421～427.
                            </a>
                        </p>
                        <p id="20">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002352349&amp;v=MjY1NzNJcEhaKzhHWTNrNXpCZGg0ajk5U1hxUnJ4b3hjTUg3UjdxZWJ1ZHRGQzNsVnJ2SUlGbz1OajdCYXJPNEh0SE9y&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVjblNCVTU2UWRrTGIyZlBkcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b> Davidson D.Truth and meaning[J].Synthese, 1967, 17:304～323.
                            </a>
                        </p>
                        <p id="22">
                            <a id="bibliography_10" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CBBD&amp;filename=0000000000348776&amp;v=MjM5NTFCYk93SUNoTTh6eFVTbURkOVNIN24zeEU5ZmJ2bktyaWZadTl1RkN2Z1U3ck1JRjRRVlYyN0hyTzRIdEhNcjR4&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVjblNCVTU2UWRrTGIyZlBkcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[10]</b> 汪培庄.模糊集和随机集落影[M].北京师范大学出版社, 1985.
                            </a>
                        </p>
                        <p id="24">
                            <a id="bibliography_11" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CBBD&amp;filename=0007312005012999&amp;v=MzA3MTFnVTdyTUlGNFFWVjI3R2JDNUhOSE1xbzlFWnVJR0JSTTh6eFVTbURkOVNIN24zeEU5ZmJ2bktyaWZadTl1RkN2&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVjblNCVTU2UWRrTGIyZlBkcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[11]</b> 鲁晨光.广义信息论[M].中国科学技术大学出版社, 1993.
                            </a>
                        </p>
                        <p id="26">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A mathematical theory of communication">

                                <b>[12]</b> Shannon C E.A mathematical theory of communication[J].Bell System Technical Journal, 1948, 27 (3) :379～429;623～656.
                            </a>
                        </p>
                        <p id="28">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Generalization of Shannon&amp;#39;s information theory">

                                <b>[13]</b> Lu C.A generalization of Shannon’s information theory[J].Int.J.of General Systems, 1999, 28 (6) :453～490.
                            </a>
                        </p>
                        <p id="30">
                            <a id="bibliography_14" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJRS&amp;filename=SJRS121022008962&amp;v=MjQwNDNuS3JpZlp1OXVGQ3ZnVTdyTUlGNFFOaWZaZmJLNkg5SE9yWTlGYk9JSkRoTTh6eFVTbURkOVNIN24zeEU5ZmJ2&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVjblNCVTU2UWRrTGIyZlBkcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[14]</b> Fisher R A.On the mathematical foundations of theoretical statistics[J].Philo.Trans.Roy.Soc., 1922, A222:309～368.
                            </a>
                        </p>
                        <p id="32">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Elements of information theory">

                                <b>[15]</b> Cover T M, Thomas J A.Elements of information theory[M] 2nd Edition.New York:John Wiley &amp; Sons, 2006.
                            </a>
                        </p>
                        <p id="34">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Conjectures and refutations">

                                <b>[16]</b> Popper K.Conjectures and refutations[M].Repr.London and New York:Routledge, 1963/2005.
                            </a>
                        </p>
                        <p id="36">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=An outline of a theory of semantic information">

                                <b>[17]</b> Carnap R, Bar-Hillel Y.An outline of a theory of semantic information[R].Tech.Rep.No.247, Research Lab.of Electronics, MIT, 1952.
                            </a>
                        </p>
                        <p id="38">
                            <a id="bibliography_18" >
                                    <b>[18]</b>
                                 Akaike H.A new look at the statistical model identification[J].IEEE Transactions on Automatic Control, 1974, 19:716～723.
                            </a>
                        </p>
                        <p id="40">
                            <a id="bibliography_19" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=FXKY201502028&amp;v=MjE1MDFIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVJyRnkzaFU3L01JelhBZDdHNEg5VE1yWTlIYklRS0Q=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVjblNCVTU2UWRrTGIyZlBkcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[19]</b> 汪培庄.因素空间和数据科学[J].辽宁工程技术大学学报, 2015, 34 (2) :273～280.
                            </a>
                        </p>
                        <p id="42">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Likelihood ratios as a measure of the diagnostic usefulness of excretory urogram information">

                                <b>[20]</b> Thornbury J R, Fryback D G, Edwards W.Likelihood ratios as a measure of the diagnostic usefulness of excretory urogram information[J].Radiology, 1975, 114 (3) :561～565.
                            </a>
                        </p>
                        <p id="44">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Newton-based maximum likelihood estimation in nonlinear state space models">

                                <b>[21]</b> Kok M, Dahlin J, Schon B, Wills T B.A Newton-based maximum likelihood estimation in nonlinear state space models[OL].IFAC-PapersOnLine, 2015, 48:398～403.
                            </a>
                        </p>
                        <p id="46">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A review on multi-label learning algorithms">

                                <b>[22]</b> Zhang M L, Zhou Z H.A review on multilabel learning algorithms[J].IEEE Trans.Knowledge and Data Engineering, 2014, 26 (8) :1819～1837.
                            </a>
                        </p>
                        <p id="48">
                            <a id="bibliography_23" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJST&amp;filename=SJST14120603923799&amp;v=MjU1MTNZOUdiZWtNQzNVd29CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVUxySUpWb1VhaFk9TmlmWWVySzhIOVBNcQ==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVjblNCVTU2UWRrTGIyZlBkcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[23]</b> Dempster A P, Laird N M, Rubin D B.Maximum likelihood from incomplete data via the EM algorithm[J].Journal of the Royal Statistical Society, Series B, 1977, 39 (1) :1～38.
                            </a>
                        </p>
                        <p id="50">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Bayesian properties of normalized maximum likelihood and its fast computation">

                                <b>[24]</b> Barron A, Roos T, Watanabe K.Bayesian properties of normalized maximum likelihood and its fast computation[C]//IEEE IT Symposium on Information Theory, 2014:1667～1671.
                            </a>
                        </p>
                        <p id="52">
                            <a id="bibliography_25" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Semantic channel and Shannon channel mutually match and iterate for tests and estimations with maximum mutual information and maximum likelihood">

                                <b>[25]</b> Lu C.Semantic channel and Shannon channel mutually match and iterate for tests and estimations with maximum mutual information and maximum likelihood[C]//Proceedings of International Conference on Big Data and Smart Computing, Shanghai, 2018.
                            </a>
                        </p>
                        <p id="54">
                            <a id="bibliography_26" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Channels&amp;#39;&amp;#39; matching algorithm for mixture models">

                                <b>[26]</b> Lu C.Channels’ matching algorithm for mixture models[C]//Proceedings of International Conference on Intelligence Science, Shanghai, 2017.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
            <div class="reference anchor-tag" id="a_footnote">
 <h3>注释</h3>
                    <p>
                        <span id="2" href="javascript:void(0)">
                            <b>1</b> <i>https</i>://<i>en</i>.<i>wikipedia</i>.<i>org</i>/<i>wiki</i>/<i>Bayesian</i>_<i>inference</i>.
                        </span>
                    </p>
            </div>
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="MUTE201902009" />
        <input id="dpi" type="hidden" value="240" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVjblNCVTU2UWRrTGIyZlBkcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MUTE201902009&amp;v=MzEzMDFyQ1VSTE9lWmVSckZ5M2hVNy9NS0RqZmE3RzRIOWpNclk5RmJZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0Y=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVDYzUnVjblNCVTU2UWRrTGIyZlBkcz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
