<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=NOOK-mvTMnxl11s6oPYV0boPXN3iP4N7Rc-A56nk4KI1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2fKXReader%2fDetail%3fTIMESTAMP%3d637141790583881250%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJZ201905056%26RESULT%3d1%26SIGN%3d1U1ZyE2c2LU%252fwWxMcq4C6S02Y60%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJZ201905056&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJZ201905056&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJZ201905056&amp;v=MDk0NDdUNmo1NE8zenFxQnRHRnJDVVI3cWZadVptRnl6a1ZyL0JMejdCZExHNEg5ak1xbzlBWW9RS0RIODR2UjQ=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZDMVR6N0NyWFRURWhvQ21Icm4vdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#35" data-title="&lt;b&gt;1 引言&lt;/b&gt; "><b>1 引言</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#42" data-title="&lt;b&gt;2 语音分离系统&lt;/b&gt; "><b>2 语音分离系统</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#47" data-title="&lt;b&gt;3 CNN模型&lt;/b&gt; "><b>3 CNN模型</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#48" data-title="&lt;b&gt;3.1 模型优化&lt;/b&gt;"><b>3.1 模型优化</b></a></li>
                                                <li><a href="#60" data-title="&lt;b&gt;3.2 实现过程&lt;/b&gt;"><b>3.2 实现过程</b></a></li>
                                                <li><a href="#75" data-title="&lt;b&gt;3.3 分离目标&lt;/b&gt;"><b>3.3 分离目标</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#80" data-title="&lt;b&gt;4 实验与分析&lt;/b&gt; "><b>4 实验与分析</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#81" data-title="&lt;b&gt;4.1 实验数据&lt;/b&gt;"><b>4.1 实验数据</b></a></li>
                                                <li><a href="#83" data-title="&lt;b&gt;4.2 实验结果评估&lt;/b&gt;"><b>4.2 实验结果评估</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#95" data-title="&lt;b&gt;5 结论&lt;/b&gt; "><b>5 结论</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#44" data-title="&lt;b&gt;图1 语音分离系统框架&lt;/b&gt;"><b>图1 语音分离系统框架</b></a></li>
                                                <li><a href="#51" data-title="&lt;b&gt;图2 卷积神经网络结构图&lt;/b&gt;"><b>图2 卷积神经网络结构图</b></a></li>
                                                <li><a href="#86" data-title="&lt;b&gt;表1&lt;/b&gt; CNN&lt;b&gt;与&lt;/b&gt;DNN&lt;b&gt;预测结果对比&lt;/b&gt;"><b>表1</b> CNN<b>与</b>DNN<b>预测结果对比</b></a></li>
                                                <li><a href="#88" data-title="&lt;b&gt;表2&lt;/b&gt; CNN&lt;b&gt;使用不同的损失函数预测结果&lt;/b&gt;"><b>表2</b> CNN<b>使用不同的损失函数预测结果</b></a></li>
                                                <li><a href="#89" data-title="&lt;b&gt;图3 不同损失函数下的分类准确率&lt;/b&gt;"><b>图3 不同损失函数下的分类准确率</b></a></li>
                                                <li><a href="#90" data-title="&lt;b&gt;图4 不同损失函数下的&lt;/b&gt;HIT-FA&lt;b&gt;值&lt;/b&gt;"><b>图4 不同损失函数下的</b>HIT-FA<b>值</b></a></li>
                                                <li><a href="#93" data-title="&lt;b&gt;表4 不同信噪比下&lt;/b&gt;CNN&lt;b&gt;预测结果对比&lt;/b&gt;"><b>表4 不同信噪比下</b>CNN<b>预测结果对比</b></a></li>
                                                <li><a href="#94" data-title="&lt;b&gt;图5 不同信噪比下分类准确率和&lt;/b&gt;HIT-FA"><b>图5 不同信噪比下分类准确率和</b>HIT-FA</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" title=" J BARKER, M COOKE, P D GREEN.Robust ASR based on clean speech models:an evaluation of missing data techniques for connected digit recognition in noise[C].Eurospeech 2001 Scandinavia, European Conference on Speech Communication and Technology, INTERSPEECH Event, Aalborg, Denmark, September.DBLP, 2001:213-217." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Robust ASR based on clean speech models an evaluation of missing data techniques for connected digit recognition in noise">
                                        <b>[1]</b>
                                         J BARKER, M COOKE, P D GREEN.Robust ASR based on clean speech models:an evaluation of missing data techniques for connected digit recognition in noise[C].Eurospeech 2001 Scandinavia, European Conference on Speech Communication and Technology, INTERSPEECH Event, Aalborg, Denmark, September.DBLP, 2001:213-217.
                                    </a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" title=" D GABOR.Theory of communication.Part 1:The analysis of information[J].Journal of the Institution of Electrical Engineers - Part III:Radio and Communication Engineering, 2010, 93 (26) :429-441." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Theory of communication. Part 1: The analysis of information">
                                        <b>[2]</b>
                                         D GABOR.Theory of communication.Part 1:The analysis of information[J].Journal of the Institution of Electrical Engineers - Part III:Radio and Communication Engineering, 2010, 93 (26) :429-441.
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title=" J CHEN, Y WANG, D L WANG.A Feature Study for Classification-Based Speech Separation at Low Signal-to-Noise Ratios[J].IEEE/ACM Transactions on Audio Speech &amp;amp; Language Processing, 2014, 22 (12) :1993-2002." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM96B1411EC863A1DAB2722BF50E6EC856&amp;v=MDcwMDFORElybzR3RitNSkR3MDR1MmRoNkRoL1NnMlVxUkpBZjhmblRiK1pDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZCckxVMDV0Qmh3TG02d0tFPU5pZklZN3ErYg==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZDMVR6N0NyWFRURWhvQ21Icm4vdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         J CHEN, Y WANG, D L WANG.A Feature Study for Classification-Based Speech Separation at Low Signal-to-Noise Ratios[J].IEEE/ACM Transactions on Audio Speech &amp;amp; Language Processing, 2014, 22 (12) :1993-2002.
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" title=" J L ROUX, J R HERSHEY, F WENINGER.Deep NMF for speech separation[C].IEEE International Conference on Acoustics, Speech and Signal Processing.IEEE, 2015:66-70." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep NMF for speech separation">
                                        <b>[4]</b>
                                         J L ROUX, J R HERSHEY, F WENINGER.Deep NMF for speech separation[C].IEEE International Conference on Acoustics, Speech and Signal Processing.IEEE, 2015:66-70.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" title=" P S HUANG, et al.Joint optimization of masks and deep recurrent neural networks for monaural source separation[J].IEEE/ACM Transactions on Audio Speech &amp;amp; Language Processing, 2015, 23 (12) :2136-2147." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM2D08C34838330EB3C6A99E76000D707A&amp;v=MTc1NjR3TG02d0tFPU5pZklZN0hNSHRtL3JJdE5aK01NRDN4TXZSVmc3RTUwUVFybHFoSTFlY2FUUmIzdUNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkJyTFUwNXRCaA==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZDMVR6N0NyWFRURWhvQ21Icm4vdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                         P S HUANG, et al.Joint optimization of masks and deep recurrent neural networks for monaural source separation[J].IEEE/ACM Transactions on Audio Speech &amp;amp; Language Processing, 2015, 23 (12) :2136-2147.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" title=" A OZEROV, VINCENTE, F BIMBOT.A General Flexible Framework for the Handling of Prior Informationin Audio Source Separation[J].IEEE Transactions on Audio Speech &amp;amp; Language Processing, 2012, 20 (4) :1118-1133." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A General Flexible Framework for the Handling of Prior Information in Audio Source Separation">
                                        <b>[6]</b>
                                         A OZEROV, VINCENTE, F BIMBOT.A General Flexible Framework for the Handling of Prior Informationin Audio Source Separation[J].IEEE Transactions on Audio Speech &amp;amp; Language Processing, 2012, 20 (4) :1118-1133.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" title=" Y WANG, A NARAYANAN, D WANG.On Training Targets for Supervised Speech Separation[J].IEEE/ACM Transactions on Audio Speech &amp;amp; Language Processing, 2014, 22 (12) :1849-1858." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM0E6E7473F6DC5B47300C24AE9430C360&amp;v=MzAyMDVqOE9TbnVUMlJzeGVyTG5ScnlmQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQnJMVTA1dEJod0xtNndLRT1OaWZJWTdQTkdLVExxNGhHRXUxN2YzbEx5eEVRNg==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZDMVR6N0NyWFRURWhvQ21Icm4vdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                         Y WANG, A NARAYANAN, D WANG.On Training Targets for Supervised Speech Separation[J].IEEE/ACM Transactions on Audio Speech &amp;amp; Language Processing, 2014, 22 (12) :1849-1858.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" title=" S LIANG, W LIU, W JIANG.A New Bayesian Method Incorporating With Local Correlation for IBM Estimation[J].IEEE Transactions on Audio Speech &amp;amp; Language Processing, 2013, 21 (3) :476-487." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A New Bayesian Method Incorporating With Local Correlation for IBM Estimation">
                                        <b>[8]</b>
                                         S LIANG, W LIU, W JIANG.A New Bayesian Method Incorporating With Local Correlation for IBM Estimation[J].IEEE Transactions on Audio Speech &amp;amp; Language Processing, 2013, 21 (3) :476-487.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" title=" C DARWIN.Computational Auditory Scene Analysis:Principles, Algorithms and Applications[J].IEEE Transactions on Neural Networks, 2008, 19 (1) :199-199." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Computational Auditory Scene Analysis: Principles, Algorithms, and Applications (Wang, D. and Brown, G.J., Eds.; 2006) [Book review]">
                                        <b>[9]</b>
                                         C DARWIN.Computational Auditory Scene Analysis:Principles, Algorithms and Applications[J].IEEE Transactions on Neural Networks, 2008, 19 (1) :199-199.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_10" title=" Y WANG.Supervised speech separation using deep neural networks[J].Dissertations &amp;amp; Theses - Gradworks, 2015." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Supervised speech separation using deep neural networks">
                                        <b>[10]</b>
                                         Y WANG.Supervised speech separation using deep neural networks[J].Dissertations &amp;amp; Theses - Gradworks, 2015.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_11" title=" D WEBSDALE, B MILNER.A Comparison of Perceptually Motivated Loss Functions for Binary Mask Estimation in Speech Separation[C].INTERSPEECH.2017:2003-2007." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Comparison of Perceptually Motivated Loss Functions for Binary Mask Estimation in Speech Separation">
                                        <b>[11]</b>
                                         D WEBSDALE, B MILNER.A Comparison of Perceptually Motivated Loss Functions for Binary Mask Estimation in Speech Separation[C].INTERSPEECH.2017:2003-2007.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_12" title=" D L WANG.On Ideal Binary Mask As the Computational Goal of Auditory Scene Analysis[M].Speech Separation by Humans and Machines.Springer US, 2005:181-197." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=On ideal binary mask as the computational goal of auditory scene analysis">
                                        <b>[12]</b>
                                         D L WANG.On Ideal Binary Mask As the Computational Goal of Auditory Scene Analysis[M].Speech Separation by Humans and Machines.Springer US, 2005:181-197.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_13" title=" M AHMADI, V L GROSS, D G SINEX.Perceptual learning for speech in noise after application of binary time-frequency masks[J].Journal of the Acoustical Society of America, 2013, 133 (3) :1687-92." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Perceptual learning for speech in noise after application of binary time-frequency masks">
                                        <b>[13]</b>
                                         M AHMADI, V L GROSS, D G SINEX.Perceptual learning for speech in noise after application of binary time-frequency masks[J].Journal of the Acoustical Society of America, 2013, 133 (3) :1687-92.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_14" title=" N LI, P C LOIZOU.Factors influencing intelligibility of idealbinary-masked speech:Implications for noise reduction[J].Journal of the Acoustical Society of America, 2008, 123 (3) " target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Factors influencing intelligibility of ideal binary-masked speech: Implications for noise reduction">
                                        <b>[14]</b>
                                         N LI, P C LOIZOU.Factors influencing intelligibility of idealbinary-masked speech:Implications for noise reduction[J].Journal of the Acoustical Society of America, 2008, 123 (3) 
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_15" title=" D WANG, et al.Speech intelligibility in background noise with ideal binary time-frequency masking.[J].Journal of the Acoustical Society of America, 2009, 125 (4) :2336." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Speech intelligibility in background noise with ideal binary time-frequency masking">
                                        <b>[15]</b>
                                         D WANG, et al.Speech intelligibility in background noise with ideal binary time-frequency masking.[J].Journal of the Acoustical Society of America, 2009, 125 (4) :2336.
                                    </a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_16" title=" R E H OTHAUSER, et al.IEEE Recommended Practice for Speech Quality Measurements[C].IEEE No.IEEE, 2016:1-24." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=IEEE Recommended Practice for Speech Quality Measurements">
                                        <b>[16]</b>
                                         R E H OTHAUSER, et al.IEEE Recommended Practice for Speech Quality Measurements[C].IEEE No.IEEE, 2016:1-24.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJZ" target="_blank">计算机仿真</a>
                2019,36(05),279-283             </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于CHF-CNN的语音分离</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E5%B7%BE%E4%BE%A0&amp;code=42021375&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZDMVR6N0NyWFRURWhvQ21Icm4vdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王巾侠</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%8E%E5%B0%91%E6%B3%A2&amp;code=06940040&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZDMVR6N0NyWFRURWhvQ21Icm4vdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">李少波</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%B1%9F%E5%8E%9A%E6%B0%91&amp;code=42021376&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZDMVR6N0NyWFRURWhvQ21Icm4vdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">江厚民</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%BE%B9%E9%9C%84%E7%BF%94&amp;code=42021377&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZDMVR6N0NyWFRURWhvQ21Icm4vdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">边霄翔</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E8%B4%B5%E5%B7%9E%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF%E5%AD%A6%E9%99%A2&amp;code=0159277&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZDMVR6N0NyWFRURWhvQ21Icm4vdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">贵州大学计算机科学与技术学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E8%B4%B5%E5%B7%9E%E5%A4%A7%E5%AD%A6%E6%9C%BA%E6%A2%B0%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0159277&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZDMVR6N0NyWFRURWhvQ21Icm4vdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">贵州大学机械工程学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>深度神经网络已经在语音分离方面取得很好的表现, 但是卷积神经网络获取的语音信息会更全面。经常用来评估预测目标好坏的分类准确率和命中率-错误率 (HIT-FA) 之间存在不平衡现象。为了解决这种不平衡, 对卷积神经网络的损失函数进行了改进, 提出使用二元交叉熵及命中率-错误率混合 (CHF) 损失函数, 构成CHF-CNN模型。实验证明, 使用CHF-CNN模型可以同时提高分类准确率和命中率-错误率 (HIT-FA) 来避免不平衡现象。此外, 还验证了不同信噪比下的语音分离成果, 发现当信噪比匹配时效果比不匹配时明显好, 同时随着信噪比的增大效果会越来越好。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%AF%AD%E9%9F%B3%E5%88%86%E7%A6%BB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZDMVR6N0NyWFRURWhvQ21Icm4vdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">语音分离;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZDMVR6N0NyWFRURWhvQ21Icm4vdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">卷积神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%BA%8C%E5%85%83%E4%BA%A4%E5%8F%89%E7%86%B5%E5%8F%8A%E5%91%BD%E4%B8%AD%E7%8E%87-%E9%94%99%E8%AF%AF%E7%8E%87%E6%B7%B7%E5%90%88&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZDMVR6N0NyWFRURWhvQ21Icm4vdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">二元交叉熵及命中率-错误率混合;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    王巾侠 (1993-) , 女 (汉) , 山西省孝义市人, 硕士研究生, 主要研究领域为语音分离;
                                </span>
                                <span>
                                    李少波 (1973-) , 男 (汉) , 湖南岳阳人, 博士、教授、贵州大学机械工程学院院长, 主要研究领域:制造信息系统、制造物联、制造大数据、计算智能等;;
                                </span>
                                <span>
                                    江厚民 (1994-) , 男 (汉) , 安徽省安庆人, 硕士研究生, 主要研究领域为人工智能;;
                                </span>
                                <span>
                                    边霄翔 (1993-) , 男 (汉) , 山东省菏泽人, 硕士研究生, 主要研究领域为人工智能。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-07-18</p>

            </div>
                    <h1><b>Speech Separation Based on CHF-CNN</b></h1>
                    <h2>
                    <span>WANG Jin-xia</span>
                    <span>LI Shao-bo</span>
                    <span>JIANG Hou-min</span>
                    <span>BIAN Xiao-xiang</span>
            </h2>
                    <h2>
                    <span>School of Computer Science and Technology, Guizhou University</span>
                    <span>School of Mechanical Engineering, Guizhou University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Deep neural networks have achieved good performance in speech separation, but the speech information obtained by convolutional neural networks will be more comprehensive. There exists an imbalance between the classification accuracy rate and HIT-FA which are often used to assess the quality of predicted targets. In order to solve this imbalance, we improved the loss function of the convolutional neural network and proposed to use the binary cross-entropy HIT-FA hybrid (CHF) loss function to form the CHF-CNN model. Experiments have shown that using the CHF-CNN model can avoid imbalance by simultaneously improving classification accuracy and HIT-FA. In addition, we also verified the results of speech separation under different SNR. It is found that when the signal-to-noise ratio is matched, the effect is better than that of the mismatch, and the effect will be better as the signal-to-noise ratio increases.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Speech%20separation&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZDMVR6N0NyWFRURWhvQ21Icm4vdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Speech separation;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Convolutional%20neural%20networks%20(CNN)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZDMVR6N0NyWFRURWhvQ21Icm4vdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Convolutional neural networks (CNN) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Binary%20cross-entropy%20HIT-FA%20hybrid&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZDMVR6N0NyWFRURWhvQ21Icm4vdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Binary cross-entropy HIT-FA hybrid;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-07-18</p>
                            </div>


        <!--brief start-->
                        <h3 id="35" name="35" class="anchor-tag"><b>1 引言</b></h3>
                <div class="p1">
                    <p id="36">随着人工智能等领域的高速发展, 语音人机交互技术越来越重要, 因此对语音分离的要求也越来越高。这一技术已经在诸多方面得到广泛应用, 如:语音增强、机器语音识别、助听器等<citation id="97" type="reference"><link href="3" rel="bibliography" /><link href="5" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>]</sup></citation>。但是分离现实生活中不稳定的各种噪音仍然面临巨大的挑战。</p>
                </div>
                <div class="p1">
                    <p id="37">语音分离主要包括这几部分:特征提取<citation id="103" type="reference"><link href="7" rel="bibliography" /><link href="9" rel="bibliography" /><link href="11" rel="bibliography" /><sup>[<a class="sup">3</a>,<a class="sup">4</a>,<a class="sup">5</a>]</sup></citation>、模型训练<citation id="98" type="reference"><link href="13" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>、目标预测<citation id="99" type="reference"><link href="15" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>。可以分为多通道语音分离和单通道语音分离。多通道语音分离通过获取语音的空间信息, 而单通道语音则获取语音时域和频域信息, 研究表明, 单通道语音效果更好但是难度更大。单通道语音分离可以分为基于信号的语音分离<citation id="100" type="reference"><link href="17" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>、计算听觉场景分析<citation id="101" type="reference"><link href="19" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>和基于模型的语音分离<citation id="102" type="reference"><link href="13" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>。基于模型的语音分离如基于深度神经网络 (DNN) 的语音分离取得了良好的效果<citation id="104" type="reference"><link href="15" rel="bibliography" /><link href="21" rel="bibliography" /><sup>[<a class="sup">7</a>,<a class="sup">10</a>]</sup></citation>。该模型实现语音分离时, 无需平稳噪音为前提, 即使是非平稳噪音也能取得较好的分离结果。再者相比于计算听觉场景分析严重依赖基音检测这种现实很难达到的要求, 神经网络的要求更容易实现。</p>
                </div>
                <div class="p1">
                    <p id="38">DNN它的多层次结构有利于处理非结构信息, 从中抽取更加抽象的特征, 取得了良好的进展。然而DNN结构面对大量的数据及复杂的结构时存在大量的参数参与计算, 导致训练耗时较长。同时DNN无法获取位置信息与结构信息。相较于DNN, CNN能更好的解决上述问题。CNN是近年发展起来的一种高效的识别方法, 在图像识别、语音识别、基因检测和人脸识别等领域取得了很好的成就。</p>
                </div>
                <div class="p1">
                    <p id="39">CNN具有一定的优势, 但是模型训练时损失函数的选取直接影响结果的好坏。</p>
                </div>
                <div class="p1">
                    <p id="40">目前经常使用的损失函数在语音分离结果评估时各项评估结果存在不平衡现象, 因此改进这种不平衡现象有利于语音分离效果的提高。</p>
                </div>
                <div class="p1">
                    <p id="41">本文使用了CNN模型进行语音分离, 同时与可以平衡各项结果的CHF损失函数, 构成CHF-CNN模型, 并取得了比较好的语音分离效果。</p>
                </div>
                <h3 id="42" name="42" class="anchor-tag"><b>2 语音分离系统</b></h3>
                <div class="p1">
                    <p id="43">语音分离主要有语音预处理、特征提取、模型训练、目标预测、波形合成。如语音分离的整个系统框架如图1所示:系统首先分别通过纯净语音和噪音得到训练时需求的预混合语音和标签即理想二值掩码;然后在预混合语音得到的能量谱基础上提取MRCG特征<citation id="105" type="reference"><link href="7" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>;得到的特征向量将作为训练模型的输入进行训练模型;最终通过模型进行测试集掩码预测。</p>
                </div>
                <div class="area_img" id="44">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJZ201905056_044.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZDMVR6N0NyWFRURWhvQ21Icm4vdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 语音分离系统框架" src="Detail/GetImg?filename=images/JSJZ201905056_044.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZDMVR6N0NyWFRURWhvQ21Icm4vdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图1 语音分离系统框架</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJZ201905056_044.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZDMVR6N0NyWFRURWhvQ21Icm4vdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="45">典型的卷积网络, 由卷积层、池化层、全连接层组成。相比其它深度学习模型卷积神经网络有三大优点:第一, 局部感受视野。相邻层之间不再是全连接而是局部连接, 因此这种局部连接很大程度上降低了网络的复杂度;第二, 权值共享, 同一个卷积核中, 所有的神经元的权重是相同的, 减少了训练中参数个数;第三个优点是池化, 池化对输入的矩阵进行了“压缩”缩小了数据规模, 不仅减少了结构中的参数及复杂度, 实验时间也得到了很大程度上的缩减, 降低了时间成本。同时CNN不再从一维向量的角度提取特征, 而是从多维的角度, 更加全方位提取信息。所以选择CNN模型更有利于语音分离。</p>
                </div>
                <div class="p1">
                    <p id="46">经常用来语音分离的损失函数交叉熵损失函数和HIT-FA损失函数在进行分离结果的分类准确率和HIT-FA值存在不平衡现象, 不能同时增大。这种现象与损失函数的选取有很大的关系。Websdale D和Milner B<citation id="106" type="reference"><link href="23" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>发现损失函数是交叉熵函数时, 主要为了增大分类准确率, HIT-FA值却没有明显变化。当损失函数为HIT-FA时呈与上述相反的情况。实际中, 这两种评价方法在语音分离评估上各有侧重点, 第一种为直接计算分类准确率, 即被正确分类的T-F单元占总的T-F单元的比例。然而这种方法对错误分类的T-F 单元存在不可预知性。HIT-FA更符合人的理解习惯, 其中HIT指预测分类为语音主导单元中真正分类正确的单元所占的比例;FA为预测分类为噪音主导的单元中分类错误的单元所占的比例。好的IBM应该有高的HIT和低的FA, 因此当两种结果得到平衡, 且同时提高时才能证明语音分离的效果有所提高。本文为了解决此问题在CNN模型上提出用新的损失函数来解决。</p>
                </div>
                <h3 id="47" name="47" class="anchor-tag"><b>3 CNN模型</b></h3>
                <h4 class="anchor-tag" id="48" name="48"><b>3.1 模型优化</b></h4>
                <div class="p1">
                    <p id="49">语音分离使用DNN模型已经得到了很好的效果<citation id="107" type="reference"><link href="23" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>。为了验证CNN对于语音分离是否更佳, 对比了目前表现较好的DNN结构和构建的CNN结构, 来选  择较好的模型参与实验研究。将实验最终确定的CNN模型 (如图2) 同目前表现较好的DNN<citation id="108" type="reference"><link href="23" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>的预测结果在保证相同信噪比的前提下进行对比。</p>
                </div>
                <div class="p1">
                    <p id="50">选择模型的类型后, 对采用的模型进一步进行优化, 使其的语音分离效果最佳。不同的损失函数对模型训练结果有明显的影响, 研究过程中使用了在语音分离时经常使用的不同的损失函数进行模型训练, 分别有交叉熵损失函数, HIT-FA损失函数。但是两者在结果上各有不足之处, 最终使用了将两者结合的CHF损失函数来避免结果的不平衡。CHF损失函数 (式 (3) ) 是在交叉熵损失函数 (式 (1) ) 和HIT-FA损失函数 (式 (2) ) 的基础上得出的, 它同时融入和中和了两种方法即分类准确率和HIT-FA对IBM预测结果评估, 避免了结果的不平衡。</p>
                </div>
                <div class="area_img" id="51">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJZ201905056_051.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZDMVR6N0NyWFRURWhvQ21Icm4vdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 卷积神经网络结构图" src="Detail/GetImg?filename=images/JSJZ201905056_051.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZDMVR6N0NyWFRURWhvQ21Icm4vdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图2 卷积神经网络结构图</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJZ201905056_051.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZDMVR6N0NyWFRURWhvQ21Icm4vdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="52"><mathml id="53"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><msup><mrow></mrow><mrow><mi>C</mi><mi>E</mi></mrow></msup><mo>=</mo><mo>-</mo><mfrac><mn>1</mn><mi>Ν</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mrow><mrow><mo>[</mo><mrow><mi>y</mi><msub><mrow></mrow><mi>n</mi></msub><mi>log</mi><mo stretchy="false"> (</mo><mover accent="true"><mi>y</mi><mo>^</mo></mover><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false">) </mo><mo>+</mo><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>y</mi><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false">) </mo><mi>log</mi><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mover accent="true"><mi>y</mi><mo>^</mo></mover><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false">) </mo></mrow><mo>]</mo></mrow></mrow></mstyle></mrow></math></mathml>      (1) </p>
                </div>
                <div class="p1">
                    <p id="54"><mathml id="55"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><msup><mrow></mrow><mrow><mi>Η</mi><mi>F</mi></mrow></msup><mo>=</mo><mfrac><mn>1</mn><mi>S</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mrow><mrow><mo>[</mo><mrow><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>y</mi><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false">) </mo><mover accent="true"><mi>y</mi><mo>^</mo></mover><msub><mrow></mrow><mi>n</mi></msub></mrow><mo>]</mo></mrow></mrow></mstyle><mo>-</mo><mfrac><mn>1</mn><mi>R</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mtext>n</mtext><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mrow><mrow><mo>[</mo><mrow><mi>y</mi><msub><mrow></mrow><mi>n</mi></msub><mover accent="true"><mi>y</mi><mo>^</mo></mover><msub><mrow></mrow><mi>n</mi></msub></mrow><mo>]</mo></mrow></mrow></mstyle></mrow></math></mathml>      (2) </p>
                </div>
                <div class="p1">
                    <p id="56"><mathml id="57"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><msup><mrow></mrow><mrow><mi>C</mi><mi>Η</mi><mi>F</mi></mrow></msup><mo>=</mo><mo>-</mo><mfrac><mn>1</mn><mi>Ν</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mrow><mrow><mo>[</mo><mrow><mi>y</mi><msub><mrow></mrow><mi>n</mi></msub><mi>log</mi><mo stretchy="false"> (</mo><mover accent="true"><mi>y</mi><mo>^</mo></mover><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false">) </mo><mo>+</mo><mfrac><mi>R</mi><mi>S</mi></mfrac><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>y</mi><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false">) </mo><mi>log</mi><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mover accent="true"><mi>y</mi><mo>^</mo></mover><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false">) </mo></mrow><mo>]</mo></mrow></mrow></mstyle></mrow></math></mathml>      (3) </p>
                </div>
                <div class="p1">
                    <p id="58">公式主要对T-F单元所对应的IBM值进行计算。R代表所有的IBM值中, 值为1的个数, 即预测的语音主导的T-F单元个数。S代表IBM值为0的个数, 即噪音主导的T-F单元的个数。通常情况下<i>R</i>要远大于<i>S</i>。</p>
                </div>
                <div class="p1">
                    <p id="59">实现语音分离时, 不同的信噪比下分离结果存在差异, 为此在训练好模型的基础上也验证了不同信噪比下模型的表现。</p>
                </div>
                <h4 class="anchor-tag" id="60" name="60"><b>3.2 实现过程</b></h4>
                <h4 class="anchor-tag" id="61" name="61">3.2.1 时频分解</h4>
                <div class="p1">
                    <p id="62">得到预混合语音后要对预混合语音以及参与预混合语音的纯净语音和噪音进行时频分解计算其对应量。首先要对预混合语音进行重采样, 采样频率为16000Hz, 以20ms为一帧长, 帧位移为10ms, 通过64个通道的Gammatone听觉滤波器<citation id="109" type="reference"><link href="7" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>, 得到每一个耳蜗值即输出的能量。</p>
                </div>
                <div class="p1">
                    <p id="63">实现Gammatone听觉滤波器如下方法</p>
                </div>
                <div class="area_img" id="64">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJZ201905056_06400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZDMVR6N0NyWFRURWhvQ21Icm4vdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="66">Gammatone听觉滤波器是使用一组<i>g</i> (<i>t</i>) 进行滤波, 经过一系列计算会得到一组滤波结果。式 (4) 中<i>l</i>代表滤波阶数, <i>b</i>代表等效矩阵带宽, <i>f</i>为中心频率, 在实验中心频率<i>f</i>等间隔分布在[50Hz, 8000Hz]这个区间内。<i>b</i>与<i>f</i>之间满足式 (5) </p>
                </div>
                <div class="p1">
                    <p id="67"><i>ERB</i> (<i>f</i>) =24.7× (0.0043×<i>f</i>+1.0)      (5) </p>
                </div>
                <div class="p1">
                    <p id="68">当<i>l</i>=4时, <i>b</i>=1.093×<i>ERB</i> (<i>f</i>) 。</p>
                </div>
                <h4 class="anchor-tag" id="69" name="69">3.2.2 提取特征</h4>
                <div class="p1">
                    <p id="70">时频分解完成后, 以预混合语音的每一帧对应的时频单元为中心进行MRCG<citation id="110" type="reference"><link href="7" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>特征提取, 得到其特征向量, 作为训练模型的输入。MRCG特征组合了四个解析度耳蜗特征, 其步骤可总结如下:</p>
                </div>
                <div class="p1">
                    <p id="71">1) 捕获高分辨率的局部细节。该过程对通过Gammatone听觉滤波器得出的值进行log计算。</p>
                </div>
                <div class="p1">
                    <p id="72">2) 同一个耳蜗第二解析度耳蜗值的计算过程同第一个相类似, 区别在于计算时帧长变为200ms, 位移为10ms。</p>
                </div>
                <div class="p1">
                    <p id="73">3) 第三个解析度耳蜗值和第四个解析度耳蜗值以选择计算的耳蜗为中心形成11*11和23*23大小的窗口, 如果窗口大小超出范围, 则0来补齐, 利用这些窗口计算中心单元平均的特征值。</p>
                </div>
                <div class="p1">
                    <p id="74">4) 最后将四部的特征值进行组合形成最终特征向量。</p>
                </div>
                <h4 class="anchor-tag" id="75" name="75"><b>3.3 分离目标</b></h4>
                <div class="p1">
                    <p id="76">实验的分离目标是IBM掩码<citation id="111" type="reference"><link href="25" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>。在时频分解后, 要计算每一个<i>T</i>-<i>F</i>单元当纯净语音的能量大于噪音的能量值时掩码为1, 否则将为0。如下为实现方法</p>
                </div>
                <div class="area_img" id="77">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJZ201905056_07700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZDMVR6N0NyWFRURWhvQ21Icm4vdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="79">式中<i>t</i>代表时间, <i>f</i>代表频率, <i>SNR</i> (<i>t</i>, <i>f</i>) 为时间<i>t</i>和频率<i>f</i>对应的信噪比, <i>LC</i>为局部信噪比指标 (Local SNR Criterion) , 通常情况下设置为0dB<citation id="112" type="reference"><link href="7" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>。为了保留更多的语音信息, 实验中局部信噪比为-10<i>dB</i>。多项研究表明:理想二值掩蔽能够显著提高正常人和听力受损者在嘈杂环境中的语音理解能力<citation id="113" type="reference"><link href="27" rel="bibliography" /><link href="29" rel="bibliography" /><sup>[<a class="sup">13</a>,<a class="sup">14</a>]</sup></citation>。在实验中, 理想二值掩码将作为训练目标, 故而将语音分离问题转化为二值分类问题。</p>
                </div>
                <h3 id="80" name="80" class="anchor-tag"><b>4 实验与分析</b></h3>
                <h4 class="anchor-tag" id="81" name="81"><b>4.1 实验数据</b></h4>
                <div class="p1">
                    <p id="82">实验的数据集, 采用了IEEE Corpus<citation id="114" type="reference"><link href="31" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>语料库中的数据得到训练集和测试集。噪音选取了NOISEX-92噪声集中的噪音与选取的纯净语音混合。将每个噪音分为两部分, 第一部分与训练集的纯净语音混合, 第二部分与测试集纯净语音混合。模型训练时, 训练集的信噪比为-2dB, 测试集的信噪比分别为-5dB、-2dB、0dB、2dB、5dB。</p>
                </div>
                <h4 class="anchor-tag" id="83" name="83"><b>4.2 实验结果评估</b></h4>
                <div class="p1">
                    <p id="84">模型最终的预测目标为IBM掩码。整个实验过程中, 对于预测结果的比较采用了两种评估方式:第一种为直接计算分类准确率;第二种HIT-FA。分类准确率表示分类正确的掩码占所有预测掩码的比例。HIT-FA表示正确分类为1的掩码占所有为1的掩码的比例减去所有错误预测为0的掩码占所有为0的掩码数的比例。通过两种方式的结合使对预测掩码的评估准确且更符合人的理解方式。</p>
                </div>
                <div class="p1">
                    <p id="85">实验中首先将实验最终确定的CNN模型同目前表现较好的DNN<citation id="115" type="reference"><link href="23" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>的预测结果进行对比, 即将信噪比同训练集相同的预混合语音提取的特征向量分别输入CNN和DNN两种不同的分类器后, 由表1明显可知, 在其它条件相同的情况下卷积神经网络比深度神经网络对语音理想二值掩码 (IBM) 的预测效果更好。所以, 实验最终的模型选择CNN模型。</p>
                </div>
                <div class="area_img" id="86">
                    <p class="img_tit"><b>表1</b> CNN<b>与</b>DNN<b>预测结果对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="86" border="1"><tr><td><br />分类器</td><td>Accuracy</td><td>HIT-FA</td></tr><tr><td><br />DNN</td><td>0.879</td><td>0.708</td></tr><tr><td><br />CNN</td><td>0.881</td><td>0.712</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="87">表2列出了CHF损失函数与经常使用的损失函数在CNN结构中的不同效果。图3柱形图显示了不同损失函数下分类准确率的差异。图4显示了不同损失函数下HIT-FA值得差异。</p>
                </div>
                <div class="area_img" id="88">
                    <p class="img_tit"><b>表2</b> CNN<b>使用不同的损失函数预测结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="88" border="1"><tr><td><br />损失函数</td><td>Accuracy</td><td>HIT-FA</td></tr><tr><td><br />交叉熵</td><td>0.881</td><td>0.712</td></tr><tr><td><br />HIT-FA</td><td>0.876</td><td>0.719</td></tr><tr><td><br />CHF</td><td>0.883</td><td>0.723</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="89">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJZ201905056_089.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZDMVR6N0NyWFRURWhvQ21Icm4vdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 不同损失函数下的分类准确率" src="Detail/GetImg?filename=images/JSJZ201905056_089.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZDMVR6N0NyWFRURWhvQ21Icm4vdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图3 不同损失函数下的分类准确率</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJZ201905056_089.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZDMVR6N0NyWFRURWhvQ21Icm4vdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="90">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJZ201905056_090.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZDMVR6N0NyWFRURWhvQ21Icm4vdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 不同损失函数下的HIT-FA值" src="Detail/GetImg?filename=images/JSJZ201905056_090.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZDMVR6N0NyWFRURWhvQ21Icm4vdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图4 不同损失函数下的</b>HIT-FA<b>值</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJZ201905056_090.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZDMVR6N0NyWFRURWhvQ21Icm4vdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="91">由图3和图4对比分析可知相比于HIT-FA的损失函数在HIT-FA值的优势和交叉熵函数在分类准确率的优势, CHF损失函数无论是在分类准确率还是HIT-FA值得表现都比较好。</p>
                </div>
                <div class="p1">
                    <p id="92">在实验中, 验证了在测试集与训练集信噪比匹配的前提下CNN对结果的预测。但是在现实中语音的信噪比是不确定的, 为此在最终优化好的CNN基础上对不同信噪比及信噪比不匹配的情况下进行了验证, 结果如表4。同时从图5可以看出随着信噪比的增大分类准确率和HIT-FA值会逐渐变大。但是当测试集与训练集的信噪比匹配且都为-2dB时效果比0dB更好。说明训练集的泛化性对结果有很大的影响。</p>
                </div>
                <div class="area_img" id="93">
                    <p class="img_tit"><b>表4 不同信噪比下</b>CNN<b>预测结果对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="93" border="1"><tr><td><br />信噪比</td><td>Accuracy</td><td>HIT-FA</td></tr><tr><td><br />-5dB</td><td>0.871</td><td>0.709</td></tr><tr><td><br />-2dB</td><td>0.883</td><td>0.723</td></tr><tr><td><br />0dB</td><td>0.880</td><td>0.721</td></tr><tr><td><br />2dB</td><td>0.885</td><td>0.727</td></tr><tr><td><br />5dB</td><td>0.912</td><td>0.739</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="94">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJZ201905056_094.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZDMVR6N0NyWFRURWhvQ21Icm4vdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 不同信噪比下分类准确率和HIT-FA" src="Detail/GetImg?filename=images/JSJZ201905056_094.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZDMVR6N0NyWFRURWhvQ21Icm4vdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图5 不同信噪比下分类准确率和</b>HIT-FA  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJZ201905056_094.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZDMVR6N0NyWFRURWhvQ21Icm4vdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h3 id="95" name="95" class="anchor-tag"><b>5 结论</b></h3>
                <div class="p1">
                    <p id="96">为了提高语音分离效果, 实验中对语音分离中的模型使用了CNN结构, 并对CNN结构进行进一步改进和分析。实验总共分为三个部分:第一部分用CNN替换传统的DNN;第二部分使用CHF损失函数进一步优化CNN 结构;第三部分, 验证在不同信噪比下, 语音分离效果的差异。通过以上这三部分验证, 总结得出, 在语音分离上, CNN对语音信息处理上更有优势, 同时使用CHF损失函数更加全面考虑了预测结果的可靠性, 提高了预测准确率。不同的信噪比下实验, 验证了信噪比对语音分离效果的影响规律, 信噪比越大分离效果会相对越好。同时在测试集和训练集测试集信噪比匹配的情况下预测效果会更好。然而, 实验中也存在许多的不足之处, 实验中CNN的结构没有进行更深入的优化, 有进一步改进和创新的空间, 需要继续努力。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
                        <h3 class="anchor-tag">作者图片</h3>
                <div class="anchor-wrap">
                        <p>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="118" type="formula" href="images/JSJZ201905056_11800.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">王巾侠</span>
                                    </div>
                        </p>
                </div>


        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Robust ASR based on clean speech models an evaluation of missing data techniques for connected digit recognition in noise">

                                <b>[1]</b> J BARKER, M COOKE, P D GREEN.Robust ASR based on clean speech models:an evaluation of missing data techniques for connected digit recognition in noise[C].Eurospeech 2001 Scandinavia, European Conference on Speech Communication and Technology, INTERSPEECH Event, Aalborg, Denmark, September.DBLP, 2001:213-217.
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Theory of communication. Part 1: The analysis of information">

                                <b>[2]</b> D GABOR.Theory of communication.Part 1:The analysis of information[J].Journal of the Institution of Electrical Engineers - Part III:Radio and Communication Engineering, 2010, 93 (26) :429-441.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM96B1411EC863A1DAB2722BF50E6EC856&amp;v=MDg1OTNSSkFmOGZuVGIrWkNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkJyTFUwNXRCaHdMbTZ3S0U9TmlmSVk3cStiTkRJcm80d0YrTUpEdzA0dTJkaDZEaC9TZzJVcQ==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZDMVR6N0NyWFRURWhvQ21Icm4vdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> J CHEN, Y WANG, D L WANG.A Feature Study for Classification-Based Speech Separation at Low Signal-to-Noise Ratios[J].IEEE/ACM Transactions on Audio Speech &amp; Language Processing, 2014, 22 (12) :1993-2002.
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep NMF for speech separation">

                                <b>[4]</b> J L ROUX, J R HERSHEY, F WENINGER.Deep NMF for speech separation[C].IEEE International Conference on Acoustics, Speech and Signal Processing.IEEE, 2015:66-70.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM2D08C34838330EB3C6A99E76000D707A&amp;v=MTg5NzR4TXZSVmc3RTUwUVFybHFoSTFlY2FUUmIzdUNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkJyTFUwNXRCaHdMbTZ3S0U9TmlmSVk3SE1IdG0vckl0TlorTU1EMw==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZDMVR6N0NyWFRURWhvQ21Icm4vdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b> P S HUANG, et al.Joint optimization of masks and deep recurrent neural networks for monaural source separation[J].IEEE/ACM Transactions on Audio Speech &amp; Language Processing, 2015, 23 (12) :2136-2147.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A General Flexible Framework for the Handling of Prior Information in Audio Source Separation">

                                <b>[6]</b> A OZEROV, VINCENTE, F BIMBOT.A General Flexible Framework for the Handling of Prior Informationin Audio Source Separation[J].IEEE Transactions on Audio Speech &amp; Language Processing, 2012, 20 (4) :1118-1133.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM0E6E7473F6DC5B47300C24AE9430C360&amp;v=MjAyOTlKSUZwbWFCdUhZZk9HUWxmQnJMVTA1dEJod0xtNndLRT1OaWZJWTdQTkdLVExxNGhHRXUxN2YzbEx5eEVRNmo4T1NudVQyUnN4ZXJMblJyeWZDT052RlNpV1dyNw==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZDMVR6N0NyWFRURWhvQ21Icm4vdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b> Y WANG, A NARAYANAN, D WANG.On Training Targets for Supervised Speech Separation[J].IEEE/ACM Transactions on Audio Speech &amp; Language Processing, 2014, 22 (12) :1849-1858.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A New Bayesian Method Incorporating With Local Correlation for IBM Estimation">

                                <b>[8]</b> S LIANG, W LIU, W JIANG.A New Bayesian Method Incorporating With Local Correlation for IBM Estimation[J].IEEE Transactions on Audio Speech &amp; Language Processing, 2013, 21 (3) :476-487.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Computational Auditory Scene Analysis: Principles, Algorithms, and Applications (Wang, D. and Brown, G.J., Eds.; 2006) [Book review]">

                                <b>[9]</b> C DARWIN.Computational Auditory Scene Analysis:Principles, Algorithms and Applications[J].IEEE Transactions on Neural Networks, 2008, 19 (1) :199-199.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Supervised speech separation using deep neural networks">

                                <b>[10]</b> Y WANG.Supervised speech separation using deep neural networks[J].Dissertations &amp; Theses - Gradworks, 2015.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Comparison of Perceptually Motivated Loss Functions for Binary Mask Estimation in Speech Separation">

                                <b>[11]</b> D WEBSDALE, B MILNER.A Comparison of Perceptually Motivated Loss Functions for Binary Mask Estimation in Speech Separation[C].INTERSPEECH.2017:2003-2007.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=On ideal binary mask as the computational goal of auditory scene analysis">

                                <b>[12]</b> D L WANG.On Ideal Binary Mask As the Computational Goal of Auditory Scene Analysis[M].Speech Separation by Humans and Machines.Springer US, 2005:181-197.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Perceptual learning for speech in noise after application of binary time-frequency masks">

                                <b>[13]</b> M AHMADI, V L GROSS, D G SINEX.Perceptual learning for speech in noise after application of binary time-frequency masks[J].Journal of the Acoustical Society of America, 2013, 133 (3) :1687-92.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Factors influencing intelligibility of ideal binary-masked speech: Implications for noise reduction">

                                <b>[14]</b> N LI, P C LOIZOU.Factors influencing intelligibility of idealbinary-masked speech:Implications for noise reduction[J].Journal of the Acoustical Society of America, 2008, 123 (3) 
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Speech intelligibility in background noise with ideal binary time-frequency masking">

                                <b>[15]</b> D WANG, et al.Speech intelligibility in background noise with ideal binary time-frequency masking.[J].Journal of the Acoustical Society of America, 2009, 125 (4) :2336.
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=IEEE Recommended Practice for Speech Quality Measurements">

                                <b>[16]</b> R E H OTHAUSER, et al.IEEE Recommended Practice for Speech Quality Measurements[C].IEEE No.IEEE, 2016:1-24.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJZ201905056" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZDMVR6N0NyWFRURWhvQ21Icm4vdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJZ201905056&amp;v=MDk0NDdUNmo1NE8zenFxQnRHRnJDVVI3cWZadVptRnl6a1ZyL0JMejdCZExHNEg5ak1xbzlBWW9RS0RIODR2UjQ=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZDMVR6N0NyWFRURWhvQ21Icm4vdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=842_VlRz_g1CMCLi-M0twVw-4EOFG_noWyi_OsMnrAg1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
