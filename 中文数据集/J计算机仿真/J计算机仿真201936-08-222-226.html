<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=NOOK-mvTMnxl11s6oPYV0boPXN3iP4N7Rc-A56nk4KI1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2fKXReader%2fDetail%3fTIMESTAMP%3d637140154189193750%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJZ201908046%26RESULT%3d1%26SIGN%3dQfDqSJYu0%252foxq3y3Erw1sp2kGns%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJZ201908046&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJZ201908046&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJZ201908046&amp;v=MTM5NDI4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1Wm9GeTduVkx2QUx6N0JkTEc0SDlqTXA0OUJZb1FLREg=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#33" data-title="&lt;b&gt;1 引言&lt;/b&gt; "><b>1 引言</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#39" data-title="&lt;b&gt;2 相机模型与点云不确定度模型的建立&lt;/b&gt; "><b>2 相机模型与点云不确定度模型的建立</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#40" data-title="&lt;b&gt;2.1 相机模型的建立与分析&lt;/b&gt;"><b>2.1 相机模型的建立与分析</b></a></li>
                                                <li><a href="#44" data-title="&lt;b&gt;2.2 点云模型不确定度建模&lt;/b&gt;"><b>2.2 点云模型不确定度建模</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#55" data-title="&lt;b&gt;3 视觉里程计轨迹估计&lt;/b&gt; "><b>3 视觉里程计轨迹估计</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#56" data-title="&lt;b&gt;3.1 算法框架&lt;/b&gt;"><b>3.1 算法框架</b></a></li>
                                                <li><a href="#61" data-title="&lt;b&gt;3.2 基于空间约束ICP的位姿估计算法&lt;/b&gt;"><b>3.2 基于空间约束ICP的位姿估计算法</b></a></li>
                                                <li><a href="#75" data-title="&lt;b&gt;3.3 帧对模型与帧间匹配的切换策略&lt;/b&gt;"><b>3.3 帧对模型与帧间匹配的切换策略</b></a></li>
                                                <li><a href="#80" data-title="&lt;b&gt;3.4 基于卡尔曼滤波器的点云模型更新&lt;/b&gt;"><b>3.4 基于卡尔曼滤波器的点云模型更新</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#83" data-title="&lt;b&gt;4 实验与分析&lt;/b&gt; "><b>4 实验与分析</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#85" data-title="&lt;b&gt;4.1 部分重叠率估计机制对比实验&lt;/b&gt;"><b>4.1 部分重叠率估计机制对比实验</b></a></li>
                                                <li><a href="#88" data-title="&lt;b&gt;4.2 帧对模型与帧间匹配的切换策略对比实验&lt;/b&gt;"><b>4.2 帧对模型与帧间匹配的切换策略对比实验</b></a></li>
                                                <li><a href="#92" data-title="&lt;b&gt;4.3 视觉里程计的综合表现能力实验&lt;/b&gt;"><b>4.3 视觉里程计的综合表现能力实验</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#97" data-title="&lt;b&gt;5 结语&lt;/b&gt; "><b>5 结语</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#54" data-title="&lt;b&gt;图1 特征点稀疏点云模型&lt;/b&gt;"><b>图1 特征点稀疏点云模型</b></a></li>
                                                <li><a href="#58" data-title="&lt;b&gt;图2 视觉里程计算法框架&lt;/b&gt;"><b>图2 视觉里程计算法框架</b></a></li>
                                                <li><a href="#87" data-title="&lt;b&gt;表1 fr1_xyz视频序列位姿估计算法对比&lt;/b&gt;"><b>表1 fr1_xyz视频序列位姿估计算法对比</b></a></li>
                                                <li><a href="#90" data-title="&lt;b&gt;表2 fr1_desk数据集上位姿估计算法对比&lt;/b&gt;"><b>表2 fr1_desk数据集上位姿估计算法对比</b></a></li>
                                                <li><a href="#91" data-title="&lt;b&gt;图3 本文与FVO相对轨迹误差对比图&lt;/b&gt;"><b>图3 本文与FVO相对轨迹误差对比图</b></a></li>
                                                <li><a href="#94" data-title="&lt;b&gt;表3 TUM RGB-D数据集综合表现能力实验&lt;/b&gt;"><b>表3 TUM RGB-D数据集综合表现能力实验</b></a></li>
                                                <li><a href="#95" data-title="&lt;b&gt;表4 TUM RGB-D综合表现对比实验&lt;/b&gt;"><b>表4 TUM RGB-D综合表现对比实验</b></a></li>
                                                <li><a href="#96" data-title="&lt;b&gt;图4 四个典型视频序列绝对轨迹图&lt;/b&gt;"><b>图4 四个典型视频序列绝对轨迹图</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="125">


                                    <a id="bibliography_1" title=" K Konolige et al.Outdoor Mapping and Navigation Using Stereo Vision[J].Tracts in AdvancedRobotics, Springer, 2008, 39:179-190." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Outdoor Mapping and Navigation Using Stereo Vision">
                                        <b>[1]</b>
                                         K Konolige et al.Outdoor Mapping and Navigation Using Stereo Vision[J].Tracts in AdvancedRobotics, Springer, 2008, 39:179-190.
                                    </a>
                                </li>
                                <li id="127">


                                    <a id="bibliography_2" title=" 朱永丰, 朱述龙, 张静静, 朱永康.基于ORB特征的单目视觉定位算法研究[J].计算机科学, 2016, 43 (6A) :198-202, 254." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJA2016S1049&amp;v=Mjc3OTF2cm85QmJZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVab0Z5N25WTHZBTHo3QmI3RzRIOWU=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         朱永丰, 朱述龙, 张静静, 朱永康.基于ORB特征的单目视觉定位算法研究[J].计算机科学, 2016, 43 (6A) :198-202, 254.
                                    </a>
                                </li>
                                <li id="129">


                                    <a id="bibliography_3" title=" M A Fischler, R C BolleS.Random sample consensus:a paradigm for model fitting with applications to image analysis and automated cartography[J].Communications of the ACM, ACM, 1981, 24 (6) :381–395." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000025011&amp;v=MzIyNjhyUmRHZXJxUVRNbndaZVp0RmlubFVybklKbHdUYmhvPU5pZklZN0s3SHRqTnI0OUZaT2tLREgwNG9CTVQ2VDRQUUgvaQ==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         M A Fischler, R C BolleS.Random sample consensus:a paradigm for model fitting with applications to image analysis and automated cartography[J].Communications of the ACM, ACM, 1981, 24 (6) :381–395.
                                    </a>
                                </li>
                                <li id="131">


                                    <a id="bibliography_4" title=" 李宇波, 朱效洲, 卢惠民, 张辉.视觉里程计技术综述[J].计算机应用研究, 2012:2801-2805." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSYJ201208002&amp;v=MTgwMjY0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVab0Z5N25WTHZBTHo3U1pMRzRIOVBNcDQ5RlpvUUtESDg0dlI=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                         李宇波, 朱效洲, 卢惠民, 张辉.视觉里程计技术综述[J].计算机应用研究, 2012:2801-2805.
                                    </a>
                                </li>
                                <li id="133">


                                    <a id="bibliography_5" title=" 张云, 尹露, 王雨婷, 罗斌.基于SiftGPU特征匹配方法的实时视觉里程计系统[J].太赫兹科学与电子信息学报, 2015:897-902, 912." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XXYD201506012&amp;v=MjgyMTlLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1Wm9GeTduVkx2QVBUWFNhckc0SDlUTXFZOUVab1E=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                         张云, 尹露, 王雨婷, 罗斌.基于SiftGPU特征匹配方法的实时视觉里程计系统[J].太赫兹科学与电子信息学报, 2015:897-902, 912.
                                    </a>
                                </li>
                                <li id="135">


                                    <a id="bibliography_6" title=" S Prakhya et al.Sparse Depth Odometry:3D keypoint based pose estimation from dense depth data[C].2013 IEEE International Conference on Robotics and Automation IEEE, 2015:4216-4223." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Sparse depth odometry:3D keypoint based pose estimation from dense depth data">
                                        <b>[6]</b>
                                         S Prakhya et al.Sparse Depth Odometry:3D keypoint based pose estimation from dense depth data[C].2013 IEEE International Conference on Robotics and Automation IEEE, 2015:4216-4223.
                                    </a>
                                </li>
                                <li id="137">


                                    <a id="bibliography_7" title=" K Christian, J Sturm, D Cremers.Robust odometry estimation for RGB-D cameras[C].2013 IEEE International Conference on Robotics and Automation IEEE, 2013:3748-3754." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Robust odometry estimation for RGB-D cameras">
                                        <b>[7]</b>
                                         K Christian, J Sturm, D Cremers.Robust odometry estimation for RGB-D cameras[C].2013 IEEE International Conference on Robotics and Automation IEEE, 2013:3748-3754.
                                    </a>
                                </li>
                                <li id="139">


                                    <a id="bibliography_8" title=" D Gutierrez-Gomez, W Mayol-Cuevas, J Guerrero.Dense RGB-D visual odometry using inverse depth[J].Robotics and Autonomous Systems, Elsevier, 2016, 75:571-583." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES7C62C554F5038DA1B5F99C3C78E18F72&amp;v=MTk4OTdoZGg3MGwwUVF6aDN4VTlETE9jTTcyZENPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkJyTFUwNXQ1aHdycTR4S0E9TmlmT2ZiVExHTk8vcW9wQkV1NFBEM1JOdg==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                         D Gutierrez-Gomez, W Mayol-Cuevas, J Guerrero.Dense RGB-D visual odometry using inverse depth[J].Robotics and Autonomous Systems, Elsevier, 2016, 75:571-583.
                                    </a>
                                </li>
                                <li id="141">


                                    <a id="bibliography_9" title=" I Dryanovski, et al.Fast visual odometry and mapping from RGB-D data[C].IEEE International Conference on Robotics and Automation IEEE, 2013:2305-2310." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fast visual odometry and mapping from RGB-D data">
                                        <b>[9]</b>
                                         I Dryanovski, et al.Fast visual odometry and mapping from RGB-D data[C].IEEE International Conference on Robotics and Automation IEEE, 2013:2305-2310.
                                    </a>
                                </li>
                                <li id="143">


                                    <a id="bibliography_10" title=" Segal, et al.Generalized-ICP[J].In Robotics:science and systems, 2009-2:435" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Generalized-ICP.">
                                        <b>[10]</b>
                                         Segal, et al.Generalized-ICP[J].In Robotics:science and systems, 2009-2:435
                                    </a>
                                </li>
                                <li id="145">


                                    <a id="bibliography_11" title=" D Chetverikov, et al.The Trimmed Iterative Closest Point Algorithm[C].2002 16 Th International Conference on Pattern Recognition.IEEE Computer Society, 2002:30545." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The trimmed iterative closest point algorithm">
                                        <b>[11]</b>
                                         D Chetverikov, et al.The Trimmed Iterative Closest Point Algorithm[C].2002 16 Th International Conference on Pattern Recognition.IEEE Computer Society, 2002:30545.
                                    </a>
                                </li>
                                <li id="147">


                                    <a id="bibliography_12" title=" L Maier-Hein, et al.Convergent iterative closest-point algorithm to accomodate anisotropic and inhomogenous localization error[J].IEEE transactions on pattern analysis and machine intelligence, 2012, 34 (8) :1520-1532." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Convergent Iterative Closest-Point Algorithm to Accomodate Anisotropic and Inhomogenous Localization Error">
                                        <b>[12]</b>
                                         L Maier-Hein, et al.Convergent iterative closest-point algorithm to accomodate anisotropic and inhomogenous localization error[J].IEEE transactions on pattern analysis and machine intelligence, 2012, 34 (8) :1520-1532.
                                    </a>
                                </li>
                                <li id="149">


                                    <a id="bibliography_13" title=" T Lin, et al.Motion-Aware Iterative Closest Point Estimation for Fast Visual Odometry[C].2017 IEEE International Symposium on Mixed and Augmented Reality IEEE, 2017:268-269." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Motion-Aware Iterative Closest Point Estimation for Fast Visual Odometry">
                                        <b>[13]</b>
                                         T Lin, et al.Motion-Aware Iterative Closest Point Estimation for Fast Visual Odometry[C].2017 IEEE International Symposium on Mixed and Augmented Reality IEEE, 2017:268-269.
                                    </a>
                                </li>
                                <li id="151">


                                    <a id="bibliography_14" title=" P Besl, N Mckay.A method for registration of 3-D shapes[J].Sensor Fusion IV:Control Paradigms and Data Structures Sensor Fusion IV:Control Paradigms and Data Structures, 1992:239-256." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Method for registration of 3-D shapes">
                                        <b>[14]</b>
                                         P Besl, N Mckay.A method for registration of 3-D shapes[J].Sensor Fusion IV:Control Paradigms and Data Structures Sensor Fusion IV:Control Paradigms and Data Structures, 1992:239-256.
                                    </a>
                                </li>
                                <li id="153">


                                    <a id="bibliography_15" title=" Sturm, et al.A benchmark for the evaluation of RGB-D SLAM systems[C].2012 IEEE International Conference on Intelligent Robots and Systems IEEE, 2012:573-580." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A benchmark for the evaluation of RGB-D SLAM systems">
                                        <b>[15]</b>
                                         Sturm, et al.A benchmark for the evaluation of RGB-D SLAM systems[C].2012 IEEE International Conference on Intelligent Robots and Systems IEEE, 2012:573-580.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJZ" target="_blank">计算机仿真</a>
                2019,36(08),222-226             </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于空间约束ICP的改进视觉里程计</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%AB%A0%E5%BC%98&amp;code=42669778&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">章弘</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%83%A1%E5%A3%AB%E5%BC%BA&amp;code=08516966&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">胡士强</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%8A%E6%B5%B7%E4%BA%A4%E9%80%9A%E5%A4%A7%E5%AD%A6%E8%88%AA%E7%A9%BA%E8%88%AA%E5%A4%A9%E5%AD%A6%E9%99%A2&amp;code=0054402&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">上海交通大学航空航天学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>针对传统的迭代最近点 (Iterative Closest Point, ICP) 算法在视觉里程计应用中存在的外点干扰与相机偶然大运动导致的位姿估计不准确或失效问题, 提出基于空间约束ICP算法的相机位姿估计方法与帧对模型与帧间估计的切换策略, 提高位姿估计准确度和鲁棒性。方法利用彩色深度相机的数学模型和空间约束为每帧图像提取的三维点云中的点赋予不同权值, 并在ICP算法起始阶段估计当前帧点云与模型点云重叠率, 减少外点对位姿估计的影响。为应对相机的偶然大运动, 提出帧间位姿计算与帧对模型位姿计算的切换策略。实验在标准测试数据集下, 与典型视觉里程计对比有效提高小规模运动下位姿估计的准确度, 同时解决了相机偶然大运动下系统失效问题。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">位姿估计;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%A7%86%E8%A7%89%E9%87%8C%E7%A8%8B%E8%AE%A1&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">视觉里程计;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%BF%AD%E4%BB%A3%E6%9C%80%E8%BF%91%E7%82%B9&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">迭代最近点;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8D%A1%E5%B0%94%E6%9B%BC%E6%BB%A4%E6%B3%A2%E5%99%A8&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">卡尔曼滤波器;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    章弘 (1993-) , 男 (汉族) , 江西省鹰潭人, 硕士研究生, 主要研究领域为计算机视觉与图形学。;
                                </span>
                                <span>
                                    胡士强 (1969-) , 男 (汉族) , 河北人, 教授, 博士研究生导师, 主要研究领域为信息融合技术与图像理解与分析。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-04-28</p>

            </div>
                    <h1><b>An Improved Visual Odometry Based on AICP</b></h1>
                    <h2>
                    <span>HONG Zhang</span>
                    <span>HU Shi-qiang</span>
            </h2>
                    <h2>
                    <span>School of Aeronautics and Astronautics, Shanghai Jiaotong University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>To avoid using prior information and motion assumptions of object and to address the error drift, a sliding window filter based object pose estimation algorithm is proposed. The algorithm requires no prior information and motion assumptions and avoids the error drift. To estimate the pose of an arbitrary object, Gauss-Newton method is used within each window to optimize the pose and structure iteratively. To tackle the error drift, Kalman filter and the strong coupling of pose and structure are explored by filtering the structure of object. Structure filtering increases the accuracy of structure estimation and thus the accuracy of estimated pose is increased without error drift. Simulation shows that the proposed algorithm can eliminate the error drift effectively and enhance the accuracy of existing method. Without prior information and assumption, the proposed algorithm can achieve the close estimation results of state-of-art methods.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Pose%20estimation&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Pose estimation;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Visual%20odometry&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Visual odometry;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Iterative%20closest%20point%20(ICP)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Iterative closest point (ICP) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Kalman%20filter&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Kalman filter;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-04-28</p>
                            </div>


        <!--brief start-->
                        <h3 id="33" name="33" class="anchor-tag"><b>1 引言</b></h3>
                <div class="p1">
                    <p id="34">视觉里程计 (Visual Odometry, VO) 旨在利用相机输入的视频, 实时估计相机相对于场景的六自由度位姿。视觉里程计在移动机器人视觉导航<citation id="155" type="reference"><link href="125" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>, 无人机, 3D重建等应用中有非常重要的地位<citation id="156" type="reference"><link href="127" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="35">按照利用信息方式的不同, 视觉里程计大致可分为两类:基于特征点的视觉里程计方法和基于稠密的视觉里程计方法。基于特征点的视觉里程计方法, 先提取彩色图像特征点并对其进行匹配, 在利用随机抽样一致 (random sample consensus, RANSAC) <citation id="157" type="reference"><link href="129" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation> 算法进行外点剔除后, 结合相机模型进行相机位姿估计。其中特征点主要有2D特征点, 如SIFT<citation id="158" type="reference"><link href="131" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>, SURF和ORB <citation id="159" type="reference"><link href="133" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>, 和3D特征点<citation id="160" type="reference"><link href="135" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>等。基于特征点的视觉里程计算法较为成熟, 而且特征点提取稳定时, 位姿估计准确度有所保证。但该类算法特征点匹配速度较慢, 有时不能满足视觉里程计对实时性的较高要求。</p>
                </div>
                <div class="p1">
                    <p id="36">基于稠密的视觉里程计方法, 指直接利用全幅图像的信息进行位姿估计。主要利用图像灰度值信息<citation id="161" type="reference"><link href="137" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>与深度图像的深度值<citation id="162" type="reference"><link href="139" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>, 以及图像中的几何空间信息<citation id="163" type="reference"><link href="141" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>等。相比基于特征点方法, 基于稠密的视觉里程计使用图像中更多的信息, 因此其精度与鲁棒性高, 但是信息量的增加也导致需要使用GPU设备来完成大量信息的处理。</p>
                </div>
                <div class="p1">
                    <p id="37">ICP作为经典3D点云位姿解算算法及其改进算法如G-ICP<citation id="164" type="reference"><link href="143" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>不仅适用于基于稠密的视觉里程计方法, 而且适用于基于特征点的视觉里程计方法。但仍旧存在一些关键问题, 如传统ICP算法基于配准的点云为目标点云子集的假设, 即在目标点云中必能找到与源点云一一对应的点对。此外, ICP受初值影响容易陷入局部最小。因此, 实际在视觉里程计的应用中难以保证两条假设, 从而导致位姿计算精度与系统鲁棒性下降。</p>
                </div>
                <div class="p1">
                    <p id="38">针对上述两个ICP在视觉里程计中应用的问题, 以及基于特征点的视觉里程计方法特征点匹配速度较慢的问题, 本文在文献<citation id="165" type="reference">[<a class="sup">9</a>]</citation>的基础上, 提出一种基于改进ICP的视觉里程计方法, 该方法基于特征点但不需要专门的特征点匹配。首先, 本文参考Trimmed ICP<citation id="166" type="reference"><link href="145" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>提出了一种自适应部分重叠率参数估计机制, 针对不同的视频输入自动估计不同的部分重叠率, 只利用部分可靠并且满足一一匹配关系的特征点进行位姿估计, 以提高ICP位姿解算准确度。其次, 利用空间关系与相机模型, 对其赋予不同的观测权重, 以削弱不稳定特征点对于位姿估计的影响, 并采用各向异性ICP (Anisotropic Iterative Closest Point, AICP) <citation id="167" type="reference"><link href="147" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>进一步提高位姿估计准确度。此外, 本文还参考文献<citation id="168" type="reference">[<a class="sup">9</a>]</citation>中卡尔曼滤波的思想, 对不同3D点的权重进行滤波, 以间接结合过去帧的信息的方式, 提高估计准确度。最后, 在偶遇相机大运动时, 提出两条准则评价当前相机运动状态与位姿估计结果, 如果符合准则那么由帧与模型匹配退化为帧间匹配作为过渡, 一定程度上直接提升了系统的鲁棒性。</p>
                </div>
                <h3 id="39" name="39" class="anchor-tag"><b>2 相机模型与点云不确定度模型的建立</b></h3>
                <h4 class="anchor-tag" id="40" name="40"><b>2.1 相机模型的建立与分析</b></h4>
                <div class="p1">
                    <p id="41">本文采用RGB-D相机的观测作为视觉里程计的输入, 其中特征点的像素坐标为<i>P</i><sub><i>RGB</i></sub>= (<i>u</i>, <i>v</i>) <sup><i>T</i></sup>, 根据彩色-深度预校准, 可获得对应点的深度值为<i>d</i>。利用相机投影模型, 可得到特征点在相机坐标系下的3<i>D</i>坐标<i>q</i>= (<i>x</i>, <i>y</i>, <i>z</i>) <sup><i>T</i></sup>为</p>
                </div>
                <div class="p1">
                    <p id="42" class="code-formula">
                        <mathml id="42"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mi>z</mi><mo>=</mo><mfrac><mrow><mi>Ζ</mi><msub><mrow></mrow><mn>0</mn></msub></mrow><mrow><mn>1</mn><mo>+</mo><mfrac><mrow><mi>Ζ</mi><msub><mrow></mrow><mn>0</mn></msub></mrow><mrow><mi>f</mi><mi>b</mi></mrow></mfrac><mi>d</mi></mrow></mfrac></mtd></mtr><mtr><mtd><mi>x</mi><mo>=</mo><mfrac><mi>z</mi><mrow><mi>f</mi><msub><mrow></mrow><mi>x</mi></msub></mrow></mfrac><mo stretchy="false"> (</mo><mi>u</mi><mo>-</mo><mi>c</mi><msub><mrow></mrow><mi>x</mi></msub><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi>y</mi><mo>=</mo><mfrac><mi>z</mi><mrow><mi>f</mi><msub><mrow></mrow><mi>y</mi></msub></mrow></mfrac><mo stretchy="false"> (</mo><mi>v</mi><mo>-</mo><mi>c</mi><msub><mrow></mrow><mi>y</mi></msub><mo stretchy="false">) </mo></mtd></mtr></mtable></mrow><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="43">式中, <i>Z</i><sub>0</sub>为相机跟参考平面之间的距离, <i>b</i>为红外发射端到相机的距离, <i>Z</i><sub>0</sub>和<i>b</i>均为<i>RGB</i>-<i>D</i>相机内结构参数;<i>f</i>为深度相机的焦距, <i>f</i><sub><i>x</i></sub><i>f</i><sub><i>y</i></sub>为彩色相机的焦距值, <i>c</i><sub><i>x</i></sub><i>c</i><sub><i>y</i></sub>为光心于图像平面的坐标。第 <i>k</i> 帧全部特征点的3<i>D</i>坐标则构成当前帧的点云模型 <i>D</i><sub><i>k</i></sub>。而<i>M</i><sub><i>k</i>-1</sub>为由第1帧图像到第<i>k</i>-1帧图像融合得到的世界坐标系下的点云模型。因此, 第<i>k</i>帧相机相对于世界坐标系的位姿 <i>T</i><sub><i>k</i></sub> 可通过<i>ICP</i>算法进行解算。因此, 基于<i>ICP</i>的视觉里程计的方法核心在于确定合理的点云模型 <i>D</i><sub><i>k</i></sub> 和 <i>M</i><sub><i>k</i>-1</sub>。</p>
                </div>
                <h4 class="anchor-tag" id="44" name="44"><b>2.2 点云模型不确定度建模</b></h4>
                <div class="p1">
                    <p id="45">根据文献<citation id="169" type="reference">[<a class="sup">9</a>]</citation>可知, 由深度相机采集的深度值<i>d</i>存在噪声, 并假定噪声符合高斯分布<i>N</i>～ (0, <i>σ</i><sub><i>d</i></sub>) , 因此可得到点云模型中<i>z</i>方向的标准差为</p>
                </div>
                <div class="p1">
                    <p id="46" class="code-formula">
                        <mathml id="46"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>σ</mi><msub><mrow></mrow><mi>z</mi></msub><mo>=</mo><mfrac><mn>1</mn><mrow><mi>f</mi><mi>b</mi></mrow></mfrac><mi>σ</mi><msub><mrow></mrow><mi>d</mi></msub><mi>μ</mi><msubsup><mrow></mrow><mi>z</mi><mn>2</mn></msubsup><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="47">将相机内参数带入上述方程可得</p>
                </div>
                <div class="p1">
                    <p id="48" class="code-formula">
                        <mathml id="48"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>σ</mi><msub><mrow></mrow><mi>z</mi></msub><mo>=</mo><mn>1</mn><mo>.</mo><mn>4</mn><mn>5</mn><mo>×</mo><mn>1</mn><mn>0</mn><msup><mrow></mrow><mrow><mo>-</mo><mn>3</mn></mrow></msup><mi>μ</mi><msubsup><mrow></mrow><mi>z</mi><mn>2</mn></msubsup><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo><mtext> </mtext></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="49">式中, <i>μ</i><sub><i>z</i></sub>为特征点在相机坐标系下<i>z</i>方向的均值。由于彩色相机同样存在噪声, 因此假设特征点的<i>x y</i>坐标服从高斯分布, 并设坐标均值为<i>μ</i>=[<i>μ</i><sub><i>x</i></sub>, <i>μ</i><sub><i>y</i></sub>, <i>μ</i><sub><i>z</i></sub>]<sup><i>T</i></sup>方差为<i>Σ</i>。利用式 (1) 可得到方差的<i>Σ</i>计算公式为</p>
                </div>
                <div class="p1">
                    <p id="50" class="code-formula">
                        <mathml id="50"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Σ</mi><mo>=</mo><mrow><mo>[</mo><mrow><mtable><mtr><mtd><mi>σ</mi><msubsup><mrow></mrow><mi>x</mi><mn>2</mn></msubsup></mtd><mtd><mi>σ</mi><msub><mrow></mrow><mrow><mi>x</mi><mi>y</mi></mrow></msub></mtd><mtd><mi>σ</mi><msub><mrow></mrow><mrow><mi>x</mi><mi>z</mi></mrow></msub></mtd></mtr><mtr><mtd><mi>σ</mi><msub><mrow></mrow><mrow><mi>y</mi><mi>x</mi></mrow></msub></mtd><mtd><mi>σ</mi><msubsup><mrow></mrow><mi>y</mi><mn>2</mn></msubsup></mtd><mtd><mi>σ</mi><msub><mrow></mrow><mrow><mi>y</mi><mi>z</mi></mrow></msub></mtd></mtr><mtr><mtd><mi>σ</mi><msub><mrow></mrow><mrow><mi>z</mi><mi>x</mi></mrow></msub></mtd><mtd><mi>σ</mi><msub><mrow></mrow><mrow><mi>z</mi><mi>y</mi></mrow></msub></mtd><mtd><mi>σ</mi><msubsup><mrow></mrow><mi>z</mi><mn>2</mn></msubsup></mtd></mtr></mtable></mrow><mo>]</mo></mrow><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo><mspace width="0.25em" /></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="51">式中</p>
                </div>
                <div class="p1">
                    <p id="52" class="code-formula">
                        <mathml id="52"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mi>σ</mi><msubsup><mrow></mrow><mi>x</mi><mn>2</mn></msubsup><mo>=</mo><mfrac><mrow><mi>σ</mi><msubsup><mrow></mrow><mi>z</mi><mn>2</mn></msubsup><mo stretchy="false"> (</mo><mi>μ</mi><msub><mrow></mrow><mi>u</mi></msub><mo>-</mo><mi>c</mi><msub><mrow></mrow><mi>x</mi></msub><mo stretchy="false">) </mo><mo stretchy="false"> (</mo><mi>μ</mi><msub><mrow></mrow><mi>v</mi></msub><mo>-</mo><mi>c</mi><msub><mrow></mrow><mi>y</mi></msub><mo stretchy="false">) </mo><mo>+</mo><mi>σ</mi><msubsup><mrow></mrow><mi>u</mi><mn>2</mn></msubsup><mo stretchy="false"> (</mo><mi>μ</mi><msubsup><mrow></mrow><mi>z</mi><mn>2</mn></msubsup><mo>+</mo><mi>σ</mi><msubsup><mrow></mrow><mi>z</mi><mn>2</mn></msubsup><mo stretchy="false">) </mo></mrow><mrow><mi>f</mi><mspace width="0.25em" /><msubsup><mrow></mrow><mi>x</mi><mn>2</mn></msubsup></mrow></mfrac></mtd></mtr><mtr><mtd><mi>σ</mi><msubsup><mrow></mrow><mi>y</mi><mn>2</mn></msubsup><mo>=</mo><mfrac><mrow><mi>σ</mi><msubsup><mrow></mrow><mi>z</mi><mn>2</mn></msubsup><mo stretchy="false"> (</mo><mi>μ</mi><msub><mrow></mrow><mi>u</mi></msub><mo>-</mo><mi>c</mi><msub><mrow></mrow><mi>x</mi></msub><mo stretchy="false">) </mo><mo stretchy="false"> (</mo><mi>μ</mi><msub><mrow></mrow><mi>v</mi></msub><mo>-</mo><mi>c</mi><msub><mrow></mrow><mi>y</mi></msub><mo stretchy="false">) </mo><mo>+</mo><mi>σ</mi><msubsup><mrow></mrow><mi>v</mi><mn>2</mn></msubsup><mo stretchy="false"> (</mo><mi>μ</mi><msubsup><mrow></mrow><mi>z</mi><mn>2</mn></msubsup><mo>+</mo><mi>σ</mi><msubsup><mrow></mrow><mi>z</mi><mn>2</mn></msubsup><mo stretchy="false">) </mo></mrow><mrow><mi>f</mi><mspace width="0.25em" /><msubsup><mrow></mrow><mi>y</mi><mn>2</mn></msubsup></mrow></mfrac></mtd></mtr><mtr><mtd><mi>σ</mi><msub><mrow></mrow><mrow><mi>x</mi><mi>z</mi></mrow></msub><mo>=</mo><mi>σ</mi><msub><mrow></mrow><mrow><mi>z</mi><mi>x</mi></mrow></msub><mo>=</mo><mi>σ</mi><msubsup><mrow></mrow><mi>z</mi><mn>2</mn></msubsup><mfrac><mrow><mi>μ</mi><msub><mrow></mrow><mi>u</mi></msub><mo>-</mo><mi>c</mi><msub><mrow></mrow><mi>x</mi></msub></mrow><mrow><mi>f</mi><msub><mrow></mrow><mi>x</mi></msub></mrow></mfrac></mtd></mtr><mtr><mtd><mi>σ</mi><msub><mrow></mrow><mrow><mi>y</mi><mi>z</mi></mrow></msub><mo>=</mo><mi>σ</mi><msub><mrow></mrow><mrow><mi>z</mi><mi>y</mi></mrow></msub><mo>=</mo><mi>σ</mi><msubsup><mrow></mrow><mi>z</mi><mn>2</mn></msubsup><mfrac><mrow><mi>μ</mi><msub><mrow></mrow><mi>v</mi></msub><mo>-</mo><mi>c</mi><msub><mrow></mrow><mi>y</mi></msub></mrow><mrow><mi>f</mi><msub><mrow></mrow><mi>y</mi></msub></mrow></mfrac></mtd></mtr><mtr><mtd><mi>σ</mi><msub><mrow></mrow><mrow><mi>x</mi><mi>y</mi></mrow></msub><mo>=</mo><mi>σ</mi><msub><mrow></mrow><mrow><mi>y</mi><mi>x</mi></mrow></msub><mo>=</mo><mi>σ</mi><msubsup><mrow></mrow><mi>z</mi><mn>2</mn></msubsup><mfrac><mrow><mo stretchy="false"> (</mo><mi>μ</mi><msub><mrow></mrow><mi>u</mi></msub><mo>-</mo><mi>c</mi><msub><mrow></mrow><mi>x</mi></msub><mo stretchy="false">) </mo><mo stretchy="false"> (</mo><mi>μ</mi><msub><mrow></mrow><mi>v</mi></msub><mo>-</mo><mi>c</mi><msub><mrow></mrow><mi>y</mi></msub><mo stretchy="false">) </mo></mrow><mrow><mi>f</mi><msub><mrow></mrow><mi>x</mi></msub><mi>f</mi><msub><mrow></mrow><mi>y</mi></msub></mrow></mfrac></mtd></mtr></mtable></mrow><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>5</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="53">由于每个点相机坐标系下坐标可以看出点云模型中每个点在相机坐标系下的3<i>D</i>坐标<i>q</i>= (<i>x</i>, <i>y</i>, <i>z</i>) <sup><i>T</i></sup>不同, 因此每个点的协方差矩阵也有所不同, 这为后续采用<i>AICP</i>算法解算位姿奠定了数学基础。</p>
                </div>
                <div class="area_img" id="54">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJZ201908046_054.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 特征点稀疏点云模型" src="Detail/GetImg?filename=images/JSJZ201908046_054.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图1 特征点稀疏点云模型</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJZ201908046_054.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h3 id="55" name="55" class="anchor-tag"><b>3 视觉里程计轨迹估计</b></h3>
                <h4 class="anchor-tag" id="56" name="56"><b>3.1 算法框架</b></h4>
                <div class="p1">
                    <p id="57">本文采用特征点云的作为图像信息输入进行相机位姿估计。为一定程度上抑制误差累积, 本文结合当前帧点云模型<b>D</b><sub><b>k</b></sub> 和世界坐标系下点云模型 <b>M</b><sub><b>k-1</b></sub> 的信息进行位姿估计, 之后采用卡尔曼滤波对模型<b>M</b><sub><b>k-1</b></sub>进行融合得到<b>M</b><sub><b>k</b></sub>, 以提高模型<b>M</b><sub><b>k</b></sub>的准确度。因此, 视觉里程计算法框架如图<b>2</b>所示。</p>
                </div>
                <div class="area_img" id="58">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJZ201908046_058.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 视觉里程计算法框架" src="Detail/GetImg?filename=images/JSJZ201908046_058.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图2 视觉里程计算法框架</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJZ201908046_058.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="59">算法首先提取<b>2D</b>彩色图像中<b><i>Shi</i>-<i>Tomasi</i></b>特征并结合对应的深度信息构造当前帧的点云模型<b>D</b><sub><b>k</b></sub>。通过与世界坐标系下点云模型<b>M</b><sub><b>k-1</b></sub>的配准, 估计部分重叠率, 用于剔除配准中产生的外点。为了更好的利用点云信息, 位姿估计对不同特征点赋予不同权重, 并采用<b>AICP</b>和非线性迭代分解做位姿解算, <b>AICP</b>的初值采用上一帧的位姿。最后采用卡尔曼滤波对点云模型<b>M</b><sub><b>k</b>-</sub>更新。</p>
                </div>
                <div class="p1">
                    <p id="60">正由于使用上一帧点云计算得到的位姿<b>T</b><sub><b>k-1</b></sub>作为当前帧点云位姿计算的初值, 当偶遇一个相机大运动时, <b>ICP</b>算法位姿计算的结果将会受到直接的影响。此时, 文献<citation id="170" type="reference">[<a class="sup">13</a>]</citation>提出两条准则。在判断过后, 本文选择跳过当前帧, 暂时转而计算下一帧<b>D</b><sub><b>k+1</b></sub>与前一帧<b>D</b><sub><b>k-1</b></sub>的相对位姿, 此时采用传统的特征点描述子来进行匹配, 并利用<b>RANSAC</b>算法剔除外点, 在获得一一对应的特征点后, 再使用原始的<b>ICP</b>算法进行位姿计算。</p>
                </div>
                <h4 class="anchor-tag" id="61" name="61"><b>3.2 基于空间约束ICP的位姿估计算法</b></h4>
                <div class="p1">
                    <p id="62">传统ICP算法<citation id="171" type="reference"><link href="151" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>的主要思路为:先采用欧式距离度量对点云模型D<sub>k</sub>与M<sub>k-<b>1</b></sub>进行最近点检索, 之后固定该配准关系求解距离和的最小二乘问题获得位姿。当得到位姿后, 将点云D<sub>k</sub>旋转平移置M<sub>k-<b>1</b></sub>所在坐标系下, 寻找下一次迭代的最近点, 之后再进行位姿解算, 并迭代操作直到位姿收敛。</p>
                </div>
                <div class="p1">
                    <p id="63">对于上述传统ICP算法需要满足点云模型D<sub>k</sub>与点云模型M<sub>k-<b>1</b></sub> 具有一一对应的匹配关系, 即N<sub>d</sub>=N<sub>m</sub>。但实际情况下, 由于相机的运动, 模型D<sub>k</sub>中可能存在新的特征点, 即外点, 因此N<sub>d</sub>与N<sub>m</sub>并不相等。当N<sub>d</sub>≠N<sub>m</sub>时, 位姿估计会受到新加外点的影响。基于此, 本文通过产生一个自适应部分重叠率, 剔除外点, 已保证位姿估计的准确度。</p>
                </div>
                <div class="p1">
                    <p id="64">文献<citation id="172" type="reference">[<a class="sup">11</a>]</citation>中的思路, 在进行位姿计算前需要估计当前帧点云与模型点云的最小部分重叠率ξ=[α, β], 该值表示了当前帧点云与模型点云之间的最小重叠百分比, 通过优化以下目标函数可以得到此值:</p>
                </div>
                <div class="p1">
                    <p id="65" class="code-formula">
                        <mathml id="65"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ψ</mi><mo stretchy="false"> (</mo><mi>ξ</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mrow><mi>e</mi><mo stretchy="false"> (</mo><mi>ξ</mi><mo stretchy="false">) </mo></mrow><mrow><mi>ξ</mi><msup><mrow></mrow><mrow><mn>1</mn><mo>+</mo><mi>λ</mi></mrow></msup></mrow></mfrac><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>6</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="66">式中, λ是一个预设参数 (通常选取λ=<b>2</b>) , e (ξ) 是第一次最近点搜索后, 各点对之间的平均距离。它是一个与部分重叠率ξ有关的函数, 即部分重叠率ξ表示选取点对的百分比。一旦得到了部分重叠率ξ的估计值后, 该值随即固定, 那么在每次ICP算法迭代的最近点搜索步骤之后选取当前帧点云数量为N<sub>d’</sub>=ξN<sub>d</sub>的点来进行ICP后续位姿解算。</p>
                </div>
                <div class="p1">
                    <p id="67">考虑到不同特征点其不确定度有所不同, 位姿解算采用各向异性的ICP算法。首先, 区别于欧氏距离, 本文考虑采用马氏距离寻找点云模型的最近点, 即点d<sub>i</sub>与m<sub>j</sub>之间的距离公式为</p>
                </div>
                <div class="p1">
                    <p id="68" class="code-formula">
                        <mathml id="68"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>d</mi><mi>i</mi><mi>s</mi><mi>t</mi><mo stretchy="false"> (</mo><mi>d</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>m</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><msup><mrow></mrow><mn>2</mn></msup><mo>=</mo><mo stretchy="false"> (</mo><mi>μ</mi><msub><mrow></mrow><mrow><mi>d</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msub><mo>-</mo><mi>μ</mi><msub><mrow></mrow><mrow><mi>m</mi><msub><mrow></mrow><mi>j</mi></msub></mrow></msub><mo stretchy="false">) </mo><msup><mrow></mrow><mi>Τ</mi></msup><mo stretchy="false"> (</mo><mi>Σ</mi><msub><mrow></mrow><mrow><mi>d</mi><msub><mrow></mrow><mi>i</mi></msub><mi>m</mi><msub><mrow></mrow><mi>j</mi></msub></mrow></msub><mo stretchy="false">) </mo><msup><mrow></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo stretchy="false"> (</mo><mi>μ</mi><msub><mrow></mrow><mrow><mi>d</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msub><mo>-</mo><mi>μ</mi><msub><mrow></mrow><mrow><mi>m</mi><msub><mrow></mrow><mi>j</mi></msub></mrow></msub><mo stretchy="false">) </mo><mspace width="0.25em" /><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>7</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="69">式中, μ<sub>d<sub>i</sub></sub>与μ<sub>m<sub>j</sub></sub>为两点<b>3</b>D坐标的均值, Σ<sub>d<sub>i</sub>m<sub>j</sub></sub>为两个点云空间的协方差矩阵, 其计算公式为Σ<sub>d<sub>i</sub>m<sub>j</sub></sub>=Σ<sub>d<sub>i</sub></sub>+Σ<sub>m<sub>j</sub></sub>。对于D<sub>k</sub>中的每一个特征点, 在模型M<sub>k-<b>1</b></sub>中以欧氏距离为度量找k个最近距离的点, 再以马氏距离为度量找一个最近的点作为其匹配点</p>
                </div>
                <div class="p1">
                    <p id="70" class="code-formula">
                        <mathml id="70"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><msub><mrow></mrow><mrow><mi>n</mi><mi>e</mi><mi>w</mi></mrow></msub><mo stretchy="false"> (</mo><mi>d</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mover accent="true"><mi>Μ</mi><mo stretchy="true">˜</mo></mover><mo stretchy="false">) </mo><mo>=</mo><mi>arg</mi><mspace width="0.25em" /><mtext>m</mtext><mtext>i</mtext><mtext>n</mtext><mi>d</mi><mi>i</mi><mi>s</mi><mi>t</mi><mo stretchy="false"> (</mo><mi>d</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>m</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>8</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="71">其中<mathml id="72"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi mathvariant="bold">Μ</mi><mo stretchy="true">˜</mo></mover></mrow></math></mathml>为通过欧氏距离找到的k个最近点的集合。位姿的解算通过优化以下误差指标来求解位姿{R<sub>k</sub>, t<sub>k</sub>}</p>
                </div>
                <div class="p1">
                    <p id="73" class="code-formula">
                        <mathml id="73"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>F</mi><mi>R</mi><mi>E</mi><msup><mrow></mrow><mn>2</mn></msup><mo stretchy="false"> (</mo><mi>R</mi><msub><mrow></mrow><mi>k</mi></msub><mo>, </mo><mi>t</mi><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false">) </mo><mo>=</mo></mtd></mtr><mtr><mtd><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>Ν</mi><mi>d</mi></mrow></munderover><mo stretchy="false"> (</mo></mstyle><mi>R</mi><msub><mrow></mrow><mi>k</mi></msub><mi>d</mi><msub><mrow></mrow><mi>i</mi></msub><mo>+</mo><mi>t</mi><msub><mrow></mrow><mi>k</mi></msub><mo>-</mo><mi>m</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><msup><mrow></mrow><mi>Τ</mi></msup><mo stretchy="false"> (</mo><mi>Σ</mi><msub><mrow></mrow><mrow><mi>d</mi><msub><mrow></mrow><mi>i</mi></msub><mi>m</mi><msub><mrow></mrow><mi>j</mi></msub></mrow></msub><mo stretchy="false">) </mo><msup><mrow></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo stretchy="false"> (</mo><mi>R</mi><msub><mrow></mrow><mi>k</mi></msub><mi>d</mi><msub><mrow></mrow><mi>i</mi></msub><mo>+</mo><mi>t</mi><msub><mrow></mrow><mi>k</mi></msub><mo>-</mo><mi>m</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mspace width="0.25em" /><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>9</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="74">式中d<sub>i</sub>与m<sub>j</sub>的对应关系由式 (<b>8</b>) 确定。式 (<b>9</b>) 可以采用非线性优化算法进行迭代求解。</p>
                </div>
                <h4 class="anchor-tag" id="75" name="75"><b>3.3 帧对模型与帧间匹配的切换策略</b></h4>
                <div class="p1">
                    <p id="76">为了应对相机的偶然大运动, 鉴于文献<citation id="173" type="reference">[<a class="sup">14</a>]</citation>提出了两条准则以评价上述改进ICP算法的效果</p>
                </div>
                <div class="p1">
                    <p id="77" class="code-formula">
                        <mathml id="77"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>e</mi><mo stretchy="false"> (</mo><mi>ξ</mi><mo stretchy="false">) </mo><msub><mrow></mrow><mrow><mtext>f</mtext><mtext>i</mtext><mtext>r</mtext><mtext>s</mtext><mtext>t</mtext></mrow></msub><mo>=</mo><mfrac><mn>1</mn><mrow><mi>Ν</mi><msub><mrow></mrow><msup><mi>d</mi><mo>′</mo></msup></msub></mrow></mfrac><mstyle displaystyle="true"><munder><mo>∑</mo><mi>i</mi></munder><mrow><mrow><mo>|</mo><mrow><mi>d</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mi>m</mi><msub><mrow></mrow><mi>j</mi></msub></mrow><mo>|</mo></mrow></mrow></mstyle><mo>≥</mo><mi>α</mi><msub><mrow></mrow><mn>1</mn></msub><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>0</mn><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mi>e</mi><mo stretchy="false"> (</mo><mi>ξ</mi><mo stretchy="false">) </mo><msub><mrow></mrow><mrow><mtext>l</mtext><mtext>a</mtext><mtext>s</mtext><mtext>t</mtext></mrow></msub><mo>=</mo><mfrac><mn>1</mn><mrow><mi>Ν</mi><msub><mrow></mrow><msup><mi>d</mi><mo>′</mo></msup></msub></mrow></mfrac><mstyle displaystyle="true"><munder><mo>∑</mo><mi>i</mi></munder><mrow><mrow><mo>|</mo><mrow><mo stretchy="false"> (</mo><mi>R</mi><mi>d</mi><msub><mrow></mrow><mi>i</mi></msub><mo>+</mo><mi>t</mi><mo stretchy="false">) </mo><mo>-</mo><mi>m</mi><msub><mrow></mrow><mi>j</mi></msub></mrow><mo>|</mo></mrow></mrow></mstyle><mo>≥</mo><mi>α</mi><msub><mrow></mrow><mn>2</mn></msub><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>1</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="78">式中, R与t是改进ICP算法的估计结果, e (ξ) <sub>first</sub>表示第一次ICP最近点检索后点对的距离平均值, e (ξ) <sub>last</sub>表示ICP迭代完成时最近点检索后点对的距离平均值。上述两个值分别显示了ICP算法位姿计算前后的两个点云的空间距离, 当这两个阈值其中有一个超出阈值可以判定相机偶遇大运动情况。</p>
                </div>
                <div class="p1">
                    <p id="79">在此情况下, 跳过点云质量可能下降的当前帧, 转而利用传统的特征描述子匹配与RANSAC算法来匹配下一帧与前一帧的特征点对并得到位姿的粗略估计, 再使用传统的ICP进行位姿的进一步优化。此处将同时丢弃模型点云, 并且从下一帧点云进行重建。</p>
                </div>
                <h4 class="anchor-tag" id="80" name="80"><b>3.4 基于卡尔曼滤波器的点云模型更新</b></h4>
                <div class="p1">
                    <p id="81">当得到当前帧位姿T<sub>k</sub> 时, 通过T<sub>k</sub>将点云D<sub>k</sub>转换到世界坐标系下得到D<sub>wk</sub>, 选取空间距离小于一定阈值的点对与M<sub>k-<b>1</b></sub>融合, 而超出阈值的点对作为新出现的点云数据加入模型点云中, 可得到当前时刻世界坐标系下的更新点云模型M<sub>k</sub>。其中, 模型的融合采用卡尔曼滤波器。卡尔曼滤波器的状态为点云模型的<b>3</b>D坐标, 其运动模型为刚体性质约束, 观测模型的输入为D'<sub>wk</sub>, D'<sub>wk</sub>为点云模型 D<sub>wk</sub> 中匹配到的特征点的<b>3</b>D坐标。对于第k 帧新观测到的特征点D<sub>wk</sub>-D'<sub>wk</sub>则将其直接存储于点云模型M<sub>k</sub>。考虑到存储空间的有限, 当点云模型M<sub>k</sub>中点的数目超出一定阈值时, 将随机地从模型中删除帧数较早的一部分特征点。</p>
                </div>
                <div class="p1">
                    <p id="82">基于卡尔曼滤波器的点云模型更新随着时间的增加, 模型M<sub>k-<b>1</b></sub>所融合的信息量也相应增加, 因此模型准确度也会增加。由于AICP需要将M<sub>k-<b>1</b></sub>与D<sub>k</sub>做匹配, 并利用匹配关系进行位姿估计, M<sub>k-<b>1</b></sub>准确度的提高也会提高位姿估计的准确度。</p>
                </div>
                <h3 id="83" name="83" class="anchor-tag"><b>4 实验与分析</b></h3>
                <div class="p1">
                    <p id="84">实验首先验证自适应部分重叠率引入对于位姿计算精度的提高;其次将本文算法与改进前的基于<i>ICP</i>的视觉里程计方法<citation id="174" type="reference"><link href="141" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>、典型的视觉里程计算法<citation id="176" type="reference"><link href="135" rel="bibliography" /><link href="137" rel="bibliography" /><sup>[<a class="sup">6</a>,<a class="sup">7</a>]</sup></citation>进行对比实验。实物实验采用<i>TUM RGB</i>-<i>D</i>标准数据集<citation id="175" type="reference"><link href="153" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>, 并选取了如图<b>3</b>所示的<b>6</b>个代表性的数据集进行测试。对于<i>TUM RGB</i>-标准数据集, 有两个指标用于评价视觉里程计与即时定位与建图, 即绝对轨迹误差 (<i>ATE</i>) 与相对轨迹误差 (<i>RPE</i>) 。一般地, 相对轨迹误差对视觉里程计评价更合适, 体现了该系统随着时间产生累积误差的程度。</p>
                </div>
                <h4 class="anchor-tag" id="85" name="85"><b>4.1 部分重叠率估计机制对比实验</b></h4>
                <div class="p1">
                    <p id="86">为验证提出的点云部分重叠率估计机制的引入对视觉里程计精度的影响, 选取<i>TUM RGB</i>-<i>D</i>数据集中<i>fr</i><b>1</b>_<i>xyz</i>视频序列与文献<citation id="177" type="reference">[<a class="sup">9</a>]</citation>中使用未加外点去除机制的视觉里程计作对比。此处以“本文”代表本文提出的方案, “<i>FVO</i>”代表文献中的方案。由表<b>1</b>可得出本文提出的方法其位姿估计误差基本上小于 <i>FVO</i>算法。实验验证了本文引入的部分重叠率估计机制是有效的且合理的, 并可以自适应的估计出部分重叠率参数, 提高了视觉里程计的精度。</p>
                </div>
                <div class="area_img" id="87">
                    <p class="img_tit"><b>表1 fr1_xyz视频序列位姿估计算法对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="87" border="1"><tr><td colspan="7"><br />绝对轨迹误差与相对轨迹误差</td></tr><tr><td><br />算法</td><td>ATE<br /> (m) <br />RMSE</td><td>ATE <br /> (m) <br />MAX</td><td>RPE<br /> (m/s) <br />RMSE</td><td>RPE <br /> (m/s) <br />MAX</td><td>RPE<br /> (deg/s) <br />RMSE</td><td>RPE<br /> (deg/s) <br />MAX</td></tr><tr><td><br />FVO</td><td>0.017</td><td>0.061</td><td>0.026</td><td>0.071</td><td>1.517</td><td>3.450</td></tr><tr><td><br />本文</td><td>0.015</td><td>0.045</td><td>0.024</td><td>0.067</td><td>1.415</td><td>4.65</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="88" name="88"><b>4.2 帧对模型与帧间匹配的切换策略对比实验</b></h4>
                <div class="p1">
                    <p id="89">对于前文所述的相机偶然大运动, 此处对比了有无切换策略对视觉里程计的影响。如表2, 相机在第5秒附近有偶然大运动, 本文算法绝对轨迹误差最大值为0.136m 而FVO最大误差为0.494m;本文算法相对误差最大值为0.24m/s, 而FVO最大误差为0.432m/s。图3同样反映了相对误差在大运动处的差异。实验表明切换策略对相机偶然大运动具有一定的鲁棒性。</p>
                </div>
                <div class="area_img" id="90">
                    <p class="img_tit"><b>表2 fr1_desk数据集上位姿估计算法对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="90" border="1"><tr><td rowspan="2"><br />算法</td><td colspan="6"><br />绝对轨迹误差与相对轨迹误差</td></tr><tr><td><br />ATE<br /> (m) <br />RMSE</td><td>ATE<br /> (m) <br />MAX</td><td>RPE<br /> (m/s) <br />RMSE</td><td>RPE<br /> (m/s) <br />MAX</td><td>RPE<br /> (deg/s) <br />RMSE</td><td>RPE<br /> (deg/s) <br />MAX</td></tr><tr><td><br />FVO</td><td>0.258</td><td>0.494</td><td>0.092</td><td>0.432</td><td>6.548</td><td>30.769</td></tr><tr><td><br />本文</td><td>0.0720</td><td>0.136</td><td>0.062</td><td>0.240</td><td>3.031</td><td>13.539</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="91">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJZ201908046_091.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 本文与FVO相对轨迹误差对比图" src="Detail/GetImg?filename=images/JSJZ201908046_091.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图3 本文与FVO相对轨迹误差对比图</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJZ201908046_091.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="92" name="92"><b>4.3 视觉里程计的综合表现能力实验</b></h4>
                <div class="p1">
                    <p id="93">实验选取<b>TUM RGB-D</b>标准数据集中常用于典型视觉里程计的<b>6</b>个数据集对本文提出的整体视觉里程计系统进行整体评估实验。并且选取了<b>SDO</b><citation id="178" type="reference"><link href="135" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>与<b>DVO</b><citation id="179" type="reference"><link href="137" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>两个典型的算法进行对比。表<b>3</b>为本文算法在<b>6</b>个测试集下的相对轨迹误差, 可以看出位姿估计准确率较高, 其中位移误差<b>RMSE</b>没有超过<b>0.06m/s</b>, 旋转误差<b>RMSE</b>未超过<b>3deg/s</b>。表<b>4</b>为与典型算法在相对位移误差<b>RMSE</b>方面的对比, 可以看出本文算法在<b>6</b>个测试集下有<b>3</b>个效果最好, 剩下<b>3</b>个跟最好的结果较为接近。实验验证精度高于典型算法的<b>3</b>个视频序列为相机做小规模运动, 而在相机有偶然大运动的视频序列上可以达到与典型算法那同等的估计效果。图<b>4</b>为算法在<b>4</b>个测试集下的轨迹图, 进一步直观地反应本文算法的有效性与鲁棒性。</p>
                </div>
                <div class="area_img" id="94">
                    <p class="img_tit"><b>表3 TUM RGB-D数据集综合表现能力实验</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="94" border="1"><tr><td rowspan="2"><br />视频序列</td><td colspan="4"><br />相对轨迹误差</td></tr><tr><td><br />RPE<br /> (m/s) <br />RMSE</td><td>RPE<br /> (m/s) <br />MAX</td><td>RPE<br /> (deg/s) <br />RMSE</td><td>RPE<br /> (deg/s) <br />MAX</td></tr><tr><td><br />fr1_xyz</td><td>0.024</td><td>0.067</td><td>1.415</td><td>4.65</td></tr><tr><td><br />fr1_desk</td><td>0.062</td><td>0.240</td><td>3.031</td><td>13.539</td></tr><tr><td><br />fr2_xyz</td><td>0.008</td><td>0.031</td><td>0.439</td><td>1.741</td></tr><tr><td><br />fr2_rpy</td><td>0.014</td><td>0.062</td><td>0.701</td><td>2.942</td></tr><tr><td><br />fr3_sitting_static</td><td>0.023</td><td>0.119</td><td>0.658</td><td>3.354</td></tr><tr><td><br />fr3_sitting_xyz</td><td>0.032</td><td>0.169</td><td>0.897</td><td>3.696<br /></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="95">
                    <p class="img_tit"><b>表4 TUM RGB-D综合表现对比实验</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="95" border="1"><tr><td rowspan="2"><br />视频序列</td><td colspan="3"><br />相对轨迹误差位移平均值 (m) </td></tr><tr><td><br />本文</td><td>SDO</td><td>DVO</td></tr><tr><td><br />fr1_xyz</td><td>0.024</td><td>0.038</td><td>0.037</td></tr><tr><td><br />fr1_desk</td><td>0.062</td><td>0.058</td><td>0.041</td></tr><tr><td><br />fr2_xyz</td><td>0.008</td><td>0.017</td><td>0.013</td></tr><tr><td><br />fr2_rpy</td><td>0.014</td><td>0.017</td><td>0.018</td></tr><tr><td><br />fr3_sitting_static</td><td>0.023</td><td>0.025</td><td>0.016</td></tr><tr><td><br />fr3_sitting_xyz</td><td>0.032</td><td>0.039</td><td>0.025</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="96">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJZ201908046_096.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 四个典型视频序列绝对轨迹图" src="Detail/GetImg?filename=images/JSJZ201908046_096.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图4 四个典型视频序列绝对轨迹图</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJZ201908046_096.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h3 id="97" name="97" class="anchor-tag"><b>5 结语</b></h3>
                <div class="p1">
                    <p id="98">针对现有视觉里程计算法中受外点影响问题与对相机偶然大运动的不鲁棒问题, 本文提出一种改进<b>ICP</b>算法用于视觉里程计系统中, 并引入一个帧对模型与帧间匹配的切换策略来提升了系统的精确度与鲁棒性。其中, 在<b>ICP</b>算法计算位姿起始阶段采用自适应估计部分重叠率的方式, 使得每次迭代都能选取有效的点云点对进行位姿估计, 并给点云中各点配以空间约束的权重, 提升了位姿计算的精度。此外, 提出的两条准则用以切换视觉里程计的匹配模式, 以应对相机的偶然大运动状况。实物实验结果显示, 该系统对小规模运动数据集效果优异, 对相机偶然运动有一定的适应能力。</p>
                </div>
                <div class="area_img" id="124">
                                <img alt="" src="Detail/GetImg?filename=images/JSJZ201908046_12400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                            <p class="img_tit"></p>

                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="125">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Outdoor Mapping and Navigation Using Stereo Vision">

                                <b>[1]</b> K Konolige et al.Outdoor Mapping and Navigation Using Stereo Vision[J].Tracts in AdvancedRobotics, Springer, 2008, 39:179-190.
                            </a>
                        </p>
                        <p id="127">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJA2016S1049&amp;v=MjM5MDM0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVab0Z5N25WTHZBTHo3QmI3RzRIOWV2cm85QmJZUUtESDg=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> 朱永丰, 朱述龙, 张静静, 朱永康.基于ORB特征的单目视觉定位算法研究[J].计算机科学, 2016, 43 (6A) :198-202, 254.
                            </a>
                        </p>
                        <p id="129">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000025011&amp;v=MTkyNjFPa0tESDA0b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcm5JSmx3VGJobz1OaWZJWTdLN0h0ak5yNDlGWg==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> M A Fischler, R C BolleS.Random sample consensus:a paradigm for model fitting with applications to image analysis and automated cartography[J].Communications of the ACM, ACM, 1981, 24 (6) :381–395.
                            </a>
                        </p>
                        <p id="131">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSYJ201208002&amp;v=MzAxMTVuVkx2QUx6N1NaTEc0SDlQTXA0OUZab1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1Wm9GeTc=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b> 李宇波, 朱效洲, 卢惠民, 张辉.视觉里程计技术综述[J].计算机应用研究, 2012:2801-2805.
                            </a>
                        </p>
                        <p id="133">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XXYD201506012&amp;v=MzE1MjFIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpvRnk3blZMdkFQVFhTYXJHNEg5VE1xWTlFWm9RS0Q=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b> 张云, 尹露, 王雨婷, 罗斌.基于SiftGPU特征匹配方法的实时视觉里程计系统[J].太赫兹科学与电子信息学报, 2015:897-902, 912.
                            </a>
                        </p>
                        <p id="135">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Sparse depth odometry:3D keypoint based pose estimation from dense depth data">

                                <b>[6]</b> S Prakhya et al.Sparse Depth Odometry:3D keypoint based pose estimation from dense depth data[C].2013 IEEE International Conference on Robotics and Automation IEEE, 2015:4216-4223.
                            </a>
                        </p>
                        <p id="137">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Robust odometry estimation for RGB-D cameras">

                                <b>[7]</b> K Christian, J Sturm, D Cremers.Robust odometry estimation for RGB-D cameras[C].2013 IEEE International Conference on Robotics and Automation IEEE, 2013:3748-3754.
                            </a>
                        </p>
                        <p id="139">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES7C62C554F5038DA1B5F99C3C78E18F72&amp;v=MzEzNzBBPU5pZk9mYlRMR05PL3FvcEJFdTRQRDNSTnZoZGg3MGwwUVF6aDN4VTlETE9jTTcyZENPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkJyTFUwNXQ1aHdycTR4Sw==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b> D Gutierrez-Gomez, W Mayol-Cuevas, J Guerrero.Dense RGB-D visual odometry using inverse depth[J].Robotics and Autonomous Systems, Elsevier, 2016, 75:571-583.
                            </a>
                        </p>
                        <p id="141">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fast visual odometry and mapping from RGB-D data">

                                <b>[9]</b> I Dryanovski, et al.Fast visual odometry and mapping from RGB-D data[C].IEEE International Conference on Robotics and Automation IEEE, 2013:2305-2310.
                            </a>
                        </p>
                        <p id="143">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Generalized-ICP.">

                                <b>[10]</b> Segal, et al.Generalized-ICP[J].In Robotics:science and systems, 2009-2:435
                            </a>
                        </p>
                        <p id="145">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The trimmed iterative closest point algorithm">

                                <b>[11]</b> D Chetverikov, et al.The Trimmed Iterative Closest Point Algorithm[C].2002 16 Th International Conference on Pattern Recognition.IEEE Computer Society, 2002:30545.
                            </a>
                        </p>
                        <p id="147">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Convergent Iterative Closest-Point Algorithm to Accomodate Anisotropic and Inhomogenous Localization Error">

                                <b>[12]</b> L Maier-Hein, et al.Convergent iterative closest-point algorithm to accomodate anisotropic and inhomogenous localization error[J].IEEE transactions on pattern analysis and machine intelligence, 2012, 34 (8) :1520-1532.
                            </a>
                        </p>
                        <p id="149">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Motion-Aware Iterative Closest Point Estimation for Fast Visual Odometry">

                                <b>[13]</b> T Lin, et al.Motion-Aware Iterative Closest Point Estimation for Fast Visual Odometry[C].2017 IEEE International Symposium on Mixed and Augmented Reality IEEE, 2017:268-269.
                            </a>
                        </p>
                        <p id="151">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Method for registration of 3-D shapes">

                                <b>[14]</b> P Besl, N Mckay.A method for registration of 3-D shapes[J].Sensor Fusion IV:Control Paradigms and Data Structures Sensor Fusion IV:Control Paradigms and Data Structures, 1992:239-256.
                            </a>
                        </p>
                        <p id="153">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A benchmark for the evaluation of RGB-D SLAM systems">

                                <b>[15]</b> Sturm, et al.A benchmark for the evaluation of RGB-D SLAM systems[C].2012 IEEE International Conference on Intelligent Robots and Systems IEEE, 2012:573-580.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJZ201908046" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJZ201908046&amp;v=MTM5NDI4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1Wm9GeTduVkx2QUx6N0JkTEc0SDlqTXA0OUJZb1FLREg=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=842_VlRz_g1CMCLi-M0twVw-4EOFG_noWyi_OsMnrAg1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
