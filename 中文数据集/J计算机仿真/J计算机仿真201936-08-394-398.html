<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=NOOK-mvTMnxl11s6oPYV0boPXN3iP4N7Rc-A56nk4KI1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2fKXReader%2fDetail%3fTIMESTAMP%3d637140160477943750%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJZ201908082%26RESULT%3d1%26SIGN%3d8Uhwy7ri9PSRrgW59BbWa50Ss4s%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJZ201908082&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJZ201908082&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJZ201908082&amp;v=MDE4MTJwNDlOWm9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpvRnk3Z1Ziek9MejdCZExHNEg5ak0=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#23" data-title="&lt;b&gt;1 引言&lt;/b&gt; "><b>1 引言</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#30" data-title="&lt;b&gt;2 行为识别原理&lt;/b&gt; "><b>2 行为识别原理</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#57" data-title="&lt;b&gt;3 行为识别模型&lt;/b&gt; "><b>3 行为识别模型</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#58" data-title="&lt;b&gt;3.1 基于双流网络构架的运动行为组合特征构建&lt;/b&gt;"><b>3.1 基于双流网络构架的运动行为组合特征构建</b></a></li>
                                                <li><a href="#86" data-title="&lt;b&gt;3.2 运动行为组合特征的分类识别&lt;/b&gt;"><b>3.2 运动行为组合特征的分类识别</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#95" data-title="&lt;b&gt;4 实验结果与分析&lt;/b&gt; "><b>4 实验结果与分析</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#109" data-title="&lt;b&gt;5 结论&lt;/b&gt; "><b>5 结论</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#98" data-title="&lt;b&gt;表1 UCF50数据集上动作识别的实验结果&lt;/b&gt;"><b>表1 UCF50数据集上动作识别的实验结果</b></a></li>
                                                <li><a href="#99" data-title="&lt;b&gt;表2 UCF50数据集上行为识别的实验结果&lt;/b&gt;"><b>表2 UCF50数据集上行为识别的实验结果</b></a></li>
                                                <li><a href="#102" data-title="&lt;b&gt;表3 UCF101数据集上动作识别的实验结果&lt;/b&gt;"><b>表3 UCF101数据集上动作识别的实验结果</b></a></li>
                                                <li><a href="#103" data-title="&lt;b&gt;表4 UCF101数据集上行为识别的实验结果&lt;/b&gt;"><b>表4 UCF101数据集上行为识别的实验结果</b></a></li>
                                                <li><a href="#107" data-title="&lt;b&gt;图1 不同识别模型的识别效率对比图&lt;/b&gt;"><b>图1 不同识别模型的识别效率对比图</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="122">


                                    <a id="bibliography_1" title=" 王晓芳, 齐春.一种运用显著性检测的行为识别方法[J].西安交通大学学报, 2018, 52 (2) :24-29." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XAJT201802004&amp;v=MDA2MzJPM3pxcUJ0R0ZyQ1VSN3FmWnVab0Z5N2dWYnpPUFN6QmVyRzRIOW5Nclk5RllJUUtESDg0dlI0VDZqNTQ=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         王晓芳, 齐春.一种运用显著性检测的行为识别方法[J].西安交通大学学报, 2018, 52 (2) :24-29.
                                    </a>
                                </li>
                                <li id="124">


                                    <a id="bibliography_2" title=" 梅阳, 等.一种基于关键帧的人体行为识别方法[J].光学技术, 2017, 43 (4) :323-328." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXJS201704008&amp;v=MjQ5MzR6cXFCdEdGckNVUjdxZlp1Wm9GeTdnVmJ6T0lqWEJmYkc0SDliTXE0OUZiSVFLREg4NHZSNFQ2ajU0TzM=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         梅阳, 等.一种基于关键帧的人体行为识别方法[J].光学技术, 2017, 43 (4) :323-328.
                                    </a>
                                </li>
                                <li id="126">


                                    <a id="bibliography_3" title=" 刘智, 黄江涛, 冯欣.构建多尺度深度卷积神经网络行为识别模型[J].光学精密工程, 2017, 25 (3) :799-805." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXJM201703035&amp;v=MzA3MTdiek9JalhCWTdHNEg5Yk1ySTlHWVlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpvRnk3Z1Y=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[3]</b>
                                         刘智, 黄江涛, 冯欣.构建多尺度深度卷积神经网络行为识别模型[J].光学精密工程, 2017, 25 (3) :799-805.
                                    </a>
                                </li>
                                <li id="128">


                                    <a id="bibliography_4" title=" 王军, 夏利民, 夏胜平.利用生成模型的人体行为识别[J].国防科技大学学报, 2016, 38 (2) :68-74." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GFKJ201602012&amp;v=MjYxMTVFWm9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpvRnk3Z1Ziek9JaXZBWkxHNEg5Zk1yWTk=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                         王军, 夏利民, 夏胜平.利用生成模型的人体行为识别[J].国防科技大学学报, 2016, 38 (2) :68-74.
                                    </a>
                                </li>
                                <li id="130">


                                    <a id="bibliography_5" title=" 王智文, 等.基于动作子空间和权重条件随机场的行为识别[J].电子科技大学学报, 2017, 46 (2) :412-418." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DKDX201702016&amp;v=MjUyODVSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1Wm9GeTdnVmJ6T0lTYlBkckc0SDliTXJZOUVZb1FLREg4NHY=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                         王智文, 等.基于动作子空间和权重条件随机场的行为识别[J].电子科技大学学报, 2017, 46 (2) :412-418.
                                    </a>
                                </li>
                                <li id="132">


                                    <a id="bibliography_6" title=" 张玉燕, 等.基于隐动态条件神经域的在线行为识别方法[J].计算机工程与设计, 2016, 37 (6) :1632-1635." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=SJSJ201606039&amp;v=MTcyNjdmWnVab0Z5N2dWYnpPTmlmWVpMRzRIOWZNcVk5R2JZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3E=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                         张玉燕, 等.基于隐动态条件神经域的在线行为识别方法[J].计算机工程与设计, 2016, 37 (6) :1632-1635.
                                    </a>
                                </li>
                                <li id="134">


                                    <a id="bibliography_7" title=" 赵晓健, 曾晓勤.基于稠密光流轨迹和稀疏编码算法的行为识别方法[J].计算机应用, 2016, 36 (1) :181-187." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201601036&amp;v=MjMxMDI3QmQ3RzRIOWZNcm85R1lvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVab0Z5N2dWYnpPTHo=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                         赵晓健, 曾晓勤.基于稠密光流轨迹和稀疏编码算法的行为识别方法[J].计算机应用, 2016, 36 (1) :181-187.
                                    </a>
                                </li>
                                <li id="136">


                                    <a id="bibliography_8" title=" 高晨兰, 朱嘉钢.静止背景下的人体行为识别方法[J].计算机工程, 2017, 43 (10) :192-197." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201710033&amp;v=MjkyNjFyQ1VSN3FmWnVab0Z5N2dWYnpPTHo3QmJiRzRIOWJOcjQ5R1o0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0Y=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                         高晨兰, 朱嘉钢.静止背景下的人体行为识别方法[J].计算机工程, 2017, 43 (10) :192-197.
                                    </a>
                                </li>
                                <li id="138">


                                    <a id="bibliography_9" title=" 周同驰, 等.基于有效轨迹和多重方向模式的行为识别[J].信号处理, 2016, 32 (5) :519-527." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XXCN201605003&amp;v=MTYzODFUWElZTEc0SDlmTXFvOUZaNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1Wm9GeTdnVmJ6T1A=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                         周同驰, 等.基于有效轨迹和多重方向模式的行为识别[J].信号处理, 2016, 32 (5) :519-527.
                                    </a>
                                </li>
                                <li id="140">


                                    <a id="bibliography_10" title=" 刘桥.游泳运动员姿势识别校正方法研究与仿真[J].计算机仿真, 2017, 34 (4) :227-230." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJZ201704048&amp;v=MjE0NDVSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1Wm9GeTdnVmJ6T0x6N0JkTEc0SDliTXE0OUJiSVFLREg4NHY=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[10]</b>
                                         刘桥.游泳运动员姿势识别校正方法研究与仿真[J].计算机仿真, 2017, 34 (4) :227-230.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJZ" target="_blank">计算机仿真</a>
                2019,36(08),394-398             </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>双流网络构架行为识别隐含层模型仿真</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%88%98%E6%9D%BE%E6%B3%89&amp;code=41103353&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">刘松泉</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%83%A1%E5%86%9B&amp;code=07068172&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">胡军</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%90%88%E8%82%A5%E5%B7%A5%E4%B8%9A%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%B8%8E%E4%BF%A1%E6%81%AF%E5%AD%A6%E9%99%A2&amp;code=0083575&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">合肥工业大学计算机与信息学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>复杂场景中特征的有效提取一直是行为识别的难点, 动作和行为的精确表达也是影响识别结果的重要因素。针对当前行为识别模型只能选择单一特征进行行为识别, 对动作和行为的表达有误, 导致识别精确度、召回率低、识别速度慢等问题, 提出双层网络架构和隐含层相融合的行为识别模型。采用CNN和RNN构建双流网络架构, 用于抽取视频动作片段的外观特征和时间特征, 同时在双流网络架构中添加一个隐含层, 以便更有效地对特征进行描述。依据求和的形式融合运动特征和外观特征, 构建运动行为组合特征, 将运动行为组合特征输入到支持向量机分类器中来完成行为识别, 在UCF101、UCF50数据集上进行行为识别实验。实验结果表明, 所提模型有效提高了行为识别率和召回率, 识别速度也优于对比模型。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8F%8C%E6%B5%81%E7%BD%91%E7%BB%9C%E6%9E%84%E6%9E%B6&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">双流网络构架;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%A1%8C%E4%B8%BA%E8%AF%86%E5%88%AB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">行为识别;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%9A%90%E5%90%AB%E5%B1%82%E6%A8%A1%E5%9E%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">隐含层模型;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">支持向量机;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    刘松泉 (1992-) , 男 (汉族) , 安徽阜阳人, 硕士生, 主要研究方向:图像处理、机器学习。;
                                </span>
                                <span>
                                    胡军 (1992-) , 男 (汉族) , 安徽芜湖人, 硕士生, 主要研究方向:智能信息处理。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-03-30</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金 (61273237, 61503111);</span>
                    </p>
            </div>
                    <h1><b>Double Stream Network Architecture Behavior Recognition Hidden Layer Model Simulation</b></h1>
                    <h2>
                    <span>LIU Song-quan</span>
                    <span>HU Jun</span>
            </h2>
                    <h2>
                    <span>School of Computer and Information, Hefei University of Technology</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>At present, the behavior recognition model can only select single feature to recognize the behavior, resulting in low recognition accuracy, low recall rate and slow recognition. This paper focuses on the behavior recognition model integration combining the dual-layer network architecture with the hidden layer. At first, CNN and RNN were used to construct the dual-flow network architecture which was applied to the extraction of the appearance characteristic and time characteristic of motion segment of video. Meanwhile, a hidden layer was added to the dual-flow network architecture, so as to describe feature more effectively. According to the form of summation, the motion feature and appearance feature were integrated to construct the compound feature of motion behavior. Finally, the compound feature of motion behavior was input into the classifier of support vector machine to complete the behavior recognition. Thus, the behavior recognition experiment was performed on UCF101 data set and UCF50 data set. Simulation results show that the proposed model effectively improves the behavior recognition rate and recall rate. Meanwhile, the recognition speed is better than the comparison model.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Double-flow%20network%20architecture&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Double-flow network architecture;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Behavior%20recognition&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Behavior recognition;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Hidden%20layer%20model&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Hidden layer model;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Support%20vector%20machine%20(SVM)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Support vector machine (SVM) ;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-03-30</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="23" name="23" class="anchor-tag"><b>1 引言</b></h3>
                <div class="p1">
                    <p id="24">行为识别在智能监控、事件检测、虚拟现实、医疗诊断等领域的广泛应用, 引起了越来越多计算机视觉研究人员的重视<citation id="146" type="reference"><link href="122" rel="bibliography" /><link href="124" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>]</sup></citation>。行为识别研究融合了计算机视觉、分类识别等多类学科的研究成果, 同时也是以上应用领域的研究难点<citation id="142" type="reference"><link href="126" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>。当前行为识别方法通过对滑动时间窗内的运动信息进行合理统计, 提取人体运动特征, 计算开销较小, 但同类行为识别结果存在较大的类内差异<citation id="143" type="reference"><link href="128" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>。同时连续视频数据引起的特征冗余也增加了复杂场景中特征有效提取的难度<citation id="144" type="reference"><link href="130" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>。在这种情况下, 如何从行为视频中提取出有效特征和设计更加有效的行为识别框架是当前计算机视觉领域亟待解决的主要问题<citation id="145" type="reference"><link href="132" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="25">稠密光流轨迹特征、运动向量模板特征是目前行为识别普遍采用的特征。文献<citation id="147" type="reference">[<a class="sup">7</a>]</citation>中, 通过构建稠密光流轨迹和稀疏编码框架来实现行为识别。对行为视频进行稠密光流轨迹提取, 对以轨迹为中心的行为视频图像块进行取样, 将其作为轨迹的初始特征。采用稀疏编码框架和词袋模型对轨迹特征进行聚类, 并统计轨迹所属码书类别, 统计不同行为动作在每个码书中出现的次数, 将其视为行为特征, 输入至支持向量机构建行为识别模型, 采用该模型实现行为识别。稠密光流轨迹特征计算量较大, 具有自遮挡以及稠密光流轨迹特征对预处理的要求较高等问题, 这些问题导致该方法行为识别速度较慢。文献<citation id="148" type="reference">[<a class="sup">8</a>]</citation>提出融合主成分分析方法和高斯混合模型的行为识别模型。通过提取行为视频中的运动前景稠密光流, 构建运动向量模板, 采用主成分分析法对该模板进行特征抽取, 通过高斯混合模型对特征抽取数据建模, 实现行为分类。该方法通过对行为视频中的行为信息进行合理压缩, 减少了计算量, 在一定程度上提高了识别的速度, 但降低了识别的精度。</p>
                </div>
                <div class="p1">
                    <p id="26">针对上述问题, 提出双层网络架构和隐含层相融合的行为识别模型。主要研究内容如下:</p>
                </div>
                <div class="p1">
                    <p id="27">1) 分别训练CNN和RNN, 在CNN中, 通过单次卷积操作后输入多个视频动作片段的外观特征图, 对输出的特征图进行均值采样, 使神经网络对视频图像旋转、平移以及尺度变换具有鲁棒性, 减少神经网络训练计算量。RNN在CNN的基础上增加了时间维度。</p>
                </div>
                <div class="p1">
                    <p id="28">2) 将CNN和RNN相融合构建了双流网络架构, 用于抽取视频动作片段的外观特征和时间特征。为了利用外观特征和运动特征在像素级别上的关联性, 在原始双流网络架构的基础上进行了改进, 选择了隐含层进行了融合, 以便更有效地对特征进行描述。</p>
                </div>
                <div class="p1">
                    <p id="29">3) 将运动行为组合特征输入到支持向量机分类器中来完成行为识别。</p>
                </div>
                <h3 id="30" name="30" class="anchor-tag"><b>2 行为识别原理</b></h3>
                <div class="p1">
                    <p id="31">采用函数型数据分析方法将运动目标捕获系统采集的运动行为函数化。通过函数设定行为数据的连续性和周期性, 由导函数来确定给定运动周期的初始位置, 提取出给定运动周期的行为数据序列。依据不同运动行为给定周期内的曲线特征差异, 采用支持向量机分类器对运动行为进行分类识别。具体过程如下所述:</p>
                </div>
                <div class="p1">
                    <p id="32">采用函数型数据分析方法<citation id="149" type="reference"><link href="138" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>将运动目标捕获系统采集的运动行为函数化, 即</p>
                </div>
                <div class="p1">
                    <p id="33" class="code-formula">
                        <mathml id="33"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mi>x</mi><mo stretchy="false"> (</mo><mi>t</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>Κ</mi></munderover><mi>c</mi></mstyle><msub><mrow></mrow><mi>k</mi></msub><mtext>ϕ</mtext><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false"> (</mo><mi>t</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="34">其中, <i>x</i> (<i>t</i><sub><i>i</i></sub>) 表示运动行为函数在观测序列<i>t</i><sub><i>i</i></sub>处的取值, ϕ<sub><i>k</i></sub>表示运动行为观测数据序列函数<i>x</i> (<i>t</i>) 的傅里叶展开式, <i>c</i><sub><i>k</i></sub>表示基函数的线性组合, <i>k</i>表示周期型观测数据序列的函数向量, <i>K</i>表示向量集。</p>
                </div>
                <div class="p1">
                    <p id="35">对于具有连续性和周期性的运动行为观测数据, 利用<i>κ</i>阶傅里叶级数拟合<citation id="150" type="reference"><link href="140" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>, 以下给出<i>κ</i>阶傅里叶拟合函数的运动行为序列</p>
                </div>
                <div class="p1">
                    <p id="36" class="code-formula">
                        <mathml id="36"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>x</mi><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo><mo>=</mo><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">[</mo><mi>c</mi><msub><mrow></mrow><mn>1</mn></msub><mo>+</mo><mi>c</mi><msub><mrow></mrow><mn>2</mn></msub><mi>sin</mi><mo stretchy="false"> (</mo><mi>ω</mi><mi>t</mi><mo stretchy="false">) </mo><mo>+</mo><mo>⋯</mo><mo>+</mo><mi>c</mi><msub><mrow></mrow><mi>κ</mi></msub><mi>cos</mi><mo stretchy="false"> (</mo><mn>4</mn><mi>ω</mi><mi>t</mi><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="37">其中, <i>ω</i>表示运动行为数据估算基波频率, <i>c</i><sub>1</sub>, …, <i>c</i><sub><i>κ</i></sub>表示拟合参数, <i>t</i>表示时间。</p>
                </div>
                <div class="p1">
                    <p id="38">采用多个传感器节点采集运动行为数据, 每个传感器在每个阶段都会产生一个样本, 以下给出单个传感器节点在<i>t</i><sub><i>i</i></sub>时段内产生的向量</p>
                </div>
                <div class="p1">
                    <p id="39" class="code-formula">
                        <mathml id="39"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>a</mi><mo stretchy="false"> (</mo><mi>t</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mo stretchy="false"> (</mo><mi>a</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false"> (</mo><mi>t</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>a</mi><msub><mrow></mrow><mi>Μ</mi></msub><mo stretchy="false"> (</mo><mi>t</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="40">其中, <i>M</i>表示传感器节点测量单元总数。单个传感器节点对运动行为进行连续采样, 生成一个观测矩阵</p>
                </div>
                <div class="p1">
                    <p id="41" class="code-formula">
                        <mathml id="41"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>s</mi><mo>=</mo><mo stretchy="false">[</mo><mi>a</mi><mo stretchy="false"> (</mo><mi>t</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false">) </mo><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>a</mi><mo stretchy="false"> (</mo><mi>t</mi><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false">) </mo><mo stretchy="false">]</mo><msup><mrow></mrow><mi>Τ</mi></msup><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="42">其中, <i>a</i> (<i>t</i><sub><i>i</i></sub>) ∈<i>a</i> (<i>t</i><sub><i>n</i></sub>) 表示观测矩阵元素, <i>T</i>表示观测矩阵维度, <i>i</i>=1, …, <i>n</i>表示某个传感器第<i>t</i><sub><i>i</i></sub>次采集的运动行为数据。对观测矩阵的各列运动行为数据利用傅里叶级数拟合, 将传感器节点每个测量单元内的运动行为数据转变成关于取样时刻<i>t</i>的导函数, 则有</p>
                </div>
                <div class="p1">
                    <p id="43" class="code-formula">
                        <mathml id="43"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>s</mi><msup><mrow></mrow><mo>*</mo></msup><mo>=</mo><mo stretchy="false"> (</mo><mi>f</mi><msub><mrow></mrow><mn>1</mn></msub><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>f</mi><msub><mrow></mrow><mi>Μ</mi></msub><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>5</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="44">其中, <i>s</i><sup>*</sup>表示观测向量, 运动行为样本数据通常由多个传感器节点同时采集获得, 存在以下形式的观测函数矩阵</p>
                </div>
                <div class="p1">
                    <p id="45" class="code-formula">
                        <mathml id="45"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>S</mi><mo>=</mo><mrow><mo>[</mo><mrow><mtable><mtr><mtd><mi>f</mi><mspace width="0.25em" /><msubsup><mrow></mrow><mn>1</mn><mn>1</mn></msubsup><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo></mtd><mtd><mo>⋯</mo></mtd><mtd><mi>f</mi><mspace width="0.25em" /><msubsup><mrow></mrow><mi>Μ</mi><mn>1</mn></msubsup><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd></mtd><mtd><mo>⋮</mo></mtd><mtd></mtd></mtr><mtr><mtd><mi>f</mi><mspace width="0.25em" /><msubsup><mrow></mrow><mn>1</mn><mi>Ν</mi></msubsup><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo></mtd><mtd><mo>⋯</mo></mtd><mtd><mi>f</mi><mspace width="0.25em" /><msubsup><mrow></mrow><mi>Μ</mi><mi>Ν</mi></msubsup><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo></mtd></mtr></mtable></mrow><mo>]</mo></mrow><msub><mrow></mrow><mrow><mi>Ν</mi><mo>×</mo><mi>Μ</mi></mrow></msub><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>6</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="46">其中, <i>N</i>表示传感器节点总数。</p>
                </div>
                <div class="p1">
                    <p id="47">设定<i>F</i>表示传感器节点采集运动行为数据的采样频率, 运动周期时间为<i>T</i><sub><i>d</i></sub>, 对运动行为样本函数矩阵<i>S</i>中的元素进行离散化处理, 利用下式给出第<i>k</i>个传感器测量单元的周期数据</p>
                </div>
                <div class="p1">
                    <p id="48" class="code-formula">
                        <mathml id="48"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>v</mi><msub><mrow></mrow><mi>k</mi></msub><mo>=</mo><mo stretchy="false"> (</mo><mi>a</mi><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false"> (</mo><mi>t</mi><msup><mrow></mrow><mo>*</mo></msup><mo stretchy="false">) </mo><mo>, </mo><mi>a</mi><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false"> (</mo><mi>t</mi><msup><mrow></mrow><mo>*</mo></msup><mo>+</mo><mi>d</mi><mo stretchy="false">) </mo><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>a</mi><msub><mrow></mrow><mi>k</mi></msub><mo stretchy="false"> (</mo><mi>t</mi><msup><mrow></mrow><mo>*</mo></msup><mo>+</mo><mi>l</mi><mo>-</mo><mn>1</mn><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>7</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="49">其中, <i>d</i>表示传感器采样间隔, <i>l</i>表示间隔为<i>d</i>=1的周期程度, <i>t</i><sup>*</sup>表示运动周期。依据上式对各个传感器节点以式 (8) 的形式构造向量</p>
                </div>
                <div class="p1">
                    <p id="50" class="code-formula">
                        <mathml id="50"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>V</mi><msup><mrow></mrow><mi>j</mi></msup><mo>=</mo><mo stretchy="false"> (</mo><mi>v</mi><msub><mrow></mrow><mn>1</mn></msub><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>v</mi><msub><mrow></mrow><mi>Μ</mi></msub><mo stretchy="false">) </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>8</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="51">将用于运动行为采集的全部传感器<i>V</i><sup><i>j</i></sup>组成以下形式的行向量</p>
                </div>
                <div class="p1">
                    <p id="52" class="code-formula">
                        <mathml id="52"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>V</mi><mo>=</mo><mo stretchy="false"> (</mo><mi>V</mi><msup><mrow></mrow><mn>1</mn></msup><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>V</mi><msup><mrow></mrow><mi>Ν</mi></msup><mo stretchy="false">) </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>9</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="53">将上式作为运动行为样本的特征向量, 依据不同运动行为在给定周期内的曲线特征差异, 使用支持向量机分类器对运动行为进行分类识别</p>
                </div>
                <div class="p1">
                    <p id="54" class="code-formula">
                        <mathml id="54"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>f</mi><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mo>=</mo><mi>sgn</mi><mspace width="0.25em" /><mi>V</mi><mo stretchy="false">{</mo><mo>-</mo><mrow><mo>|</mo><mrow><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mi>x</mi><msub><mrow></mrow><mi>j</mi></msub></mrow><mo>|</mo></mrow><mo>/</mo><mi>σ</mi><msup><mrow></mrow><mn>2</mn></msup><mo stretchy="false">}</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>0</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="55">其中, <i>x</i><sub><i>i</i></sub>、<i>x</i><sub><i>j</i></sub>表示不同类别的训练样本集, <i>σ</i>表示径向基核函数。</p>
                </div>
                <div class="p1">
                    <p id="56">综上所述为行为识别原理, 采用该原理实现了复杂场景下的行为识别。不足之处在于:①将运动目标捕获系统采集的运动行为数据直接函数化, 从中选取周期数据作为行为特征向量, 并没有更多地获取周期运动的行为特征信息。②主要处理的是周期型运动行为序列, 未涉及对复杂场景下运动序列的识别。</p>
                </div>
                <h3 id="57" name="57" class="anchor-tag"><b>3 行为识别模型</b></h3>
                <h4 class="anchor-tag" id="58" name="58"><b>3.1 基于双流网络构架的运动行为组合特征构建</b></h4>
                <div class="p1">
                    <p id="59">为利用视频中的目标外观特征和运动特征来识别行为动作, 采用<i>CNN</i>和<i>RNN</i>双流网络架构分别抽取视频动作片段的外观特征和运动特征, 依据求和的形式融合运动特征和外观特征, 构建运动行为组合特征。具体过程如下所述:</p>
                </div>
                <div class="p1">
                    <p id="60"><i>CNN</i>根据卷积计算自动学习视频动作片段的外观特征, 对接收的数据采用多个卷积核进行卷积, 利用下式给出卷积计算过程</p>
                </div>
                <div class="p1">
                    <p id="61" class="code-formula">
                        <mathml id="61"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Y</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mi>j</mi></mrow><mi>l</mi></msubsup><mo>=</mo><mi>f</mi><mo stretchy="false"> (</mo><mstyle displaystyle="true"><msub><mo>∑</mo><mrow><mi>i</mi><mo>∈</mo><mi>m</mi><msub><mrow></mrow><mi>h</mi></msub></mrow></msub><mspace width="0.25em" /></mstyle><mstyle displaystyle="true"><msub><mo>∑</mo><mrow><mi>j</mi><mo>∈</mo><mi>n</mi><msub><mrow></mrow><mi>w</mi></msub></mrow></msub><mspace width="0.25em" /></mstyle><mi>x</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mi>j</mi></mrow><mi>l</mi></msubsup><mi>G</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mi>j</mi></mrow><mi>l</mi></msubsup><mo>+</mo><mi>b</mi><msup><mrow></mrow><mi>l</mi></msup><mo stretchy="false">) </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="62">其中, <i>Y</i><mathml id="63"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mi>j</mi></mrow><mi>l</mi></msubsup></mrow></math></mathml>、<i>x</i><mathml id="64"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mi>j</mi></mrow><mi>l</mi></msubsup></mrow></math></mathml>表示<i>CNN</i>第<i>l</i>层输出、输入的第 (<i>i</i>, <i>j</i>) 点的值, <i>G</i><mathml id="65"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mi>j</mi></mrow><mi>l</mi></msubsup></mrow></math></mathml>表示卷积核, <i>b</i><sup><i>l</i></sup>表示卷积操作偏执项, <i>m</i><sub><i>h</i></sub>、<i>n</i><sub><i>w</i></sub>分别表示<i>CNN</i>第<i>l</i>层中的窗口大小。</p>
                </div>
                <div class="p1">
                    <p id="66">在<i>CNN</i>中, 通过单次卷积操作后输入多个视频动作片段的外观特征图, 对输出的特征图进行均值采样, 使神经网络对视频图像旋转、平移以及尺度变换具有鲁棒性, 减少神经网络训练计算量。均值采样对给定采样窗口内的视频动作片段外观特征值求取平均后, 将结果作为采样结果, 以下给出<i>CNN</i>均值采样计算式</p>
                </div>
                <div class="p1">
                    <p id="67" class="code-formula">
                        <mathml id="67"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Y</mi><msub><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mi>j</mi></mrow></msub><mo>=</mo><mfrac><mn>1</mn><mrow><mi>Η</mi><mi>W</mi></mrow></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>h</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>Η</mi><mo>-</mo><mn>1</mn></mrow></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>w</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>W</mi><mo>-</mo><mn>1</mn></mrow></munderover><mi>X</mi></mstyle></mrow></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mo>×</mo><mi>Η</mi><mo>+</mo><mi>h</mi><mo>, </mo><mi>j</mi><mo>×</mo><mi>W</mi><mo>+</mo><mi>w</mi></mrow></msub><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="68">其中, <i>Y</i><sub><i>i</i>, <i>j</i></sub>表示均值采样的输出值, <i>X</i><sub><i>i</i>×<i>H</i>+<i>h</i>, <i>j</i>×<i>W</i>+<i>w</i></sub>表示均值采样的输入值, <i>H</i>、<i>W</i>分别表示采样窗口的长度和宽度。</p>
                </div>
                <div class="p1">
                    <p id="69"><i>RNN</i>在<i>CNN</i>的基础上增加了时间维度, <i>RNN</i>卷积层对应输入为连续的视频帧, 以下给出<i>RNN</i>卷积计算过程</p>
                </div>
                <div class="p1">
                    <p id="70" class="code-formula">
                        <mathml id="70"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Y</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mi>j</mi><mo>, </mo><mi>k</mi></mrow><mi>l</mi></msubsup><mo>=</mo><mi>f</mi><mo stretchy="false"> (</mo><mstyle displaystyle="true"><msub><mo>∑</mo><mrow><mi>i</mi><mo>∈</mo><mi>m</mi><msub><mrow></mrow><mi>h</mi></msub></mrow></msub><mspace width="0.25em" /></mstyle><mstyle displaystyle="true"><msub><mo>∑</mo><mrow><mi>j</mi><mo>∈</mo><mi>n</mi><msub><mrow></mrow><mi>w</mi></msub></mrow></msub><mspace width="0.25em" /></mstyle><mstyle displaystyle="true"><msub><mo>∑</mo><mrow><mi>k</mi><mo>∈</mo><mi>q</mi><msub><mrow></mrow><mi>t</mi></msub></mrow></msub><mspace width="0.25em" /></mstyle><mi>x</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mi>j</mi><mo>, </mo><mi>k</mi></mrow><mi>l</mi></msubsup><mo>+</mo><mi>b</mi><msup><mrow></mrow><mi>l</mi></msup><mo stretchy="false">) </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="71">其中, <i>Y</i><mathml id="72"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mi>j</mi><mo>, </mo><mi>k</mi></mrow><mi>l</mi></msubsup></mrow></math></mathml>、<i>x</i><mathml id="73"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mi>j</mi><mo>, </mo><mi>k</mi></mrow><mi>l</mi></msubsup></mrow></math></mathml>表示<i>RNN</i>第<i>l</i>层输出、输入的第 (<i>i</i>, <i>j</i>, <i>k</i>) 个点的值, 以下给出<i>RNN</i>均值采样计算式</p>
                </div>
                <div class="p1">
                    <p id="74" class="code-formula">
                        <mathml id="74"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>y</mi><msub><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mi>j</mi><mo>, </mo><mi>k</mi></mrow></msub><mo>=</mo><mfrac><mn>1</mn><mrow><mi>Η</mi><mi>W</mi><msup><mi>Τ</mi><mo>′</mo></msup></mrow></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>h</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>Η</mi><mo>-</mo><mn>1</mn></mrow></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>w</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>W</mi><mo>-</mo><mn>1</mn></mrow></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mrow><msup><mi>Τ</mi><mo>′</mo></msup><mo>-</mo><mn>1</mn></mrow></munderover><mi>x</mi></mstyle></mrow></mstyle></mrow></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mo>×</mo><mi>Η</mi><mo>+</mo><mi>h</mi><mo>, </mo><mi>j</mi><mo>×</mo><mi>W</mi><mo>+</mo><mi>w</mi><mo>, </mo><mi>k</mi><mo>×</mo><mi>Τ</mi><mo>+</mo><mi>t</mi></mrow></msub><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>4</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="75">其中, <i>T</i>′表示视频采样窗口帧长。</p>
                </div>
                <div class="p1">
                    <p id="76">采用CNN与RNN构建双流网络架构, 用于采用视频动作片段的外观特征与运动特征的关联性判断目标的行为。CNN与RNN之间的融合不是简单的将CNN叠加至RNN中, 首先需要考虑的是视频动作片段的外观特征和运动特征图大小是否相同, 假设不相同则需要对较小的特征进行上采样, 在此基础上需要考虑CNN和RNN神经网络通道间的相应关系, 利用下式给出构建双流网络构架采用的特征融合方法</p>
                </div>
                <div class="p1">
                    <p id="77" class="code-formula">
                        <mathml id="77"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>y</mi><msup><mrow></mrow><mrow><mi>s</mi><mi>u</mi><mi>m</mi></mrow></msup><mo>=</mo><mi>f</mi><mspace width="0.25em" /><msup><mrow></mrow><mrow><mi>s</mi><mi>u</mi><mi>m</mi></mrow></msup><mo stretchy="false"> (</mo><mi>m</mi><msup><mrow></mrow><mi>a</mi></msup><mo>, </mo><mi>m</mi><msup><mrow></mrow><mi>b</mi></msup><mo stretchy="false">) </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>5</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="78">式中, 将CNN和RNN神经网络的特征图<i>m</i><sup><i>a</i></sup>∈<i>R</i><sup><i>H</i>×<i>W</i>×<i>D</i></sup>和<i>m</i><sup><i>b</i></sup>∈<i>R</i><sup><i>H</i>′×<i>W</i>′×<i>D</i>′</sup>, 依据求和的形式融合运动特征和外观特征, 构建运动行为组合特征</p>
                </div>
                <div class="p1">
                    <p id="79" class="code-formula">
                        <mathml id="79"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>y</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mi>j</mi><mo>, </mo><mi>d</mi></mrow><mrow><mi>s</mi><mi>u</mi><mi>m</mi></mrow></msubsup><mo>=</mo><mo stretchy="false"> (</mo><mi>m</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mi>j</mi><mo>, </mo><mi>d</mi></mrow><mi>a</mi></msubsup><mo>+</mo><mi>m</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mi>j</mi><mo>, </mo><mi>d</mi></mrow><mi>b</mi></msubsup><mo stretchy="false">) </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>6</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="80">其中, <i>y</i><mathml id="81"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mi>j</mi><mo>, </mo><mi>d</mi></mrow><mrow><mi>s</mi><mi>u</mi><mi>m</mi></mrow></msubsup></mrow></math></mathml>∈<i>R</i><sup><i>H</i>″×<i>W</i>″×<i>D</i>″</sup>, <i>H</i>、<i>W</i>分别表示新生成的运动行为描述特征图高度、宽度, <i>D</i>表示特征图通道数量, 满足以下关系<i>H</i>=<i>H</i>′=<i>H</i>″, <i>W</i>=<i>W</i>′=<i>W</i>″, <i>D</i>=<i>D</i>′=<i>D</i>″。上式可用于CNN和RNN神经网络卷基层、全连接层以及池化层的融合。</p>
                </div>
                <div class="p1">
                    <p id="82"><i>CNN</i>神经网络的融合位置可在<i>RNN</i>神经网络的任何位置, 在<i>RNN</i>神经网络中的约束条件仅是在时间<i>t</i>用于融合外观特征图<i>M</i><mathml id="83"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>t</mi><mi>a</mi></msubsup></mrow></math></mathml>∈<i>R</i><sup><i>H</i>×<i>W</i>×<i>D</i></sup>和运动特征图<i>M</i><mathml id="84"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>t</mi><mi>b</mi></msubsup></mrow></math></mathml>∈<i>R</i><sup><i>H</i>′×<i>W</i>′×<i>D</i>′</sup>具有相同的规模, 即<i>H</i>=<i>H</i>′, <i>W</i>=<i>W</i>′, <i>D</i>=<i>D</i>′, <i>H</i>、<i>W</i>、<i>D</i>分别表示新生成的特征图像素宽度、高度以及通道的数量。为了利用外观特征和运动特征在像素级别上的关联性, 在原始双流网络架构的基础上进行了改进, 选择了隐含层进行了融合, 以便更有效地对特征进行描述, 去除CNN神经网络在Conv5后的结构, 在数据集上进行再训练, 根据前馈与方向传播调整融合后的双流网络架构参数。采用训练好的双流网络架构将需要分类的视频动作片段输入到神经网络中, 输出融合隐含层的特征图作为运动目标行为识别的关键特征。</p>
                </div>
                <div class="p1">
                    <p id="85">为了利用视频动作片段的运动特征和行为特征, 构建了双流网络架构, 该结构含有CNN神经网络和RNN神经网络。CNN、RNN均以等间隔抽样视频动作片段的一系列光流图片作为输入, 提取视频动作片段的运动特征和外观特征, 并将这两方面信息进行融合, 用于辨别视频人物行为类别。</p>
                </div>
                <h4 class="anchor-tag" id="86" name="86"><b>3.2 运动行为组合特征的分类识别</b></h4>
                <div class="p1">
                    <p id="87">采用视频动作片段的运动特征和外观特征, 进行人物行为组合特征的分类识别, 具体过程如下所述:</p>
                </div>
                <div class="p1">
                    <p id="88">假设, 对`<i>N</i>个类别的行为视频进行识别, 则需要构造`<i>N</i>× (`<i>N</i>-1) /2个分类器, 训练不同的分类器使用投票法, 票数最多的结果即为未知运动目标行为样本所属类别。设定, <i>n</i><sub><i>i</i>′</sub>、<i>n</i><sub><i>j</i>′</sub>分别表示第<i>i</i>′类和第<i>j</i>′类行为视频训练样本数量, 则以<i>i</i>′类为正例, 第<i>j</i>′类为负类, 构成的行为视频训练集可利用下式表示</p>
                </div>
                <div class="p1">
                    <p id="89" class="code-formula">
                        <mathml id="89"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>Τ</mi><msub><mrow></mrow><mrow><msup><mi>i</mi><mo>′</mo></msup><mo>, </mo><msup><mi>j</mi><mo>′</mo></msup></mrow></msub><mo>=</mo><mo stretchy="false">{</mo><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mrow><msup><mi>k</mi><mo>′</mo></msup><mo>, </mo><msup><mi>i</mi><mo>′</mo></msup><mo>, </mo><msup><mi>j</mi><mo>′</mo></msup></mrow></msub><mo>, </mo><mi>y</mi><msub><mrow></mrow><mrow><msup><mi>k</mi><mo>′</mo></msup><mo>, </mo><msup><mi>i</mi><mo>′</mo></msup><mo>, </mo><msup><mi>j</mi><mo>′</mo></msup></mrow></msub><mo stretchy="false">) </mo><mrow><mo>|</mo><mrow><msup><mi>k</mi><mo>′</mo></msup><mo>=</mo><mn>1</mn><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>n</mi><msub><mrow></mrow><msup><mi>i</mi><mo>′</mo></msup></msub><mo>+</mo><mi>n</mi><msub><mrow></mrow><msup><mi>j</mi><mo>′</mo></msup></msub><mo>, </mo></mrow></mrow></mtd></mtr><mtr><mtd><mi>y</mi><msub><mrow></mrow><mrow><msup><mi>k</mi><mo>′</mo></msup><mo>, </mo><msup><mi>i</mi><mo>′</mo></msup><mo>, </mo><msup><mi>j</mi><mo>′</mo></msup></mrow></msub><mo>∈</mo><mo stretchy="false">{</mo><mo>+</mo><mn>1</mn><mo>, </mo><mo>-</mo><mn>1</mn><mo stretchy="false">}</mo><mo stretchy="false">}</mo><mspace width="0.25em" /><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>7</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="90">其中, <i>x</i><sub><i>k</i>′, <i>i</i>′, <i>j</i>′</sub>表示此样本属于第<i>i</i>′类或是第<i>j</i>′类, 在<i>T</i><sub><i>i</i>′, <i>j</i>′</sub>行为视频训练集中编号为<i>k</i>′。<i>y</i><sub><i>k</i>′, <i>i</i>′, <i>j</i>′</sub>表示<i>x</i><sub><i>k</i>′, <i>i</i>′, <i>j</i>′</sub>所属的类别, 构造并求解相应的二次规划问题, 可得到识别决策函数</p>
                </div>
                <div class="p1">
                    <p id="91" class="code-formula">
                        <mathml id="91"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>f</mi><msub><mrow></mrow><mrow><msup><mi>i</mi><mo>′</mo></msup><mo>, </mo><msup><mi>j</mi><mo>′</mo></msup></mrow></msub><mo>=</mo><mi>sgn</mi><mo stretchy="false"> (</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><msup><mi>k</mi><mo>′</mo></msup><mo>=</mo><mn>1</mn></mrow><mi>l</mi></munderover><mi>α</mi></mstyle><msubsup><mrow></mrow><mrow><msup><mi>k</mi><mo>′</mo></msup><mo>, </mo><msup><mi>i</mi><mo>′</mo></msup><mo>, </mo><msup><mi>j</mi><mo>′</mo></msup></mrow><mo>*</mo></msubsup><mi>y</mi><msub><mrow></mrow><mrow><msup><mi>k</mi><mo>′</mo></msup><mo>, </mo><msup><mi>i</mi><mo>′</mo></msup><mo>, </mo><mi>j</mi></mrow></msub><mi>Κ</mi><mo stretchy="false"> (</mo><mi>x</mi><mo>, </mo><mi>x</mi><msub><mrow></mrow><mrow><msup><mi>k</mi><mo>′</mo></msup><mo>, </mo><msup><mi>i</mi><mo>′</mo></msup><mo>, </mo><mi>j</mi></mrow></msub><mo stretchy="false">) </mo><mo>+</mo><mi>b</mi><msubsup><mrow></mrow><mrow><msup><mi>i</mi><mo>′</mo></msup><mo>, </mo><msup><mi>j</mi><mo>′</mo></msup></mrow><mo>*</mo></msubsup><mo stretchy="false">) </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>8</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="92">其中, <i>α</i><sup>*</sup><sub><i>k</i>′, <i>i</i>′, <i>j</i>′</sub>、<i>y</i><sub><i>k</i>′, <i>i</i>′, <i>j</i></sub>表示运动行为组合特征分类识别最优化问题的解。假设满足条件<i>f</i><sub><i>i</i>′, <i>j</i>′</sub>=1, 则第<i>i</i>′类获得一票;假设满足条件<i>f</i><sub><i>i</i>′, <i>j</i>′</sub>=-1, 则第<i>j</i>′类得到一票。</p>
                </div>
                <div class="p1">
                    <p id="93">设定<i>vote</i> (<i>i</i>′) 表示累积第<i>i</i>′类所得到的票数, 在运动目标行为分类完全正确的情况下, 设定未知运动目标行为样本<i>x</i>属于第<i>i</i>′类, 则应有<i>vote</i> (<i>i</i>′) =`<i>N</i>-1票。在实际行为识别过程中, 复杂场景中噪声以及未知因素的影响, 分类器难以实现对全部未知样本的精确分类。但从概率角度进行分析, 得票最高的类别, 为未知样本<i>x</i>正确所属类别的几率较大。根据此机制, 给出运动行为组合特征的分类识别结果</p>
                </div>
                <div class="p1">
                    <p id="94" class="code-formula">
                        <mathml id="94"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>s</mi><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>arg</mi><mspace width="0.25em" /></mrow></mstyle><msup><mi>i</mi><mo>′</mo></msup></munder><mo stretchy="false"> (</mo><mi>max</mi><mo stretchy="false"> (</mo><mi>v</mi><mi>o</mi><mi>t</mi><mi>e</mi><mo stretchy="false"> (</mo><msup><mi>i</mi><mo>′</mo></msup><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>9</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <h3 id="95" name="95" class="anchor-tag"><b>4 实验结果与分析</b></h3>
                <div class="p1">
                    <p id="96">为了验证所提模型的综合有效性, 需要进行一次实验, 实验数据集来源于UCF50和UCF101数据集, 这两类数据集中包含从BBC/ESPN的广播电视频道采集的不同样本, 以及从视频网站上下载来的样本, 背景复杂, 符合复杂场景的要求。UCF101数据集中含有12580段短视频, 共101种类型。</p>
                </div>
                <div class="p1">
                    <p id="97">对于UCF50和UCF101数据集中的视频数据, 采用人工标注的形式, 通过添加正确标签数据对视频进行识别, 识别结果对应动作和行为两种类型。在UCF50数据集上, 分别对比所提模型、结合稠密光流轨迹和稀疏编码框架的行为识别模型以及融合主成分分析方法和高斯混合模型的行为识别模型的精确度 (%) 、召回率 (%) , 对比结果如表1、2所示。表1中, DN表示不同模型, N-H表示所提模型 (无隐含层) , H-H表示所提模型 (有隐含层) , UV、UR分别表示结合稠密光流轨迹和稀疏编码框架的行为识别模型以及融合主成分分析方法和高斯混合模型的行为识别模型, Accuracy表示精确度 (%) , Recall rate表示召回率 (%) 。</p>
                </div>
                <div class="area_img" id="98">
                    <p class="img_tit"><b>表1 UCF50数据集上动作识别的实验结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="98" border="1"><tr><td><br />DN</td><td>Accuracy/%</td><td>Recall rate/%</td></tr><tr><td><br />UV</td><td>49.5±0.8</td><td>45.0±2.7</td></tr><tr><td><br />UR</td><td>47.5±0.8</td><td>50.5±3.5</td></tr><tr><td><br />N-H</td><td>52.5±0.9</td><td>51.5±3.0</td></tr><tr><td><br />H-H</td><td>56.8±0.9</td><td>53.2±2.5</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="99">
                    <p class="img_tit"><b>表2 UCF50数据集上行为识别的实验结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="99" border="1"><tr><td><br />DN</td><td>Accuracy/%</td><td>Recall rate/%</td></tr><tr><td><br />UV</td><td>43.0±2.5</td><td>45.4±4.5</td></tr><tr><td><br />UR</td><td>42.5±2.8</td><td>48.5±2.9</td></tr><tr><td><br />N-H</td><td>38.2±2.8</td><td>48.5±2.7</td></tr><tr><td><br />H-H</td><td>47.9±2.2</td><td>52.5±2.5</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="100">分析表1和表2结果可知, 所提模型 (有隐含层) 在视频动作识别和行为识别上优于无隐含层的模型。所提模型 (有隐含层) 相比结合稠密光流轨迹和稀疏编码框架的行为识别模型, 动作识别精确度提高了7.3个百分点, 召回率提高了8个百分点;所提模型 (有隐含层) 相比融合主成分分析方法和高斯混合模型的行为识别模型动作识别精确度提高了9.4个百分点, 召回率提高了1.7个百分点。所提模型 (有隐含层) 相比结合稠密光流轨迹和稀疏编码框架的行为识别模型, 行为识别精度提高了4.6个百分点, 召回率提高了5.1个百分点;所提模型 (有隐含层) 相比融合主成分分析方法和高斯混合模型的行为识别模型行为识别精确度提高了6.8个百分点, 召回率提高了3.6个百分点。结果表明, 所提模型加入隐含层后的识别效果是非常显著的。</p>
                </div>
                <div class="p1">
                    <p id="101">在UCF101数据集上, 分别对比所提模型、结合稠密光流轨迹和稀疏编码框架的行为识别模型以及融合主成分分析方法和高斯混合模型的行为识别模型的精确度 (%) 、召回率 (%) , 对比结果如表3、4所示。</p>
                </div>
                <div class="area_img" id="102">
                    <p class="img_tit"><b>表3 UCF101数据集上动作识别的实验结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="102" border="1"><tr><td><br />DN</td><td>Accuracy/%</td><td>Recall rate/%</td></tr><tr><td><br />UV</td><td>39.2±0.3</td><td>45.0±2.7</td></tr><tr><td><br />UR</td><td>41.2±0.4</td><td>39.8±2.4</td></tr><tr><td><br />H-H</td><td>45.5±0.5</td><td>46.7±2.1</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="103">
                    <p class="img_tit"><b>表4 UCF101数据集上行为识别的实验结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="103" border="1"><tr><td><br />DN</td><td>Accuracy/%</td><td>Recall rate/%</td></tr><tr><td><br />UV</td><td>42.5±2.3</td><td>42.6±3.0</td></tr><tr><td><br />UR</td><td>40.5±2.5</td><td>49.5±2.5</td></tr><tr><td><br />H-H</td><td>45.2±1.6</td><td>49.5±3.0</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="104">分析表3和表4结果可知, 所提模型相比结合稠密光流轨迹和稀疏编码框架的行为识别模型在动作识别精确度上提升了6.5个百分点, 在召回率上提高了1.1个百分点;所提模型相比融合主成分分析方法和高斯混合模型的行为识别模型在动作识别精确度上提升了4.4个百分点, 在召回率上提高了6.6个百分点。所提模型相比结合稠密光流轨迹和稀疏编码框架的行为识别模型在行为识别精确度上提升了2.0个百分点, 在召回率上提高了6.9个百分点;所提模型相比融合主成分分析方法和高斯混合模型的行为识别模型在行为识别精确度上提升了3.8个百分点, 在召回率上提高了0.5个百分点。</p>
                </div>
                <div class="p1">
                    <p id="105">结果表明, 所提模型在UCF50和UCF101数据集上胜过了稠密光流轨迹和稀疏编码框架的行为识别模型以及融合主成分分析方法和高斯混合模型的行为识别模型。</p>
                </div>
                <div class="p1">
                    <p id="106">在UCF50数据集上, 对不同识别模型相应的行为平均识别时间 (ms) 进行统计, 统计结果如图1所示。为了简化描述, 将所提模型、结合稠密光流轨迹和稀疏编码框架的行为识别模型、融合主成分分析方法和高斯混合模型的行为识别模型分别表示为K、Q、R。</p>
                </div>
                <div class="area_img" id="107">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJZ201908082_107.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 不同识别模型的识别效率对比图" src="Detail/GetImg?filename=images/JSJZ201908082_107.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图1 不同识别模型的识别效率对比图</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJZ201908082_107.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="108">分析图1可知, 当行为视频帧数为100帧时, 所提模型的视频行为识别时间为38ms左右, 结合稠密光流轨迹和稀疏编码框架的行为识别模型以及融合主成分分析方法和高斯混合模型的行为识别模型相应的识别时间分别为108ms和85ms。将实验结果进行对比可知, 所提模型进行行为识别的速度最快, 识别效率最高, 能够满足实际应用中对行为识别的实时性要求。</p>
                </div>
                <h3 id="109" name="109" class="anchor-tag"><b>5 结论</b></h3>
                <div class="p1">
                    <p id="110">提出双层网络架构和隐含层相融合的行为识别模型。所提模型采用双流网络构架来提取视频动作片段的外观特征和时间特征, 根据双流网络构架获取动作与行为之间的关联关系。在此框架下加入隐含层, 加深和丰富视频动作片段的外观特征和时间特征描述。并通过加权融合的方式对外观特征和时间特征进行融合, 采用支持向量机对运动行为组合特征进行行为识别。实验结果表明, 所提模型在<b>UCF50</b>和<b>UCF101</b>数据集上优于当前行为识别模型。</p>
                </div>
                <div class="p1">
                    <p id="111">所提模型相比结合稠密光流轨迹和稀疏编码框架的行为识别模型以及融合主成分分析方法和高斯混合模型的行为识别模型, 有效提高了行为识别率和召回率以及识别速度。但仍然存在很多不足之处需要改进, 未来阶段将研究采用更有效的分类方法来进一步提高行为识别率。</p>
                </div>
                <div class="area_img" id="121">
                                <img alt="" src="Detail/GetImg?filename=images/JSJZ201908082_12100.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                            <p class="img_tit"></p>

                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="122">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XAJT201802004&amp;v=MDk4NzR6cXFCdEdGckNVUjdxZlp1Wm9GeTdnVmJ6T1BTekJlckc0SDluTXJZOUZZSVFLREg4NHZSNFQ2ajU0TzM=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> 王晓芳, 齐春.一种运用显著性检测的行为识别方法[J].西安交通大学学报, 2018, 52 (2) :24-29.
                            </a>
                        </p>
                        <p id="124">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXJS201704008&amp;v=MTM3NDJYQmZiRzRIOWJNcTQ5RmJJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVab0Z5N2dWYnpPSWo=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> 梅阳, 等.一种基于关键帧的人体行为识别方法[J].光学技术, 2017, 43 (4) :323-328.
                            </a>
                        </p>
                        <p id="126">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXJM201703035&amp;v=MDcxNjR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpvRnk3Z1Ziek9JalhCWTdHNEg5Yk1ySTlHWVlRS0RIODQ=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[3]</b> 刘智, 黄江涛, 冯欣.构建多尺度深度卷积神经网络行为识别模型[J].光学精密工程, 2017, 25 (3) :799-805.
                            </a>
                        </p>
                        <p id="128">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GFKJ201602012&amp;v=Mjg1NzQ5RVpvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVab0Z5N2dWYnpPSWl2QVpMRzRIOWZNclk=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b> 王军, 夏利民, 夏胜平.利用生成模型的人体行为识别[J].国防科技大学学报, 2016, 38 (2) :68-74.
                            </a>
                        </p>
                        <p id="130">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DKDX201702016&amp;v=MjM3NTJGeTdnVmJ6T0lTYlBkckc0SDliTXJZOUVZb1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1Wm8=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b> 王智文, 等.基于动作子空间和权重条件随机场的行为识别[J].电子科技大学学报, 2017, 46 (2) :412-418.
                            </a>
                        </p>
                        <p id="132">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=SJSJ201606039&amp;v=MTA3NjVSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1Wm9GeTdnVmJ6T05pZllaTEc0SDlmTXFZOUdiWVFLREg4NHY=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b> 张玉燕, 等.基于隐动态条件神经域的在线行为识别方法[J].计算机工程与设计, 2016, 37 (6) :1632-1635.
                            </a>
                        </p>
                        <p id="134">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201601036&amp;v=MjM4NDBESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVab0Z5N2dWYnpPTHo3QmQ3RzRIOWZNcm85R1lvUUs=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b> 赵晓健, 曾晓勤.基于稠密光流轨迹和稀疏编码算法的行为识别方法[J].计算机应用, 2016, 36 (1) :181-187.
                            </a>
                        </p>
                        <p id="136">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201710033&amp;v=MTAxNjFIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpvRnk3Z1Ziek9MejdCYmJHNEg5Yk5yNDlHWjRRS0Q=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b> 高晨兰, 朱嘉钢.静止背景下的人体行为识别方法[J].计算机工程, 2017, 43 (10) :192-197.
                            </a>
                        </p>
                        <p id="138">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XXCN201605003&amp;v=MTYyMTFNcW85Rlo0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVab0Z5N2dWYnpPUFRYSVlMRzRIOWY=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b> 周同驰, 等.基于有效轨迹和多重方向模式的行为识别[J].信号处理, 2016, 32 (5) :519-527.
                            </a>
                        </p>
                        <p id="140">
                            <a id="bibliography_10" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJZ201704048&amp;v=MjcyMjl1Wm9GeTdnVmJ6T0x6N0JkTEc0SDliTXE0OUJiSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlo=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[10]</b> 刘桥.游泳运动员姿势识别校正方法研究与仿真[J].计算机仿真, 2017, 34 (4) :227-230.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJZ201908082" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJZ201908082&amp;v=MDE4MTJwNDlOWm9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpvRnk3Z1Ziek9MejdCZExHNEg5ak0=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=842_VlRz_g1CMCLi-M0twVw-4EOFG_noWyi_OsMnrAg1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
