<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=NOOK-mvTMnxl11s6oPYV0boPXN3iP4N7Rc-A56nk4KI1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2fKXReader%2fDetail%3fTIMESTAMP%3d637139885664322500%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJZ201909061%26RESULT%3d1%26SIGN%3dQQfD7Nq5Y48HJ4YWW6e62CdUZuY%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJZ201909061&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJZ201909061&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJZ201909061&amp;v=MDk5NjF6N0JkTEc0SDlqTXBvOURaWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1Wm9GeW5rVzd6T0w=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc1FTMHdYQkFCcm1KK2ZPTjlDdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#31" data-title="&lt;b&gt;1 引言&lt;/b&gt; "><b>1 引言</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#34" data-title="&lt;b&gt;2 应用算法背景&lt;/b&gt; "><b>2 应用算法背景</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#35" data-title="&lt;b&gt;2.1 单演信号&lt;/b&gt;"><b>2.1 单演信号</b></a></li>
                                                <li><a href="#52" data-title="&lt;b&gt;2.2 中心对称三值模式(CS-LTP&lt;/b&gt;)"><b>2.2 中心对称三值模式(CS-LTP</b>)</a></li>
                                                <li><a href="#61" data-title="&lt;b&gt;2.3 改进的M-CSLTP算法&lt;/b&gt;"><b>2.3 改进的M-CSLTP算法</b></a></li>
                                                <li><a href="#74" data-title="&lt;b&gt;2.4 M-CSLTP算法流程&lt;/b&gt;"><b>2.4 M-CSLTP算法流程</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#81" data-title="&lt;b&gt;3 实验设计与结果分析&lt;/b&gt; "><b>3 实验设计与结果分析</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#82" data-title="&lt;b&gt;3.1 实验参数设置&lt;/b&gt;"><b>3.1 实验参数设置</b></a></li>
                                                <li><a href="#93" data-title="&lt;b&gt;3.2 实验结果分析&lt;/b&gt;"><b>3.2 实验结果分析</b></a></li>
                                                <li><a href="#107" data-title="&lt;b&gt;3.3 不同算法对比结果&lt;/b&gt;"><b>3.3 不同算法对比结果</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#117" data-title="&lt;b&gt;4 总结&lt;/b&gt; "><b>4 总结</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#51" data-title="&lt;b&gt;图1 单演滤波效果图&lt;/b&gt;"><b>图1 单演滤波效果图</b></a></li>
                                                <li><a href="#54" data-title="&lt;b&gt;图2&lt;/b&gt;&lt;i&gt;CS&lt;/i&gt;-&lt;i&gt;LTP&lt;/i&gt;&lt;b&gt;编码&lt;/b&gt;"><b>图2</b><i>CS</i>-<i>LTP</i><b>编码</b></a></li>
                                                <li><a href="#60" data-title="&lt;b&gt;图3&lt;/b&gt;&lt;i&gt;CS&lt;/i&gt;-&lt;i&gt;LTP&lt;/i&gt;&lt;b&gt;的裂化过程&lt;/b&gt;"><b>图3</b><i>CS</i>-<i>LTP</i><b>的裂化过程</b></a></li>
                                                <li><a href="#76" data-title="&lt;b&gt;图4 表情图像的预处理&lt;/b&gt;"><b>图4 表情图像的预处理</b></a></li>
                                                <li><a href="#80" data-title="&lt;b&gt;图5 实验流程图&lt;/b&gt;"><b>图5 实验流程图</b></a></li>
                                                <li><a href="#85" data-title="&lt;b&gt;表1 子区域大小及块数&lt;/b&gt;"><b>表1 子区域大小及块数</b></a></li>
                                                <li><a href="#97" data-title="&lt;b&gt;表2&lt;/b&gt;&lt;i&gt;JAFFE&lt;/i&gt;&lt;b&gt;数据库上5次实验结果&lt;/b&gt;(%)"><b>表2</b><i>JAFFE</i><b>数据库上5次实验结果</b>(%)</a></li>
                                                <li><a href="#99" data-title="&lt;b&gt;表3&lt;/b&gt;&lt;i&gt;JAFFE&lt;/i&gt;&lt;b&gt;数据库第5次实验的混淆矩阵&lt;/b&gt;"><b>表3</b><i>JAFFE</i><b>数据库第5次实验的混淆矩阵</b></a></li>
                                                <li><a href="#103" data-title="&lt;b&gt;表4&lt;/b&gt;&lt;i&gt;CK&lt;/i&gt;+&lt;b&gt;数据库3次实验结果&lt;/b&gt;(%)"><b>表4</b><i>CK</i>+<b>数据库3次实验结果</b>(%)</a></li>
                                                <li><a href="#106" data-title="&lt;b&gt;表5&lt;/b&gt;&lt;i&gt;CK&lt;/i&gt;+&lt;b&gt;库上第1次实验的混淆矩阵&lt;/b&gt;"><b>表5</b><i>CK</i>+<b>库上第1次实验的混淆矩阵</b></a></li>
                                                <li><a href="#109" data-title="&lt;b&gt;表6 基于整脸各算法识别率的比较&lt;/b&gt;(%)"><b>表6 基于整脸各算法识别率的比较</b>(%)</a></li>
                                                <li><a href="#112" data-title="&lt;b&gt;表7 基于子区域各算法识别率的比较&lt;/b&gt;(%)"><b>表7 基于子区域各算法识别率的比较</b>(%)</a></li>
                                                <li><a href="#115" data-title="&lt;b&gt;表8 不同噪声强度干扰实验&lt;/b&gt;(%)"><b>表8 不同噪声强度干扰实验</b>(%)</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" title=" &lt;i&gt;Y Gaffary&lt;/i&gt;,&lt;i&gt;V Eyharabide&lt;/i&gt;.&lt;i&gt;The Impact of Combining Kinesthetic and Facial Expression Displays on Emotion Recognition by Users&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;International Journal of Human&lt;/i&gt;-&lt;i&gt;Computer Interaction&lt;/i&gt;,2014,30(11):904-920." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=STJD&amp;filename=STJD14100800000681&amp;v=MDA0OTFGWk9zUENuUTRvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp0RmlubFVybklJVjhjYVJRPU5qbkJhcks4SDlITXA0OQ==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc1FTMHdYQkFCcm1KK2ZPTjlDdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         &lt;i&gt;Y Gaffary&lt;/i&gt;,&lt;i&gt;V Eyharabide&lt;/i&gt;.&lt;i&gt;The Impact of Combining Kinesthetic and Facial Expression Displays on Emotion Recognition by Users&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;International Journal of Human&lt;/i&gt;-&lt;i&gt;Computer Interaction&lt;/i&gt;,2014,30(11):904-920.
                                    </a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" title=" &lt;i&gt;M Liu&lt;/i&gt;,&lt;i&gt;et al&lt;/i&gt;.&lt;i&gt;Learning Expressionlets via Universal Manifold Model for Dynamic Facial Expression Recognition&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;IEEE Transactions on Image Processing&lt;/i&gt;,2015,25(12):5920-5932." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning Expressionlets via Universal Manifold Model for Dynamic Facial Expression Recognition">
                                        <b>[2]</b>
                                         &lt;i&gt;M Liu&lt;/i&gt;,&lt;i&gt;et al&lt;/i&gt;.&lt;i&gt;Learning Expressionlets via Universal Manifold Model for Dynamic Facial Expression Recognition&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;IEEE Transactions on Image Processing&lt;/i&gt;,2015,25(12):5920-5932.
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title=" &lt;i&gt;H Gao&lt;/i&gt;,&lt;i&gt;A Y&lt;/i&gt;&#252;&lt;i&gt;ce&lt;/i&gt;,&lt;i&gt;J P Thiran&lt;/i&gt;.&lt;i&gt;Detecting emotional stress from facial expressions for driving safety&lt;/i&gt;[&lt;i&gt;C&lt;/i&gt;].&lt;i&gt;IEEE International Conference on Image Processing&lt;/i&gt;.&lt;i&gt;IEEE&lt;/i&gt;,2014:5961-5965." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Detecting emotional stress from facial expressions for driving safety">
                                        <b>[3]</b>
                                         &lt;i&gt;H Gao&lt;/i&gt;,&lt;i&gt;A Y&lt;/i&gt;&#252;&lt;i&gt;ce&lt;/i&gt;,&lt;i&gt;J P Thiran&lt;/i&gt;.&lt;i&gt;Detecting emotional stress from facial expressions for driving safety&lt;/i&gt;[&lt;i&gt;C&lt;/i&gt;].&lt;i&gt;IEEE International Conference on Image Processing&lt;/i&gt;.&lt;i&gt;IEEE&lt;/i&gt;,2014:5961-5965.
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" title=" &lt;i&gt;W Gu&lt;/i&gt;,&lt;i&gt;et al&lt;/i&gt;.&lt;i&gt;Facial expression recognition using radial encoding of local Gabor features and classifier synthesis&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;Pattern Recognition&lt;/i&gt;,2012,45(1):80-91." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011600738060&amp;v=MDI5OTVPZmJLN0h0RE5xWTlGWStnSERIbzVvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp0RmlubFVybklJVjhjYVJRPU5pZg==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc1FTMHdYQkFCcm1KK2ZPTjlDdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                         &lt;i&gt;W Gu&lt;/i&gt;,&lt;i&gt;et al&lt;/i&gt;.&lt;i&gt;Facial expression recognition using radial encoding of local Gabor features and classifier synthesis&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;Pattern Recognition&lt;/i&gt;,2012,45(1):80-91.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" title=" &lt;i&gt;M Yang&lt;/i&gt;,&lt;i&gt;et al&lt;/i&gt;.&lt;i&gt;Monogenic Binary Coding&lt;/i&gt;:&lt;i&gt;An Efficient Local Feature Extraction Approach to Face Recognition&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;IEEE Transactions on Information Forensics&lt;/i&gt; &amp;amp; &lt;i&gt;Security&lt;/i&gt;,2012,7(6):1738-1751." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Monogenic Binary Coding: An Efficient Local Feature Extraction Approach to Face Recognition">
                                        <b>[5]</b>
                                         &lt;i&gt;M Yang&lt;/i&gt;,&lt;i&gt;et al&lt;/i&gt;.&lt;i&gt;Monogenic Binary Coding&lt;/i&gt;:&lt;i&gt;An Efficient Local Feature Extraction Approach to Face Recognition&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;IEEE Transactions on Information Forensics&lt;/i&gt; &amp;amp; &lt;i&gt;Security&lt;/i&gt;,2012,7(6):1738-1751.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" title=" &lt;i&gt;X Huang&lt;/i&gt;,&lt;i&gt;et al&lt;/i&gt;.&lt;i&gt;Improved Spatiotemporal Local Monogenic Binary Pattern for Emotion Recognition in The Wild&lt;/i&gt;[&lt;i&gt;C&lt;/i&gt;].&lt;i&gt;International Conference on Multimodal Interaction&lt;/i&gt;.&lt;i&gt;ACM&lt;/i&gt;,2014:514-520." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Improved Spatiotemporal Local Monogenic Binary Pattern for Emotion Recognition in The Wild">
                                        <b>[6]</b>
                                         &lt;i&gt;X Huang&lt;/i&gt;,&lt;i&gt;et al&lt;/i&gt;.&lt;i&gt;Improved Spatiotemporal Local Monogenic Binary Pattern for Emotion Recognition in The Wild&lt;/i&gt;[&lt;i&gt;C&lt;/i&gt;].&lt;i&gt;International Conference on Multimodal Interaction&lt;/i&gt;.&lt;i&gt;ACM&lt;/i&gt;,2014:514-520.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" title=" 胡敏,等.融合局部纹理和形状特征的人脸表情识别[&lt;i&gt;J&lt;/i&gt;].电子与信息学报,2018." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZYX201806010&amp;v=MTE5NDJmU2RyRzRIOW5NcVk5RVpJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVab0Z5bmtXN3pPSVQ=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc1FTMHdYQkFCcm1KK2ZPTjlDdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                         胡敏,等.融合局部纹理和形状特征的人脸表情识别[&lt;i&gt;J&lt;/i&gt;].电子与信息学报,2018.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" title=" 叶棪,等.基于多尺度等价模式&lt;i&gt;LBP&lt;/i&gt;的人脸表情识别[&lt;i&gt;J&lt;/i&gt;].计算机与数字工程,2016,44(1):40-44." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSSG201601010&amp;v=MDQ4MjdUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpvRnlua1c3ek9MejdZYWJHNEg5Zk1ybzlFWklRS0RIODR2UjQ=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc1FTMHdYQkFCcm1KK2ZPTjlDdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                         叶棪,等.基于多尺度等价模式&lt;i&gt;LBP&lt;/i&gt;的人脸表情识别[&lt;i&gt;J&lt;/i&gt;].计算机与数字工程,2016,44(1):40-44.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" title=" &lt;i&gt;M Felsberg&lt;/i&gt;,&lt;i&gt;G Sommer&lt;/i&gt;.&lt;i&gt;The monogenic signal&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;Signal Processing IEEE Transactions on&lt;/i&gt;,2009,49(12):3136-3144." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The monogenic signal">
                                        <b>[9]</b>
                                         &lt;i&gt;M Felsberg&lt;/i&gt;,&lt;i&gt;G Sommer&lt;/i&gt;.&lt;i&gt;The monogenic signal&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;Signal Processing IEEE Transactions on&lt;/i&gt;,2009,49(12):3136-3144.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_10" title=" 李伟生,王立逗,周丽芳.一种基于&lt;i&gt;LTP&lt;/i&gt;自适应阈值的人脸识别方法[&lt;i&gt;J&lt;/i&gt;].小型微型计算机系统,2014,35(9):2099-2103." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XXWX201409033&amp;v=MDU0MzZxQnRHRnJDVVI3cWZadVpvRnlua1c3ek9QVFhjZHJHNEg5WE1wbzlHWjRRS0RIODR2UjRUNmo1NE8zenE=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc1FTMHdYQkFCcm1KK2ZPTjlDdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[10]</b>
                                         李伟生,王立逗,周丽芳.一种基于&lt;i&gt;LTP&lt;/i&gt;自适应阈值的人脸识别方法[&lt;i&gt;J&lt;/i&gt;].小型微型计算机系统,2014,35(9):2099-2103.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_11" title=" &lt;i&gt;M Huang&lt;/i&gt;,&lt;i&gt;Z Mu&lt;/i&gt;,&lt;i&gt;H Zeng&lt;/i&gt;.&lt;i&gt;Efficient image classification via sparse coding spatial pyramid matching representation of SIFT&lt;/i&gt;-&lt;i&gt;WCS&lt;/i&gt;-&lt;i&gt;LTP feature&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;Image Processing Iet&lt;/i&gt;,2016,10(1):61-67." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Efficient image classification via sparse coding spatial pyramid matching representation of SIFT-WCS-LTP feature">
                                        <b>[11]</b>
                                         &lt;i&gt;M Huang&lt;/i&gt;,&lt;i&gt;Z Mu&lt;/i&gt;,&lt;i&gt;H Zeng&lt;/i&gt;.&lt;i&gt;Efficient image classification via sparse coding spatial pyramid matching representation of SIFT&lt;/i&gt;-&lt;i&gt;WCS&lt;/i&gt;-&lt;i&gt;LTP feature&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;Image Processing Iet&lt;/i&gt;,2016,10(1):61-67.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_12" title=" 周宇旋,等.判别性完全局部二值模式人脸表情识别[&lt;i&gt;J&lt;/i&gt;].计算机工程与应用,2017,53(4):163-169." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSGG201704029&amp;v=MzIzOTFvRnlua1c3ek9MejdNYWJHNEg5Yk1xNDlIYllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVo=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc1FTMHdYQkFCcm1KK2ZPTjlDdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[12]</b>
                                         周宇旋,等.判别性完全局部二值模式人脸表情识别[&lt;i&gt;J&lt;/i&gt;].计算机工程与应用,2017,53(4):163-169.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_13" title=" &lt;i&gt;K Lekdioui&lt;/i&gt;,&lt;i&gt;et al&lt;/i&gt;.&lt;i&gt;Facial decomposition for expression recognition using texture&lt;/i&gt;/&lt;i&gt;shape descriptors and SVM classifier&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;Signal Processing Image Communication&lt;/i&gt;,2017,58." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES57B3E36284F990F75FEDF89B4735230F&amp;v=MTQ2MDRZZk9HUWxmQnJMVTA1dDVoeGJtM3c2ND1OaWZPZmJhL2JOSzVySWxIYk85NUJYVTV1UkVXbkVvSlBuZnIzaFl5ZXJlV1JycnBDT052RlNpV1dyN0pJRnBtYUJ1SA==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc1FTMHdYQkFCcm1KK2ZPTjlDdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[13]</b>
                                         &lt;i&gt;K Lekdioui&lt;/i&gt;,&lt;i&gt;et al&lt;/i&gt;.&lt;i&gt;Facial decomposition for expression recognition using texture&lt;/i&gt;/&lt;i&gt;shape descriptors and SVM classifier&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;Signal Processing Image Communication&lt;/i&gt;,2017,58.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_14" title=" &lt;i&gt;D Selvathi&lt;/i&gt;,&lt;i&gt;S Bama&lt;/i&gt;.&lt;i&gt;Phase based distance regularized level set for the segmentation of ultrasound kidney images&lt;/i&gt;[&lt;i&gt;M&lt;/i&gt;].&lt;i&gt;Elsevier Science Inc&lt;/i&gt;.2017." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Phase based distance regularized level set for the segmentation of ultrasound kidney images">
                                        <b>[14]</b>
                                         &lt;i&gt;D Selvathi&lt;/i&gt;,&lt;i&gt;S Bama&lt;/i&gt;.&lt;i&gt;Phase based distance regularized level set for the segmentation of ultrasound kidney images&lt;/i&gt;[&lt;i&gt;M&lt;/i&gt;].&lt;i&gt;Elsevier Science Inc&lt;/i&gt;.2017.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJZ" target="_blank">计算机仿真</a>
                2019,36(09),304-308+351             </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>改进单演中心对称LTP的表情识别算法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E7%9D%BF&amp;code=22086719&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc1FTMHdYQkFCcm1KK2ZPTjlDdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王睿</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%A9%AC%E4%B8%B9&amp;code=38818128&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc1FTMHdYQkFCcm1KK2ZPTjlDdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">马丹</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%88%98%E9%B9%8F&amp;code=36269450&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc1FTMHdYQkFCcm1KK2ZPTjlDdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">刘鹏</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%B5%96%E6%83%A0%E6%88%90&amp;code=09253980&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc1FTMHdYQkFCcm1KK2ZPTjlDdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">赖惠成</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%96%B0%E7%96%86%E5%A4%A7%E5%AD%A6%E4%BF%A1%E6%81%AF%E7%A7%91%E5%AD%A6%E4%B8%8E%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0181515&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc1FTMHdYQkFCcm1KK2ZPTjlDdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">新疆大学信息科学与工程学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>为了解决人脸表情识别过程中传统方法存在的特征模式表征能力不足的问题,提出了改进的基于表情子区域单演中心对称局部三值编码的表情特征算法。通过对表情子区域提取单演特征获得人脸表情的振幅、相位、方向信息;针对获取的单演特征进行量化编码,其中单演振幅特征采用了改进的中心对称动态阈值三值编码,进而得到人脸表情的单演中心对称局部三值特征;将单演中心对称局部三值模式特征的直方图引入最近邻分类器进行分类,最终实现人脸表情识别。在JAFFE数据库和CK+数据库上的仿真结果表明:改进算法相比传统特征方法具有更强的特征解析能力,更高的人脸表情识别率。在光照变化、噪声干扰等复杂环境中,算法仍然具有较强的鲁棒性。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%A1%A8%E6%83%85%E8%AF%86%E5%88%AB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc1FTMHdYQkFCcm1KK2ZPTjlDdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">表情识别;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8D%95%E6%BC%94%E7%89%B9%E5%BE%81&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc1FTMHdYQkFCcm1KK2ZPTjlDdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">单演特征;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%B8%AD%E5%BF%83%E5%AF%B9%E7%A7%B0%E5%B1%80%E9%83%A8%E4%B8%89%E5%80%BC%E7%BC%96%E7%A0%81&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc1FTMHdYQkFCcm1KK2ZPTjlDdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中心对称局部三值编码;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8A%A8%E6%80%81%E9%98%88%E5%80%BC&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc1FTMHdYQkFCcm1KK2ZPTjlDdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">动态阈值;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    王睿(1997-)，男(汉族)，新疆乌鲁木齐人，硕士研究生，主要研究领域为图像处理，模式识别;;
                                </span>
                                <span>
                                    刘鹏(1993-)，男(汉族)，山东日照人，硕士研究生，主要研究领域为图像处理，视频跟踪;;
                                </span>
                                <span>
                                    马丹(1992-)女(汉族)，河北张家口人，硕士研究生，主要研究领域为图像处理，人脸检测;;
                                </span>
                                <span>
                                    赖惠成(1963-)，男(汉族)，四川德阳人，教授，硕士研究生导师，主要研究领域为图像理解与识别、通信技术。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-05-14</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金(61561048);</span>
                    </p>
            </div>
                    <h1><b>Improvement of Monogenic Center-Symmetric LTP Expression Recognition Algorithm</b></h1>
                    <h2>
                    <span>WANG Rui</span>
                    <span>MA Dan</span>
                    <span>LIU Peng</span>
                    <span>LAI Hui-Cheng</span>
            </h2>
                    <h2>
                    <span>College of Information Science and Engineering,Xinjiang University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>In order to solve the problem that feature pattern representation is insufficient in facial expression recognition, an algorithm based on center-symmetric local ternary encoding of the sub-region monogenic feature is proposed in the paper. The amplitude, phase, and orientation information of the face expression were obtained by performing monogenic feature extraction on the expression sub-region; and the monogenic feature obtained was quantized and coded, wherein the monogenic amplitude feature adopted an improved dynamic threshold center-symmetric local ternary coding, thereby the monogenic center-symmetry local ternary feature of the facial expression was obtained. The histogram of the local symmetry local ternary feature was introduced into the nearest neighbor classifier for classification. The simulation results on the JEFFE and CK+ database show that the improved algorithm has stronger feature parsing ability than the traditional feature method and improves the facial expression recognition rate. The algorithm still has strong robustness in complex environments such as illumination changes and noise interference.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Expression%20recognition&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc1FTMHdYQkFCcm1KK2ZPTjlDdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Expression recognition;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Monogenic%20signal%20analysis&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc1FTMHdYQkFCcm1KK2ZPTjlDdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Monogenic signal analysis;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Center-symmetric%20local%20ternary%20pattern&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc1FTMHdYQkFCcm1KK2ZPTjlDdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Center-symmetric local ternary pattern;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Dynamic%20threshold&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc1FTMHdYQkFCcm1KK2ZPTjlDdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Dynamic threshold;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-05-14</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="31" name="31" class="anchor-tag"><b>1 引言</b></h3>
                <div class="p1">
                    <p id="32">表情在人类情感交流中起着重要的作用,由于实际应用中对表情识别要求的不断提高,表情识别已经成为非常重要的研究课题。近年来,表情识别已被广泛应用于人机交互、智能机器人、安全驾驶等领域<citation id="133" type="reference"><link href="3" rel="bibliography" /><link href="5" rel="bibliography" /><link href="7" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>,<a class="sup">3</a>]</sup></citation>。但由于表情动作组合的复杂性,使得表情特征的表达并不充分或存在冗余信息。如何选择一种高效、鲁棒的表情特征成为当前研究的热点。Gu W等<citation id="128" type="reference"><link href="9" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>利用径向网格对多尺度Gabor进行编码,模仿HVC的地形图结构,产生代表面部表情的全局特征。Yang M等<citation id="129" type="reference"><link href="11" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>提出了一种单演二值模式(MBP),从新的角度来进行特征的提取,该方法是将单演滤波后的振幅和方向信息装载在一个两层特征框架内增强面部表征。Huang X等<citation id="130" type="reference"><link href="13" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>提出基于时空单演二值模式(STLMBP),在单演幅值信息的基础上结合单演方向编码信息,增强了表情特征对光照的鲁棒性,取得较好的分类效果。胡敏等<citation id="131" type="reference"><link href="15" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>采用CS-LSBP与HOAG融合算法,对纹理细节描绘更加充分。叶棪等<citation id="132" type="reference"><link href="17" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>利用多尺度的LBP模式提高了表情的识别率。但以上改进方法未解决表情识别算法复杂度高、特征维数大、特征表达不足等问题。</p>
                </div>
                <div class="p1">
                    <p id="33">针对上述存在的问题,本文提出将单演信息和改进的中心对称局部三值模式信息通过一种融合编码的方式得到单演CS-LTP算法(简称M-CSLTP算法),其既引入了单演算法的高表征能力,同时改进的CS-LTP算法又具有低维性以及在复杂环境中鲁棒性,最后使用最近邻算法完成了人脸表情的识别。改进算法克服了特征维数大、特征表征不足等问题,并有效的提高了表情的识别率。</p>
                </div>
                <h3 id="34" name="34" class="anchor-tag"><b>2 应用算法背景</b></h3>
                <h4 class="anchor-tag" id="35" name="35"><b>2.1 单演信号</b></h4>
                <div class="p1">
                    <p id="36">单演信号分析是由Felsberg和Sommer<citation id="134" type="reference"><link href="19" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>提出,单演信号是基于希尔伯特变换的Riesz核构建而来。通过旋转不变的Riesz变换可将图像分解为三个相互正交的分量:单演振幅、单演方向、单演相位。</p>
                </div>
                <div class="p1">
                    <p id="37">若定义输入图像信号<i>z</i>=(<i>x</i>,<i>y</i>),可以得到<i>Riesz</i>核的二维空间表示如下</p>
                </div>
                <div class="p1">
                    <p id="38"><mathml id="120"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo stretchy="false">(</mo><mi>R</mi></mrow><msub><mrow></mrow><mi>X</mi></msub><mo>,</mo><mi>R</mi><msub><mrow></mrow><mi>y</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mrow><mo>(</mo><mrow><mfrac><mi>x</mi><mrow><mn>2</mn><mi>π</mi><mo stretchy="false">∥</mo><mi>z</mi><mo stretchy="false">∥</mo><msup><mrow></mrow><mn>3</mn></msup></mrow></mfrac><mo>,</mo><mfrac><mi>y</mi><mrow><mn>2</mn><mi>π</mi><mo stretchy="false">∥</mo><mi>z</mi><mo stretchy="false">∥</mo><msup><mrow></mrow><mn>3</mn></msup></mrow></mfrac></mrow><mo>)</mo></mrow></mrow></math></mathml>      (1)</p>
                </div>
                <div class="p1">
                    <p id="39">设输入图像灰度值为<i>h</i>(<i>z</i>),二维单演信号数学表达式为</p>
                </div>
                <div class="p1">
                    <p id="40"><i>H</i><sub><i>M</i></sub>(<i>z</i>)=(<i>h</i><sub><i>d</i></sub>(<i>z</i>),<i>h</i><sub><i>x</i></sub>(<i>z</i>),<i>h</i><sub><i>y</i></sub>(<i>z</i>))</p>
                </div>
                <div class="p1">
                    <p id="41">=(<i>h</i><sub><i>d</i></sub>(<i>z</i>),<i>R</i><sub><i>x</i></sub>*<i>h</i><sub><i>d</i></sub>(<i>z</i>),<i>R</i><sub><i>y</i></sub>*<i>h</i><sub><i>d</i></sub>(<i>z</i>))      (2)</p>
                </div>
                <div class="p1">
                    <p id="42">其中<i>h</i><sub><i>x</i></sub>(<i>z</i>)和<i>h</i><sub><i>y</i></sub>(<i>z</i>)分别表示在通过<i>Riesz</i>变换后<i>x</i>轴和<i>y</i>轴的输出,式(2)中的<i>h</i><sub><i>d</i></sub>(<i>z</i>)如下所示</p>
                </div>
                <div class="p1">
                    <p id="43"><i>h</i><sub><i>d</i></sub>(<i>z</i>)=<i>h</i>(<i>z</i>)*<i>F</i><sup>-1</sup>(<i>G</i><sub><i>ch</i></sub>(<i>ω</i>))      (3)</p>
                </div>
                <div class="p1">
                    <p id="44">其中<i>F</i><sup>-1</sup>表示傅里叶逆变换,<i>G</i><sub><i>ch</i></sub>(<i>ω</i>)表示频域的柯西滤波器,其中二维柯西滤波器的数学表达式如下</p>
                </div>
                <div class="p1">
                    <p id="45"><i>G</i><sub><i>ch</i></sub>(<i>ω</i>)=<i>ω</i><sup><i>α</i></sup><sup>/2</sup>exp(-<i>σω</i>)      (4)</p>
                </div>
                <div class="p1">
                    <p id="46">其中<i>σ</i>/<i>a</i>是中心频率,<i>a</i>是带宽参数。</p>
                </div>
                <div class="p1">
                    <p id="47">信号通过单演滤波后的会产生单演振幅<i>A</i>,单演方向<i>T</i>,单演相位<i>P</i>,计算方法如下</p>
                </div>
                <div class="area_img" id="48">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJZ201909061_04800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc1FTMHdYQkFCcm1KK2ZPTjlDdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="50">经过单演滤波器的特征模式图如图1所示。</p>
                </div>
                <div class="area_img" id="51">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJZ201909061_051.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc1FTMHdYQkFCcm1KK2ZPTjlDdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 单演滤波效果图" src="Detail/GetImg?filename=images/JSJZ201909061_051.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc1FTMHdYQkFCcm1KK2ZPTjlDdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图1 单演滤波效果图</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJZ201909061_051.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc1FTMHdYQkFCcm1KK2ZPTjlDdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="52" name="52"><b>2.2 中心对称三值模式(CS-LTP</b>)</h4>
                <div class="p1">
                    <p id="53">为了获得表情图像的更多的细节信息,采用<i>CS</i>-<i>LTP</i><citation id="135" type="reference"><link href="21" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>描述子,其基本思想是对中心像素与其领域内的像素差值进行三值编码,它是一个很强大的纹理描述子,引入了阈值变量<i>T</i>,使得图像的梯度方向信息有更加具体的表征,有效的提高特征描述子对噪声和光照的鲁棒性。<i>CS</i>-<i>LTP</i>编码过程如下图所示:</p>
                </div>
                <div class="area_img" id="54">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJZ201909061_054.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc1FTMHdYQkFCcm1KK2ZPTjlDdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2CS-LTP编码" src="Detail/GetImg?filename=images/JSJZ201909061_054.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc1FTMHdYQkFCcm1KK2ZPTjlDdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图2</b><i>CS</i>-<i>LTP</i><b>编码</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJZ201909061_054.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc1FTMHdYQkFCcm1KK2ZPTjlDdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="55"><i>CS</i>-<i>LTP</i>编码规则如下:</p>
                </div>
                <div class="p1">
                    <p id="56"><mathml id="121"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>C</mi><mi>S</mi><mo>-</mo><mi>L</mi><mi>Τ</mi><mi>Ρ</mi><msub><mrow></mrow><mrow><mi>R</mi><mo>,</mo><mi>Ν</mi></mrow></msub><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>Ν</mi><mo>/</mo><mn>2</mn><mo>-</mo><mn>1</mn></mrow></munderover><mi>s</mi></mstyle><mo stretchy="false">(</mo><mi>m</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mi>m</mi><msub><mrow></mrow><mrow><mi>i</mi><mo>+</mo><mi>Ν</mi><mo>/</mo><mn>2</mn></mrow></msub><mo stretchy="false">)</mo><mn>3</mn><msup><mrow></mrow><mi>i</mi></msup></mrow></math></mathml>      (6)</p>
                </div>
                <div class="area_img" id="57">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJZ201909061_05700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc1FTMHdYQkFCcm1KK2ZPTjlDdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="59">使用CS-LTP<citation id="136" type="reference"><link href="23" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>模式会造成特征维数成指数增长,为了避免特征维数过高,采用正负模式的CS-LTP,即把三值模式裂化为正负二值模式,这样做的目的不仅保留了原来加入阈值后的细节信息,也同时大大降低了特征维数,其裂化过程如图3所示:</p>
                </div>
                <div class="area_img" id="60">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJZ201909061_060.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc1FTMHdYQkFCcm1KK2ZPTjlDdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3CS-LTP的裂化过程" src="Detail/GetImg?filename=images/JSJZ201909061_060.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc1FTMHdYQkFCcm1KK2ZPTjlDdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图3</b><i>CS</i>-<i>LTP</i><b>的裂化过程</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJZ201909061_060.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc1FTMHdYQkFCcm1KK2ZPTjlDdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="61" name="61"><b>2.3 改进的M-CSLTP算法</b></h4>
                <div class="p1">
                    <p id="62">M-CSLTP特征提取的步骤如下所述。</p>
                </div>
                <div class="p1">
                    <p id="63">得到输入表情子区域图像,对每一个子区域进行单一尺度的单演滤波,得到单演幅值、单演相位、单演方向特征图。</p>
                </div>
                <div class="p1">
                    <p id="64">单演振幅编码:局部振幅是图像能量信息的表达。计算表情图像单演振幅A的CS-LTP特征,从而得到三值模式的特征编码,为了简化计算,将CS-LTP裂化为正负两种二值模式,得到正<i>MA</i><sup><i>U</i></sup>和负<i>MA</i><sup><i>L</i></sup>两类编码。</p>
                </div>
                <div class="p1">
                    <p id="65">单演方向编码:局部方向<i>V</i><mathml id="122"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>Τ</mi><mi>d</mi></msubsup></mrow></math></mathml>表征图像的几何信息,在中心像素点<i>Z</i><sub><i>c</i></sub>的局部方向计算采用象限位量化编码,公式如下</p>
                </div>
                <div class="area_img" id="66">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJZ201909061_06600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc1FTMHdYQkFCcm1KK2ZPTjlDdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="68">单演相位编码:局部相位表达了图像的结构信息,将单演相位信息<i>P</i>通过式(9)计算得到相位编码</p>
                </div>
                <div class="p1">
                    <p id="69"><i>MP</i>=⎣(4<i>P</i>)/<i>π</i>」      (9)</p>
                </div>
                <div class="p1">
                    <p id="70">⎣<i>x</i>」表示通过运算后取小于<i>x</i>的整数,得到的<i>MP</i>∈[0～3],二进制码值可取00,01,10,11。</p>
                </div>
                <div class="p1">
                    <p id="71">单演编码融合:单演幅值<i>A</i>的<i>CS</i>-<i>LTP</i><sub><i>U</i></sub>编码信息用<i>MA</i><sup><i>U</i></sup>表示,并作为<i>M</i>-<i>CSLTP</i><sub><i>U</i></sub>模式的低四位。局部相位的码值<i>MP</i>和局部方向的码值<i>MV</i><mathml id="123"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>Τ</mi><mi>d</mi></msubsup></mrow></math></mathml>作为<i>M</i>-<i>CSLTP</i><sub><i>U</i></sub>模式的高四位,最后串联三部分形成一个八位二进制编</p>
                </div>
                <div class="p1">
                    <p id="72"><i>MCSLTP</i><sub><i>U</i></sub>= (<i>MV</i><mathml id="124"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>Τ</mi><mi>x</mi></msubsup></mrow></math></mathml>,<i>MV</i><mathml id="125"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>Τ</mi><mi>y</mi></msubsup></mrow></math></mathml>,<i>MP</i>,<i>MA</i><sup><i>U</i></sup>)      (10)</p>
                </div>
                <div class="p1">
                    <p id="73">同理可得到<i>M</i>-<i>CSLTP</i><sub><i>L</i></sub>模式,式中<i>M</i>-<i>CSLTP</i><sub><i>U</i></sub>,<i>M</i>-<i>CSLTP</i><sub><i>L</i></sub>∈[0～255]。对得到的两层特征图<i>M</i>-<i>CSLTP</i><sub>(</sub><sub><i>U</i></sub><sub>,</sub><sub><i>L</i></sub><sub>)</sub>分别计算其分块直方图,再将得到的块直方图串连起来得到最终的M-CSLTP特征。</p>
                </div>
                <h4 class="anchor-tag" id="74" name="74"><b>2.4 M-CSLTP算法流程</b></h4>
                <div class="p1">
                    <p id="75">预处理:在表情识别系统中,表情特征主要集中在人眼,鼻子,嘴巴等五官附近<citation id="137" type="reference"><link href="25" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>将表情图像分为5个子区域,预处理的生气表情图像和子区域图像如图4所示。</p>
                </div>
                <div class="area_img" id="76">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJZ201909061_076.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc1FTMHdYQkFCcm1KK2ZPTjlDdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 表情图像的预处理" src="Detail/GetImg?filename=images/JSJZ201909061_076.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc1FTMHdYQkFCcm1KK2ZPTjlDdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图4 表情图像的预处理</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJZ201909061_076.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc1FTMHdYQkFCcm1KK2ZPTjlDdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="77">特征提取过程:将裁剪出的子区域提取<i>M</i>-<i>CSLTP</i>特征,并将各个子区域特征进行融合得到最终的类表情特征。</p>
                </div>
                <div class="p1">
                    <p id="78">分类过程:选择欧式距离的最近邻分类器进行分类。</p>
                </div>
                <div class="p1">
                    <p id="79">具体的实验流程图如图1。</p>
                </div>
                <div class="area_img" id="80">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJZ201909061_080.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc1FTMHdYQkFCcm1KK2ZPTjlDdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 实验流程图" src="Detail/GetImg?filename=images/JSJZ201909061_080.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc1FTMHdYQkFCcm1KK2ZPTjlDdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图5 实验流程图</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJZ201909061_080.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc1FTMHdYQkFCcm1KK2ZPTjlDdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h3 id="81" name="81" class="anchor-tag"><b>3 实验设计与结果分析</b></h3>
                <h4 class="anchor-tag" id="82" name="82"><b>3.1 实验参数设置</b></h4>
                <h4 class="anchor-tag" id="83" name="83">3.1.1 分块大小及分块数设置</h4>
                <div class="p1">
                    <p id="84">子区域的大小和分块数决定着特征的表达能力和维数,所以选择一个合适的子区域大小以及特征块个数<citation id="138" type="reference"><link href="27" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>,不仅能在特征维数上得到控制,还能保留较多的细节信息。将<i>JAFFE</i>库和<i>CK</i>+库的表情图像统一尺寸为168×180。通过设定坐标点的方式,将表情图像裁剪为左眼,右眼,眉心,鼻子,嘴这5个子区域,并将每一个子区域分块(2×2,2×2,1×1,2×2,3×3),子区域大小及分块数见表1。</p>
                </div>
                <div class="area_img" id="85">
                    <p class="img_tit"><b>表1 子区域大小及块数</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="85" border="1"><tr><td><br />表情子区域</td><td>左眼</td><td>右眼</td><td>眉心</td><td>鼻子</td><td>嘴</td></tr><tr><td><br />像素</td><td>50<i>x</i>36</td><td>50<i>x</i>36</td><td>25<i>x</i>18</td><td>36<i>x</i>50</td><td>75<i>x</i>54</td></tr><tr><td><br />块数量</td><td>4</td><td>4</td><td>1</td><td>4</td><td>9</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="86" name="86">3.1.2 改进的<i>M</i>-<i>CSLTP</i>参数设置</h4>
                <div class="p1">
                    <p id="87">在单演信号中的带通滤波器选择柯西滤波器。<i>Klein</i><sup></sup><citation id="139" type="reference"><link href="29" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>文中阐述了柯西滤波器的优势。相比<i>Gabor</i><citation id="140" type="reference"><link href="9" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>滤波器的带宽限制、<i>Log</i>-<i>Gabor</i><citation id="141" type="reference"><link href="11" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation> 缺失空间域解析表达式等不足,柯西带通滤波器具有空间频率协调一致、局部相位稳定等特性,在纹理的细节的表征要更稳定。柯西滤波器的参数参照文献<citation id="142" type="reference">[<a class="sup">14</a>]</citation>设置,<i>σ</i>取1.7,<i>a</i>取1.52。</p>
                </div>
                <div class="p1">
                    <p id="88">在采用中心对称三值模式对单演振幅提取分量特征过程中,中心对称三值模式可以很好地减少对中心像素的依赖。而在阈值的选择方面:传统阈值的设定是固定值或者局部图像的均值,并不具有很好的适应性,所以本文采用一种动态阈值方法,并对比不同动态阈值在本文算法中的匹配程度,下式是3个动态阈值变量。</p>
                </div>
                <div class="p1">
                    <p id="89"><i>T</i><sub>1</sub>=<i>p</i><sub><i>c</i></sub>*<i>δ</i>      (11)</p>
                </div>
                <div class="p1">
                    <p id="90"><mathml id="126"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Τ</mi><msub><mrow></mrow><mn>2</mn></msub><mo>=</mo><mfrac><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>Ν</mi><mo>-</mo><mn>1</mn></mrow></munderover><mrow><msqrt><mrow><mi>p</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msqrt></mrow></mstyle></mrow><mi>Ν</mi></mfrac></mrow></math></mathml>      (12)</p>
                </div>
                <div class="p1">
                    <p id="91"><mathml id="127"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Τ</mi><msub><mrow></mrow><mn>3</mn></msub><mo>=</mo><msqrt><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>Ν</mi><mo>-</mo><mn>1</mn></mrow></munderover><mi>p</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mo>/</mo><mi>Ν</mi></mrow></msqrt></mrow></math></mathml>      (13)</p>
                </div>
                <div class="p1">
                    <p id="92">其中<i>T</i><sub>1</sub>中<i>P</i><sub><i>c</i></sub>是中心像素的值,<i>δ</i>是不同的尺度因子(0.02,0.05,0.08,0.1,0.2),本文<i>δ</i>选取0.05。<i>T</i><sub>2</sub>和<i>T</i><sub>3</sub>中<i>P</i><sub><i>i</i></sub>是邻域中第<i>i</i>个像素的值,<i>N</i>是邻域中像素的个数(<i>N</i>=8)。</p>
                </div>
                <h4 class="anchor-tag" id="93" name="93"><b>3.2 实验结果分析</b></h4>
                <div class="p1">
                    <p id="94">为了验证本文算法的有效性,分别在<i>JAFFE</i>数据库和<i>CK</i>+数据库上进行实验。<i>JAFFE</i>数据库由10位日本女性的七种表情组成,总共有213张表情图像;<i>CK</i>+表情数据库是由卡耐基梅隆大学(<i>CMU</i>)建立的,这个数据库由男女共123个人,1400多张表情图像。</p>
                </div>
                <h4 class="anchor-tag" id="95" name="95">3.2.1 <i>JAFFE</i>库</h4>
                <div class="p1">
                    <p id="96">本文以2:1的比例随机选取训练和测试样本,<i>JAFFE</i>训练组样本数为140,测试组样本数为73。由于<i>JAFFE</i>库的样本太少,且样本间表情差异较大,为了保证实验的准确性,实验过程重复5次,并将5次的识别率进行平均得到最终的识别率。</p>
                </div>
                <div class="area_img" id="97">
                    <p class="img_tit"><b>表2</b><i>JAFFE</i><b>数据库上5次实验结果</b>(%) <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="97" border="1"><tr><td><br />表情</td><td>1</td><td>2</td><td>3</td><td>4</td><td>5</td><td>平均识别率</td></tr><tr><td><br />高兴</td><td>11/11</td><td>10/11</td><td>10/11</td><td>10/11</td><td>10/11</td><td>92.72</td></tr><tr><td><br />生气</td><td>9/10</td><td>8/10</td><td>10/10</td><td>10/10</td><td>9/10</td><td>92.00</td></tr><tr><td><br />厌恶</td><td>10/10</td><td>8/10</td><td>9/10</td><td>8/10</td><td>9/10</td><td>88.00</td></tr><tr><td><br />悲伤</td><td>8/10</td><td>9/10</td><td>8/10</td><td>9/10</td><td>9/10</td><td>86.00</td></tr><tr><td><br />恐惧</td><td>8/10</td><td>9/10</td><td>9/10</td><td>9/10</td><td>9/10</td><td>88.00</td></tr><tr><td><br />自然</td><td>11/11</td><td>10/11</td><td>10/11</td><td>10/11</td><td>11/11</td><td>94.54</td></tr><tr><td><br />惊讶</td><td>10/11</td><td>11/11</td><td>11/11</td><td>11/11</td><td>11/11</td><td>98.18</td></tr><tr><td><br /></td><td>90.41</td><td>89.04</td><td>91.78</td><td>91.78</td><td>93.15</td><td>91.23</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="98">表2看出在以亚洲女性为主的<i>JAFFE</i>数据库中在<i>M</i>-<i>CSLTP</i>算法下5次仿真的平均识别率为91.23%。惊讶的表情特征变化更加明显,使得其识别率明显高于其它表情。惊讶表情识别率为98.18%,对样本分析可以发现惊讶时眉毛会上挑,眼睛、嘴巴明显张大。而自然表情处于一种中性的形态,相对于其它表情面部器官的变化较小,自然表情的识别率达到94.54%。</p>
                </div>
                <div class="area_img" id="99">
                    <p class="img_tit"><b>表3</b><i>JAFFE</i><b>数据库第5次实验的混淆矩阵</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="99" border="1"><tr><td><br />表情</td><td>高兴</td><td>生气</td><td>厌恶</td><td>悲伤</td><td>恐惧</td><td>自然</td><td>惊讶</td></tr><tr><td><br />高兴</td><td>10/11</td><td>0/10</td><td>0/10</td><td>0/10</td><td>0/10</td><td>0/11</td><td>0/11</td></tr><tr><td><br />生气</td><td>0/11</td><td>9/10</td><td>0/10</td><td>1/10</td><td>0/10</td><td>0/11</td><td>0/11</td></tr><tr><td><br />厌恶</td><td>0/11</td><td>1/10</td><td>9/10</td><td>0/10</td><td>1/10</td><td>0/11</td><td>0/11</td></tr><tr><td><br />悲伤</td><td>0/11</td><td>0/10</td><td>1/10</td><td>9/10</td><td>0/10</td><td>0/11</td><td>0/11</td></tr><tr><td><br />恐惧</td><td>1/11</td><td>0/10</td><td>0/10</td><td>0/10</td><td>9/10</td><td>0/11</td><td>0/11</td></tr><tr><td><br />自然</td><td>0/11</td><td>0/10</td><td>0/10</td><td>0/10</td><td>0/10</td><td>11/11</td><td>0/11</td></tr><tr><td><br />惊讶</td><td>0/11</td><td>0/10</td><td>0/10</td><td>0/10</td><td>0/10</td><td>0/11</td><td>11/11</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="100">在第5次实验结果的混淆矩阵如表3所示,高兴、生气、厌恶、悲伤、恐惧这5类表情之间都存在相互错分的情况,因为在<i>JAFFE</i>库中日本女性含蓄的表情动作幅度较小,故这些表情难易区分,自然和惊讶表情并未与其它表情产生混淆。</p>
                </div>
                <h4 class="anchor-tag" id="101" name="101">3.2.2 <i>CK</i>+库</h4>
                <div class="p1">
                    <p id="102"><i>CK</i>+数据库中有完整7种表情图像的样本只有62个,本文选取表情图像完整的50个样本进行实验,训练和测试样本以2:1的比例设置,训练样本选峰值附近的两张,测试样本选峰值样本。训练样本数为780,测试样本数为390。实验重复3次,取3次的识别率得平均值作为最终的识别率。</p>
                </div>
                <div class="area_img" id="103">
                    <p class="img_tit"><b>表4</b><i>CK</i>+<b>数据库3次实验结果</b>(%) <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="103" border="1"><tr><td><br />表情</td><td>1</td><td>2</td><td>3</td><td>平均识别率</td></tr><tr><td><br />高兴</td><td>53/58</td><td>52/58</td><td>52/58</td><td>90.80</td></tr><tr><td><br />生气</td><td>52/56</td><td>50/56</td><td>52/56</td><td>91.67</td></tr><tr><td><br />厌恶</td><td>46/51</td><td>46/51</td><td>46/51</td><td>90.19</td></tr><tr><td><br />悲伤</td><td>56/61</td><td>55/61</td><td>56/61</td><td>91.25</td></tr><tr><td><br />恐惧</td><td>48/50</td><td>47/50</td><td>46/50</td><td>94.00</td></tr><tr><td><br />自然</td><td>48/53</td><td>48/53</td><td>48/53</td><td>90.56</td></tr><tr><td><br />惊讶</td><td>59/61</td><td>60/61</td><td>60/61</td><td>97.81</td></tr><tr><td><br /></td><td>92.82</td><td>91.79</td><td>92.30</td><td>92.30</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="104">表4是<i>CK</i>+数据库在<i>M</i>-<i>CSLTP</i>算法下3次仿真的平均识别率,并且达到了92.30%,通过对表中数据分析可以看到惊讶和恐惧的识别率相对较高,分别高达97.81%和94.00%。</p>
                </div>
                <div class="p1">
                    <p id="105">分析表5可知,7种表情之间都会存在相互错分的情况,相比于东方人含蓄的情感表达,<i>CK</i>+数据库中的西方人在情感的表达上更加的直接,表情动作比较明显,使得特征之间的混淆程度是相对均衡且较低,使得在<i>CK</i>+库上<i>M</i>-<i>CSLTP</i>算法有较好的分类效果。</p>
                </div>
                <div class="area_img" id="106">
                    <p class="img_tit"><b>表5</b><i>CK</i>+<b>库上第1次实验的混淆矩阵</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="106" border="1"><tr><td><br />表情</td><td>高兴</td><td>生气</td><td>厌恶</td><td>悲伤</td><td>恐惧</td><td>自然</td><td>惊讶</td></tr><tr><td><br />高兴</td><td>53/58</td><td>0/56</td><td>0/51</td><td>2/61</td><td>1/50</td><td>0/53</td><td>0/61</td></tr><tr><td><br />生气</td><td>0/58</td><td>52/56</td><td>1/51</td><td>0/61</td><td>0/50</td><td>2/53</td><td>0/61</td></tr><tr><td><br />厌恶</td><td>1/58</td><td>2/56</td><td>46/51</td><td>1/61</td><td>1/50</td><td>1/53</td><td>2/61</td></tr><tr><td><br />悲伤</td><td>2/58</td><td>1/56</td><td>1/51</td><td>56/61</td><td>0/50</td><td>1/53</td><td>0/61</td></tr><tr><td><br />恐惧</td><td>1/58</td><td>0/56</td><td>2/51</td><td>2/61</td><td>48/50</td><td>1/53</td><td>0/61</td></tr><tr><td><br />自然</td><td>0/58</td><td>1/56</td><td>0/51</td><td>0/61</td><td>0/50</td><td>48/53</td><td>0/61</td></tr><tr><td><br />惊讶</td><td>1/58</td><td>0/56</td><td>1/51</td><td>0/61</td><td>0/50</td><td>0/53</td><td>59/61</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="107" name="107"><b>3.3 不同算法对比结果</b></h4>
                <div class="p1">
                    <p id="108">在<i>CK</i>+库和<i>JAFFE</i>库上进行了不同算法的对比,其中<i>M</i>-<i>CSLTP</i>算法的阈值按3.1.2节设置了3个动态阈值。</p>
                </div>
                <div class="area_img" id="109">
                    <p class="img_tit"><b>表6 基于整脸各算法识别率的比较</b>(%) <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="109" border="1"><tr><td rowspan="2"><br />数据库</td><td rowspan="2"><i>MBP</i></td><td rowspan="2"><i>LBP</i></td><td rowspan="2"><i>CSLTP</i></td><td colspan="3"><br />本文算法</td></tr><tr><td><br /><i>T</i><sub>1</sub></td><td><i>T</i><sub>2</sub></td><td><i>T</i><sub>3</sub></td></tr><tr><td><br /><i>JAFFE</i></td><td>84.62</td><td>83.56</td><td>82.19</td><td>91.23</td><td>89.31</td><td>89.04</td></tr><tr><td><br /><i>CK</i>+</td><td>86.32</td><td>85.47</td><td>86.67</td><td>92.30</td><td>89.74</td><td>89.31</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="110">实验结果反映了三点:①<i>M</i>-<i>CSLTP</i>特征相比于<i>LBP</i>、<i>MBP</i>、<i>CSLTP</i>特征在分类效果上是具有明显优势的。②当阈值为<i>T</i><sub>1</sub>时<i>M</i>-<i>CSLTP</i>算法在<i>JAFFE</i>和<i>CK</i>+数据库上的识别率分别是91.23%和92.30%。基于中心像素的动态阈值<i>T</i><sub>1</sub>明显高于领域像素动态阈值<i>T</i><sub>2</sub>和<i>T</i><sub>3</sub>,因为中心对称三值模式通过比较领域像素间的差值,忽略了中心像素的影响,而加入了基于中心像素的动态阈值调节,能很好的弥补<i>CSLTP</i>的这一缺陷。③<i>CK</i>+库的总体识别率要高于<i>JAFFE</i>库的识别率,因为<i>CK</i>+库的实验样本差异较小,并且<i>CK</i>+库图像质量也是要高于<i>JAFFE</i>库。</p>
                </div>
                <div class="p1">
                    <p id="111">为了验证表情子区域对表情识别率的影响,按照3.1.1节对表情子区域参数设置,并对这些子区域提取<i>M</i>-<i>CSLTP</i>特征,实验结果如表7所示。</p>
                </div>
                <div class="area_img" id="112">
                    <p class="img_tit"><b>表7 基于子区域各算法识别率的比较</b>(%) <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="112" border="1"><tr><td rowspan="2"><br />数据库</td><td rowspan="2"><i>MBP</i></td><td rowspan="2"><i>LBP</i></td><td rowspan="2"><i>CSLTP</i></td><td colspan="3"><br />本文算法</td></tr><tr><td><br /><i>T</i><sub>1</sub></td><td><i>T</i><sub>2</sub></td><td><i>T</i><sub>3</sub></td></tr><tr><td><br /><i>JAFFE</i></td><td>85.75</td><td>86.30</td><td>83.01</td><td>93.15</td><td>92.87</td><td>92.32</td></tr><tr><td><br /><i>CK</i>+</td><td>89.74</td><td>88.89</td><td>87.35</td><td>94.45</td><td>91.02</td><td>90.59</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="113">并且对比表6和表7发现:相同算法在基于子区域的识别率高于整脸的识别率,所以对表情图像裁剪出子区域在一定程度上去除了其它区域冗余信息,并减少其对表情特征的干扰。而在子区域动态阈值取<i>T</i><sub>1</sub>时<i>M</i>-<i>CSLTP</i>算法在<i>JAFFE</i>数据库和<i>CK</i>+数据库的识别率分别为93.15%和94.45%,同时验证了无论是基于整脸表情还是子区域表情,阈值为<i>T</i><sub>1</sub>时的<i>M</i>-<i>CSLTP</i>算法在分类效果上是最优的。</p>
                </div>
                <div class="p1">
                    <p id="114">而在实际情况下表情图像无法回避噪声的干扰,为了验证<i>M</i>-<i>CSLTP</i>算法对噪声的鲁棒性,选择<i>JAFFE</i>数据库中训练组样本数为140,测试组样本数为73。特征算法选取<i>MBP</i>和阈值为<i>T</i>1的<i>M</i>-<i>CSLTP</i>进行对比,并且在表情图像加入均值为0和不同方差<i>V</i>的高斯噪声得识别率对比结果。</p>
                </div>
                <div class="area_img" id="115">
                    <p class="img_tit"><b>表8 不同噪声强度干扰实验</b>(%) <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="115" border="1"><tr><td><br />算法</td><td><i>V</i>=0.01</td><td><i>V</i>=0.04</td><td><i>V</i>=0.08</td></tr><tr><td><br /><i>MBP</i></td><td>83.56</td><td>82.19</td><td>72.60</td></tr><tr><td><br /><i>M</i>-<i>CSLTP</i></td><td>89.04</td><td>86.30</td><td>79.45</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="116">综合表8来看,在加入噪声后分类准确率都有不同程度的下降,<i>M</i>-<i>CSLTP</i>算法相比于传统的<i>MBP</i>对噪声具有更强的鲁棒性。</p>
                </div>
                <h3 id="117" name="117" class="anchor-tag"><b>4 总结</b></h3>
                <div class="p1">
                    <p id="118">本文提出的<i>M</i>-<i>CSLTP</i>算法,在基于强表征能力的单演信号下融合了改进的中心对称三值模式(<i>CS</i>-<i>LTP</i>),结合了各自的特点,相对于传统算法具有优势如下:1)将表情图像分为表情子区域能有效的剔除冗余的特征信息。2)选择<i>CS</i>-<i>LTP</i>算子避免了对中心像素过分的依赖,加入基于中心像素的动态阈值调节,也让算法在光照不均和噪声干扰较大的情况下更具优势。3)采用单演信号分析,提出的<i>M</i>-<i>CSLTP</i>编码结合了幅值、相位、方向信息的编码信息,能更好的挖掘表情图像的空间结构和纹理信息,表情特征信息更丰富,有效的提高了表情的识别率。</p>
                </div>
                <div class="p1">
                    <p id="119">实验结果表明:本文提出的算法不仅在识别率上高于传统的<i>LBP</i>、<i>MBP</i>方法,并且对噪声和光照也具备较强的鲁棒性。</p>
                </div>
                <div class="area_img" id="143">
                                <img alt="" src="Detail/GetImg?filename=images/JSJZ201909061_14300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc1FTMHdYQkFCcm1KK2ZPTjlDdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                            <p class="img_tit"></p>

                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=STJD&amp;filename=STJD14100800000681&amp;v=MDM5MDJNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcm5JSVY4Y2FSUT1Oam5CYXJLOEg5SE1wNDlGWk9zUENuUTRvQg==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc1FTMHdYQkFCcm1KK2ZPTjlDdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> <i>Y Gaffary</i>,<i>V Eyharabide</i>.<i>The Impact of Combining Kinesthetic and Facial Expression Displays on Emotion Recognition by Users</i>[<i>J</i>].<i>International Journal of Human</i>-<i>Computer Interaction</i>,2014,30(11):904-920.
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning Expressionlets via Universal Manifold Model for Dynamic Facial Expression Recognition">

                                <b>[2]</b> <i>M Liu</i>,<i>et al</i>.<i>Learning Expressionlets via Universal Manifold Model for Dynamic Facial Expression Recognition</i>[<i>J</i>].<i>IEEE Transactions on Image Processing</i>,2015,25(12):5920-5932.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Detecting emotional stress from facial expressions for driving safety">

                                <b>[3]</b> <i>H Gao</i>,<i>A Y</i>ü<i>ce</i>,<i>J P Thiran</i>.<i>Detecting emotional stress from facial expressions for driving safety</i>[<i>C</i>].<i>IEEE International Conference on Image Processing</i>.<i>IEEE</i>,2014:5961-5965.
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011600738060&amp;v=MDAxNTMrZ0hESG81b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcm5JSVY4Y2FSUT1OaWZPZmJLN0h0RE5xWTlGWQ==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc1FTMHdYQkFCcm1KK2ZPTjlDdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b> <i>W Gu</i>,<i>et al</i>.<i>Facial expression recognition using radial encoding of local Gabor features and classifier synthesis</i>[<i>J</i>].<i>Pattern Recognition</i>,2012,45(1):80-91.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Monogenic Binary Coding: An Efficient Local Feature Extraction Approach to Face Recognition">

                                <b>[5]</b> <i>M Yang</i>,<i>et al</i>.<i>Monogenic Binary Coding</i>:<i>An Efficient Local Feature Extraction Approach to Face Recognition</i>[<i>J</i>].<i>IEEE Transactions on Information Forensics</i> &amp; <i>Security</i>,2012,7(6):1738-1751.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Improved Spatiotemporal Local Monogenic Binary Pattern for Emotion Recognition in The Wild">

                                <b>[6]</b> <i>X Huang</i>,<i>et al</i>.<i>Improved Spatiotemporal Local Monogenic Binary Pattern for Emotion Recognition in The Wild</i>[<i>C</i>].<i>International Conference on Multimodal Interaction</i>.<i>ACM</i>,2014:514-520.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZYX201806010&amp;v=MDAyOTJGeW5rVzd6T0lUZlNkckc0SDluTXFZOUVaSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1Wm8=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc1FTMHdYQkFCcm1KK2ZPTjlDdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b> 胡敏,等.融合局部纹理和形状特征的人脸表情识别[<i>J</i>].电子与信息学报,2018.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSSG201601010&amp;v=MTIzODI4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1Wm9GeW5rVzd6T0x6N1lhYkc0SDlmTXJvOUVaSVFLREg=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc1FTMHdYQkFCcm1KK2ZPTjlDdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b> 叶棪,等.基于多尺度等价模式<i>LBP</i>的人脸表情识别[<i>J</i>].计算机与数字工程,2016,44(1):40-44.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The monogenic signal">

                                <b>[9]</b> <i>M Felsberg</i>,<i>G Sommer</i>.<i>The monogenic signal</i>[<i>J</i>].<i>Signal Processing IEEE Transactions on</i>,2009,49(12):3136-3144.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_10" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XXWX201409033&amp;v=MTc0MjFIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpvRnlua1c3ek9QVFhjZHJHNEg5WE1wbzlHWjRRS0Q=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc1FTMHdYQkFCcm1KK2ZPTjlDdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[10]</b> 李伟生,王立逗,周丽芳.一种基于<i>LTP</i>自适应阈值的人脸识别方法[<i>J</i>].小型微型计算机系统,2014,35(9):2099-2103.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Efficient image classification via sparse coding spatial pyramid matching representation of SIFT-WCS-LTP feature">

                                <b>[11]</b> <i>M Huang</i>,<i>Z Mu</i>,<i>H Zeng</i>.<i>Efficient image classification via sparse coding spatial pyramid matching representation of SIFT</i>-<i>WCS</i>-<i>LTP feature</i>[<i>J</i>].<i>Image Processing Iet</i>,2016,10(1):61-67.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_12" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSGG201704029&amp;v=MTg3MTZXN3pPTHo3TWFiRzRIOWJNcTQ5SGJZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVab0Z5bms=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc1FTMHdYQkFCcm1KK2ZPTjlDdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[12]</b> 周宇旋,等.判别性完全局部二值模式人脸表情识别[<i>J</i>].计算机工程与应用,2017,53(4):163-169.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_13" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES57B3E36284F990F75FEDF89B4735230F&amp;v=MTk3MThvSlBuZnIzaFl5ZXJlV1JycnBDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZCckxVMDV0NWh4Ym0zdzY0PU5pZk9mYmEvYk5LNXJJbEhiTzk1QlhVNXVSRVduRQ==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc1FTMHdYQkFCcm1KK2ZPTjlDdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[13]</b> <i>K Lekdioui</i>,<i>et al</i>.<i>Facial decomposition for expression recognition using texture</i>/<i>shape descriptors and SVM classifier</i>[<i>J</i>].<i>Signal Processing Image Communication</i>,2017,58.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Phase based distance regularized level set for the segmentation of ultrasound kidney images">

                                <b>[14]</b> <i>D Selvathi</i>,<i>S Bama</i>.<i>Phase based distance regularized level set for the segmentation of ultrasound kidney images</i>[<i>M</i>].<i>Elsevier Science Inc</i>.2017.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJZ201909061" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc1FTMHdYQkFCcm1KK2ZPTjlDdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJZ201909061&amp;v=MDk5NjF6N0JkTEc0SDlqTXBvOURaWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1Wm9GeW5rVzd6T0w=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc1FTMHdYQkFCcm1KK2ZPTjlDdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=842_VlRz_g1CMCLi-M0twVw-4EOFG_noWyi_OsMnrAg1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
