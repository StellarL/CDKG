<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=NOOK-mvTMnxl11s6oPYV0boPXN3iP4N7Rc-A56nk4KI1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2fKXReader%2fDetail%3fTIMESTAMP%3d637139247393725000%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJZ201910041%26RESULT%3d1%26SIGN%3dxPx4B3Mx%252fjf4FYbLBHOlsBFyrEY%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJZ201910041&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJZ201910041&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJZ201910041&amp;v=MDk1NThRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpwRnkvbVY3M0FMejdCZExHNEg5ak5yNDlCWlk=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVRyNFB2YmhVNFRrV0FxZi9KeHFTZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#53" data-title="&lt;b&gt;1 引言&lt;/b&gt; "><b>1 引言</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#63" data-title="&lt;b&gt;2 相关工作&lt;/b&gt; "><b>2 相关工作</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#64" data-title="&lt;b&gt;2.1 情感分类研究&lt;/b&gt;"><b>2.1 情感分类研究</b></a></li>
                                                <li><a href="#66" data-title="&lt;b&gt;2.2 基于深度学习的情感分类研究&lt;/b&gt;"><b>2.2 基于深度学习的情感分类研究</b></a></li>
                                                <li><a href="#68" data-title="&lt;b&gt;2.3 主题概率模型&lt;/b&gt;"><b>2.3 主题概率模型</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#70" data-title="&lt;b&gt;3 研究内容&lt;/b&gt; "><b>3 研究内容</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#71" data-title="&lt;b&gt;3.1 word2vec简介&lt;/b&gt;"><b>3.1 word2vec简介</b></a></li>
                                                <li><a href="#77" data-title="&lt;b&gt;3.2 LDA主题模型&lt;/b&gt;"><b>3.2 LDA主题模型</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#96" data-title="&lt;b&gt;4 研究方法&lt;/b&gt; "><b>4 研究方法</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#99" data-title="&lt;b&gt;4.1 文本表示层&lt;/b&gt;"><b>4.1 文本表示层</b></a></li>
                                                <li><a href="#115" data-title="&lt;b&gt;4.2 CNN网络层&lt;/b&gt;"><b>4.2 CNN网络层</b></a></li>
                                                <li><a href="#126" data-title="&lt;b&gt;4.3 情感计算层&lt;/b&gt;"><b>4.3 情感计算层</b></a></li>
                                                <li><a href="#130" data-title="&lt;b&gt;4.4 模型训练&lt;/b&gt;"><b>4.4 模型训练</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#134" data-title="&lt;b&gt;5 实验准备&lt;/b&gt; "><b>5 实验准备</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#135" data-title="&lt;b&gt;5.1 情感分析数据集&lt;/b&gt;"><b>5.1 情感分析数据集</b></a></li>
                                                <li><a href="#140" data-title="&lt;b&gt;5.2 评价指标&lt;/b&gt;"><b>5.2 评价指标</b></a></li>
                                                <li><a href="#144" data-title="&lt;b&gt;5.2 数据预处理&lt;/b&gt;"><b>5.2 数据预处理</b></a></li>
                                                <li><a href="#152" data-title="&lt;b&gt;5.3 实验参数设置&lt;/b&gt;"><b>5.3 实验参数设置</b></a></li>
                                                <li><a href="#155" data-title="&lt;b&gt;5.4 实验对比模型&lt;/b&gt;"><b>5.4 实验对比模型</b></a></li>
                                                <li><a href="#163" data-title="&lt;b&gt;5.5 实验结果及分析&lt;/b&gt;"><b>5.5 实验结果及分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#180" data-title="&lt;b&gt;6 结论&lt;/b&gt; "><b>6 结论</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#75" data-title="&lt;b&gt;图1 Skip-gram模型结构&lt;/b&gt;"><b>图1 Skip-gram模型结构</b></a></li>
                                                <li><a href="#79" data-title="&lt;b&gt;图2 LDA模型框架图&lt;/b&gt;"><b>图2 LDA模型框架图</b></a></li>
                                                <li><a href="#81" data-title="&lt;b&gt;表1 LDA模型中参数符号的含义&lt;/b&gt;"><b>表1 LDA模型中参数符号的含义</b></a></li>
                                                <li><a href="#98" data-title="&lt;b&gt;图3 TWE-UMCNN模型框架图&lt;/b&gt;"><b>图3 TWE-UMCNN模型框架图</b></a></li>
                                                <li><a href="#106" data-title="&lt;b&gt;表2 维吾尔语词性一级标记集&lt;/b&gt;"><b>表2 维吾尔语词性一级标记集</b></a></li>
                                                <li><a href="#137" data-title="&lt;b&gt;表3 维吾尔二分类数据集&lt;/b&gt;"><b>表3 维吾尔二分类数据集</b></a></li>
                                                <li><a href="#139" data-title="&lt;b&gt;表4 维吾尔五分类数据集&lt;/b&gt;"><b>表4 维吾尔五分类数据集</b></a></li>
                                                <li><a href="#154" data-title="&lt;b&gt;表5 神经网络参数设置&lt;/b&gt;"><b>表5 神经网络参数设置</b></a></li>
                                                <li><a href="#168" data-title="&lt;b&gt;表6 基准模型对比实验结果&lt;/b&gt;"><b>表6 基准模型对比实验结果</b></a></li>
                                                <li><a href="#172" data-title="&lt;b&gt;表7 扩展词性特征和主题特征的实验结果&lt;/b&gt;"><b>表7 扩展词性特征和主题特征的实验结果</b></a></li>
                                                <li><a href="#175" data-title="&lt;b&gt;图4 词向量维度的影响&lt;/b&gt;"><b>图4 词向量维度的影响</b></a></li>
                                                <li><a href="#177" data-title="&lt;b&gt;图5 dropout参数实验结果&lt;/b&gt;"><b>图5 dropout参数实验结果</b></a></li>
                                                <li><a href="#179" data-title="&lt;b&gt;图6 优化函数实验结果&lt;/b&gt;"><b>图6 优化函数实验结果</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" title=" B Shin,L Timothy,C Jinho.Lexicon Integrated CNN Models with Attention for Sentiment Analysis[J].arXiv preprint arXiv:1610.06272,2016." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Lexicon Integrated CNN Models with Attention for Sentiment Analysis">
                                        <b>[1]</b>
                                         B Shin,L Timothy,C Jinho.Lexicon Integrated CNN Models with Attention for Sentiment Analysis[J].arXiv preprint arXiv:1610.06272,2016.
                                    </a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" title=" 赵妍妍,秦兵,刘挺.文本情感分析[J].软件学报,2010,21(8):1834-1848." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=RJXB201008009&amp;v=MzE1MzJPM3pxcUJ0R0ZyQ1VSN3FmWnVacEZ5L21WNzNBTnlmVGJMRzRIOUhNcDQ5RmJZUUtESDg0dlI0VDZqNTQ=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVRyNFB2YmhVNFRrV0FxZi9KeHFTZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         赵妍妍,秦兵,刘挺.文本情感分析[J].软件学报,2010,21(8):1834-1848.
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title=" Y Q Wang,M Huang,L Zhao,et al.Attention-based LSTM for aspect-level sentiment classification[C].Proc of Conference on Empirical Methods in Natural Language Processing,2016:606-615." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Attention-based LSTM for Aspect-level Sentiment Classification">
                                        <b>[3]</b>
                                         Y Q Wang,M Huang,L Zhao,et al.Attention-based LSTM for aspect-level sentiment classification[C].Proc of Conference on Empirical Methods in Natural Language Processing,2016:606-615.
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" title=" Y S Zhang,Y R Jiang,Y X Tong.Study of sentiment classification for Chinese microblog based on recurrent neural network[J].Chinese Journal of Electronics,2016,25(4):601-607." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=EDZX201604003&amp;v=MjA1MDJDVVI3cWZadVpwRnkvbVY3M0FJQ25SZHJHNEg5Zk1xNDlGWjRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnI=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVRyNFB2YmhVNFRrV0FxZi9KeHFTZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                         Y S Zhang,Y R Jiang,Y X Tong.Study of sentiment classification for Chinese microblog based on recurrent neural network[J].Chinese Journal of Electronics,2016,25(4):601-607.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" title=" J Wang,L C Yu,K R Lai,et al.Dimensional sentiment analysis using a regional CNN-LSTM model [C].Proceedings of the Annual Meeting of the Association for Computational Linguistics.Berlin,Germany,2016:225-230." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Dimensional sentiment analysis using a regional CNN-LSTM model">
                                        <b>[5]</b>
                                         J Wang,L C Yu,K R Lai,et al.Dimensional sentiment analysis using a regional CNN-LSTM model [C].Proceedings of the Annual Meeting of the Association for Computational Linguistics.Berlin,Germany,2016:225-230.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" title=" M Kuta,M Morawiec,J Kitowski.Sentiment Analysis with Tree-Structured Gated Recurrent Units[C].Text,Speech,and Dialogue:20th International Conference,TSD 2017,Prague,Czech Republic,August 27-31,2017." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Sentiment Analysis with Tree-Structured Gated Recurrent Units">
                                        <b>[6]</b>
                                         M Kuta,M Morawiec,J Kitowski.Sentiment Analysis with Tree-Structured Gated Recurrent Units[C].Text,Speech,and Dialogue:20th International Conference,TSD 2017,Prague,Czech Republic,August 27-31,2017.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" title=" D Bespalov,B Bai,A Shokoufandeh,et al.Sentiment Classification Based on Supervised Latent n-gram Analysis[C].Acm International Conference on Information &amp;amp; Knowledge Management.ACM,2011." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Sentiment classification based on supervised latent n-gram analysis">
                                        <b>[7]</b>
                                         D Bespalov,B Bai,A Shokoufandeh,et al.Sentiment Classification Based on Supervised Latent n-gram Analysis[C].Acm International Conference on Information &amp;amp; Knowledge Management.ACM,2011.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" title=" B Das,S Chakraborty.An Improved Text Sentiment Classification Model Using TF-IDF and Next Word Negation[J].arXiv preprint arXiv:1806.06407v1,2018." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=An Improved Text Sentiment Classification Model Using TF-IDF and Next Word Negation">
                                        <b>[8]</b>
                                         B Das,S Chakraborty.An Improved Text Sentiment Classification Model Using TF-IDF and Next Word Negation[J].arXiv preprint arXiv:1806.06407v1,2018.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" title=" V Narayanan,I Arora,A Bhatia.Fast and accurate sentiment classification using an enhanced Naive Bayes model[J].arXiv preprint arXiv:1305.6143 201,3." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fast and accurate sentiment classification using an enhanced Naive Bayes model">
                                        <b>[9]</b>
                                         V Narayanan,I Arora,A Bhatia.Fast and accurate sentiment classification using an enhanced Naive Bayes model[J].arXiv preprint arXiv:1305.6143 201,3.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_10" title=" A Joulin,E Grave,P Bojanowski,et al.Bag of Tricks for Efficient Text Classification[J].arXiv preprint arXiv:1610.06272,2016." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Bag of Tricks for Efficient Text Classification">
                                        <b>[10]</b>
                                         A Joulin,E Grave,P Bojanowski,et al.Bag of Tricks for Efficient Text Classification[J].arXiv preprint arXiv:1610.06272,2016.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_11" title=" 刘金硕,李哲,叶馨,等.文本情感倾向性分析方法:bfsmPMI-SVM[J].武汉大学学报:理学版.2017,63(3):259-264." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=WHDY201703011&amp;v=MTAxODNVUjdxZlp1WnBGeS9tVjczQU1pWFBkN0c0SDliTXJJOUVaWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckM=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVRyNFB2YmhVNFRrV0FxZi9KeHFTZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[11]</b>
                                         刘金硕,李哲,叶馨,等.文本情感倾向性分析方法:bfsmPMI-SVM[J].武汉大学学报:理学版.2017,63(3):259-264.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_12" >
                                        <b>[12]</b>
                                     B Pang,L Lee.Opinion mining and sentiment analysis[J].Foundations and Trend s in Information Retrieval.2008,2(1/2):1-135.</a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_13" title=" T Mikolov,I Sutskever,K Chen,et al.Distributed representations of words and phrases and their compositionality[J].Advances in Neural Information Processing Systems,2013,26:3111-3119." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Distributed Representations of Words and Phrases and their Compositionality">
                                        <b>[13]</b>
                                         T Mikolov,I Sutskever,K Chen,et al.Distributed representations of words and phrases and their compositionality[J].Advances in Neural Information Processing Systems,2013,26:3111-3119.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_14" title=" A Alessa,M Faezipour,Z Alhassan.Text Classification of Flu-Related Tweets Using FastText with Sentiment and Keyword Features[C].2018 IEEE International Conference on Healthcare Informatics (ICHI),2018:366-367." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Text Classification of Flu-Related Tweets Using FastText with Sentiment and Keyword Features">
                                        <b>[14]</b>
                                         A Alessa,M Faezipour,Z Alhassan.Text Classification of Flu-Related Tweets Using FastText with Sentiment and Keyword Features[C].2018 IEEE International Conference on Healthcare Informatics (ICHI),2018:366-367.
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_15" title=" Y Sharma,G Agrawal,P Jain,et al.Vector representation of words for sentiment analysis using GloVe[C].International Conference on Intelligent Communication &amp;amp; Computational Techniques.IEEE Computer Society,2017." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Vector representation of words for sentiment analysis using GloVe">
                                        <b>[15]</b>
                                         Y Sharma,G Agrawal,P Jain,et al.Vector representation of words for sentiment analysis using GloVe[C].International Conference on Intelligent Communication &amp;amp; Computational Techniques.IEEE Computer Society,2017.
                                    </a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_16" title=" Y Lecun,Y Bengio,G Hinton.Deep learning[J].Nature.2015,521(7553):436-444." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep learning">
                                        <b>[16]</b>
                                         Y Lecun,Y Bengio,G Hinton.Deep learning[J].Nature.2015,521(7553):436-444.
                                    </a>
                                </li>
                                <li id="35">


                                    <a id="bibliography_17" title=" Yoon Kim.Convolutional neural networks for sentence classification[C].In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing.2014:1746-1751." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Convolutional Neural Networks for Sentence Classification">
                                        <b>[17]</b>
                                         Yoon Kim.Convolutional neural networks for sentence classification[C].In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing.2014:1746-1751.
                                    </a>
                                </li>
                                <li id="37">


                                    <a id="bibliography_18" title=" N Kalchbrenner,E Grefenstette,P Blunsom.A convolutional neural network for modelling sentences[C].In:Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,2014:1749-1751" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Convolutional Neural Network for Modelling Sentences">
                                        <b>[18]</b>
                                         N Kalchbrenner,E Grefenstette,P Blunsom.A convolutional neural network for modelling sentences[C].In:Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,2014:1749-1751
                                    </a>
                                </li>
                                <li id="39">


                                    <a id="bibliography_19" title=" S Hochreiter,J Schmidhuber.Long Short-Term Memory[J].Neural Computation,1997,9(8):1735-1780." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJBK&amp;filename=SJBK15090500014198&amp;v=MjA1NjY0UFFIL2lyUmRHZXJxUVRNbndaZVp0RmlubFVyaklKMTBRYUJvPU5pZkpaYks5SHRqTXFvOUZaT29MRFhVeG9CTVQ2VA==&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVRyNFB2YmhVNFRrV0FxZi9KeHFTZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[19]</b>
                                         S Hochreiter,J Schmidhuber.Long Short-Term Memory[J].Neural Computation,1997,9(8):1735-1780.
                                    </a>
                                </li>
                                <li id="41">


                                    <a id="bibliography_20" title=" D M BLEI,A Y NG.Probabilistic Topic Models[J].Communications of the ACM.2011,55(4):55-65." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000004805&amp;v=Mjg0MDlpbmxVcmpJSjEwUWFCbz1OaWZJWTdLN0h0ak5yNDlGWk9zTEJIdzhvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp0Rg==&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVRyNFB2YmhVNFRrV0FxZi9KeHFTZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[20]</b>
                                         D M BLEI,A Y NG.Probabilistic Topic Models[J].Communications of the ACM.2011,55(4):55-65.
                                    </a>
                                </li>
                                <li id="43">


                                    <a id="bibliography_21" title=" Lin Chenghua,He Yulan.Joint sentiment/topic model for sentiment analysis[J].International Conference on Information and Knowledge Management,Proceedings,2009." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Joint Sentiment/Topic Model for Sentiment Analysis">
                                        <b>[21]</b>
                                         Lin Chenghua,He Yulan.Joint sentiment/topic model for sentiment analysis[J].International Conference on Information and Knowledge Management,Proceedings,2009.
                                    </a>
                                </li>
                                <li id="45">


                                    <a id="bibliography_22" title=" CAO Ziqiang,LI Sujian.LIU Yang,et al.A Novel Neural Topic Model and Its Supervised Extension[C].Proceedings of the 29th AAAI Conference on Artificial Inteligence.Austin,USA:AAAI.2015:2210-2216." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A novel neural topic model and its supervised extension">
                                        <b>[22]</b>
                                         CAO Ziqiang,LI Sujian.LIU Yang,et al.A Novel Neural Topic Model and Its Supervised Extension[C].Proceedings of the 29th AAAI Conference on Artificial Inteligence.Austin,USA:AAAI.2015:2210-2216.
                                    </a>
                                </li>
                                <li id="47">


                                    <a id="bibliography_23" title=" LIU Yang,LIU Zhiyuan,CHUA T S,et al.Topical Word Embeddings [C].Proceedings of the 29th AAAI Conference on Artificial Inteligence.Austin,USA:AAAI.2015:2418-2424." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Topical word embeddings">
                                        <b>[23]</b>
                                         LIU Yang,LIU Zhiyuan,CHUA T S,et al.Topical Word Embeddings [C].Proceedings of the 29th AAAI Conference on Artificial Inteligence.Austin,USA:AAAI.2015:2418-2424.
                                    </a>
                                </li>
                                <li id="49">


                                    <a id="bibliography_24" title=" XIE Pengtao,YANG Diyi,XING E.Incorporating Word Correlation Knowledge into Topic Modeling[C].Proceedings of 2015 Conference of the North American Chapter of the Association for Computational Linguistics:Human Language Technologies[S.l.]:Association for Computational Linguistics.2015:725-734." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Incorporating word correlation knowledge into topic modeling">
                                        <b>[24]</b>
                                         XIE Pengtao,YANG Diyi,XING E.Incorporating Word Correlation Knowledge into Topic Modeling[C].Proceedings of 2015 Conference of the North American Chapter of the Association for Computational Linguistics:Human Language Technologies[S.l.]:Association for Computational Linguistics.2015:725-734.
                                    </a>
                                </li>
                                <li id="51">


                                    <a id="bibliography_25" title=" D Q Nguyen,R Billingsley,L Du,et a1.Improving topic models with latent feature word representations[J].Transactions of the Association for Computational Linguistics.2015-3:299-313." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Improving Topic Models with Latent Feature Word Representations">
                                        <b>[25]</b>
                                         D Q Nguyen,R Billingsley,L Du,et a1.Improving topic models with latent feature word representations[J].Transactions of the Association for Computational Linguistics.2015-3:299-313.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJZ" target="_blank">计算机仿真</a>
                2019,36(10),194-201+205             </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于LDA与深度神经网络的维吾尔文情感分类</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E4%B9%B0%E4%B9%B0%E6%8F%90%E9%98%BF%E4%BE%9D%E7%94%AB&amp;code=24782945&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVRyNFB2YmhVNFRrV0FxZi9KeHFTZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">买买提阿依甫</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%90%BE%E5%AE%88%E5%B0%94%C2%B7%E6%96%AF%E6%8B%89%E6%9C%A8&amp;code=17705001&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVRyNFB2YmhVNFRrV0FxZi9KeHFTZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">吾守尔·斯拉木</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%B8%95%E4%B8%BD%E6%97%A6%C2%B7%E6%9C%A8%E5%90%88%E5%A1%94%E5%B0%94&amp;code=27456016&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVRyNFB2YmhVNFRrV0FxZi9KeHFTZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">帕丽旦·木合塔尔</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%A8%E6%96%87%E5%BF%A0&amp;code=09256021&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVRyNFB2YmhVNFRrV0FxZi9KeHFTZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">杨文忠</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E6%96%B0%E7%96%86%E5%A4%A7%E5%AD%A6%E4%BF%A1%E6%81%AF%E7%A7%91%E5%AD%A6%E4%B8%8E%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0181515&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVRyNFB2YmhVNFRrV0FxZi9KeHFTZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">新疆大学信息科学与工程学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>针对维吾尔论坛文本具有稀疏性、主题不明确性、不规范性等问题,并考虑到普通神经网络只将词粒度层面的词向量作为输入,忽略语义粒度层面的全局语义特征的表示,而导致文档特征表示不充分的现象。提出一种基于主题联合词向量的多通道卷积神经网络的情感分类方法。方法结合word2vec和LDA模型生成主题特征矩阵,获取语义粒度层面特征信息,以丰富卷积网络的池化层特征,从而提高了情感分类的准确率。在维吾尔文情感二分类和五分类数据集上的实验结果表明,提出的模型相比于传统机器学习方法取得了更好的情感分类性能。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVRyNFB2YmhVNFRrV0FxZi9KeHFTZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">情感分类;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVRyNFB2YmhVNFRrV0FxZi9KeHFTZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">卷积神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%B8%BB%E9%A2%98%E6%A6%82%E7%8E%87%E6%A8%A1%E5%9E%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVRyNFB2YmhVNFRrV0FxZi9KeHFTZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">主题概率模型;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%AF%8D%E5%90%91%E9%87%8F&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVRyNFB2YmhVNFRrV0FxZi9KeHFTZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">词向量;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%BB%B4%E5%90%BE%E5%B0%94%E8%AF%AD&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVRyNFB2YmhVNFRrV0FxZi9KeHFTZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">维吾尔语;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    买买提阿依甫(1981-),男(柯族),新疆人,博士生,主要研究方向为网络舆情分析关键技术。;
                                </span>
                                <span>
                                    吾守尔·斯拉木(1942-),男(维吾尔族),新疆人,教授,中国工程院院士,博导,主要研究方向为自然语言处理。;
                                </span>
                                <span>
                                    *帕丽旦·木合塔尔(1986-),女(柯族),新疆人,博士生,主要研究领域为语音合成。;
                                </span>
                                <span>
                                    杨文忠(1971-),男(汉族),新疆人,副教授,硕士生导师,研究方向为舆情分析。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-01-19</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家“973”重点基础研究计划基金资助项目(2014CB340506);</span>
                                <span>国家自然科学基金资助项目(61363063);</span>
                                <span>新疆大学多语种重点实验室开放课题(XJDX0905-2013-01);</span>
                    </p>
            </div>
                    <h1><b>Uighur Sentiment Classification Based on LDA and Deep Neural Network</b></h1>
                    <h2>
                    <span>Maimaitayifu</span>
                    <span>SILAMU Wushouer</span>
                    <span>MUHETAER Palidan</span>
                    <span>YANG Wen-zhong</span>
            </h2>
                    <h2>
                    <span>College of Information Science and Engineering,Xinjiang University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Aiming at the problems of sparsity, non-standard, subject ambiguous and the problem that common neural network takes only the word vector at the lexical level as input, and neglects the representation of the global semantic features at the semantic level, which leads to the problem that the document features are not sufficient, this paper presents a sentimental classification method of multi-channel convolution neural network based on topic joint word vector. This method combines the word2 vec and LDA topic model to generate the topic feature matrix, and obtains the global semantic feature information to enrich the max-pooling layer features of the convolutional network, thus improving the accuracy of sentiment classification. The experimental results on the Uyghur emotional two-category and five-category datasets show that the proposed model can achieve better sentimental classification performance than the traditional machine learning method.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Sentiment%20classification&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVRyNFB2YmhVNFRrV0FxZi9KeHFTZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Sentiment classification;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Convolutional%20neural%20network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVRyNFB2YmhVNFRrV0FxZi9KeHFTZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Convolutional neural network;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Topic%20probability%20model&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVRyNFB2YmhVNFRrV0FxZi9KeHFTZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Topic probability model;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Word%20embedding&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVRyNFB2YmhVNFRrV0FxZi9KeHFTZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Word embedding;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Uyghur&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVRyNFB2YmhVNFRrV0FxZi9KeHFTZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Uyghur;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2019-01-19</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="53" name="53" class="anchor-tag"><b>1 引言</b></h3>
                <div class="p1">
                    <p id="54">情感分析(sentiment analysis),也称为观点挖掘,是分析人们对于商品、事件、服务对象所怀有的情感、态度、倾向和意见等主观感受的研究领域<citation id="187" type="reference"><link href="3" rel="bibliography" /><link href="5" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>]</sup></citation>。任何一个包含情感的事务,如文本、音频、视频和图像都是情感分析的研究对象。新闻、论坛、微博等社会媒体,除了包含文本外,还包含大量的带情感信息的表情符号,同样有助于分析情感倾向。</p>
                </div>
                <div class="p1">
                    <p id="55">情感分析技术作为自然语言处理技术的热门研究课题,广泛应用于网络舆情分析等领域。文本情感分类是通过分析网民发表的主观性文本内容,挖掘其包含的深层情感倾向,从而判断其倾向的极性(网民发布的信息一般情况下带有正面、负面情感和中立)。其中负面信息可能会对社会造成负面影响。无法靠人工手段收集和处理这些大量涌现出来的海量信息,因此,如何通过计算机技术从海量维吾尔文本中捕获用户的情感倾向信息可以为相关部门在民意调查、信息监测等工作中提供参考,也能够帮助政府和国家安全部门及时发现网民的舆论倾向和动态,使其在信息监控和侦察工作中起到准确定位所需信息的作用。本文研究成果对于新疆社会稳定和长治久安有着重要的理论意义和应用价值。</p>
                </div>
                <div class="p1">
                    <p id="56">基于深度神经网络的情感分析方法主要是采用词向量对文本中的词语进行表示,进而构建句子、篇章的语义表示。在这些语义表示的基础上,采用深度神经网络模型对文本中蕴含的情感信息进行学习,从而实现对文本情感的分析,目前常用于情感分析的神经网络模型包括:循环神经网络(recurrent neural network, RNN<citation id="189" type="reference"><link href="7" rel="bibliography" /><link href="9" rel="bibliography" /><link href="11" rel="bibliography" /><sup>[<a class="sup">3</a>,<a class="sup">4</a>,<a class="sup">5</a>]</sup></citation>)、卷积神经网络(convolutional neural network, CNN<citation id="190" type="reference"><link href="7" rel="bibliography" /><link href="9" rel="bibliography" /><link href="11" rel="bibliography" /><sup>[<a class="sup">3</a>,<a class="sup">4</a>,<a class="sup">5</a>]</sup></citation>)、长短记忆网络(long short-term memory, LSTM<citation id="191" type="reference"><link href="7" rel="bibliography" /><link href="9" rel="bibliography" /><link href="11" rel="bibliography" /><sup>[<a class="sup">3</a>,<a class="sup">4</a>,<a class="sup">5</a>]</sup></citation>)、门控循环单元(gated recurrent unit, GRU<citation id="188" type="reference"><link href="13" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>)等。</p>
                </div>
                <div class="p1">
                    <p id="57">本文提出的基于LDA主题概率模型和深度学习结合的维吾尔文情感分类模型主要贡献如下:</p>
                </div>
                <div class="p1">
                    <p id="58">1)使用Word2vec词向量生成工具对大量无标记维吾尔文本语料进行训练,生成了维度为100、200、300、400的词向量,并通过实验验证了词向量对情感分类的作用。</p>
                </div>
                <div class="p1">
                    <p id="59">2)使用词向量表示技术,为常用表情符号构建了情感空间的特征表示矩阵,从而增强了文本情感表示,提高了情感分类结果。</p>
                </div>
                <div class="p1">
                    <p id="60">3)使用LDA主题模型对论坛评论文本生成主题,并将其与词向量结合,从“词”粒度和“文本”粒度层面同时对文本进行建模,解决了评论文本特征稀疏和主题聚焦性差的问题。</p>
                </div>
                <div class="p1">
                    <p id="61">4)提出了一种基于主题联合词向量与多通道卷积神经网络的维吾尔文深度学习情感分类方法(以下简称为TWE-UMCNN)。</p>
                </div>
                <div class="p1">
                    <p id="62">5)对比了本文模型和基准模型在情感二分类和五分类数据集上的分类效果,验证了本文模型的有效性。</p>
                </div>
                <h3 id="63" name="63" class="anchor-tag"><b>2 相关工作</b></h3>
                <h4 class="anchor-tag" id="64" name="64"><b>2.1 情感分类研究</b></h4>
                <div class="p1">
                    <p id="65">目前主流的情感分析方法主要分为三种:1)基于词典的方法;2)基于机器学习的方法;3)基于深度学习的方法。基于词典的方法主要是构建包括情感词、情感短语的情感词典或者建立倾向性语义模式库,其核心思想是通过情感词典和情感规则匹配实现文本语义的理解,从而达到情感信息的识别。词典的构建和设计一个合适的判断规则一般都需要耗费很多人力和先验知识。基于传统机器学习技术的情感分类任务的研究工作包括两个方面:1)特征工程是核心,而特征工程中常见的特征有n-gram<citation id="192" type="reference"><link href="15" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>、词性特征、句法特征、音节特征和TF-IDF特征<citation id="193" type="reference"><link href="17" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>等。然而,机器学习方法仍然依赖人工设计的特征,容易受到人为因素的影响,在某领域达到最好效果的特征集不一定在另一个领域表现优秀。2)基于传统机器学习方法通常利用朴素贝叶斯(NB)<citation id="194" type="reference"><link href="19" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>、最大熵(ME)<citation id="195" type="reference"><link href="21" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>、支持向量机(SVM)<citation id="196" type="reference"><link href="23" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>等机器学习算法进行情感分类。多数分类模型的性能依赖于标注数据集的质量。Pang等人<citation id="197" type="reference"><link href="25" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>首次使用机器学习方法解决了文档层次的二元情感分类问题。基于词典和基于机器学习的方法要求人员具有较高的领域知识和研究经验,且方法的泛化能力较差。近年来,随着深度神经网络技术的快速发展,深度学习逐渐成为机器学习领域的热门研究方向。深度神经网络模仿人脑的分层组织结构,相比于浅层计算模型具有高度的表达能力,理论上能够捕捉从词语本身到高层次语义的复杂映射函数。目前深度神经网络针对资源丰富的英语或汉语等语言在机器翻译、语音识别、问答系统和文本摘要、关系提取和情感分析等各种NLP任务中取得了良好的性能。针对情感分类任务的深度学习方法有2个步骤:1)使用word2vec<citation id="198" type="reference"><link href="27" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>、fasttext<citation id="199" type="reference"><link href="29" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>或者Glove<citation id="200" type="reference"><link href="31" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>等词向量训练工具,从海量无标记数据语料中学习出语义词向量(word embedding)<citation id="201" type="reference"><link href="27" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>;2)通过不同的语义合成方法用词向量得到句子的特征表示。</p>
                </div>
                <h4 class="anchor-tag" id="66" name="66"><b>2.2 基于深度学习的情感分类研究</b></h4>
                <div class="p1">
                    <p id="67">由于深度神经网络相对于传统的机器学习方法拥有优秀的自学习特征提取的能力,深度学习模型在自然语言处理的各领域得到了广泛应用。CNN最初由LeCun等人<citation id="202" type="reference"><link href="33" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>提出用于计算机视觉,在计算机视觉任务中得到了很好的效果。前人已经证明CNN 在潜在特征展示方面表现出色。例如词性标注、句子分类等。2014年Kim等人<citation id="203" type="reference"><link href="35" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>使用CNN解决了情感分类问题,并通过实验证明了CNN的分类性能优于递归神经网络。Kalchbrenner等人<citation id="204" type="reference"><link href="33" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>提出了一种采用k-max池化和多层卷积神经网络相结合的新颖模型。另一个流行的深度学习模型是序列模型,即循环神经网络(RNN)<citation id="205" type="reference"><link href="39" rel="bibliography" /><sup>[<a class="sup">19</a>]</sup></citation>,它通过隐藏状态能够保留文本的历史信息,因此,它更好地捕获文本中词语之间的语义关系。RNN的变体模型已经成功应用于机器翻译、文本生成等任务。LSTM可以捕获到文本中的长依赖关系,能够从整体上理解评论信息的情感语义。目前尚未出现基于CNN网络的维吾尔文情感分类的相关研究。</p>
                </div>
                <h4 class="anchor-tag" id="68" name="68"><b>2.3 主题概率模型</b></h4>
                <div class="p1">
                    <p id="69">近年来,主题模型在许多NLP(natural language processing)任务中也得到了广泛应用,成为了文本挖掘领域的热点研究,其中LDA(Latent Dirichlet Allocation)<citation id="206" type="reference"><link href="41" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>模型是一个比较流行的主题概率模型,能够捕获文本语料中的潜在主题信息。文献<citation id="207" type="reference">[<a class="sup">21</a>]</citation>对传统的LDA模型进行改进,提出了一种能够提取情感-主题信息的无监督的情感-主题联合JST (Joint Sentiment/ Topic) 模型。文献<citation id="208" type="reference">[<a class="sup">22</a>]</citation>对LDA模型进行改造,构建了一种基于监督LDA的文本分类方法。文献<citation id="209" type="reference">[<a class="sup">23</a>]</citation>利用LDA模型能够捕获资源中的潜在主题信息的特点,在动态自动标注任务中得到较好的效果。文献<citation id="210" type="reference">[<a class="sup">24</a>]</citation>对LDA模型特征提取和情感极性强度的改进,并应用于社交媒体用户推荐,取得了较好的应用效果,这些研究利用主题与情感之间的联系,在情感分析方面取得了良好的进展。论坛评论信息所隐含的情感信息通常情况下都与其所表达的主题密不可分,因此本文针对论坛评论表达的特点,提出一种基于LDA主题模型联合word2vec的深度学习的维吾尔文情感分类方法,并通过实验验证本文方法的有效性。</p>
                </div>
                <h3 id="70" name="70" class="anchor-tag"><b>3 研究内容</b></h3>
                <h4 class="anchor-tag" id="71" name="71"><b>3.1 word2vec简介</b></h4>
                <div class="p1">
                    <p id="72">深度神经网络研究崛起的背景下,谷歌公开了开源的词向量训练工具word2vec. 其包含CBOW和Skip-gram两种模型,并使用了k-means聚类算法解决了一词多义的问题。其中Skip-gram模型的目标是预测给定词的上下文单词,该模型主要由输入层、隐藏层和输出层三层的神经网络模型构成。在给定无标记文本语料集的训练过程中,每个词向量都需要最大化其上下文单词的对数概率,对于给定的单词序列<i>W</i>={<i>w</i><sub>1</sub>,<i>w</i><sub>2</sub>,…,<i>w</i><sub><i>N</i></sub>},Skip-gram模型的目标函数是最大化平均对数概率,计算公式如下</p>
                </div>
                <div class="p1">
                    <p id="73" class="code-formula">
                        <mathml id="73"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><mo>=</mo><mfrac><mn>1</mn><mi>Ν</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mrow><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mo stretchy="false">|</mo><mi>c</mi><mo stretchy="false">|</mo><mo>≤</mo><mi>k</mi><mo>,</mo><mi>c</mi><mo>≠</mo><mn>0</mn></mrow></munder><mtext>l</mtext></mstyle></mrow></mstyle><mtext>o</mtext><mtext>g</mtext><mi>p</mi><mo stretchy="false">(</mo><mi>w</mi><msub><mrow></mrow><mrow><mi>i</mi><mo>+</mo><mi>c</mi></mrow></msub><mo stretchy="false">|</mo><mi>w</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="74">式中,<i>c</i>表示窗口大小为<i>k</i>的单词的上下文,即当前词<i>w</i><sub><i>i</i></sub>的前<i>k</i>个单词和后<i>k</i>个单词,<i>N</i>表示语料库中单词的总数,<i>p</i>(<i>w</i><sub><i>i</i></sub><sub>+</sub><sub><i>c</i></sub>|<i>w</i><sub><i>i</i></sub>)是词向量<i>V</i><sub><i>wi</i></sub><sub>+</sub><sub><i>c</i></sub>和<i>V</i><sub><i>wi</i></sub>的softmax回归值。其神经网络结构如图1所示。</p>
                </div>
                <div class="area_img" id="75">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJZ201910041_075.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVRyNFB2YmhVNFRrV0FxZi9KeHFTZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 Skip-gram模型结构" src="Detail/GetImg?filename=images/JSJZ201910041_075.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVRyNFB2YmhVNFRrV0FxZi9KeHFTZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图1 Skip-gram模型结构</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJZ201910041_075.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVRyNFB2YmhVNFRrV0FxZi9KeHFTZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="76">由于softmax函数采用随机梯度下降算法(简称为SGD算法)进行训练时,其计算复杂度随着语料库中词汇量的大幅增大而变高。因此为了减轻计算复杂度,一般情况下,Skip-gram模型采用层次softmax或负采样两种近似训练法,为了减轻计算开销本文采用层次softmax进行训练词向量。</p>
                </div>
                <h4 class="anchor-tag" id="77" name="77"><b>3.2 LDA主题模型</b></h4>
                <div class="p1">
                    <p id="78"><b>LDA</b>模型是典型的非监督的主题概率模型<citation id="211" type="reference"><link href="35" rel="bibliography" /><link href="37" rel="bibliography" /><sup>[<a class="sup">17</a>,<a class="sup">18</a>]</sup><sup><b>17-18</b></sup><sup>]</sup></citation>,每篇文章都包含多个主题,每个主题表示为不同单词的概率分布,是一种“文档-主题-词”的三层贝叶斯概率生成模型,又是一个话题模型。利用<b>LDA</b>模型可以获取文本内容的主题分布信息,文档-主题之间服从先验狄利克雷分布,而主题-词之间服从多项式分布。<b>LDA</b>模型主题生成过程的概率模型如图<b>2</b>所示。</p>
                </div>
                <div class="area_img" id="79">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJZ201910041_079.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVRyNFB2YmhVNFRrV0FxZi9KeHFTZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 LDA模型框架图" src="Detail/GetImg?filename=images/JSJZ201910041_079.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVRyNFB2YmhVNFRrV0FxZi9KeHFTZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图2 LDA模型框架图</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJZ201910041_079.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVRyNFB2YmhVNFRrV0FxZi9KeHFTZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="80">表<b>1</b>中给出了模型中各符号的含义。</p>
                </div>
                <div class="area_img" id="81">
                    <p class="img_tit"><b>表1 LDA模型中参数符号的含义</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="81" border="1"><tr><td><br />符号</td><td>含义</td></tr><tr><td><br /><i>α</i></td><td><i>θ</i>的先验分布</td></tr><tr><td><br /><i>β</i></td><td><i>φ</i>的先验分布</td></tr><tr><td><br /><i>θ</i></td><td>文档-主题概率分布</td></tr><tr><td><br /><i>φ</i></td><td>主题-词概率分布</td></tr><tr><td><br /><i>N</i><sub><i>m</i></sub></td><td>第<i>m</i>个文档的单词数</td></tr><tr><td><br /><i>M</i></td><td>文档数量</td></tr><tr><td><br /><i>K</i></td><td>主题数</td></tr><tr><td><br /><i>z</i><sub><i>m</i>,<i>n</i></sub></td><td>第<i>m</i>个文档中第<i>n</i>个词的主题</td></tr><tr><td><br /><i>w</i><sub><i>m</i>,<i>n</i></sub></td><td>第<i>m</i>个文档中的第<i>n</i>个词</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="82">LDA模型通过概率生成模式,将文档表示为主题的混合分布<i>p</i>(<i>z</i>),其联合概率公式为如下</p>
                </div>
                <div class="p1">
                    <p id="83" class="code-formula">
                        <mathml id="83"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>p</mi><mo stretchy="false">(</mo><mi>θ</mi><mo>,</mo><mi>z</mi><mo>,</mo><mi>w</mi><mo stretchy="false">|</mo><mi>α</mi><mo>,</mo><mi>β</mi><mo stretchy="false">)</mo><mo>=</mo><mi>p</mi><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">|</mo><mi>α</mi><mo stretchy="false">)</mo><mstyle displaystyle="true"><munderover><mo>∏</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mi>p</mi></mstyle><mo stretchy="false">(</mo><mi>z</mi><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false">|</mo><mi>θ</mi><mo stretchy="false">)</mo><mi>p</mi><mo stretchy="false">(</mo><mi>w</mi><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false">|</mo><mi>z</mi><msub><mrow></mrow><mi>n</mi></msub><mo>,</mo><mi>β</mi><mo stretchy="false">)</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="84">LDA主题概率模型的建模过程是一个通过文档集合建立生成模型的反向过程。假如给定一个文档集<i>D</i>,其包含<i>M</i>个文档,主题数为<i>K</i>,即<i>D</i>={<i>d</i><sub>1</sub>,<i>d</i><sub>2</sub>,…,<i>d</i><sub><i>m</i></sub>}, <i>d</i><sub><i>m</i></sub>={<i>w</i><sub><i>m</i></sub><sub>1</sub>, <i>w</i><sub><i>m</i></sub><sub>2</sub>,…, <i>w</i><sub><i>mn</i></sub>}表示第<i>m</i>篇文档, <i>z</i><sub><i>m</i></sub>={<i>z</i><sub><i>m</i></sub><sub>1</sub>, <i>z</i><sub><i>m</i></sub><sub>2</sub>,…, <i>z</i><sub><i>mn</i></sub>}表示文档<i>d</i><sub><i>m</i></sub>中每个单词对应所属主题的集合。</p>
                </div>
                <div class="p1">
                    <p id="85">LDA主题模型生成文档时生成的过程如下:</p>
                </div>
                <div class="p1">
                    <p id="86">1)根据从狄利克雷(Dirichlet)分布先验知识<i>α</i>中取样生成文档<i>d</i><sub><i>m</i></sub>的主题概率分布<i>θ</i><sub><i>m</i></sub>,即<i>θ</i><sub><i>m</i></sub>～<i>Dir</i>(<i>α</i>).</p>
                </div>
                <div class="p1">
                    <p id="87">2)对于文档<i>d</i><sub><i>m</i></sub>中单词<i>w</i><sub><i>m</i></sub><sub>,</sub><sub><i>n</i></sub>,根据<i>z</i>服从<i>θ</i><sub><i>m</i></sub>的多项式<i>z</i><sub><i>m</i></sub><sub>,</sub><sub><i>n</i></sub>～<i>Multinomial</i>(<i>θ</i><sub><i>m</i></sub>),为单词<i>w</i><sub><i>m</i></sub><sub>,</sub><sub><i>n</i></sub>确定一个<i>z</i><sub><i>m</i></sub><sub>,</sub><sub><i>n</i></sub><sub>。</sub></p>
                </div>
                <div class="p1">
                    <p id="88">3)根据<i>φ</i>服从参数<i>β</i>的狄利克雷分布<i>φ</i><sub><i>m</i></sub>,同时根据步骤2)确定<i>z</i><sub><i>m</i></sub><sub>,</sub><sub><i>n</i></sub>,为<i>w</i><sub><i>m</i></sub><sub>,</sub><sub><i>n</i></sub>确定一个词分布。</p>
                </div>
                <div class="p1">
                    <p id="89">4)根据单词<i>w</i><sub><i>m</i></sub><sub>,</sub><sub><i>n</i></sub>服从<i>φ</i><sub><i>zm</i></sub><sub>,</sub><sub><i>n</i></sub>的多项分布式<i>w</i><sub><i>m</i></sub><sub>,</sub><sub><i>n</i></sub>～<i>Multinomial</i>(<i>θ</i><sub><i>m</i></sub>)生成<i>w</i><sub><i>m</i></sub><sub>,</sub><sub><i>n</i></sub></p>
                </div>
                <div class="p1">
                    <p id="90">5)遍历文档中全部单词,重复步骤2)-步骤4),生成<i>d</i><sub><i>m</i></sub>.</p>
                </div>
                <div class="p1">
                    <p id="91">6)遍历全部文档<i>D</i>中的每篇文档<i>d</i><sub><i>m</i></sub>,重复步骤1)-步骤5),生成整个文档集<i>D</i>。</p>
                </div>
                <div class="p1">
                    <p id="92">参数为<i>α</i>的狄利克雷分布如下</p>
                </div>
                <div class="p1">
                    <p id="93" class="code-formula">
                        <mathml id="93"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>D</mi><mi>i</mi><mi>r</mi><mo stretchy="false">(</mo><mi>α</mi><mo stretchy="false">)</mo><mo>=</mo><mi>Ρ</mi><mi>r</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">|</mo><mi>α</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><mi>Γ</mi><mo stretchy="false">(</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mi>α</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∏</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mi>Γ</mi></mstyle><mo stretchy="false">(</mo><mi>α</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></mfrac><mstyle displaystyle="true"><munderover><mo>∏</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mi>x</mi></mstyle><msubsup><mrow></mrow><mi>i</mi><mrow><mi>a</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mn>1</mn></mrow></msubsup><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>3</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="94">式中,<i>Γ</i>()是Gamma分布,<i>x</i><sub><i>i</i></sub>是语料库中第<i>i</i>个单词的概率。狄利克雷分布是多项</p>
                </div>
                <div class="p1">
                    <p id="95">分布的共轭先验分布。利用<i>LDA</i>模型训练文本语料后输出文件包括文本-主题矩阵<i>θ</i>、主题-词分布矩阵<i>φ</i>和主题词文件。</p>
                </div>
                <h3 id="96" name="96" class="anchor-tag"><b>4 研究方法</b></h3>
                <div class="p1">
                    <p id="97">本文提出的基于主题联合词向量与多通道卷积神经网络模型维吾尔文情感分类方法,将word2vec词向量和lda主题向量结合起来,从“词”粒度和“文本”粒度,也就是局部和全局两个层次的向量,再加上词性向量表示文本的特征矩阵卷积神经网络,作为卷积神经网络的输入进行训练神经网络、实现高精度的情感文本分类。word2vec根据滑动窗口中的词共现信息,将特征提取细化到词的粒度,对每个词的独立词向量进行建模,语义向量简单地采用词向量重叠组合,从而忽略了词与词之间的相互关联的整体语义关系,弱化了单词间的差异性。LDA模型通过主题概率模型构造文档的主题分布,主题分布反映了文本的整体语义信息。通过结合主题语义特征和词向量特征,可以更加丰富卷积神经网络的池化层特征,从而有效地提高维吾尔文情感分类的准确率。本文提出的基于主题联合词向量与多通道卷积神经网络的维吾尔文情感分类,其模型框架如图3所示。</p>
                </div>
                <div class="area_img" id="98">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJZ201910041_098.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVRyNFB2YmhVNFRrV0FxZi9KeHFTZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 TWE-UMCNN模型框架图" src="Detail/GetImg?filename=images/JSJZ201910041_098.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVRyNFB2YmhVNFRrV0FxZi9KeHFTZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图3 TWE-UMCNN模型框架图</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJZ201910041_098.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVRyNFB2YmhVNFRrV0FxZi9KeHFTZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="99" name="99"><b>4.1 文本表示层</b></h4>
                <h4 class="anchor-tag" id="100" name="100"><b>4.1.1</b> 词向量</h4>
                <div class="p1">
                    <p id="101">对于给定长度为<i><b>n</b></i>的句子<i><b>S</b></i><b>={</b><i><b>w</b></i><sub><b>1</b></sub>,<i><b>w</b></i><sub><b>2</b></sub>,…,<i><b>w</b></i><sub><i><b>n</b></i></sub>},其中<i><b>w</b></i><sub><i><b>i</b></i></sub>为句子<i><b>S</b></i>中第<i><b>i</b></i>个单词,情感分类的任务是根据<i><b>S</b></i>中单词序列的特征信息,通过多通道卷积神经网络对其进行训练,捕获更多隐含的情感信息,从而得到较好的极性分类的结果。为了训练词向量,首先,为了精准下载网页中标题、内容信息,对<b>Nutch</b>开源工具进行了二次开发,降低了无用的噪音数据。使用该爬虫工具从天山网(<b>uy.ts.cn</b>)等网站下载大料维吾尔文数据,本文实验中利用<b>python</b>库中<b>Gensim</b>工具的<b>skip-gram</b>模型,对大规模无标记维吾尔文本语料进行训练,构建了维吾尔文词向量模型。对于给定的语料库,将生成的单词向量存储在一个词向量查询矩阵<b>M=R</b><mathml id="182"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mtext>w</mtext><mrow><mo stretchy="false">|</mo><mtext>V</mtext><mo stretchy="false">|</mo><mo>×</mo><mtext>d</mtext><msub><mrow></mrow><mtext>w</mtext></msub></mrow></msubsup></mrow></math></mathml>,其中|<b>V</b>|是给定无标记维吾尔文本语料库的词汇量,<b>d</b><sub><b>w</b></sub>是单词向量的维数,本次实验中,用<b>Skip-gram</b>模型预先生成了维度为<b>100、200、300</b>和<b>400</b>的实数词向量,通过实验验证维度为<b>300</b>的词向量对情感分类的效果较好,未登录词使用均匀分布<b>U(-0.1,0.1</b>)来随机初始化。最终生成的词向量矩阵表示为如下</p>
                </div>
                <div class="p1">
                    <p id="102" class="code-formula">
                        <mathml id="102"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Μ</mi><mo>=</mo><mrow><mo>[</mo><mrow><mtable><mtr><mtd><mi>w</mi><msub><mrow></mrow><mrow><mn>1</mn><mn>1</mn></mrow></msub></mtd><mtd><mi>w</mi><msub><mrow></mrow><mrow><mn>1</mn><mn>2</mn></mrow></msub></mtd><mtd><mi>w</mi><msub><mrow></mrow><mrow><mn>1</mn><mn>3</mn></mrow></msub></mtd><mtd><mo>⋯</mo></mtd><mtd><mi>w</mi><msub><mrow></mrow><mrow><mn>1</mn><mi>Ν</mi></mrow></msub></mtd></mtr><mtr><mtd><mi>w</mi><msub><mrow></mrow><mrow><mn>2</mn><mn>1</mn></mrow></msub></mtd><mtd><mi>w</mi><msub><mrow></mrow><mrow><mn>2</mn><mn>2</mn></mrow></msub></mtd><mtd><mi>w</mi><msub><mrow></mrow><mrow><mn>2</mn><mn>3</mn></mrow></msub></mtd><mtd><mo>⋯</mo></mtd><mtd><mi>w</mi><msub><mrow></mrow><mrow><mn>2</mn><mi>Ν</mi></mrow></msub></mtd></mtr><mtr><mtd><mo>⋮</mo></mtd><mtd><mo>⋮</mo></mtd><mtd><mo>⋮</mo></mtd><mtd><mo>⋮</mo></mtd><mtd><mo>⋮</mo></mtd></mtr><mtr><mtd><mi>w</mi><msub><mrow></mrow><mrow><mi>k</mi><mn>1</mn></mrow></msub></mtd><mtd><mi>w</mi><msub><mrow></mrow><mrow><mi>k</mi><mn>2</mn></mrow></msub></mtd><mtd><mi>w</mi><msub><mrow></mrow><mrow><mi>k</mi><mn>3</mn></mrow></msub></mtd><mtd><mo>⋯</mo></mtd><mtd><mi>w</mi><msub><mrow></mrow><mrow><mi>k</mi><mi>Ν</mi></mrow></msub></mtd></mtr></mtable></mrow><mo>]</mo></mrow><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>4</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <h4 class="anchor-tag" id="103" name="103"><b>4.1.2</b> 词性向量</h4>
                <div class="p1">
                    <p id="104">词性特征包含了词语丰富的语义信息,将词性特征转换成词性向量后,可以将其作为神经网络模型的输入,能够进一步发现语句中词语的结构联系以及单词情感信息。例如“<image href="images/JSJZ201910041_183.jpg" type="" display="inline" placement="inline"><alt></alt></image>”(很)或者“<image href="images/JSJZ201910041_184.jpg" type="" display="inline" placement="inline"><alt></alt></image>”(少点)等形容词可以用来表示情感强度,神经网络模型通过词性特征可以发现类似的情感词语,因此在本文中加入了词性特征,试图进一步文本中潜在的情感信息,从而情感分类准确率。</p>
                </div>
                <div class="p1">
                    <p id="105">目前,维吾尔文没有一个统一规范的词性标注集。新疆多语种信息技术实验室等各研究单位都建立了自己的词性标注集。新疆多语种信息技术实验室手工建立了包含<b>120</b>万个单词的词性标注集,其中包括一级词性标注集(<b>15</b>个标签)(如表<b>1</b>所示)、二级词性标注集(<b>71</b>个标签)、三级词性标注集(<b>51</b>个标签)。</p>
                </div>
                <div class="area_img" id="106">
                    <p class="img_tit"><b>表2 维吾尔语词性一级标记集</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="106" border="1"><tr><td><br />序号</td><td>名称</td><td>标记</td><td>序号</td><td>名称</td><td>标记</td></tr><tr><td><br />1</td><td>名词</td><td>N</td><td>10</td><td>语气词</td><td>T</td></tr><tr><td><br />2</td><td>动词</td><td>V</td><td>11</td><td>标点符号</td><td>Y</td></tr><tr><td><br />3</td><td>形容词</td><td>A</td><td>12</td><td>附加成分</td><td>X</td></tr><tr><td><br />4</td><td>副词</td><td>D</td><td>13</td><td>后置词</td><td>R</td></tr><tr><td><br />5</td><td>代词</td><td>P</td><td>14</td><td>拉丁文</td><td>L</td></tr><tr><td><br />6</td><td>数词</td><td>M</td><td>15</td><td>量词</td><td>Q</td></tr><tr><td><br />7</td><td>连词</td><td>C</td><td>16</td><td>正面情感词</td><td>POS</td></tr><tr><td><br />8</td><td>模拟词</td><td>I</td><td>17</td><td>负面情感词</td><td>NEG</td></tr><tr><td><br />9</td><td>感叹词</td><td>E</td><td></td><td></td><td></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="107">为了更好地学习句子中情感词的情感特征信息,本文在一级词性标注集的基础上增加了两个词性,即正面情感词重新标注为“POS”标签、负面情感词重新标注为“NEG”标签,使情感词的词性更加突出。实验使用17位的one-hot向量表示词性向量(以下<i>v</i><sub><i>pos</i></sub>表示词性特征向量,<i>R</i><sub><i>pos</i></sub>表示词性向量矩阵),然后与词向量和主题向量拼接生成混合向量作为CNN网络的输入进行训练,提高了情感分类的准确率。</p>
                </div>
                <h4 class="anchor-tag" id="108" name="108">4.1.3 主题向量表示</h4>
                <div class="p1">
                    <p id="109">LDA模型能从“文档”粒度角度预测文档中的单词。首先,获取文档的主题分布和主题。然后,通过主题词的分布生成文档中的单词。Word2vec从“词”粒度角度预测文档中的单词。它根据给定单词的上下文预测单词。本文将LDA模型与Word2vec词向量结合,从全局“文档”粒度角度和局部“词”粒度角度考虑文本表示,试图获取文本中的更潜在情感信息。具体来说,对于文档集<i>D</i>={<i>d</i><sub>1</sub>,<i>d</i><sub>2</sub>,<i>d</i><sub>3</sub>,…,<i>d</i><sub><i>n</i></sub>},包含的单词集<i>V</i>={<i>w</i><sub>1</sub>,<i>w</i><sub>2</sub>,<i>w</i><sub>3</sub>,…,<i>w</i><sub><i>N</i></sub>},使用<i>LDA</i>模型对文档集<i>D</i>训练生成<i>K</i>个主题<i>Z</i>={<i>t</i><sub>1</sub>,<i>t</i><sub>2</sub>,<i>t</i><sub>3</sub>,…,<i>t</i><sub><i>K</i></sub>}和每个主题包含的单词的概率。利用<i>word</i>2<i>vec</i>模型对文档集<i>D</i>进行训练后对每个单词生成对应的稠密实数向量<i>v</i>(<i>w</i><sub><i>i</i></sub>)={<i>v</i>(<i>w</i><sub>1</sub>),<i>v</i>(<i>w</i><sub>2</sub>),…,<i>v</i>(<i>w</i><sub><i>N</i></sub>)},在<i>LDA</i>模型生成的主题词矩阵<i>φ</i>中,<i>φ</i><sub><i>i</i></sub><sub>,</sub><sub><i>j</i></sub>是主题<i>t</i><sub><i>i</i></sub>产生第<i>j</i>个单词的概率。为了计算每个主题的主题向量,选取该主题的主题词分布中的概率最大的<i>h</i>个单词。利用式(5)对主题中的单词进行归一化处理,然后通过式(6)对<i>h</i>个单词的词向量乘以每个单词的归一化权重来生成主题向量<i>v</i>(<i>t</i><sub><i>i</i></sub>)和权重。</p>
                </div>
                <div class="p1">
                    <p id="110" class="code-formula">
                        <mathml id="110"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>ω</mi><msub><mrow></mrow><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mo>=</mo><mfrac><mrow><mi>φ</mi><msub><mrow></mrow><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub></mrow><mrow><mstyle displaystyle="true"><msubsup><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn><mspace width="0.25em" /></mrow><mi>h</mi></msubsup><mi>φ</mi></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mo>,</mo><mi>k</mi></mrow></msub></mrow></mfrac><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>5</mn><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mi>v</mi><mo stretchy="false">(</mo><mi>t</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mstyle displaystyle="true"><msubsup><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn><mspace width="0.25em" /></mrow><mi>h</mi></msubsup><mi>ω</mi></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mi>v</mi><mo stretchy="false">(</mo><mi>w</mi><msub><mrow></mrow><mrow><mi>i</mi><msub><mrow></mrow><mi>j</mi></msub></mrow></msub><mo stretchy="false">)</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>6</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="111">此外,LDA中主题的概率分布仍然保持在此方法生成的空间中,这些主题更接近特定文档的内容。与此同时,由于结合了word2vec单词级别词向量,有助于更深层的情感信息。</p>
                </div>
                <div class="p1">
                    <p id="112">LDA模型训练后生成的输出文本的主题分布矩阵</p>
                </div>
                <div class="p1">
                    <p id="113" class="code-formula">
                        <mathml id="113"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ζ</mi><mo>=</mo><mrow><mo>[</mo><mrow><mtable><mtr><mtd><mi>z</mi><msub><mrow></mrow><mrow><mn>1</mn><mn>1</mn></mrow></msub></mtd><mtd><mi>z</mi><msub><mrow></mrow><mrow><mn>1</mn><mn>2</mn></mrow></msub></mtd><mtd><mi>z</mi><msub><mrow></mrow><mrow><mn>1</mn><mn>3</mn></mrow></msub></mtd><mtd><mo>⋯</mo></mtd><mtd><mi>z</mi><msub><mrow></mrow><mrow><mn>1</mn><mi>Ν</mi></mrow></msub></mtd></mtr><mtr><mtd><mi>z</mi><msub><mrow></mrow><mrow><mn>2</mn><mn>1</mn></mrow></msub></mtd><mtd><mi>z</mi><msub><mrow></mrow><mrow><mn>2</mn><mn>2</mn></mrow></msub></mtd><mtd><mi>z</mi><msub><mrow></mrow><mrow><mn>2</mn><mn>3</mn></mrow></msub></mtd><mtd><mo>⋯</mo></mtd><mtd><mi>z</mi><msub><mrow></mrow><mrow><mn>2</mn><mi>Ν</mi></mrow></msub></mtd></mtr><mtr><mtd><mo>⋮</mo></mtd><mtd><mo>⋮</mo></mtd><mtd><mo>⋮</mo></mtd><mtd><mo>⋮</mo></mtd><mtd><mo>⋮</mo></mtd></mtr><mtr><mtd><mi>z</mi><msub><mrow></mrow><mrow><mi>Κ</mi><mn>1</mn></mrow></msub></mtd><mtd><mi>z</mi><msub><mrow></mrow><mrow><mi>Κ</mi><mn>2</mn></mrow></msub></mtd><mtd><mi>z</mi><msub><mrow></mrow><mrow><mi>Κ</mi><mn>3</mn></mrow></msub></mtd><mtd><mo>⋯</mo></mtd><mtd><mi>z</mi><msub><mrow></mrow><mrow><mi>Κ</mi><mi>Ν</mi></mrow></msub></mtd></mtr></mtable></mrow><mo>]</mo></mrow><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>7</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="114">对于一个文档的主题向量矩阵为主题向量矩阵为<i>Z</i>∈<i>R</i><sup>|</sup><sup><i>K</i></sup><sup>|×</sup><sup><i>d</i></sup><sub><sup><i>t</i></sup></sub>,其中主题向量的维度<i>d</i><sub><i>t</i></sub>与词向量维度<i>d</i><sub><i>w</i></sub>相同,即<i>d</i><sub><i>t</i></sub>=<i>d</i><sub><i>w</i></sub>=300,<i>K</i>为文档中包含的主题数量。</p>
                </div>
                <h4 class="anchor-tag" id="115" name="115"><b>4.2 CNN网络层</b></h4>
                <div class="p1">
                    <p id="116">本文利用卷积神经网络(CNN)能够捕获文本中的局部特征信息的优点,将主题向量模型生成的文档向量和词向量与词性向量拼接的混合向量分别作为CNN网络的输入对其进行训练,获取更多局部隐藏的情感特征信息。</p>
                </div>
                <div class="p1">
                    <p id="117">卷积神经网络由输入层、卷积层、池化层和全连接层组成(如图3所示)。本次试验中,将词向量<i>v</i>(<i>w</i><sub><i>i</i></sub>)与词性向量<i>v</i>(<i>w</i><sub><i>pos</i></sub>)的拼接混合向量作为输入进行训练(同时对主向量进行训练)。即</p>
                </div>
                <div class="p1">
                    <p id="118" class="code-formula">
                        <mathml id="118"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>r</mi><msup><mrow></mrow><mo>*</mo></msup><mo>=</mo><mo stretchy="false">[</mo><mi>v</mi><mo stretchy="false">(</mo><mi>w</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo>⊕</mo><mi>v</mi><mo stretchy="false">(</mo><mi>w</mi><msub><mrow></mrow><mrow><mi>p</mi><mi>o</mi><mi>s</mi></mrow></msub><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>8</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="119">卷积层使用不同的滤波器对输入向量进行卷积操作,从而获取输入向量中的局部特征,计算公式如下</p>
                </div>
                <div class="p1">
                    <p id="120" class="code-formula">
                        <mathml id="120"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>u</mi><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo><mo>=</mo><mi>f</mi><mo stretchy="false">(</mo><mi>W</mi><mo>⋅</mo><mi>r</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mo>:</mo><mo stretchy="false">(</mo><mi>i</mi><mo>+</mo><mi>h</mi><mo>-</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><mo>*</mo></msubsup><mo>+</mo><mi>b</mi><mo stretchy="false">)</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>9</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="121">式中,<i>r</i><sup>*</sup><sub><i>i</i></sub><sub>:(</sub><sub><i>i</i></sub><sub>+</sub><sub><i>h</i></sub><sub>-1)</sub>表示输入向量的第<i>i</i>行至<i>i</i>+<i>h</i>-1行抽取的局部特征矩阵,<i>u</i>(<i>i</i>)是某个卷积核在位置<i>i</i>的卷积输出,<i>W</i>是滤波器,<i>b</i>是卷积偏置项,<i>f</i>(·)是非线性卷积核函数,采用<i>relu</i>作为激活函数。</p>
                </div>
                <div class="p1">
                    <p id="122">之后,最大池化层对向量<i>u</i>中所有<i>u</i>(<i>i</i>)求最大值以获取其中最显著的特征值,计算公式如下</p>
                </div>
                <div class="p1">
                    <p id="123" class="code-formula">
                        <mathml id="123"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>c</mi><mo>^</mo></mover><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>m</mi><mi>a</mi><mi>x</mi></mrow></mstyle><mi>i</mi></munder><mo stretchy="false">{</mo><mi>u</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo><mo stretchy="false">}</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>0</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="124">式中,<i>j</i>表示第<i>j</i>个卷积核。本文实验中使用了不同尺寸的卷积核,分别为<i>h</i>=3,<i>h</i>=4,<i>h</i>=5。对于有<i>T</i>个卷积核的窗口采样得到的特征信息如式(11)所示。下采样层输出的特征向量作为全连接层的输入。</p>
                </div>
                <div class="p1">
                    <p id="125" class="code-formula">
                        <mathml id="125"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>c</mi><mo>^</mo></mover><mo>=</mo><mo stretchy="false">[</mo><mover accent="true"><mi>c</mi><mo>^</mo></mover><msub><mrow></mrow><mn>1</mn></msub><mo>,</mo><mover accent="true"><mi>c</mi><mo>^</mo></mover><msub><mrow></mrow><mn>2</mn></msub><mo>,</mo><mo>⋯</mo><mo>,</mo><mover accent="true"><mi>c</mi><mo>^</mo></mover><msub><mrow></mrow><mi>Τ</mi></msub><mo stretchy="false">]</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>1</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <h4 class="anchor-tag" id="126" name="126"><b>4.3 情感计算层</b></h4>
                <div class="p1">
                    <p id="127">本文中使用softmax函数作为情感分类器,将全连接层输出的特征表示<mathml id="185"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mtext>c</mtext><mo>^</mo></mover></math></mathml>作为softmax函数的输出情感极性分类判别的结果,计算公式如下</p>
                </div>
                <div class="p1">
                    <p id="128" class="code-formula">
                        <mathml id="128"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>y</mi><mo>^</mo></mover><mo>=</mo><mi>s</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy="false">(</mo><mi>W</mi><msub><mrow></mrow><mi>f</mi></msub><mo>⋅</mo><mover accent="true"><mi>c</mi><mo>^</mo></mover><mo>+</mo><mi>b</mi><msub><mrow></mrow><mi>f</mi></msub><mo stretchy="false">)</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>2</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="129">式中,<mathml id="186"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover accent="true"><mi>y</mi><mo>^</mo></mover></math></mathml>是模型预测的文本情感类别,<i>W</i><sub><i>f</i></sub>,<i>b</i><sub><i>f</i></sub>分别是全连接层权重矩阵和偏置。</p>
                </div>
                <h4 class="anchor-tag" id="130" name="130"><b>4.4 模型训练</b></h4>
                <div class="p1">
                    <p id="131">本文使用反向传播算法来训练和更新模型,通过最小化交叉熵来优化模型<citation id="212" type="reference"><link href="51" rel="bibliography" /><sup>[<a class="sup">25</a>]</sup></citation>,交叉熵损失函数的计算公式如下</p>
                </div>
                <div class="p1">
                    <p id="132" class="code-formula">
                        <mathml id="132"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>l</mi><mi>o</mi><mi>s</mi><mi>s</mi><mo>=</mo><mo>-</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Τ</mi></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>C</mi></munderover><mi>y</mi></mstyle></mrow></mstyle><msub><mrow></mrow><mi>i</mi></msub><mi>l</mi><mi>o</mi><mi>g</mi><mover accent="true"><mi>y</mi><mo>^</mo></mover><msubsup><mrow></mrow><mi>i</mi><mi>j</mi></msubsup><mo>+</mo><mi>λ</mi><mo stretchy="false">∥</mo><mi>θ</mi><mo stretchy="false">∥</mo><msup><mrow></mrow><mn>2</mn></msup><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>3</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="133">式中,<i>T</i>是训练数据集,<i>C</i>为情感类别数,<i>y</i>为文本实际情感类别,<i>λ</i>为正则项,是损失函数的惩罚项,<i>θ</i>是设置的参数。</p>
                </div>
                <h3 id="134" name="134" class="anchor-tag"><b>5 实验准备</b></h3>
                <h4 class="anchor-tag" id="135" name="135"><b>5.1 情感分析数据集</b></h4>
                <div class="p1">
                    <p id="136">为了构建维吾尔文句子级情感语料。本文实验选取天山网等维文网站上公开发布的文章和评论信息。本次试验利用实验室开发的网络爬虫工具下载网页,经过去重、去噪等操作筛选出包含情感倾向的评论信息。对收集好的语料进行分句,然后人工对其进行分类,构建了二分类(UySenti2Data)如表3所示。</p>
                </div>
                <div class="area_img" id="137">
                    <p class="img_tit"><b>表3 维吾尔二分类数据集</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="137" border="1"><tr><td><br />数据</td><td>正面句子</td><td>负面句子</td><td>总计</td></tr><tr><td><br />训练集</td><td>1.2万</td><td>1.2万</td><td>2.4万</td></tr><tr><td><br />开发集</td><td>0.15万</td><td>0.15万</td><td>0.3万</td></tr><tr><td><br />测试集</td><td>0.1万</td><td>0.1万</td><td>0.2万</td></tr><tr><td><br />总计</td><td>1.45万</td><td>1.45万</td><td>2.9万</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="138">除了维吾尔文情感二分类(正面、负面)语料库外,还构建了情感五分类(中性、高兴、生气、惊讶、难过)语料库(UySenti5Data)如表4所示。</p>
                </div>
                <div class="area_img" id="139">
                    <p class="img_tit"><b>表4 维吾尔五分类数据集</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="139" border="1"><tr><td><br />数据</td><td>中性</td><td>高兴</td><td>生气</td><td>惊讶</td><td>难过</td></tr><tr><td><br />训练集</td><td>1.5万</td><td>1.2万</td><td>1.2万</td><td>0.8万</td><td>0.8万</td></tr><tr><td><br />开发集</td><td>0.15万</td><td>0.1万</td><td>0.1万</td><td>0.1万</td><td>0.1万</td></tr><tr><td><br />测试集</td><td>0.1万</td><td>0.1万</td><td>0.1万</td><td>0.1万</td><td>0.1万</td></tr><tr><td><br />总计</td><td>1.75万</td><td>1.4万</td><td>1.4万</td><td>1万</td><td>1万</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="140" name="140"><b>5.2 评价指标</b></h4>
                <div class="p1">
                    <p id="141">文本情感分类作为一种文本分类任务,为了评估本文模型的情感分类性能,使用准确率(Precision)、召回率(Recall)和F值作为评价标准<citation id="213" type="reference"><link href="41" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>。计算情感分类的准确率、召回率和F1值公式如下</p>
                </div>
                <div class="p1">
                    <p id="142" class="code-formula">
                        <mathml id="142"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mi>Ρ</mi><mo>=</mo><mfrac><mrow><mi>Τ</mi><mi>Ρ</mi></mrow><mrow><mi>Τ</mi><mi>Ρ</mi><mo>+</mo><mi>F</mi><mi>Ρ</mi></mrow></mfrac></mtd></mtr><mtr><mtd><mi>R</mi><mo>=</mo><mfrac><mrow><mi>Τ</mi><mi>Ρ</mi></mrow><mrow><mi>Τ</mi><mi>Ρ</mi><mo>+</mo><mi>F</mi><mi>Ν</mi></mrow></mfrac></mtd></mtr><mtr><mtd><mi>F</mi><mn>1</mn><mo>=</mo><mfrac><mrow><mn>2</mn><mo>×</mo><mi>Ρ</mi><mo>×</mo><mi>R</mi></mrow><mrow><mi>Ρ</mi><mo>+</mo><mi>R</mi></mrow></mfrac></mtd></mtr></mtable></mrow><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>4</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="143">其中TP:将正类预测为正类的句子数(True Positive);FN:将正类预测为负类的句子数(False Negative);FP:将负类预测为正类的句子数;TN:将负类预测为负类的句子数。</p>
                </div>
                <h4 class="anchor-tag" id="144" name="144"><b>5.2 数据预处理</b></h4>
                <div class="p1">
                    <p id="145">由于从网上下载的文本语料不是规范正式,这种非正式语料通常包含URL、邮箱,各种标点符号,数字、用户名等,而这些非正式文本给情感分类带来了极大挑战。</p>
                </div>
                <div class="p1">
                    <p id="146">为了去除噪声干扰,本文预先对维吾尔文情感文本语料进行预处理,具体规则如下:</p>
                </div>
                <div class="p1">
                    <p id="147">1)所有数字都被替换为“0”处理。</p>
                </div>
                <div class="p1">
                    <p id="148">2)如果句子里存在网址和电子邮件字符串,则对其进行删除。</p>
                </div>
                <div class="p1">
                    <p id="149">3)删除除了句号、逗号、感叹号、问号以外的标点符号,以移除任何可能影响情感分类性能的噪音。</p>
                </div>
                <div class="p1">
                    <p id="150">4 )如果句子中存在汉文单词则用实验室提供的汉维翻译接口对其进行翻译,用翻译结果替换原汉文单词。</p>
                </div>
                <div class="p1">
                    <p id="151">5)如果情感句子中出现连续的感叹号、问号、句号或其它字符,则只保留一个对应的符号或字符。</p>
                </div>
                <h4 class="anchor-tag" id="152" name="152"><b>5.3 实验参数设置</b></h4>
                <div class="p1">
                    <p id="153">为了达到理想的情感分类效果,进行了不同的实验,调节了模型的超参数。通过交最小化交叉熵证来选择实验参数。词向量维度和主题向量维度均设置为300,。dropout设置为0.5,以防止过度拟合。使用样本数量(mini-batch)为128和Adam优化算法来训练模型。在实验中,模型训练的迭代的次数为30时达到了最好的准确率,更多细节如5表所示。</p>
                </div>
                <div class="area_img" id="154">
                    <p class="img_tit"><b>表5 神经网络参数设置</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="154" border="1"><tr><td><br />网络层</td><td>参数</td><td>参数/函数名称</td><td>取值</td></tr><tr><td><br />CNN</td><td>m</td><td>滤波器窗口大小</td><td>3,4,5</td></tr><tr><td><br /></td><td>T</td><td>滤波器数量</td><td>100,150,200</td></tr><tr><td><br />文本表示层</td><td>d<sub>w</sub></td><td>词向量</td><td>300</td></tr><tr><td><br /></td><td>d<sub>pos</sub></td><td>词性向量</td><td>17</td></tr><tr><td><br /></td><td>d<sub>t</sub></td><td>主题向量</td><td>300</td></tr><tr><td><br />其它参数</td><td>p</td><td>dropout比例</td><td>0.5</td></tr><tr><td><br /></td><td>--</td><td>优化函数</td><td>Adam</td></tr><tr><td><br /></td><td>batch size</td><td>批量梯度采样样本数</td><td>32</td></tr><tr><td><br /></td><td>lr</td><td>学习率</td><td>0.01</td></tr><tr><td><br /></td><td>it</td><td>迭代次数</td><td>30</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="155" name="155"><b>5.4 实验对比模型</b></h4>
                <div class="p1">
                    <p id="156">本文实验中使用不同的情感分类方法,与本文提出的混合模型进行对比,验证本文提出的方法的有效性,并使用词向量、词性向量和主题向量特征提高了本文提出的方法的情感分类准确率。</p>
                </div>
                <div class="p1">
                    <p id="157">1)基于情感词典的方法(SentiDict):根据实验室构建的情感词典来对维吾尔文进行情感分类,根据句子包含的情感词的正面或负面极性对句子进行分类。</p>
                </div>
                <div class="p1">
                    <p id="158">2)多项式朴素贝叶斯 (Multinomial Naive Bayes,MNB):是一个典型的传统机器学习方法之一,在许多文本分类和情感分类任务中得到了广泛应用。</p>
                </div>
                <div class="p1">
                    <p id="159">3)S-CNN模型:采用简单单通道卷积神经网络对维吾尔文句子的词向量表示进行学习,通过卷积和最大池化操作捕获维吾尔文句子的局部特征实现文本的情感分类。</p>
                </div>
                <div class="p1">
                    <p id="160">4)支持向量机(Support Vector Machine,SVM)模型:SVM是最常用的传统机器学习情感分类方法,通常采用n-gram作为特征进行分类,本次试验中使用unigram、bigram和trigram作为SVM的特征进行分类。</p>
                </div>
                <div class="p1">
                    <p id="161">5)长短记忆网络(LSTM):使用标准的单向LSTM网络对维吾尔文句子进行编码表示映射到可变长目标序列。</p>
                </div>
                <div class="p1">
                    <p id="162">6)本文提出的基于主题联合词向量和多通道卷积神经网络的维吾尔文情感分类方法。</p>
                </div>
                <h4 class="anchor-tag" id="163" name="163"><b>5.5 实验结果及分析</b></h4>
                <div class="p1">
                    <p id="164">为了验证本文提出的TWE-UMCNN模型在维吾尔文情感分类任务上的有效性,在实验室构建的二分类和五分类情感数据集上进行对比实验。两个情感分类数据集中分配的训练数据集、开发集和测试集的具体数据量如表3,4所示。</p>
                </div>
                <div class="p1">
                    <p id="165">本文实验中选取了基于词典的分类方法、典型的传统机器学习和简单神经网络模型与本文提出的模型进行了对比。</p>
                </div>
                <h4 class="anchor-tag" id="166" name="166">5.5.1 神经网络方法与基准方法对比实验</h4>
                <div class="p1">
                    <p id="167">为了对比神经网络方法与词典方法和传统机器学习方法,第一组实验在UySenti2Data和UySenti5Data两种情感分类数据集上进行了实验。为了公平对比各方法的分类效果,除了词典方法外,其它机器学习方法和神经网络方法都使用相同的参数进行实验,只使用维度为d<sub>w</sub>=300的词向量作为特征输入来进行实验,如表6所示。</p>
                </div>
                <div class="area_img" id="168">
                    <p class="img_tit"><b>表6 基准模型对比实验结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="168" border="1"><tr><td rowspan="2"><br />模型</td><td colspan="6"><br />情感2分类</td><td colspan="6">情感5分类</td></tr><tr><td colspan="2"><br />准确率<br />/%</td><td colspan="2">召回率<br />/%</td><td colspan="2">F1值</td><td colspan="2">准确<br />率</td><td colspan="2">召回<br />率</td><td colspan="2">F1值</td></tr><tr><td colspan="2"><br />词典方法</td><td colspan="2">56.23</td><td colspan="2">61.89</td><td colspan="2">54.44</td><td colspan="2">44.5</td><td colspan="2">46.72</td><td>43.07</td></tr><tr><td colspan="2"><br />MNB</td><td colspan="2">62.54</td><td colspan="2">61.34</td><td colspan="2">61.93</td><td colspan="2">52.2</td><td colspan="2">50.86</td><td>51.52</td></tr><tr><td colspan="2"><br />SVM-unigram</td><td colspan="2">77.4</td><td colspan="2">78.95</td><td colspan="2">78.12</td><td colspan="2">64.5</td><td colspan="2">63.78</td><td>63.14</td></tr><tr><td colspan="2"><br />SVM-bigram</td><td colspan="2">84.23</td><td colspan="2">82.87</td><td colspan="2">83.54</td><td colspan="2">72.71</td><td colspan="2">71.83</td><td>72.27</td></tr><tr><td colspan="2"><br />SVM-trigram</td><td colspan="2">81.5</td><td colspan="2">80.24</td><td colspan="2">80.87</td><td colspan="2">71.65</td><td colspan="2">69.24</td><td>70.42</td></tr><tr><td colspan="2"><br />LSTM</td><td colspan="2">72.59</td><td colspan="2">71.2</td><td colspan="2">71.89</td><td colspan="2">59.73</td><td colspan="2">58.48</td><td>59.1</td></tr><tr><td colspan="2"><br />S-CNN</td><td colspan="2">83.65</td><td colspan="2">82.64</td><td colspan="2">83.14</td><td colspan="2">74.51</td><td colspan="2">74.24</td><td>74.37</td></tr><tr><td colspan="2"><br />TWE-UMCNN</td><td colspan="2">84.28</td><td colspan="2">83.65</td><td colspan="2">83.96</td><td colspan="2">74.89</td><td colspan="2">75.62</td><td>75.25</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="169">从表6可以看出,本组实验中对词典方法、MNB 方法、SVM机器学习方法、LSTM模型、S-CNN模型、TWE-UMCNN模型进行了分类,实验结果表明,本文提出的TWE-UMCNN模型的二分类、五分类情感F1值均高于其它基准方法。</p>
                </div>
                <h4 class="anchor-tag" id="170" name="170">5.5.2 词性特征向量和主题特征向量对情感分类的影响</h4>
                <div class="p1">
                    <p id="171">为了验证词性特征向量对情感分类的有效性,首先,将词性向量(<i>F</i><sub><i>pos</i></sub>)与词向量(<i>F</i><sub><i>w</i></sub>)拼接生成的混合向量(<i>F</i>=<i>F</i><sub><i>pos</i></sub>⊕<i>F</i><sub><i>w</i></sub>)作为TWE-UMCNN模型的输入进行训练,从表10可以看出,使用词性特征向量后,在二分类和五分类数据集上的F1值分别提升了1.51%、1.42%。其次,将主题特征向量矩阵作为单独CNN网络的输入,对其进行卷积和最大池化操作,将从池化层出来的信息全连接到另一个通道CNN网络池化层出来的信息,最后对其进行softmax操作输出分类结果。从表10中可以看到,添加主题特征向量后本文模型在两种数据集上的F1值别提升了2.89%,1.69%。</p>
                </div>
                <div class="area_img" id="172">
                    <p class="img_tit"><b>表7 扩展词性特征和主题特征的实验结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="172" border="1"><tr><td colspan="2"><br />模型及特征向量</td><td colspan="3">情感2分类</td><td colspan="3">情感5分类</td></tr><tr><td><br />模型</td><td>特征</td><td>准确率<br />/%</td><td>召回率<br />/%</td><td>F1值</td><td>准确率<br />/%</td><td>召回率<br />/%</td><td>F1值<br />/%</td></tr><tr><td><br />TWE-</td><td><i>F</i><sub><i>w</i></sub>⊕<i>F</i><sub><i>pos</i></sub></td><td>85.73</td><td>85.21</td><td>85.47</td><td>76.26</td><td>75.85</td><td>76.67</td></tr><tr><td><br />UMCNN</td><td><i>F</i><sub><i>w</i></sub>⊕<i>F</i><sub><i>pos</i></sub>⊕<i>F</i><sub><i>t</i></sub></td><td>87.43</td><td>86.28</td><td>86.85</td><td>77.52</td><td>76.39</td><td>76.95</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="173" name="173">5.5.3 网络参数对模型情感分类的影响</h4>
                <div class="p1">
                    <p id="174">本组实验中观察词向量维、dropout和优化函数等主要网络参数的变化对情感分类的影响。首先,对词向量的维度进行实验,本次实验选取d<sub>w</sub>=[100、200、300、400]作为词向量的维度观察其对分类效果的影响,当维度为300时达到了最好的分类效果。本文模型,其实验结果如图4所示。</p>
                </div>
                <div class="area_img" id="175">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJZ201910041_175.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVRyNFB2YmhVNFRrV0FxZi9KeHFTZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 词向量维度的影响" src="Detail/GetImg?filename=images/JSJZ201910041_175.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVRyNFB2YmhVNFRrV0FxZi9KeHFTZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图4 词向量维度的影响</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJZ201910041_175.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVRyNFB2YmhVNFRrV0FxZi9KeHFTZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="176">其次,为了缓解过拟合,使用dropout方法进行正则化,通常情况下dropout的值选为0.5,在本次实验中dropout的值范围选为p=[0.2, 0.3, 0.4, 0.5, 0.6, 0.7]进行实验,其实验结果如图5所示。</p>
                </div>
                <div class="area_img" id="177">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJZ201910041_177.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVRyNFB2YmhVNFRrV0FxZi9KeHFTZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 dropout参数实验结果" src="Detail/GetImg?filename=images/JSJZ201910041_177.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVRyNFB2YmhVNFRrV0FxZi9KeHFTZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图5 dropout参数实验结果</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJZ201910041_177.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVRyNFB2YmhVNFRrV0FxZi9KeHFTZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="178">最后,分别选取RMSprop、SGD、adadelta,Adam作为优化函数观察每个优化函数对情感分类的影响,当优化函数选为Adam函数时,模型达到了最好的分类效果,其实验结果如图6所示。</p>
                </div>
                <div class="area_img" id="179">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJZ201910041_179.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVRyNFB2YmhVNFRrV0FxZi9KeHFTZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 优化函数实验结果" src="Detail/GetImg?filename=images/JSJZ201910041_179.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVRyNFB2YmhVNFRrV0FxZi9KeHFTZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图6 优化函数实验结果</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJZ201910041_179.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVRyNFB2YmhVNFRrV0FxZi9KeHFTZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h3 id="180" name="180" class="anchor-tag"><b>6 结论</b></h3>
                <div class="p1">
                    <p id="181">本文针对维吾尔文情感分类任务,丰富了卷积神经网络的输入矩阵特征,提出了一种基于主题向量、词性向量和词向量的多通道卷积神经网络方法。首先,对神经网络模型的表示层进行改进,将词性特征向量作为词向量的补充,从词粒度层面挖掘单词的情感信息,然后通过<b>LDA</b>模型生成了主题向量,从语义粒度层面挖掘情感信息。其次,将词粒度层面的混合向量和语义粒度层面的主题向量作为<b>CNN</b>网络的输入进行训练模型,提高了特征表示不充分的问题。最终,使用<b>softmax</b>函数得到了情感分类结果。实验结果表明,本文所提出的混合模型在维吾尔文情感二分类和五分类任务的准确率、召回率和<b>F1</b>值上要明显高于传统机器学习方法和简单深度学习方法,并通过实验验证了该模型的有效性。</p>
                </div>
                <div class="area_img" id="219">
                                <img alt="" src="Detail/GetImg?filename=images/JSJZ201910041_21900.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVRyNFB2YmhVNFRrV0FxZi9KeHFTZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                            <p class="img_tit"></p>

                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Lexicon Integrated CNN Models with Attention for Sentiment Analysis">

                                <b>[1]</b> B Shin,L Timothy,C Jinho.Lexicon Integrated CNN Models with Attention for Sentiment Analysis[J].arXiv preprint arXiv:1610.06272,2016.
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=RJXB201008009&amp;v=MDUyNjNVUjdxZlp1WnBGeS9tVjczQU55ZlRiTEc0SDlITXA0OUZiWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckM=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVRyNFB2YmhVNFRrV0FxZi9KeHFTZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> 赵妍妍,秦兵,刘挺.文本情感分析[J].软件学报,2010,21(8):1834-1848.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Attention-based LSTM for Aspect-level Sentiment Classification">

                                <b>[3]</b> Y Q Wang,M Huang,L Zhao,et al.Attention-based LSTM for aspect-level sentiment classification[C].Proc of Conference on Empirical Methods in Natural Language Processing,2016:606-615.
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=EDZX201604003&amp;v=MzI3NjVyRzRIOWZNcTQ5Rlo0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVacEZ5L21WNzNBSUNuUmQ=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVRyNFB2YmhVNFRrV0FxZi9KeHFTZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b> Y S Zhang,Y R Jiang,Y X Tong.Study of sentiment classification for Chinese microblog based on recurrent neural network[J].Chinese Journal of Electronics,2016,25(4):601-607.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Dimensional sentiment analysis using a regional CNN-LSTM model">

                                <b>[5]</b> J Wang,L C Yu,K R Lai,et al.Dimensional sentiment analysis using a regional CNN-LSTM model [C].Proceedings of the Annual Meeting of the Association for Computational Linguistics.Berlin,Germany,2016:225-230.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Sentiment Analysis with Tree-Structured Gated Recurrent Units">

                                <b>[6]</b> M Kuta,M Morawiec,J Kitowski.Sentiment Analysis with Tree-Structured Gated Recurrent Units[C].Text,Speech,and Dialogue:20th International Conference,TSD 2017,Prague,Czech Republic,August 27-31,2017.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Sentiment classification based on supervised latent n-gram analysis">

                                <b>[7]</b> D Bespalov,B Bai,A Shokoufandeh,et al.Sentiment Classification Based on Supervised Latent n-gram Analysis[C].Acm International Conference on Information &amp; Knowledge Management.ACM,2011.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=An Improved Text Sentiment Classification Model Using TF-IDF and Next Word Negation">

                                <b>[8]</b> B Das,S Chakraborty.An Improved Text Sentiment Classification Model Using TF-IDF and Next Word Negation[J].arXiv preprint arXiv:1806.06407v1,2018.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fast and accurate sentiment classification using an enhanced Naive Bayes model">

                                <b>[9]</b> V Narayanan,I Arora,A Bhatia.Fast and accurate sentiment classification using an enhanced Naive Bayes model[J].arXiv preprint arXiv:1305.6143 201,3.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Bag of Tricks for Efficient Text Classification">

                                <b>[10]</b> A Joulin,E Grave,P Bojanowski,et al.Bag of Tricks for Efficient Text Classification[J].arXiv preprint arXiv:1610.06272,2016.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_11" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=WHDY201703011&amp;v=MTgxOTdZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVacEZ5L21WNzNBTWlYUGQ3RzRIOWJNckk5RVo=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVRyNFB2YmhVNFRrV0FxZi9KeHFTZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[11]</b> 刘金硕,李哲,叶馨,等.文本情感倾向性分析方法:bfsmPMI-SVM[J].武汉大学学报:理学版.2017,63(3):259-264.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_12" >
                                    <b>[12]</b>
                                 B Pang,L Lee.Opinion mining and sentiment analysis[J].Foundations and Trend s in Information Retrieval.2008,2(1/2):1-135.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Distributed Representations of Words and Phrases and their Compositionality">

                                <b>[13]</b> T Mikolov,I Sutskever,K Chen,et al.Distributed representations of words and phrases and their compositionality[J].Advances in Neural Information Processing Systems,2013,26:3111-3119.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Text Classification of Flu-Related Tweets Using FastText with Sentiment and Keyword Features">

                                <b>[14]</b> A Alessa,M Faezipour,Z Alhassan.Text Classification of Flu-Related Tweets Using FastText with Sentiment and Keyword Features[C].2018 IEEE International Conference on Healthcare Informatics (ICHI),2018:366-367.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Vector representation of words for sentiment analysis using GloVe">

                                <b>[15]</b> Y Sharma,G Agrawal,P Jain,et al.Vector representation of words for sentiment analysis using GloVe[C].International Conference on Intelligent Communication &amp; Computational Techniques.IEEE Computer Society,2017.
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_16" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep learning">

                                <b>[16]</b> Y Lecun,Y Bengio,G Hinton.Deep learning[J].Nature.2015,521(7553):436-444.
                            </a>
                        </p>
                        <p id="35">
                            <a id="bibliography_17" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Convolutional Neural Networks for Sentence Classification">

                                <b>[17]</b> Yoon Kim.Convolutional neural networks for sentence classification[C].In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing.2014:1746-1751.
                            </a>
                        </p>
                        <p id="37">
                            <a id="bibliography_18" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Convolutional Neural Network for Modelling Sentences">

                                <b>[18]</b> N Kalchbrenner,E Grefenstette,P Blunsom.A convolutional neural network for modelling sentences[C].In:Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,2014:1749-1751
                            </a>
                        </p>
                        <p id="39">
                            <a id="bibliography_19" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJBK&amp;filename=SJBK15090500014198&amp;v=MTUzMTNvOUZaT29MRFhVeG9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaW5sVXJqSUoxMFFhQm89TmlmSlpiSzlIdGpNcQ==&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVRyNFB2YmhVNFRrV0FxZi9KeHFTZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[19]</b> S Hochreiter,J Schmidhuber.Long Short-Term Memory[J].Neural Computation,1997,9(8):1735-1780.
                            </a>
                        </p>
                        <p id="41">
                            <a id="bibliography_20" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000004805&amp;v=MDMzMjdRYUJvPU5pZklZN0s3SHRqTnI0OUZaT3NMQkh3OG9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaW5sVXJqSUoxMA==&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVRyNFB2YmhVNFRrV0FxZi9KeHFTZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[20]</b> D M BLEI,A Y NG.Probabilistic Topic Models[J].Communications of the ACM.2011,55(4):55-65.
                            </a>
                        </p>
                        <p id="43">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Joint Sentiment/Topic Model for Sentiment Analysis">

                                <b>[21]</b> Lin Chenghua,He Yulan.Joint sentiment/topic model for sentiment analysis[J].International Conference on Information and Knowledge Management,Proceedings,2009.
                            </a>
                        </p>
                        <p id="45">
                            <a id="bibliography_22" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A novel neural topic model and its supervised extension">

                                <b>[22]</b> CAO Ziqiang,LI Sujian.LIU Yang,et al.A Novel Neural Topic Model and Its Supervised Extension[C].Proceedings of the 29th AAAI Conference on Artificial Inteligence.Austin,USA:AAAI.2015:2210-2216.
                            </a>
                        </p>
                        <p id="47">
                            <a id="bibliography_23" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Topical word embeddings">

                                <b>[23]</b> LIU Yang,LIU Zhiyuan,CHUA T S,et al.Topical Word Embeddings [C].Proceedings of the 29th AAAI Conference on Artificial Inteligence.Austin,USA:AAAI.2015:2418-2424.
                            </a>
                        </p>
                        <p id="49">
                            <a id="bibliography_24" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Incorporating word correlation knowledge into topic modeling">

                                <b>[24]</b> XIE Pengtao,YANG Diyi,XING E.Incorporating Word Correlation Knowledge into Topic Modeling[C].Proceedings of 2015 Conference of the North American Chapter of the Association for Computational Linguistics:Human Language Technologies[S.l.]:Association for Computational Linguistics.2015:725-734.
                            </a>
                        </p>
                        <p id="51">
                            <a id="bibliography_25" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Improving Topic Models with Latent Feature Word Representations">

                                <b>[25]</b> D Q Nguyen,R Billingsley,L Du,et a1.Improving topic models with latent feature word representations[J].Transactions of the Association for Computational Linguistics.2015-3:299-313.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJZ201910041" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdXNzY2Z1OVRyNFB2YmhVNFRrV0FxZi9KeHFTZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJZ201910041&amp;v=MDk1NThRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpwRnkvbVY3M0FMejdCZExHNEg5ak5yNDlCWlk=&amp;uid=WEEvREcwSlJHSldRa1FhdXNzY2Z1OVRyNFB2YmhVNFRrV0FxZi9KeHFTZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="0" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=P0ea_aWnQfx5aXr8bS4u3GGmKX_ORbqAf_1HJ_4Trvg1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
