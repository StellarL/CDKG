<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=NOOK-mvTMnxl11s6oPYV0boPXN3iP4N7Rc-A56nk4KI1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2fKXReader%2fDetail%3fTIMESTAMP%3d637141814958256250%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJZ201904047%26RESULT%3d1%26SIGN%3drHdj3MIb4T3b8bK8iS10GB9zUnk%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJZ201904047&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJZ201904047&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJZ201904047&amp;v=MDUxMzJPM3pxcUJ0R0ZyQ1VSN3FmWnVabUZ5emdWcnZNTHo3QmRMRzRIOWpNcTQ5Qlk0UUtESDg0dlI0VDZqNTQ=&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#31" data-title="&lt;b&gt;1 引言&lt;/b&gt; "><b>1 引言</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#36" data-title="&lt;b&gt;2 本文算法&lt;/b&gt; "><b>2 本文算法</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#37" data-title="&lt;b&gt;2.1 算法框架&lt;/b&gt;"><b>2.1 算法框架</b></a></li>
                                                <li><a href="#40" data-title="&lt;b&gt;2.2 预处理&lt;/b&gt;"><b>2.2 预处理</b></a></li>
                                                <li><a href="#48" data-title="&lt;b&gt;2.3 卷积神经网络&lt;/b&gt;"><b>2.3 卷积神经网络</b></a></li>
                                                <li><a href="#80" data-title="&lt;b&gt;2.4 跟踪算法&lt;/b&gt;"><b>2.4 跟踪算法</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#96" data-title="&lt;b&gt;3 实验结果与分析&lt;/b&gt; "><b>3 实验结果与分析</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#97" data-title="&lt;b&gt;3.1 实验设置&lt;/b&gt;"><b>3.1 实验设置</b></a></li>
                                                <li><a href="#100" data-title="&lt;b&gt;3.2 定量分析&lt;/b&gt;"><b>3.2 定量分析</b></a></li>
                                                <li><a href="#108" data-title="&lt;b&gt;3.3 定性分析&lt;/b&gt;"><b>3.3 定性分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#119" data-title="&lt;b&gt;4 总结&lt;/b&gt; "><b>4 总结</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#39" data-title="&lt;b&gt;图1 算法框架&lt;/b&gt;"><b>图1 算法框架</b></a></li>
                                                <li><a href="#44" data-title="&lt;b&gt;图2 相关跟踪&lt;/b&gt;"><b>图2 相关跟踪</b></a></li>
                                                <li><a href="#115" data-title="&lt;b&gt;表1 平均中心位置误差、平均重叠率和成功率比较&lt;/b&gt;"><b>表1 平均中心位置误差、平均重叠率和成功率比较</b></a></li>
                                                <li><a href="#116" data-title="&lt;b&gt;图3 Sequence 1视频的跟踪结果 (a, b, c) Sequence 2视频的跟踪结果 (d, e, f&lt;/b&gt;) "><b>图3 Sequence 1视频的跟踪结果 (a, b, c) Sequence 2视频的跟踪结果 (d, e, f</b>) </a></li>
                                                <li><a href="#117" data-title="&lt;b&gt;图4 Sequence 3视频的跟踪结果 (a, b, c) Sequence 4视频的跟踪结果 (d, e, f&lt;/b&gt;) "><b>图4 Sequence 3视频的跟踪结果 (a, b, c) Sequence 4视频的跟踪结果 (d, e, f</b>) </a></li>
                                                <li><a href="#118" data-title="&lt;b&gt;图5 Labman视频的跟踪结果 (a, b, c) Intersection视频的跟踪结果 (d, e, f&lt;/b&gt;) "><b>图5 Labman视频的跟踪结果 (a, b, c) Intersection视频的跟踪结果 (d, e, f</b>) </a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" title=" J Fan, W Xu, Y Wu, and Y Gong.Human tracking using convolutional neural networks[J].IEEE Transactions on Neural Networks, 2010, 21 (10) :1610-1623." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Human tracking using convolutional neural networks">
                                        <b>[1]</b>
                                         J Fan, W Xu, Y Wu, and Y Gong.Human tracking using convolutional neural networks[J].IEEE Transactions on Neural Networks, 2010, 21 (10) :1610-1623.
                                    </a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" title=" Li Hanxi, Yi Li, and Fatih Porikli.Deeptrack:Learning discriminative feature representations online for robust visual tracking[J].IEEE Transactions on Image Processing, 2016, 25 (4) :1834-1848." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep Track:learning discriminative feature representations online for robust visual tracking">
                                        <b>[2]</b>
                                         Li Hanxi, Yi Li, and Fatih Porikli.Deeptrack:Learning discriminative feature representations online for robust visual tracking[J].IEEE Transactions on Image Processing, 2016, 25 (4) :1834-1848.
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title=" Wang Naiyan, and Dit-Yan Yeung.Learning a deep compact image representation for visual tracking[J].In Advances in neural information processing systems, 2013:809-817." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning a Deep Compact Image Representation forVisual Tracking">
                                        <b>[3]</b>
                                         Wang Naiyan, and Dit-Yan Yeung.Learning a deep compact image representation for visual tracking[J].In Advances in neural information processing systems, 2013:809-817.
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" title=" K Zhang, Q Liu, Y Wu, and M H Yang.Robust visual tracking via convolutional networks without training[J].IEEE Transactions on Image Processing, 2016, 25, (4) :1779-1792." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Robust visual tracking via convolutional networks without training">
                                        <b>[4]</b>
                                         K Zhang, Q Liu, Y Wu, and M H Yang.Robust visual tracking via convolutional networks without training[J].IEEE Transactions on Image Processing, 2016, 25, (4) :1779-1792.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" title=" J W Davis, and V Sharma.Background-subtraction using contour-based fusion of thermal and visible imagery[J].Computer Vision and Image Understanding, 2007, 106 (2) :162-182." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501083985&amp;v=MTI1NjhVcmZJSkZzUmJoWT1OaWZPZmJLN0h0RE5xbzlFWk9NTUJYUThvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp0RmlubA==&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!">
                                        <b>[5]</b>
                                         J W Davis, and V Sharma.Background-subtraction using contour-based fusion of thermal and visible imagery[J].Computer Vision and Image Understanding, 2007, 106 (2) :162-182.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" title=" C O&#39;Conaire, N E O&#39;Connor, E Cooke, and A F Smeaton.Comparison of fusion methods for thermo-visual surveillance tracking[C].Information Fusion, 2006 9th International Conference on.IEEE.2006." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Comparison of Fusion Methods for Thermo-Visual Surveillance Tracking">
                                        <b>[6]</b>
                                         C O&#39;Conaire, N E O&#39;Connor, E Cooke, and A F Smeaton.Comparison of fusion methods for thermo-visual surveillance tracking[C].Information Fusion, 2006 9th International Conference on.IEEE.2006.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" title=" D A Ross, J Lim, R S Lin, and M H Yang.Incremental learning for robust visual tracking[J].International Journal of Computer Vision, 2008, 77 (1) :125-141." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002831201&amp;v=MTg1NTk9Tmo3QmFyTzRIdEhPcDR4RVp1c09ZM2s1ekJkaDRqOTlTWHFScnhveGNNSDdSN3FkWitadUZpRGxWN3JOSkZv&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!">
                                        <b>[7]</b>
                                         D A Ross, J Lim, R S Lin, and M H Yang.Incremental learning for robust visual tracking[J].International Journal of Computer Vision, 2008, 77 (1) :125-141.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" title=" B Babenko, M H Yang, and S Belongie.Robust object tracking with online multiple instance learning[J].IEEE transactions on pattern analysis and machine intelligence, 2011, 33 (8) :1619-1632." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Robust Object Tracking with Online Multiple Instance Learning">
                                        <b>[8]</b>
                                         B Babenko, M H Yang, and S Belongie.Robust object tracking with online multiple instance learning[J].IEEE transactions on pattern analysis and machine intelligence, 2011, 33 (8) :1619-1632.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" title=" Z Kalal, K Mikolajczyk, and J Matas.Tracking-learning-detection[J].IEEE transactions on pattern analysis and machine intelligence, 2012, 34 (7) :1409-1422." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Tracking-Learning-Detection">
                                        <b>[9]</b>
                                         Z Kalal, K Mikolajczyk, and J Matas.Tracking-learning-detection[J].IEEE transactions on pattern analysis and machine intelligence, 2012, 34 (7) :1409-1422.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_10" title=" K Zhang, L Zhang, and M H Yang.Fast compressive tracking[J].IEEE transactions on pattern analysis and machine intelligence, 2014, 36 (10) :2002-2015." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fast compressive tracking">
                                        <b>[10]</b>
                                         K Zhang, L Zhang, and M H Yang.Fast compressive tracking[J].IEEE transactions on pattern analysis and machine intelligence, 2014, 36 (10) :2002-2015.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_11" title=" S Hare, S Golodetz, A Saffari, V Vineet, M M Cheng, S L Hicks, and P H Torr.Struck:Structured output tracking with kernels[J].IEEE transactions on pattern analysis and machine intelligence, 2016, 38 (10) :2096-2109." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Struck:Structured Output Tracking with Kernels">
                                        <b>[11]</b>
                                         S Hare, S Golodetz, A Saffari, V Vineet, M M Cheng, S L Hicks, and P H Torr.Struck:Structured output tracking with kernels[J].IEEE transactions on pattern analysis and machine intelligence, 2016, 38 (10) :2096-2109.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_12" title=" X Jia, H Lu, and M H Yang.Visual tracking via adaptive structural local sparse appearance model[C].In Computer vision and pattern recognition (CVPR) , 2012 IEEE Conference on (pp.1822-1829) .IEEE, 2012." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Visual tracking via adaptive structural local sparse appearance model">
                                        <b>[12]</b>
                                         X Jia, H Lu, and M H Yang.Visual tracking via adaptive structural local sparse appearance model[C].In Computer vision and pattern recognition (CVPR) , 2012 IEEE Conference on (pp.1822-1829) .IEEE, 2012.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_13" title=" G Xiao, X Yun, and J Wu.A multi-cue mean-shift target tracking approach based on fuzzified region dynamic image fusion[J].Science China Information Sciences, 2012, 55 (3) :577-589." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD1A35285D6EF41A246A55332A2644B312&amp;v=MjAyNDdKSGRUT3A0b3hZcDU1Q0gxSXpSSVZtenA0UzN6ZzNSQXpmYmJtUnJ1ZENPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkJyTFUwNXRCaHdMMjZ4S3c9Tmo3QmFyTA==&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!">
                                        <b>[13]</b>
                                         G Xiao, X Yun, and J Wu.A multi-cue mean-shift target tracking approach based on fuzzified region dynamic image fusion[J].Science China Information Sciences, 2012, 55 (3) :577-589.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_14" title=" X Yun, Z Jing, and B Jin.Visible and infrared tracking based on multi-view multi-kernel fusion model[J].Optical Review, 2016, 23 (2) :244-25." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Visible and infrared tracking based on multi-view multi-kernel fusion model">
                                        <b>[14]</b>
                                         X Yun, Z Jing, and B Jin.Visible and infrared tracking based on multi-view multi-kernel fusion model[J].Optical Review, 2016, 23 (2) :244-25.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJZ" target="_blank">计算机仿真</a>
                2019,36(04),223-228             </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于卷积神经网络的可见光和红外跟踪算法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="javascript:;">徐宁文</a>
                                <a href="javascript:;">肖刚</a>
                </h2>
                    <h2>

                    <span>上海交通大学航空航天学院</span>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>针对目标遮挡、运动模糊和复杂背景等问题, 提出一种可见光和红外视频融合的有效卷积神经网络的相关目标跟踪算法, 以提高跟踪性能。两层的卷积神经网络通过使用卷积滤波器, 提取可见光和红外视频的稀疏特征。其中的卷积滤波器包含两种类型, 目标滤波器和相关滤波器。在第一帧中, 算法使用一组归一化融合局部图像作为目标滤波器。在其它帧, 算法使用相关模型, 生成相关滤波器, 将前景和背景信息相结合, 构建精确的表观模型。实验表明, 上述无需训练的轻量级跟踪算法, 与传统的目标跟踪算法相比, 显著的提升了目标跟踪性能。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA&amp;code=&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!" target="_blank">目标跟踪;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!" target="_blank">卷积神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%9B%B8%E5%85%B3%E6%A8%A1%E5%9E%8B&amp;code=&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!" target="_blank">相关模型;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%9B%BE%E5%83%8F%E8%9E%8D%E5%90%88&amp;code=&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!" target="_blank">图像融合;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    徐宁文 (1993-) , 女 (汉族) , 河南省郑州市人, 硕士研究生, 主要研究领域为图像融合与目标跟踪。;
                                </span>
                                <span>
                                    肖刚 (1974-) , 男 (汉族) , 江苏省苏州市人, 教授, 博士研究生导师, 主要研究领域为图像融合与目标跟踪。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2017-12-27</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家重点基础研究发展规划项目973计划 (2014CB744903);</span>
                                <span>国家自然基金 (61673270);</span>
                                <span>上海浦江人才计划 (16PJD028);</span>
                                <span>中国航天科技创新基金 (HTKJCX2015CAAA09);</span>
                                <span>上海市科委科研计划项目 (17DZ1204304);</span>
                    </p>
            </div>
                    <h1><b>Object Tracking Algorithm Based on Convolutional Neural Network for Visible and Infrared Video Sequences</b></h1>
                    <h2>
                    <span>XU Ning-wen</span>
                    <span>XIAO Gang</span>
            </h2>
                    <h2>
                    <span>School of Aeronautics and Astronautics, Shanghai Jiaotong University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>To address the problems such as occlusion, motion blur and complex backgrounds, a novel relative object tracking algorithm using a convolutional neural network is proposed aiming to boost the tracking performance. A two-layer convolutional neural network extracts sparse feature representation of visible and infrared sequences via convolutional filters. The convolutional filters contain two types, object filter, and relative filters. In the first frame, we employed a set of normalized fusion patches as the object filters. Moreover, a relative model was explored to generate relative filters, which integrated information from both foreground and background to build accurate appearance model. This algorithm without training is robust and efficient. Quantitative and qualitative evaluations demonstrate that the performance of this algorithm improves significantly the performance of target tracking.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Object%20tracking&amp;code=&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!" target="_blank">Object tracking;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Convolutional%20neural%20network&amp;code=&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!" target="_blank">Convolutional neural network;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Relative%20model&amp;code=&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!" target="_blank">Relative model;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Image%20fusion&amp;code=&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!" target="_blank">Image fusion;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2017-12-27</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="31" name="31" class="anchor-tag"><b>1 引言</b></h3>
                <div class="p1">
                    <p id="32">目标跟踪算法是一种利用视频序列追踪定位单个或多个感兴趣目标位置的算法, 是机器视觉研究领域中很重要的一部分。</p>
                </div>
                <div class="p1">
                    <p id="33">近年来, 目标跟踪算法专注于不同类型的特征提取算法, 例如Haar-like特征、HOG描述符和主成分分析等。这些基于手工提取的特征不能有效地捕捉目标的语义信息从而限制跟踪算法的性能。并且这些特征描述了基于特定模板或场景的对象, 因而对通用的对象不具有适应性。</p>
                </div>
                <div class="p1">
                    <p id="34">卷积神经网络在机器视觉领域引起了人们的广泛关注。但离线深度学习因为目标样本的限制不适用通用目标的跟踪。而在线学习的目标跟踪算法无法提供大量的训练数据并且训练时间有限, 因此只有很少的研究者尝试将卷积神经网络算法直接用于目标跟踪<citation id="122" type="reference"><link href="3" rel="bibliography" /><link href="5" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>]</sup></citation>。除此以外, 大多数相关算法基于卷积神经网络提取视频中的特征<citation id="121" type="reference"><link href="7" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>, 进而完成跟踪算法。但相关算法存在以下不足:①大多算法仅适用可见光视频作为跟踪数据源, 数据有限, 对于目标遮挡失踪问题无法解决。②这些算法一般使用候选窗或完整图像进行特征提取, 但前者无法使用有效的目标或背景信息, 后者运算速度有限。③使用卷积神经网络进行在线特征学习算法有限, 并且无法有效的获取特征。</p>
                </div>
                <div class="p1">
                    <p id="35">为了完善以上不足, 本文提出了一种用于通用对象的轻量级卷积神经网络的跟踪算法, 有效的解决目标遮挡、运动模糊和复杂背景等问题。算法贡献在于:本文首先设计了可见光和红外视频融合算法, 有利于增强了信息的多样性并保留了特征的不变性。并且提出了一种相关模型用于相关候选窗提取, 有效的获取背景以及相关目标信息。同时, 本文提出了一种无需训练的两层卷积神经网络, 并且对卷积滤波器进行分类, 构建了一种具有鲁棒性的表观模型。最后实验结论证明了本文算法在测试视频中有效的提高跟踪结果。</p>
                </div>
                <h3 id="36" name="36" class="anchor-tag"><b>2 本文算法</b></h3>
                <h4 class="anchor-tag" id="37" name="37"><b>2.1 算法框架</b></h4>
                <div class="p1">
                    <p id="38">目标跟踪的本质必须学习平移变化的能力, 但算法同时需要利用卷积神经网络的平移不变性克服漂移问题。本文针对这个问题, 提出了基于相关跟踪的卷积神经网络算法, 算法由三个基本部分组成, 即预处理, 卷积神经网络和跟踪算法。算法的框架如图1所示。</p>
                </div>
                <div class="area_img" id="39">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJZ201904047_039.jpg&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!">
                                    <img alt="图1 算法框架" src="Detail/GetImg?filename=images/JSJZ201904047_039.jpg&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!" />
                                </a>
                                <p class="img_tit"><b>图1 算法框架</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJZ201904047_039.jpg&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="40" name="40"><b>2.2 预处理</b></h4>
                <div class="p1">
                    <p id="41">预处理算法是对从可见光和红外视频中提取的图像块进行归一化处理的过程, 包含三个步骤:候选窗生成、扭曲和规范化和图像融合。</p>
                </div>
                <div class="p1">
                    <p id="42">候选窗生成有两种不同的方法:直接模型和相对模型。现有的大多数目标跟踪算法都是直接模型。直接模型利用目标或背景的局部图像利用二元分类进行跟踪, 但是这类算法对于目标表观建模会由于部分遮挡、复杂背景或一些传感器杂波等其它变化因素, 出现偏差。相反, 相对模型有效地利用了不同候选窗之间的相对关系, 将所有相关对象使用相对权重进行了整合。这类算法利用了更多的信息因此有效的解决遮挡等问题。因此, 本文设计一种相对模型, 该模型基于上一帧的跟踪结果的重叠率将候选窗划分为<b>6</b>类:<mathml id="43"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mn>0</mn><mo stretchy="false"> (</mo><mn>0</mn><mo>, </mo><mn>0</mn><mo>.</mo><mn>3</mn><mo stretchy="false">) </mo><mrow><mo>[</mo><mrow><mn>0</mn><mo>.</mo><mn>3</mn><mo>, </mo><mn>0</mn><mo>.</mo><mn>6</mn></mrow><mo>]</mo></mrow><mrow><mo>[</mo><mrow><mn>0</mn><mo>.</mo><mn>6</mn><mo>, </mo><mn>0</mn><mo>.</mo><mn>8</mn></mrow><mo>]</mo></mrow><mo stretchy="false">[</mo><mn>0</mn><mo>.</mo><mn>8</mn><mo>, </mo><mn>0</mn><mo>.</mo><mn>9</mn><mo stretchy="false">) </mo><mrow><mo>[</mo><mrow><mn>0</mn><mo>.</mo><mn>9</mn><mo>, </mo><mn>1</mn></mrow><mo>]</mo></mrow></mrow></math></mathml>中每类具有<b><i>m</i></b>选窗, 具体如图<b>2</b>所示。</p>
                </div>
                <div class="area_img" id="44">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJZ201904047_044.jpg&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!">
                                    <img alt="图2 相关跟踪" src="Detail/GetImg?filename=images/JSJZ201904047_044.jpg&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!" />
                                </a>
                                <p class="img_tit"><b>图2 相关跟踪</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJZ201904047_044.jpg&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="45">基于已经获得的相关候选窗, 为了使算法满足对不同大小的候选窗的适应性, 每个候选窗图像均需要进行图像扭曲、<b>0</b>均值和归一化操作。为了融合和平衡可见光和红外图像, 本文将可见光图像进行灰度化, 公式如下:</p>
                </div>
                <div class="p1">
                    <p id="46" class="code-formula">
                        <mathml id="46"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>G</mi><mo>=</mo><mn>0</mn><mo>.</mo><mn>2</mn><mn>9</mn><mn>8</mn><mn>9</mn><mo>×</mo><mi>R</mi><mo>+</mo><mn>0</mn><mo>.</mo><mn>5</mn><mn>8</mn><mn>7</mn><mn>0</mn><mo>×</mo><mi>G</mi><mo>+</mo><mn>0</mn><mo>.</mo><mn>1</mn><mn>1</mn><mn>4</mn><mn>0</mn><mo>×</mo><mi>B</mi><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="47">式中, <b>G</b>表示灰度化后的像素值, 而<b><i>RGB</i></b>分别对应着图像中<b><i>RGB</i></b>三个通道的像素值。灰度化后本文将灰度化的图像与红外图像进行堆叠得到一组尺寸为<b>n×n</b>的双通道候选窗<b>I</b>。并进行<b>0</b>均值和归一化算法的预处理。基于此, 算法使用大小为<b>w×w</b>的滑动窗口密集的选取具有重叠内容的局部图像块<b>Y={Y</b><sub><b>1</b></sub>, <b>Y</b><sub><b>2</b></sub>, …, <b>Y</b><sub> (<b>n-w+1</b>) <sup><b>2</b></sup></sub>}。</p>
                </div>
                <h4 class="anchor-tag" id="48" name="48"><b>2.3 卷积神经网络</b></h4>
                <div class="p1">
                    <p id="49">卷积神经网络的第一层是简单卷积层, 本层根据相关候选窗的选取, 针对不同帧的可见光和红外光图像, 进行特征提取, 其中的特征不单单结合了目标、背景信息, 还结合了目标周边的相关信息。</p>
                </div>
                <div class="p1">
                    <p id="50">第一帧的跟踪结果经过预处理获得的局部图像块, 可以使用<b><i>K</i>-<i>means</i></b>算法进行聚类, 从而获得一系列具有目标外观特性的卷积滤波器<b>P</b><sup><b>t</b></sup>={<b>P</b><mathml id="51"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>1</mn><mi>t</mi></msubsup></mrow></math></mathml>, <b>P</b><mathml id="52"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mn>2</mn><mi>t</mi></msubsup></mrow></math></mathml>, …, <b>P</b><mathml id="53"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>d</mi><mi>t</mi></msubsup></mrow></math></mathml>}⊂<b>Y</b>。这些目标卷积滤波器具有确定的目标特征, 对于这些目标滤波器可以候选窗<b>I</b>进行卷积操作, 进而得到目标特征图。</p>
                </div>
                <div class="p1">
                    <p id="54" class="code-formula">
                        <mathml id="54"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>F</mi><msubsup><mrow></mrow><mi>i</mi><mi>t</mi></msubsup><mo>=</mo><mi>Ρ</mi><msubsup><mrow></mrow><mi>i</mi><mi>t</mi></msubsup><mo>⊗</mo><mi>Ι</mi><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="55">式中, ⨂表示卷积操作, <b>F</b><mathml id="56"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mi>t</mi></msubsup></mrow></math></mathml>表示生成的目标特征图。</p>
                </div>
                <div class="p1">
                    <p id="57">不同于其它算法, 本文提取与目标相关的其它特征。类似目标特征的提取方法, 本文对相关候选窗使用<b><i>K</i>-<i>means</i></b>算法获得相关卷积滤波器<b>P</b><mathml id="58"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mi>r</mi></msubsup></mrow></math></mathml>={<b>P</b><mathml id="59"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mn>1</mn><mi>j</mi></mrow><mi>r</mi></msubsup></mrow></math></mathml>, <b>P</b><mathml id="60"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mn>2</mn><mi>j</mi></mrow><mi>r</mi></msubsup></mrow></math></mathml>, …, <b>P</b><mathml id="61"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>d</mi><mi>j</mi></mrow><mi>r</mi></msubsup></mrow></math></mathml>}⊂<b>Y</b>, 式中<b>P</b><mathml id="62"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mi>r</mi></msubsup></mrow></math></mathml>表示这是第<b>r</b>类候选窗中的第<b>j</b>个滤波器, 同时满足<b>j≤m, r≤6</b>条件。因此一共获得了<b>6md</b>个相关卷积滤波器。在此对滤波器进行合并, 生成<b>6</b>个滤波器组</p>
                </div>
                <div class="p1">
                    <p id="63" class="code-formula">
                        <mathml id="63"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><msup><mrow></mrow><mi>r</mi></msup><mo>=</mo><mrow><mo>{</mo><mrow><mfrac><mrow><mstyle displaystyle="true"><msubsup><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></msubsup><mi>Ρ</mi></mstyle><msubsup><mrow></mrow><mrow><mn>1</mn><mi>j</mi></mrow><mi>r</mi></msubsup></mrow><mi>m</mi></mfrac><mo>, </mo><mfrac><mrow><mstyle displaystyle="true"><msubsup><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></msubsup><mi>Ρ</mi></mstyle><msubsup><mrow></mrow><mrow><mn>2</mn><mi>j</mi></mrow><mi>r</mi></msubsup></mrow><mi>m</mi></mfrac><mo>, </mo><mo>⋯</mo><mo>, </mo><mfrac><mrow><mstyle displaystyle="true"><msubsup><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></msubsup><mi>Ρ</mi></mstyle><msubsup><mrow></mrow><mrow><mi>d</mi><mi>j</mi></mrow><mi>r</mi></msubsup></mrow><mi>m</mi></mfrac></mrow><mo>}</mo></mrow><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="64">本文同样使用相关滤波器对候选窗<b>I</b>进行卷积操作, 进而得到特征图。</p>
                </div>
                <div class="p1">
                    <p id="65" class="code-formula">
                        <mathml id="65"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>F</mi><msubsup><mrow></mrow><mi>i</mi><mi>r</mi></msubsup><mo>=</mo><mi>Ρ</mi><msubsup><mrow></mrow><mi>i</mi><mi>r</mi></msubsup><mo>⊗</mo><mi>Ι</mi><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="66">因此根据目标卷积滤波器和相关卷积滤波器可以得到具有完整信息的特征图:</p>
                </div>
                <div class="p1">
                    <p id="67" class="code-formula">
                        <mathml id="67"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>F</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mi>F</mi><msubsup><mrow></mrow><mi>i</mi><mi>t</mi></msubsup><mo>+</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>r</mi><mo>=</mo><mn>0</mn></mrow><mn>6</mn></munderover><mi>W</mi></mstyle><msub><mrow></mrow><mi>r</mi></msub><mi>F</mi><msubsup><mrow></mrow><mi>i</mi><mi>r</mi></msubsup><mo>=</mo><mo stretchy="false"> (</mo><mi>Ρ</mi><msubsup><mrow></mrow><mi>i</mi><mi>t</mi></msubsup><mo>+</mo><mi>W</mi><mi>Ρ</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo><mo>⊗</mo><mi>Ι</mi><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>5</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="68">式中, <b>F</b><sub><b>i</b></sub>表示由第<b>i</b>个滤波器生成的具有完整信息的特征图, <b>W</b>是一个权重向量, 分别给不同的相关卷积滤波器提供权重, 因此其长度为<b>6</b>。根据<b>6</b>个<b>F</b><sub><b>i</b></sub>可以生成一个 (<b>n-w+1) × (n-w+1) ×d</b>的特征<b>F</b>, 其中第<b>i</b>层的特征即为<b>F</b><sub><b>i</b></sub>。</p>
                </div>
                <div class="p1">
                    <p id="69">为了使得特征<b>F</b>对于图像模糊、外表变化等问题具有更强的鲁棒性, 本文首先将特征<b>F</b>转化为向量<b>f</b>∈ℝ<sup> (<b>n-w+1</b>) <sup><b>2</b></sup><b>d</b></sup>, 然后使用以下公式<citation id="123" type="reference"><link href="9" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>来获得稀疏向量<b>f</b> ′</p>
                </div>
                <div class="p1">
                    <p id="70" class="code-formula">
                        <mathml id="70"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>f</mi><msup><mspace width="0.25em" /><mo>′</mo></msup><mo>=</mo><mi>a</mi><mi>r</mi><mi>g</mi><mspace width="0.25em" /><mi>m</mi><mi>i</mi><mi>n</mi><msub><mrow></mrow><mrow><mi>f</mi><msup><mspace width="0.25em" /><mo>′</mo></msup></mrow></msub><mi>α</mi><mo stretchy="false">∥</mo><mi>f</mi><msup><mspace width="0.25em" /><mo>′</mo></msup><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mn>1</mn><mn>1</mn></msubsup><mo>+</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mo stretchy="false">∥</mo><mi>f</mi><mspace width="0.25em" /><msup><mrow></mrow><mo>´</mo></msup><mo>-</mo><mi>f</mi><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>6</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="71">利用软阈值可以得到一个近似解</p>
                </div>
                <div class="p1">
                    <p id="72" class="code-formula">
                        <mathml id="72"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>f</mi><msup><mspace width="0.25em" /><mo>′</mo></msup><mo>=</mo><mtext>s</mtext><mtext>i</mtext><mtext>g</mtext><mtext>n</mtext><mo stretchy="false"> (</mo><mi>f</mi><mo stretchy="false">) </mo><mi>max</mi><mo stretchy="false"> (</mo><mn>0</mn><mo>, </mo><mi>a</mi><mi>b</mi><mi>s</mi><mo stretchy="false"> (</mo><mi>f</mi><mo stretchy="false">) </mo><mo>-</mo><mi>α</mi><mo stretchy="false">) </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>7</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="73">式中<b><i>sign</i> (<i>f</i></b>) 表示获取向量<b>f</b>的符号。</p>
                </div>
                <div class="p1">
                    <p id="74">稀疏向量<b>f</b> ′作为表观模型需要不断更新, 从而确保跟踪算法可以适应当前帧的目标情况, 确保算法的鲁棒性。因此使用以下公式进行更新</p>
                </div>
                <div class="p1">
                    <p id="75" class="code-formula">
                        <mathml id="75"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>f</mi><mo>^</mo></mover><msub><mrow></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>β</mi><mo stretchy="false">) </mo><mover accent="true"><mi>f</mi><mo>^</mo></mover><msub><mrow></mrow><mi>t</mi></msub><mo>+</mo><mi>β</mi><mi>f</mi><mspace width="0.25em" /><msubsup><mrow></mrow><mi>t</mi><mo>´</mo></msubsup><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>8</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="76">式中β表示为更新参数, <mathml id="77"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>f</mi><mo>^</mo></mover><msub><mrow></mrow><mi>t</mi></msub></mrow></math></mathml>和<mathml id="78"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>f</mi><mo>^</mo></mover><msub><mrow></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow></math></mathml>意味第<b>t</b>帧和第<b>t+1</b>帧的表观模型。这种在线更新方案不仅可以快速使用目标的外观变化, 利用快速更新卷积滤波器生成最合适的表观模型<mathml id="79"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>f</mi><mo>^</mo></mover><msub><mrow></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow></math></mathml>, 并且因为使用了第一帧保留的目标特征, 因此可以保留目标特征, 缓解了跟踪漂移问题。</p>
                </div>
                <h4 class="anchor-tag" id="80" name="80"><b>2.4 跟踪算法</b></h4>
                <div class="p1">
                    <p id="81">跟踪算法可以基于粒子滤波算法确定。已知<b>s</b><sub><b>t</b></sub><b>y</b><sub><b>t</b></sub>分别表示是第<b>t</b>帧的目标状态和目标观察值。算法的解题思路使用朴素贝叶斯算法确定一个后验概率</p>
                </div>
                <div class="p1">
                    <p id="82" class="code-formula">
                        <mathml id="82"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>p</mi><mo stretchy="false"> (</mo><mi>s</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">|</mo><mi>y</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">) </mo><mo>∝</mo><mi>p</mi><mo stretchy="false"> (</mo><mi>y</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">|</mo><mi>s</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">) </mo><mstyle displaystyle="true"><mrow><mo>∫</mo><mi>p</mi></mrow></mstyle><mo stretchy="false"> (</mo><mi>s</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">|</mo><mi>s</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo stretchy="false">) </mo><mi>p</mi><mo stretchy="false"> (</mo><mi>s</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo stretchy="false">|</mo><mi>y</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo stretchy="false">) </mo><mspace width="0.25em" /><mi>d</mi><mi>s</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>9</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="83">在实际的目标跟踪算法中, 后验概率<b>p (s</b><sub><b>t</b></sub>|<b>y</b><sub><b>t</b></sub>) 是基于<b>n</b>个粒子<b>s</b><mathml id="84"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>t</mi><mi>i</mi></msubsup></mrow></math></mathml>进行估计的</p>
                </div>
                <div class="p1">
                    <p id="85" class="code-formula">
                        <mathml id="85"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>p</mi><mo stretchy="false"> (</mo><mi>s</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">|</mo><mi>y</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">) </mo><mo>≈</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>w</mi></mstyle><msubsup><mrow></mrow><mi>t</mi><mi>i</mi></msubsup><mi>δ</mi><mo stretchy="false"> (</mo><mi>s</mi><msub><mrow></mrow><mi>t</mi></msub><mo>, </mo><mi>s</mi><msubsup><mrow></mrow><mi>t</mi><mi>i</mi></msubsup><mo stretchy="false">) </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>0</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="86">式中δ () 是狄拉克三角函数。基于此, 算法可以简化为</p>
                </div>
                <div class="p1">
                    <p id="87" class="code-formula">
                        <mathml id="87"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>S</mi><mo>^</mo></mover><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mi>arg</mi><mspace width="0.25em" /><mi>max</mi><msub><mrow></mrow><mrow><mi>s</mi><msubsup><mrow></mrow><mi>t</mi><mi>i</mi></msubsup></mrow></msub><mi>w</mi><msubsup><mrow></mrow><mi>t</mi><mi>i</mi></msubsup><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="88">而<b>w</b><mathml id="89"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>t</mi><mi>i</mi></msubsup></mrow></math></mathml>使用第<b>t</b>帧的<mathml id="90"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>f</mi><mo>^</mo></mover><msub><mrow></mrow><mi>t</mi></msub></mrow></math></mathml>表观模型表示</p>
                </div>
                <div class="p1">
                    <p id="91" class="code-formula">
                        <mathml id="91"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>w</mi><msubsup><mrow></mrow><mi>t</mi><mi>i</mi></msubsup><mo>∝</mo><mi>e</mi><msup><mrow></mrow><mrow><mo>-</mo><mo stretchy="false">∥</mo><mover accent="true"><mi>f</mi><mo>^</mo></mover><msub><mrow></mrow><mi>t</mi></msub><mo>-</mo><mi>f</mi><mspace width="0.25em" /><msubsup><mrow></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mi>i</mi></msubsup><mi>w</mi><mo stretchy="false">∥</mo><msubsup><mrow></mrow><mn>2</mn><mn>1</mn></msubsup></mrow></msup><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="92">式中⊙是点乘算法, <b>f</b><mathml id="93"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mi>i</mi></msubsup></mrow></math></mathml>表示第<b>i</b>个粒子<b>s</b><mathml id="94"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>t</mi><mi>i</mi></msubsup></mrow></math></mathml>的特征表示, 而权重<b>w</b>的第<b>i</b>个元素是依据模板<mathml id="95"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>f</mi><mo>^</mo></mover><msub><mrow></mrow><mi>t</mi></msub></mrow></math></mathml>的对应元素决定的, 如果元素为<b>0</b>, 则权重为<b>0</b>, 否则为<b>1</b>。</p>
                </div>
                <h3 id="96" name="96" class="anchor-tag"><b>3 实验结果与分析</b></h3>
                <h4 class="anchor-tag" id="97" name="97"><b>3.1 实验设置</b></h4>
                <div class="p1">
                    <p id="98">本文使用<b><i>MATLAB</i></b>在主频<b>3.60<i>GHz</i>, <i>CPU</i></b>为<b><i>Intel Core i</i>7-3820</b>的电脑运行。为了验证算法的可靠性, 本文使用<b><i>OTCBVS</i></b>数据库<citation id="124" type="reference"><link href="11" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>、<b><i>AIC</i></b>数据库<citation id="125" type="reference"><link href="13" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>以及实验室拍摄的共<b>6</b>个视频数据进行实验。其中<b><i>Sequence</i> 1、<i>Sequeonce</i> 2、<i>Sequence</i> 3</b>和<b><i>Sequence</i> 4</b>视频是来自<b><i>OTCBVS</i></b>数据库, 而<b><i>Labman</i></b>是来自<b><i>AIC</i></b>数据库, <b><i>Intersection</i></b>是来自实验室自主拍摄的。基于这些视频序列, 本文将和<b><i>IVT</i></b><citation id="126" type="reference"><link href="15" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>、<b><i>MIL</i></b><citation id="127" type="reference"><link href="17" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>、<b><i>TLD</i></b><citation id="128" type="reference"><link href="19" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>、<b><i>CT</i></b><citation id="129" type="reference"><link href="21" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>、<b><i>STRUCK</i></b><citation id="130" type="reference"><link href="23" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>、<b><i>ASLA</i></b><citation id="131" type="reference"><link href="25" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>、<b><i>FRDIF</i></b><citation id="132" type="reference"><link href="27" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>和<b><i>MVMKF</i></b><citation id="133" type="reference"><link href="29" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation><b>9</b>种算法进行比较。为确保算法的有效性和准确性, 相关参数是经过多次实验确定的。对于预处理部分, 设置<b>n=32, w=6, d=100</b>。对于卷积神经网络部分, 对于每个相关候选窗对应的权重需要赋值<mathml id="99"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>W</mi><mo>=</mo><mrow><mo>[</mo><mrow><mo>-</mo><mn>2</mn><mo>, </mo><mn>0</mn><mo>.</mo><mn>0</mn><mn>5</mn><mo>, </mo><mn>0</mn><mo>.</mo><mn>1</mn><mn>5</mn><mo>, </mo><mn>0</mn><mo>.</mo><mn>2</mn><mo>, </mo><mn>0</mn><mo>.</mo><mn>2</mn><mn>5</mn><mo>, </mo><mn>0</mn><mo>.</mo><mn>3</mn><mn>5</mn></mrow><mo>]</mo></mrow></mrow></math></mathml>。设置软阈值参数α为特征<i>f</i>的中位数, 而模板更新参数<i>β</i>=<b>0.95</b>。</p>
                </div>
                <h4 class="anchor-tag" id="100" name="100"><b>3.2 定量分析</b></h4>
                <div class="p1">
                    <p id="101">定量分析是指使用中心位置误差、重叠率和成功率对算法进行评估, 具体如表<b>1</b>所示。其中, 中心位置误差是计算真实中心点到跟踪窗口中心点的欧氏距离。其公式如下</p>
                </div>
                <div class="p1">
                    <p id="102" class="code-formula">
                        <mathml id="102"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtext>l</mtext><mtext>o</mtext><mtext>c</mtext><mtext>a</mtext><mtext>t</mtext><mtext>i</mtext><mtext>o</mtext><mtext>n</mtext><mspace width="0.25em" /><mtext>e</mtext><mtext>r</mtext><mtext>r</mtext><mtext>o</mtext><mtext>r</mtext><mo>=</mo><mroot><mrow><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>R</mi></msub><mo>-</mo><mi>x</mi><msub><mrow></mrow><mi>Τ</mi></msub><mo stretchy="false">) </mo><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mo stretchy="false"> (</mo><mi>y</mi><msub><mrow></mrow><mi>R</mi></msub><mo>-</mo><mi>y</mi><msub><mrow></mrow><mi>Τ</mi></msub><mo stretchy="false">) </mo><msup><mrow></mrow><mn>2</mn></msup></mrow><mrow></mrow></mroot><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="103">式中, (<b><i>x</i></b><sub><b><i>R</i></b></sub>, <b><i>y</i></b><sub><b><i>R</i></b></sub>) (<b><i>x</i></b><sub><b><i>T</i></b></sub>, <b><i>y</i></b><sub><b><i>T</i></b></sub>) 分别表示真实中心点和跟踪窗口中心点。重叠率是基于目标跟踪结果对应的区域和真实的区域的重合面积来评判跟踪效果的一个指标, 它定义为</p>
                </div>
                <div class="p1">
                    <p id="104" class="code-formula">
                        <mathml id="104"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtext>o</mtext><mtext>v</mtext><mtext>e</mtext><mtext>r</mtext><mtext>l</mtext><mtext>a</mtext><mtext>p</mtext><mspace width="0.25em" /><mtext>r</mtext><mtext>a</mtext><mtext>t</mtext><mtext>i</mtext><mtext>o</mtext><mo>=</mo><mfrac><mrow><mtext>A</mtext><mtext>r</mtext><mtext>e</mtext><mtext>a</mtext><mo stretchy="false"> (</mo><mi>R</mi><mi>Ο</mi><mi>Ι</mi><msub><mrow></mrow><mi>R</mi></msub><mstyle displaystyle="true"><mo>∩</mo><mi>R</mi></mstyle><mi>Ο</mi><mi>Ι</mi><msub><mrow></mrow><mi>Τ</mi></msub><mo stretchy="false">) </mo></mrow><mrow><mtext>A</mtext><mtext>r</mtext><mtext>e</mtext><mtext>a</mtext><mo stretchy="false"> (</mo><mi>R</mi><mi>Ο</mi><mi>Ι</mi><msub><mrow></mrow><mi>R</mi></msub><mstyle displaystyle="true"><mo>∪</mo><mi>R</mi></mstyle><mi>Ο</mi><mi>Ι</mi><msub><mrow></mrow><mi>Τ</mi></msub><mo stretchy="false">) </mo></mrow></mfrac><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>4</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="105">式中真实窗口是<b><i>ROI</i></b><sub><b><i>R</i></b></sub>而跟踪窗口是<b><i>ROI</i></b><sub><b><i>T</i></b></sub>。如果重叠率大于一个给定的阈值 (一般设为<b>0.5</b>) , 这一帧的结果被视为跟踪成功。而成功率就是基于跟踪成功的数目而计算得到的。</p>
                </div>
                <div class="p1">
                    <p id="106" class="code-formula">
                        <mathml id="106"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtext>s</mtext><mtext>u</mtext><mtext>c</mtext><mtext>c</mtext><mtext>e</mtext><mtext>s</mtext><mtext>s</mtext><mspace width="0.25em" /><mtext>r</mtext><mtext>a</mtext><mtext>t</mtext><mtext>e</mtext><mo>=</mo><mfrac><mrow><mi>Ν</mi><msub><mrow></mrow><mrow><mi>s</mi><mi>u</mi><mi>c</mi><mi>c</mi><mi>e</mi><mi>s</mi><mi>s</mi></mrow></msub></mrow><mtext>Ν</mtext></mfrac><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>5</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="107">式中<b><i>N</i></b><sub><b><i>success</i></b></sub>是成功的帧数。表格显示:本文提出的算法与其它算法相比, 具有最好的跟踪结果。虽然运算速度较慢, 但可以使用<b>GPU</b>加快运行效果。</p>
                </div>
                <h4 class="anchor-tag" id="108" name="108"><b>3.3 定性分析</b></h4>
                <div class="p1">
                    <p id="109">图<b>3</b>到图<b>5</b>显示了九个算法在六个视频序列中重要帧的跟踪结果。</p>
                </div>
                <div class="p1">
                    <p id="110"><b>Sequence 1、Sequence 3</b>和<b>Intersection</b>视频显示了目标出现部分遮挡甚至出现消失的现象。基于卷积神经网络的目标跟踪算法显示了稳定的跟踪效果, 而包括<b>CT、MIL、ASLA</b>等优秀算法均产生一定的漂移现象。这主要是因为当目标被具有相似可见光特征的物体遮挡时, 这些只依赖可见光视频生成的表观特征无法辨别目标。同时, <b>FRDIF</b>和<b>MVMKF</b>算法虽然使用可见光和红外视频, 因为无法较准确地定位目标, 无法解决严重的遮挡问题。</p>
                </div>
                <div class="p1">
                    <p id="111"><b>Sequeonce 2</b>视频展示了光线变化对跟踪算法的影响。视频中可以发现当白云移动时, 会造成光线变化和阴影移动。而当目标走进阴影区域时, 大多数经典算法如<b>CT、IVT</b>等无法完成跟踪, 这是因为这些算法的表观模型无法适应复杂的光线变化。需要注意的是, 这不能归咎于这些算法的建模失败。在其中第#<b>400</b>帧前后, 实际上无法用肉眼直接找到目标, 但本文提供的多传感器融合算法可以进行准确的跟踪。同理, <b>MVMKF</b>算法也获得优异的跟踪结果。</p>
                </div>
                <div class="p1">
                    <p id="112">图<b>4</b>显示了相似背景对跟踪算法的影响。由于背景与目标颜色特征相似, 跟踪算法容易出现跟丢目标 (如<b>MIL、TLD、CV</b>) , 或者跟错目标的情况 (如<b>MVMKF、ALSA</b>) 。因此基于背景和相关目标的特征提取的算法重要性也体现于此, 本文算法能够正确准确的跟踪。</p>
                </div>
                <div class="p1">
                    <p id="113">尺度变化是一个在每个视频都会出现的问题, 比如目标的由远及近或由近及远的运动, 目标的大小会随之变化。在视频<b>Sequence 1、Sequence 2</b>和<b>Sequence 4</b>中, 目标具有明显的变化。无法适应尺度变化的算法其重叠率也会很差。如<b>CT</b>算法和<b>MVMKF</b>算法提供相同大小的候选窗, 因此无法适应尺度变化。相反卷积神经网络算法对尺度变化敏感, 解决了尺度变化的问题。</p>
                </div>
                <div class="p1">
                    <p id="114">视频<b>Labman</b>提供了目标快速移动甚至产生图像模糊问题的情况, 即在视频中目标突然左右摇头, 如第#<b>280</b>帧和第#<b>300</b>帧;或者突然转头, 如第#<b>334</b>帧。在这些情况时, 实验发现大多数的跟踪算法均可以得到较好的跟踪结果, 与其它算法相比, 卷积神经网络提供平移不变性的特点, 因此基于卷积神经网络的算法对目标快速移动等问题具有鲁棒性。</p>
                </div>
                <div class="area_img" id="115">
                    <p class="img_tit"><b>表1 平均中心位置误差、平均重叠率和成功率比较</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="115" border="1"><tr><td><br />视频</td><td>Ours</td><td>IVT</td><td>MIL</td><td>CT</td><td>TLD</td><td>STRUCK</td><td>ASLA</td><td>FRDIF</td><td>MVMKF</td></tr><tr><td><br />Sequence 1</td><td>3<br /> (82%, 95%) </td><td>18<br /> (39%, 37%) </td><td>4<br /> (<u>75%, 82%</u>) </td><td>17<br /> (43%, 32%) </td><td>25<br /> (34%, 25%) </td><td>33<br /> (48%, 48%) </td><td>86<br /> (11%, 13%) </td><td>50<br /> (10%, 5%) </td><td>7<br /> (73%, 78%) </td></tr><tr><td><br />Sequence 2</td><td>4<br /> (94%, 89%) </td><td>97<br /> (21%, 16%) </td><td>34<br /> (11%, 21%) </td><td>23<br /> (31%, 29%) </td><td>38<br /> (23%, 18%) </td><td>9<br /> (65%, 69%) </td><td>26<br /> (17%, 25%) </td><td>45<br /> (9%, 17%) </td><td>4<br /> (<u>85%, 78%</u>) </td></tr><tr><td><br />Sequence 3</td><td>2<br /> (87%, 97%) </td><td>6<br /> (<u>81%, 92%</u>) </td><td>30<br /> (40%, 30%) </td><td>26<br /> (17%, 27%) </td><td>36<br /> (55%, 16%) </td><td>157<br /> (14%, 10%) </td><td>22<br /> (79%, 5%) </td><td>134<br /> (13%, 3%) </td><td>4<br /> (80%, 89%) </td></tr><tr><td><br />Sequence 4</td><td>12<br /> (75%, 90%) </td><td>73<br /> (23%, 22%) </td><td>38<br /> (43%, 65%) </td><td>52<br /> (37%, 41%) </td><td>87<br /> (36%, 31%) </td><td>20<br /> (<u>68%</u>, 72%) </td><td>91<br /> (24%, 27%) </td><td>105<br /> (11%, 5%) </td><td>14<br /> (67%, 72%) </td></tr><tr><td><br />Labman</td><td>3<br /> (99%, 98%) </td><td>4<br /> (<u>98%</u>, 92%) </td><td>23<br /> (29%, 93%) </td><td>7<br /> (70%, 91%) </td><td>5<br /> (<u>98%</u>, 91%) </td><td>14<br /> (68%, 91%) </td><td>9<br /> (90%, 96%) </td><td>23<br /> (89%, 92%) </td><td>7<br /> (97%, 90%) </td></tr><tr><td><br />Intersection</td><td>2<br /> (99%, 92%) </td><td>95<br /> (22%, 47%) </td><td>38<br /> (82%, 55%) </td><td>9<br /> (87%, 61%) </td><td>25<br /> (49%, 58%) </td><td>4<br /> (99%, <u>90%</u>) </td><td>22<br /> (86%, 65%) </td><td>57<br /> (19%, 15%) </td><td>6<br /> (92%, 92%) </td></tr><tr><td><br />平均值</td><td>4.3<br />89.3%<br />93.5%</td><td>48.8<br />47.1%<br />51.0%</td><td>27.8<br />46.7%<br />57.7%</td><td>22.3<br />47.3%<br />46.8%</td><td>36.0<br />49.1%<br />39.8%</td><td>39.5<br />60.3%<br />63.3%</td><td>76.8<br />39.5%<br />38.5%</td><td>69.0<br />25.1%<br />22.8%</td><td>7.0<br />82.3%<br />83.2%</td></tr><tr><td><br />速度 (fps) </td><td>7</td><td>6</td><td>17</td><td>149</td><td>18</td><td>17</td><td>2</td><td>1</td><td>5</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note">*<b>加粗字体</b>表明算法在序列里面效果最好, <u>下划线</u>字体表明算法在序列里效果其次。三个数值按顺序分别表示平均中心位置误差、平均重叠率和成功率。</p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="116">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJZ201904047_116.jpg&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!">
                                    <img alt="图3 Sequence 1视频的跟踪结果 (a, b, c) Sequence 2视频的跟踪结果 (d, e, f)" src="Detail/GetImg?filename=images/JSJZ201904047_116.jpg&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!" />
                                </a>
                                <p class="img_tit"><b>图3 Sequence 1视频的跟踪结果 (a, b, c) Sequence 2视频的跟踪结果 (d, e, f</b>)   <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJZ201904047_116.jpg&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="117">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJZ201904047_117.jpg&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!">
                                    <img alt="图4 Sequence 3视频的跟踪结果 (a, b, c) Sequence 4视频的跟踪结果 (d, e, f)" src="Detail/GetImg?filename=images/JSJZ201904047_117.jpg&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!" />
                                </a>
                                <p class="img_tit"><b>图4 Sequence 3视频的跟踪结果 (a, b, c) Sequence 4视频的跟踪结果 (d, e, f</b>)   <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJZ201904047_117.jpg&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="118">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJZ201904047_118.jpg&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!">
                                    <img alt="图5 Labman视频的跟踪结果 (a, b, c) Intersection视频的跟踪结果 (d, e, f)" src="Detail/GetImg?filename=images/JSJZ201904047_118.jpg&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!" />
                                </a>
                                <p class="img_tit"><b>图5 Labman视频的跟踪结果 (a, b, c) Intersection视频的跟踪结果 (d, e, f</b>)   <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJZ201904047_118.jpg&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h3 id="119" name="119" class="anchor-tag"><b>4 总结</b></h3>
                <div class="p1">
                    <p id="120">本文针对目标跟踪, 采用自行设计的融合算法、相关模型以及卷积神经网络解决实际场景中出现的遮挡、复杂运动等问题。本文创新点如下:①本文提出了一种无需大量离线数据的, 具有简单有效的无训练的在线卷积神经网络模型, 并使用该模型进行鲁棒性的目标跟踪。②算法基于已标记的第一帧图像, 从目标区域提取一组标准化的局部图像作为目标卷积滤波器, 而这些滤波器实际上是一系列围绕目标的自适应上下文特征, 这些特征有利于在后续帧中定义和筛选对应目标的特征映射模型。③算法根据可见光和红外图像, 利用灰度化的方法融合可见光的三个通道后, 将其与红外图像进行堆叠, 因此无需特殊的融合算法, 使得算法有效高速运行。④算法利用相对跟踪的概念, 从可见光和红外图像中分别获取一系列相对跟踪候选窗得到了一系列具有特殊的权重的相关卷积滤波器, 与传统的跟踪算法使用背景目标两类分类相比, 这类滤波器对于目标的相关信息进行了补充。<b>5</b>.经过实验证明, 算法在测试视频中有效提升了鲁棒性和准确性。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
                        <h3 class="anchor-tag">作者图片</h3>
                <div class="anchor-wrap">
                        <p>
                                    <div class="anchor-box">
                                        <span class="anchor-a"><image id="136" type="formula" href="images/JSJZ201904047_13600.jpg" display="inline" placement="inline"><alt></alt></image></span>
                                        <span class="anchor-a">徐宁文</span>
                                    </div>
                        </p>
                </div>


        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Human tracking using convolutional neural networks">

                                <b>[1]</b> J Fan, W Xu, Y Wu, and Y Gong.Human tracking using convolutional neural networks[J].IEEE Transactions on Neural Networks, 2010, 21 (10) :1610-1623.
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep Track:learning discriminative feature representations online for robust visual tracking">

                                <b>[2]</b> Li Hanxi, Yi Li, and Fatih Porikli.Deeptrack:Learning discriminative feature representations online for robust visual tracking[J].IEEE Transactions on Image Processing, 2016, 25 (4) :1834-1848.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning a Deep Compact Image Representation forVisual Tracking">

                                <b>[3]</b> Wang Naiyan, and Dit-Yan Yeung.Learning a deep compact image representation for visual tracking[J].In Advances in neural information processing systems, 2013:809-817.
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Robust visual tracking via convolutional networks without training">

                                <b>[4]</b> K Zhang, Q Liu, Y Wu, and M H Yang.Robust visual tracking via convolutional networks without training[J].IEEE Transactions on Image Processing, 2016, 25, (4) :1779-1792.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501083985&amp;v=MTkzNDhyUmRHZXJxUVRNbndaZVp0RmlubFVyZklKRnNSYmhZPU5pZk9mYks3SHRETnFvOUVaT01NQlhROG9CTVQ2VDRQUUgvaQ==&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!">

                                <b>[5]</b> J W Davis, and V Sharma.Background-subtraction using contour-based fusion of thermal and visible imagery[J].Computer Vision and Image Understanding, 2007, 106 (2) :162-182.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Comparison of Fusion Methods for Thermo-Visual Surveillance Tracking">

                                <b>[6]</b> C O'Conaire, N E O'Connor, E Cooke, and A F Smeaton.Comparison of fusion methods for thermo-visual surveillance tracking[C].Information Fusion, 2006 9th International Conference on.IEEE.2006.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002831201&amp;v=MDAzMTVOSkZvPU5qN0Jhck80SHRIT3A0eEVadXNPWTNrNXpCZGg0ajk5U1hxUnJ4b3hjTUg3UjdxZForWnVGaURsVjdy&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!">

                                <b>[7]</b> D A Ross, J Lim, R S Lin, and M H Yang.Incremental learning for robust visual tracking[J].International Journal of Computer Vision, 2008, 77 (1) :125-141.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Robust Object Tracking with Online Multiple Instance Learning">

                                <b>[8]</b> B Babenko, M H Yang, and S Belongie.Robust object tracking with online multiple instance learning[J].IEEE transactions on pattern analysis and machine intelligence, 2011, 33 (8) :1619-1632.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Tracking-Learning-Detection">

                                <b>[9]</b> Z Kalal, K Mikolajczyk, and J Matas.Tracking-learning-detection[J].IEEE transactions on pattern analysis and machine intelligence, 2012, 34 (7) :1409-1422.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fast compressive tracking">

                                <b>[10]</b> K Zhang, L Zhang, and M H Yang.Fast compressive tracking[J].IEEE transactions on pattern analysis and machine intelligence, 2014, 36 (10) :2002-2015.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Struck:Structured Output Tracking with Kernels">

                                <b>[11]</b> S Hare, S Golodetz, A Saffari, V Vineet, M M Cheng, S L Hicks, and P H Torr.Struck:Structured output tracking with kernels[J].IEEE transactions on pattern analysis and machine intelligence, 2016, 38 (10) :2096-2109.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Visual tracking via adaptive structural local sparse appearance model">

                                <b>[12]</b> X Jia, H Lu, and M H Yang.Visual tracking via adaptive structural local sparse appearance model[C].In Computer vision and pattern recognition (CVPR) , 2012 IEEE Conference on (pp.1822-1829) .IEEE, 2012.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_13" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD1A35285D6EF41A246A55332A2644B312&amp;v=MDI4NDYyNnhLdz1OajdCYXJMSkhkVE9wNG94WXA1NUNIMUl6UklWbXpwNFMzemczUkF6ZmJibVJydWRDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZCckxVMDV0Qmh3TA==&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!">

                                <b>[13]</b> G Xiao, X Yun, and J Wu.A multi-cue mean-shift target tracking approach based on fuzzified region dynamic image fusion[J].Science China Information Sciences, 2012, 55 (3) :577-589.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Visible and infrared tracking based on multi-view multi-kernel fusion model">

                                <b>[14]</b> X Yun, Z Jing, and B Jin.Visible and infrared tracking based on multi-view multi-kernel fusion model[J].Optical Review, 2016, 23 (2) :244-25.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJZ201904047" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJZ201904047&amp;v=MDUxMzJPM3pxcUJ0R0ZyQ1VSN3FmWnVabUZ5emdWcnZNTHo3QmRMRzRIOWpNcTQ5Qlk0UUtESDg0dlI0VDZqNTQ=&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=842_VlRz_g1CMCLi-M0twVw-4EOFG_noWyi_OsMnrAg1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
