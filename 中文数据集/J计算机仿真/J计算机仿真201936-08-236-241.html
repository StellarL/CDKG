<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=NOOK-mvTMnxl11s6oPYV0boPXN3iP4N7Rc-A56nk4KI1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2fKXReader%2fDetail%3fTIMESTAMP%3d637140154299193750%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJZ201908049%26RESULT%3d1%26SIGN%3dEK3Az8DBbokLEnF581ALbxYtgOE%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJZ201908049&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJZ201908049&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJZ201908049&amp;v=MTk5MTdiN0FMejdCZExHNEg5ak1wNDlCYllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpvRnk3blY=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#13" data-title="&lt;b&gt;1 引言&lt;/b&gt; "><b>1 引言</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#18" data-title="&lt;b&gt;2 算法框架&lt;/b&gt; "><b>2 算法框架</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#19" data-title="&lt;b&gt;2.1 前端获取物体蒙板 (mask) 的方法&lt;/b&gt;"><b>2.1 前端获取物体蒙板 (mask) 的方法</b></a></li>
                                                <li><a href="#24" data-title="&lt;b&gt;2.2 物体位姿坐标系及其表示方法&lt;/b&gt;"><b>2.2 物体位姿坐标系及其表示方法</b></a></li>
                                                <li><a href="#31" data-title="&lt;b&gt;2.3 对Inception-ResNet V2 的改进&lt;/b&gt;"><b>2.3 对Inception-ResNet V2 的改进</b></a></li>
                                                <li><a href="#39" data-title="&lt;b&gt;2.3 Loss函数设计&lt;/b&gt;"><b>2.3 Loss函数设计</b></a></li>
                                                <li><a href="#44" data-title="&lt;b&gt;2.4 准确率表述函数设计&lt;/b&gt;"><b>2.4 准确率表述函数设计</b></a></li>
                                                <li><a href="#47" data-title="&lt;b&gt;2.5 整个网络优化器的选择&lt;/b&gt;"><b>2.5 整个网络优化器的选择</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#49" data-title="&lt;b&gt;3 数据集的制作&lt;/b&gt; "><b>3 数据集的制作</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#54" data-title="&lt;b&gt;4 实验与分析&lt;/b&gt; "><b>4 实验与分析</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#60" data-title="&lt;b&gt;5 总结&lt;/b&gt; "><b>5 总结</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#21" data-title="&lt;b&gt;图1 mask R-CNN图像分割效果图&lt;/b&gt;"><b>图1 mask R-CNN图像分割效果图</b></a></li>
                                                <li><a href="#23" data-title="&lt;b&gt;图2 算法框架&lt;/b&gt;"><b>图2 算法框架</b></a></li>
                                                <li><a href="#26" data-title="&lt;b&gt;图3 相机针孔模型&lt;/b&gt;"><b>图3 相机针孔模型</b></a></li>
                                                <li><a href="#33" data-title="&lt;b&gt;表1 ILSVRC2012 数据集测试结果&lt;/b&gt;"><b>表1 ILSVRC2012 数据集测试结果</b></a></li>
                                                <li><a href="#35" data-title="&lt;b&gt;图4 Inception-ResNet V2网络框架结构&lt;/b&gt;"><b>图4 Inception-ResNet V2网络框架结构</b></a></li>
                                                <li><a href="#37" data-title="&lt;b&gt;图5 经改进的Inception-ResNet V2&lt;/b&gt;"><b>图5 经改进的Inception-ResNet V2</b></a></li>
                                                <li><a href="#56" data-title="&lt;b&gt;图6 数据集生成流程图、数据以及mask图像&lt;/b&gt;"><b>图6 数据集生成流程图、数据以及mask图像</b></a></li>
                                                <li><a href="#57" data-title="&lt;b&gt;图7 mask与输出数据渲染后的对比图&lt;/b&gt;"><b>图7 mask与输出数据渲染后的对比图</b></a></li>
                                                <li><a href="#92" data-title="图8 旋转与位置误差">图8 旋转与位置误差</a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="131">


                                    <a id="bibliography_1" title="王鹏, 孙长库, 张子淼.单目视觉位姿测量的线性求解[J].仪器仪表学报, 2011, 32 (5) :1126-1131." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=YQXB201105028&amp;v=MTY5NTMzenFxQnRHRnJDVVI3cWZadVpvRnk3blZiL0pQRHpUYkxHNEg5RE1xbzlIYklRS0RIODR2UjRUNmo1NE8=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                        王鹏, 孙长库, 张子淼.单目视觉位姿测量的线性求解[J].仪器仪表学报, 2011, 32 (5) :1126-1131.
                                    </a>
                                </li>
                                <li id="133">


                                    <a id="bibliography_2" title="刘巍, 陈玲, 马鑫, 等.基于彩色图像的高速目标单目位姿测量方法[J].仪器仪表学报, 2016, 37 (3) :675-682." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=YQXB201603026&amp;v=MjY0NDdmWnVab0Z5N25WYi9KUER6VGJMRzRIOWZNckk5SFlvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3E=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                        刘巍, 陈玲, 马鑫, 等.基于彩色图像的高速目标单目位姿测量方法[J].仪器仪表学报, 2016, 37 (3) :675-682.
                                    </a>
                                </li>
                                <li id="135">


                                    <a id="bibliography_3" title="A Kendall, M Grimes, R Cipolla.Pose Net:A Convolutional Network for Real-Time 6-DOF Camera Relocalization[J].Education for Information, 2015, 31:2938-2946." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=PoseNet:A Convolutional Network for Real-Time 6-DOF Camera Relocalization">
                                        <b>[3]</b>
                                        A Kendall, M Grimes, R Cipolla.Pose Net:A Convolutional Network for Real-Time 6-DOF Camera Relocalization[J].Education for Information, 2015, 31:2938-2946.
                                    </a>
                                </li>
                                <li id="137">


                                    <a id="bibliography_4" title="W Kehl.SSD-6D:Making RGB-Based 3D Detection and 6DPose Estimation Great Again[C].IEEE International Conference on Computer Vision, 2017:1530-1538" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=SSD-6D:Making RGB-Based 3D Detection and 6DPose Estimation Great Again">
                                        <b>[4]</b>
                                        W Kehl.SSD-6D:Making RGB-Based 3D Detection and 6DPose Estimation Great Again[C].IEEE International Conference on Computer Vision, 2017:1530-1538
                                    </a>
                                </li>
                                <li id="139">


                                    <a id="bibliography_5" title="M Rad, V Lepetit.BB8:A Scalable, Accurate, Robust to Partial Occlusion Method for Predicting the 3D Poses of Challenging Objects without Using Depth[C].IEEE International Conference on Computer Vision.IEEE Computer Society, 2017:3848-3856." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=BB8:A Scalable,Accurate,Robust to Partial Occlusion Method for Predicting the 3D Poses of Challenging Objects without Using Depth">
                                        <b>[5]</b>
                                        M Rad, V Lepetit.BB8:A Scalable, Accurate, Robust to Partial Occlusion Method for Predicting the 3D Poses of Challenging Objects without Using Depth[C].IEEE International Conference on Computer Vision.IEEE Computer Society, 2017:3848-3856.
                                    </a>
                                </li>
                                <li id="141">


                                    <a id="bibliography_6" title="T T Do, Cai M, Pham T, et al.Deep-6DPose:Recovering 6DObject Pose from a Single RGB Image[D].Cornell University, 2018." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep-6DPose:Recovering 6DObject Pose from a Single RGB Image">
                                        <b>[6]</b>
                                        T T Do, Cai M, Pham T, et al.Deep-6DPose:Recovering 6DObject Pose from a Single RGB Image[D].Cornell University, 2018.
                                    </a>
                                </li>
                                <li id="143">


                                    <a id="bibliography_7" title="K He, et al.Mask R-CNN[J].IEEE Transactions on Pattern A-nalysis&amp;amp;Machine Intelligence, 2017, (99) :1-1." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Mask R-CNN">
                                        <b>[7]</b>
                                        K He, et al.Mask R-CNN[J].IEEE Transactions on Pattern A-nalysis&amp;amp;Machine Intelligence, 2017, (99) :1-1.
                                    </a>
                                </li>
                                <li id="145">


                                    <a id="bibliography_8" title="C Szegedy, et al.Inception-v4, Inception-Res Net and the Impact of Residual Connections on Learning[J].ar Xiv:1602.07261[cs.CV], 2016." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Inception-v4,Inception-Res Net and the Impact of Residual Connections on Learning">
                                        <b>[8]</b>
                                        C Szegedy, et al.Inception-v4, Inception-Res Net and the Impact of Residual Connections on Learning[J].ar Xiv:1602.07261[cs.CV], 2016.
                                    </a>
                                </li>
                                <li id="147">


                                    <a id="bibliography_9" title="E Rublee, et al.ORB:An efficient alternative to SIFT or SURF[C].International Conference on Computer Vision.IEEE, 2012:2564-2571." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=ORB:An efficient alternative to SIFT or SURF">
                                        <b>[9]</b>
                                        E Rublee, et al.ORB:An efficient alternative to SIFT or SURF[C].International Conference on Computer Vision.IEEE, 2012:2564-2571.
                                    </a>
                                </li>
                                <li id="149">


                                    <a id="bibliography_10" title="H Bay, T Tuytelaars, L V Gool.SURF:speeded up robust features[C].European Conference on Computer Vision.SpringerVerlag, 2006:404-417." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=SURF:Speeded Up Robust Fea-tures">
                                        <b>[10]</b>
                                        H Bay, T Tuytelaars, L V Gool.SURF:speeded up robust features[C].European Conference on Computer Vision.SpringerVerlag, 2006:404-417.
                                    </a>
                                </li>
                                <li id="151">


                                    <a id="bibliography_11" title="J Wu, et al.Real-Time Object Pose Estimation with Pose Interpreter Networks[J].IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2018) .Code available at this https URL, 2018." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Real-Time Object Pose Estimation with Pose Interpreter Networks">
                                        <b>[11]</b>
                                        J Wu, et al.Real-Time Object Pose Estimation with Pose Interpreter Networks[J].IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2018) .Code available at this https URL, 2018.
                                    </a>
                                </li>
                                <li id="153">


                                    <a id="bibliography_12" title="D Tran, P Toulis, Airoldi E M.Stochastic gradient descent methods for estimation with large data sets[J].Statistics, 2015." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Stochastic gradient descent methods for estimation with large data sets">
                                        <b>[12]</b>
                                        D Tran, P Toulis, Airoldi E M.Stochastic gradient descent methods for estimation with large data sets[J].Statistics, 2015.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJZ" target="_blank">计算机仿真</a>
                2019,36(08),236-241             </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>利用RGB图像和DNN进行物体6DOf位姿推算</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%B4%94%E6%AF%85%E5%8D%9A&amp;code=42669779&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">崔毅博</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%88%98%E9%B9%8F%E8%BF%9C&amp;code=38634022&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">刘鹏远</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BC%A0%E5%B3%BB%E5%AE%81&amp;code=38634023&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">张峻宁</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%AE%B8%E7%8A%B6%E7%94%B7&amp;code=42669780&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">许状男</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E9%99%86%E5%86%9B%E5%B7%A5%E7%A8%8B%E5%A4%A7%E5%AD%A6%E5%AF%BC%E5%BC%B9%E5%B7%A5%E7%A8%8B%E7%B3%BB&amp;code=1701801&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">陆军工程大学导弹工程系</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>提出一种基于RGB图片信息进行物体位姿推算的方法。方法利用二维图像分割得到的二值mask图像, 并在此基础上进行物体的位姿推算, 实现实时准确的物体6Dof位姿信息输出。目前传统上基于RGB-D图像推算物体的6Dof位姿精度较高, 但RGB-D双目摄像头往往功耗和体积较大且成本高, 而利用RGB图像来进行位姿推算无论在工业还是移动设备上更为实用。针对深度神经网络训练需要大量数据且数据集获取困难的问题, 利用blender软件实现了一套物体6Dof位姿训练数据集生成的方法。通用实验表明, 所提方法的推算的位姿精度相对较高, 位姿推算用时约为0.1s, 具有良好的准确性、实时性, 且有效解决了计算量问题。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%85%AD%E8%87%AA%E7%94%B1%E5%BA%A6%E4%BD%8D%E5%A7%BF%E6%8E%A8%E7%AE%97&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">六自由度位姿推算;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%B8%89%E9%80%9A%E9%81%93%E5%BD%A9%E8%89%B2%E5%9B%BE%E5%83%8F&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">三通道彩色图像;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%AE%9E%E6%97%B6%E4%BD%8D%E5%A7%BF%E6%8E%A8%E7%AE%97&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">实时位姿推算;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%89%A9%E4%BD%93%E5%85%AD%E8%87%AA%E7%94%B1%E5%BA%A6%E4%BD%8D%E5%A7%BF&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">物体六自由度位姿;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    崔毅博 (1990.3-) , 男 (汉族) , 黑龙江齐齐哈尔人, 硕士研究生, 主要研究方向为计算机视觉与深度学习。;
                                </span>
                                <span>
                                    刘鹏远 (1975.7-) , 男 (汉族) , 河南永城县人, 副教授, 硕士生导师, 主要研究方向为计算机视觉。;
                                </span>
                                <span>
                                    张峻宁 (1992.3-) , 男 (汉族) , 四川巴中人, 博士研究生, 主要研究方向为深度学习, SLAM技术, 计算机视觉与模式识别。;
                                </span>
                                <span>
                                    许状男 (1990.3-) , 男 (汉族) , 吉林双辽人, 硕士研究生, 主要研究方向为计算机视觉VR系统。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-11-27</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金 (51205405, 51305454);</span>
                    </p>
            </div>
                    <h1><b>The Estimation of Object Position and Orientation Using the RGB Images and the DNN</b></h1>
                    <h2>
                    <span>CUI YI-bo</span>
                    <span>LIU Peng-yuan</span>
                    <span>ZHANG Jun-ning</span>
                    <span>XU Zhuang-nan</span>
            </h2>
                    <h2>
                    <span>Department of Missile Engineering in Army Engineering University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>In the paper, we put forward a method to estimate the 6Dof pose of an object based on RGB image and Inception-ResNet V2. On the basis of image segmentation, we used the mask to estimate the pose of an object with an real-time output. The traditional algorithm is usually based on RGB-D images, but the RGB-D cameras are always big, expensive and use a lot of power. Ccompared with the RGB cameras, they are not fit for industrial situation and robot, so we used RGB image to estimate the pose. Training a DNN needs a large number of data, for the purpose of training a better Net, we found a way to produce the datasets. Expriments show that, our method has a lower position error and orientation error, the time of estimation is about 0.1 s. We have achieved better accuracy and real-time performance, and solved the problem of a larger amount of calculation.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=6Dof%20pose%20estimation&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">6Dof pose estimation;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Inception-ResNet%20V2&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Inception-ResNet V2;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=RGB%20pose%20estiamtion&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">RGB pose estiamtion;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Realtime%20pose%20estimation&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Realtime pose estimation;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=6Dof%20pose%20of%20object&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">6Dof pose of object;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-11-27</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="13" name="13" class="anchor-tag"><b>1 引言</b></h3>
                <div class="p1">
                    <p id="14">目前在一些高精度加工、无损作业等环境要求较高的场合, 基于计算机视觉的物体位姿识别因其快速高效得以广泛使用, 精确的物体位姿识别信息可以为机器人加工定位、导弹制导、无人机 (车) 导航等提供极大辅助。目前制造业有大量依赖计算机视觉的工业机器人, 这些机器人在工作时最常用的就是抓取物体、寻找焊点等工作, 而这些工作都需要获取目标物体准确的位姿信息。在装配等行业需要大量的装配部件定位, 而人工定位往往效率很低, 对基于计算机视觉的物体位姿识别技术的需求就很大。在无人车、无人机在利用视觉导航时, 也需要确定行人、物体的位姿信息, 此外应用到计算机视觉很多领域都要快速准确的物体位姿识别技术, AR、VR等前沿技术中物体位姿识别也是核心技术之一。</p>
                </div>
                <div class="p1">
                    <p id="15">虽然基于计算机视觉的物体位姿识别已成功应用于某些领域。但是该项技术的研究远未成熟, 面临着很多新的问题。例如, 在研究深度学习之前, 为了提高识别的准确率, 往往使用一些人工设计的算法, 这些算法往往针对某种特定类型的物体与环境, 而在普通野外环境下干扰较多, 识别精度就很差。人工设计的算法通常首先利用算法提取物体二维图像中的点、线、面等特征, 再基于这些特征之间的联系直接求解物体位姿或者利用优化的方法优化出物体位姿。</p>
                </div>
                <div class="p1">
                    <p id="16">目前国内在人工算法方面研究较多, 比较有代表性的是王鹏、孙长库等在<citation id="155" type="reference"><link href="131" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>中提出的在距离因子基础上的非线性位姿测量模型, 利用P4P求解, 该方法有效抑制了图像噪声干扰, 提高了位姿测量的精度。以及刘巍等在<citation id="156" type="reference"><link href="133" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>中对风洞运动目标位姿测量提出图像差叠加法和标记点位置估计的图像快速分割和目标定位方法, 并基于方向估计标记点实现了合作标记点的快速识别和提取, 最后利用单目视觉原理求解运动目标的位姿信息, 其精度在1m*1m视场范围内, 位移精度0.19mm, 角度精度达到0.18°。人工算法存在利用RGB图片进行无标记三维物体位姿识别在一般条件下准确度不高、抗遮挡能力差、计算量大耗费计算资源且往往效果越好的算法实时性越差等问题, 针对传统人工算法存在的问题, 目前利用深度神经网络来做位姿识别可以很好的克服以上问题。</p>
                </div>
                <div class="p1">
                    <p id="17">国外利用神经网络解决物体6Dof位姿识别问题中比较有代表性的方法是PoseNet<citation id="157" type="reference"><link href="135" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>、SSD-6D<citation id="158" type="reference"><link href="137" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>、BB8<citation id="159" type="reference"><link href="139" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>和Deep-6DPose<citation id="160" type="reference"><link href="141" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>, 以上方法中均使用RGB或RGB-D图片作为输入, 应用深度神经网络取代传统的算法进行物体位姿推算, 所得到的位姿推算数据无论在准确度还是实时性上都优于传统算法。本文就结合目前比较先进的深度神经网络模型, 提出了一种对一般物体无标记的情况下利用单目RGB信息来进行位姿测量的方法。</p>
                </div>
                <h3 id="18" name="18" class="anchor-tag"><b>2 算法框架</b></h3>
                <h4 class="anchor-tag" id="19" name="19"><b>2.1 前端获取物体蒙板 (mask) 的方法</b></h4>
                <div class="p1">
                    <p id="20">本文通过经过改进的Inception-ResNetV2深度学习框架实现利用RGB图像来进行物体6Dof位姿推算, 其算法主要包括两部分:首先原始RGB图像通过环境mask R-CNN模型分割得到物体的mask 图像, 由于目前mask R-CNN识别准确率已经非常高 (如图1) , 所以直接采用此网络结构进行图像分割, 并将得到mask图像作为本算法框架输入。</p>
                </div>
                <div class="area_img" id="21">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJZ201908049_021.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 mask R-CNN图像分割效果图" src="Detail/GetImg?filename=images/JSJZ201908049_021.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图1 mask R-CNN图像分割效果图</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJZ201908049_021.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="22">然后再将第一部分得到的目标物体的mask图像作为物体位姿网络模型的输入, 经过Inception-ResNetV2深度神经网络, 得到物体的位姿信息。图2中红色框内为图像分割部分, 本文将比较成熟且准确率较高的mask R-CNN<citation id="161" type="reference"><link href="143" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>模型作为本文框架的图像分割部分, 图2中蓝色框架内为本文重点研究内容, 即利用Inception-ResNetV2来进行物体位姿推算的算法框架。</p>
                </div>
                <div class="area_img" id="23">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJZ201908049_023.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 算法框架" src="Detail/GetImg?filename=images/JSJZ201908049_023.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图2 算法框架</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJZ201908049_023.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="24" name="24"><b>2.2 物体位姿坐标系及其表示方法</b></h4>
                <div class="p1">
                    <p id="25">一个物体的位姿是物体位置和姿态的统称。首先是位置信息的表示, 如果把相机看做针孔成像模型的话 (如图<b>3</b>所示) 。</p>
                </div>
                <div class="area_img" id="26">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJZ201908049_026.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 相机针孔模型" src="Detail/GetImg?filename=images/JSJZ201908049_026.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图3 相机针孔模型</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJZ201908049_026.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="27">相机焦距为<b><i>f</i></b>所在轴为<b><i>z</i></b>正方向, <b><i>x</i></b>与<b><i>y</i></b>轴处在光心<b><i>O</i></b>所在平面, 以光心<b><i>O</i></b>为相机坐标系原点, 在此相机坐标系下则物体中心所在位置可以用<b><i>W</i>= (<i>W</i></b><sub><b><i>x</i></b></sub>, <b><i>W</i></b><sub><b><i>y</i></b></sub>, <b><i>W</i></b><sub><b><i>z</i></b></sub>) 来表示, 其中规定物体中心就是物体<b>3<i>D</i></b>模型中心所在位置, 以便和生成的数据集数据一致。若<b><i>P</i>=[<i>u</i>, <i>v</i></b>]为物体对应像素在图像上的坐标, <b><i>K</i></b>为相机内参矩阵, 则可以得到此等式</p>
                </div>
                <div class="p1">
                    <p id="28" class="code-formula">
                        <mathml id="28"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>W</mi><msub><mrow></mrow><mi>z</mi></msub><mrow><mo> (</mo><mtable columnalign="left"><mtr><mtd><mi>u</mi></mtd></mtr><mtr><mtd><mi>v</mi></mtd></mtr><mtr><mtd><mn>1</mn></mtd></mtr></mtable><mo>) </mo></mrow><mo>≜</mo><mi>Κ</mi><mi>W</mi><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="29">此等式就是实际物体中心所在位置 (<b><i>W</i></b>) 在相机坐标系下经过相机 (<b><i>K</i></b>) 后投影到图像后的二维坐标位置 (<b><i>P</i></b>) , 所以在本文所有位置均为<b><i>W</i></b>所表示位置, 在实际应用时, 由于输入是单目<b>RGB</b>图片且通过物体的<b>mask</b>来判断物体的位姿, 所得位姿为相对数据, 即真实物体若是等比例增大, 则其位姿所在世界坐标则应等比例增大, 所以后文所得位置都是相对位置, 即<b>W</b>单位并非是米或毫米, 而是相对物体尺度而言的距离。</p>
                </div>
                <div class="p1">
                    <p id="30">物体的旋转信息可以用许多种方法表示, 比较普遍的表示方法是用欧拉角和四元数来表示, 而由于用欧拉角来表示物体旋转信息时, 在某一轴的正半轴旋转一定的正角度与在负半轴旋转同样大小的负角度所代表的旋转是一样的, 这样的话在生成数据集时信息就会有二义性, 可能导致训练结果不准确, 所以用四元数<b><i>Q</i></b>来表示物体的旋转, 在本文中<b><i>Q</i></b>所代表的是物体围绕其中心<b><i>T</i></b>旋转的角度, 即物体坐标系下所旋转的角度, 若四元数为<b><i>Q</i>= (<i>a</i>, <i>b</i>, <i>c</i>, <i>d</i></b>) , 则四元数应满足约束:<b><i>a</i></b><sup><b>2</b></sup>+<b><i>b</i></b><sup><b>2</b></sup>+<b><i>c</i></b><sup><b>2</b></sup>+<b><i>d</i></b><sup><b>2</b></sup>=<b>1</b>。</p>
                </div>
                <h4 class="anchor-tag" id="31" name="31"><b>2.3 对Inception-ResNet V2 的改进</b></h4>
                <div class="p1">
                    <p id="32">Inception-ResNet V2是google团队提出的一种卷积神经网络架构, 其是从Inception V3模型结合Resnet (残差网络) 发展而来, 在文献<citation id="162" type="reference">[<a class="sup">8</a>]</citation>2016年ILSVRC图像分类基准测试中实现了最好成绩, 相比于其它网络结构Inciption-Resnet V2误差率更低 (图3) , 综合后文实验验证所得数据, 本文选用此结构作为输入mask的特征提取网络结构。</p>
                </div>
                <div class="area_img" id="33">
                    <p class="img_tit"><b>表1 ILSVRC2012 数据集测试结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="33" border="1"><tr><td><br />Network</td><td>Top-1 Error</td><td>Top-5 Error</td></tr><tr><td><br />BN-Inception<br />Inception-v3<br />Inception-ResNet-v1<br />Inception-v4<br />Inception-Resnet-v2</td><td>25.2%<br />21.2%<br />21.3%<br />20.0%<br />19.9</td><td>7.8%<br />5.6%<br />5.5%<br />5.0%<br />4.9%</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="34">其原始结构请参考文献<citation id="163" type="reference">[<a class="sup">8</a>]</citation>以及图4, 其中每一部分都是由若干2D卷积层、池化层以及全连接层组成。其中卷积层的作用是提取图像的特征以及升维, 由原来的单通道图像, 经过卷积层, 变为多维特征信息, 为下一步位姿识别提供基本的特征信息, 池化层用来进一步提取图片特征并对数据进行降维, 在深度神经网络中通常会混合使用这两个层来组成不同的结构, 对目标图像进行处理。在传统算法中这一步往往利用算法提取稀疏特征点 (SIFT<citation id="164" type="reference">[<a class="sup">9</a>]</citation>、SURF<citation id="165" type="reference">[<a class="sup">10</a>]</citation>等) 进行匹配, 再利用对极几何等构造等式利用数学方法解出物体位姿, 大量的计算资源都浪费在了计算特征点以及匹配上, 而利用深度神经网络模型, 对物体的特征提取的计算速度远远高于传统方法, 所以在实时性能上占有优势。</p>
                </div>
                <div class="area_img" id="35">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJZ201908049_035.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 Inception-ResNet V2网络框架结构" src="Detail/GetImg?filename=images/JSJZ201908049_035.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图4 Inception-ResNet V2网络框架结构</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJZ201908049_035.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="36">由于Inception-ResNet V2 原始框架是用来进行图像分类的, 不能直接用于推算物体位姿, 所以需要对其原始结构加以改进, 用来进行物体的位姿推算, 在本文实验中使用的经改进后的结构如图5所示, 图5中红色框内为改进部分。其输入为3通道图片, 而实际输入可能是单通道的mask也可能是3通道的mask遮罩下生成的RGB图像, 所以在输入层设置为1通道和3通道可自动检测, 方便训练时输入数据, 提高了对图像分割部输出的兼容性。</p>
                </div>
                <div class="area_img" id="37">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJZ201908049_037.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 经改进的Inception-ResNet V2" src="Detail/GetImg?filename=images/JSJZ201908049_037.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图5 经改进的Inception-ResNet V2</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJZ201908049_037.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="38">在Inception-ResNet V2中, 整体结构的后三个框架分别是池化层、Dropout层以及softmax层, 在原有的模型上Inception-ResNet-C层之后直接连接了池化层以及dropout层, 池化层作用如上文所述, dropout层用来防止模型过拟合, 最后用softmax层的softmax函数输出图片种类信息。通过图片需要得到的是物体的位姿信息, 而经过池化层之后的空间特征信息量会变少, 所以选择直接利用Inception-ResNet-C层输出的信息, 通过一个共享层, 将特征信息分别输入<i>T</i>和<i>Q</i>网络, 其中<i>T</i>和<i>Q</i>分别是2个全连接层, 使得高维信息经两次降维连接后输出所需的<i>T</i>和<i>Q</i>信息。利用此<i>T</i>、<i>Q</i>信息与真实信息进行比较得到<i>Loss</i>, 利用此<i>Loss</i>反向传播来训练网络。</p>
                </div>
                <h4 class="anchor-tag" id="39" name="39"><b>2.3 Loss函数设计</b></h4>
                <div class="p1">
                    <p id="40">Loss函数是整个网络优化的主要函数, 如果输出为<i>T</i> ′、<i>Q</i> ′, 真实数据为<i>T</i>、<i>Q</i>, 则整个网络的优化函数</p>
                </div>
                <div class="p1">
                    <p id="41" class="code-formula">
                        <mathml id="41"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><mi>o</mi><mi>s</mi><mi>s</mi><mo>=</mo><mo stretchy="false">∥</mo><mi>Τ</mi><mo>-</mo><msup><mi>Τ</mi><mo>′</mo></msup><mo stretchy="false">∥</mo><msub><mrow></mrow><mn>1</mn></msub><mo>+</mo><mi>α</mi><mo stretchy="false">|</mo><mn>1</mn><mo>-</mo><mi>Q</mi><mo>⋅</mo><msup><mi>Q</mi><mo>′</mo></msup><mo stretchy="false">|</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="42">其中‖<i>T</i>-<i>T</i>′‖<sub><b>1</b></sub>是输出<i>T</i> ′与真实数据<i>T</i>之间的<i>L</i><b>1</b>范数, <mathml id="43"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>|</mo><mrow><mn>1</mn><mo>-</mo><mi>Q</mi><mo>⋅</mo><msup><mi>Q</mi><mo>′</mo></msup></mrow><mo>|</mo></mrow></mrow></math></mathml>为角度误差, 若<i>Q</i>与<i>Q</i> ′相同, 则两个四元数由于约束关系, 相乘结果应该等于<b>1</b>, 不等于<b>1</b>说明有误差, 等式中的<i>α</i>可以调节位置误差和角度误差对<i>Loss</i>的影响大小, 在实际训练时, 位置误差误差往往在<b>1</b>左右不再下降, 可以把角度误差的权重相对设大, 在实际训练时, 由于位置和角度误差下降到一定程度后就不再下降, 保持收敛, 所以<i>α</i>值设置可以影响网络整体训练速度, 但不影响最后网络输出的准确率, 所以可以设置为<b>3</b>。</p>
                </div>
                <h4 class="anchor-tag" id="44" name="44"><b>2.4 准确率表述函数设计</b></h4>
                <div class="p1">
                    <p id="45">位置偏差为PositonError=‖<i>T</i>-<i>T</i>′‖<sub><b>2</b></sub>, 即可以认为是<i>T</i>与<i>T</i>′的二范数, 旋转误差由于是四元数, 不便于直接表示为范数的形式, 因此设计函数如下:</p>
                </div>
                <div class="p1">
                    <p id="46">假设<i>Q</i>= ([<i>a</i><b>1</b>, <i>b</i><b>1</b>, <i>c</i><b>1</b>, <i>d</i><b>1</b>]) , <i>Q</i>′= ([<i>a</i><b>2</b>, <i>b</i><b>2</b>, <i>c</i><b>2</b>, <i>d</i><b>2</b>]) (在实际程序中为张量, 所以用 ([]) 来表示, 在此处运算可以按照向量来计算) , 则其OrientationError=<b>180/<i>π</i>* (2</b>* (<i>a</i><b>1</b>*<i>a</i><b>2</b>+<i>b</i><b>1</b>*<i>b</i><b>2</b>+<i>c</i><b>1</b>*<i>c</i><b>2</b>+<i>d</i><b>1</b>*<i>d</i><b>2</b>) <sup><b>2</b></sup>-<b>1</b>) 。若是两个四元数相等则其对应位置各元素的相乘后和等于<b>1</b>, 若是不相等则其值为 (<b>0, 1</b>) 区间内, 经此函数最终旋转误差值在[<b>0, 180</b>]范围内变动, 此函数设计参考了文献<citation id="166" type="reference">[<a class="sup">11</a>]</citation>中所用评价函数。</p>
                </div>
                <h4 class="anchor-tag" id="47" name="47"><b>2.5 整个网络优化器的选择</b></h4>
                <div class="p1">
                    <p id="48">整个网络的优化器选择为随机最速下降法 (SGD<citation id="167" type="reference">[<a class="sup">12</a>]</citation>) , 这种优化方式出自最速下降法 (GD) , 最速下降法就是沿着模型最大梯度下降, 但是一旦遇到局部极小值的“陷阱”, 走出来就很慢。但SGD算法克服了最速下降法中局部最小值问题, 使得整个网络可以随机的沿着最大梯度方向下降, 使用此方法可以有效提高整体网络的训练速度, 同时增强了网络对噪音的鲁棒性, 使得最终训练后的网络对物体物体的位姿识别更加准确。</p>
                </div>
                <h3 id="49" name="49" class="anchor-tag"><b>3 数据集的制作</b></h3>
                <div class="p1">
                    <p id="50">训练一个性能良好的神经网络首先需要制作一个精度较高的数据集, 而普通的真实场景下带有mask的位姿数据集制作成本较高, 需要专业位姿采集设备, 为了方便训练且仅训练位姿识别网络, 可以利用软件来生成相应的数据集来训练神经网络, 训练本网络的数据集采用blender软件渲染生成, blender是一个轻量级的开源三维动画制作软件, 其渲染功能满足实验要求。数据集制作的具体流程如下:</p>
                </div>
                <div class="p1">
                    <p id="51">Step<b>1</b>:用python在指定范围内随机生成满足四元数约束条件的位置以及以旋转数据, 以便下一步用blender渲染图像时读取;</p>
                </div>
                <div class="p1">
                    <p id="52">Step<b>2</b>:利用blender渲染时, 首先设置相机坐标系, 把相机设置位置为 (<b>0, 0, 0</b>) , 使得相机位置与blender的世界坐标系原点相同, 而后读取上一步生成的一系列位置与四元数数据data= (T, Q) ;</p>
                </div>
                <div class="p1">
                    <p id="53">Step<b>3</b>:根据此数据渲染模型在不同位置的mask图像。由于距离较远的图像渲染后像素较少, 特征不明显, 所以仅渲染一定范围内的图像, 其范围可用上一步生成的数据来控制, 根据物体的大小, 一般物体建议距离可以设置为<b>5-10</b>m以内, 较大物体距离可以根据比例适当设置远一点, 图<b>6</b>为流程图及其生成的位姿数据与渲染后得到的mask图像。</p>
                </div>
                <h3 id="54" name="54" class="anchor-tag"><b>4 实验与分析</b></h3>
                <div class="p1">
                    <p id="55">所有实验均是在ubuntu<b>16.04</b>系统下完成, 电脑配置如下:CPU (<b>2.0</b> GHZ Intel i<b>7</b>) , 运行内存<b>16</b>GB, 深度神经网络使用pytorch进行构建。实验数据分别来自于自制数据集, 实验主要做了三方面验证:①实验验证了利用深度神经网络来推算物体位姿的可行性;②对不同的网络结构估计的准确性与速度做了对比。③计算了整个神经网络推算所用时间, 验证了网络的实时性能。试验效果如图<b>7</b>所示 (包括远处近处以及遮挡等情况) 其中左上两个图是远处遮挡情况下位姿识别情况, 左下是无遮挡情况下远处物体的位姿识别情况, 右侧为近距离的遮挡以及无遮挡情况下的位姿识别情况。</p>
                </div>
                <div class="area_img" id="56">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJZ201908049_056.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 数据集生成流程图、数据以及mask图像" src="Detail/GetImg?filename=images/JSJZ201908049_056.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图6 数据集生成流程图、数据以及mask图像</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJZ201908049_056.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="57">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJZ201908049_057.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 mask与输出数据渲染后的对比图" src="Detail/GetImg?filename=images/JSJZ201908049_057.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图7 mask与输出数据渲染后的对比图</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJZ201908049_057.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="58">图<b>8</b>是旋转与位置误差随着训练轮数增加而逐渐收敛, 利用深度神经网络推算物体位姿由图中数据来看, 其位置误差基本差不多都在<b>1</b>上下浮动, 而其旋转误差差别较大, 在采用相同数据集训练同样的轮数时, Inception-ResNet V<b>2</b>的准确度是最高的, RES<b>18</b>虽然可以在准确度上接近, 但训练时间上较长, 而InceptionV<b>4</b>虽然收敛速度快, 但是其旋转误差较大, 在模型运行速度上以上三个模型速度基本相当均为<b>0.1</b>s以内, 可以达到实时输出的目的。</p>
                </div>
                <div class="p1">
                    <p id="59">综合以上数据分析, 采用Inception-ResNet V<b>2</b> 来进行物体位姿识别的效果最好, 位姿相对准确且速度较快, 可以实时输出。</p>
                </div>
                <h3 id="60" name="60" class="anchor-tag"><b>5 总结</b></h3>
                <div class="p1">
                    <p id="61">将<b>RGB</b>图片作为输入, 基于<b>Inception-ResNet V2</b>模型实现了基于物体<b>mask</b>图像的<b>6Dof</b>位姿推算;其次, 实现了利用.<b>stl</b>格式三维模型和<b>blender</b>生成数据集;最后利用自行开发的数据集训练并对比了<b>RES18, Inception v4</b>与<b>Inception-ResNet V2</b>三种不同的神经网络结构在推算位姿时的效果和模型的计算时间。实验结果得出了本文所提网络模型在计算时间相同的情况下位姿精度最高的结论。</p>
                </div>
                <div class="area_img" id="92">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJZ201908049_09200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 旋转与位置误差" src="Detail/GetImg?filename=images/JSJZ201908049_09200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图8 旋转与位置误差  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJZ201908049_09200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="93">
                                <img alt="" src="Detail/GetImg?filename=images/JSJZ201908049_09300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                            <p class="img_tit"></p>

                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="131">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=YQXB201105028&amp;v=MjE0ODFEelRiTEc0SDlETXFvOUhiSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1Wm9GeTduVmIvSlA=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b>王鹏, 孙长库, 张子淼.单目视觉位姿测量的线性求解[J].仪器仪表学报, 2011, 32 (5) :1126-1131.
                            </a>
                        </p>
                        <p id="133">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=YQXB201603026&amp;v=MDQ1MzdCdEdGckNVUjdxZlp1Wm9GeTduVmIvSlBEelRiTEc0SDlmTXJJOUhZb1FLREg4NHZSNFQ2ajU0TzN6cXE=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b>刘巍, 陈玲, 马鑫, 等.基于彩色图像的高速目标单目位姿测量方法[J].仪器仪表学报, 2016, 37 (3) :675-682.
                            </a>
                        </p>
                        <p id="135">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=PoseNet:A Convolutional Network for Real-Time 6-DOF Camera Relocalization">

                                <b>[3]</b>A Kendall, M Grimes, R Cipolla.Pose Net:A Convolutional Network for Real-Time 6-DOF Camera Relocalization[J].Education for Information, 2015, 31:2938-2946.
                            </a>
                        </p>
                        <p id="137">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=SSD-6D:Making RGB-Based 3D Detection and 6DPose Estimation Great Again">

                                <b>[4]</b>W Kehl.SSD-6D:Making RGB-Based 3D Detection and 6DPose Estimation Great Again[C].IEEE International Conference on Computer Vision, 2017:1530-1538
                            </a>
                        </p>
                        <p id="139">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=BB8:A Scalable,Accurate,Robust to Partial Occlusion Method for Predicting the 3D Poses of Challenging Objects without Using Depth">

                                <b>[5]</b>M Rad, V Lepetit.BB8:A Scalable, Accurate, Robust to Partial Occlusion Method for Predicting the 3D Poses of Challenging Objects without Using Depth[C].IEEE International Conference on Computer Vision.IEEE Computer Society, 2017:3848-3856.
                            </a>
                        </p>
                        <p id="141">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep-6DPose:Recovering 6DObject Pose from a Single RGB Image">

                                <b>[6]</b>T T Do, Cai M, Pham T, et al.Deep-6DPose:Recovering 6DObject Pose from a Single RGB Image[D].Cornell University, 2018.
                            </a>
                        </p>
                        <p id="143">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Mask R-CNN">

                                <b>[7]</b>K He, et al.Mask R-CNN[J].IEEE Transactions on Pattern A-nalysis&amp;Machine Intelligence, 2017, (99) :1-1.
                            </a>
                        </p>
                        <p id="145">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Inception-v4,Inception-Res Net and the Impact of Residual Connections on Learning">

                                <b>[8]</b>C Szegedy, et al.Inception-v4, Inception-Res Net and the Impact of Residual Connections on Learning[J].ar Xiv:1602.07261[cs.CV], 2016.
                            </a>
                        </p>
                        <p id="147">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=ORB:An efficient alternative to SIFT or SURF">

                                <b>[9]</b>E Rublee, et al.ORB:An efficient alternative to SIFT or SURF[C].International Conference on Computer Vision.IEEE, 2012:2564-2571.
                            </a>
                        </p>
                        <p id="149">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=SURF:Speeded Up Robust Fea-tures">

                                <b>[10]</b>H Bay, T Tuytelaars, L V Gool.SURF:speeded up robust features[C].European Conference on Computer Vision.SpringerVerlag, 2006:404-417.
                            </a>
                        </p>
                        <p id="151">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Real-Time Object Pose Estimation with Pose Interpreter Networks">

                                <b>[11]</b>J Wu, et al.Real-Time Object Pose Estimation with Pose Interpreter Networks[J].IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2018) .Code available at this https URL, 2018.
                            </a>
                        </p>
                        <p id="153">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Stochastic gradient descent methods for estimation with large data sets">

                                <b>[12]</b>D Tran, P Toulis, Airoldi E M.Stochastic gradient descent methods for estimation with large data sets[J].Statistics, 2015.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJZ201908049" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJZ201908049&amp;v=MTk5MTdiN0FMejdCZExHNEg5ak1wNDlCYllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpvRnk3blY=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc0JOOW42MnpYRXFvaytYcVVzVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=842_VlRz_g1CMCLi-M0twVw-4EOFG_noWyi_OsMnrAg1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
