<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=NOOK-mvTMnxl11s6oPYV0boPXN3iP4N7Rc-A56nk4KI1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2fKXReader%2fDetail%3fTIMESTAMP%3d637141814925287500%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJZ201904046%26RESULT%3d1%26SIGN%3dXgsNOFp5WcfG2%252fGiRBG%252b7G0onsM%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJZ201904046&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJZ201904046&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJZ201904046&amp;v=MTI4OTFtRnl6Z1ZydkxMejdCZExHNEg5ak1xNDlCWW9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVo=&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#29" data-title="&lt;b&gt;1 引言&lt;/b&gt; "><b>1 引言</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#33" data-title="&lt;b&gt;2 强化学习&lt;/b&gt; "><b>2 强化学习</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#34" data-title="&lt;b&gt;2.1 简介&lt;/b&gt;"><b>2.1 简介</b></a></li>
                                                <li><a href="#44" data-title="&lt;b&gt;2.2 Sarsa算法&lt;/b&gt;"><b>2.2 Sarsa算法</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#52" data-title="&lt;b&gt;3 基于模拟退火策略的Sarsa算法&lt;/b&gt; "><b>3 基于模拟退火策略的Sarsa算法</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#53" data-title="&lt;b&gt;3.1 模拟退火算法&lt;/b&gt;"><b>3.1 模拟退火算法</b></a></li>
                                                <li><a href="#60" data-title="&lt;b&gt;3.2 基于模拟退火策略的Sarsa算法&lt;/b&gt;"><b>3.2 基于模拟退火策略的Sarsa算法</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#70" data-title="&lt;b&gt;4 实验及结果分析&lt;/b&gt; "><b>4 实验及结果分析</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#71" data-title="&lt;b&gt;4.1 实验环境&lt;/b&gt;"><b>4.1 实验环境</b></a></li>
                                                <li><a href="#74" data-title="&lt;b&gt;4.2 实验参数&lt;/b&gt;"><b>4.2 实验参数</b></a></li>
                                                <li><a href="#81" data-title="&lt;b&gt;4.3 实验结果与分析&lt;/b&gt;"><b>4.3 实验结果与分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#92" data-title="&lt;b&gt;5 结束语&lt;/b&gt; "><b>5 结束语</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#42" data-title="&lt;b&gt;图1 强化学习学习框架&lt;/b&gt;"><b>图1 强化学习学习框架</b></a></li>
                                                <li><a href="#68" data-title="&lt;b&gt;表1 SA-Sarsa算法流程&lt;/b&gt;"><b>表1 SA-Sarsa算法流程</b></a></li>
                                                <li><a href="#73" data-title="&lt;b&gt;图2 迷宫仿真图&lt;/b&gt;"><b>图2 迷宫仿真图</b></a></li>
                                                <li><a href="#78" data-title="&lt;b&gt;表2 学习率参数效果表&lt;/b&gt;"><b>表2 学习率参数效果表</b></a></li>
                                                <li><a href="#80" data-title="&lt;b&gt;表3 三种算法参数设置&lt;/b&gt;"><b>表3 三种算法参数设置</b></a></li>
                                                <li><a href="#84" data-title="&lt;b&gt;表4 算法路线的动作选择&lt;/b&gt;"><b>表4 算法路线的动作选择</b></a></li>
                                                <li><a href="#86" data-title="&lt;b&gt;图3 算法路线图&lt;/b&gt;"><b>图3 算法路线图</b></a></li>
                                                <li><a href="#88" data-title="&lt;b&gt;图4 平均时间步奖励&lt;/b&gt;"><b>图4 平均时间步奖励</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" title=" R S Sutton, A G Barto.Reinforcement Learning:An Introduction[M].Cambridge:The MIT Press, 1998." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Reinforcement Learning: An Introduction">
                                        <b>[1]</b>
                                         R S Sutton, A G Barto.Reinforcement Learning:An Introduction[M].Cambridge:The MIT Press, 1998.
                                    </a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" title=" R S Sutton.Learning to Predict by the Methods of Temporal Differences[M].Kluwer Academic Publishers, 1988." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning to Predict by the Methods of Temporal Differences">
                                        <b>[2]</b>
                                         R S Sutton.Learning to Predict by the Methods of Temporal Differences[M].Kluwer Academic Publishers, 1988.
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title=" S Mabu, et al.Genetic Network Programming with Rein-forcement Learning Using Sarsa Algorithm[C].Evolutionary Computation, 2006.CEC 2006.IEEE Congress on.IEEE, 2006:463-469." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Genetic Network Programming with Rein-forcement Learning Using Sarsa Algorithm">
                                        <b>[3]</b>
                                         S Mabu, et al.Genetic Network Programming with Rein-forcement Learning Using Sarsa Algorithm[C].Evolutionary Computation, 2006.CEC 2006.IEEE Congress on.IEEE, 2006:463-469.
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" title=" F Wen, X Wang.Sarsa Learning Based Route Guidance System with Global and Local Parameter Strategy[J].Ieice Transactions on Fundamentals of Electronics Communications &amp;amp; Computer Sciences, 2015, E98.A (12) :2686-2693." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Sarsa Learning Based Route Guidance System with Global and Local Parameter Strategy">
                                        <b>[4]</b>
                                         F Wen, X Wang.Sarsa Learning Based Route Guidance System with Global and Local Parameter Strategy[J].Ieice Transactions on Fundamentals of Electronics Communications &amp;amp; Computer Sciences, 2015, E98.A (12) :2686-2693.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" title=" 刘全, 翟建伟, 章宗长, 钟珊, 周倩, 章鹏, 徐进.深度强化学习综述[J].计算机学报, 2017." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJX201801001&amp;v=MTM2ODZHNEg5bk1ybzlGWllRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVptRnl6Z1ZydkxMejdCZHI=&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!">
                                        <b>[5]</b>
                                         刘全, 翟建伟, 章宗长, 钟珊, 周倩, 章鹏, 徐进.深度强化学习综述[J].计算机学报, 2017.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" title=" R S Sutton.Dyna, an integrated architecture for learning, planning, and reacting[J].Acm Sigart Bulletin, 1991, 2 (4) :160-163." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000059490&amp;v=MTMxNjdRVE1ud1plWnRGaW5sVXJmSUpGc1JiaEU9TmlmSVk3SzdIdGpOcjQ5RlpPNEdDSFU1b0JNVDZUNFBRSC9pclJkR2VycQ==&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!">
                                        <b>[6]</b>
                                         R S Sutton.Dyna, an integrated architecture for learning, planning, and reacting[J].Acm Sigart Bulletin, 1991, 2 (4) :160-163.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" title=" 高阳, 陈世福, 陆鑫.强化学习研究综述[J].自动化学报, 2004, 30 (1) :86-100." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO200401010&amp;v=MDkzMDc0SHRYTXJvOUVaSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1Wm1GeXpnVnJ2TEtDTGZZYkc=&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!">
                                        <b>[7]</b>
                                         高阳, 陈世福, 陆鑫.强化学习研究综述[J].自动化学报, 2004, 30 (1) :86-100.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" title=" 许亚.基于强化学习的移动机器人路径规划研究[D].山东大学, 2013." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=1013221634.nh&amp;v=MjAyMjc2SDlmUHE1RWJQSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1Wm1GeXpnVnJ2TFZGMjZIYkc=&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!">
                                        <b>[8]</b>
                                         许亚.基于强化学习的移动机器人路径规划研究[D].山东大学, 2013.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" title=" 黄炳强.强化学习方法及其应用研究[D].上海交通大学, 2007." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CDFD&amp;filename=2008051996.nh&amp;v=MjM3OTlHRnJDVVI3cWZadVptRnl6Z1ZydkxWMTI3RnJPOUg5akZxWkViUElRS0RIODR2UjRUNmo1NE8zenFxQnQ=&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!">
                                        <b>[9]</b>
                                         黄炳强.强化学习方法及其应用研究[D].上海交通大学, 2007.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_10" title=" H V Hasselt, A Guez, D Silver.Deep Reinforcement Learning with Double Q-learning[J].Computer Science, 2015." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep Reinforcement Learning with Double Q-learning">
                                        <b>[10]</b>
                                         H V Hasselt, A Guez, D Silver.Deep Reinforcement Learning with Double Q-learning[J].Computer Science, 2015.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_11" title=" 马朋委.Q_learning强化学习算法的改进及应用研究[D].安徽理工大学, 2016." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=1016185774.nh&amp;v=MTQwMTlHRnJDVVI3cWZadVptRnl6Z1ZydkxWRjI2R0xLd0c5YkxxNUViUElRS0RIODR2UjRUNmo1NE8zenFxQnQ=&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!">
                                        <b>[11]</b>
                                         马朋委.Q_learning强化学习算法的改进及应用研究[D].安徽理工大学, 2016.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_12" title=" R S Sutton.Introduction:The Challenge of Reinforcement Learning[M].MIT Press, 1992." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Introduction:The challenge of reinforcement learning">
                                        <b>[12]</b>
                                         R S Sutton.Introduction:The Challenge of Reinforcement Learning[M].MIT Press, 1992.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_13" title=" 郭茂祖, 等.基于MetrOPOlis准则的Q-学习算法研究[J].计算机研究与发展, 2002, 39 (6) :684-688." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ200206009&amp;v=MjIwMzNZOUZiWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1Wm1GeXpnVnJ2TEx5dlNkTEc0SHRQTXE=&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!">
                                        <b>[13]</b>
                                         郭茂祖, 等.基于MetrOPOlis准则的Q-学习算法研究[J].计算机研究与发展, 2002, 39 (6) :684-688.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJZ" target="_blank">计算机仿真</a>
                2019,36(04),219-222+228             </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于模拟退火策略的Sarsa强化学习方法</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E7%8E%B0%E7%A3%8A&amp;code=41614095&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!" target="_blank">王现磊</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%83%9D%E6%96%87%E5%AE%81&amp;code=38970579&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!" target="_blank">郝文宁</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%99%88%E5%88%9A&amp;code=38797970&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!" target="_blank">陈刚</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E4%BD%99%E6%99%93%E6%99%97&amp;code=38785608&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!" target="_blank">余晓晗</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E4%BA%BA%E6%B0%91%E8%A7%A3%E6%94%BE%E5%86%9B%E9%99%86%E5%86%9B%E5%B7%A5%E7%A8%8B%E5%A4%A7%E5%AD%A6%E6%8C%87%E6%8C%A5%E4%BF%A1%E6%81%AF%E7%B3%BB%E7%BB%9F%E5%AD%A6%E9%99%A2&amp;code=1701801&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!" target="_blank">中国人民解放军陆军工程大学指挥信息系统学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>针对传统强化学习算法 (如Sarsa算法) 收敛速度缓慢的问题, 提出了基于模拟退火策略的Sarsa (SA-Sarsa) 算法。在策略选择上使用模拟退火策略替代ε-greedy策略, 利用退火速率控制算法的收敛速度, 有效克服了Sarsa算法直接通过随机数与贪婪值比较选择策略而导致的陷入局部最优解的问题, 达到了保证最优解、提高收敛速度的目的。通过迷宫的路径规划问题仿真, 将SA-Sarsa算法与Q-Learning和Sarsa两种传统算法进行了对比, 实验表明, SA-Sarsa学习算法在取得同等最优解下探索效率高且收敛速度更快。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!" target="_blank">强化学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%AE%97%E6%B3%95&amp;code=&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!" target="_blank">算法;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%A8%A1%E6%8B%9F%E9%80%80%E7%81%AB&amp;code=&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!" target="_blank">模拟退火;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%BF%B7%E5%AE%AB%E4%BB%BF%E7%9C%9F&amp;code=&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!" target="_blank">迷宫仿真;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    王现磊 (1993-) , 男 (汉族) , 河北人, 硕士研究生, 主要研究领域为强化学习。;
                                </span>
                                <span>
                                    郝文宁 (1971-) , 男 (汉族) , 山西人, 教授, 主要研究领域为海量高维数据归约, 作战效能评估。;
                                </span>
                                <span>
                                    陈刚 (1974-) , 男 (汉族) , 重庆人, 教授, 主要研究领域为作战指挥训练模拟。;
                                </span>
                                <span>
                                    余晓晗 (1985-) , 男 (汉族) , 江西人, 讲师, 博士, 主要研究领域为多准则决策, 军事运筹, 信息集成。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2017-12-06</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金青年科学基金项目 (71501186);</span>
                    </p>
            </div>
                    <h1><b>The Sarsa Reinforcement Learning Method Based on Simulated Annealing Strategy</b></h1>
                    <h2>
                    <span>WANG Xian-lei</span>
                    <span>HAO Wen-ning</span>
                    <span>CHEN Gang</span>
                    <span>YU Xiao-han</span>
            </h2>
                    <h2>
                    <span>College of Command Information Systems, Army Engineering University of PLA</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>A Sarsa (SA-Sarsa) algorithm based on simulated annealing strategy proposed in order to solve the problem that the convergence speed of traditional reinforcement learning algorithm (such as Sarsa algorithm) is slow. Simulated annealing strategy was used to controll the convergence speed of SA-Sarsa instead of ε-greedy strategy, which can overcome the disadvantage of failing into the local optimal solution in the original Sarsa algorithm and achieve a faster convergence speed. The SA-Sarsa algorithm was compared with the traditional algorithms of Q-Learning and Sarsa by simulation experiments of maze path planning problem. Experiments show that the SA-Sarsa learning algorithm has higher exploration efficiency and faster convergence speed under the same optimal solution.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Reinforcement%20learning&amp;code=&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!" target="_blank">Reinforcement learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Algorithm&amp;code=&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!" target="_blank">Algorithm;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Simulated%20annealing&amp;code=&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!" target="_blank">Simulated annealing;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Maze%20simulation&amp;code=&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!" target="_blank">Maze simulation;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2017-12-06</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="29" name="29" class="anchor-tag"><b>1 引言</b></h3>
                <div class="p1">
                    <p id="30">通过与环境的交互学习是人类获取知识的主要方法, 同时也是人类提高智能水平的基本途径。构建一个能适应未知环境, 并逐渐增强其自身能力的系统是人工智能研究的一个核心问题。而强化学习 (Reinforcement Learning, RL) <citation id="94" type="reference"><link href="3" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>在未知环境中智能体 (Agent) 主动地对环境进行探索, 同时环境对试探动作产生的回馈是评价性的, 智能体根据对环境的评价来调整即将采取的行为<citation id="95" type="reference"><link href="5" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>, 逐渐学习熟知环境, 因此强化学习是一种交互式的学习方法, 试错搜索和延迟回报是其两个最显著的特征。强化学习已广泛应用于运筹学、博弈论、统计学、控制论等领域。</p>
                </div>
                <div class="p1">
                    <p id="31">传统强化学习Sarsa算法<citation id="96" type="reference"><link href="7" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>中Agent与环境的每一次交互, 则对所采取的动作进行评估, 因此Sarsa算法在进行策略评估中有过程变化速度快的优势<citation id="97" type="reference"><link href="9" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>。但Sarsa算法中每一个情节 (episode) 中的时间步采取ε-greedy策略, 使得一个情节中不能保证智能体搜索到空间的每个位置, 而是通过情节的不断增加访问并记录空间的各个位置, 致使算法收敛速度慢<citation id="98" type="reference"><link href="11" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="32">针对传统强化学习Sarsa算法收敛速度较慢的缺点, 本文在传统强化学习Sarsa算法中, 加入模拟退火 (Simulated Annealing, SA) 策略替换ε-greedy策略, 以期提高算法的收敛速度, 具有较高的理论意义与使用价值。</p>
                </div>
                <h3 id="33" name="33" class="anchor-tag"><b>2 强化学习</b></h3>
                <h4 class="anchor-tag" id="34" name="34"><b>2.1 简介</b></h4>
                <div class="p1">
                    <p id="35">强化学习又称评价学习, 是Agent在环境中学习如何将状态映射到动作从而最大化数字奖励信号的一种学习方法<citation id="99" type="reference"><link href="13" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>。其学习过程可以归结为三个信号在一个Agent和它的环境之间依据策略来回传递。三个信号分别为:Agent所处环境的状态 (state) , 记为s;Agent所做出的动作 (action) , 记为a和Agent在状态s下做出的动作a所对应的奖赏值 (reward) <citation id="100" type="reference"><link href="15" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>, 记为r。Agent和环境间传递模式<citation id="101" type="reference"><link href="17" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>如下:</p>
                </div>
                <div class="p1">
                    <p id="36">1) Agent感知当前的环境状态s;</p>
                </div>
                <div class="p1">
                    <p id="37">2) 根据当前的s, Agent选择一个动作a并执行;</p>
                </div>
                <div class="p1">
                    <p id="38">3) 当Agent所做出的a作用于环境时, 环境转移到s′, 并给出奖赏值r;</p>
                </div>
                <div class="p1">
                    <p id="39">4) Agent根据环境反馈的r, 计算回报 (return) , 并以此作为内部策略更新的依据。</p>
                </div>
                <div class="p1">
                    <p id="40">强化学习框架由5个基本部分组成:状态s, 动作a, 策略π, 奖赏值r和环境。策略<i>π</i>:S→A被定义为从状态空间到动作空间的映射。Agent在当前s下依据策略π来选择一个a, 执行a后转移至下一个状态s′, 并接受环境反馈回来的r。环境是强化学习过程中对外界环境状态的仿真。</p>
                </div>
                <div class="p1">
                    <p id="41">强化学习是根据当前的状态通过调整策略择优动作以最大化取得数值回报的问题。框架<citation id="102" type="reference"><link href="19" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>如图1。</p>
                </div>
                <div class="area_img" id="42">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJZ201904046_042.jpg&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!">
                                    <img alt="图1 强化学习学习框架" src="Detail/GetImg?filename=images/JSJZ201904046_042.jpg&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!" />
                                </a>
                                <p class="img_tit"><b>图1 强化学习学习框架</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJZ201904046_042.jpg&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="43">强化学习常用的方法包括蒙特卡洛 (<b>MC</b>) 、瞬时拆分 (<b>temportal difference, TD</b>) 学习、<b>Q-Learning、Sarsa</b>算法和自适应动态规划等。根据需要, 下面仅对<b>Sarsa</b>算法做简单介绍。</p>
                </div>
                <h4 class="anchor-tag" id="44" name="44"><b>2.2 Sarsa算法</b></h4>
                <div class="p1">
                    <p id="45"><b>Sarsa</b>算法因其迭代过程&lt;<b><i>s</i>, <i>a</i>, <i>r</i>, <i>s</i>′, <i>a</i></b>′&gt;而得出其名, 于<b>1994</b>年由<b>Rummery</b>和<b>Niranjan</b>提出。<b>Sarsa</b>算法实际上是一种在线<b>Q-Learning</b><citation id="103" type="reference"><link href="21" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>, <b><i>s</i>, <i>a</i></b>表示状态动作对, <b><i>r</i></b>表示为 (<b><i>s</i>, <i>a</i></b>) 的奖赏值, <b><i>s</i></b>′表示转移状态, <b><i>a</i></b>′表示转移状态的抽样动作, 因此<b><i>Sarsa</i></b>算法在迭代中严格按照<i>ε</i>-<b><i>greedy</i></b>策略选择<b><i>a</i></b>得出的<b><i>Q</i> (<i>s</i>, <i>a</i></b>) 来进行迭代更新, 且策略的选择和<b><i>Q</i> (<i>s</i>, <i>a</i></b>) 的更新过程是一致的, 更新规则表示为</p>
                </div>
                <div class="p1">
                    <p id="46" class="code-formula">
                        <mathml id="46"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Q</mi><mo stretchy="false"> (</mo><mi>s</mi><mo>, </mo><mi>a</mi><mo stretchy="false">) </mo><mo>←</mo><mi>Q</mi><mo stretchy="false"> (</mo><mi>s</mi><mo>, </mo><mi>a</mi><mo stretchy="false">) </mo><mo>+</mo><mi>α</mi><mo stretchy="false">[</mo><mi>r</mi><mo>+</mo><mi>γ</mi><mi>Q</mi><mo stretchy="false"> (</mo><msup><mi>s</mi><mo>′</mo></msup><mo>, </mo><msup><mi>a</mi><mo>′</mo></msup><mo stretchy="false">) </mo><mo>-</mo><mi>Q</mi><mo stretchy="false"> (</mo><mi>s</mi><mo>, </mo><mi>a</mi><mo stretchy="false">) </mo><mo stretchy="false">]</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="47">式中<i>α</i>表示学习率, <i>γ</i>表示为折扣因子。<b><i>Sarsa</i></b>算法中<b><i>Agent</i></b>与环境的每一次交互, 则对所采取的动作进行评估, 因此对<b><i>Q</i> (<i>s</i>, <i>a</i></b>) 的评估过程变化速度比较快, 同时<b><i>Sarsa</i></b>算法在进行策略评估上有过程变化速度快的优势。</p>
                </div>
                <div class="p1">
                    <p id="48"><b><i>Sarsa</i></b>算法在所有<b><i>Q</i> (<i>s</i>, <i>a</i></b>) 都访问无数次的情况下, 以概率<b>1</b>达到最佳的策略和状态动作对。算法的收敛性取决于<i>ε</i>-<b><i>greedy</i></b>策略的性质, <i>ε</i>-<b><i>greedy</i></b>策略定义如下<citation id="104" type="reference"><link href="23" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation><sup></sup></p>
                </div>
                <div class="p1">
                    <p id="49" class="code-formula">
                        <mathml id="49"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtext>π</mtext><mo stretchy="false"> (</mo><mi>s</mi><msub><mrow></mrow><mi>t</mi></msub><mo stretchy="false">) </mo><mo>=</mo><mrow><mo>{</mo><mrow><mtable><mtr><mtd><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mtext>a</mtext><mtext>r</mtext><mtext>g</mtext><mtext>m</mtext><mtext>a</mtext><mtext>x</mtext></mrow></mstyle><mi>a</mi></munder><mi>Q</mi><mo stretchy="false"> (</mo><mi>s</mi><mo>, </mo><mi>a</mi><mo stretchy="false">) </mo></mtd><mtd><mtable><mtr><mtd><mi>i</mi><mi>f</mi></mtd><mtd><mi>δ</mi><mo>&lt;</mo><mi>ε</mi></mtd></mtr></mtable></mtd><mtd></mtd></mtr><mtr><mtd><mi>a</mi><msub><mrow></mrow><mrow><mi>r</mi><mi>a</mi><mi>n</mi><mi>d</mi><mi>o</mi><mi>m</mi></mrow></msub></mtd><mtd><mtext>o</mtext><mtext>t</mtext><mtext>h</mtext><mtext>e</mtext><mtext>r</mtext><mtext>w</mtext><mtext>i</mtext><mtext>s</mtext><mtext>e</mtext></mtd><mtd></mtd></mtr></mtable></mrow></mrow><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="50">式中<i>π</i> (<b><i>s</i></b><sub><b><i>t</i></b></sub>) 为策略选择, <i>δ</i>∈ (<b>0, 1</b>) 随机数, <i>ε</i>为贪婪值, 由<b><i>Sarsa</i></b>算法策略选择分析可知, 通过<i>δ</i>与<i>ε</i>的比较选择动作, <i>ε</i>取值较大时即在大概率选择状态<b><i>s</i></b><sub><b><i>t</i></b></sub>下<b><i>Q</i> (<i>s</i>, <i>a</i></b>) 最大的动作, 偶尔随机的选择动作, 使得算法容易陷入局部最优解, 随着<i>ε</i>取值变小即大概率随机选择动作, 并通过随机动作选择来迭代计算状态动作对值, 虽然算法最终以概率<b>1</b>达到最佳的策略和状态动作对, 但导致其收敛速度缓慢<citation id="105" type="reference"><link href="25" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="51">针对策略选择导致的算法易陷入局部最优和收敛速度慢的问题, 本文提出基于模拟退火策略的<b>Sarsa</b>算法。</p>
                </div>
                <h3 id="52" name="52" class="anchor-tag"><b>3 基于模拟退火策略的Sarsa算法</b></h3>
                <h4 class="anchor-tag" id="53" name="53"><b>3.1 模拟退火算法</b></h4>
                <div class="p1">
                    <p id="54"><b>SA</b>算法思想最早由<b>Metropolis</b>等人于<b>1953</b>年提出。<b>S. Kirkpatrick</b>等<citation id="106" type="reference"><link href="27" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>成功将退火思想引入到优化组合问题领域。<b>SA</b>是一个全局优化算法, 其思想来源于固体退火原理, 算法可以分解为解空间、目标函数和初始解三个部分。<b>SA</b>算法新解的产生和接受可分为四个步骤:</p>
                </div>
                <div class="p1">
                    <p id="55"><b>1</b>) 由一个产生函数从当前解产生一个位于解空间的新解;</p>
                </div>
                <div class="p1">
                    <p id="56"><b>2</b>) 计算与新解所对应的目标函数差;</p>
                </div>
                <div class="p1">
                    <p id="57"><b>3</b>) 判断新解是否被接受, 其依据为<b>Metropolis</b>准则:若<i>Δ</i><b><i>T</i>&lt;0</b>则接受新解作为新的当前解, 否则以概率<b>e</b><sup> (-<i>Δ</i><b>T / T</b>) </sup>接受新解作为新的当前解;</p>
                </div>
                <div class="p1">
                    <p id="58"><b>4</b>) 当新解被确定接收时, 用新解代替当前解。</p>
                </div>
                <div class="p1">
                    <p id="59">由步骤<b>3</b>) 可知, <b>SA</b>算法不是完全拒绝恶化解, 因此<b>SA</b>算法可跳出局部最优解, 避免陷入局部搜索。</p>
                </div>
                <h4 class="anchor-tag" id="60" name="60"><b>3.2 基于模拟退火策略的Sarsa算法</b></h4>
                <div class="p1">
                    <p id="61">通过将模拟退火算法跳出局部最优解的思想融入于<b>Sarsa</b>算法策略选择, 本文提出基于模拟退火策略的<b>Sarsa</b>算法, 模拟退火策略的步骤如下:</p>
                </div>
                <div class="p1">
                    <p id="62"><b>1</b>) 生成一个随机数δ;</p>
                </div>
                <div class="p1">
                    <p id="63"><b>2</b>) 在温度<b>T</b><sub><b>t</b></sub>时刻, 计算随机选择<b>a</b>的接受概率<b>e</b><sup> ( (<b>Q (s′, a′) - Q (s, a) ) / Tt</b>) </sup>;</p>
                </div>
                <div class="p1">
                    <p id="64"><b>3</b>) 策略判断做出选择:若<b>e</b><sup> ( (<b>Q (s′, a′) -Q (s, a) ) / Tt</b>) </sup>&gt;δ, 则随机选择一个动作, 反之选择动作<mathml id="65"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>a</mi><mi>r</mi><mi>g</mi><mspace width="0.25em" /><mi>m</mi><mi>a</mi><mi>x</mi></mrow></mstyle><mtext>a</mtext></munder><mtext>Q</mtext><mo stretchy="false"> (</mo><mtext>s</mtext><mo>, </mo><mtext>a</mtext><mo stretchy="false">) </mo><mo>;</mo></mrow></math></mathml></p>
                </div>
                <div class="p1">
                    <p id="66"><b>4</b>) 判断是否至冷却状态, 若至冷却状态, 则<b><i>T</i>=<i>Tmin</i>。</b></p>
                </div>
                <div class="p1">
                    <p id="67"><b>Sarsa</b>算法在状态<b>s</b>时以ε-<b>greedy</b>策略选择动作<b>a</b>, 而基于模拟退火策略的<b>Sarsa</b>算法则使用模拟退火策略代替ε-<b>greedy</b>策略, 通过温度函数控制搜索速率, 达到了智能体前期搜索率高且减少幅度小, 伴着退火策略的降温的过程, 搜索率下降, 冷却后, 搜索率稳定于结束温度时刻策略的效果。<b>SA-Sarsa</b>算法描述见表<b>1</b>。</p>
                </div>
                <div class="area_img" id="68">
                    <p class="img_tit"><b>表1 SA-Sarsa算法流程</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="68" border="1"><tr><td><br />Q (s, a) 表示lookup表中状态动作对, α:学习率, γ:折扣因子。</td></tr><tr><td><br />1. 给定初始温度T, 结束温度T<sub>min</sub>, 降温速率q<br />2. 初始化Q (s, a) ← (0, 0) <br />3. 重复 (对于每一个情节) <br />4.初始化状态s<br />5.模拟退火过程<i>T</i>=<i>T</i>×<i>q</i><br />6.s在模拟退火策略下中选择a<br />7.重复 (对情节中的每一时间步) <br />8. (1) 执行a, 转移至s′, 反馈r<br />9. (2) s′在退火策略下中选择a′<br />10. (3) 式 (1) 更新Q (s, a) <br />11. (4) s←s′;a←a′<br />12.直至s是目标状态终止<br />13. 收敛一定的情节数, 终止</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="69">因此SA-Sarsa算法前期较大概率的随机搜索保证了算法跳出局部解, 伴随着等温、冷却保证算法收敛性, 相比Sarsa算法其收敛速度大大提高, 有效克服了Sarsa算法直接通过δ与ε比较选择策略导致算法易陷入局部最优解和收敛速度慢的缺陷, 在后期试验中得以验证。</p>
                </div>
                <h3 id="70" name="70" class="anchor-tag"><b>4 实验及结果分析</b></h3>
                <h4 class="anchor-tag" id="71" name="71"><b>4.1 实验环境</b></h4>
                <div class="p1">
                    <p id="72">实验采用迷宫静态未知环境对智能体进行仿真, 通过路径规划问题, 将Q-Learning算法, Sarsa算法与本文SA-Sarsa算法进行对比, 证明SA-Sarsa算法在搜索效率和收敛速度上的优越性, 实验环境源于openai gym开源社区, 并基于实验环境在python中重新构造500×500单位像素的2D迷宫环境, 每个迷宫格为25×25单位像素, 如图2。</p>
                </div>
                <div class="area_img" id="73">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJZ201904046_073.jpg&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!">
                                    <img alt="图2 迷宫仿真图" src="Detail/GetImg?filename=images/JSJZ201904046_073.jpg&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!" />
                                </a>
                                <p class="img_tit"><b>图2 迷宫仿真图</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJZ201904046_073.jpg&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="74" name="74"><b>4.2 实验参数</b></h4>
                <div class="p1">
                    <p id="75">将智能体的起始位置置于未知迷宫环境的 (<b>0, 0</b>) 处 (红球) , 目标状态为 (<b>19, 19</b>) (黑球) , 智能体所在位置的坐标 (<b>x, y</b>) 为强化学习中的一个<b>s</b>。智能体所在状态的行为选择<b>a</b>包括{上, 下, 右, 左}分别记为{<b>0, 1, 2, 3</b>}, 因此每个状态所对应四个状态-动作对<b>Q (s, a</b>) 。若进行行为选择后碰到迷宫墙壁则下一个状态<b>s</b>′记为此刻状态<b>s</b>, 否则进入下一个状态<b>s</b>′。在实验中, 本文设置每做出一次<b>a</b>选择, 环境奖赏值<b><i>r</i>=-0.0005</b>, 达到目标状态则环境奖赏值<b><i>r</i>=10</b>。根据需要本文将模拟退火参数设置为:初始温度<b>T=300</b>, 降温速率<b>q=0.95</b>, 常温<b>T</b><sub><b>min</b></sub>=<b>10</b><sup>-<b>8</b></sup>。</p>
                </div>
                <div class="p1">
                    <p id="76">本文<b>20×20</b>仿真中, 设置最大情节数为<b>40000</b>, 每个情节中最大时间步为迷宫格的<b>100</b>倍, 智能体在每次情节的开始被设置为 (<b>0, 0</b>) , 达到目标或者时间步超过<b>40000</b>表明此情节结束。</p>
                </div>
                <div class="p1">
                    <p id="77">对不同学习率进行仿真, 通过开始收敛时<b>episode</b>测定学习率对收敛速度的影响, 效果如表<b>2</b>所示。</p>
                </div>
                <div class="area_img" id="78">
                    <p class="img_tit"><b>表2 学习率参数效果表</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="78" border="1"><tr><td><br />学习率 (α) </td><td>Q-Learning</td><td>Sarsa</td><td>SA-Sarsa</td></tr><tr><td><br />0.3</td><td>175</td><td>175</td><td>180</td></tr><tr><td><br />0.5</td><td>150</td><td>150</td><td>145</td></tr><tr><td><br />0.7</td><td>140</td><td>140</td><td>135</td></tr><tr><td><br />0.8</td><td>135</td><td>135</td><td>95</td></tr><tr><td><br />0.9</td><td>140</td><td>140</td><td>100</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="79">表2中表示不同学习率在三种算法中的收敛速度效果由收敛时刻的episode表示。本文选择不同学习率参数进行实验, 因动作选择的环境奖赏值与目标状态环境奖赏值差距较大, 因此学习率α应该较大, 由表2中结果, 选择效果较好实验参数进行对比实验, 不同算法参数设置如表3所示。</p>
                </div>
                <div class="area_img" id="80">
                    <p class="img_tit"><b>表3 三种算法参数设置</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="80" border="1"><tr><td><br />算法</td><td>学习率α</td><td>折扣因子γ</td><td>策略</td></tr><tr><td><br />Q-Learning</td><td>0.8</td><td>0.95</td><td>ε-0.01</td></tr><tr><td><br />Sarsa</td><td>0.8</td><td>0.95</td><td>ε-0.01</td></tr><tr><td><br />SA-Sarsa</td><td>0.8</td><td>0.95</td><td>SA</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="81" name="81"><b>4.3 实验结果与分析</b></h4>
                <div class="p1">
                    <p id="82">由上述仿真环境以及设置的参数对Q-Learning、Sarsa和SA-Sarsa三种算法进行实验。Agent的total_reward由每一步得到的奖赏值相加得出, 对于每个情节的完成记录一次total_reward。</p>
                </div>
                <div class="p1">
                    <p id="83">三种算法在仿真环境中学习路线由对应动作分别表示如表4。</p>
                </div>
                <div class="area_img" id="84">
                    <p class="img_tit"><b>表4 算法路线的动作选择</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="84" border="1"><tr><td><br />算法</td><td>动作选择</td></tr><tr><td><br />Q-Learning</td><td>2, 1, 2, 1, 1, 2, 2, 2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 3, 1, 1, 1, 1, 3, 3, 1, 1, 1, 3, 1, 1, 2, 2, 2, 1, 3, 1, 1, 2, 2, 1, 1, 2, 2, 0, 2, 2, 1, 1</td></tr><tr><td><br />Sarsa</td><td>2, 2, 2, 1, 1, 2, 0, 0, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 3, 1, 1, 1, 1, 1, 3, 3, 1, 1, 1, 3, 1, 1, 2, 2, 2, 1, 1, 3, 1, 2, 2, 1, 1, 2, 2, 0, 2, 2, 1, 1</td></tr><tr><td><br />SA-Sarsa</td><td>2, 2, 2, 1, 1, 1, 2, 2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 3, 1, 1, 1, 1, 1, 3, 3, 1, 1, 1, 3, 1, 1, 2, 2, 2, 1, 3, 1, 1, 2, 2, 1, 1, 2, 2, 0, 2, 1, 2, 1</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="85">由情节数中的时间步的动作绘制出路线图如图3所示。</p>
                </div>
                <div class="area_img" id="86">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJZ201904046_086.jpg&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!">
                                    <img alt="图3 算法路线图" src="Detail/GetImg?filename=images/JSJZ201904046_086.jpg&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!" />
                                </a>
                                <p class="img_tit"><b>图3 算法路线图</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJZ201904046_086.jpg&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="87">以每个情节中所获得平均时间步奖励作为横坐标, 每一个对应的情节作为纵坐标, 来比较三种算法其效果如图<b>4</b>所示。</p>
                </div>
                <div class="area_img" id="88">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJZ201904046_088.jpg&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!">
                                    <img alt="图4 平均时间步奖励" src="Detail/GetImg?filename=images/JSJZ201904046_088.jpg&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!" />
                                </a>
                                <p class="img_tit"><b>图4 平均时间步奖励</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJZ201904046_088.jpg&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="89">实验结果表明三种算法均搜索出最优路径, 且收敛于相同时间步的最优解, 但由图<b>3</b>可知, 三种算法最终路径并不相同, 得出本实验路径规划中最优路径并不唯一的结论。</p>
                </div>
                <div class="p1">
                    <p id="90">由图<b>4</b>情节时间步奖励均值趋势图分析可得:实验前期初始温度高搜索率大且搜索率减少幅度较小, 策略上随机选择动作, 因此在前<b>50</b>情节数中搜索空间较大, 时间步奖励均值低于<b>Q-Learning</b>和<b>Sarsa</b>算法, 伴随降温过程于<b>65</b>情节数处搜索率下降加快, 搜索空间减少, 时间步均值开始上升且搜索效率明显增高, 收敛速度加快。<b>Q-Learning</b>和<b>Sarsa</b>算法选择较小的ε保证实验结果的准确性和收敛性, 通过随机动作选择迭代计算状态动作对, 导致算法收敛速度变缓于<b>135</b>情节数左右开始收敛, 且稳定性差, 后随着情节数的增加最终趋于稳定的收敛状态, 而<b>SA-Sarsa</b>算法在<b>95</b>情节数左右开始收敛, 且收敛稳定性好, 弥补了传统强化学习收敛速度慢, 稳定性弱的缺点, 验证了<b>SA-Sarsa</b>算法的优势, 取得了较好的实验效果。</p>
                </div>
                <div class="p1">
                    <p id="91">综合以上实验结果, 在迷宫静态未知环境路径规划仿真研究中, <b>Sarsa</b>算法在收敛速度上与<b>Q-Learning</b>算法相当, 但在收敛稳定性上略优于<b>Q-Learning</b>算法, 而<b>SA-Sarsa</b>算法在其搜索效率、收敛速度和稳定性上都比另外两种算法效果好。</p>
                </div>
                <h3 id="92" name="92" class="anchor-tag"><b>5 结束语</b></h3>
                <div class="p1">
                    <p id="93">本文提出了基于模拟退火的<b>Sarsa</b>算法, 通过智能体在状态到动作的选择策略上将模拟退火策略代替ε-<b>greedy</b>策略, 旨在提高算法的搜索效率、收敛速度和稳定性, 并基于迷宫仿真环境实验, 得出基于<b>SA-Sarsa</b>算法在前期模拟退火策略下搜索率高, 随着退火冷却过程算法逐渐收敛, 通过对三种算法实验结果直观的对比验证了<b>SA-Sarsa</b>算法搜索效率高、收敛速度快且稳定性好的结论。</p>
                </div>
                <div class="area_img" id="107">
                                <img alt="" src="Detail/GetImg?filename=images/JSJZ201904046_10700.jpg&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!" />
                            <p class="img_tit"></p>

                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Reinforcement Learning: An Introduction">

                                <b>[1]</b> R S Sutton, A G Barto.Reinforcement Learning:An Introduction[M].Cambridge:The MIT Press, 1998.
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning to Predict by the Methods of Temporal Differences">

                                <b>[2]</b> R S Sutton.Learning to Predict by the Methods of Temporal Differences[M].Kluwer Academic Publishers, 1988.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Genetic Network Programming with Rein-forcement Learning Using Sarsa Algorithm">

                                <b>[3]</b> S Mabu, et al.Genetic Network Programming with Rein-forcement Learning Using Sarsa Algorithm[C].Evolutionary Computation, 2006.CEC 2006.IEEE Congress on.IEEE, 2006:463-469.
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Sarsa Learning Based Route Guidance System with Global and Local Parameter Strategy">

                                <b>[4]</b> F Wen, X Wang.Sarsa Learning Based Route Guidance System with Global and Local Parameter Strategy[J].Ieice Transactions on Fundamentals of Electronics Communications &amp; Computer Sciences, 2015, E98.A (12) :2686-2693.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJX201801001&amp;v=MDI4MjF6N0Jkckc0SDluTXJvOUZaWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1Wm1GeXpnVnJ2TEw=&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!">

                                <b>[5]</b> 刘全, 翟建伟, 章宗长, 钟珊, 周倩, 章鹏, 徐进.深度强化学习综述[J].计算机学报, 2017.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000059490&amp;v=MTA0MzdIL2lyUmRHZXJxUVRNbndaZVp0RmlubFVyZklKRnNSYmhFPU5pZklZN0s3SHRqTnI0OUZaTzRHQ0hVNW9CTVQ2VDRQUQ==&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!">

                                <b>[6]</b> R S Sutton.Dyna, an integrated architecture for learning, planning, and reacting[J].Acm Sigart Bulletin, 1991, 2 (4) :160-163.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=MOTO200401010&amp;v=MzEyNjFDTGZZYkc0SHRYTXJvOUVaSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1Wm1GeXpnVnJ2TEs=&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!">

                                <b>[7]</b> 高阳, 陈世福, 陆鑫.强化学习研究综述[J].自动化学报, 2004, 30 (1) :86-100.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=1013221634.nh&amp;v=MDk0NzBabUZ5emdWcnZMVkYyNkhiRzZIOWZQcTVFYlBJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnU=&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!">

                                <b>[8]</b> 许亚.基于强化学习的移动机器人路径规划研究[D].山东大学, 2013.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CDFD&amp;filename=2008051996.nh&amp;v=MTExNTR6Z1ZydkxWMTI3RnJPOUg5akZxWkViUElRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVptRnk=&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!">

                                <b>[9]</b> 黄炳强.强化学习方法及其应用研究[D].上海交通大学, 2007.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep Reinforcement Learning with Double Q-learning">

                                <b>[10]</b> H V Hasselt, A Guez, D Silver.Deep Reinforcement Learning with Double Q-learning[J].Computer Science, 2015.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_11" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=1016185774.nh&amp;v=MDU3MjdUNmo1NE8zenFxQnRHRnJDVVI3cWZadVptRnl6Z1ZydkxWRjI2R0xLd0c5YkxxNUViUElRS0RIODR2UjQ=&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!">

                                <b>[11]</b> 马朋委.Q_learning强化学习算法的改进及应用研究[D].安徽理工大学, 2016.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Introduction:The challenge of reinforcement learning">

                                <b>[12]</b> R S Sutton.Introduction:The Challenge of Reinforcement Learning[M].MIT Press, 1992.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_13" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JFYZ200206009&amp;v=MjQwMTFNcVk5RmJZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVabUZ5emdWcnZMTHl2U2RMRzRIdFA=&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!">

                                <b>[13]</b> 郭茂祖, 等.基于MetrOPOlis准则的Q-学习算法研究[J].计算机研究与发展, 2002, 39 (6) :684-688.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJZ201904046" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJZ201904046&amp;v=MTI4OTFtRnl6Z1ZydkxMejdCZExHNEg5ak1xNDlCWW9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVo=&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=842_VlRz_g1CMCLi-M0twVw-4EOFG_noWyi_OsMnrAg1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
