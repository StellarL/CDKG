<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=NOOK-mvTMnxl11s6oPYV0boPXN3iP4N7Rc-A56nk4KI1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2fKXReader%2fDetail%3fTIMESTAMP%3d637141933554975000%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJZ201903045%26RESULT%3d1%26SIGN%3dyETtnt9X2%252fghmUJVjA7UjCqvTmc%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJZ201903045&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJZ201903045&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJZ201903045&amp;v=MDkwMTFNckk5QllZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVabUZ5RGtVTDNNTHo3QmRMRzRIOWo=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZDMVREcXYrcHRHTVovelQ5amowVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#33" data-title="&lt;b&gt;1 引言&lt;/b&gt; "><b>1 引言</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#35" data-title="&lt;b&gt;2 TF-IDF模型和LSA模型&lt;/b&gt; "><b>2 TF-IDF模型和LSA模型</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#39" data-title="&lt;b&gt;3 语义哈希模型&lt;/b&gt; "><b>3 语义哈希模型</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#40" data-title="&lt;b&gt;3.1 问题定义&lt;/b&gt;"><b>3.1 问题定义</b></a></li>
                                                <li><a href="#42" data-title="&lt;b&gt;3.2 模型概括&lt;/b&gt;"><b>3.2 模型概括</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#47" data-title="&lt;b&gt;4 模型设计与预训练&lt;/b&gt; "><b>4 模型设计与预训练</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#93" data-title="&lt;b&gt;5 实验结果及分析&lt;/b&gt; "><b>5 实验结果及分析</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#100" data-title="&lt;b&gt;5.1 冲量对性能影响&lt;/b&gt;"><b>5.1 冲量对性能影响</b></a></li>
                                                <li><a href="#107" data-title="&lt;b&gt;5.2 随机梯度下降法与小批量梯度下降法&lt;/b&gt;"><b>5.2 随机梯度下降法与小批量梯度下降法</b></a></li>
                                                <li><a href="#110" data-title="&lt;b&gt;5.3 语义哈希维度分析&lt;/b&gt;"><b>5.3 语义哈希维度分析</b></a></li>
                                                <li><a href="#114" data-title="&lt;b&gt;5.4 信息检索方法比较&lt;/b&gt;"><b>5.4 信息检索方法比较</b></a></li>
                                                <li><a href="#117" data-title="&lt;b&gt;5.5 基于深度神经网络的语义哈希模型比较&lt;/b&gt;"><b>5.5 基于深度神经网络的语义哈希模型比较</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#121" data-title="&lt;b&gt;6 结论&lt;/b&gt; "><b>6 结论</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#44" data-title="&lt;b&gt;图1 语义哈希模型网络架构&lt;/b&gt;"><b>图1 语义哈希模型网络架构</b></a></li>
                                                <li><a href="#45" data-title="&lt;b&gt;图2 模型预训练示意图&lt;/b&gt;"><b>图2 模型预训练示意图</b></a></li>
                                                <li><a href="#105" data-title="&lt;b&gt;图3&lt;/b&gt;&lt;i&gt;MSN&lt;/i&gt;&lt;b&gt;新闻数据随训练次数变化的损失函数&lt;/b&gt;"><b>图3</b><i>MSN</i><b>新闻数据随训练次数变化的损失函数</b></a></li>
                                                <li><a href="#109" data-title="&lt;b&gt;表1 采用小批量梯度下降方法和随机梯度下 降方法训练所有样本一次所需的时间 (秒&lt;/b&gt;) "><b>表1 采用小批量梯度下降方法和随机梯度下 降方法训练所有样本一次所需的时间 (秒</b>) </a></li>
                                                <li><a href="#113" data-title="&lt;b&gt;图4 检索精度随哈希码维度的变化&lt;/b&gt;"><b>图4 检索精度随哈希码维度的变化</b></a></li>
                                                <li><a href="#116" data-title="&lt;b&gt;图5&lt;/b&gt; TF-IDF、LSA&lt;b&gt;与&lt;/b&gt;EDNN &lt;b&gt;结果对比&lt;/b&gt;"><b>图5</b> TF-IDF、LSA<b>与</b>EDNN <b>结果对比</b></a></li>
                                                <li><a href="#119" data-title="&lt;b&gt;表2 基于堆叠自编码器和玻尔兹曼机的 语义哈希模型结果对比&lt;/b&gt;"><b>表2 基于堆叠自编码器和玻尔兹曼机的 语义哈希模型结果对比</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" title=" S Febastiani. Machine Learning in Autom ated T ex t Categorization[J] .ACM Computing Surveys , 2002, 34 (1) :1-47 ." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000017594&amp;v=MTk4ODJNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcmZJS0Y4WGFCWT1OaWZJWTdLN0h0ak5yNDlGWk9vSUNYVTlvQg==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZDMVREcXYrcHRHTVovelQ5amowVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         S Febastiani. Machine Learning in Autom ated T ex t Categorization[J] .ACM Computing Surveys , 2002, 34 (1) :1-47 .
                                    </a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" title=" S Chakrabarti et al. Scalable Featu re Selection, Classification and Signature Generation for O rganizin g Large Text Databases Into H ierarchical Topic T ax on omies[J]. V LDB Journal, 1998, 7 (3) :163-178." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002607992&amp;v=MDQ3NzN4b3hjTUg3UjdxZForWnVGaURsVzc3TElsbz1OajdCYXJPNEh0SE9xWTlDYmVJTlkzazV6QmRoNGo5OVNYcVJy&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZDMVREcXYrcHRHTVovelQ5amowVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[2]</b>
                                         S Chakrabarti et al. Scalable Featu re Selection, Classification and Signature Generation for O rganizin g Large Text Databases Into H ierarchical Topic T ax on omies[J]. V LDB Journal, 1998, 7 (3) :163-178.
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title=" G Forman. An Experimental Study of Feature S election Metrics for Text Categorization[J] .Jou rnal of Machine Learning Research , 2003, 3 (1) :1289-1305 ." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=An extensive empirical study of feature selection metrics for text classification">
                                        <b>[3]</b>
                                         G Forman. An Experimental Study of Feature S election Metrics for Text Categorization[J] .Jou rnal of Machine Learning Research , 2003, 3 (1) :1289-1305 .
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" title=" R Salakhutdinov, G Hinton. Semantic hashing[J]. International Journal of Approximate Reasoning, 2009, 50 (7) :969-978." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012100143198&amp;v=MjIzMzdIL2lyUmRHZXJxUVRNbndaZVp0RmlubFVyZklLRjhYYUJZPU5pZk9mYks3SHRET3JvOUZaZThNRFhVeG9CTVQ2VDRQUQ==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZDMVREcXYrcHRHTVovelQ5amowVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                         R Salakhutdinov, G Hinton. Semantic hashing[J]. International Journal of Approximate Reasoning, 2009, 50 (7) :969-978.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" title=" G Hinton. Training Products of Experts by Minimizing Contrastive Divergence[J]. Neural Computation, 2002, 14 (8) :1771." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJBK&amp;filename=SJBK15090500012097&amp;v=Mjc4MjY5RlpPb05ESFUrb0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcmZJS0Y4WGFCWT1OaWZKWmJLOUh0ak1xbw==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZDMVREcXYrcHRHTVovelQ5amowVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                         G Hinton. Training Products of Experts by Minimizing Contrastive Divergence[J]. Neural Computation, 2002, 14 (8) :1771.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" title=" Salton, Buckley. Team weighting approaches in automatic text retireval, Readings in Information Retrieval[C]. International Conference on Computer Vision Theory and Applications, 1998: 652-657." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Team weighting approaches in automatic text retireval, Readings in Information Retrieval">
                                        <b>[6]</b>
                                         Salton, Buckley. Team weighting approaches in automatic text retireval, Readings in Information Retrieval[C]. International Conference on Computer Vision Theory and Applications, 1998: 652-657.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" title=" G Salton. Developments in Automatic Text Retrieval[J]. Science, 1991, 253 (5023) :974." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Developments in automatic text retrieval">
                                        <b>[7]</b>
                                         G Salton. Developments in Automatic Text Retrieval[J]. Science, 1991, 253 (5023) :974.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" title=" S C Deerwester, S T Dumais, T K Landauer, G W Furnas, and R A Harshman. Indexing by latent semantic analysis[J]. Journal of the American Society of Information Science, 1990, 41 (6) :391-407." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Indexing by Latent Semantic Analysis">
                                        <b>[8]</b>
                                         S C Deerwester, S T Dumais, T K Landauer, G W Furnas, and R A Harshman. Indexing by latent semantic analysis[J]. Journal of the American Society of Information Science, 1990, 41 (6) :391-407.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" title=" T Hofmann. Probabilistic latent semantic analysis[C]. Fifteenth Conference on Uncertainty in Artificial Intelligence. Morgan Kaufmann Publishers Inc. 1999: 289-296." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Probabilistic latent semantic analysis">
                                        <b>[9]</b>
                                         T Hofmann. Probabilistic latent semantic analysis[C]. Fifteenth Conference on Uncertainty in Artificial Intelligence. Morgan Kaufmann Publishers Inc. 1999: 289-296.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_10" >
                                        <b>[10]</b>
                                     D M Blei, A Y Ng, M I Jordan. Latent dirichlet allocation[J]. Journal of Machine Learning Research, 2003-3:993-1022.</a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_11" title=" Jun Han and Claudio Moraga. The influence of the sigmoid function parameters on the speed of backpropagation learning[C]. In IWANN, 1995:195-201." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The Influence of the Sigmoid Function Parameters onthe Speed of Backpropagation Learning">
                                        <b>[11]</b>
                                         Jun Han and Claudio Moraga. The influence of the sigmoid function parameters on the speed of backpropagation learning[C]. In IWANN, 1995:195-201.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_12" title=" A Cotter, et al. Better mini-batch algorithms via accelerated gradient methods[C]. International Conference on Neural Information Processing Systems. Curran Associates Inc. 2011:1647-1655." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Better mini-batch algorithms via accelerated gradient methods">
                                        <b>[12]</b>
                                         A Cotter, et al. Better mini-batch algorithms via accelerated gradient methods[C]. International Conference on Neural Information Processing Systems. Curran Associates Inc. 2011:1647-1655.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_13" title=" M Welling, M Rosen-Zvi, G Hinton. Exponential family harmoniums with an application to information retrieval[C]. International Conference on Neural Information Processing Systems. MIT Press, 2004:1481-1488." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Exponential family harmoniums with an application to information retrieval">
                                        <b>[13]</b>
                                         M Welling, M Rosen-Zvi, G Hinton. Exponential family harmoniums with an application to information retrieval[C]. International Conference on Neural Information Processing Systems. MIT Press, 2004:1481-1488.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_14" >
                                        <b>[14]</b>
                                     G E Hinton. Training products of experts by minimizing contrastive divergence[J]. Neural Computation, 2002, 14 (8) :1711-1800.</a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_15" title=" E P Xing, R Yan, A G Hauptmann. Mining Associated Text and Images with Dual-Wing Harmoniums[J]. Computer Science, 2012." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Mining Associated Text and Images with Dual-Wing Harmoniums">
                                        <b>[15]</b>
                                         E P Xing, R Yan, A G Hauptmann. Mining Associated Text and Images with Dual-Wing Harmoniums[J]. Computer Science, 2012.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJZ" target="_blank">计算机仿真</a>
                2019,36(03),225-229+260             </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于自编码器语义哈希的大规模文本预处理</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BC%A0%E5%BF%A0%E6%9E%97&amp;code=08547448&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZDMVREcXYrcHRHTVovelQ5amowVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">张忠林</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%A8%E6%9C%B4%E8%88%9F&amp;code=41438528&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZDMVREcXYrcHRHTVovelQ5amowVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">杨朴舟</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%85%B0%E5%B7%9E%E4%BA%A4%E9%80%9A%E5%A4%A7%E5%AD%A6&amp;code=0231149&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZDMVREcXYrcHRHTVovelQ5amowVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">兰州交通大学</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>展示了一种从大规模文本中学习文本索引的深层图形模型, 深层图形模型采用自编码器作为基础结构。该图模型最终输出的值具有较强的解释性, 并且比潜在语义索引更好地表示每个文档。当最深层使用少数二进制变量输出时 (例如32位) , 图形模型将文档通过语义散列的方式映射到存储器对应的地址上, 使得语义上相似的文档位于附近的地址处。然后可以通过访问所有仅相差几位的地址来找到类似于查询文本的文本。通过查询文件地址的方式, 基于近似匹配方式的散列编码的效率比局部敏感散列快得多, 通过使用语义哈希来过滤采用TF-IDF表示的文本, 将实现更高的准确性。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZDMVREcXYrcHRHTVovelQ5amowVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">自编码器;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%AF%AD%E4%B9%89%E5%93%88%E5%B8%8C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZDMVREcXYrcHRHTVovelQ5amowVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">语义哈希;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%BD%9C%E5%9C%A8%E8%AF%AD%E4%B9%89%E7%B4%A2%E5%BC%95&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZDMVREcXYrcHRHTVovelQ5amowVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">潜在语义索引;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%96%87%E6%9C%AC%E7%B4%A2%E5%BC%95&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZDMVREcXYrcHRHTVovelQ5amowVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">文本索引;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    张忠林 (1965-) , 男 (汉族) , 河北衡水人, 教授, 博士, CCF会员, 主要研究方向:智能信息处理、软件工程;;
                                </span>
                                <span>
                                    杨朴舟 (1994-) , 男 (汉族) , 陕西西安人, 硕士研究生导师, 主要研究领域为数据挖掘。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2017-12-05</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金 (61662043);</span>
                    </p>
            </div>
                    <h1><b>Large Scale Text Preprocessing Based on Self-Encoder Semantic Hashing</b></h1>
                    <h2>
                    <span>ZHANG Zhong-lin</span>
                    <span>YANG Pu-zhou</span>
            </h2>
                    <h2>
                    <span>Lanzhou Jiaotong University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>This article shows a deep graphics model for learning text indexes from large-scale text, using a self-encoder as the underlying structure. The final output of this graph model is strongly explanatory and represents each document better than a potential semantic index. When the deepest output is using a few binary variables (for example, 32 bits) , the graphical model maps the document through semantic hashing to the address corresponding to the memory so that semantically similar documents are located at nearby addresses. You can then find text similar to the query text by accessing all addresses that differ only by a few digits. By querying file addresses, this proximity-based hashing code is much more efficient than locally-sensitive hashing, and by using semantic hashing to filter text represented by TF-IDF, we achieve higher accuracy.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Self-encoder&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZDMVREcXYrcHRHTVovelQ5amowVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Self-encoder;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Semantic%20hash&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZDMVREcXYrcHRHTVovelQ5amowVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Semantic hash;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Potential%20semantic%20index&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZDMVREcXYrcHRHTVovelQ5amowVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Potential semantic index;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Text%20index&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZDMVREcXYrcHRHTVovelQ5amowVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Text index;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2017-12-05</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="33" name="33" class="anchor-tag"><b>1 引言</b></h3>
                <div class="p1">
                    <p id="34">随着互联网的发展以及企业信息化程度的提高, 面对不断增长的文本信息, 使公司、政府以及科研机构在信息处理上面临巨大的挑战, 文本分类作为当前机器学习研究的热点<citation id="123" type="reference"><link href="3" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>, 其困难之一是特征空间的高维性。高维特征空间中, 特征表示往往具有稀疏性, 还可能存在冗余信息, 造成对分类的处理不便, 还容易出现过拟合现象。之前通常使用词袋模型并结合TF-IDF表示一个文本, 即用该文本所包含的词语作为向量空间中的特征, 这就造成了特征空间的维数会随着文本的规模而增长, 有时会达到几万甚至几十万, 所以文本分类将会面临“高维诅咒” (Curse of Demontion) 问题<citation id="124" type="reference"><link href="5" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>, 文本预处理作为数据挖掘以及机器学习领域重要的预处理步骤, 特征选择以及降维技术已经得到广泛的研究<citation id="125" type="reference"><link href="7" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>, Salakhutdinov等人<citation id="126" type="reference"><link href="9" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>第一次提出了基于受限玻尔兹曼机 (Restricted Boltzmann Machines, RBMs) <citation id="127" type="reference"><link href="11" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>的语义哈希模型, 并通过实验证明了该模型在信息检索任务上能够取得与传统方法, 比如Latent Semantic Analysis (LSA) 和TF-IDF等近似的检索精度<citation id="128" type="reference"><link href="13" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>。与之不同的是, 本章提出了基于叠加自编码器 (stacked autoencoder) 构建深层神经网络 (deep neural network, DNN) 对文本进行语义哈希的方法。与基于RBMs的语义哈希模型在训练过程中具有复杂的转化过程不同, 本章提出的基于自编码器的语义哈希模型更加简单, 便于训练, 且仍具有很强的学习能力。具体而言, RBMs属于生成模型, 在进行模型优化时, 其梯度计算比较困难且是近似的;但对于自编码器而言, 梯度确定, 且模型训练目标非常简单一一重构模型输入。因此本文提出了基于自编码器的语义哈希模型, 并使用语义哈希来过滤采用TF-IDF表示的文本, 将实现更高的准确性。</p>
                </div>
                <h3 id="35" name="35" class="anchor-tag"><b>2 TF-IDF模型和LSA模型</b></h3>
                <div class="p1">
                    <p id="36">TF-IDF检索两个文本之间相似度其中最流行的和广泛使用的算法<citation id="129" type="reference"><link href="15" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>, 它是通过对每个文本词频向量采用逆文档频率进行加权, 最后根据两向量间的余弦相似度进行判定, 但是TF-IDF有以下缺点:①它直接在词空间上计算文本相似度, 如果对于大型词汇表来说, 效率会很低;②它假设不同单词间的计数彼此相互独立;③它不利用单词之间的语义相似性。</p>
                </div>
                <div class="p1">
                    <p id="37">为了弥补这些缺点, 许多更好捕获文本潜在语义的低维模型被提出, 并成功地应用于文本检索领域, 一种简单而广泛使用的方法是潜在语义分析 (LSA) <citation id="130" type="reference"><link href="17" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>, 它利用SVD分解提取低维语义结构, 得到单词 - 文档共生矩阵的低秩近似。这使得文档检索可以基于“语义”内容, 而不是单独加权的单词。然而, LSA在它可以捕获的语义内容的类型上是非常有限的, 因为它是一种线性方法, 所以它只能捕获单词之间的成对相关性。之后, T. Hofmann等<citation id="131" type="reference"><link href="19" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>提出了pLSA, 假设每个词看作是从一个文档特定的多词混合的词分布中的一个样本。Blei等<citation id="132" type="reference"><link href="21" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>提出了一个服从狄利克雷分配的生成模型对pLSA提出了改进。</p>
                </div>
                <div class="p1">
                    <p id="38">这些模型的复杂性是O (NV) , 其中N是文档语料库的大小, V是潜在语义空间的词汇数量。通过使用倒排索引, 可以将TF-IDF的时间复杂度提高到O (BV) , 其中B是查询文档中所有项时间的平均值, 对于所有这些模型, 文本的规模越大, 搜索相关文本需要的时间就越长。</p>
                </div>
                <h3 id="39" name="39" class="anchor-tag"><b>3 语义哈希模型</b></h3>
                <h4 class="anchor-tag" id="40" name="40"><b>3.1 问题定义</b></h4>
                <div class="p1">
                    <p id="41">假设训练数据集包含<i>n</i>条短文本, 记作<i>X</i>={<i>x</i><sub>1</sub>, <i>x</i><sub>2</sub>, …, <i>x</i><sub><i>n</i></sub>}∈<i>R</i><sup><i>k</i>×<i>n</i></sup>, 即<i>X</i>为一<i>k</i>行<i>n</i>列的矩阵, 其中<i>x</i><sub><i>i</i></sub>表示第<i>i</i>条文本的语义特征向量。目标是使用语义哈希模型学习出一套二进制编码<i>Y</i>={<i>y</i><sub>1</sub>, <i>y</i><sub>2</sub>, …, <i>y</i><sub><i>n</i></sub>}∈{0, 1}<sup><i>m</i>×<i>n</i></sup>使其能够表示训练数据的语义信息, 该模型可记为<i>F</i>:<i>R</i><sup><i>k</i></sup>→[0, 1]<sup><i>m</i></sup>, 即将短文本的语义特征表示映射到<i>m</i>位的二进制编码。</p>
                </div>
                <h4 class="anchor-tag" id="42" name="42"><b>3.2 模型概括</b></h4>
                <div class="p1">
                    <p id="43">自动编码器除了用作深层神经网络的构成组件, 还可以用于提取比输入特征维数更低的判别特征, 从而实现对输入的高维数据进行降维。自编码器也可以在所谓的过完备环境中, 利用某种行的正则化提取比输入维数更高的。同时又具有实际意义的判别特征。自编码器是关于中间层具有结构对称性的前馈网络。它的期望输出与输入相同, 可以学习恒等映射并抽取无监督特征。图1展示了设计的一个4层神经网络的语义哈希模型。该模型的输入为短文本的语义特征向量, 输出为表示该文本语义的二进制编码。为了使模型能够成功捕获输入文本的语义特征信息并将其编妈到最终的二进制哈希值中, 使用一种分为二阶段的半监督式方法训练该模型。首先, 将该神经网络模型看做叠加的自编码器, 每一层 (输入层除外) 既是当前自编码器的中间层又是下一自编码器的输入层, 该模型的展开结构如图2所示。</p>
                </div>
                <div class="area_img" id="44">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJZ201903045_044.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZDMVREcXYrcHRHTVovelQ5amowVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 语义哈希模型网络架构" src="Detail/GetImg?filename=images/JSJZ201903045_044.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZDMVREcXYrcHRHTVovelQ5amowVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图1 语义哈希模型网络架构</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJZ201903045_044.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZDMVREcXYrcHRHTVovelQ5amowVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="45">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJZ201903045_045.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZDMVREcXYrcHRHTVovelQ5amowVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 模型预训练示意图" src="Detail/GetImg?filename=images/JSJZ201903045_045.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZDMVREcXYrcHRHTVovelQ5amowVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图2 模型预训练示意图</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJZ201903045_045.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZDMVREcXYrcHRHTVovelQ5amowVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="46">对于该叠加自编码器, 使用梯度下降法逐步训练优化每一个独立的自编码器, 并将该训练过程称为模型的预训练阶段, 且该阶段完全采用非监督式学习方法。预训练之后, 将每个独立的自编码器作为整体网络模型使用监督式学习方法再进行精密 (<i>fine</i>-<i>tuneing</i>) 训练, 其训练目标为预测输入文本的标签信息 (比如类别) 等。使用叠加自编码构建语义哈希模型的原理在于叠加自编码器的学习能为要远远高于单个自编码器, 第一个自编码器通常从输入信息中学习初级特征, 在此基础上, 后续自编码器可进一步从这些初级信息中学习到更加抽象的特征。</p>
                </div>
                <h3 id="47" name="47" class="anchor-tag"><b>4 模型设计与预训练</b></h3>
                <div class="p1">
                    <p id="48">在模型预训练阶段, 每一个自编码器作为独立的神经网络通过重构输入变量学习隐变量特征来达到训练目的。可以说, 自编码器的输出与输入越近似, 它学习到的隐变量特征越好。从图1可以看到, 该语义哈希模型由3个自编码器叠加而成, 每个自编码器逐步减少隐变量特征维度, 最终达到将输入文本语义映射到低绅向量的目的。</p>
                </div>
                <div class="p1">
                    <p id="49">对第一个自编码器直接将输入向量映射到几百维、稠密的隐变量特征上, 该隐变量特征能够有效地从高缔、稀疏的输入向量中捕获到有用信息如图所示, 对于任一自编码器, 分别使用<i>k</i>, <i>j</i>, <i>i</i>表示输入层、隐层和输出层的一个结点。每一个自编码器的隐层具有相同的非线性激活函数, 用来学习隐变量特征, 其作用在结点<i>i</i>上的结果为</p>
                </div>
                <div class="p1">
                    <p id="50"><i>h</i><sub><i>j</i></sub>=<i>α</i> (<i>b</i><sub><i>j</i></sub>+∑<sub><i>k</i></sub><i>ω</i><sub><i>jk</i></sub><i>v</i><sub><i>k</i></sub>      (1) </p>
                </div>
                <div class="p1">
                    <p id="51">此处使用向量<i>v</i>表示自编码器的输入。显然, 对于第一个自编码器, 输入向量为代表该文本语义的<i>K</i>维向量;对于第二个自编码器, 输入第一个自编码学习的隐变量特征。公式中的<i>α</i>表示sigmoid激活函数, 表达式为</p>
                </div>
                <div class="p1">
                    <p id="52"><mathml id="53"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>y</mi><mo>=</mo><mi>α</mi><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mn>1</mn><mrow><mo stretchy="false"> (</mo><mn>1</mn><mo>+</mo><mi>e</mi><msup><mrow></mrow><mrow><mo>-</mo><mi>x</mi></mrow></msup><mo stretchy="false">) </mo></mrow></mfrac></mrow></math></mathml>      (2) </p>
                </div>
                <div class="p1">
                    <p id="54">sigmoid函数关于输入正的导数计算非常简单, 表达式为</p>
                </div>
                <div class="p1">
                    <p id="55"><mathml id="56"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mi>d</mi><mi>y</mi></mrow><mrow><mi>d</mi><mi>x</mi></mrow></mfrac><mo>=</mo><mi>y</mi><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>y</mi><mo stretchy="false">) </mo></mrow></math></mathml>      (3) </p>
                </div>
                <div class="p1">
                    <p id="57">其中<i>y</i>为函数输出, 而该对于在使用反向传播算法计算参数的偏导是非常快捷的<citation id="133" type="reference"><link href="23" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>。同时根据自编码器的不同目标设计了不同的激活函数和损失函数, 下面将一一介绍。</p>
                </div>
                <div class="p1">
                    <p id="58">自编码器1:采用语义特征<i>X</i>为输入, 首先对输入<i>X</i>进行归一化</p>
                </div>
                <div class="p1">
                    <p id="59"><mathml id="60"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>v</mi><msub><mrow></mrow><mi>j</mi></msub><mo>=</mo><mfrac><mrow><mi>x</mi><msub><mrow></mrow><mi>j</mi></msub></mrow><mrow><mstyle displaystyle="true"><msubsup><mo>∑</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></msubsup><mi>x</mi></mstyle><msub><mrow></mrow><mi>n</mi></msub></mrow></mfrac></mrow></math></mathml>      (4) </p>
                </div>
                <div class="p1">
                    <p id="61"><i>v</i>可以看作各语义特征在当前文本上的概率分布。归一化使所有输入层节点的值之和为1, 这样可以更利于模型学习数据以及解决了输入文本长度不同的问题对于该自编码器的输出层, 采用<i>softmax</i>作为激活函数, 因此结点<i>i</i>上的值计算为</p>
                </div>
                <div class="p1">
                    <p id="62"><mathml id="63"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>θ</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mfrac><mrow><mi>exp</mi><mo stretchy="false"> (</mo><mi>a</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo></mrow><mrow><mstyle displaystyle="true"><msubsup><mo>∑</mo><mrow><mi>m</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><mtext>e</mtext></mstyle><mtext>x</mtext><mtext>p</mtext><mo stretchy="false"> (</mo><mi>a</mi><msub><mrow></mrow><mi>m</mi></msub><mo stretchy="false">) </mo></mrow></mfrac></mrow></math></mathml>      (5) </p>
                </div>
                <div class="p1">
                    <p id="64">显然<i>θ</i><sub><i>i</i></sub>介于0与1之间。输出结果<i>θ</i>是对输入向量的重建, 训练目标为减小输出向量与输入向量的交叉烦, 因此损失函数为</p>
                </div>
                <div class="p1">
                    <p id="65"><i>J</i>=-∑<sub><i>i</i></sub><i>v</i><sub><i>i</i></sub>log<i>θ</i><sub><i>i</i></sub>      (6) </p>
                </div>
                <div class="p1">
                    <p id="66">对模型参数<i>w</i>, <i>b</i>求偏导</p>
                </div>
                <div class="p1">
                    <p id="67"><i>Δw</i><sub><i>ij</i></sub>=<i>h</i><sub><i>j</i></sub><i>δ</i><sub><i>i</i></sub>      (7) </p>
                </div>
                <div class="p1">
                    <p id="68"><i>Δb</i><sub><i>i</i></sub>=<i>δ</i><sub><i>i</i></sub>      (8) </p>
                </div>
                <div class="p1">
                    <p id="69">其中<i>δ</i><sub><i>i</i></sub>=<i>θ</i><sub><i>i</i></sub>-<i>v</i><sub><i>i</i></sub>。使用反向传播算法, <i>w</i><sub><i>jk</i></sub><i>b</i><sub><i>j</i></sub>的梯度更新计算为</p>
                </div>
                <div class="p1">
                    <p id="70"><mathml id="71"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Δ</mi><mi>w</mi><msub><mrow></mrow><mrow><mi>j</mi><mi>k</mi></mrow></msub><mo>=</mo><mi>v</mi><msub><mrow></mrow><mi>k</mi></msub><mo>*</mo><mi>h</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>h</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mo>*</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>i</mi></munder><mi>w</mi></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mi>δ</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></math></mathml>      (9) </p>
                </div>
                <div class="p1">
                    <p id="72"><mathml id="73"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Δ</mi><mi>b</mi><msub><mrow></mrow><mi>j</mi></msub><mo>=</mo><mi>h</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>h</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mo>*</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>i</mi></munder><mi>w</mi></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mi>δ</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></math></mathml>      (10) </p>
                </div>
                <div class="p1">
                    <p id="74">在预训练阶段, 对于每一个编码器, 使用梯度下降法迭代优化网络参数减小损失函数<i>J</i>在所有训练数据上的值, 直至损失函数收敛到阈值范围内或达到最大迭代次数 (比如200) 。</p>
                </div>
                <div class="p1">
                    <p id="75">自编码器2: 当编码器1预训练完成后, 将其输出的隐变量特征作为输入, 此时考虑到再次重新构建输入向量, 因此使用<i>sigmoid</i>函数为激活函数, 它的值域同为 (0, 1) 。输出层任一节点<i>i</i>的值<i>θ</i><sub><i>i</i></sub>为</p>
                </div>
                <div class="p1">
                    <p id="76"><mathml id="77"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>θ</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mi>α</mi><mo stretchy="false"> (</mo><mi>b</mi><msub><mrow></mrow><mi>i</mi></msub><mo>+</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>j</mi></munder><mi>w</mi></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mi>h</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo></mrow></math></mathml>      (11) </p>
                </div>
                <div class="p1">
                    <p id="78">因为输入与输出向量都为实数, 采用平方差为损失函数, 即</p>
                </div>
                <div class="p1">
                    <p id="79"><mathml id="80"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>J</mi><mo>=</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mstyle displaystyle="true"><munder><mo>∑</mo><mi>i</mi></munder><mo stretchy="false"> (</mo></mstyle><mi>θ</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mi>v</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><msup><mrow></mrow><mn>2</mn></msup></mrow></math></mathml>      (12) </p>
                </div>
                <div class="p1">
                    <p id="81">在整个神经网络结构中, 第二个自编码器充当了转换 (或过渡) 作用, 它以学习更加抽象的隐变量特征并降低特征维度为目标。理论上, 可<i>W</i>添加更多的自编码器2来构建更深层的神经网络。但是这样会同时増加模型的复杂度和训练代价, 且模型的学习能力不一定会得到相应程度的提高。因此在实际应用中, 通常根据任务性质和实验结果选择合适的网络层数。</p>
                </div>
                <div class="p1">
                    <p id="82">自编码器3:对于该语文哈希模型, 自编码器3做为最后一层, 其学习到的隐变量特征将转换成二进制编码用<i>W</i>表示原文本的语义信息。因此, 该自编码器除了重建自身输入向量外, 另一个重要目标是使得学习到的隐变量特征值非常接近0或1。为了实现这一目标, 在隐层任一结点<i>J</i>的输入值<i>a</i>, 上人工添加均值为0的高斯噪音, 则该结点输出值为</p>
                </div>
                <div class="p1">
                    <p id="83"><i>h</i><sup>′</sup><sub><i>j</i></sub>=<i>α</i> (<i>a</i><sub><i>j</i></sub>+<i>η</i>)      (13) </p>
                </div>
                <div class="p1">
                    <p id="84">其中<i>η</i>为高斯噪音, <i>a</i>表示<i>sigmoid</i>函数。加入高斯噪音后, 自编码器3为了消除噪音影响并正确地重建输入数据, <i>a</i><sub><i>j</i></sub>必须拥有较大的绝对值才能使得新加入的高斯噪音对函数结果产生较小的影响。原因是;若<i>a</i><sub><i>j</i></sub>绝对值很大, 即使添加噪音<i>η</i>到<i>a</i><sub><i>j</i></sub>上, 函数产生的正确结果<i>h</i><sub><i>j</i></sub> (<i>h</i><sub><i>j</i></sub>=<i>a</i> (<i>a</i><sub><i>j</i></sub>) ) 与实际结果<i>h</i><sup>′</sup><sub><i>j</i></sub>也只有细微差别。所以, 在训练过程中, 自编码器3会逐步优化网络参数使得的绝对值大到足以消除高斯噪音, 随之带来的影响就是使得该自编码器学习到的隐变量特征<i>h</i><sub><i>j</i></sub>慢慢逼近0或1, 并使得绝大部分隐变量分布在0和1附近。</p>
                </div>
                <div class="p1">
                    <p id="85">微调训练</p>
                </div>
                <div class="p1">
                    <p id="86">经过预训练后的自编码器模型参数对于语义哈希模型来说并不是最好的结果, 因此需要对这个网络参数进行进一步优化。即在原有四层神经网络的基础上添加<i>softmax</i>层预测输入文本的标签, 具体而言, 新添加的一层使用<i>softmax</i>函数作为激活函数, 对于<i>softmax</i>层上任一节点<i>i</i>, 其输出值<i>β</i><sub><i>i</i></sub>计算为</p>
                </div>
                <div class="p1">
                    <p id="87"><mathml id="88"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>β</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mfrac><mrow><mi>exp</mi><mo stretchy="false"> (</mo><mi>a</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">) </mo></mrow><mrow><mstyle displaystyle="true"><msubsup><mo>∑</mo><mrow><mi>m</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><mtext>e</mtext></mstyle><mtext>x</mtext><mtext>p</mtext><mo stretchy="false"> (</mo><mi>a</mi><msub><mrow></mrow><mi>m</mi></msub><mo stretchy="false">) </mo></mrow></mfrac></mrow></math></mathml>      (14) </p>
                </div>
                <div class="p1">
                    <p id="89">其中<i>n</i>表示最后一层的节点数。采用最小化实际概率分布向量<i>β</i>与目标向量<i>t</i>交叉熵误差函数</p>
                </div>
                <div class="p1">
                    <p id="90"><mathml id="91"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>J</mi><mo>=</mo><mo>-</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>Ι</mi></munder><mi>t</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mrow><mi>log</mi></mrow><mi>β</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></math></mathml>      (15) </p>
                </div>
                <div class="p1">
                    <p id="92">作为整体训练的目标函数, 并通过训练使目标函数最小化。其中目标向量<i>t</i>的值标记位为1外, 其它位都为0。</p>
                </div>
                <h3 id="93" name="93" class="anchor-tag"><b>5 实验结果及分析</b></h3>
                <div class="p1">
                    <p id="94">本文使用结构为2000-600-400-128的4层神经网络作为语义哈希模型, 使用小批量 (<i>mini</i>-<i>batc</i>) 梯度下降优化算法<citation id="134" type="reference"><link href="25" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>。每个小批量包含100个数据, 整个训练过程中最大迭代次数为200, 为了进行微调, 在较大的1000个批量数据向量上使用共轭梯度法训练参数。实验数据集采用<i>MSN</i>新闻数据, 分别是:商业, 体育, 娱乐, 天气和科技, 每个类别包含12000条数据, 从中选取80%作为训练集, 10%作为验证集, 10%作为测试集。</p>
                </div>
                <div class="p1">
                    <p id="95">采用二进制编码的海明距离来衡量文本之间的相似性, 并采用召回率-精度曲线来比较不同方法的结果优劣。相关定义如下</p>
                </div>
                <div class="p1">
                    <p id="96">召回率<mathml id="97"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo>=</mo><mfrac><mrow><mtext>检</mtext><mtext>索</mtext><mtext>到</mtext><mtext>相</mtext><mtext>关</mtext><mtext>文</mtext><mtext>档</mtext><mtext>数</mtext><mtext>目</mtext></mrow><mrow><mtext>所</mtext><mtext>有</mtext><mtext>相</mtext><mtext>关</mtext><mtext>文</mtext><mtext>档</mtext><mtext>数</mtext><mtext>目</mtext></mrow></mfrac></mrow></math></mathml></p>
                </div>
                <div class="p1">
                    <p id="98">精度<mathml id="99"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo>=</mo><mfrac><mrow><mtext>检</mtext><mtext>索</mtext><mtext>到</mtext><mtext>的</mtext><mtext>相</mtext><mtext>关</mtext><mtext>文</mtext><mtext>档</mtext><mtext>数</mtext><mtext>目</mtext></mrow><mrow><mtext>所</mtext><mtext>有</mtext><mtext>检</mtext><mtext>索</mtext><mtext>到</mtext><mtext>的</mtext><mtext>文</mtext><mtext>档</mtext><mtext>数</mtext><mtext>目</mtext></mrow></mfrac></mrow></math></mathml></p>
                </div>
                <h4 class="anchor-tag" id="100" name="100"><b>5.1 冲量对性能影响</b></h4>
                <div class="p1">
                    <p id="101">冲量算法的主要思想在于每一次梯度改变非彻底化, 即通过结合当前计算得到的梯度下降和上一步的梯度变化, 来对模型参数进行更新。比如, 对于参数<i>w</i> (<i>t</i>) 引入冲量后, 它的更新变量<i>Δw</i><sub><i>t</i></sub>计算为</p>
                </div>
                <div class="p1">
                    <p id="102"><mathml id="103"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Δ</mi><mi>w</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mfrac><mrow><mo>∂</mo><mi>J</mi></mrow><mrow><mo>∂</mo><mi>w</mi><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo></mrow></mfrac><mo>+</mo><mi>λ</mi><mi>Δ</mi><mi>w</mi><mo stretchy="false"> (</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>      (16) </p>
                </div>
                <div class="p1">
                    <p id="104">图3展示了冲量机制的作用, 可以发现使用冲量更新模型参数能够加快参数优化的收敛速度, 使模型经过较少的迭代次数就能获得相对较好的参数。</p>
                </div>
                <div class="area_img" id="105">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJZ201903045_105.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZDMVREcXYrcHRHTVovelQ5amowVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3MSN新闻数据随训练次数变化的损失函数" src="Detail/GetImg?filename=images/JSJZ201903045_105.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZDMVREcXYrcHRHTVovelQ5amowVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图3</b><i>MSN</i><b>新闻数据随训练次数变化的损失函数</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJZ201903045_105.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZDMVREcXYrcHRHTVovelQ5amowVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="106">该结果由监督式细微调整训练方法分别在使用和不使用冲量进行参数优化而得。</p>
                </div>
                <h4 class="anchor-tag" id="107" name="107"><b>5.2 随机梯度下降法与小批量梯度下降法</b></h4>
                <div class="p1">
                    <p id="108">表1列出了使用小批量梯度下降方法和随机梯度下降 (SGD) 方法在时间代价上的对比结果。可以看到, 由于小批量梯度下降法可以在内存中使用矩阵乘法同时计算多个训练样本产生的梯度更新, 因此大幅度减少了模型的训练时间。实验条件:单线程, 因特尔i5-7200, 3.1GHz处理器, 8GB内存, windows环境。</p>
                </div>
                <div class="area_img" id="109">
                    <p class="img_tit"><b>表1 采用小批量梯度下降方法和随机梯度下 降方法训练所有样本一次所需的时间 (秒</b>)  <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="109" border="1"><tr><td><br />Method</td><td>1<sup>st</sup> layer</td><td>2<sup>nd</sup> layer</td><td>3<sup>rd</sup> layer</td><td>Fine-tuning</td></tr><tr><td><br />Mini-batch</td><td>204</td><td>40</td><td>9</td><td>165</td></tr><tr><td><br />SGD</td><td>1832</td><td>486</td><td>120</td><td>1905</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="110" name="110"><b>5.3 语义哈希维度分析</b></h4>
                <div class="p1">
                    <p id="111">最后进一步分析了改变语义哈希码维度 (即输出层结点数目) 对短文本信息检索和分类任务的影响。</p>
                </div>
                <div class="p1">
                    <p id="112">图4.展示了使用EDNN方法返回的前100个检索结果的精度随哈希码维度的变化情况, 而图4展示了使用EDNN方法获得的分类准确率随哈希码维度的变化。从变化曲线可以看出, 当哈希码维度从32上升到128, 此时的检索精度和分类准确率也随之快速上升;而当哈希码从128维变到256维时, 精度和准确率仅提高了一点;之后, 随着哈希码维度的增加, 精度和准确率却逐渐下降。原因在于, 当哈希码维度过高时, 空间会变得非常稀疏, 拥有在一定范围内的海明距离的点数目会减少很多, 使得文本间的语义相似度变弱。因此在本工作中, 考虑到海明距离的有效性和模型的训练代价, 使用128维的二进制编码表示文本。</p>
                </div>
                <div class="area_img" id="113">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJZ201903045_113.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZDMVREcXYrcHRHTVovelQ5amowVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 检索精度随哈希码维度的变化" src="Detail/GetImg?filename=images/JSJZ201903045_113.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZDMVREcXYrcHRHTVovelQ5amowVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图4 检索精度随哈希码维度的变化</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJZ201903045_113.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZDMVREcXYrcHRHTVovelQ5amowVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="114" name="114"><b>5.4 信息检索方法比较</b></h4>
                <div class="p1">
                    <p id="115">本文之前讲的TF-IDF和LSA等以及其它改进模型这些引入概率的模型可以看作是其中隐藏的主题变量直接与表示文本的词频向量连接的图形模型。Welling等<citation id="135" type="reference"><link href="27" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>指出它们的主要缺点是最终的结果不具有解释性, 所以不得不近似计算主题的后验分布。这使得模型不能较好拟合数据。为了改进这些缺点, 引入了一类两层无向图模型, 将受限玻尔兹曼机 (RBM) <citation id="136" type="reference"><link href="29" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>推广到指数族分布。这使得模型可以对非二进制数据建模, 并通过隐藏变量转换二进制数据。最大似然学习在深度学习模型中是难以处理的, 但是可以通过对比散度的方法优化目标函数的梯度来有效地进行学习。随着这些无向图模型的进一步发展表明<citation id="137" type="reference"><link href="31" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>, 基于无向图的深度学习模型在检索准确性方面相较于传统方法是有提高的。</p>
                </div>
                <div class="area_img" id="116">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJZ201903045_116.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZDMVREcXYrcHRHTVovelQ5amowVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 TF-IDF、LSA与EDNN 结果对比" src="Detail/GetImg?filename=images/JSJZ201903045_116.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZDMVREcXYrcHRHTVovelQ5amowVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图5</b> TF-IDF、LSA<b>与</b>EDNN <b>结果对比</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJZ201903045_116.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZDMVREcXYrcHRHTVovelQ5amowVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="117" name="117"><b>5.5 基于深度神经网络的语义哈希模型比较</b></h4>
                <div class="p1">
                    <p id="118">本文提出的基于自编码器的语义哈希模型和引言提出己有的基于受限玻尔兹曼机的语义哈希模型, 且该两种模型的训练数据都为MSN新闻文本。表2比较了这两种模型的训练时间, 可以看到, 由这两种语义哈希模型取得的精度-召回率很高且效果非常接近, 但是对于训综时间, 基于自编码器的模型要优于基于受限玻尔兹曼机的模型。该实验说明:提出的基于自编码器的语义哈希模型结构简单, 训练代价小, 且仍然具有竞争力的语义哈希能力。</p>
                </div>
                <div class="area_img" id="119">
                    <p class="img_tit"><b>表2 基于堆叠自编码器和玻尔兹曼机的 语义哈希模型结果对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="119" border="1"><tr><td><br />Model</td><td>1<sup>st</sup> layer</td><td>2<sup>nd</sup> layer</td><td>3<sup>rd</sup> layer</td><td>Fine-tunnig</td></tr><tr><td><br />SAE</td><td>206s</td><td>35s</td><td>8s</td><td>155s</td></tr><tr><td><br />RBMs</td><td>249s</td><td>52s</td><td>13s</td><td>203s</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="120">实验条件:intel i5 7200处理器, 主频3.1GHz, 8G内存, windows环境。</p>
                </div>
                <h3 id="121" name="121" class="anchor-tag"><b>6 结论</b></h3>
                <div class="p1">
                    <p id="122">在本文中, 描述了一个两阶段学习过程, 用于生成可用于快速文档检索的二进制码值。在预训阶段, 贪婪地学习一个深层生成模型, 其中最下层代表文档的字数向量而最高层代表该文档的学习二进制代码。在微调阶段, 模型被“展开”以产生深度自编码器网络, 并且使用反向传播来微调权重以实现最佳重构。通过在输入层添加噪声, 并通过反向传播使输出大部分分布在0和1周围。通过将输出的二进制码值视为内存地址, 可以在独立于文档集合大小的时间内找到语义上相似的文档, 这是其它检索方法无法实现的。使用MSN新闻数据集, 发现通过使用语义散列作为TF-IDF模型的过滤器, 在整个文档集合中, 获得了比TF-IDF模型或TF-IDF模型经过局部敏感散列获得更高的精度和召回率。</p>
                </div>
                <div class="area_img" id="138">
                                <img alt="" src="Detail/GetImg?filename=images/JSJZ201903045_13800.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZDMVREcXYrcHRHTVovelQ5amowVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                            <p class="img_tit"></p>

                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000017594&amp;v=MjY4MTlUNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp0RmlubFVyZklLRjhYYUJZPU5pZklZN0s3SHRqTnI0OUZaT29JQ1hVOW9CTQ==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZDMVREcXYrcHRHTVovelQ5amowVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> S Febastiani. Machine Learning in Autom ated T ex t Categorization[J] .ACM Computing Surveys , 2002, 34 (1) :1-47 .
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJD00002607992&amp;v=MTk2NjZJTlkzazV6QmRoNGo5OVNYcVJyeG94Y01IN1I3cWRaK1p1RmlEbFc3N0xJbG89Tmo3QmFyTzRIdEhPcVk5Q2Jl&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZDMVREcXYrcHRHTVovelQ5amowVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[2]</b> S Chakrabarti et al. Scalable Featu re Selection, Classification and Signature Generation for O rganizin g Large Text Databases Into H ierarchical Topic T ax on omies[J]. V LDB Journal, 1998, 7 (3) :163-178.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=An extensive empirical study of feature selection metrics for text classification">

                                <b>[3]</b> G Forman. An Experimental Study of Feature S election Metrics for Text Categorization[J] .Jou rnal of Machine Learning Research , 2003, 3 (1) :1289-1305 .
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012100143198&amp;v=MTIxNzRmT2ZiSzdIdERPcm85RlplOE1EWFV4b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcmZJS0Y4WGFCWT1OaQ==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZDMVREcXYrcHRHTVovelQ5amowVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b> R Salakhutdinov, G Hinton. Semantic hashing[J]. International Journal of Approximate Reasoning, 2009, 50 (7) :969-978.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJBK&amp;filename=SJBK15090500012097&amp;v=MjEzOTFHZXJxUVRNbndaZVp0RmlubFVyZklLRjhYYUJZPU5pZkpaYks5SHRqTXFvOUZaT29OREhVK29CTVQ2VDRQUUgvaXJSZA==&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZDMVREcXYrcHRHTVovelQ5amowVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b> G Hinton. Training Products of Experts by Minimizing Contrastive Divergence[J]. Neural Computation, 2002, 14 (8) :1771.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Team weighting approaches in automatic text retireval, Readings in Information Retrieval">

                                <b>[6]</b> Salton, Buckley. Team weighting approaches in automatic text retireval, Readings in Information Retrieval[C]. International Conference on Computer Vision Theory and Applications, 1998: 652-657.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Developments in automatic text retrieval">

                                <b>[7]</b> G Salton. Developments in Automatic Text Retrieval[J]. Science, 1991, 253 (5023) :974.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Indexing by Latent Semantic Analysis">

                                <b>[8]</b> S C Deerwester, S T Dumais, T K Landauer, G W Furnas, and R A Harshman. Indexing by latent semantic analysis[J]. Journal of the American Society of Information Science, 1990, 41 (6) :391-407.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Probabilistic latent semantic analysis">

                                <b>[9]</b> T Hofmann. Probabilistic latent semantic analysis[C]. Fifteenth Conference on Uncertainty in Artificial Intelligence. Morgan Kaufmann Publishers Inc. 1999: 289-296.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_10" >
                                    <b>[10]</b>
                                 D M Blei, A Y Ng, M I Jordan. Latent dirichlet allocation[J]. Journal of Machine Learning Research, 2003-3:993-1022.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The Influence of the Sigmoid Function Parameters onthe Speed of Backpropagation Learning">

                                <b>[11]</b> Jun Han and Claudio Moraga. The influence of the sigmoid function parameters on the speed of backpropagation learning[C]. In IWANN, 1995:195-201.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Better mini-batch algorithms via accelerated gradient methods">

                                <b>[12]</b> A Cotter, et al. Better mini-batch algorithms via accelerated gradient methods[C]. International Conference on Neural Information Processing Systems. Curran Associates Inc. 2011:1647-1655.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Exponential family harmoniums with an application to information retrieval">

                                <b>[13]</b> M Welling, M Rosen-Zvi, G Hinton. Exponential family harmoniums with an application to information retrieval[C]. International Conference on Neural Information Processing Systems. MIT Press, 2004:1481-1488.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_14" >
                                    <b>[14]</b>
                                 G E Hinton. Training products of experts by minimizing contrastive divergence[J]. Neural Computation, 2002, 14 (8) :1711-1800.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Mining Associated Text and Images with Dual-Wing Harmoniums">

                                <b>[15]</b> E P Xing, R Yan, A G Hauptmann. Mining Associated Text and Images with Dual-Wing Harmoniums[J]. Computer Science, 2012.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJZ201903045" />
        <input id="dpi" type="hidden" value="400" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZDMVREcXYrcHRHTVovelQ5amowVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJZ201903045&amp;v=MDkwMTFNckk5QllZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVabUZ5RGtVTDNNTHo3QmRMRzRIOWo=&amp;uid=WEEvREcwSlJHSldRa1FhcTdnTnhXM3ZDMVREcXYrcHRHTVovelQ5amowVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=842_VlRz_g1CMCLi-M0twVw-4EOFG_noWyi_OsMnrAg1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
