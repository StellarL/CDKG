<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=NOOK-mvTMnxl11s6oPYV0boPXN3iP4N7Rc-A56nk4KI1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2fKXReader%2fDetail%3fTIMESTAMP%3d637139883157916250%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJZ201909052%26RESULT%3d1%26SIGN%3d52aWQ%252beT%252bA7tto1HoSxIizktqdc%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJZ201909052&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJZ201909052&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJZ201909052&amp;v=Mjk2NTNvOUFab1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1Wm9GeW5rVjcvT0x6N0JkTEc0SDlqTXA=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc1FTMHdYQkFCcm1KK2ZPTjlDdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#35" data-title="&lt;b&gt;1 引言&lt;/b&gt; "><b>1 引言</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#40" data-title="&lt;b&gt;2 Staple跟踪器&lt;/b&gt; "><b>2 Staple跟踪器</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#44" data-title="&lt;b&gt;3 缺陷分析&lt;/b&gt; "><b>3 缺陷分析</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#46" data-title="&lt;b&gt;3.1 背景干扰&lt;/b&gt;"><b>3.1 背景干扰</b></a></li>
                                                <li><a href="#49" data-title="&lt;b&gt;3.2 线性加权&lt;/b&gt;"><b>3.2 线性加权</b></a></li>
                                                <li><a href="#51" data-title="&lt;b&gt;3.3 更新机制&lt;/b&gt;"><b>3.3 更新机制</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#54" data-title="&lt;b&gt;4 改进措施&lt;/b&gt; "><b>4 改进措施</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#56" data-title="&lt;b&gt;4.1 空间约束&lt;/b&gt;"><b>4.1 空间约束</b></a></li>
                                                <li><a href="#59" data-title="&lt;b&gt;4.2 自适应机制&lt;/b&gt;"><b>4.2 自适应机制</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#67" data-title="&lt;b&gt;5 实验&lt;/b&gt; "><b>5 实验</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#69" data-title="&lt;b&gt;5.1 实验评测集&lt;/b&gt;"><b>5.1 实验评测集</b></a></li>
                                                <li><a href="#73" data-title="&lt;b&gt;5.2 验证实验&lt;/b&gt;"><b>5.2 验证实验</b></a></li>
                                                <li><a href="#77" data-title="&lt;b&gt;5.3 对比实验&lt;/b&gt;"><b>5.3 对比实验</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#80" data-title="&lt;b&gt;6 总结&lt;/b&gt; "><b>6 总结</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#48" data-title="&lt;b&gt;图1 背景干扰示例&lt;/b&gt;"><b>图1 背景干扰示例</b></a></li>
                                                <li><a href="#53" data-title="&lt;b&gt;图2 遮挡示例&lt;/b&gt;"><b>图2 遮挡示例</b></a></li>
                                                <li><a href="#62" data-title="&lt;b&gt;图3 响应图示例&lt;/b&gt;"><b>图3 响应图示例</b></a></li>
                                                <li><a href="#71" data-title="&lt;b&gt;表1 实验评测集的挑战分布&lt;/b&gt;"><b>表1 实验评测集的挑战分布</b></a></li>
                                                <li><a href="#72" data-title="&lt;b&gt;图4 实验评测集序列首帧集&lt;/b&gt;"><b>图4 实验评测集序列首帧集</b></a></li>
                                                <li><a href="#75" data-title="&lt;b&gt;表2&lt;/b&gt; AUC&lt;b&gt;对比表&lt;/b&gt;"><b>表2</b> AUC<b>对比表</b></a></li>
                                                <li><a href="#76" data-title="&lt;b&gt;表3 准确率(20&lt;/b&gt;)px&lt;b&gt;对比表&lt;/b&gt;"><b>表3 准确率(20</b>)px<b>对比表</b></a></li>
                                                <li><a href="#79" data-title="&lt;b&gt;图5&lt;/b&gt; OPE&lt;b&gt;测试结果&lt;/b&gt;"><b>图5</b> OPE<b>测试结果</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" title=" Y Lecun,Y Bengio,G Hinton.Deep learning [J].Nature,2015,521(7553):436." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep learning">
                                        <b>[1]</b>
                                         Y Lecun,Y Bengio,G Hinton.Deep learning [J].Nature,2015,521(7553):436.
                                    </a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" title=" D S Bolme,et al.Visual object tracking using adaptive correlation filters[C].Computer Vision and Pattern Recognition IEEE,2010:2544-2550." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Visual object tracking using adaptive correlation filters">
                                        <b>[2]</b>
                                         D S Bolme,et al.Visual object tracking using adaptive correlation filters[C].Computer Vision and Pattern Recognition IEEE,2010:2544-2550.
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title=" J F Henriques,et al.Exploiting the Circulant Structure of Tracking-by-Detection with Kernels[C].European Conference on Computer Vision,2012:702-715." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Exploiting the circulant structure of tracking-by-detection with kernels">
                                        <b>[3]</b>
                                         J F Henriques,et al.Exploiting the Circulant Structure of Tracking-by-Detection with Kernels[C].European Conference on Computer Vision,2012:702-715.
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" title=" J F Henriques,et al.High-Speed Tracking with Kernelized Correlation Filters[J].IEEE Transactions on Pattern Analysis &amp;amp; Machine Intelligence,2014,37(3):583-596." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=High-speed tracking with kernelized correlation filters">
                                        <b>[4]</b>
                                         J F Henriques,et al.High-Speed Tracking with Kernelized Correlation Filters[J].IEEE Transactions on Pattern Analysis &amp;amp; Machine Intelligence,2014,37(3):583-596.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" title=" Y Li,J Zhu.A Scale Adaptive Kernel Correlation Filter Tracker with Feature Integration[J].2014,8926:254-265." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A Scale Adaptive Kernel Correlation Filter Tracker with Feature Integration">
                                        <b>[5]</b>
                                         Y Li,J Zhu.A Scale Adaptive Kernel Correlation Filter Tracker with Feature Integration[J].2014,8926:254-265.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" title=" M Danelljan,G H&#228;ger,F S Khan.Accurate scale estimation for robust visual tracking[C].British Machine Vision Conference,2014:61.1-65.11." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Accurate Scale Estimation for Robust Visual Tracking">
                                        <b>[6]</b>
                                         M Danelljan,G H&#228;ger,F S Khan.Accurate scale estimation for robust visual tracking[C].British Machine Vision Conference,2014:61.1-65.11.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" title=" M Danelljan,et al.Learning Spatially Regularized Correlation Filters for Visual Tracking[C].IEEE International Conference on Computer Vision,2015:4310-4318." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning spatially regularized correlation filters for visual tracking">
                                        <b>[7]</b>
                                         M Danelljan,et al.Learning Spatially Regularized Correlation Filters for Visual Tracking[C].IEEE International Conference on Computer Vision,2015:4310-4318.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" title=" P F Felzenszwalb,et al.Object detection with discriminatively trained part-based models[J].IEEE Transactions on Pattern Analysis &amp;amp; Machine Intelligence,2014,47(2):6-7." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Object Detection with Discriminatively Trained Part-Based Models">
                                        <b>[8]</b>
                                         P F Felzenszwalb,et al.Object detection with discriminatively trained part-based models[J].IEEE Transactions on Pattern Analysis &amp;amp; Machine Intelligence,2014,47(2):6-7.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" title=" L Bertinetto,et al.Staple:Complementary Learners for Real-Time Tracking[C].Computer Vision and Pattern Recognition IEEE,2016:1401-1409." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Staple:Complementary learners for real-time tracking">
                                        <b>[9]</b>
                                         L Bertinetto,et al.Staple:Complementary Learners for Real-Time Tracking[C].Computer Vision and Pattern Recognition IEEE,2016:1401-1409.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_10" title=" 周志华.机器学习[M].清华大学出版社,2016:53-56." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CBBD&amp;filename=9787302423287000&amp;v=MjIyODFOWE9ySTFOWStzUERCTTh6eFVTbURkOVNIN24zeEU5ZmJ2bktyaWZaZVp2RnlualU3N0pKRjRTWEZxekdiQzRI&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc1FTMHdYQkFCcm1KK2ZPTjlDdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[10]</b>
                                         周志华.机器学习[M].清华大学出版社,2016:53-56.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_11" title=" M Wang,Y Liu,Z Huang.Large Margin Object Tracking with Circulant Feature Maps[J].2017:4800-4808." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Large Margin Object Tracking with Circulant Feature Maps">
                                        <b>[11]</b>
                                         M Wang,Y Liu,Z Huang.Large Margin Object Tracking with Circulant Feature Maps[J].2017:4800-4808.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_12" title=" Y Wu,J Lim,M H Yang.Online Object Tracking:A Benchmark[C].Computer Vision and Pattern Recognition IEEE,2013:2411-2418." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Online Object Tracking:A Benchmark">
                                        <b>[12]</b>
                                         Y Wu,J Lim,M H Yang.Online Object Tracking:A Benchmark[C].Computer Vision and Pattern Recognition IEEE,2013:2411-2418.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_13" title=" Y Wu,J Lim,M H Yang.Object Tracking Benchmark[J].IEEE Transactions on Pattern Analysis &amp;amp; Machine Intelligence,2015,37(9):1834-1848." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Object tracking benchmark">
                                        <b>[13]</b>
                                         Y Wu,J Lim,M H Yang.Object Tracking Benchmark[J].IEEE Transactions on Pattern Analysis &amp;amp; Machine Intelligence,2015,37(9):1834-1848.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_14" title=" M Danelljan,et al.Adaptive Color Attributes for Real-Time Visual Tracking[C].Computer Vision and Pattern Recognition IEEE,2014:1090-1097." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Adaptive Color Attributes for Real-Time Visual Tracking">
                                        <b>[14]</b>
                                         M Danelljan,et al.Adaptive Color Attributes for Real-Time Visual Tracking[C].Computer Vision and Pattern Recognition IEEE,2014:1090-1097.
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_15" title=" J Zhang,S Ma,S Sclaroff.MEEM:Robust Tracking via Multiple Experts Using Entropy Minimization[C].European Conference on Computer Vision,2014,8694:188-203." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=MEEM:Robust tracking via multiple experts using entropy minimization">
                                        <b>[15]</b>
                                         J Zhang,S Ma,S Sclaroff.MEEM:Robust Tracking via Multiple Experts Using Entropy Minimization[C].European Conference on Computer Vision,2014,8694:188-203.
                                    </a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_16" >
                                        <b>[16]</b>
                                     C Ma,et al.Long-term correlation tracking [C].Computer Vision and Pattern Recognition IEEE,2015:5388-5396.</a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJZ" target="_blank">计算机仿真</a>
                2019,36(09),257-262             </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>自适应空间约束互补偿跟踪器的研究</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%AD%99%E5%9F%B9%E9%92%A6&amp;code=37225229&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc1FTMHdYQkFCcm1KK2ZPTjlDdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">孙培钦</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%88%98%E6%99%94&amp;code=09059543&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc1FTMHdYQkFCcm1KK2ZPTjlDdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">刘晔</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E4%BA%8E%E6%96%BD%E6%B7%BC&amp;code=39908095&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc1FTMHdYQkFCcm1KK2ZPTjlDdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">于施淼</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E8%A5%BF%E5%AE%89%E4%BA%A4%E9%80%9A%E5%A4%A7%E5%AD%A6%E7%94%B5%E6%B0%94%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0189085&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc1FTMHdYQkFCcm1KK2ZPTjlDdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">西安交通大学电气工程学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>目标跟踪器是计算机视觉系统中重要的组成部件,要求在满足实时性的前提具备良好的跟踪能力。在Staple跟踪器基础上,通过分析其存在的缺陷提出高斯加窗和自适应机制两种措施进行改善,将措施与Staple跟踪器进行整合提出自适应空间约束互补偿跟踪器SRA-MCT。通过验证实验,提出的改进措施有效地提高跟踪器面对特定挑战场景时的性能。通过将SRA-MCT跟踪器与多个跟踪器在实验评测集上进行OPE测试,证明提出的SRA-MCT跟踪器不仅满足实时性要求且具有优异的跟踪性能。SRA-MCT跟踪器在实验评测集的AUC数值和准确率(20px)数值相比Staple跟踪器分别高出4.5%和7.4%。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%B7%9F%E8%B8%AA%E5%99%A8&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc1FTMHdYQkFCcm1KK2ZPTjlDdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">跟踪器;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%AB%98%E6%96%AF%E5%8A%A0%E7%AA%97&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc1FTMHdYQkFCcm1KK2ZPTjlDdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">高斯加窗;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%87%AA%E9%80%82%E5%BA%94&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc1FTMHdYQkFCcm1KK2ZPTjlDdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">自适应;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%9B%B8%E5%85%B3%E6%BB%A4%E6%B3%A2&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc1FTMHdYQkFCcm1KK2ZPTjlDdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">相关滤波;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    孙培钦(1992-)，男(汉族)，福建省晋江市，硕士研究生，主要研究领域为计算机视觉;;
                                </span>
                                <span>
                                    刘晔(1962-)，男(汉族)，陕西省宝鸡市，教授，硕士研究生导师，主要研究领域为测控技术与自动化;;
                                </span>
                                <span>
                                    于施淼(1994-)，女(汉族)，黑龙江省鸡西市人，硕士研究生，主要研究领域为测控技术与自动化。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-05-22</p>

            </div>
                    <h1><b>Adaptive Spatially Restricted Mutual Compensation Tracker</b></h1>
                    <h2>
                    <span>SUN Pei-qin</span>
                    <span>LIU Ye</span>
                    <span>YU Shi-miao</span>
            </h2>
                    <h2>
                    <span>School of Electrical Engineering, Xi&apos;an Jiaotong University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Visual target is an important component of the computer vision system and requires good tracking ability on the premise of meeting real-time performance. Based on the tracker Staple, this paper analyzed its existing defects. It was proposed that both Gaussian windowing and adaptive mechanisms improve performance. Combining the measures with the Staple tracker, an adaptive space constraint mutual compensation tracker SRA-MCT was proposed. Through verification experiments, the proposed improvements effectively improved the performance of the Staple when facing specific challenges. Through the OPE test on the evaluation set of the SRA-MCT and multiple trackers, the SRA-MCT not only can meet the real-time requirements but also has excellent tracking performance. The SRA-MCT tracker is 4.5% higher and 7.4% higher than the Staple in the AUC and accuracy(20 px).</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Tracker&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc1FTMHdYQkFCcm1KK2ZPTjlDdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Tracker;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Gaussian%20window&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc1FTMHdYQkFCcm1KK2ZPTjlDdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Gaussian window;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Adaptive&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc1FTMHdYQkFCcm1KK2ZPTjlDdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Adaptive;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Correlation%20filter&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc1FTMHdYQkFCcm1KK2ZPTjlDdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">Correlation filter;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-05-22</p>
                            </div>


        <!--brief start-->
                        <h3 id="35" name="35" class="anchor-tag"><b>1 引言</b></h3>
                <div class="p1">
                    <p id="36">视觉目标跟踪是计算机视觉的核心任务之一,被广泛应用在人机交互、安防监控、自动驾驶、机器人等领域。其任务目标是利用第一帧给定的目标区域对跟踪器进行初始化,令跟踪器在图像序列的后续帧当中准确预估目标的运动轨迹。由于先验知识少且存在许多类似目标变形、尺度变化、光照变化等挑战因素,使得建立一个鲁棒的、实时的跟踪器成为计算机视觉研究领域的难题。</p>
                </div>
                <div class="p1">
                    <p id="37">目前,跟踪器研究主要可以分为两大分支。一是利用深度学习<citation id="84" type="reference"><link href="3" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>的强大特征表示能力获得目标的高层语义特征以实现跟踪器的性能提升,特点是跟踪器的性能表现优异,但实时性差,对计算设备的要求高;另一分支是基于相关滤波的跟踪器,其特点是性能好,实时性好,对计算设备要求低且实现简单。从实用角度出发,实时性是跟踪器得以应用到现实世界的最基本要求。因此,研究基于相关滤波的跟踪器具有更大的应用价值。</p>
                </div>
                <div class="p1">
                    <p id="38">2010年,Bolme等人基于最小化输出误差平方和的优化目标提出MOSSE跟踪器<citation id="85" type="reference"><link href="5" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>,首次将相关滤波应用到目标跟踪领域,开启基于相关滤波的跟踪器的研究道路。Henriques等人提出的CSK跟踪器<citation id="86" type="reference"><link href="7" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>通过周期平移的方式产生大量虚拟负样本解决跟踪器训练样本稀缺的问题,利用数据集矩阵为循环矩阵的结构特点搭起岭回归算法与傅利叶变换之间的桥梁,实现基于快速傅利叶变换的快速训练和快速预测。为克服仅能使用灰度图作为基于相关滤波的跟踪器的输入信号,Henriques等人于2014年提出的KCF跟踪器<citation id="87" type="reference"><link href="9" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>通过将单通道输入信号扩展成多通道形式,使跟踪器性能获得极大的提升。为使基于相关滤波的跟踪器具备尺度预估的能力,李扬等人和Danelljan等人分别提出SAMF跟踪器<citation id="88" type="reference"><link href="11" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>和DSST跟踪器<citation id="89" type="reference"><link href="13" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>。为把距离因素结合到相关滤波器的系数中,Danelljan等人通过将损失函数的L2正则化项替换成空间距离惩罚项提出空间惩罚相关滤波器SRDCF<citation id="90" type="reference"><link href="15" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>。Bertinetto等人发现之前基于相关滤波的跟踪器由于采用梯度方向直方图(Histograms of Oriented Gradients,以下简称HOG)特征<citation id="91" type="reference"><link href="17" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>导致跟踪器在形变挑战中性能很差。为此,Bertinetto等人借助颜色直方图特征具有抗形变能力强的特点对基于HOG特征的相关滤波器进行补偿提出Staple跟踪器<citation id="92" type="reference"><link href="19" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>,在满足实时性前提下实现性能的进一步提升。</p>
                </div>
                <div class="p1">
                    <p id="39">在这篇论文中,通过分析Staple跟踪器存在的不足,提出有效的改进措施用于改善原跟踪器在特定挑战下的性能,并将多个改进措施与原跟踪器进行结合得到一个新跟踪器SRA-MCT。通过详尽的实验,验证改进措施的有效性并证明SRA-MCT具有优秀的性能。</p>
                </div>
                <h3 id="40" name="40" class="anchor-tag"><b>2 Staple跟踪器</b></h3>
                <div class="p1">
                    <p id="41">Staple跟踪器通过解决两个独立的岭回归问题<citation id="93" type="reference"><link href="21" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>分别获得基于HOG特征的响应结果和基于颜色直方图特征的响应结果,采用线性加权的方式对两个响应结果进行合并,使两种特征能通过发挥自身的优势互补各自的劣势,从而提高跟踪器在预测目标位置方面的性能。</p>
                </div>
                <div class="p1">
                    <p id="42">Staple跟踪器按照执行流程可被分为三个阶段,分别是平移预估、尺度预估和模型更新。在平移预估阶段,Staple跟踪器将图像块分别送入基于HOG特征的相关滤波器(以下简称HOG-相关滤波器)和基于颜色直方图的岭回归模型(以下简称颜色-岭回归模型)得到两个模型各自输出的响应结果,采用线性加权的方式将两个响应结果进行合并得到最终响应结果。式(1)描述这一合并过程,式中α为权重值用于调整两个响应结果在最终响应结果的比例,Y<sub>cf</sub>为HOG-相关滤波器的响应结果,Y<sub>hist</sub>为颜色-岭回归模型的响应结果,Y为最终响应结果。Staple跟踪器采用DSST跟踪器的尺度预估方法实现对当前帧目标尺度的预估,即采用一维相关滤波器实现尺度预估。在模型更新阶段,Staple跟踪器采用在线每帧更新的机制,即利用当前帧通过预测得到的目标区域中的目标表观信息对HOG-相关滤波器、颜色-岭回归模型和尺度相关滤波器进行更新。三个模型的更新方式均基于移动平均数的思想,从而保证在一定程度上保留前面帧已习得知识的同时吸纳当前帧提供的新信息。</p>
                </div>
                <div class="p1">
                    <p id="43"><i>Y</i>=(1-<i>α</i>)<i>Y</i><sub><i>cf</i></sub>+<i>αY</i><sub><i>hist</i></sub>      (1)</p>
                </div>
                <h3 id="44" name="44" class="anchor-tag"><b>3 缺陷分析</b></h3>
                <div class="p1">
                    <p id="45">Staple跟踪器虽然性能优异,但设计原理上仍存在易受背景干扰、线性加权合并和每帧更新机制等缺陷,限制跟踪器性能的进一步提高。</p>
                </div>
                <h4 class="anchor-tag" id="46" name="46"><b>3.1 背景干扰</b></h4>
                <div class="p1">
                    <p id="47">Staple跟踪器利用颜色直方图特征抗形变能力强的特点对HOG-相关滤波器进行补偿,却引入易受背景中相似颜色区域干扰的缺陷。图1给出背景干扰示例,跟踪目标为篮球运动员。图中红框表示标注,绿框为DSST跟踪器输出的预测框,蓝框为Staple跟踪器输出的预测框,左上角黄色数字为帧数。观察图1a)所示的第631帧图像,蓝框与红框接近完全重合,而绿框与红框有明显的偏差,表明Staple跟踪器在第631帧的表现要优于DSST跟踪器。观察图1b)所示的第641帧图像,由于周围同队球员的靠近导致Staple跟踪器的输出发生漂移,跟踪目标突变为同队球员,而DSST跟踪器仍保持正确的跟踪目标。由于Staple跟踪器可以被近似地认为是DSST跟踪器和颜色-岭回归模型的组合,所以从上述分析可以推断:在目标周围背景中无相似颜色区域时,Staple跟踪器由于引入颜色-岭回归模型有助于提高跟踪器的输出跟踪框质量;当遇到目标周围背景有相似颜色区域时,Staple跟踪器的输出会因为颜色-岭回归模型的影响易发生漂移。</p>
                </div>
                <div class="area_img" id="48">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJZ201909052_048.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc1FTMHdYQkFCcm1KK2ZPTjlDdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 背景干扰示例" src="Detail/GetImg?filename=images/JSJZ201909052_048.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc1FTMHdYQkFCcm1KK2ZPTjlDdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图1 背景干扰示例</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJZ201909052_048.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc1FTMHdYQkFCcm1KK2ZPTjlDdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="49" name="49"><b>3.2 线性加权</b></h4>
                <div class="p1">
                    <p id="50">Staple跟踪器在平移预估阶段采用线性加权的方式融合HOG-相关滤波器响应结果和颜色-岭回归模型响应结果。但线性加权方式存在明显的缺点,因为每种特征具有自己的优势场景和劣势场景,若在不同场景下均对每种特征对应的响应图分配固定的权重,必然不能达到在优势场景下充分发挥对应特征对于最终响应结果的影响力和在劣势场景下弱化该特征对于最终响应结果的干扰的目的。</p>
                </div>
                <h4 class="anchor-tag" id="51" name="51"><b>3.3 更新机制</b></h4>
                <div class="p1">
                    <p id="52">Staple跟踪器以每帧更新的方式对模型进行更新,即不加区分地认为每一帧预测的目标区域都包含有目标表观的新信息。上述机制在实际应用场景中经常会出现错误,因为目标在运动过程中会经常受到其它物体的遮挡,使目标的表观信息由于受到较大程度的遮挡而急剧减少即预测区域内所含的背景信息大于目标的表观信息。若以发生遮挡的目标区域对模型进行更新,会导致对目标表观信息的建模出现偏差从而产生漂移。图2给出目标受到其它物体遮挡的示例,跟踪目标为墨水盒,集成从450帧到第473帧截取的图像块(从左往右从上到下排列),红框表示目标区域。顺序观察帧序列,发现由于受到游标卡尺的遮挡影响,红框内的目标表观信息随着遮挡程度的增大而减少,而背景信息(游标卡尺)逐渐增多。采用类似的目标区域对模型进行一小段时间的连续更新之后,遮挡物的信息会淹没之前模型已习得的目标表观信息,使跟踪器的跟踪目标变为遮挡物,引起漂移。</p>
                </div>
                <div class="area_img" id="53">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJZ201909052_053.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc1FTMHdYQkFCcm1KK2ZPTjlDdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 遮挡示例" src="Detail/GetImg?filename=images/JSJZ201909052_053.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc1FTMHdYQkFCcm1KK2ZPTjlDdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图2 遮挡示例</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJZ201909052_053.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc1FTMHdYQkFCcm1KK2ZPTjlDdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h3 id="54" name="54" class="anchor-tag"><b>4 改进措施</b></h3>
                <div class="p1">
                    <p id="55">针对存在的背景干扰、线性加权合并和每帧更新机制等不足,提出空间约束、自适应机制等改进措施进行改善。</p>
                </div>
                <h4 class="anchor-tag" id="56" name="56"><b>4.1 空间约束</b></h4>
                <div class="p1">
                    <p id="57">为解决背景中相似颜色区域干扰易引起跟踪器漂移的问题,本文提出在Staple跟踪器的目标似然图(颜色-岭回归模型输出的中间结果,图上每个像素位置对应数值代表该位置属于目标区域的可能性)进行高斯加窗处理以实现空间约束。式(2)描述对目标似然图进行高斯加窗的具体方式。式中<i>X</i>为经高斯加窗的目标似然图;<i>u</i>是位置向量;<i>c</i>为目标似然图的中心位置向量;<i>X</i><sup>′</sup>是原目标似然图;<i>β</i>为权重用于控制空间约束的强弱。通过对目标似然图进行高斯加窗处理,可以强调离上一帧预测位置较近的区域为目标当前帧区域的可能性大于距离较远的区域。</p>
                </div>
                <div class="p1">
                    <p id="58"><mathml id="82"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>X</mi><mo stretchy="false">(</mo><mi>u</mi><mo stretchy="false">)</mo><mo>=</mo><mi>e</mi><msup><mrow></mrow><mrow><mfrac><mrow><mo>-</mo><mi>β</mi><mrow><mo stretchy="false">∥</mo><mi>u</mi><mo>-</mo><mi>c</mi><mo stretchy="false">∥</mo></mrow><msup><mrow></mrow><mn>2</mn></msup></mrow><mrow><mn>2</mn><mi>σ</mi><msup><mrow></mrow><mn>2</mn></msup></mrow></mfrac></mrow></msup><mi>X</mi><msup><mrow></mrow><mo>´</mo></msup><mo stretchy="false">(</mo><mi>u</mi><mo stretchy="false">)</mo></mrow></math></mathml>      (2)</p>
                </div>
                <h4 class="anchor-tag" id="59" name="59"><b>4.2 自适应机制</b></h4>
                <div class="p1">
                    <p id="60">提出的自适应机制包括自适应合并和自适应阈值更新两个措施,分别用于解决3.2节和3.3节分析的缺陷。为量化HOG-相关滤波器响应图的可信程度,引入平均峰值相关能量(Average Peak-to-Correlation Energy,APCE)<citation id="94" type="reference"><link href="23" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>。式(3)给出<i>APCE</i>的计算公式,式中<i>Y</i><sub>max</sub>和<i>Y</i><sub>min</sub>分别表示响应图的全局最大值和全局最小值,<i>M</i>和<i>N</i>分别为响应图的高和宽。当相关滤波器的响应图有尖锐峰值且周围无相近的干扰峰值时,响应结果可信度高对应<i>APCE</i>数值大;当响应图中有多个数值较为接近的峰值时,响应结果可信度低对应<i>APCE</i>数值低。图3a)和图3b)分别给出可信度高和可信度低的响应图示例。</p>
                </div>
                <div class="p1">
                    <p id="61"><mathml id="83"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>A</mi><mi>Ρ</mi><mi>C</mi><mi>E</mi><mo>=</mo><mfrac><mrow><mrow><mrow><mo>|</mo><mrow><mi>Y</mi><msub><mrow></mrow><mrow><mi>max</mi></mrow></msub><mo>-</mo><mi>Y</mi><msub><mrow></mrow><mrow><mi>min</mi></mrow></msub></mrow><mo>|</mo></mrow></mrow><msup><mrow></mrow><mn>2</mn></msup></mrow><mrow><mfrac><mn>1</mn><mrow><mi>Μ</mi><mi>Ν</mi></mrow></mfrac><mo stretchy="false">(</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>u</mi><mo>∈</mo><mi>Μ</mi><mo>×</mo><mi>Ν</mi></mrow></munder><mi>Y</mi></mstyle><mo stretchy="false">(</mo><mi>u</mi><mo stretchy="false">)</mo><mo>-</mo><mi>Y</mi><msub><mrow></mrow><mrow><mi>min</mi></mrow></msub><mo stretchy="false">)</mo></mrow></mfrac></mrow></math></mathml>      (3)</p>
                </div>
                <div class="area_img" id="62">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJZ201909052_062.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc1FTMHdYQkFCcm1KK2ZPTjlDdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 响应图示例" src="Detail/GetImg?filename=images/JSJZ201909052_062.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc1FTMHdYQkFCcm1KK2ZPTjlDdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图3 响应图示例</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJZ201909052_062.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc1FTMHdYQkFCcm1KK2ZPTjlDdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="63">利用对数函数在自变量小于底数时因变量下降速度快而在自变量大于底数时因变量上升速度慢的特点,提出在式(1)中增加随<i>APCE</i>数值发生变化的系数以改变<i>HOG</i>-相关滤波器响应结果在最终响应结果的占比,实现自适应地根据响应图的可信程度来决定对最终响应结果的影响。式(4)描述提出的自适应合并措施,式中δ为可信度阈值。</p>
                </div>
                <div class="p1">
                    <p id="64">Y=(1-α)<i>log</i><sub>δ</sub>(APCE)Y<sub>cf</sub>+αY<sub>hist</sub>       (4)</p>
                </div>
                <div class="p1">
                    <p id="65">在APCE的基础上提出自适应阈值更新措施,使跟踪器具备根据当前帧预测目标区域的可信程度选择是否对模型进行更新的能力。设有一图像序列,累加序列前n帧(n在文中取20)的APCE值,在第n帧时刻对累加和取平均得到基准阈值φ。在后续帧跟踪器进行更新时,令权重值φ(小于1)与φ相乘得到更新阈值φ<sup>′</sup>。若当前帧的APCE小于φ<sup>′</sup>,模型不更新,反之则更新。采用移动平均数的方式使基准阈值具备自我学习的能力,式(5)给出基准阈值的学习公式,式中η为学习率。为避免基准阈值过高影响模型适应目标表观变化的能力,需设定学习上界(本文取30)。</p>
                </div>
                <div class="p1">
                    <p id="66">φ<sub>t</sub>=(1-η)φ<sub>t-1</sub>+ηAPCE      (5)</p>
                </div>
                <h3 id="67" name="67" class="anchor-tag"><b>5 实验</b></h3>
                <div class="p1">
                    <p id="68">在OTB评测集<citation id="96" type="reference"><link href="25" rel="bibliography" /><link href="27" rel="bibliography" /><sup>[<a class="sup">12</a>,<a class="sup">13</a>]</sup></citation>基础上构建实验评测集,利用评测集进行验证实验和对比实验。本节实验涉及的算法均采用MATLAB进行实现。运行环境:CPU Intel Xeon E3-1231v3@3.4GHz, 无GPU支持。所有算法的运行速度均可满足实时性(&gt;15FPS),SRA-MCT的速度为43fps,Staple的速度为54fps。实验一致采用成功率曲线下的面积值(the Area Under Curve,以下简称AUC)和像素距离阈值为20px对应的百分比数值(以下用准确率(20px)代替)分别度量跟踪器的成功率和准确率<citation id="95" type="reference"><link href="25" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>。</p>
                </div>
                <h4 class="anchor-tag" id="69" name="69"><b>5.1 实验评测集</b></h4>
                <div class="p1">
                    <p id="70">采用OTB50<citation id="97" type="reference"><link href="25" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>为实验评测集的主要组成部分,由于OTB50提出年限(2013)距今较远,导致涉及部分分辨率过低的图像序列,而过低分辨率的图像随摄像头的发展不再符合当前的应用场合。因此为了更聚集于主要挑战因素<citation id="98" type="reference"><link href="25" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>,去除OTB50评测集中目标分辨率过低的图像序列并补充9个来自OTB100<citation id="99" type="reference"><link href="27" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>的图片序列,组成含有49个图片序列50个不同跟踪目标共28279帧图片的实验评测集。表1给出实验评测集的挑战分布情况。图4给出每个图片序列首帧,红框标识跟踪目标,若序列存在两个跟踪目标则用蓝框标识另一个跟踪目标。</p>
                </div>
                <div class="area_img" id="71">
                    <p class="img_tit"><b>表1 实验评测集的挑战分布</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="71" border="1"><tr><td><br />挑战</td><td>IV</td><td>OPR</td><td>SV</td><td>OCC</td><td>DEF</td><td>MB</td><td>FM</td><td>IPR</td><td>OV</td><td>BC</td></tr><tr><td><br />数量</td><td>21</td><td>34</td><td>36</td><td>29</td><td>27</td><td>16</td><td>23</td><td>28</td><td>8</td><td>16</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="72">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJZ201909052_072.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc1FTMHdYQkFCcm1KK2ZPTjlDdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 实验评测集序列首帧集" src="Detail/GetImg?filename=images/JSJZ201909052_072.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc1FTMHdYQkFCcm1KK2ZPTjlDdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图4 实验评测集序列首帧集</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJZ201909052_072.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc1FTMHdYQkFCcm1KK2ZPTjlDdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="73" name="73"><b>5.2 验证实验</b></h4>
                <div class="p1">
                    <p id="74">分别实现带高斯加窗的改进跟踪器和带自适应机制的改进跟踪器及SRA-MCT跟踪器,并与Staple跟踪器一起在实验评测集上进行OPE测试。表2、表3分别给出各个跟踪器在不同挑战因素对应评测子集和实验评测集上的AUC数值和准确率(20px)数值,其中红色为第一名,粉红色为第二名,蓝色为第三名,绿色为第四名。从表2和表3中背景混杂(BC)评测子集的结果可以看出,通过采用高斯加窗实现空间约束可有效地提高跟踪器的抗背景干扰能力,在AUC数值和准确率(20px)数值方面相比Staple跟踪器分别提高4.9%和5.5%。通过表2和表3中光照变化(IV)、运动模糊(MB)和目标形变(DEF)三个评测子集的结果看出,采用自适应机制可进一步发挥HOG特征和颜色直方图特征在优势场景下的影响力并弱化在劣势场景中的干扰。观察表2和表3中遮挡(OCC)评测子集的结果,发现由于自适应机制的引入可有效地解决跟踪器在遮挡场景中易产生跟踪目标突变的问题,在AUC数值和准确率(20px)数值方面相比Staple跟踪器分别提高5.7%和7.2%。观察表2和表3的第一行与第二、三列的数值可以发现,通过将两个改进措施进行整合可以得到一个更优的跟踪器SRA-MCT,表明两个改进措施之间耦合程度低且能互相促进,在评测集的AUC数值和准确率(20px)数值方面相比Staple跟踪器分别提高4.5%和7.4%。</p>
                </div>
                <div class="area_img" id="75">
                    <p class="img_tit"><b>表2</b> AUC<b>对比表</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="75" border="1"><tr><td><br /></td><td>SRA-MCT</td><td>自适应</td><td>高斯加窗</td><td>Staple</td></tr><tr><td><br />IV</td><td>0.655</td><td>0.624</td><td>0.625</td><td>0.592</td></tr><tr><td><br />OCC</td><td>0.585</td><td>0.592</td><td>0.544</td><td>0.535</td></tr><tr><td><br />DEF</td><td>0.538</td><td>0.538</td><td>0.503</td><td>0.513</td></tr><tr><td><br />MB</td><td>0.603</td><td>0.596</td><td>0.559</td><td>0.508</td></tr><tr><td><br />BC</td><td>0.612</td><td>0.569</td><td>0.565</td><td>0.516</td></tr><tr><td><br />ALL</td><td>0.586</td><td>0.579</td><td>0.568</td><td>0.541</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="76">
                    <p class="img_tit"><b>表3 准确率(20</b>)px<b>对比表</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="76" border="1"><tr><td><br /></td><td>SRA-MCT</td><td>自适应</td><td>高斯加窗</td><td>Staple</td></tr><tr><td><br />IV</td><td>0.869</td><td>0.843</td><td>0.824</td><td>0.780</td></tr><tr><td><br />OCC</td><td>0.775</td><td>0.773</td><td>0.706</td><td>0.701</td></tr><tr><td><br />DEF</td><td>0.738</td><td>0.723</td><td>0.674</td><td>0.698</td></tr><tr><td><br />MB</td><td>0.817</td><td>0.788</td><td>0.752</td><td>0.678</td></tr><tr><td><br />BC</td><td>0.804</td><td>0.770</td><td>0.728</td><td>0.673</td></tr><tr><td><br />ALL</td><td>0.794</td><td>0.772</td><td>0.757</td><td>0.720</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="77" name="77"><b>5.3 对比实验</b></h4>
                <div class="p1">
                    <p id="78">将提出的SRA-MCT跟踪器与性能优且满足实时性的跟踪器包括有CNT跟踪器<citation id="100" type="reference"><link href="29" rel="bibliography" /><sup>[<a class="sup">14</a>]</sup></citation>、KCF跟踪器、DSST跟踪器、MEEM跟踪器<citation id="101" type="reference"><link href="31" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>、LCT跟踪器<citation id="102" type="reference"><link href="33" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>在实验评测集上进行OPE测试。图5a)和图5b)分别给出实验得到的成功率曲线和准确率曲线,图5a)中的图例数值表示AUC,图5b中的图例数值表示准确率(20px)。观察图5a),SRA-MCT跟踪器比其它跟踪器在IOU阈值大于30%的区间内表现更好,IOU阈值小于30%区间内与MEEM趋近于并列,在AUC数值方面比第二名高出5.6%。观察图5b),SRA-MCT跟踪器在各个中心距离阈值对应的准确率数值几乎均为最高数值,且在中心距离阈值大于10px的区间内下降速度相比于其它跟踪器更慢,表明跟踪器的预测位置与标注之间误差普通在10px左右,体现跟踪器的预测位置精准度高,在准确率(20px)方面比第二名高出5.1%。通过对比实验,可以证明本文提出的SRA-MCT跟踪器在满足实时性的同时具备优异的跟踪性能。</p>
                </div>
                <div class="area_img" id="79">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJZ201909052_079.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc1FTMHdYQkFCcm1KK2ZPTjlDdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 OPE测试结果" src="Detail/GetImg?filename=images/JSJZ201909052_079.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc1FTMHdYQkFCcm1KK2ZPTjlDdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图5</b> OPE<b>测试结果</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJZ201909052_079.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc1FTMHdYQkFCcm1KK2ZPTjlDdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h3 id="80" name="80" class="anchor-tag"><b>6 总结</b></h3>
                <div class="p1">
                    <p id="81">本文以基于相关滤波的Staple跟踪器为研究对象,通过分析得到Staple跟踪器存在易受背景相似颜色区域干扰、线性合并方式无法充分发挥HOG特征和颜色直方图特征的各自优势、每帧更新机制容易导致跟踪漂移等缺陷。针对上述缺陷,分别提出高斯加窗主自适应机制进行解决,通过将两种措施与Staple跟踪器进行整合提出新跟踪器SRKA-MCT。通过验证实验,表明高斯加窗措施可有效解决跟踪器在背景混杂场合下易发生漂移的问题以及采用自适应机制可进一步发挥不同特征的优势并弱化干扰同时解决由更新机制引起的漂移现象。通过对比实验,表明提出的SRA-MCT跟踪器不仅满足实时性要求且具有优异的跟踪性能。</p>
                </div>
                <div class="area_img" id="103">
                                <img alt="" src="Detail/GetImg?filename=images/JSJZ201909052_10300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc1FTMHdYQkFCcm1KK2ZPTjlDdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                            <p class="img_tit"></p>

                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep learning">

                                <b>[1]</b> Y Lecun,Y Bengio,G Hinton.Deep learning [J].Nature,2015,521(7553):436.
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Visual object tracking using adaptive correlation filters">

                                <b>[2]</b> D S Bolme,et al.Visual object tracking using adaptive correlation filters[C].Computer Vision and Pattern Recognition IEEE,2010:2544-2550.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Exploiting the circulant structure of tracking-by-detection with kernels">

                                <b>[3]</b> J F Henriques,et al.Exploiting the Circulant Structure of Tracking-by-Detection with Kernels[C].European Conference on Computer Vision,2012:702-715.
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=High-speed tracking with kernelized correlation filters">

                                <b>[4]</b> J F Henriques,et al.High-Speed Tracking with Kernelized Correlation Filters[J].IEEE Transactions on Pattern Analysis &amp; Machine Intelligence,2014,37(3):583-596.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A Scale Adaptive Kernel Correlation Filter Tracker with Feature Integration">

                                <b>[5]</b> Y Li,J Zhu.A Scale Adaptive Kernel Correlation Filter Tracker with Feature Integration[J].2014,8926:254-265.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Accurate Scale Estimation for Robust Visual Tracking">

                                <b>[6]</b> M Danelljan,G Häger,F S Khan.Accurate scale estimation for robust visual tracking[C].British Machine Vision Conference,2014:61.1-65.11.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning spatially regularized correlation filters for visual tracking">

                                <b>[7]</b> M Danelljan,et al.Learning Spatially Regularized Correlation Filters for Visual Tracking[C].IEEE International Conference on Computer Vision,2015:4310-4318.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Object Detection with Discriminatively Trained Part-Based Models">

                                <b>[8]</b> P F Felzenszwalb,et al.Object detection with discriminatively trained part-based models[J].IEEE Transactions on Pattern Analysis &amp; Machine Intelligence,2014,47(2):6-7.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Staple:Complementary learners for real-time tracking">

                                <b>[9]</b> L Bertinetto,et al.Staple:Complementary Learners for Real-Time Tracking[C].Computer Vision and Pattern Recognition IEEE,2016:1401-1409.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_10" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CBBD&amp;filename=9787302423287000&amp;v=MDA2NzQ3bjN4RTlmYnZuS3JpZlplWnZGeW5qVTc3SkpGNFNYRnF6R2JDNEhOWE9ySTFOWStzUERCTTh6eFVTbURkOVNI&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc1FTMHdYQkFCcm1KK2ZPTjlDdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[10]</b> 周志华.机器学习[M].清华大学出版社,2016:53-56.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Large Margin Object Tracking with Circulant Feature Maps">

                                <b>[11]</b> M Wang,Y Liu,Z Huang.Large Margin Object Tracking with Circulant Feature Maps[J].2017:4800-4808.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Online Object Tracking:A Benchmark">

                                <b>[12]</b> Y Wu,J Lim,M H Yang.Online Object Tracking:A Benchmark[C].Computer Vision and Pattern Recognition IEEE,2013:2411-2418.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Object tracking benchmark">

                                <b>[13]</b> Y Wu,J Lim,M H Yang.Object Tracking Benchmark[J].IEEE Transactions on Pattern Analysis &amp; Machine Intelligence,2015,37(9):1834-1848.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Adaptive Color Attributes for Real-Time Visual Tracking">

                                <b>[14]</b> M Danelljan,et al.Adaptive Color Attributes for Real-Time Visual Tracking[C].Computer Vision and Pattern Recognition IEEE,2014:1090-1097.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_15" target="_blank" href="http://scholar.cnki.net/result.aspx?q=MEEM:Robust tracking via multiple experts using entropy minimization">

                                <b>[15]</b> J Zhang,S Ma,S Sclaroff.MEEM:Robust Tracking via Multiple Experts Using Entropy Minimization[C].European Conference on Computer Vision,2014,8694:188-203.
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_16" >
                                    <b>[16]</b>
                                 C Ma,et al.Long-term correlation tracking [C].Computer Vision and Pattern Recognition IEEE,2015:5388-5396.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJZ201909052" />
        <input id="dpi" type="hidden" value="400" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc1FTMHdYQkFCcm1KK2ZPTjlDdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJZ201909052&amp;v=Mjk2NTNvOUFab1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1Wm9GeW5rVjcvT0x6N0JkTEc0SDlqTXA=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDMyc1FTMHdYQkFCcm1KK2ZPTjlDdz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=842_VlRz_g1CMCLi-M0twVw-4EOFG_noWyi_OsMnrAg1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
