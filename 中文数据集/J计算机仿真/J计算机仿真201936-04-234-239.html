<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=NOOK-mvTMnxl11s6oPYV0boPXN3iP4N7Rc-A56nk4KI1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2fKXReader%2fDetail%3fTIMESTAMP%3d637141815012787500%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJSJZ201904049%26RESULT%3d1%26SIGN%3dvz99lfpoHsbN%252b3UlQJiHEjHN6sU%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJZ201904049&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJZ201904049&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJZ201904049&amp;v=MDkzMjBESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVabUZ5emdWNzdJTHo3QmRMRzRIOWpNcTQ5QmJZUUs=&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#45" data-title="&lt;b&gt;1 引言&lt;/b&gt; "><b>1 引言</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#50" data-title="&lt;b&gt;2 受限玻尔兹曼机&lt;/b&gt; "><b>2 受限玻尔兹曼机</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#51" data-title="&lt;b&gt;2.1 RBM的基本模型&lt;/b&gt;"><b>2.1 RBM的基本模型</b></a></li>
                                                <li><a href="#66" data-title="&lt;b&gt;2.2 RBM的快速学习算法&lt;/b&gt;"><b>2.2 RBM的快速学习算法</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#79" data-title="&lt;b&gt;3 基于光滑L&lt;/b&gt;&lt;sub&gt;&lt;b&gt;0&lt;/b&gt;&lt;/sub&gt;&lt;b&gt;范数的RBM&lt;/b&gt; "><b>3 基于光滑L</b><sub><b>0</b></sub><b>范数的RBM</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#102" data-title="&lt;b&gt;4 实验分析&lt;/b&gt; "><b>4 实验分析</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#106" data-title="&lt;b&gt;4.1 表示的稀疏性比较&lt;/b&gt;"><b>4.1 表示的稀疏性比较</b></a></li>
                                                <li><a href="#114" data-title="&lt;b&gt;4.2 可视化提取特征的评价&lt;/b&gt;"><b>4.2 可视化提取特征的评价</b></a></li>
                                                <li><a href="#120" data-title="&lt;b&gt;4.3 表示判别力的比较&lt;/b&gt;"><b>4.3 表示判别力的比较</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#126" data-title="&lt;b&gt;5 结论&lt;/b&gt; "><b>5 结论</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#54" data-title="&lt;b&gt;图1 RBM的结构示意图&lt;/b&gt;"><b>图1 RBM的结构示意图</b></a></li>
                                                <li><a href="#86" data-title="&lt;b&gt;图2 Smooth L&lt;/b&gt;&lt;sub&gt;&lt;b&gt;0&lt;/b&gt;&lt;/sub&gt;&lt;b&gt;范数比L&lt;/b&gt;&lt;sub&gt;&lt;b&gt;1&lt;/b&gt;&lt;/sub&gt;&lt;b&gt;范数更好的近似L&lt;/b&gt;&lt;sub&gt;&lt;b&gt;0&lt;/b&gt;&lt;/sub&gt;&lt;b&gt;范数&lt;/b&gt;"><b>图2 Smooth L</b><sub><b>0</b></sub><b>范数比L</b><sub><b>1</b></sub><b>范数更好的近似L</b><sub><b>0</b></sub><b>范数</b></a></li>
                                                <li><a href="#105" data-title="&lt;b&gt;图3 MNIST数据集的样例&lt;/b&gt;"><b>图3 MNIST数据集的样例</b></a></li>
                                                <li><a href="#108" data-title="&lt;b&gt;图4 隐单元的激活概率情况&lt;/b&gt;"><b>图4 隐单元的激活概率情况</b></a></li>
                                                <li><a href="#113" data-title="&lt;b&gt;表1 不同模型学习到表示的稀疏度&lt;/b&gt;"><b>表1 不同模型学习到表示的稀疏度</b></a></li>
                                                <li><a href="#150" data-title="图5 学习特征的可视化">图5 学习特征的可视化</a></li>
                                                <li><a href="#150" data-title="图5 学习特征的可视化">图5 学习特征的可视化</a></li>
                                                <li><a href="#123" data-title="&lt;b&gt;表2 100个样本时各模型的分类精度&lt;/b&gt;"><b>表2 100个样本时各模型的分类精度</b></a></li>
                                                <li><a href="#124" data-title="&lt;b&gt;表3 500个样本时各模型的分类精度&lt;/b&gt;"><b>表3 500个样本时各模型的分类精度</b></a></li>
                                                <li><a href="#125" data-title="&lt;b&gt;表4 1000个样本时各模型的分类精度&lt;/b&gt;"><b>表4 1000个样本时各模型的分类精度</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" title=" Y Bengio, O Delalleau.On the expressive power of deep architectures[C].Proceedings of the 14th International Conference on Discovery Science, 2011:18-36" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=On the expressive power of deep architectures">
                                        <b>[1]</b>
                                         Y Bengio, O Delalleau.On the expressive power of deep architectures[C].Proceedings of the 14th International Conference on Discovery Science, 2011:18-36
                                    </a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" title=" G E Hinton, S Osinder.A fast learning algorithm for deep belief nets[J].Neural Computation, 2006, 18 (7) :1527-1554." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJBK&amp;filename=SJBK15090500012538&amp;v=MDQzNDRKRnNRYXhJPU5pZkpaYks5SHRqTXFvOUZaT29OQ1g4eG9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaW5sVXJmSQ==&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!">
                                        <b>[2]</b>
                                         G E Hinton, S Osinder.A fast learning algorithm for deep belief nets[J].Neural Computation, 2006, 18 (7) :1527-1554.
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title=" R Salakhutdinov, G E Hinton.Semantic hashing[J].International Journal of Approximate Reasoning, 2009, 50 (7) :969-978" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012100143198&amp;v=MjE5NTJVcmZJSkZzUWF4ST1OaWZPZmJLN0h0RE9ybzlGWmU4TURYVXhvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp0RmlubA==&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!">
                                        <b>[3]</b>
                                         R Salakhutdinov, G E Hinton.Semantic hashing[J].International Journal of Approximate Reasoning, 2009, 50 (7) :969-978
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" title=" R Salakhutdinov, G E Hinton.Reducing the dimensionality of data with neural networks[J].Science, 2006, 313 (5786) :504-507." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Reducing the Dimensionality of Data with Neural Networks">
                                        <b>[4]</b>
                                         R Salakhutdinov, G E Hinton.Reducing the dimensionality of data with neural networks[J].Science, 2006, 313 (5786) :504-507.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" title=" D H Hubel, T H Wiesel.Receptive fields of single neurones in the cats striate cortex[J].Journal of Physiol, 1959, 148 (3) :574-591." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Receptive fields of single neurons in the cat&amp;#39;s striate cortex">
                                        <b>[5]</b>
                                         D H Hubel, T H Wiesel.Receptive fields of single neurones in the cats striate cortex[J].Journal of Physiol, 1959, 148 (3) :574-591.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" title=" B A Olshau, D J Field.Sparse coding of sensory inputs[J].Current Opinion in Neurobiol, 2004, 14 (4) :481-487" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501611081&amp;v=MDAzNjdUNlQ0UFFIL2lyUmRHZXJxUVRNbndaZVp0RmlubFVyZklKRnNRYXhJPU5pZk9mYks3SHRETnFvOUVZdW9PREhRNG9CTQ==&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!">
                                        <b>[6]</b>
                                         B A Olshau, D J Field.Sparse coding of sensory inputs[J].Current Opinion in Neurobiol, 2004, 14 (4) :481-487
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" title=" L Mancera, J Portilla.L0-norm-based sparse representation through alternate projections[C].Proceedings of IEEE International Conference on Image Processing, 2006:2089-2092" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=L0-norm-based sparse representation through alternate projections">
                                        <b>[7]</b>
                                         L Mancera, J Portilla.L0-norm-based sparse representation through alternate projections[C].Proceedings of IEEE International Conference on Image Processing, 2006:2089-2092
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" title=" H Lee, C Ekanadham, A Y Ng.Sparse deep belief net model for visual area V2[C].Advances in Neural Information Processing Systems, 2007:873-880" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Sparse deep belief net model for visual area v2">
                                        <b>[8]</b>
                                         H Lee, C Ekanadham, A Y Ng.Sparse deep belief net model for visual area V2[C].Advances in Neural Information Processing Systems, 2007:873-880
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" title=" 罗恒.基于协同过滤视角的受限玻尔兹曼机研究[D].上海交通大学, 2011" target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CDFD&amp;filename=1011298181.nh&amp;v=MDEwMTVnVjc3SVZGMjZIN0d4RnRERXJwRWJQSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1Wm1GeXo=&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!">
                                        <b>[9]</b>
                                         罗恒.基于协同过滤视角的受限玻尔兹曼机研究[D].上海交通大学, 2011
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_10" title=" N N Ji, et al.A sparse-response deep belief network based on rate distortion theory[J].Pattern Recognition, 2014, 47 (9) :3179-3191." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14061600062033&amp;v=MjA2Mjc2b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcmZJSkZzUWF4ST1OaWZPZmJLOEh0Zk5xWTlGWk8wTkRIOA==&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!">
                                        <b>[10]</b>
                                         N N Ji, et al.A sparse-response deep belief network based on rate distortion theory[J].Pattern Recognition, 2014, 47 (9) :3179-3191.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_11" title=" D L Donoho.Compressed sensing[J].IEEE Transactions on Information Theory, 2006, 52 (4) :1289-1306." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Compressed sensing">
                                        <b>[11]</b>
                                         D L Donoho.Compressed sensing[J].IEEE Transactions on Information Theory, 2006, 52 (4) :1289-1306.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_12" title=" L Mancera, J Portilla.L0-norm-based sparse representation through alternate projections[C].Proceedings of the IEEE Conference on Image Processing, 2006:2089-2092." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=L0-norm-based sparse representation through alternate projections">
                                        <b>[12]</b>
                                         L Mancera, J Portilla.L0-norm-based sparse representation through alternate projections[C].Proceedings of the IEEE Conference on Image Processing, 2006:2089-2092.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_13" title=" B K Natarajan.Sparse approximate solutions to linear systems[J].Siam Journal on Computing, 1995, (2) :227-234." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Sparse approximate solutions to linear systems">
                                        <b>[13]</b>
                                         B K Natarajan.Sparse approximate solutions to linear systems[J].Siam Journal on Computing, 1995, (2) :227-234.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_14" title=" H Mohimani, M Babaie-Zadeh, C Jutten.A fast approch for over-complete sparse decomposition based on smoothed &lt;i&gt;L&lt;/i&gt;&lt;sub&gt;0&lt;/sub&gt; Norm[J].IEEE Transactions on Signal Processing, 2009, 57 (1) :289-301" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A fast approach for overcomplete sparse decomposition based on smoothed ?&amp;lt;sup&amp;gt;0&amp;lt;/sup&amp;gt; norm">
                                        <b>[14]</b>
                                         H Mohimani, M Babaie-Zadeh, C Jutten.A fast approch for over-complete sparse decomposition based on smoothed &lt;i&gt;L&lt;/i&gt;&lt;sub&gt;0&lt;/sub&gt; Norm[J].IEEE Transactions on Signal Processing, 2009, 57 (1) :289-301
                                    </a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_15" title=" 王宏志, 王贤龙, 周婷婷.基于光滑L&lt;sub&gt;0&lt;/sub&gt;范数的图像分块压缩感知恢复算法[J].吉林大学学报, 2015, 45 (1) :323-327." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JLGY201501047&amp;v=MzI1OTlLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1Wm1GeXpnVjc3SUx5SE1kN0c0SDlUTXJvOUJZNFE=&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!">
                                        <b>[15]</b>
                                         王宏志, 王贤龙, 周婷婷.基于光滑L&lt;sub&gt;0&lt;/sub&gt;范数的图像分块压缩感知恢复算法[J].吉林大学学报, 2015, 45 (1) :323-327.
                                    </a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_16" title=" 张春霞, 姬楠楠, 王冠伟.受限玻尔兹曼机[J].工程数学学报, 2015, 32 (3) :160-176." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GCSX201502001&amp;v=MTY4MThRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVptRnl6Z1Y3N0lJaTdZZHJHNEg5VE1yWTlGWlk=&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!">
                                        <b>[16]</b>
                                         张春霞, 姬楠楠, 王冠伟.受限玻尔兹曼机[J].工程数学学报, 2015, 32 (3) :160-176.
                                    </a>
                                </li>
                                <li id="35">


                                    <a id="bibliography_17" title=" G E Hinton.Training products of experts by minimizing contrastive divergence[J].Neural Computation, 2002, 14 (8) :1771-1800." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJBK&amp;filename=SJBK15090500012097&amp;v=MTA1ODd3WmVadEZpbmxVcmZJSkZzUWF4ST1OaWZKWmJLOUh0ak1xbzlGWk9vTkRIVStvQk1UNlQ0UFFIL2lyUmRHZXJxUVRNbg==&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!">
                                        <b>[17]</b>
                                         G E Hinton.Training products of experts by minimizing contrastive divergence[J].Neural Computation, 2002, 14 (8) :1771-1800.
                                    </a>
                                </li>
                                <li id="37">


                                    <a id="bibliography_18" title=" N N Ji, et al.Enhancing performance of restricted boltzmann machines via log-sum regularization[J].Knowledge-Based Systems, 2014 63 (1) :82-96." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14061700208630&amp;v=MjIwNDhadXNIQ244NW9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaW5sVXJmSUpGc1FheEk9TmlmT2ZiSzhIdGZOcUk5Rg==&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!">
                                        <b>[18]</b>
                                         N N Ji, et al.Enhancing performance of restricted boltzmann machines via log-sum regularization[J].Knowledge-Based Systems, 2014 63 (1) :82-96.
                                    </a>
                                </li>
                                <li id="39">


                                    <a id="bibliography_19" title=" G E Hinton.A practical guide to training restricted boltzmann machines[R].Montreal:Department of Computer Science, University of Toronto, 2010." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A practical guide to training restricted">
                                        <b>[19]</b>
                                         G E Hinton.A practical guide to training restricted boltzmann machines[R].Montreal:Department of Computer Science, University of Toronto, 2010.
                                    </a>
                                </li>
                                <li id="41">


                                    <a id="bibliography_20" title=" M Ranzato, et al.Efficient learning of sparse representations with an energy-based model[J].Proceedings of Advances in Neural Information Processing Systems, 2006:1137-1144" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Efficient learning of sparse representationswith an energy-based model&amp;quot;">
                                        <b>[20]</b>
                                         M Ranzato, et al.Efficient learning of sparse representations with an energy-based model[J].Proceedings of Advances in Neural Information Processing Systems, 2006:1137-1144
                                    </a>
                                </li>
                                <li id="43">


                                    <a id="bibliography_21" title=" P O Hoyer.Non-negative matrix factorization with sparseness constraints[J].Journal of Machine Learning Research, 2004, 5 (1) :1457-1469" target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Non-negative matrix factorization with sparseness constraints">
                                        <b>[21]</b>
                                         P O Hoyer.Non-negative matrix factorization with sparseness constraints[J].Journal of Machine Learning Research, 2004, 5 (1) :1457-1469
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJZ" target="_blank">计算机仿真</a>
                2019,36(04),234-239             </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm">一种新的基于光滑L<sub>0</sub>范数的受限玻尔兹曼机</span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%83%91%E5%BC%BA&amp;code=38537451&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!" target="_blank">郑强</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%A7%AC%E6%A5%A0%E6%A5%A0&amp;code=23307115&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!" target="_blank">姬楠楠</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%82%96%E7%8E%89%E6%9F%B1&amp;code=27508691&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!" target="_blank">肖玉柱</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%AE%8B%E5%AD%A6%E5%8A%9B&amp;code=23593269&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!" target="_blank">宋学力</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E9%95%BF%E5%AE%89%E5%A4%A7%E5%AD%A6%E7%90%86%E5%AD%A6%E9%99%A2&amp;code=0054413&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!" target="_blank">长安大学理学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>受限玻尔兹曼机 (Restricted Boltzmann machines, RBM) 是一种有效的特征提取器, 它是深度信念网络的基本组成模块。为了进一步提升RBM的数据表示性能, 受人类大脑视觉稀疏表示启发, 提出一种新的稀疏RBM, 即SmoothRBM。它通过添加一个光滑L<sub>0</sub>范数的正则项来直接约束隐层单元的总体激活概率, 可以根据不同的学习任务学习到不同的稀疏水平。MNIST数据集上的相关实验表明, SmoothRBM模型与当前的一些优秀模型相比, 可以更有效的提取数据集中的特征信息, 学习到更稀疏和更具判别力的表示形式。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8F%97%E9%99%90%E7%8E%BB%E5%B0%94%E5%85%B9%E6%9B%BC%E6%9C%BA&amp;code=&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!" target="_blank">受限玻尔兹曼机;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%A8%80%E7%96%8F%E8%A1%A8%E7%A4%BA&amp;code=&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!" target="_blank">稀疏表示;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%BF%80%E6%B4%BB%E6%A6%82%E7%8E%87&amp;code=&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!" target="_blank">激活概率;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%89%B9%E5%BE%81%E4%BF%A1%E6%81%AF&amp;code=&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!" target="_blank">特征信息;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    郑强 (1992-) , 男 (汉族) , 河南省商丘市人, 硕士研究生, 主要研究领域为:深度学习。;
                                </span>
                                <span>
                                    姬楠楠 (1985-) , 女 (汉族) , 陕西省渭南市人, 博士, 讲师, 主要研究领域为:深度学习, 稀疏信息处理等。;
                                </span>
                                <span>
                                    肖玉柱 (1980-) , 男 (汉族) , 辽宁省大石桥市人, 博士, 副教授, 主要研究领域为:人工神经网络, 复杂网络等。;
                                </span>
                                <span>
                                    宋学力 (1980-) , 男 (汉族) , 河南省开封市人, 博士, 教授, 博士研究生导师, 主要研究领域为:深度学习, 统计过程控制等。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-07-27</p>

                    <p>

                            <b>基金：</b>
                                                        <span>中央高校基本科研业务费专项资金资助项目 (310812163504);</span>
                    </p>
            </div>
                    <h1><b>A New Restricted Boltzmann Machine Based on Smooth <i>L</i></b><sub>0</sub><b>Norm</b></h1>
                    <h2>
                    <span>ZHENG Qiang</span>
                    <span>JI Nan-nan</span>
                    <span>XIAO Yu-zhu</span>
                    <span>SONG Xue-li</span>
            </h2>
                    <h2>
                    <span>School of Science, Chang'an University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>It is known that Restricted Boltzmann machines (RBM) can be used as an effective feature extractor and building blocks of the deep belief networks. To further improve the performance of data representation, this paper proposes a new sparse RBM which is inspired by the vision sparse representation of the human brain, referred to as SmoothRBM. It directly constrains the activation probability of the hidden units by adding a smooth L0 norm regularization and can learn different sparse levels according to different learning tasks. Experiments on the MNIST dataset show that the SmoothRBM can extract feature information from dataset more efficiently, learn more sparser and more discriminative representations than the related state-of-the-art models.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Restricted%20boltzmann%20machines%20(RBM)%20&amp;code=&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!" target="_blank">Restricted boltzmann machines (RBM) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Sparse%20representation&amp;code=&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!" target="_blank">Sparse representation;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Activation%20probability&amp;code=&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!" target="_blank">Activation probability;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Feature%20information&amp;code=&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!" target="_blank">Feature information;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-07-27</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="45" name="45" class="anchor-tag"><b>1 引言</b></h3>
                <div class="p1">
                    <p id="46">近年来, 作为发现数据的分布式特征表示方式, 深度学习方法已经引起了人们的极大兴趣<citation id="128" type="reference"><link href="3" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>。深度信念网络 (Deep Belief Networks, DBN) 是深度学习模型中最受欢迎的模型之一, 已被成功应用于手写字体识别<citation id="129" type="reference"><link href="5" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>、文本检测<citation id="130" type="reference"><link href="7" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>、信息检索<citation id="131" type="reference"><link href="9" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>等领域中。作为DBN基本组成模块的RBM是一个非常强大的生成模型, 它能以一种完全无监督的方式来提取数据的特征。因此进一步的提升RBM的性能可以促进深度学习技术的发展。</p>
                </div>
                <div class="p1">
                    <p id="47">对哺乳动物的大脑研究表明, 它们的视觉系统在加工处理图像信息时, 大脑皮层上的视觉细胞只有部分被激活<citation id="136" type="reference"><link href="11" rel="bibliography" /><link href="13" rel="bibliography" /><sup>[<a class="sup">5</a>,<a class="sup">6</a>]</sup></citation>, 基于此, 有学者提出了稀疏编码理论来模拟视觉系统的稀疏表示<citation id="132" type="reference"><link href="15" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>。为了进一步提升RBM的性能, 部分学者将稀疏编码理论与其结合, 提出很多的改进的RBM。例如, Lee等人在文献<citation id="133" type="reference">[<a class="sup">8</a>]</citation>中提出了一种非常有效的稀疏版RBM, 即SparseRBM, 它是通过引入一个稀疏惩罚项来限制隐单元的平均激活概率接近稀疏目标来产生稀疏表示, 能够提取到手写字体的部分笔画特征和自然图像中类似Gabor滤波的特征。罗在文献<citation id="134" type="reference">[<a class="sup">9</a>]</citation>中将组稀疏的思想引入到RBM中, 在似然函数中引入<mathml id="48"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mi>L</mi><msub><mrow></mrow><mn>1</mn></msub></mrow><mrow><mi>L</mi><msub><mrow></mrow><mn>2</mn></msub></mrow></mfrac></mrow></math></mathml>正则项提出了稀疏组<i>RBM</i>, 这个正则项使得组内和组间都产生了稀疏表示。<i>JI</i>等人将率失真理论引入到<i>RBM</i>中, 提出了<i>SR</i>-<i>RBM</i><citation id="135" type="reference"><link href="21" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>, 在此模型中<i>Kullback</i>-<i>Leibler</i>散度被视为原始数据分布与<i>RBM</i>所定义的均衡分布之间的失真函数, 而使用隐单元激活概率的L<sub>1</sub>范数完成一个较小的码率。</p>
                </div>
                <div class="p1">
                    <p id="49">在<i>RBM</i>框架内融合稀疏性的众多方法中, 由<i>Lee</i>等人提出的<i>SparseRBM</i>是应用最为广泛的。它通过鼓励每个隐单元只在少量训练样本上处于激活状态来实现稀疏表示, 然而它限制了每个隐单元以相同的平均激活概率达到一个预先设置的稀疏目标<i>p</i>来实现稀疏表示。在压缩感知领域, 最有效的稀疏正则项是<i>L</i><sub>0</sub>范数<citation id="138" type="reference"><link href="23" rel="bibliography" /><link href="25" rel="bibliography" /><sup>[<a class="sup">11</a>,<a class="sup">12</a>]</sup></citation>, 受此启发, 尝试在<i>RBM</i>的似然函数后面增添一个<i>L</i><sub>0</sub>范数惩罚项。然而, 由于<i>L</i><sub>0</sub>范数的正则化问题是<i>NP</i>难题<citation id="137" type="reference"><link href="27" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>, 非常难以解决。为了克服这个问题, 在信号处理和图像恢复等领域有很多学者选择<i>Smooth L</i><sub>0</sub>范数来代替<i>L</i><sub>0</sub>范数, 取得了良好的实验效果, 且与<i>L</i><sub>1</sub>范数相比, 它可以提供更有效的稀疏诱导性质<citation id="139" type="reference"><link href="29" rel="bibliography" /><link href="31" rel="bibliography" /><sup>[<a class="sup">14</a>,<a class="sup">15</a>]</sup></citation>。因此, 尝试将<i>Smooth L</i><sub>0</sub>范数引入到<i>RBM</i>中, 在其似然函数后面增添一个<i>Smooth L</i><sub>0</sub>范数惩罚项, 提出了一个新的稀疏<i>RBM</i>, 它直接作用于隐单元的总体激活概率上。通过这样做, 每个隐单元的稀疏性不受约束, 可以根据不同的任务自适应地学习。与当前的一些优秀模型相比, 可以学习到更稀疏和更具判别力的表示形式。</p>
                </div>
                <h3 id="50" name="50" class="anchor-tag"><b>2 受限玻尔兹曼机</b></h3>
                <h4 class="anchor-tag" id="51" name="51"><b>2.1 RBM的基本模型</b></h4>
                <div class="p1">
                    <p id="52">RBM是一种无向图模型<citation id="140" type="reference"><link href="33" rel="bibliography" /><sup>[<a class="sup">16</a>]</sup></citation>, 如图1所示, RBM拥有两层结构, 第一层叫做可见层, 图中用v表示, 表示观测数据;第二层叫做隐层, 图中用<i>h</i>表示, 可视为一些特征提取器, 它们均为二值变量, 即对任意的<mathml id="53"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>i</mi><mo>, </mo><mi>j</mi><mo>, </mo><mi>v</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>h</mi><msub><mrow></mrow><mi>j</mi></msub><mo>∈</mo><mrow><mo>{</mo><mrow><mn>0</mn><mo>, </mo><mn>1</mn></mrow><mo>}</mo></mrow></mrow></math></mathml>, 对称的权值<i>W</i><sup><i>n</i>×<i>m</i></sup>连接在<i>n</i>维的可见层与<i>m</i>维的隐层之间, 但在可见层单元与隐层单元的内部均不存在连接。</p>
                </div>
                <div class="area_img" id="54">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJZ201904049_054.jpg&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!">
                                    <img alt="图1 RBM的结构示意图" src="Detail/GetImg?filename=images/JSJZ201904049_054.jpg&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!" />
                                </a>
                                <p class="img_tit"><b>图1 RBM的结构示意图</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJZ201904049_054.jpg&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="55">对于确定的一组状态 (<b>v, h) , RBM</b>作为一个整体的系统, 其能量函数定义为</p>
                </div>
                <div class="p1">
                    <p id="56" class="code-formula">
                        <mathml id="56"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>E</mi><mo stretchy="false"> (</mo><mi>v</mi><mo>, </mo><mi>h</mi><mo stretchy="false">|</mo><mi>θ</mi><mo stretchy="false">) </mo><mo>=</mo><mo>-</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>a</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mi>v</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mi>b</mi></mstyle><msub><mrow></mrow><mi>j</mi></msub><mi>h</mi><msub><mrow></mrow><mi>j</mi></msub><mo>-</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mi>v</mi></mstyle></mrow></mstyle><msub><mrow></mrow><mi>i</mi></msub><mi>W</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mi>h</mi><msub><mrow></mrow><mi>j</mi></msub><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="57">其中<b>a</b><sub><b>i</b></sub>表示可见层中第<b>i</b>个单元的偏置, <b>b</b><sub><b>j</b></sub>表示隐层中第<b>j</b>个单元的偏置, <mathml id="58"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>θ</mi><mo>=</mo><mrow><mo>{</mo><mrow><mi>W</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>, </mo><mi>a</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>b</mi><msub><mrow></mrow><mi>j</mi></msub></mrow><mo>}</mo></mrow></mrow></math></mathml>是<i>RBM</i>中的参数, 它们均为实数。基于该函数, 得到 (v, h) 的联合概率分布函数为</p>
                </div>
                <div class="p1">
                    <p id="59" class="code-formula">
                        <mathml id="59"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>v</mi><mo>, </mo><mi>h</mi><mo stretchy="false">|</mo><mi>θ</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mrow><mi>e</mi><msup><mrow></mrow><mrow><mo>-</mo><mi>E</mi><mo stretchy="false"> (</mo><mi>v</mi><mo>, </mo><mi>h</mi><mo stretchy="false">|</mo><mi>θ</mi><mo stretchy="false">) </mo></mrow></msup></mrow><mrow><mi>Ζ</mi><mo stretchy="false"> (</mo><mi>θ</mi><mo stretchy="false">) </mo></mrow></mfrac><mo>, </mo><mi>Ζ</mi><mo stretchy="false"> (</mo><mi>θ</mi><mo stretchy="false">) </mo><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>v</mi><mo>, </mo><mi>h</mi></mrow></munder><mi>e</mi></mstyle><msup><mrow></mrow><mrow><mo>-</mo><mi>E</mi><mo stretchy="false"> (</mo><mi>v</mi><mo>, </mo><mi>h</mi><mo stretchy="false">|</mo><mi>θ</mi><mo stretchy="false">) </mo></mrow></msup><mspace width="0.25em" /><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="60">其中Z (θ) 称为归一化因子。面对一个实际的任务, 最为关注<i>RBM</i>的联合概率分布P (v, h|θ) 的边际分布, 也叫做似然函数, 即</p>
                </div>
                <div class="p1">
                    <p id="61" class="code-formula">
                        <mathml id="61"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>v</mi><mo stretchy="false">|</mo><mi>θ</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mn>1</mn><mrow><mi>Ζ</mi><mo stretchy="false"> (</mo><mi>θ</mi><mo stretchy="false">) </mo></mrow></mfrac><mstyle displaystyle="true"><munder><mo>∑</mo><mi>h</mi></munder><mi>e</mi></mstyle><msup><mrow></mrow><mrow><mo>-</mo><mi>E</mi><mo stretchy="false"> (</mo><mi>v</mi><mo>, </mo><mi>h</mi><mo stretchy="false">|</mo><mi>θ</mi><mo stretchy="false">) </mo></mrow></msup><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="62">由<i>RBM</i>层内无连接, 层间有连接的特殊拓扑结构可知:当可见单元的状态已知时, 则各隐单元节点之间是条件独立的, 这时, 第j个隐单元的激活概率为</p>
                </div>
                <div class="p1">
                    <p id="63" class="code-formula">
                        <mathml id="63"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>h</mi><msub><mrow></mrow><mi>j</mi></msub><mo>=</mo><mn>1</mn><mo stretchy="false">|</mo><mi>v</mi><mo>, </mo><mi>θ</mi><mo stretchy="false">) </mo><mo>=</mo><mi>σ</mi><mo stretchy="false"> (</mo><mi>b</mi><msub><mrow></mrow><mi>j</mi></msub><mo>+</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>i</mi></munder><mi>v</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mi>W</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo stretchy="false">) </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="64">式中<mathml id="65"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>σ</mi><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mi>exp</mi><mo stretchy="false"> (</mo><mo>-</mo><mi>x</mi><mo stretchy="false">) </mo></mrow></mfrac></mrow></math></mathml>为<i>sigmoid</i>激活函数。</p>
                </div>
                <h4 class="anchor-tag" id="66" name="66"><b>2.2 RBM的快速学习算法</b></h4>
                <div class="p1">
                    <p id="67">给定一个包含<b>T</b>个样本的训练集, 通过最大化对数似然函数的方法, <b><i>RBM</i></b>中的参数θ可以被学习到, 问题等价于下式</p>
                </div>
                <div class="p1">
                    <p id="68" class="code-formula">
                        <mathml id="68"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>θ</mi><msup><mrow></mrow><mo>*</mo></msup><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>arg</mi><mspace width="0.25em" /><mi>max</mi></mrow></mstyle><mi>θ</mi></munder><mi>L</mi><mo stretchy="false"> (</mo><mi>θ</mi><mo stretchy="false">) </mo><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>arg</mi><mspace width="0.25em" /><mi>max</mi></mrow></mstyle><mi>θ</mi></munder><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>Τ</mi></munderover><mtext>l</mtext></mstyle><mtext>o</mtext><mtext>g</mtext><mi>Ρ</mi><mo stretchy="false"> (</mo><mi>v</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo></mrow></msup><mo stretchy="false">|</mo><mi>θ</mi><mo stretchy="false">) </mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>5</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="69">可以使用随机梯度上升法来求最优参数θ<sup>*</sup>, 该方法的核心是需要计算<b>L</b> (θ) 对于模型参数的偏导数, 精确的梯度计算如下</p>
                </div>
                <div class="p1">
                    <p id="70" class="code-formula">
                        <mathml id="70"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mfrac><mrow><mo>∂</mo><mi>L</mi></mrow><mrow><mo>∂</mo><mi>θ</mi></mrow></mfrac><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>Τ</mi></munderover><mrow></mrow></mstyle><msup><mrow></mrow><mrow><mi>p</mi><mo stretchy="false"> (</mo><mi>h</mi><mo stretchy="false">|</mo><mi>v</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo></mrow></msup><mo stretchy="false">) </mo></mrow></msup><mrow><mo>[</mo><mrow><mfrac><mrow><mo>∂</mo><mo stretchy="false"> (</mo><mo>-</mo><mi>E</mi><mo stretchy="false"> (</mo><mi>v</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>t</mi><mo stretchy="false">) </mo></mrow></msup><mo>, </mo><mi>h</mi><mo stretchy="false">|</mo><mi>θ</mi><mo stretchy="false">) </mo></mrow><mrow><mo>∂</mo><mi>θ</mi></mrow></mfrac></mrow><mo>]</mo></mrow><mo>-</mo></mtd></mtr><mtr><mtd><mtext> </mtext><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>Τ</mi></munderover><mrow></mrow></mstyle><msup><mrow></mrow><mrow><mi>p</mi><mo stretchy="false"> (</mo><mi>v</mi><mo>, </mo><mi>h</mi><mo stretchy="false">) </mo></mrow></msup><mrow><mo>[</mo><mrow><mfrac><mrow><mo>∂</mo><mo stretchy="false"> (</mo><mo>-</mo><mi>E</mi><mo stretchy="false"> (</mo><mi>v</mi><mo>, </mo><mi>h</mi><mo stretchy="false">|</mo><mi>θ</mi><mo stretchy="false">) </mo></mrow><mrow><mo>∂</mo><mi>θ</mi></mrow></mfrac></mrow><mo>]</mo></mrow><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>6</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="71">这里<image id="152" type="formula" href="images/JSJZ201904049_15200.jpg" display="inline" placement="inline"><alt></alt></image>代表关于分布p (v) 的数学期望。在 (6) 式中第一项比较容易计算, 然而计算第二项需要考虑到可见单元和隐单元所有可能的激活情况, 即使使用Gibbs采样技术, 计算代价还是太大。幸运的是, 由Hinton教授等人提出的对比散度算法<citation id="153" type="reference"><link href="35" rel="bibliography" /><sup>[<a class="sup">17</a>]</sup></citation>仅需使用k (通常设为1) 步Gibbs采样便可以得到对 (6) 式中第二项足够好的近似。此时模型中各参数的更新准则为</p>
                </div>
                <div class="area_img" id="154">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJZ201904049_15400.jpg&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="155">上式中<image id="156" type="formula" href="images/JSJZ201904049_15600.jpg" display="inline" placement="inline"><alt></alt></image>代表关于分布P<sub>data</sub> (v, h|θ) 的数学期望, <image id="157" type="formula" href="images/JSJZ201904049_15700.jpg" display="inline" placement="inline"><alt></alt></image><image id="157" type="formula" href="images/JSJZ201904049_15701.jpg" display="inline" placement="inline"><alt></alt></image>代表通过一次数据重构后关于模型分布的期望, α表示学习率。关于对比散度算法的详细推导过程可参见文献<citation id="158" type="reference">[<a class="sup">17</a>]</citation>。</p>
                </div>
                <h3 id="79" name="79" class="anchor-tag"><b>3 基于光滑L</b><sub><b>0</b></sub><b>范数的RBM</b></h3>
                <div class="p1">
                    <p id="80">在<b><i>Lee</i></b>等人提出的<b><i>SparseRBM</i></b>中, 稀疏性是通过一个正则项来惩罚每个隐单元激活的期望与一固定的稀疏目标值的偏差来实现的, 其中正则项具体形式为</p>
                </div>
                <div class="p1">
                    <p id="81" class="code-formula">
                        <mathml id="81"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><msub><mrow></mrow><mrow><mi>s</mi><mn>1</mn></mrow></msub><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><mrow><mrow><mo>|</mo><mrow><mi>p</mi><mo>-</mo><mfrac><mn>1</mn><mi>m</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>l</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mi>E</mi></mstyle><mrow><mo>[</mo><mrow><mi>h</mi><msubsup><mrow></mrow><mi>j</mi><mrow><mo stretchy="false"> (</mo><mi>l</mi><mo stretchy="false">) </mo></mrow></msubsup><mo stretchy="false">|</mo><mi>v</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>l</mi><mo stretchy="false">) </mo></mrow></msup></mrow><mo>]</mo></mrow></mrow><mo>|</mo></mrow></mrow></mstyle><msup><mrow></mrow><mn>2</mn></msup><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>8</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="82">这里<b>E</b>[]表示条件期望, <b>p</b>代表稀疏目标, 它是一常数, 代表着每个隐单元的稀疏水平。<b>k, m</b>分别代表隐单元的个数和样本数据的容量。</p>
                </div>
                <div class="p1">
                    <p id="83">不同于<b><i>SparseRBM</i></b>, 本文使用<b><i>Smooth L</i></b><sub><b>0</b></sub>范数来促进隐单元的稀疏, 提出了一种新的稀疏<b><i>RBM</i></b>, 即<b><i>SmoothRBM</i></b>, 与<b><i>L</i></b><sub><b>1</b></sub>范数相比, 它的斜率更接近零, 可以提供更有效的稀疏诱导性质, 如图<b>2</b>所示。其稀疏正则项定义为</p>
                </div>
                <div class="p1">
                    <p id="84" class="code-formula">
                        <mathml id="84"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><msub><mrow></mrow><mrow><mi>s</mi><mn>2</mn></mrow></msub><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>l</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mrow><mrow><mo>[</mo><mrow><mi>k</mi><mo>-</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><mi>e</mi></mstyle><msup><mrow></mrow><mrow><mo>-</mo><mfrac><mrow><mo stretchy="false"> (</mo><mi>E</mi><mo stretchy="false">[</mo><mi>h</mi><msubsup><mrow></mrow><mi>j</mi><mrow><mo stretchy="false"> (</mo><mi>l</mi><mo stretchy="false">) </mo></mrow></msubsup><mo stretchy="false">|</mo><mi>v</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>l</mi><mo stretchy="false">) </mo></mrow></msup><mo stretchy="false">]</mo><mo stretchy="false">) </mo><msup><mrow></mrow><mn>2</mn></msup></mrow><mrow><mn>2</mn><mi>ε</mi><msup><mrow></mrow><mn>2</mn></msup></mrow></mfrac></mrow></msup></mrow><mo>]</mo></mrow></mrow></mstyle><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>9</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="85">其中ε是一个超参数, 控制着<b>Smooth L</b><sub><b>0</b></sub>范数和<b>L</b><sub><b>0</b></sub>范数之间的相似度。</p>
                </div>
                <div class="area_img" id="86">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJZ201904049_086.jpg&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!">
                                    <img alt="图2 Smooth L0范数比L1范数更好的近似L0范数" src="Detail/GetImg?filename=images/JSJZ201904049_086.jpg&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!" />
                                </a>
                                <p class="img_tit"><b>图2 Smooth L</b><sub><b>0</b></sub><b>范数比L</b><sub><b>1</b></sub><b>范数更好的近似L</b><sub><b>0</b></sub><b>范数</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJZ201904049_086.jpg&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="87">尽管<b><i>SparseRBM</i></b>和<b><i>SmoothRBM</i></b>都可以产生稀疏表示, 但是它们产生稀疏表示的方式是不同的。<b><i>SparseRBM</i></b>是通过推动每个隐单元在<b><i>m</i></b>个样本上的平均激活概率达到一个稀疏目标<b><i>p</i></b>来诱导稀疏表示的, 此时每个隐单元具有相同的稀疏水平, 然而这意味着在某些情况下所有的隐单元可能对于表示输入数据具有相同的作用<citation id="143" type="reference"><link href="37" rel="bibliography" /><sup>[<a class="sup">18</a>]</sup></citation>, 这显然是不利于提取数据的特征的。不同于<b><i>SparseRBM</i>, <i>SmoothRBM</i></b>是通过在隐单元的总体激活概率上施加一个约束来实现稀疏表示, 可以根据不同的学习任务学习到不同的稀疏水平, 同时这是最自然的稀疏诱导方式<citation id="144" type="reference"><link href="25" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="88">给定样本数据集, 则<b><i>SmoothRBM</i></b>对应的优化问题等价于下式</p>
                </div>
                <div class="p1">
                    <p id="89" class="code-formula">
                        <mathml id="89"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>m</mi><mi>i</mi><mi>n</mi></mrow></mstyle><mrow><mrow><mo>{</mo><mrow><mi>w</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>, </mo><mi>a</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mi>b</mi><msub><mrow></mrow><mi>j</mi></msub></mrow><mo>}</mo></mrow></mrow></munder><mo>-</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>l</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mi>l</mi></mstyle><mi>n</mi><mo stretchy="false"> (</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>h</mi></munder><mi>p</mi></mstyle><mo stretchy="false"> (</mo><mi>v</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>l</mi><mo stretchy="false">) </mo></mrow></msup><mo>, </mo><mi>h</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>l</mi><mo stretchy="false">) </mo></mrow></msup><mo stretchy="false">) </mo><mo stretchy="false">) </mo><mo>+</mo></mtd></mtr><mtr><mtd><mi>λ</mi><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>l</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mrow><mrow><mo>[</mo><mrow><mi>k</mi><mo>-</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><mi>e</mi></mstyle><msup><mrow></mrow><mrow><mo>-</mo><mfrac><mrow><mo stretchy="false"> (</mo><mi>E</mi><mo stretchy="false">[</mo><mi>h</mi><msubsup><mrow></mrow><mi>j</mi><mrow><mo stretchy="false"> (</mo><mi>l</mi><mo stretchy="false">) </mo></mrow></msubsup><mo stretchy="false">|</mo><mi>v</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>l</mi><mo stretchy="false">) </mo></mrow></msup><mo stretchy="false">]</mo><mo stretchy="false">) </mo><msup><mrow></mrow><mn>2</mn></msup></mrow><mrow><mn>2</mn><mi>ε</mi><msup><mrow></mrow><mn>2</mn></msup></mrow></mfrac></mrow></msup></mrow><mo>]</mo></mrow></mrow></mstyle><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>0</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="90">这里, <i>λ</i>是一个正则化常数。原则上可以使用梯度下降来解决这个问题, 但是计算对数似然函数梯度项的计算代价太大。受文献<citation id="145" type="reference">[<a class="sup">8</a>]</citation>所用方法的启示, 使用对比散度算法去近似对数似然函数的梯度项, 然后使用梯度下降法解决正则项的问题。这也就意味在每次迭代的过程中可以首先使用对比散度算法去更新参数, 然后在正则项上使用梯度下降再次更新参数, 它的梯度计算具体如下</p>
                </div>
                <div class="p1">
                    <p id="91" class="code-formula">
                        <mathml id="91"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mfrac><mo>∂</mo><mrow><mo>∂</mo><mi>w</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow></mfrac><mi>L</mi><msub><mrow></mrow><mrow><mi>s</mi><mn>2</mn></mrow></msub><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>l</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mrow><mfrac><mo>∂</mo><mrow><mo>∂</mo><mi>w</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow></mfrac></mrow></mstyle><mo stretchy="false"> (</mo><mi>k</mi><mo>-</mo><mi>e</mi><msup><mrow></mrow><mrow><mo>-</mo><mfrac><mrow><mo stretchy="false"> (</mo><mi>E</mi><mo stretchy="false">[</mo><mi>h</mi><msubsup><mrow></mrow><mi>j</mi><mrow><mo stretchy="false"> (</mo><mi>l</mi><mo stretchy="false">) </mo></mrow></msubsup><mo stretchy="false">|</mo><mi>v</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>l</mi><mo stretchy="false">) </mo></mrow></msup><mo stretchy="false">]</mo><mo stretchy="false">) </mo><msup><mrow></mrow><mn>2</mn></msup></mrow><mrow><mn>2</mn><mi>ε</mi><msup><mrow></mrow><mn>2</mn></msup></mrow></mfrac></mrow></msup><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>l</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mrow><mfrac><mo>∂</mo><mrow><mo>∂</mo><mi>w</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow></mfrac></mrow></mstyle><mo stretchy="false"> (</mo><mi>k</mi><mo>-</mo><mi>e</mi><msup><mrow></mrow><mrow><mo>-</mo><mfrac><mrow><mo stretchy="false"> (</mo><mi>p</mi><mo stretchy="false">[</mo><mi>h</mi><msubsup><mrow></mrow><mi>j</mi><mrow><mo stretchy="false"> (</mo><mi>l</mi><mo stretchy="false">) </mo></mrow></msubsup><mo stretchy="false">|</mo><mi>v</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>l</mi><mo stretchy="false">) </mo></mrow></msup><mo stretchy="false">]</mo><mo stretchy="false">) </mo><msup><mrow></mrow><mn>2</mn></msup></mrow><mrow><mn>2</mn><mi>ε</mi><msup><mrow></mrow><mn>2</mn></msup></mrow></mfrac></mrow></msup><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mo>=</mo><mfrac><mn>1</mn><mrow><mi>ε</mi><msup><mrow></mrow><mn>2</mn></msup></mrow></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>l</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mi>e</mi></mstyle><msup><mrow></mrow><mrow><mo>-</mo><mfrac><mrow><mo stretchy="false"> (</mo><mi>p</mi><msubsup><mrow></mrow><mi>j</mi><mrow><mo stretchy="false"> (</mo><mi>l</mi><mo stretchy="false">) </mo></mrow></msubsup><mo stretchy="false">) </mo><msup><mrow></mrow><mn>2</mn></msup></mrow><mrow><mn>2</mn><mi>ε</mi><msup><mrow></mrow><mn>2</mn></msup></mrow></mfrac></mrow></msup><mi>p</mi><msubsup><mrow></mrow><mi>j</mi><mrow><mo stretchy="false"> (</mo><mi>l</mi><mo stretchy="false">) </mo></mrow></msubsup><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>p</mi><msubsup><mrow></mrow><mi>j</mi><mrow><mo stretchy="false"> (</mo><mi>l</mi><mo stretchy="false">) </mo></mrow></msubsup><mo stretchy="false">) </mo><mi>p</mi><msubsup><mrow></mrow><mi>j</mi><mrow><mo stretchy="false"> (</mo><mi>l</mi><mo stretchy="false">) </mo></mrow></msubsup><mi>v</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mo stretchy="false"> (</mo><mi>l</mi><mo stretchy="false">) </mo></mrow></msubsup><mo>, </mo></mtd></mtr><mtr><mtd><mfrac><mo>∂</mo><mrow><mo>∂</mo><mi>b</mi><msub><mrow></mrow><mi>j</mi></msub></mrow></mfrac><mi>L</mi><msub><mrow></mrow><mrow><mi>s</mi><mn>2</mn></mrow></msub><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>l</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mrow><mfrac><mo>∂</mo><mrow><mo>∂</mo><mi>b</mi><msub><mrow></mrow><mi>j</mi></msub></mrow></mfrac></mrow></mstyle><mo stretchy="false"> (</mo><mi>k</mi><mo>-</mo><mi>e</mi><msup><mrow></mrow><mrow><mo>-</mo><mfrac><mrow><mo stretchy="false"> (</mo><mi>E</mi><mo stretchy="false">[</mo><mi>h</mi><msubsup><mrow></mrow><mi>j</mi><mrow><mo stretchy="false"> (</mo><mi>l</mi><mo stretchy="false">) </mo></mrow></msubsup><mo stretchy="false">|</mo><mi>v</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>l</mi><mo stretchy="false">) </mo></mrow></msup><mo stretchy="false">]</mo><mo stretchy="false">) </mo><msup><mrow></mrow><mn>2</mn></msup></mrow><mrow><mn>2</mn><mi>ε</mi><msup><mrow></mrow><mn>2</mn></msup></mrow></mfrac></mrow></msup><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>l</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mrow><mfrac><mo>∂</mo><mrow><mo>∂</mo><mi>b</mi><msub><mrow></mrow><mi>j</mi></msub></mrow></mfrac></mrow></mstyle><mo stretchy="false"> (</mo><mi>k</mi><mo>-</mo><mi>e</mi><msup><mrow></mrow><mrow><mo>-</mo><mfrac><mrow><mo stretchy="false"> (</mo><mi>p</mi><mo stretchy="false">[</mo><mi>h</mi><msubsup><mrow></mrow><mi>j</mi><mrow><mo stretchy="false"> (</mo><mi>l</mi><mo stretchy="false">) </mo></mrow></msubsup><mo stretchy="false">|</mo><mi>v</mi><msup><mrow></mrow><mrow><mo stretchy="false"> (</mo><mi>l</mi><mo stretchy="false">) </mo></mrow></msup><mo stretchy="false">]</mo><mo stretchy="false">) </mo><msup><mrow></mrow><mn>2</mn></msup></mrow><mrow><mn>2</mn><mi>ε</mi><msup><mrow></mrow><mn>2</mn></msup></mrow></mfrac></mrow></msup><mo stretchy="false">) </mo></mtd></mtr><mtr><mtd><mo>=</mo><mfrac><mn>1</mn><mrow><mi>ε</mi><msup><mrow></mrow><mn>2</mn></msup></mrow></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>l</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mi>e</mi></mstyle><msup><mrow></mrow><mrow><mo>-</mo><mfrac><mrow><mo stretchy="false"> (</mo><mi>p</mi><msubsup><mrow></mrow><mi>j</mi><mrow><mo stretchy="false"> (</mo><mi>l</mi><mo stretchy="false">) </mo></mrow></msubsup><mo stretchy="false">) </mo><msup><mrow></mrow><mn>2</mn></msup></mrow><mrow><mn>2</mn><mi>ε</mi><msup><mrow></mrow><mn>2</mn></msup></mrow></mfrac></mrow></msup><mi>p</mi><msubsup><mrow></mrow><mi>j</mi><mrow><mo stretchy="false"> (</mo><mi>l</mi><mo stretchy="false">) </mo></mrow></msubsup><mo stretchy="false"> (</mo><mn>1</mn><mo>-</mo><mi>p</mi><msubsup><mrow></mrow><mi>j</mi><mrow><mo stretchy="false"> (</mo><mi>l</mi><mo stretchy="false">) </mo></mrow></msubsup><mo stretchy="false">) </mo><mi>p</mi><msubsup><mrow></mrow><mi>j</mi><mrow><mo stretchy="false"> (</mo><mi>l</mi><mo stretchy="false">) </mo></mrow></msubsup><mo>, </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="92">这里</p>
                </div>
                <div class="p1">
                    <p id="93" class="code-formula">
                        <mathml id="93"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>p</mi><msubsup><mrow></mrow><mi>j</mi><mrow><mo stretchy="false"> (</mo><mi>l</mi><mo stretchy="false">) </mo></mrow></msubsup><mo>=</mo><mi>σ</mi><mo stretchy="false"> (</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mi>i</mi></munder><mi>v</mi></mstyle><msubsup><mrow></mrow><mi>i</mi><mrow><mo stretchy="false"> (</mo><mi>l</mi><mo stretchy="false">) </mo></mrow></msubsup><mi>w</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>+</mo><mi>b</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">) </mo><mo>。</mo><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="94">类似文献<citation id="146" type="reference">[<a class="sup">8</a>]</citation>中算法的实现细节, 为了提高最小化正则项时的计算效率, 仅仅更新隐单元的偏置b<sub>j</sub>, 不再对权重进行更新。具体的参数更新步骤如下:</p>
                </div>
                <div class="p1">
                    <p id="95">算法 SmoothRBM的学习算法 (以一个样本的一次迭代过程为例) </p>
                </div>
                <div class="p1">
                    <p id="96"><b>1</b>) 使用对比散度算法更新参数</p>
                </div>
                <div class="area_img" id="159">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JSJZ201904049_15900.jpg&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="160">这里α代表学习率, <image id="161" type="formula" href="images/JSJZ201904049_16100.jpg" display="inline" placement="inline"><alt></alt></image>代表通过一次数据重构后关于模型分布的期望。</p>
                </div>
                <div class="p1">
                    <p id="100"><b>2</b>) 通过正则项的梯度即 (<b>11</b>) 式更新参数<i>b</i><sub><i>j</i></sub>。</p>
                </div>
                <div class="p1">
                    <p id="101"><b>3</b>) 重复步骤<b>1</b>和<b>2</b>直到收敛。</p>
                </div>
                <h3 id="102" name="102" class="anchor-tag"><b>4 实验分析</b></h3>
                <div class="p1">
                    <p id="103">本小节中, 在<b><i>MNIST</i></b>数据集上做相关实验来验证本文提出新模型的性能。<b><i>MNIST</i></b>数集是由包含<b>10</b>个手写数字体 (<b>0-9</b>) 的<b>60000</b>个训练样本和<b>10000</b>测试样本图像构成, 每个图像的大小有<b>28×28</b>个像素, 图<b>3</b>展示了数据集的部分样例。在实验中, 考虑到计算的成本, 本文从<b>60000</b>个训练样本选取每类样本的前<b>2000</b>个作为训练样本, 同时使用批训练的方式, 将其分成<b>200</b>组的小批量数据。</p>
                </div>
                <div class="p1">
                    <p id="104">在下面所有实验中, 由于<b><i>SparseRBM</i></b>是最为经典的稀疏<b><i>RBM</i></b>模型, 故本节的对比对象选为<b><i>SparseRBM</i></b>和<b><i>RBM</i></b>。参数设置按照文献<citation id="147" type="reference">[<a class="sup">19</a>]</citation>中的建议, 初始化权重矩阵中的每个元素为来自于服从均值<b>0</b>, 方差为<b>0.1</b>的正态分布的随机数, 可见层和隐层的偏置初始为<b>0</b>。与此同时, 从剩余的<b>40000</b>个训练样本中选取<b>2000</b>个样本作为验证集来进行<b><i>SparseRBM</i></b>和<b><i>SmoothRBM</i></b>模型中参数的选择。</p>
                </div>
                <div class="area_img" id="105">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJZ201904049_105.jpg&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!">
                                    <img alt="图3 MNIST数据集的样例" src="Detail/GetImg?filename=images/JSJZ201904049_105.jpg&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!" />
                                </a>
                                <p class="img_tit"><b>图3 MNIST数据集的样例</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJZ201904049_105.jpg&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="106" name="106"><b>4.1 表示的稀疏性比较</b></h4>
                <div class="p1">
                    <p id="107">在本小节的实验中, 分别训练了具有<b>784</b>个可见层单元和<b>500</b>个隐单元的<b>RBM, SparseRBM, SmoothRBM</b>。尽管<b>500</b>小于输入的维数<b>784</b>, 但是表示依然是过完备的, 因为这个手写数据集的有效维数远小于<b>784</b><citation id="148" type="reference"><link href="41" rel="bibliography" /><sup>[<a class="sup">20</a>]</sup></citation>。图<b>4</b>的<b>a～ (c</b>) 展现了各个模型在同一训练样本下隐单元的激活概率情况。图中, 横轴代表隐单元的标签, 纵轴代表对应单元的激活概率。从图中可以看出, 与另外的两个模型相比, <b>SmoothRBM</b>可以学习到更稀疏的表示。</p>
                </div>
                <div class="area_img" id="108">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJZ201904049_108.jpg&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!">
                                    <img alt="图4 隐单元的激活概率情况" src="Detail/GetImg?filename=images/JSJZ201904049_108.jpg&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!" />
                                </a>
                                <p class="img_tit"><b>图4 隐单元的激活概率情况</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJZ201904049_108.jpg&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="110">为了进一步对各模型学习到的稀疏性做出定量的评价, 采用<b>Hoyer</b>提出的稀疏度度量方法<citation id="149" type="reference"><link href="43" rel="bibliography" /><sup>[<a class="sup">21</a>]</sup></citation>来进行三个模型稀疏度的比较, 这种稀疏性的度量方式是基于<b>L</b><sub><b>1</b></sub>范数和<b>L</b><sub><b>2</b></sub>范数的关系来实现的, 具体的表达式为</p>
                </div>
                <div class="p1">
                    <p id="111" class="code-formula">
                        <mathml id="111"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>s</mi><mi>p</mi><mi>a</mi><mi>r</mi><mi>s</mi><mi>e</mi><mi>n</mi><mi>e</mi><mi>s</mi><mi>s</mi><mo stretchy="false"> (</mo><mi>x</mi><mo stretchy="false">) </mo><mo>=</mo><mfrac><mrow><msqrt><mi>n</mi></msqrt><mo>-</mo><mo stretchy="false"> (</mo><mstyle displaystyle="true"><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn><mspace width="0.25em" /></mrow><mi>n</mi></msubsup><mrow><mrow><mo>|</mo><mrow><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mo>|</mo></mrow></mrow></mstyle><mo stretchy="false">) </mo><mo>/</mo><msqrt><mrow><mstyle displaystyle="true"><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn><mspace width="0.25em" /></mrow><mi>n</mi></msubsup><mi>x</mi></mstyle><msubsup><mrow></mrow><mi>i</mi><mn>2</mn></msubsup></mrow></msqrt></mrow><mrow><msqrt><mi>n</mi></msqrt><mo>-</mo><mn>1</mn></mrow></mfrac><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mn>2</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="112">这里<b>n</b>是<b>x</b>的维度。从上式中可以看出该稀疏度量的取值位于[<b>0, 1</b>]之间, 它越接近于<b>1</b>就意味着向量<b>x</b>越稀疏, 此时向量中接近<b>0</b>或者等于<b>0</b>的元素越多。对于每个模型学习到的表示, 本文都计算了所有训练样本表示的平均稀疏度, 表一展示了效果。从表中可以看出, 与另外的两个模型相比, <b>SmoothRBM</b>可以学习到更稀疏的表示。</p>
                </div>
                <div class="area_img" id="113">
                    <p class="img_tit"><b>表1 不同模型学习到表示的稀疏度</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="113" border="1"><tr><td><br />RBM</td><td>SparseRBM</td><td>SmoothRBM</td></tr><tr><td><br />0.23</td><td>0.60</td><td>0.88</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="114" name="114"><b>4.2 可视化提取特征的评价</b></h4>
                <div class="p1">
                    <p id="115">为了更好的展示各个模型学习特征的好坏, 本文对它们学习到的特征进行了可视化, 图5的a～ (c) 展示了各模型提取的部分特征。从下图中可以发现, 传统RBM模型学习到的特征多为随机分布的灰度块, 只有少量的特征为字符的笔画。而SmoothRBM模型学习到的特征不再是难以名状的了, 更多的特征为清晰的字符笔画。同时也可看出, 与<b>SparseRBM</b>模型相比, <b>SmoothRBM</b>模型可以学习到更多有意义的特征。</p>
                </div>
                <div class="area_img" id="150">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJZ201904049_15000.jpg&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!">
                                    <img alt="图5 学习特征的可视化" src="Detail/GetImg?filename=images/JSJZ201904049_15000.jpg&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!" />
                                </a>
                                <p class="img_tit">图5 学习特征的可视化  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJZ201904049_15000.jpg&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="150">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJZ201904049_15001.jpg&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!">
                                    <img alt="图5 学习特征的可视化" src="Detail/GetImg?filename=images/JSJZ201904049_15001.jpg&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!" />
                                </a>
                                <p class="img_tit">图5 学习特征的可视化  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJZ201904049_15001.jpg&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="120" name="120"><b>4.3 表示判别力的比较</b></h4>
                <div class="p1">
                    <p id="121">为了验证由<b>SmoothRBM</b>模型学习到的表示具有优秀的判别能力, 在<b>20000</b>个训练样本 (也就是每类中取<b>2000</b>个) 上分别训练了<b>SmoothRBM, RBM</b>和<b>SparseRBM</b>三个无监督模型, 然后用它们所产生的表示作为同一个线性分类器的输入。<b>SmoothRBM, RBM</b>和<b>SparseRBM</b>的隐单元个数设置为<b>500</b>。在分类阶段, 分别用每类样本中的<b>100, 500</b>和<b>1000</b>个图像所产生的表示去训练一个<b>Fisher</b>线性分类器。剩余图像所产生的表示被用作为一个测试集。对于每个模型及其训练样本数的组合, 都重复训练<b>10</b>次, 用平均的分类错误率来评价对应模型的性能。</p>
                </div>
                <div class="p1">
                    <p id="122">此外, 基于未经任何模型加工处理的原始数据的识别错误率也被作为一个比较对象。对比结果如下列表所示。从三个表中可以看出, 在不同数量的训练集和测试集上, 与另外两个模型相比, <b>SmoothRBM</b>总是可以完成的更佳的识别精度。而将原始数据直接输入分类器得到的效果最差。与此同时, 由于<b>SparseRBM</b>和<b>SmoothRBM</b>的分类错误率明显的比<b>RBM</b>低, 因此将稀疏性融入<b>RBM</b>中似乎可以帮助找到更具判别力的特征。</p>
                </div>
                <div class="area_img" id="123">
                    <p class="img_tit"><b>表2 100个样本时各模型的分类精度</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="123" border="1"><tr><td rowspan="2"><br />Algorithm</td><td colspan="2"><br />100samples</td></tr><tr><td><br />Training error</td><td>Test error</td></tr><tr><td><br />Raw</td><td>19.60</td><td>20.74</td></tr><tr><td><br />RBM</td><td>0.06</td><td>16.05</td></tr><tr><td><br />SparseRBM</td><td>0.04</td><td>13.23</td></tr><tr><td><br />SmoothRBM</td><td>0.04</td><td>13.15</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="124">
                    <p class="img_tit"><b>表3 500个样本时各模型的分类精度</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="124" border="1"><tr><td rowspan="2"><br />Algorithm</td><td colspan="2"><br />500samples</td></tr><tr><td><br />Training error</td><td>Test error</td></tr><tr><td><br />Raw</td><td>19.72</td><td>20.51</td></tr><tr><td><br />RBM</td><td>4.62</td><td>9.25</td></tr><tr><td><br />SparseRBM</td><td>3.78</td><td>8.36</td></tr><tr><td><br />SmoothRBM</td><td>3.74</td><td>7.62</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="125">
                    <p class="img_tit"><b>表4 1000个样本时各模型的分类精度</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="125" border="1"><tr><td rowspan="2"><br />Algorithm</td><td colspan="2"><br />1000samples</td></tr><tr><td><br />Training error</td><td>Test error</td></tr><tr><td><br />Raw</td><td>19.89</td><td>20.14</td></tr><tr><td><br />RBM</td><td>6.37</td><td>8.89</td></tr><tr><td><br />SparseRBM</td><td>5.31</td><td>7.64</td></tr><tr><td><br />SmoothRBM</td><td>5.22</td><td>6.45</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h3 id="126" name="126" class="anchor-tag"><b>5 结论</b></h3>
                <div class="p1">
                    <p id="127">本文受压缩感知理论的启发, 提出了一种新的稀疏RBM, 即SmoothRBM。不同于传统的SparseRBM是约束每个隐单元的平均激活概率达到相同的稀疏水平, 本文模型是通过在隐单元的总体激活概率上施加一个约束来实现稀疏表示, 可以根据不同的学习任务学习到不同的稀疏水平。在MNIST数据集上的三个实验表明, SmoothRBM模型与RBM和SparseRBM相比, 可以更有效的提取数据集中的特征信息, 学习到更稀疏和更具判别力的表示形式。</p>
                </div>
                <div class="area_img" id="151">
                                <img alt="" src="Detail/GetImg?filename=images/JSJZ201904049_15100.jpg&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!" />
                            <p class="img_tit"></p>

                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=On the expressive power of deep architectures">

                                <b>[1]</b> Y Bengio, O Delalleau.On the expressive power of deep architectures[C].Proceedings of the 14th International Conference on Discovery Science, 2011:18-36
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJBK&amp;filename=SJBK15090500012538&amp;v=MDUzMTg5RlpPb05DWDh4b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcmZJSkZzUWF4ST1OaWZKWmJLOUh0ak1xbw==&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!">

                                <b>[2]</b> G E Hinton, S Osinder.A fast learning algorithm for deep belief nets[J].Neural Computation, 2006, 18 (7) :1527-1554.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13012100143198&amp;v=MzEwNzdpZk9mYks3SHRET3JvOUZaZThNRFhVeG9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaW5sVXJmSUpGc1FheEk9Tg==&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!">

                                <b>[3]</b> R Salakhutdinov, G E Hinton.Semantic hashing[J].International Journal of Approximate Reasoning, 2009, 50 (7) :969-978
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Reducing the Dimensionality of Data with Neural Networks">

                                <b>[4]</b> R Salakhutdinov, G E Hinton.Reducing the dimensionality of data with neural networks[J].Science, 2006, 313 (5786) :504-507.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Receptive fields of single neurons in the cat&amp;#39;s striate cortex">

                                <b>[5]</b> D H Hubel, T H Wiesel.Receptive fields of single neurones in the cats striate cortex[J].Journal of Physiol, 1959, 148 (3) :574-591.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES13011501611081&amp;v=MjM0MjlSZEdlcnFRVE1ud1plWnRGaW5sVXJmSUpGc1FheEk9TmlmT2ZiSzdIdEROcW85RVl1b09ESFE0b0JNVDZUNFBRSC9pcg==&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!">

                                <b>[6]</b> B A Olshau, D J Field.Sparse coding of sensory inputs[J].Current Opinion in Neurobiol, 2004, 14 (4) :481-487
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=L0-norm-based sparse representation through alternate projections">

                                <b>[7]</b> L Mancera, J Portilla.L0-norm-based sparse representation through alternate projections[C].Proceedings of IEEE International Conference on Image Processing, 2006:2089-2092
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Sparse deep belief net model for visual area v2">

                                <b>[8]</b> H Lee, C Ekanadham, A Y Ng.Sparse deep belief net model for visual area V2[C].Advances in Neural Information Processing Systems, 2007:873-880
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CDFD&amp;filename=1011298181.nh&amp;v=Mjg3MDFIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVptRnl6Z1Y3N0lWRjI2SDdHeEZ0REVycEViUElRS0Q=&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!">

                                <b>[9]</b> 罗恒.基于协同过滤视角的受限玻尔兹曼机研究[D].上海交通大学, 2011
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_10" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14061600062033&amp;v=MDAxNjJNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcmZJSkZzUWF4ST1OaWZPZmJLOEh0Zk5xWTlGWk8wTkRIODZvQg==&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!">

                                <b>[10]</b> N N Ji, et al.A sparse-response deep belief network based on rate distortion theory[J].Pattern Recognition, 2014, 47 (9) :3179-3191.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Compressed sensing">

                                <b>[11]</b> D L Donoho.Compressed sensing[J].IEEE Transactions on Information Theory, 2006, 52 (4) :1289-1306.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=L0-norm-based sparse representation through alternate projections">

                                <b>[12]</b> L Mancera, J Portilla.L0-norm-based sparse representation through alternate projections[C].Proceedings of the IEEE Conference on Image Processing, 2006:2089-2092.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_13" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Sparse approximate solutions to linear systems">

                                <b>[13]</b> B K Natarajan.Sparse approximate solutions to linear systems[J].Siam Journal on Computing, 1995, (2) :227-234.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A fast approach for overcomplete sparse decomposition based on smoothed ?&amp;lt;sup&amp;gt;0&amp;lt;/sup&amp;gt; norm">

                                <b>[14]</b> H Mohimani, M Babaie-Zadeh, C Jutten.A fast approch for over-complete sparse decomposition based on smoothed <i>L</i><sub>0</sub> Norm[J].IEEE Transactions on Signal Processing, 2009, 57 (1) :289-301
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_15" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JLGY201501047&amp;v=MDk4ODBGckNVUjdxZlp1Wm1GeXpnVjc3SUx5SE1kN0c0SDlUTXJvOUJZNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEc=&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!">

                                <b>[15]</b> 王宏志, 王贤龙, 周婷婷.基于光滑L<sub>0</sub>范数的图像分块压缩感知恢复算法[J].吉林大学学报, 2015, 45 (1) :323-327.
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_16" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GCSX201502001&amp;v=MDk5MzlHRnJDVVI3cWZadVptRnl6Z1Y3N0lJaTdZZHJHNEg5VE1yWTlGWllRS0RIODR2UjRUNmo1NE8zenFxQnQ=&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!">

                                <b>[16]</b> 张春霞, 姬楠楠, 王冠伟.受限玻尔兹曼机[J].工程数学学报, 2015, 32 (3) :160-176.
                            </a>
                        </p>
                        <p id="35">
                            <a id="bibliography_17" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJBK&amp;filename=SJBK15090500012097&amp;v=MDk2NjFiSzlIdGpNcW85RlpPb05ESFUrb0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbmxVcmZJSkZzUWF4ST1OaWZKWg==&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!">

                                <b>[17]</b> G E Hinton.Training products of experts by minimizing contrastive divergence[J].Neural Computation, 2002, 14 (8) :1771-1800.
                            </a>
                        </p>
                        <p id="37">
                            <a id="bibliography_18" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14061700208630&amp;v=MjY3MjJJPU5pZk9mYks4SHRmTnFJOUZadXNIQ244NW9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaW5sVXJmSUpGc1FheA==&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!">

                                <b>[18]</b> N N Ji, et al.Enhancing performance of restricted boltzmann machines via log-sum regularization[J].Knowledge-Based Systems, 2014 63 (1) :82-96.
                            </a>
                        </p>
                        <p id="39">
                            <a id="bibliography_19" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A practical guide to training restricted">

                                <b>[19]</b> G E Hinton.A practical guide to training restricted boltzmann machines[R].Montreal:Department of Computer Science, University of Toronto, 2010.
                            </a>
                        </p>
                        <p id="41">
                            <a id="bibliography_20" target="_blank" href="http://scholar.cnki.net/result.aspx?q=&amp;quot;Efficient learning of sparse representationswith an energy-based model&amp;quot;">

                                <b>[20]</b> M Ranzato, et al.Efficient learning of sparse representations with an energy-based model[J].Proceedings of Advances in Neural Information Processing Systems, 2006:1137-1144
                            </a>
                        </p>
                        <p id="43">
                            <a id="bibliography_21" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Non-negative matrix factorization with sparseness constraints">

                                <b>[21]</b> P O Hoyer.Non-negative matrix factorization with sparseness constraints[J].Journal of Machine Learning Research, 2004, 5 (1) :1457-1469
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJZ201904049" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJZ201904049&amp;v=MDkzMjBESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVabUZ5emdWNzdJTHo3QmRMRzRIOWpNcTQ5QmJZUUs=&amp;uid=WEEvREdxOWJmbC9oM1NjYkZCcDMwV0J4bGdLbWpqYS9tZVBuZ3FmMU1ESmg=$R1yZ0H6jyaa0en3RxVUd8df-oHi7XMMDo7mtKT6mSmEvTuk11l2gFA!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=842_VlRz_g1CMCLi-M0twVw-4EOFG_noWyi_OsMnrAg1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
