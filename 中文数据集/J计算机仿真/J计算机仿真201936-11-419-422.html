<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=NOOK-mvTMnxl11s6oPYV0boPXN3iP4N7Rc-A56nk4KI1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2fKXReader%2fDetail%3fTIMESTAMP%3d637139153787631250%26DBCODE%3dCJFD%26TABLEName%3dCJFDTEMP%26FileName%3dJSJZ201911092%26RESULT%3d1%26SIGN%3dvYbBf39avu8zvxsaR%252bDPUJtIygk%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJZ201911092&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JSJZ201911092&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJZ201911092&amp;v=MDc3MzFwRnlyZ1c3M0JMejdCZExHNEg5ak5ybzlNWm9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVo=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hOVEpzeWorTktOTStJV3FqdFBKWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4ggI8Fm4gTkoUKaID8j8gFw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#27" data-title="&lt;b&gt;1 引言&lt;/b&gt; "><b>1 引言</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#32" data-title="&lt;b&gt;2 基于眼动实验的分类视觉注意显著性预测方法&lt;/b&gt; "><b>2 基于眼动实验的分类视觉注意显著性预测方法</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#33" data-title="&lt;b&gt;2.1 多尺度图像特征提取&lt;/b&gt;"><b>2.1 多尺度图像特征提取</b></a></li>
                                                <li><a href="#54" data-title="&lt;b&gt;2.2 分类视觉注意显著性预测&lt;/b&gt;"><b>2.2 分类视觉注意显著性预测</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#84" data-title="&lt;b&gt;3 实验结果与分析&lt;/b&gt; "><b>3 实验结果与分析</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#99" data-title="&lt;b&gt;4 结束语&lt;/b&gt; "><b>4 结束语</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#86" data-title="&lt;b&gt;图1 立体数据库图像&lt;/b&gt;"><b>图1 立体数据库图像</b></a></li>
                                                <li><a href="#92" data-title="&lt;b&gt;表1 不同方法评价指标对比结果&lt;/b&gt;"><b>表1 不同方法评价指标对比结果</b></a></li>
                                                <li><a href="#95" data-title="&lt;b&gt;图2 文献&lt;/b&gt;&lt;b&gt;方法客观评价结果&lt;/b&gt;"><b>图2 文献</b><b>方法客观评价结果</b></a></li>
                                                <li><a href="#96" data-title="&lt;b&gt;图3 文献&lt;/b&gt;&lt;b&gt;方法客观评价结果&lt;/b&gt;"><b>图3 文献</b><b>方法客观评价结果</b></a></li>
                                                <li><a href="#97" data-title="&lt;b&gt;图4 所提方法客观评价结果&lt;/b&gt;"><b>图4 所提方法客观评价结果</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="138">


                                    <a id="bibliography_1" title=" 杨凡,蔡超.对象级特征引导的显著性视觉注意方法[J].计算机应用,2016,36(11):3217-3221." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201611051&amp;v=MDYyMjBGckNVUjdxZlp1WnBGeXJnVzczQkx6N0JkN0c0SDlmTnJvOUFaWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEc=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hOVEpzeWorTktOTStJV3FqdFBKWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4ggI8Fm4gTkoUKaID8j8gFw!!">
                                        <b>[1]</b>
                                         杨凡,蔡超.对象级特征引导的显著性视觉注意方法[J].计算机应用,2016,36(11):3217-3221.
                                    </a>
                                </li>
                                <li id="140">


                                    <a id="bibliography_2" title=" 周洋,何永健,唐向宏,等.融合双目多维感知特征的立体视频显著性检测[J].中国图象图形学报,2017,22(3):305-314." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZGTB201703004&amp;v=MDg3MDk5Yk1ySTlGWUlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpwRnlyZ1c3M0JQeXJmYkxHNEg=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hOVEpzeWorTktOTStJV3FqdFBKWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4ggI8Fm4gTkoUKaID8j8gFw!!">
                                        <b>[2]</b>
                                         周洋,何永健,唐向宏,等.融合双目多维感知特征的立体视频显著性检测[J].中国图象图形学报,2017,22(3):305-314.
                                    </a>
                                </li>
                                <li id="142">


                                    <a id="bibliography_3" title=" 刘峰,沈同圣,娄树理,等.全局模型和局部优化的深度网络显著性检测[J].光学学报,2017,37(12):264-272." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201712032&amp;v=MTM0OTgzQklqWFRiTEc0SDliTnJZOUdab1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnBGeXJnVzc=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hOVEpzeWorTktOTStJV3FqdFBKWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4ggI8Fm4gTkoUKaID8j8gFw!!">
                                        <b>[3]</b>
                                         刘峰,沈同圣,娄树理,等.全局模型和局部优化的深度网络显著性检测[J].光学学报,2017,37(12):264-272.
                                    </a>
                                </li>
                                <li id="144">


                                    <a id="bibliography_4" title=" 孙菲菲,徐平华,丁雪梅,等.服饰设计视觉显著性检测[J].纺织学报,2018,(3):126-131." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=FZXB201803022&amp;v=MjU4OTh0R0ZyQ1VSN3FmWnVacEZ5cmdXNzNCSXpmVGJMRzRIOW5Nckk5SFpvUUtESDg0dlI0VDZqNTRPM3pxcUI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hOVEpzeWorTktOTStJV3FqdFBKWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4ggI8Fm4gTkoUKaID8j8gFw!!">
                                        <b>[4]</b>
                                         孙菲菲,徐平华,丁雪梅,等.服饰设计视觉显著性检测[J].纺织学报,2018,(3):126-131.
                                    </a>
                                </li>
                                <li id="146">


                                    <a id="bibliography_5" title=" 李璐,温静,王文剑.基于视觉显著性的目标分割算法[J].计算机科学与探索,2016,10(3):398-406." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=KXTS201603010&amp;v=MjkwODZxZlp1WnBGeXJnVzczQkxqWGZmYkc0SDlmTXJJOUVaSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjc=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hOVEpzeWorTktOTStJV3FqdFBKWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4ggI8Fm4gTkoUKaID8j8gFw!!">
                                        <b>[5]</b>
                                         李璐,温静,王文剑.基于视觉显著性的目标分割算法[J].计算机科学与探索,2016,10(3):398-406.
                                    </a>
                                </li>
                                <li id="148">


                                    <a id="bibliography_6" title=" 卢晶,张晓林,陈利利,等.融合运动信息的三维视觉显著性算法研究[J].计算机工程,2018 44(1):238-246." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201801040&amp;v=MTE0MzMzenFxQnRHRnJDVVI3cWZadVpwRnlyZ1c3M0JMejdCYmJHNEg5bk1ybzlCWklRS0RIODR2UjRUNmo1NE8=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hOVEpzeWorTktOTStJV3FqdFBKWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4ggI8Fm4gTkoUKaID8j8gFw!!">
                                        <b>[6]</b>
                                         卢晶,张晓林,陈利利,等.融合运动信息的三维视觉显著性算法研究[J].计算机工程,2018 44(1):238-246.
                                    </a>
                                </li>
                                <li id="150">


                                    <a id="bibliography_7" title=" 陈曦,范敏,熊庆宇.基于马尔科夫链的显著性区域检测算法研究[J].计算机工程与应用,2016,52(7):171-175." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSGG201607031&amp;v=Mjc5NzBmTXFJOUdaWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnBGeXJnVzczQkx6N01hYkc0SDk=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hOVEpzeWorTktOTStJV3FqdFBKWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4ggI8Fm4gTkoUKaID8j8gFw!!">
                                        <b>[7]</b>
                                         陈曦,范敏,熊庆宇.基于马尔科夫链的显著性区域检测算法研究[J].计算机工程与应用,2016,52(7):171-175.
                                    </a>
                                </li>
                                <li id="152">


                                    <a id="bibliography_8" title=" 陈海永,徐森,刘坤,等.基于谱残差视觉显著性的带钢表面缺陷检测[J].光学精密工程,2016,24(10):2572-2580." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXJM201610028&amp;v=MjM5MzN5cmdXNzNCSWpYQlk3RzRIOWZOcjQ5SGJJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVacEY=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hOVEpzeWorTktOTStJV3FqdFBKWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4ggI8Fm4gTkoUKaID8j8gFw!!">
                                        <b>[8]</b>
                                         陈海永,徐森,刘坤,等.基于谱残差视觉显著性的带钢表面缺陷检测[J].光学精密工程,2016,24(10):2572-2580.
                                    </a>
                                </li>
                                <li id="154">


                                    <a id="bibliography_9" title=" 陈超,王晓东,陈美子.基于DCT系数的图像显著区域检测[J].计算机辅助设计与图形学学报,2016,28(4):638-644." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJF201604014&amp;v=MDg0ODY0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVacEZ5cmdXNzNCTHo3QmFMRzRIOWZNcTQ5RVlJUUtESDg0dlI=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hOVEpzeWorTktOTStJV3FqdFBKWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4ggI8Fm4gTkoUKaID8j8gFw!!">
                                        <b>[9]</b>
                                         陈超,王晓东,陈美子.基于DCT系数的图像显著区域检测[J].计算机辅助设计与图形学学报,2016,28(4):638-644.
                                    </a>
                                </li>
                                <li id="156">


                                    <a id="bibliography_10" title=" 耿爱辉,万春明,李毅,等.基于分层差分表达理论的图像视觉增强[J].电子与信息学报,2017,39(4):922-929." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZYX201704022&amp;v=MjI5OTQ5SFpvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVacEZ5cmdXNzNCSVRmU2RyRzRIOWJNcTQ=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hOVEpzeWorTktOTStJV3FqdFBKWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4ggI8Fm4gTkoUKaID8j8gFw!!">
                                        <b>[10]</b>
                                         耿爱辉,万春明,李毅,等.基于分层差分表达理论的图像视觉增强[J].电子与信息学报,2017,39(4):922-929.
                                    </a>
                                </li>
                                <li id="158">


                                    <a id="bibliography_11" title=" 赵迪,王晓鹏,陈丹淇.基于Boolean编码的自下而上视觉显著性检测方法研究[J].航天控制,2017,(2):66-71." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=HTKZ201702012&amp;v=MDg5MDVSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnBGeXJnVzczQkxUbkFkTEc0SDliTXJZOUVab1FLREg4NHY=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hOVEpzeWorTktOTStJV3FqdFBKWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4ggI8Fm4gTkoUKaID8j8gFw!!">
                                        <b>[11]</b>
                                         赵迪,王晓鹏,陈丹淇.基于Boolean编码的自下而上视觉显著性检测方法研究[J].航天控制,2017,(2):66-71.
                                    </a>
                                </li>
                                <li id="160">


                                    <a id="bibliography_12" title=" 肖骏,肖晶,王中元,等.面向刑事侦查的监控视频显著性检测仿真[J].计算机仿真,2018,(7):13-17." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJZ201807082&amp;v=MTM1OTlHRnJDVVI3cWZadVpwRnlyZ1c3M0JMejdCZExHNEg5bk1xSTlOWm9RS0RIODR2UjRUNmo1NE8zenFxQnQ=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hOVEpzeWorTktOTStJV3FqdFBKWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4ggI8Fm4gTkoUKaID8j8gFw!!">
                                        <b>[12]</b>
                                         肖骏,肖晶,王中元,等.面向刑事侦查的监控视频显著性检测仿真[J].计算机仿真,2018,(7):13-17.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JSJZ" target="_blank">计算机仿真</a>
                2019,36(11),419-422             </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于眼动实验的分类视觉注意显著性预测仿真</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E4%BA%8E%E6%99%93%E9%9B%AF&amp;code=06454424&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hOVEpzeWorTktOTStJV3FqdFBKWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4ggI8Fm4gTkoUKaID8j8gFw!!" target="_blank">于晓雯</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BC%A0%E7%AB%8B%E6%B8%85&amp;code=24462908&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hOVEpzeWorTktOTStJV3FqdFBKWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4ggI8Fm4gTkoUKaID8j8gFw!!" target="_blank">张立清</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E9%A2%96%E6%B7%91&amp;code=06442658&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hOVEpzeWorTktOTStJV3FqdFBKWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4ggI8Fm4gTkoUKaID8j8gFw!!" target="_blank">王颖淑</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E9%95%BF%E6%98%A5%E5%A4%A7%E5%AD%A6%E6%9C%BA%E6%A2%B0%E4%B8%8E%E8%BD%A6%E8%BE%86%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0186899&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hOVEpzeWorTktOTStJV3FqdFBKWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4ggI8Fm4gTkoUKaID8j8gFw!!" target="_blank">长春大学机械与车辆工程学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>为了准确地预测出图像中吸引视觉的关键部分,提出基于眼动实验的分类视觉注意显著性预测方法。通过MSRA10K图像库训练全卷积神经网络,得到图像的初步显著性区域特征。对其进行超像素优化,提取多尺度图像特征,对比局部融合颜色和全局颜色,形成低层特征显著图。提取各个图像块的主成分,计算主成分空间中图像块的局部以及全局可区分性,获取模式显著图。引用空间离散度度量分配相应的权重,将两者进行融合,准确预测分类视觉注意显著性区域。将所提方法与较为经典的两种方法进行实验对比,实验结果表明,所提方法能够更加准确预测出图像中的显著性区域。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%9C%BC%E5%8A%A8%E5%AE%9E%E9%AA%8C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hOVEpzeWorTktOTStJV3FqdFBKWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4ggI8Fm4gTkoUKaID8j8gFw!!" target="_blank">眼动实验;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%88%86%E7%B1%BB%E8%A7%86%E8%A7%89&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hOVEpzeWorTktOTStJV3FqdFBKWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4ggI8Fm4gTkoUKaID8j8gFw!!" target="_blank">分类视觉;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B3%A8%E6%84%8F%E6%98%BE%E8%91%97%E6%80%A7&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hOVEpzeWorTktOTStJV3FqdFBKWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4ggI8Fm4gTkoUKaID8j8gFw!!" target="_blank">注意显著性;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%A2%84%E6%B5%8B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hOVEpzeWorTktOTStJV3FqdFBKWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4ggI8Fm4gTkoUKaID8j8gFw!!" target="_blank">预测;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    于晓雯(1972-),女(汉族),吉林长春人,硕士,讲师,主要研究领域:工业工程;;
                                </span>
                                <span>
                                    张立清(1980-),女(汉族),吉林长春人,硕士,讲师,主要研究领域:工业工程;;
                                </span>
                                <span>
                                    王颖淑(1973-),女(汉族),吉林长春人,硕士,实验师,主要研究领域:工业工程。;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-04-01</p>

                    <p>

                            <b>基金：</b>
                                                        <span>长春大学校级项目(XJYB17-12);</span>
                    </p>
            </div>
                    <h1><b>Classification Visual Attention Saliency Prediction Simulation Based on Eye Movement Experiment</b></h1>
                    <h2>
                    <span>YU Xiao-wen</span>
                    <span>ZHANG Li-qing</span>
                    <span>WANG Ying-shu</span>
            </h2>
                    <h2>
                    <span>Department of Mechanical and Vehicle Engineering, Changchun University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>In order to accurately predict the key parts of image attracting vision, this article puts forward a method to predict the classification visual attention saliency based on eye movement experiment. The whole convolution neural network was trained by MSRA10 K image database, so that the preliminary saliency region features of image could be obtained. Based on super-pixel optimization, multi-scale image features were extracted. After local fusion color and global color were compared, low-level feature saliency map was formed. Moreover, the principal components of each image block were extracted. Meanwhile, the local distinguish ability and global distinguish ability of image block in the principal component space was calculated to get the pattern saliency map. The spatial dispersion measure was used to allocate the corresponding weights. Based on the fusion result, the visual attention saliency regions were accurately predicted. Finally, the proposed method was compared with the classical methods. Simulation results show that the proposed method can predict the saliency region in image more accurately.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Eye%20movement%20test&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hOVEpzeWorTktOTStJV3FqdFBKWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4ggI8Fm4gTkoUKaID8j8gFw!!" target="_blank">Eye movement test;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Classification%20vision&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hOVEpzeWorTktOTStJV3FqdFBKWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4ggI8Fm4gTkoUKaID8j8gFw!!" target="_blank">Classification vision;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Attention%20significance&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hOVEpzeWorTktOTStJV3FqdFBKWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4ggI8Fm4gTkoUKaID8j8gFw!!" target="_blank">Attention significance;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=Prediction&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hOVEpzeWorTktOTStJV3FqdFBKWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4ggI8Fm4gTkoUKaID8j8gFw!!" target="_blank">Prediction;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2019-04-01</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="27" name="27" class="anchor-tag"><b>1 引言</b></h3>
                <div class="p1">
                    <p id="28">人类获取的大部分信息都通过眼睛识别,通过人眼进行更高层次的认知与处理<citation id="162" type="reference"><link href="138" rel="bibliography" /><link href="140" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>]</sup></citation>。人们会在所生活的环境内获取大量的信息,但是其中有一部分信息人类已经无法通过视觉进行处理。为了能够更加及时、全面地处理这些信息中的重要部分,需要利用视觉注意机制及时对信息进行处理以及筛选,该机制是一个十分重要的问题,它主要涵盖了不同的领域,现阶段该方面的问题已经引起了相关专家的广泛关注。</p>
                </div>
                <div class="p1">
                    <p id="29">眼动追踪技术能够实时记录人眼识别过程的具体位置以及浏览路径,准确判断出人类的真实想法<citation id="163" type="reference"><link href="142" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>。现阶段的眼动设备价格虽然十分昂贵,但是却无法实时进行数据处理,所以现阶段需要研究一种新的技术来代替人眼设备,准确预测人眼的视觉焦点。</p>
                </div>
                <div class="p1">
                    <p id="30">网络技术的迅猛发展促使视觉注意模型已经被广泛应用于不同领域中<citation id="164" type="reference"><link href="144" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>。该模型的建立有效促进了各个领域的应用研究,不同的研究方法针对不同领域的交叉验证奠定了坚实的基础。</p>
                </div>
                <div class="p1">
                    <p id="31">近几年来,出现了大量有关于注意模型的研究,例如J. Harel等人利用GBVS模型对图像中重要的区域进行信息提取,在形成显著图的过程中引用纯数学方法进行计算,通过马尔科夫链能够预测出图像中的显著区域。J.M.Zhang等人组建基于布尔图的显著性模型,该模型主要引用例如全局拓扑测度,通过该拓扑度实现分类视觉注意显著性预测。上述方法的使用范围十分有限,在科技飞速发展的时代,上述方法已经无法满足新的需求,所以需要研究一种新的预测方法——基于眼动实验的分类视觉注意显著性预测方法。</p>
                </div>
                <h3 id="32" name="32" class="anchor-tag"><b>2 基于眼动实验的分类视觉注意显著性预测方法</b></h3>
                <h4 class="anchor-tag" id="33" name="33"><b>2.1 多尺度图像特征提取</b></h4>
                <div class="p1">
                    <p id="34">结合两种不同的学习方法,将其应用于不同模式的分类或者识别中。根据相关的经验知识可知,进行深层次的多核学习,能够将网络中隐藏节点的卷积模板组转换为能够提高特征维度的函数组,来改善数据的丰富度。与传统的反馈神经网络相比较,在同等规模的情况下进行训练,能够有效增强系统的泛化能力。在图像识别的过程中,传统方法只能够对整个图像进行分类或者判定,获取的最终结果通常利用标量的形式进行表示。但是显著性预测的主要目的在于能够准确预测出单一图像显著的区域,且输出结果需要利用矩阵的形式表示,它能够准确反映像素级的预测结果。传统方法并不适用于显著性预测。</p>
                </div>
                <div class="p1">
                    <p id="35">在所提方法中加入卷积神经网络(FCNN)学习显著性区域的相同特征,组成最初的显著性区域预测图。提取显著图中的训练样本,引用支持向量机(SVM)分类器进行训练,通过分类器得到图像本身带有的显著性特征结构的显著性图<citation id="165" type="reference"><link href="146" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="36">FCNN和CNN两者全部具有很多隐秘的神经网络模型,它们都是通过感知器、神经网络等发展而来的<citation id="166" type="reference"><link href="148" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>。上述两者的主要区别在于,FCNN不包含全层连接,所以获取的输出矩阵对图像不会产生任何的影响,不需要单独对图像进行分割等操作。</p>
                </div>
                <div class="p1">
                    <p id="37">FCNN的显著特征主要为像素级别,各个像素之间都是通过网络隐层的卷积操作相互连接起来的,不同图像的内部结构也是不同的。该小节主要根据FCNN显著图,提取图像的相关特征,对样本进行多尺度训练,训练SVM分类器<citation id="167" type="reference"><link href="150" rel="bibliography" /><link href="152" rel="bibliography" /><sup>[<a class="sup">7</a>,<a class="sup">8</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="38">首先需要对未经过检测的图像进行分割处理,然后准确提取图像的重要区域特征。在传统方法中,加入多种不同的图像特征以及背景种子的选择算法。该算法主要是在指在边缘超像素中,能够获取属于背景的超像素区域。选择图像四个边缘的像素区域P,找出该区域内的全部超像素{<i>R</i><sub><i>i</i></sub>},则有</p>
                </div>
                <div class="p1">
                    <p id="39"><mathml id="107"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><mi>B</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mn>1</mn><mo>-</mo><mfrac><mn>1</mn><mrow><mo stretchy="false">|</mo><mi>R</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">|</mo></mrow></mfrac><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>C</mi><mo>∈</mo><mi>R</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></munder><mi>C</mi></mstyle></mrow></math></mathml>      (1)</p>
                </div>
                <div class="p1">
                    <p id="40">其中,<i>PB</i><sub><i>i</i></sub>代表图像的程度值,|<i>R</i><sub><i>i</i></sub>|代表全部超像素的内部信息,以下使用<i>Otsu</i>分割法获取背景种子。</p>
                </div>
                <div class="p1">
                    <p id="41">为了能够更好地将背景种子的全部相关信息引入到图像特征中,需要计算不同图像在不同特征下的差异值。在计算前需要对全部特征进行归一化处理,以下给出不同向量之间的差异性计算式</p>
                </div>
                <div class="p1">
                    <p id="42"><mathml id="108"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>χ</mi><msup><mrow></mrow><mn>2</mn></msup><mo stretchy="false">(</mo><mi>h</mi><msub><mrow></mrow><mn>1</mn></msub><mo>,</mo><mi>h</mi><msub><mrow></mrow><mn>2</mn></msub><mo stretchy="false">)</mo><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><mfrac><mrow><mn>2</mn><mrow><mo stretchy="false">(</mo><mi>h</mi><msub><mrow></mrow><mrow><mn>1</mn><mi>j</mi></mrow></msub><mo>-</mo><mi>h</mi><msub><mrow></mrow><mrow><mn>2</mn><mi>j</mi></mrow></msub><mo stretchy="false">)</mo></mrow><msup><mrow></mrow><mn>2</mn></msup></mrow><mrow><mi>h</mi><msub><mrow></mrow><mrow><mn>1</mn><mi>j</mi></mrow></msub><mo>+</mo><mi>h</mi><msub><mrow></mrow><mrow><mn>2</mn><mi>j</mi></mrow></msub></mrow></mfrac></mrow></mstyle></mrow></math></mathml>      (2)</p>
                </div>
                <div class="p1">
                    <p id="43">其中,<i>n</i>代表直方图的特征维度,<i>h</i><sub>1</sub><sub><i>j</i></sub>、<i>h</i><sub>2</sub><sub><i>j</i></sub>分别代表不同背景种子所对应的特征。</p>
                </div>
                <div class="p1">
                    <p id="44">以下给出直方图之间欧式距离的计算式</p>
                </div>
                <div class="p1">
                    <p id="45"><i>D</i>(<i>x</i><sub>1</sub>,<i>x</i><sub>2</sub>)=(<i>x</i><sub>11</sub>-<i>x</i><sub>21</sub>)<sup>2</sup>+…+(<i>x</i><sub>1</sub><sub><i>d</i></sub>-<i>x</i><sub>2</sub><sub><i>d</i></sub>)<sup>2</sup>      (3)</p>
                </div>
                <div class="p1">
                    <p id="46">式中,<i>d</i>代表向量的特征长度。</p>
                </div>
                <div class="p1">
                    <p id="47">根据<i>SLIC</i>方法对不同尺度的图像进行分割处理,全面分析时间的复杂度和准确性,如果<i>S</i>=6。在不同的尺度上,获取像素区域<i>R</i>在<i>FCNN</i>显著图相对应的区域<i>G</i>,区域中包含百分之八十的训练样本。</p>
                </div>
                <div class="p1">
                    <p id="48">多核学习是一种较为灵活的学习模型,它能够有效删除无用的信息,准确提高预测精度,改善算法的计算效率<citation id="168" type="reference"><link href="154" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>。针对不同的特征利用<i>SVM</i>进行训练,则能够得到</p>
                </div>
                <div class="p1">
                    <p id="49"><mathml id="109"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>α</mi></mstyle><msub><mrow></mrow><mi>i</mi></msub><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub><mi>Κ</mi><msub><mrow></mrow><mi>m</mi></msub><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>,</mo><mi>x</mi><mo stretchy="false">)</mo><mo>+</mo><mi>b</mi></mrow></math></mathml>      (4)</p>
                </div>
                <div class="p1">
                    <p id="50">式中,<i>α</i><sub><i>i</i></sub>代表拉格朗日乘子,<i>y</i><sub><i>i</i></sub>代表训练样本所对应的标记,<i>K</i><sub><i>m</i></sub>(<i>x</i><sub><i>i</i></sub>,<i>x</i>)代表样本在多维空间内的乘积,<i>b</i>代表偏置常数,根据多核信息,组建多个分类器,得到单核模型更好的性能,则有</p>
                </div>
                <div class="p1">
                    <p id="51" class="code-formula">
                        <mathml id="51"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>F</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mrow><mo>{</mo><mtable columnalign="left"><mtr><mtd><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Μ</mi></munderover><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>α</mi></mstyle></mrow></mstyle><msub><mrow></mrow><mi>j</mi></msub><mi>y</mi><msub><mrow></mrow><mi>j</mi></msub><mi>Κ</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>x</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo><mo>+</mo><mi>b</mi></mtd></mtr><mtr><mtd><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>α</mi></mstyle><msub><mrow></mrow><mi>j</mi></msub><mi>y</mi><msub><mrow></mrow><mi>j</mi></msub><mi>Κ</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>x</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">)</mo><mo>+</mo><mi>b</mi></mtd></mtr></mtable></mrow></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="52">上式中,<i>M</i>代表核函数的种类数;将全部特征的映射能力相结合,形成图像内部特征点,它能够加快特征选择的速度。</p>
                </div>
                <div class="p1">
                    <p id="53">在使用SVM分类器输出的图像进行特征融合,再将输出结果与FCNN初始显著图进行融合,最终得到图像的显著性区域特征,将两者相结合,能够有效提高预测准确性。</p>
                </div>
                <h4 class="anchor-tag" id="54" name="54"><b>2.2 分类视觉注意显著性预测</b></h4>
                <div class="p1">
                    <p id="55">传统方法主要根据图像特征点、边缘、颜色等计算显著性,但是却没有全面考虑图像中剩余部分的可区分性特征所造成的影响。为了消除这些不良影响,分析图像的内部结构,将二维图像转换到低维空间内,获取投影特征<citation id="169" type="reference"><link href="156" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>。该方法不需要再将二维向量转换为一维向量,它能够直接通过图像矩阵组建散布矩阵,且具有较好的鲁棒性。</p>
                </div>
                <div class="p1">
                    <p id="56">将图像块<i>A</i>投影到<i>n</i>维线性序列<i>X</i>上,则有</p>
                </div>
                <div class="p1">
                    <p id="57"><i>Y</i>=<i>AX</i>      (6)</p>
                </div>
                <div class="p1">
                    <p id="58">其中,<i>Y</i>代表图像<i>m</i>维的投影向量,<i>X</i>代表最佳投影矩阵。利用下式能够得到最佳投影方向</p>
                </div>
                <div class="p1">
                    <p id="59"><i>J</i>(<i>x</i>)=<i>tr</i>(<i>S</i><sub><i>x</i></sub>)=<i>X</i><sup><i>T</i></sup><i>G</i><sub><i>t</i></sub><i>X</i>      (7)</p>
                </div>
                <div class="p1">
                    <p id="60">式中,<i>S</i><sub><i>x</i></sub>代表协方差矩阵,<i>G</i><sub><i>t</i></sub>代表非负矩阵,以下给出<i>G</i><sub><i>t</i></sub>的计算式</p>
                </div>
                <div class="p1">
                    <p id="61"><mathml id="110"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>G</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mfrac><mn>1</mn><mi>Μ</mi></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>Μ</mi></munderover><mrow><mo stretchy="false">(</mo><mi>A</mi><msub><mrow></mrow><mi>j</mi></msub><mo>-</mo><mover accent="true"><mi>A</mi><mo>¯</mo></mover><mo stretchy="false">)</mo></mrow></mstyle><msup><mrow></mrow><mi>Τ</mi></msup><mo stretchy="false">(</mo><mi>A</mi><msub><mrow></mrow><mi>j</mi></msub><mo>-</mo><mover accent="true"><mi>A</mi><mo>¯</mo></mover><mo stretchy="false">)</mo></mrow></math></mathml>      (8)</p>
                </div>
                <div class="p1">
                    <p id="62">一般情况下一个投影轴无法全面进行展示,通常需要选取正交条件和判定条件准确得到前<i>d</i>个投影轴,获取图像块<i>A</i>相关的成分向量,进而组建特征矩阵。</p>
                </div>
                <div class="p1">
                    <p id="63">准确计算各个图像块之间的差异性,能够更好地区分不同的模式。在第<i>c</i>通道上<mathml id="111"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo stretchy="false">(</mo><mi>c</mi><mo>∈</mo><mrow><mo>{</mo><mrow><mi>L</mi><mo>,</mo><mi>a</mi><mo>,</mo><mi>b</mi></mrow><mo>}</mo></mrow><mo stretchy="false">)</mo></mrow></math></mathml>,图像块<i>r</i><sub><i>i</i></sub>的特征矩阵为<i>f</i><mathml id="112"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mn>0</mn></msubsup></mrow></math></mathml>,将空间块划分为不同的特征矩阵,一共包含<i>N</i>个领域分块。以下给出<i>c</i>通道上的局部模式可划分性计算式</p>
                </div>
                <div class="p1">
                    <p id="64"><mathml id="113"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>S</mi><msubsup><mrow></mrow><mi>l</mi><mn>0</mn></msubsup><mo stretchy="false">(</mo><mi>r</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>Ν</mi></munderover><mi>D</mi></mstyle><msub><mrow></mrow><mn>0</mn></msub><mrow><mo stretchy="false">(</mo><mi>f</mi><msubsup><mrow></mrow><mi>i</mi><mn>0</mn></msubsup><mo>,</mo><mi>f</mi><msubsup><mrow></mrow><mi>j</mi><mn>0</mn></msubsup><mo stretchy="false">)</mo></mrow><msup><mrow></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup><mo stretchy="false">|</mo><mo stretchy="false">|</mo><mi>f</mi><msubsup><mrow></mrow><mi>i</mi><mn>0</mn></msubsup><mo>-</mo><mi>f</mi><msubsup><mrow></mrow><mi>j</mi><mn>0</mn></msubsup><mo stretchy="false">|</mo><mo stretchy="false">|</mo><msub><mrow></mrow><mrow><mi>m</mi><msub><mrow></mrow><mn>1</mn></msub></mrow></msub></mrow></math></mathml>      (9)</p>
                </div>
                <div class="p1">
                    <p id="65">以下给出<i>r</i><sub><i>i</i></sub>的局部显著性计算式</p>
                </div>
                <div class="p1">
                    <p id="66"><mathml id="114"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>S</mi><msub><mrow></mrow><mrow><mi>p</mi><mi>i</mi></mrow></msub><mo stretchy="false">(</mo><mi>r</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>e</mi><mo>∈</mo><mo stretchy="false">{</mo><mi>L</mi><mo>,</mo><mi>a</mi><mo>,</mo><mi>b</mi><mo stretchy="false">}</mo></mrow></munder><mi>S</mi></mstyle><msubsup><mrow></mrow><mi>l</mi><mn>0</mn></msubsup><mo stretchy="false">(</mo><mi>r</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></math></mathml>      (10)</p>
                </div>
                <div class="p1">
                    <p id="67">部分图像块之间有可能和其它领域之间的主成分空间存在一定的相似性,但是大部分的图像块之间还是存在较大的差异。图像块<i>r</i><sub><i>i</i></sub>的全局模式之间和先验背景图像块之间在主成分空间存在差异。如果在第<i>c</i>个通道中,先验背景中的特征矩阵为<i>f</i><mathml id="115"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mn>0</mn></msubsup></mrow></math></mathml>,即</p>
                </div>
                <div class="p1">
                    <p id="68"><mathml id="116"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>S</mi><msubsup><mrow></mrow><mi>g</mi><mn>0</mn></msubsup><mo stretchy="false">(</mo><mi>r</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>m</mi><mi>i</mi><mi>n</mi></mrow></mstyle><mi>h</mi></munder><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>h</mi></munderover><mi>D</mi></mstyle><msub><mrow></mrow><mn>0</mn></msub><mo stretchy="false">(</mo><mi>f</mi><msubsup><mrow></mrow><mi>i</mi><mn>0</mn></msubsup><mo>,</mo><mi>f</mi><msubsup><mrow></mrow><mi>j</mi><mn>0</mn></msubsup><mo stretchy="false">)</mo><mo stretchy="false">|</mo><mo stretchy="false">|</mo><mi>f</mi><msubsup><mrow></mrow><mi>i</mi><mn>0</mn></msubsup><mo>-</mo><mi>f</mi><msubsup><mrow></mrow><mi>j</mi><mn>0</mn></msubsup><mo stretchy="false">|</mo><mo stretchy="false">|</mo><msub><mrow></mrow><mrow><mi>m</mi><msub><mrow></mrow><mn>1</mn></msub></mrow></msub></mrow></math></mathml>      (11)</p>
                </div>
                <div class="p1">
                    <p id="69">以下给出<i>r</i><sub><i>i</i></sub>的计算式</p>
                </div>
                <div class="p1">
                    <p id="70"><mathml id="117"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>S</mi><msub><mrow></mrow><mrow><mi>p</mi><mi>g</mi></mrow></msub><mo stretchy="false">(</mo><mi>r</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>e</mi><mo>∈</mo><mo stretchy="false">{</mo><mi>L</mi><mo>,</mo><mi>a</mi><mo>,</mo><mi>b</mi><mo stretchy="false">}</mo></mrow></munder><mi>S</mi></mstyle><msubsup><mrow></mrow><mi>g</mi><mn>0</mn></msubsup><mo stretchy="false">(</mo><mi>r</mi><msub><mrow></mrow><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></math></mathml>      (12)</p>
                </div>
                <div class="p1">
                    <p id="71">将上述两者进行结合,能够得到图像的显著性</p>
                </div>
                <div class="p1">
                    <p id="72"><i>S</i><sub><i>p</i></sub>(<i>r</i><sub><i>i</i></sub>)=<i>S</i><sub><i>pl</i></sub>(<i>r</i><sub><i>i</i></sub>)+<i>S</i><sub><i>pg</i></sub>(<i>r</i><sub><i>i</i></sub>)      (13)</p>
                </div>
                <div class="p1">
                    <p id="73">图像中的显著性目标较为集中,但是背景相对分散<citation id="170" type="reference"><link href="158" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>。在一般情况下,人们所关注的焦点也将会集中于某一个区域。传统方法无法深入研究图像整合这一点,所以需要通过显著图的空间离散度选取相适应的权重,对其进行融合处理。选取相位一致的显著图,对其进行标准化处理,则能够计算其空间离散程度</p>
                </div>
                <div class="p1">
                    <p id="74"><mathml id="118"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><mo stretchy="false">(</mo><mi>h</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><mrow><mo stretchy="false">|</mo><mi>S</mi><msub><mrow></mrow><mi>Ρ</mi></msub><mo stretchy="false">|</mo></mrow></mfrac><mstyle displaystyle="true"><munder><mo>∑</mo><mi>x</mi></munder><mi>S</mi></mstyle><msub><mrow></mrow><mi>Ρ</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>×</mo><mo stretchy="false">(</mo><mi>x</mi><msub><mrow></mrow><mi>h</mi></msub><mo>-</mo><mi>μ</mi><msub><mrow></mrow><mi>h</mi></msub><mo stretchy="false">)</mo></mrow></math></mathml>      (14)</p>
                </div>
                <div class="p1">
                    <p id="75">式中,<i>x</i><sub><i>h</i></sub>代表横轴坐标,<i>μ</i><sub><i>h</i></sub>代表纵轴坐标,则有</p>
                </div>
                <div class="p1">
                    <p id="76"><mathml id="119"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>μ</mi><msub><mrow></mrow><mi>h</mi></msub><mo>=</mo><mfrac><mn>1</mn><mrow><mo stretchy="false">|</mo><mi>S</mi><msub><mrow></mrow><mi>Ρ</mi></msub><mo stretchy="false">|</mo></mrow></mfrac><mstyle displaystyle="true"><munder><mo>∑</mo><mi>x</mi></munder><mi>S</mi></mstyle><msub><mrow></mrow><mi>Ρ</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>×</mo><mi>x</mi><msub><mrow></mrow><mi>h</mi></msub></mrow></math></mathml>      (15)</p>
                </div>
                <div class="p1">
                    <p id="77">其中,|<i>S</i><sub><i>P</i></sub>|代表全部点的显著结合,在进行计算时,<i>S</i><sub><i>P</i></sub>中全部点的坐标进行归一化处理。同样,垂直方向的空间离散度<i>V</i><sub><i>P</i></sub>(<i>v</i>)的计算方法同式(15)所示,通过式(16)计算空间离散程度</p>
                </div>
                <div class="p1">
                    <p id="78"><i>V</i><sub><i>P</i></sub>=<i>V</i><sub><i>P</i></sub>(<i>v</i>)+<i>V</i><sub><i>P</i></sub>(<i>h</i>)      (16)</p>
                </div>
                <div class="p1">
                    <p id="79">通过计算上式公式可知,空间离散程度越大,则说明较高显著点的值就相对越分散<citation id="171" type="reference"><link href="160" rel="bibliography" /><sup>[<a class="sup">12</a>]</sup></citation>,以下给出权重的计算式</p>
                </div>
                <div class="p1">
                    <p id="80"><i>w</i><sub><i>i</i></sub>=1/<i>V</i><sub><i>i</i></sub>      (17)</p>
                </div>
                <div class="p1">
                    <p id="81">对<i>w</i><sub><i>i</i></sub>进行归一化处理,并对图像进行融合出处理,实现基于眼动实验的分类视觉注意显著性预测</p>
                </div>
                <div class="p1">
                    <p id="82"><i>S</i>(<i>x</i>)=<i>w</i><sub><i>P</i></sub><i>S</i><sub><i>P</i></sub>(<i>x</i>)+<i>w</i><sub><i>p</i></sub><i>S</i><sub><i>p</i></sub>(<i>x</i>)      (18)</p>
                </div>
                <div class="p1">
                    <p id="83">综上所述,完成了于眼动实验的分类视觉注意显著性预测。</p>
                </div>
                <h3 id="84" name="84" class="anchor-tag"><b>3 实验结果与分析</b></h3>
                <div class="p1">
                    <p id="85">为了充分验证所提方法的综合有效性,选取某研究院提供的立体图像数据库作为数据样本集合,该图像按照相关标准进行设定,如图1所示。</p>
                </div>
                <div class="area_img" id="86">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJZ201911092_086.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hOVEpzeWorTktOTStJV3FqdFBKWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4ggI8Fm4gTkoUKaID8j8gFw!!">
                                    <img alt="图1 立体数据库图像" src="Detail/GetImg?filename=images/JSJZ201911092_086.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hOVEpzeWorTktOTStJV3FqdFBKWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4ggI8Fm4gTkoUKaID8j8gFw!!" />
                                </a>
                                <p class="img_tit"><b>图1 立体数据库图像</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJZ201911092_086.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hOVEpzeWorTktOTStJV3FqdFBKWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4ggI8Fm4gTkoUKaID8j8gFw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="87">实验需要100次的10折交叉验证方法来具体评估所提方法的整体预测性能。以上所引用的方法就是将样本数据集合任意划分为10个彼此之间互不交叉的子集,选取其中的9份代表训练样本集,组建训练模型,将剩下的1份作为测试样本。在实验的过程中,会引入相关的非线性因素,为了有效阻止上述因素对实验结果造成不良影响,需要对模型的客观评分进行线性拟合,则有</p>
                </div>
                <div class="p1">
                    <p id="88"><mathml id="120"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtext>Μ</mtext><mtext>Ο</mtext><mtext>S</mtext><msub><mrow></mrow><mtext>p</mtext></msub><mo>=</mo><mtext>β</mtext><msub><mrow></mrow><mn>1</mn></msub><mo>⋅</mo><mrow><mo>(</mo><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo>-</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mi>exp</mi><mo stretchy="false">(</mo><mtext>β</mtext><msub><mrow></mrow><mn>2</mn></msub><mo>⋅</mo><mo stretchy="false">(</mo><mtext>Q</mtext><mo>-</mo><mtext>β</mtext><msub><mrow></mrow><mn>3</mn></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow></mfrac></mrow><mo>)</mo></mrow><mo>+</mo><mtext>β</mtext><msub><mrow></mrow><mn>4</mn></msub><mo>⋅</mo><mtext>Q</mtext><mo>+</mo><mtext>β</mtext><msub><mrow></mrow><mn>5</mn></msub></mrow></math></mathml>      (19)</p>
                </div>
                <div class="p1">
                    <p id="89">式中,β<sub>1</sub>、β<sub>2</sub>、β<sub>3</sub>、β<sub>4</sub>、β<sub>5</sub>分别代表不同的参数。</p>
                </div>
                <div class="p1">
                    <p id="90">实验中主要选取4个较为常用的指标用历史评价客观模型的预测性能。为了确保10折交叉验证对数据集之间不存在依赖性,将客观参量的平均值作为模型的评价性能。</p>
                </div>
                <div class="p1">
                    <p id="91">利用表1给出文献<citation id="172" type="reference">[<a class="sup">5</a>]</citation>方法、文献<citation id="173" type="reference">[<a class="sup">6</a>]</citation>方法以及所提方法三种方法的性能比较结果。为了简化描述,表1中的1-4分别代表不同指标。其中,1-2代表预测值的准确性,3-4代表预测值的单调性。</p>
                </div>
                <div class="area_img" id="92">
                    <p class="img_tit"><b>表1 不同方法评价指标对比结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="92" border="1"><tr><td rowspan="2"><br />指标</td><td colspan="3"><br />方法</td></tr><tr><td><br />文献[5]方法</td><td>文献[6]方法</td><td>所提方法</td></tr><tr><td><br />1</td><td>0.65</td><td>0.68</td><td>0.85</td></tr><tr><td><br />2</td><td>0.62</td><td>0.71</td><td>0.89</td></tr><tr><td><br />3</td><td>0.48</td><td>0.51</td><td>0.44</td></tr><tr><td><br />4</td><td>0.52</td><td>0.49</td><td>0.39</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="93">通过对比可知,所提方法的预测准确性在85%以上,且3-4的各项指标明显低于其它两种方法,通过实验数据充分证明了所提方法的忧郁型,且所提方法预测得到的客观评价值和人眼主观感知具有较高的一致性。</p>
                </div>
                <div class="p1">
                    <p id="94">以下利用图2至图4给出不同方法在Q值不断变化情况下的客观预测值。</p>
                </div>
                <div class="area_img" id="95">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJZ201911092_095.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hOVEpzeWorTktOTStJV3FqdFBKWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4ggI8Fm4gTkoUKaID8j8gFw!!">
                                    <img alt="图2 文献[5]方法客观评价结果" src="Detail/GetImg?filename=images/JSJZ201911092_095.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hOVEpzeWorTktOTStJV3FqdFBKWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4ggI8Fm4gTkoUKaID8j8gFw!!" />
                                </a>
                                <p class="img_tit"><b>图2 文献</b><citation id="174" type="reference">[<a class="sup">5</a>]</citation><b>方法客观评价结果</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJZ201911092_095.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hOVEpzeWorTktOTStJV3FqdFBKWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4ggI8Fm4gTkoUKaID8j8gFw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="96">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJZ201911092_096.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hOVEpzeWorTktOTStJV3FqdFBKWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4ggI8Fm4gTkoUKaID8j8gFw!!">
                                    <img alt="图3 文献[6]方法客观评价结果" src="Detail/GetImg?filename=images/JSJZ201911092_096.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hOVEpzeWorTktOTStJV3FqdFBKWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4ggI8Fm4gTkoUKaID8j8gFw!!" />
                                </a>
                                <p class="img_tit"><b>图3 文献</b><citation id="175" type="reference">[<a class="sup">6</a>]</citation><b>方法客观评价结果</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJZ201911092_096.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hOVEpzeWorTktOTStJV3FqdFBKWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4ggI8Fm4gTkoUKaID8j8gFw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="97">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JSJZ201911092_097.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hOVEpzeWorTktOTStJV3FqdFBKWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4ggI8Fm4gTkoUKaID8j8gFw!!">
                                    <img alt="图4 所提方法客观评价结果" src="Detail/GetImg?filename=images/JSJZ201911092_097.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hOVEpzeWorTktOTStJV3FqdFBKWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4ggI8Fm4gTkoUKaID8j8gFw!!" />
                                </a>
                                <p class="img_tit"><b>图4 所提方法客观评价结果</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JSJZ201911092_097.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hOVEpzeWorTktOTStJV3FqdFBKWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4ggI8Fm4gTkoUKaID8j8gFw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="98">分析上图可知,文献<citation id="176" type="reference">[<a class="sup">5</a>]</citation>方法、文献<citation id="177" type="reference">[<a class="sup">6</a>]</citation>方法的客观评价值主要集中在3到5之间,通过具体的实验数据更加证实了上述两种方法的预测结果和实际结果之间存在较大的差距。通过分析图3可知,所提方法能够准确地预测出客观评价值,且得到的图像,更加符合人眼的主观感知,获取更加准确的预测结果。</p>
                </div>
                <h3 id="99" name="99" class="anchor-tag"><b>4 结束语</b></h3>
                <div class="p1">
                    <p id="100">人类能够得到的大部分信息全部来自于人类的眼睛,通过视觉对信息进行筛选,使其能够更加有效对信息进行处理。针对传统方法存在的不足,提出基于眼动实验的分类视觉注意显著性预测方法,以下给出所提方法的主要研究内容以及未来展望:</p>
                </div>
                <div class="p1">
                    <p id="101">1)对图像进行特征提取。</p>
                </div>
                <div class="p1">
                    <p id="102">2)对图像进行融合处理,实现基于眼动实验的分类视觉注意显著性预测。</p>
                </div>
                <div class="p1">
                    <p id="103">展望:</p>
                </div>
                <div class="p1">
                    <p id="104">1)现有的图像类别较少,需要进一步进行研究,增加视觉注意模型的类别。</p>
                </div>
                <div class="p1">
                    <p id="105">2)在进行特征提取的过程中,无法将全部的特征目标都加入其中,如果全部加入则会花费大量的时间,所以未来阶段需要对图像特征进行优化。</p>
                </div>
                <div class="p1">
                    <p id="106">3)所提方法在进行参数优化时,没有全面细致地对参数进行调整,导致算法无法达到最优,未来阶段将会对其进行改进。</p>
                </div>
                <div class="area_img" id="137">
                                <img alt="" src="Detail/GetImg?filename=images/JSJZ201911092_13700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hOVEpzeWorTktOTStJV3FqdFBKWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4ggI8Fm4gTkoUKaID8j8gFw!!" />
                            <p class="img_tit"></p>

                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="138">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201611051&amp;v=MDkxNzZXNzNCTHo3QmQ3RzRIOWZOcm85QVpZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVacEZ5cmc=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hOVEpzeWorTktOTStJV3FqdFBKWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4ggI8Fm4gTkoUKaID8j8gFw!!">

                                <b>[1]</b> 杨凡,蔡超.对象级特征引导的显著性视觉注意方法[J].计算机应用,2016,36(11):3217-3221.
                            </a>
                        </p>
                        <p id="140">
                            <a id="bibliography_2" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=ZGTB201703004&amp;v=MjQxMTJGeXJnVzczQlB5cmZiTEc0SDliTXJJOUZZSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnA=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hOVEpzeWorTktOTStJV3FqdFBKWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4ggI8Fm4gTkoUKaID8j8gFw!!">

                                <b>[2]</b> 周洋,何永健,唐向宏,等.融合双目多维感知特征的立体视频显著性检测[J].中国图象图形学报,2017,22(3):305-314.
                            </a>
                        </p>
                        <p id="142">
                            <a id="bibliography_3" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXXB201712032&amp;v=Mjk4NDI4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnBGeXJnVzczQklqWFRiTEc0SDliTnJZOUdab1FLREg=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hOVEpzeWorTktOTStJV3FqdFBKWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4ggI8Fm4gTkoUKaID8j8gFw!!">

                                <b>[3]</b> 刘峰,沈同圣,娄树理,等.全局模型和局部优化的深度网络显著性检测[J].光学学报,2017,37(12):264-272.
                            </a>
                        </p>
                        <p id="144">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=FZXB201803022&amp;v=MDU3MjFIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpwRnlyZ1c3M0JJemZUYkxHNEg5bk1ySTlIWm9RS0Q=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hOVEpzeWorTktOTStJV3FqdFBKWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4ggI8Fm4gTkoUKaID8j8gFw!!">

                                <b>[4]</b> 孙菲菲,徐平华,丁雪梅,等.服饰设计视觉显著性检测[J].纺织学报,2018,(3):126-131.
                            </a>
                        </p>
                        <p id="146">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=KXTS201603010&amp;v=MDg4Nzc3M0JMalhmZmJHNEg5Zk1ySTlFWklRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpwRnlyZ1c=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hOVEpzeWorTktOTStJV3FqdFBKWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4ggI8Fm4gTkoUKaID8j8gFw!!">

                                <b>[5]</b> 李璐,温静,王文剑.基于视觉显著性的目标分割算法[J].计算机科学与探索,2016,10(3):398-406.
                            </a>
                        </p>
                        <p id="148">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJC201801040&amp;v=MDE4OTdCdEdGckNVUjdxZlp1WnBGeXJnVzczQkx6N0JiYkc0SDluTXJvOUJaSVFLREg4NHZSNFQ2ajU0TzN6cXE=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hOVEpzeWorTktOTStJV3FqdFBKWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4ggI8Fm4gTkoUKaID8j8gFw!!">

                                <b>[6]</b> 卢晶,张晓林,陈利利,等.融合运动信息的三维视觉显著性算法研究[J].计算机工程,2018 44(1):238-246.
                            </a>
                        </p>
                        <p id="150">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSGG201607031&amp;v=MjQ1NzMzenFxQnRHRnJDVVI3cWZadVpwRnlyZ1c3M0JMejdNYWJHNEg5Zk1xSTlHWllRS0RIODR2UjRUNmo1NE8=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hOVEpzeWorTktOTStJV3FqdFBKWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4ggI8Fm4gTkoUKaID8j8gFw!!">

                                <b>[7]</b> 陈曦,范敏,熊庆宇.基于马尔科夫链的显著性区域检测算法研究[J].计算机工程与应用,2016,52(7):171-175.
                            </a>
                        </p>
                        <p id="152">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=GXJM201610028&amp;v=MDU2NThRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpwRnlyZ1c3M0JJalhCWTdHNEg5Zk5yNDlIYkk=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hOVEpzeWorTktOTStJV3FqdFBKWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4ggI8Fm4gTkoUKaID8j8gFw!!">

                                <b>[8]</b> 陈海永,徐森,刘坤,等.基于谱残差视觉显著性的带钢表面缺陷检测[J].光学精密工程,2016,24(10):2572-2580.
                            </a>
                        </p>
                        <p id="154">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJF201604014&amp;v=MzA2MzA1NE8zenFxQnRHRnJDVVI3cWZadVpwRnlyZ1c3M0JMejdCYUxHNEg5Zk1xNDlFWUlRS0RIODR2UjRUNmo=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hOVEpzeWorTktOTStJV3FqdFBKWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4ggI8Fm4gTkoUKaID8j8gFw!!">

                                <b>[9]</b> 陈超,王晓东,陈美子.基于DCT系数的图像显著区域检测[J].计算机辅助设计与图形学学报,2016,28(4):638-644.
                            </a>
                        </p>
                        <p id="156">
                            <a id="bibliography_10" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZYX201704022&amp;v=MTU2MjdUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpwRnlyZ1c3M0JJVGZTZHJHNEg5Yk1xNDlIWm9RS0RIODR2UjQ=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hOVEpzeWorTktOTStJV3FqdFBKWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4ggI8Fm4gTkoUKaID8j8gFw!!">

                                <b>[10]</b> 耿爱辉,万春明,李毅,等.基于分层差分表达理论的图像视觉增强[J].电子与信息学报,2017,39(4):922-929.
                            </a>
                        </p>
                        <p id="158">
                            <a id="bibliography_11" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=HTKZ201702012&amp;v=MDM0OTlLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1WnBGeXJnVzczQkxUbkFkTEc0SDliTXJZOUVab1E=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hOVEpzeWorTktOTStJV3FqdFBKWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4ggI8Fm4gTkoUKaID8j8gFw!!">

                                <b>[11]</b> 赵迪,王晓鹏,陈丹淇.基于Boolean编码的自下而上视觉显著性检测方法研究[J].航天控制,2017,(2):66-71.
                            </a>
                        </p>
                        <p id="160">
                            <a id="bibliography_12" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJZ201807082&amp;v=MjM2OTRyZ1c3M0JMejdCZExHNEg5bk1xSTlOWm9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVpwRnk=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hOVEpzeWorTktOTStJV3FqdFBKWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4ggI8Fm4gTkoUKaID8j8gFw!!">

                                <b>[12]</b> 肖骏,肖晶,王中元,等.面向刑事侦查的监控视频显著性检测仿真[J].计算机仿真,2018,(7):13-17.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JSJZ201911092" />
        <input id="dpi" type="hidden" value="400" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJtNEYxa3hOVEpzeWorTktOTStJV3FqdFBKWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4ggI8Fm4gTkoUKaID8j8gFw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJZ201911092&amp;v=MDc3MzFwRnlyZ1c3M0JMejdCZExHNEg5ak5ybzlNWm9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWZadVo=&amp;uid=WEEvREcwSlJHSldRa1FhdkJtNEYxa3hOVEpzeWorTktOTStJV3FqdFBKWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4ggI8Fm4gTkoUKaID8j8gFw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=P0ea_aWnQfx5aXr8bS4u3GGmKX_ORbqAf_1HJ_4Trvg1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
