<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=NOOK-mvTMnxl11s6oPYV0boPXN3iP4N7Rc-A56nk4KI1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2fKXReader%2fDetail%3fTIMESTAMP%3d637140726247631250%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dJDGC201905021%26RESULT%3d1%26SIGN%3ddWkbh4dX7Tws%252bBaKjGoT2ZgCc6k%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JDGC201905021&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=JDGC201905021&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JDGC201905021&amp;v=MDA1NTNvOUhaWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1Wm5GaURuVXJ6TEx5bk1iYkc0SDlqTXE=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDM1V1I1M3JjbjUxV3Mxa3FjaFBuRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#35" data-title="0 引 言 ">0 引 言</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#40" data-title="1 3D重建系统的硬件和软件 ">1 3D重建系统的硬件和软件</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#43" data-title="2 球体的3D重建和定位算法研究 ">2 球体的3D重建和定位算法研究</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#47" data-title="2.1 球体图像的三维重建">2.1 球体图像的三维重建</a></li>
                                                <li><a href="#61" data-title="2.2 背景分割">2.2 背景分割</a></li>
                                                <li><a href="#67" data-title="2.3 点云簇分割">2.3 点云簇分割</a></li>
                                                <li><a href="#74" data-title="2.4 基于法线的阈值分割">2.4 基于法线的阈值分割</a></li>
                                                <li><a href="#83" data-title="2.5 球体参数估计">2.5 球体参数估计</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#93" data-title="3 实验结果和分析 ">3 实验结果和分析</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#94" data-title="3.1 球体图像获取和三维重建">3.1 球体图像获取和三维重建</a></li>
                                                <li><a href="#100" data-title="3.2 背景分割">3.2 背景分割</a></li>
                                                <li><a href="#102" data-title="3.3 点云簇分割">3.3 点云簇分割</a></li>
                                                <li><a href="#106" data-title="3.4 基于法线的条件滤波">3.4 基于法线的条件滤波</a></li>
                                                <li><a href="#113" data-title="3.5 球体参数计算及误差分析">3.5 球体参数计算及误差分析</a></li>
                                                <li><a href="#119" data-title="3.6 算法效率分析">3.6 算法效率分析</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#121" data-title="4 结束语 ">4 结束语</a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#45" data-title="图1 算法流程图">图1 算法流程图</a></li>
                                                <li><a href="#50" data-title="图2 Kinect相机坐标系">图2 Kinect相机坐标系</a></li>
                                                <li><a href="#98" data-title="图3 Kinect获取的图像">图3 Kinect获取的图像</a></li>
                                                <li><a href="#99" data-title="图4 球体三维重建点云图">图4 球体三维重建点云图</a></li>
                                                <li><a href="#104" data-title="图5 球体点云欧氏距离分割结果">图5 球体点云欧氏距离分割结果</a></li>
                                                <li><a href="#108" data-title="图6 法线阈值分割结果">图6 法线阈值分割结果</a></li>
                                                <li><a href="#111" data-title="图7 欧几里得距离条件下的法线阈值分割结果">图7 欧几里得距离条件下的法线阈值分割结果</a></li>
                                                <li><a href="#115" data-title="&lt;b&gt;表1 球体半径计算结果平均误差 (单位&lt;/b&gt;:mm) "><b>表1 球体半径计算结果平均误差 (单位</b>:mm) </a></li>
                                                <li><a href="#116" data-title="&lt;b&gt;表2 球心坐标计算结果平均误差 (单位&lt;/b&gt;:mm) "><b>表2 球心坐标计算结果平均误差 (单位</b>:mm) </a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="5">


                                    <a id="bibliography_1" title=" LASKEY M, LEE J, CHUCK C, et al.Robot grasping in clutter:Using a hierarchy of supervisors for learning from demonstrations[C].IEEE International Conference on Automation Science and Engineering, Fort Worth:IEEE, 2016." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Robot grasping in clutter:Using a hierarchy of supervisors for learning from demonstrations">
                                        <b>[1]</b>
                                         LASKEY M, LEE J, CHUCK C, et al.Robot grasping in clutter:Using a hierarchy of supervisors for learning from demonstrations[C].IEEE International Conference on Automation Science and Engineering, Fort Worth:IEEE, 2016.
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_2" title=" SHARMA D J, KAUR J.Robotic apple harvesting using computer vision based on shape &amp;amp; colour analysis and object positioning[J].Digital Signal Processing, 2014, 6 (7) :226-233." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Robotic apple harvesting using computer vision based on shape&amp;amp;colour analysis and object positioning">
                                        <b>[2]</b>
                                         SHARMA D J, KAUR J.Robotic apple harvesting using computer vision based on shape &amp;amp; colour analysis and object positioning[J].Digital Signal Processing, 2014, 6 (7) :226-233.
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_3" title=" KHOSHROO A, AREFI A, KHODAEI J.Detection of red tomato on plants using image processing techniques[J].Agricultural Communications, 2014, 2 (4) :9-15." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Detection of Red Tomato on Plants using Image Processing Techniques">
                                        <b>[3]</b>
                                         KHOSHROO A, AREFI A, KHODAEI J.Detection of red tomato on plants using image processing techniques[J].Agricultural Communications, 2014, 2 (4) :9-15.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_4" title=" SERAFIN J, GRISETTI G.Using extended measurements and scene merging for efficient and robust point cloud registration[J].Robotics &amp;amp; Autonomous Systems, 2017, 92 (6) :91-106." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESC69E2D615F50E5B54599BE157D7FA938&amp;v=MDM4ODh2Uk1YN3paME9ncmpxUlZCZnNUbFRMbVhDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZCckxVMDV0Rmd6THErdzZzPU5pZk9mY0MrRjZUTzI0bEVZWjBLREFrOA==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDM1V1I1M3JjbjUxV3Mxa3FjaFBuRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                         SERAFIN J, GRISETTI G.Using extended measurements and scene merging for efficient and robust point cloud registration[J].Robotics &amp;amp; Autonomous Systems, 2017, 92 (6) :91-106.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_5" title=" IZADI S, KIM D, HILLIGES O, et al.KinectFusion:real-time 3D reconstruction and interaction using a moving depth camera[C].Proceedings of the 24th annual ACM Symposim on User Interface Software and Technology, Santa Barbara:ACM, 2011." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Kinectfusion:real-time 3d reconstruction and interaction using a moving depth camera">
                                        <b>[5]</b>
                                         IZADI S, KIM D, HILLIGES O, et al.KinectFusion:real-time 3D reconstruction and interaction using a moving depth camera[C].Proceedings of the 24th annual ACM Symposim on User Interface Software and Technology, Santa Barbara:ACM, 2011.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_6" title=" NEWCOMBE R A, FOX D, SEITZ S M.DynamicFusion:reconstruction and tracking of non-rigid scenes in real-time[C].2015 IEEE Conference on Computer Vision Pattern Recongnition, Boston:IEEE, 2015." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=DynamicFusion:reconstruction and tracking of non-rigid scenes in real-time">
                                        <b>[6]</b>
                                         NEWCOMBE R A, FOX D, SEITZ S M.DynamicFusion:reconstruction and tracking of non-rigid scenes in real-time[C].2015 IEEE Conference on Computer Vision Pattern Recongnition, Boston:IEEE, 2015.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_7" title=" FLEISHMAN S, DRORI l, COHENOR D.Bilateral mesh denoising[J].Pro Siggraph, 2003, 22 (3) :950-953." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000098080&amp;v=MTc5MDlCTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnRGaW5sVXJiSktGd1ZhUkU9TmlmSVk3SzdIdGpOcjQ5RlpPSUhESFE1bw==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDM1V1I1M3JjbjUxV3Mxa3FjaFBuRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                         FLEISHMAN S, DRORI l, COHENOR D.Bilateral mesh denoising[J].Pro Siggraph, 2003, 22 (3) :950-953.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_8" title=" ZHENG Y, FU H, AU K C, et al.Bilateral normal filtering for mesh denoising[J].IEEE Trans Vis Comput Graph, 2011, 17 (10) :1521-1530." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Bilateral Normal Filtering for Mesh Denoising">
                                        <b>[8]</b>
                                         ZHENG Y, FU H, AU K C, et al.Bilateral normal filtering for mesh denoising[J].IEEE Trans Vis Comput Graph, 2011, 17 (10) :1521-1530.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_9" title=" 麦春艳, 郑立华, 孙红, 等.基于RGB-D相机的果树三维重构与果实识别定位[J].农业机械学报, 2015 (s1) :35-40." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=NYJX2015S1006&amp;v=MDcyNDhadVpuRmlEblVyekxLelRCZHJHNEg5U3ZybzlGWW9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3cWY=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDM1V1I1M3JjbjUxV3Mxa3FjaFBuRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[9]</b>
                                         麦春艳, 郑立华, 孙红, 等.基于RGB-D相机的果树三维重构与果实识别定位[J].农业机械学报, 2015 (s1) :35-40.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_10" title=" SVEIER A, KLEPPE A L, TINGELSTAD L, et al.Object detection in point clouds using conformal geometric algebra[J].Advances in Applied Clifford Algebras, 2017, 27 (2) :1961-1976." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJDAD30DE7B41F8E5CAA1430A2CE3080045&amp;v=MTE0NDBTQTdnMzJjMmVicVVSYjZhQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQnJMVTA1dEZnekxxK3c2cz1OajdCYXNMTUhkRzQyb2czWU9wNUJBazh2R2RpNnp0Kw==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDM1V1I1M3JjbjUxV3Mxa3FjaFBuRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[10]</b>
                                         SVEIER A, KLEPPE A L, TINGELSTAD L, et al.Object detection in point clouds using conformal geometric algebra[J].Advances in Applied Clifford Algebras, 2017, 27 (2) :1961-1976.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_11" title=" BAZAZIAN D, CASAS J R, RUIZ-HIDALGO J.Fast and robust edge extraction in unorganized point clouds[C].International Conference on Digital Image Computing:Techniques and Applications, Adelaide:IEEE, 2016." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fast and robust edge extraction in unorganized point clouds">
                                        <b>[11]</b>
                                         BAZAZIAN D, CASAS J R, RUIZ-HIDALGO J.Fast and robust edge extraction in unorganized point clouds[C].International Conference on Digital Image Computing:Techniques and Applications, Adelaide:IEEE, 2016.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_12" title=" LIMBERGER F A, OLIVEIRA M M.Real-time detection of planar regions in unorganized point clouds[J].Pattern Recognition, 2015, 48 (6) :2043-2053." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES15122300162485&amp;v=MzA0NDFNbndaZVp0RmlubFVyYkpLRndWYVJFPU5pZk9mYks5SDlQT3JJOUZaZTBOQ0hROG9CTVQ2VDRQUUgvaXJSZEdlcnFRVA==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDM1V1I1M3JjbjUxV3Mxa3FjaFBuRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[12]</b>
                                         LIMBERGER F A, OLIVEIRA M M.Real-time detection of planar regions in unorganized point clouds[J].Pattern Recognition, 2015, 48 (6) :2043-2053.
                                    </a>
                                </li>
                                <li id="29">


                                    <a id="bibliography_13" >
                                        <b>[13]</b>
                                     ZHANG Z Y.A flexible new technique for camera calibration[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2000, 22 (11) :1330-1334.</a>
                                </li>
                                <li id="31">


                                    <a id="bibliography_14" title=" KANG D J, LEE W H.Automatic circle pattern extraction and camera calibration using fast adaptive binarization and plane homography[J].International Journal of Precision Engineering &amp;amp; Manufacturing, 2010, 11 (1) :13-21." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Automatic circle pattern extraction and camera calibration using fast adaptive binarization and plane homography">
                                        <b>[14]</b>
                                         KANG D J, LEE W H.Automatic circle pattern extraction and camera calibration using fast adaptive binarization and plane homography[J].International Journal of Precision Engineering &amp;amp; Manufacturing, 2010, 11 (1) :13-21.
                                    </a>
                                </li>
                                <li id="33">


                                    <a id="bibliography_15" title=" SCHNABEL, R, WAHL R, KLEIN, R.Efficient RANSAC for point-cloud shape detection[J].Computer Graphics Forum, 2010, 26 (2) :214-226." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJWD&amp;filename=SJWD00000017953&amp;v=MTc2OTQ5OVNYcVJyeG94Y01IN1I3cWRaK1p1RmlIa1c3M0pJMTA9TmlmY2FyTzRIdEhNcjQ1Q2JlNE1ZM2s1ekJkaDRq&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDM1V1I1M3JjbjUxV3Mxa3FjaFBuRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[15]</b>
                                         SCHNABEL, R, WAHL R, KLEIN, R.Efficient RANSAC for point-cloud shape detection[J].Computer Graphics Forum, 2010, 26 (2) :214-226.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=JDGC" target="_blank">机电工程</a>
                2019,36(05),549-554 DOI:10.3969/j.issn.1001-4551.2019.05.020            </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于RGB-D相机的球体三维重建及定位方法研究</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%86%AF%E6%98%A5&amp;code=32456247&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDM1V1I1M3JjbjUxV3Mxa3FjaFBuRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">冯春</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E8%8A%B1%E7%9C%81&amp;code=37263485&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDM1V1I1M3JjbjUxV3Mxa3FjaFBuRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">花省</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E6%A0%91%E7%A3%8A&amp;code=38737032&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDM1V1I1M3JjbjUxV3Mxa3FjaFBuRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王树磊</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%B0%B9%E9%A3%9E%E9%B8%BF&amp;code=06479472&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDM1V1I1M3JjbjUxV3Mxa3FjaFBuRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">尹飞鸿</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%B1%9F%E7%82%9C&amp;code=36327005&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDM1V1I1M3JjbjUxV3Mxa3FjaFBuRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">江炜</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%B8%B8%E5%B7%9E%E5%B7%A5%E5%AD%A6%E9%99%A2%E6%9C%BA%E6%A2%B0%E4%B8%8E%E8%BD%A6%E8%BE%86%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0168968&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDM1V1I1M3JjbjUxV3Mxa3FjaFBuRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">常州工学院机械与车辆工程学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>针对单个和多个球体识别和定位问题, 采用一种利用RGB-D相机进行三维重建的方法, 对RGB-D相机的标定、深度图像和彩色图像的配准、彩色三维点云的生成等进行了研究。提出了一种阈值分割结合随机采样一致性算法的定位方法, 利用色彩阈值分割和深度阈值分割方法实现了背景分割, 得到了整体的球体区域的点云数据;利用欧几里得点云分割方法将整体的球体区域分割成了独立的球体点云, 并辅以基于曲面法线的阈值分割方法分离相接触球体的点云;采用随机采样一致性算法分别计算出了每个球体的参数, 得到了球体的球心坐标、半径以及深度信息。研究结果表明:在不同光照条件下该方法具有较好的实时性和定位精度, 在0.5 m～1.5 m的深度范围内, 半径30 mm球体的半径平均测量误差为4 mm, 球心坐标在<i>x</i>, <i>y</i>, <i>z</i>方向的平均偏差为1.3 mm、2.6 mm、1.4 mm, 单帧图像处理平均时间约为1 s。</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=RGB-D%E7%9B%B8%E6%9C%BA&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDM1V1I1M3JjbjUxV3Mxa3FjaFBuRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">RGB-D相机;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDM1V1I1M3JjbjUxV3Mxa3FjaFBuRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">三维重建;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%AF%86%E5%88%AB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDM1V1I1M3JjbjUxV3Mxa3FjaFBuRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">识别;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%AE%9A%E4%BD%8D&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDM1V1I1M3JjbjUxV3Mxa3FjaFBuRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">定位;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%82%B9%E4%BA%91%E5%88%86%E5%89%B2&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDM1V1I1M3JjbjUxV3Mxa3FjaFBuRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">点云分割;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    冯春 (1981-) , 男, 内蒙古赤峰人, 博士, 副教授, 主要从事机器视觉、机器人技术方面的研究。E-mail:chun0471@163.com;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-10-29</p>

                    <p>

                            <b>基金：</b>
                                                        <span>江苏省自然科学基金青年项目 (BK20140252, BK20150256);</span>
                    </p>
            </div>
                    <h1>3<b>D reconstruction and positioning for spheres based on RGB-D camera</b></h1>
                    <h2>
                    <span>FENG Chun</span>
                    <span>HUA Xing</span>
                    <span>WANG Shu-lei</span>
                    <span>YIN Fei-hong</span>
                    <span>JIANG Wei</span>
            </h2>
                    <h2>
                    <span>Department of Mechanical and Vehicular Engineering, Changzhou Institute of Technology</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Aiming at single and multiple sphere recognition and localization problems, a method of three-dimensional reconstruction using an RGB-D camera was employed. The calibration of RGB-D camera, the registration of depth image and color image, and the generation of color 3 D point cloud were studied. A positioning method based on threshold segmentation combined with random sampling consistency algorithm was proposed. The method of color threshold segmentation and the depth threshold segmentation method was used to realize the background segmentation, and the point cloud data of the whole sphere region was obtained. The Euclidean point cloud segmentation method was applied to divide the whole sphere region into independent sphere point clouds, and the threshold segmentation method based on surface normal was used for separating the point cloud of the contact sphere. The random sampling consistency algorithm was used to calculate the parameters of each sphere separately, and get the spherical coordinates, radius and depth information of the sphere. The results indicate that the method has good real-time and positioning accuracy under different illumination conditions. In the range of 0.5 m～1.5 m depth, the average radius error of the sphere with a radius of 30 mm is 4 mm, and the spherical coordinates error at <i>x</i>, <i>y</i> and <i>z</i> direction is 2.3 mm, 4.5 mm, 3.7 mm in average. Meanwhile single frame image average processing time is about 1 s.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=RGB-D%20camera&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDM1V1I1M3JjbjUxV3Mxa3FjaFBuRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">RGB-D camera;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=3D%20reconstruction&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDM1V1I1M3JjbjUxV3Mxa3FjaFBuRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">3D reconstruction;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=recognition&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDM1V1I1M3JjbjUxV3Mxa3FjaFBuRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">recognition;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=positioning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDM1V1I1M3JjbjUxV3Mxa3FjaFBuRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">positioning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=point%20cloud%20segmentation&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDM1V1I1M3JjbjUxV3Mxa3FjaFBuRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">point cloud segmentation;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-10-29</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="35" name="35" class="anchor-tag">0 引 言</h3>
                <div class="p1">
                    <p id="36">在工件的形状聚类<citation id="125" type="reference"><link href="5" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>, 以及苹果、西红柿等的采摘中<citation id="127" type="reference"><link href="7" rel="bibliography" /><link href="9" rel="bibliography" /><sup>[<a class="sup">2</a>,<a class="sup">3</a>]</sup></citation>, 对球体 (或类球体) 的识别和定位具有重要的意义。近些年, 利用3D重建方法对目标物进行识别和定位是一种有效的技术。3D重建可以作为物体尺寸测量、形状识别、目标跟踪等的基础<citation id="126" type="reference"><link href="11" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>, 通常利用3D系统或者3D传感器来实现。立体视觉系统常用的是双目立体视觉, 它从不同角度进行拍摄, 获取图像并对图像处理从而完成3D重建。由于图像处理涉及的计算量比较大, 该方法通常涉及较为复杂的图像处理算法, 实时性不强, 同时彩色相机对外界光照的抗干扰性差。采用激光雷达传感器是3D重建的有效手段, 但是存在价格昂贵且点云数据量大的问题。</p>
                </div>
                <div class="p1">
                    <p id="37">RGB-D相机不但能够获取彩色图像, 同时可以得到深度图像信息, 具有价格低、分辨率高等优势, 因此, 近几年获得了较为广泛的应用。利用RGB-D相机进行三维重建主要包含以下几个步骤:图像信息的获取及预处理, 三维点云的生成、配准和拼接, 点云数据的分割、对象检测和表面重建等。</p>
                </div>
                <div class="p1">
                    <p id="38">lZADI S等<citation id="128" type="reference"><link href="13" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>实现了实时的Kinect Fusion三维重建系统, 但是该算法不能适应过快的表面变化;NEWCOMBE R A等<citation id="129" type="reference"><link href="15" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>提出了改进的Dynamic Fusion三维重建系统, 实现了对快速变形的有效反映;在Kinect进行三维重建中的关键步骤深度图像处理中, 针对缺陷问题, FLEISHMAN S等<citation id="130" type="reference"><link href="17" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>提出了利用三维空间中的双边滤波, 对曲面附近的噪声进行了平顺化, 但是对离群噪声点效果有限;ZHENG Y等<citation id="131" type="reference"><link href="19" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>提出了一种基于一阶特征即小平面法线的简单而有效的各向异性网络去噪框架, 在设计新的双边滤波器的基础上实现了更好的去噪效果;点云分割方面, 麦春艳等<citation id="132" type="reference"><link href="21" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>利用随机采样一致性算法, 简单高效地实时完成了苹果果实点云的分割与检测;除此之外, 欧几里得点群提取算法、区域生产分割算法、凸凹边分割算法等也都有各自的应用。上述的3D重建技术均有各自的应用场合, 但是它们实时性相对比较差, 处理单帧图像的时间在十几甚至二十几秒<citation id="133" type="reference"><link href="23" rel="bibliography" /><link href="25" rel="bibliography" /><link href="27" rel="bibliography" /><sup>[<a class="sup">10</a>,<a class="sup">11</a>,<a class="sup">12</a>]</sup></citation>。</p>
                </div>
                <div class="p1">
                    <p id="39">针对球体等简单形状的物体, 本文基于Kinect v2相机搭建实时精度较高的通用三维重建系统, 在此基础上实现目标物的识别和定位。</p>
                </div>
                <h3 id="40" name="40" class="anchor-tag">1 3D重建系统的硬件和软件</h3>
                <div class="p1">
                    <p id="41">本文使用Kinect V2传感器获取环境的RGB图像信息和深度图像信息。借助Kinect for Windows SDK 2.0可以高效地从Kienct V2传感器中获取环境的彩色图像和深度图像。其中深度信息基于TOF技术, 在0.5 m～4.5 m视野内精度为4 mm。</p>
                </div>
                <div class="p1">
                    <p id="42">软件开发环境为Visual Studio+Qt+OpenCV+PCL+Kinect for Windows SDK 2.0, 采用C++编程语言。其中Visual Sudio作为集成开发环境, Qt用于交互界面开发, OpenCV和PCL分别是二维图像和三维点云处理的C++库, Kinect for Windows SDK 2.0辅助从Kinect V2传感器获取环境图像。</p>
                </div>
                <h3 id="43" name="43" class="anchor-tag">2 球体的3D重建和定位算法研究</h3>
                <div class="p1">
                    <p id="44">实现球体的识别和定位需要在进行三维重建的基础上完成球体的坐标和半径的计算, 整体算法的流程图如图1所示。</p>
                </div>
                <div class="area_img" id="45">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JDGC201905021_045.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDM1V1I1M3JjbjUxV3Mxa3FjaFBuRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 算法流程图" src="Detail/GetImg?filename=images/JDGC201905021_045.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDM1V1I1M3JjbjUxV3Mxa3FjaFBuRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图1 算法流程图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JDGC201905021_045.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDM1V1I1M3JjbjUxV3Mxa3FjaFBuRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="46">在利用RGB-D相机进行三维重建的过程中, 首先需要获取彩色图像和深度图像信息, 并进行彩色相机和深度相机的标定以及二者的配准;然后, 本研究对目标物进行背景分割, 采用颜色作为分割特征;最后, 对单个及多个球体进行三维模型的确定以及位姿参数的确定。</p>
                </div>
                <h4 class="anchor-tag" id="47" name="47">2.1 球体图像的三维重建</h4>
                <div class="p1">
                    <p id="48">球体图像的三维重建需要融合Kienct传感器的深度图像和彩色图像, 得到场景的三维点云数据。在三维重建过程中最关键的两个步骤是完成相机标定和相机配准。</p>
                </div>
                <div class="p1">
                    <p id="49">首先, 本研究建立kinect相机的坐标系, 如图2所示。</p>
                </div>
                <div class="area_img" id="50">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JDGC201905021_050.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDM1V1I1M3JjbjUxV3Mxa3FjaFBuRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 Kinect相机坐标系" src="Detail/GetImg?filename=images/JDGC201905021_050.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDM1V1I1M3JjbjUxV3Mxa3FjaFBuRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图2 Kinect相机坐标系  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JDGC201905021_050.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDM1V1I1M3JjbjUxV3Mxa3FjaFBuRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="51">笔者利用相机标定技术, 得到深度相机坐标系和深度图像像素坐标系之间的映射关系:</p>
                </div>
                <div class="area_img" id="52">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JDGC201905021_05200.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDM1V1I1M3JjbjUxV3Mxa3FjaFBuRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="54">式中: (<i>X</i><sub><i>c</i></sub>, <i>Y</i><sub><i>c</i></sub>, <i>Z</i><sub><i>c</i></sub>) —空间一点在相机坐标系中坐标; (<i>u</i>, <i>v</i>) —该空间点对应像素坐标;<i>f</i><sub><i>u</i></sub>, <i>f</i><sub><i>v</i></sub>—相机归一化参数;<i>s</i>—相机内参数系数; (<i>u</i><sub>0</sub>, <i>v</i><sub>0</sub>) —光学中心坐标。</p>
                </div>
                <div class="p1">
                    <p id="55">融合深度图像和彩色图像生成三维点云, 需要确定深度图像像素坐标系与彩色图像像素坐标系之间的映射关系 (即彩色摄像头与深度摄像头的配准) , 即:</p>
                </div>
                <div class="area_img" id="56">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JDGC201905021_05600.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDM1V1I1M3JjbjUxV3Mxa3FjaFBuRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="58">式中:<i>Z</i><sub><i>rgb</i></sub>, <i>Z</i><sub><i>d</i></sub>—空间一点分别在彩色和深度相机坐标系中的<i>Z</i>坐标; (<i>u</i><sub><i>rgb</i></sub>, <i>v</i><sub><i>rgb</i></sub>) , (<i>u</i><sub><i>d</i></sub>, <i>v</i><sub><i>d</i></sub>) —该点在彩色和深度图像的像素坐标;<b><i>T</i></b><sub><i>rgb</i>-<i>d</i></sub>—彩色相机向深度相机的位姿变换矩阵;<b><i>K</i></b><sub><i>rgb</i></sub>, <b><i>K</i></b><mathml id="59"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>d</mi><mrow><mo>-</mo><mn>1</mn></mrow></msubsup></mrow></math></mathml>—彩色和深度相机内参矩阵。</p>
                </div>
                <div class="p1">
                    <p id="60">相机标定的算法有很多<citation id="134" type="reference"><link href="29" rel="bibliography" /><link href="31" rel="bibliography" /><sup>[<a class="sup">13</a>,<a class="sup">14</a>]</sup></citation>, 其中张正友标定法最为常用。本文在该方法的基础上, 利用Kinect forWindows SDK 2.0中的函数为基础, 分别得到深度相机的内部参数、彩色图像向深度图像的映射以及深度图像向相机坐标系的转换矩阵等参数, 完成相机的标定和相机配准。</p>
                </div>
                <h4 class="anchor-tag" id="61" name="61">2.2 背景分割</h4>
                <div class="p1">
                    <p id="62">原始的三维点云数据包含大量的背景点云, 需要将目标球体的点云从背景中分割出来。色彩阈值分割是一种简便、高效的分割方法。根据目标物体与背景环境色彩的不同选择合适的色彩阈值 (阈值的确定主要是通过实验测试的方法获取) , 可以快速地将背景与目标分割开。本文的目标球体为黄色, 故选择了<i>R</i>-<i>G</i>, <i>R</i>-<i>B</i>分割方法如下式所示:</p>
                </div>
                <div class="area_img" id="63">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JDGC201905021_06300.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDM1V1I1M3JjbjUxV3Mxa3FjaFBuRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="65">式中:<i>R</i>, <i>G</i>, <i>B</i>—彩色通道灰度值;<i>a</i>, <i>b</i>, <i>d</i>—阈值;<i>Z</i>—目标深度。</p>
                </div>
                <div class="p1">
                    <p id="66">由于彩色摄像头与深度摄像头视角不同, 存在前景与背景的重叠与遮挡问题, 融合彩色图像和深度图像时会错误的将前景色附着到背景上。尤其在前景与背景空间距离较远时, 目标球体会在背景上产生大片投影, 影响色彩阈值分割效果。本文在色彩阈值分割的基础上, 增加了基于三维点云<i>Z</i>坐标的阈值分割方法, 很好地解决了这一问题。</p>
                </div>
                <h4 class="anchor-tag" id="67" name="67">2.3 点云簇分割</h4>
                <div class="p1">
                    <p id="68">场景中存在多个球体时, 为了方便计算单个球体的位置和尺寸信息, 需要将每个球体的点云从整体中分割出来。由于球体内点云间距小于球体间点云间距, 可以采用基于欧几里得距离的点云分割方法。该分割方法在分割球体点云的同时, 还有对离群噪声点进行滤波的作用。</p>
                </div>
                <div class="p1">
                    <p id="69">欧几里得点云分割算法通过计算点与点之间的欧式距离判断其亲疏关系, <i>k</i>维空间的欧式距离为:</p>
                </div>
                <div class="p1">
                    <p id="70"><mathml id="71"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>d</mi><mo>=</mo><msqrt><mrow><mrow><mo> (</mo><mrow><mi>Δ</mi><mi>x</mi><msub><mrow></mrow><mn>1</mn></msub></mrow><mo>) </mo></mrow><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mrow><mo> (</mo><mrow><mi>Δ</mi><mi>x</mi><msub><mrow></mrow><mn>2</mn></msub></mrow><mo>) </mo></mrow><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mo>⋯</mo><mo>+</mo><mrow><mo> (</mo><mrow><mi>Δ</mi><mi>x</mi><msub><mrow></mrow><mi>n</mi></msub></mrow><mo>) </mo></mrow><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mo>⋯</mo><mo>+</mo><mrow><mo> (</mo><mrow><mi>Δ</mi><mi>x</mi><msub><mrow></mrow><mi>k</mi></msub></mrow><mo>) </mo></mrow><msup><mrow></mrow><mn>2</mn></msup></mrow></msqrt></mrow></math></mathml>      (4) </p>
                </div>
                <div class="p1">
                    <p id="72">式中:<i>d</i>—欧式距离;<i>Δx</i><sub><i>n</i></sub> (1≤<i>n</i>≤<i>k</i>) —两点在第<i>n</i>维度上的距离。</p>
                </div>
                <div class="p1">
                    <p id="73">由上式可知:在二维和三维空间中, 欧式距离就是点的几何距离。该分割算法从点云中一点出发, 计算其邻近点的欧式距离, 当距离小于设定的阈值, 就将其归入同一簇点云, 并以新加入的点为基点进行运算, 迭代直至不再有新的点加入点云。</p>
                </div>
                <h4 class="anchor-tag" id="74" name="74">2.4 基于法线的阈值分割</h4>
                <div class="p1">
                    <p id="75">当两球体互相接触时, 两球体的点云互相粘连, 会导致欧几里得点云分割方法无法有效分割两球体。因而在欧几里得点云分割之前, 本研究使用基于表面法线的点云滤波方法去除两球体间粘连的点。</p>
                </div>
                <div class="p1">
                    <p id="76">在相机坐标系中, 球体的可视半球的边缘几乎与<i>Z</i>方向相切, 即表面法线的单位向量的<i>Z</i>坐标值很小, 因而可以使用<i>Z</i>坐标阈值分割方法切除半球体的边缘, 阈值分割为:</p>
                </div>
                <div class="area_img" id="77">
                            <div class="imgformula">
                                <img class="pFormula" alt="" src="Detail/GetImg?filename=images/JDGC201905021_07700.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDM1V1I1M3JjbjUxV3Mxa3FjaFBuRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                <p class="formula_seq"></p>
                            </div>

                </div>
                <div class="p1">
                    <p id="79">式中:<mathml id="80"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi mathvariant="bold-italic">e</mi><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false"> (</mo><mi>x</mi><msub><mrow></mrow><mi>n</mi></msub><mo>, </mo><mi>y</mi><msub><mrow></mrow><mi>n</mi></msub><mo>, </mo><mi>z</mi><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false">) </mo><mo>, </mo><mi mathvariant="bold-italic">n</mi><mrow><mo> (</mo><mrow><mi>p</mi><msub><mrow></mrow><mn>0</mn></msub><mo>, </mo><mi>Κ</mi></mrow><mo>) </mo></mrow></mrow></math></mathml>—点云拟合曲面在<i>p</i><sub>0</sub>点的法线的单位向量;<mathml id="81"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo> (</mo><mrow><mi>x</mi><msub><mrow></mrow><mi>n</mi></msub><mo>, </mo><mi>y</mi><msub><mrow></mrow><mi>n</mi></msub><mo>, </mo><mi>z</mi><msub><mrow></mrow><mi>n</mi></msub></mrow><mo>) </mo></mrow></mrow></math></mathml>—点云坐标;<i>t</i>—阈值;<i>K</i>—<i>p</i><sub>0</sub>点的临近点维度;<i>p</i><sub>0</sub>—点云;<i>S</i><sub><i>in</i></sub>, <i>S</i><sub><i>ex</i></sub>—球体内和球体外。</p>
                </div>
                <div class="p1">
                    <p id="82"><i>e</i><sub><i>n</i></sub>可通过<i>p</i><sub>0</sub>点及其<i>K</i>维邻近点计算得来, 并且当<i>e</i><sub><i>n</i></sub>的<i>Z</i>坐标小于阈值<i>t</i>时, 点<i>p</i><sub>0</sub>在球体点云的边缘, 可以将其去除, 进而切除两个接触球体的粘连部分。</p>
                </div>
                <h4 class="anchor-tag" id="83" name="83">2.5 球体参数估计</h4>
                <div class="p1">
                    <p id="84">RANSAC算法是一种经典的数据处理算法<citation id="135" type="reference"><link href="33" rel="bibliography" /><sup>[<a class="sup">15</a>]</sup></citation>。本文研究的球体的数学模型为:</p>
                </div>
                <div class="p1">
                    <p id="85"><mathml id="86"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo> (</mo><mrow><mi>x</mi><mo>-</mo><mi>x</mi><msub><mrow></mrow><mn>0</mn></msub></mrow><mo>) </mo></mrow><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mrow><mo> (</mo><mrow><mi>y</mi><mo>-</mo><mi>y</mi><msub><mrow></mrow><mn>0</mn></msub></mrow><mo>) </mo></mrow><msup><mrow></mrow><mn>2</mn></msup><mo>+</mo><mrow><mo> (</mo><mrow><mi>z</mi><mo>-</mo><mi>z</mi><msub><mrow></mrow><mn>0</mn></msub></mrow><mo>) </mo></mrow><msup><mrow></mrow><mn>2</mn></msup><mo>=</mo><mi>r</mi><msup><mrow></mrow><mn>2</mn></msup></mrow></math></mathml>      (6) </p>
                </div>
                <div class="p1">
                    <p id="87">式中: (<i>x</i><sub>0</sub>, <i>y</i><sub>0</sub>, <i>z</i><sub>0</sub>) —相机坐标系中的球心坐标;<i>r</i>—球面半径。</p>
                </div>
                <div class="p1">
                    <p id="88">笔者在分割得到单个球体的点云后, 对点云进行RANSAC运算, 得到该球体的模型参数。依次对每个球体点云进行运算, 即可得到场景中所有球体的位置和尺寸。</p>
                </div>
                <div class="p1">
                    <p id="89">对于已知半径或半径范围的球体, 还可以对RANSAC计算结果进一步优化。可以在RANSAC计算结果超出范围的时候调整RANSAC参数迭代地计算, 一般可以获得更好的结果, 由此可提高系统鲁棒性和正确率。由于RANSAC得到的球面模型与实际球体的可视半球面拟合, 计算结果中<i>z</i>坐标偏差与半径误差相关, 即半径越大, <i>z</i>坐标越大。所以对于半径已知的球体, 可以根据半径计算结果调整<i>z</i>坐标, 以减小误差, 即:</p>
                </div>
                <div class="p1">
                    <p id="90"><mathml id="91"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>z</mi><mo>=</mo><mi>z</mi><msub><mrow></mrow><mi>c</mi></msub><mo>-</mo><mrow><mo> (</mo><mrow><mi>r</mi><msub><mrow></mrow><mi>c</mi></msub><mo>-</mo><mi>r</mi><msub><mrow></mrow><mrow><mtext>r</mtext><mtext>e</mtext><mtext>a</mtext><mtext>l</mtext></mrow></msub></mrow><mo>) </mo></mrow></mrow></math></mathml>      (7) </p>
                </div>
                <div class="p1">
                    <p id="92">式中:<i>z</i><sub><i>c</i></sub>, <i>r</i><sub><i>c</i></sub>—RANSAC计算得到的球心<i>z</i>坐标和球体半径;<i>r</i><sub>real</sub>—球体实际半径。</p>
                </div>
                <h3 id="93" name="93" class="anchor-tag">3 实验结果和分析</h3>
                <h4 class="anchor-tag" id="94" name="94">3.1 球体图像获取和三维重建</h4>
                <div class="p1">
                    <p id="95">实验使用普通工控机作为软件系统运行平台, 工控机采用<i>Intel</i> (<i>R</i>) <i>Core</i> (<i>TM</i>) <i>i</i>3-4030<i>U</i>处理器, 主频1.90 <i>GHz</i>。利用基于<i>Kinect V</i>2的3<i>D</i>重建系统, 实时地获取场景图像如图3所示。</p>
                </div>
                <div class="p1">
                    <p id="96">融合图3的彩色图像和深度图像生成具有色彩信息的三维点云数据如图4所示, 完成场景的三维重建。</p>
                </div>
                <div class="p1">
                    <p id="97">由图4可见:场景中有若干球体, 分别置于地面和悬挂于支架上。</p>
                </div>
                <div class="area_img" id="98">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JDGC201905021_098.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDM1V1I1M3JjbjUxV3Mxa3FjaFBuRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 Kinect获取的图像" src="Detail/GetImg?filename=images/JDGC201905021_098.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDM1V1I1M3JjbjUxV3Mxa3FjaFBuRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图3 Kinect获取的图像  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JDGC201905021_098.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDM1V1I1M3JjbjUxV3Mxa3FjaFBuRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="99">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JDGC201905021_099.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDM1V1I1M3JjbjUxV3Mxa3FjaFBuRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 球体三维重建点云图" src="Detail/GetImg?filename=images/JDGC201905021_099.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDM1V1I1M3JjbjUxV3Mxa3FjaFBuRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图4 球体三维重建点云图  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JDGC201905021_099.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDM1V1I1M3JjbjUxV3Mxa3FjaFBuRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="100" name="100">3.2 背景分割</h4>
                <div class="p1">
                    <p id="101">本研究使用色彩阈值分割方法和距离阈值分割方法, 对场景的三维点云进行背景分割。经过背景分割, 无用的背景信息完全去除了, 得到了目标球体区域的三维点云。</p>
                </div>
                <h4 class="anchor-tag" id="102" name="102">3.3 点云簇分割</h4>
                <div class="p1">
                    <p id="103">为了快速地对众多目标球体进行分割, 本研究使用欧几里得距离方法将单个球体从整体中分割出来, 结果如图5所示。</p>
                </div>
                <div class="area_img" id="104">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JDGC201905021_104.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDM1V1I1M3JjbjUxV3Mxa3FjaFBuRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 球体点云欧氏距离分割结果" src="Detail/GetImg?filename=images/JDGC201905021_104.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDM1V1I1M3JjbjUxV3Mxa3FjaFBuRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图5 球体点云欧氏距离分割结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JDGC201905021_104.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDM1V1I1M3JjbjUxV3Mxa3FjaFBuRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="105">由图5可知:各球体的点云很好地分割开来, 球体点云边缘的离群噪声点也被去除, 但对相接触的球体分割效果不稳定。</p>
                </div>
                <h4 class="anchor-tag" id="106" name="106">3.4 基于法线的条件滤波</h4>
                <div class="p1">
                    <p id="107">为了更好地分割相接触的球体, 本研究使用基于法线的滤波方法, 切断两球体点云的粘连部分, 法线阀值分割结果如图6所示。</p>
                </div>
                <div class="area_img" id="108">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JDGC201905021_108.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDM1V1I1M3JjbjUxV3Mxa3FjaFBuRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 法线阈值分割结果" src="Detail/GetImg?filename=images/JDGC201905021_108.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDM1V1I1M3JjbjUxV3Mxa3FjaFBuRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图6 法线阈值分割结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JDGC201905021_108.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDM1V1I1M3JjbjUxV3Mxa3FjaFBuRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="109">由图6可知:球面上的各点的法线计算结果与球面一致, 可见分割算法可以有效地切除两个球体粘连的点云。</p>
                </div>
                <div class="p1">
                    <p id="110">基于欧几里得分割算法得到了更好的分割效果, 如图7所示。</p>
                </div>
                <div class="area_img" id="111">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/JDGC201905021_111.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDM1V1I1M3JjbjUxV3Mxa3FjaFBuRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 欧几里得距离条件下的法线阈值分割结果" src="Detail/GetImg?filename=images/JDGC201905021_111.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDM1V1I1M3JjbjUxV3Mxa3FjaFBuRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit">图7 欧几里得距离条件下的法线阈值分割结果  <a class="btn-zoomin" href="Detail/GetImg?filename=images/JDGC201905021_111.jpg&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDM1V1I1M3JjbjUxV3Mxa3FjaFBuRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="112">由图7可知:该算法与基于法线阈值分割的算法相比有更好的分割效果。</p>
                </div>
                <h4 class="anchor-tag" id="113" name="113">3.5 球体参数计算及误差分析</h4>
                <div class="p1">
                    <p id="114">完成球体点云的分割之后, 即可用RANSAC算法计算每个球体的参数。本文对处于不同深度的半径30 mm标准球体进行了定位和尺寸测量, 分析了不同条件下球体定位和尺寸测量的正确率、球心坐标误差和半径误差。实验结果如表 (1～2) 所示。</p>
                </div>
                <div class="area_img" id="115">
                    <p class="img_tit"><b>表1 球体半径计算结果平均误差 (单位</b>:mm)  <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="115" border="1"><tr><td rowspan="2"><br />光照条件</td><td colspan="4"><br />深度<i>z</i> (Δ) </td></tr><tr><td><br />0.5 m～<br />0.8 m</td><td>0.8 m～<br />1.0 m</td><td>1.0 m～<br />1.3 m</td><td>1.3 m～<br />1.5 m</td></tr><tr><td><br />自然光</td><td>3.08</td><td>3.29</td><td>3.85</td><td>4.82</td></tr><tr><td><br />室内日光灯</td><td>2.75</td><td>3.41</td><td>3.51</td><td>5.15</td></tr><tr><td><br />加光源</td><td>2.64</td><td>3.18</td><td>3.42</td><td>4.68</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="116">
                    <p class="img_tit"><b>表2 球心坐标计算结果平均误差 (单位</b>:mm)  <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="116" border="1"><tr><td rowspan="2"><br />深度值</td><td colspan="4"><br />坐标 (Δ) </td></tr><tr><td><br /><i>x</i></td><td><i>y</i></td><td><i>z</i>未优化</td><td><i>z</i>优化后</td></tr><tr><td><br />0.5 m～0.8 m</td><td>0.93</td><td>2.29</td><td>1.67</td><td>1.12</td></tr><tr><td><br />0.8 m～1.0 m</td><td>1.47</td><td>3.63</td><td>4.10</td><td>1.57</td></tr><tr><td><br />1.0 m～1.3 m</td><td>1.38</td><td>1.74</td><td>3.38</td><td>1.27</td></tr><tr><td><br />1.3 m～1.5 m</td><td>1.17</td><td>1.70</td><td>2.58</td><td>1.38</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="117">由表1可知:本文方法在不同光照、不同深度条件下均有很好的表现, 半径平均误差在4 mm左右。由表2可知:球心坐标计算误差小于4.1 mm之间, 但是考虑到kinect V2深度相机的精度为4 mm, 本文方法计算误差在可接受范围内。</p>
                </div>
                <div class="p1">
                    <p id="118">限于篇幅本文仅给出自然光条件下的结果, 其他光照条件下的偏差相差不大。由实验可知:在不同距离上, 球心坐标误差很小。综合实验结果可知:本文所提出的方法在0.5 m～1.3 m范围内球体检测识别的正确率约为99.5%, 1.3 m～1.5 m范围的正确率约为98%, 需要说明的是“优化”对应2.5节公式 (7) 所对应的基于预知的球体半径信息对<i>z</i>坐标采用的优化方法, 深度值 (即为<i>z</i>坐标值) 以及球心坐标均以深度相机坐标系为参考。</p>
                </div>
                <h4 class="anchor-tag" id="119" name="119">3.6 算法效率分析</h4>
                <div class="p1">
                    <p id="120">通过实验可知:本文方法具有很高的实时性和准确性, 仅从相机获取一对彩色图像和深度图像即可完成球体的定位和尺寸测量, 精度高;检测10个30 mm半径的球体仅需0.6 s左右, 检测20个30 mm球体需要1.4 s左右。</p>
                </div>
                <h3 id="121" name="121" class="anchor-tag">4 结束语</h3>
                <div class="p1">
                    <p id="122">针对球体识别和定位问题, 本文采用一种利用RGB-D相机进行三维重建的方法, 对RGB-D相机的标定、深度图像和彩色图像的配准、彩色三维点云的生成等进行了研究。</p>
                </div>
                <div class="p1">
                    <p id="123">该检测方法可以在多种光照、距离条件完成对多个球体的定位与尺寸测量, 具有很高的实时性、准确性、鲁棒性, 仅从相机获取一帧图像即可在1.4 s内完成对20个球体的定位和半径测量, 正确率98%以上, 半径平均误差仅4 mm, 球心坐标误差仅2 mm左右。</p>
                </div>
                <div class="p1">
                    <p id="124">本文提出的方法不但适用于球体和类球体的识别和定位, 对于其他物体的3D重建和识别也具有借鉴价值。</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="5">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Robot grasping in clutter:Using a hierarchy of supervisors for learning from demonstrations">

                                <b>[1]</b> LASKEY M, LEE J, CHUCK C, et al.Robot grasping in clutter:Using a hierarchy of supervisors for learning from demonstrations[C].IEEE International Conference on Automation Science and Engineering, Fort Worth:IEEE, 2016.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Robotic apple harvesting using computer vision based on shape&amp;amp;colour analysis and object positioning">

                                <b>[2]</b> SHARMA D J, KAUR J.Robotic apple harvesting using computer vision based on shape &amp; colour analysis and object positioning[J].Digital Signal Processing, 2014, 6 (7) :226-233.
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Detection of Red Tomato on Plants using Image Processing Techniques">

                                <b>[3]</b> KHOSHROO A, AREFI A, KHODAEI J.Detection of red tomato on plants using image processing techniques[J].Agricultural Communications, 2014, 2 (4) :9-15.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJESC69E2D615F50E5B54599BE157D7FA938&amp;v=MTQwOThrOHZSTVg3elowT2dyanFSVkJmc1RsVExtWENPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkJyTFUwNXRGZ3pMcSt3NnM9TmlmT2ZjQytGNlRPMjRsRVlaMEtEQQ==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDM1V1I1M3JjbjUxV3Mxa3FjaFBuRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b> SERAFIN J, GRISETTI G.Using extended measurements and scene merging for efficient and robust point cloud registration[J].Robotics &amp; Autonomous Systems, 2017, 92 (6) :91-106.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Kinectfusion:real-time 3d reconstruction and interaction using a moving depth camera">

                                <b>[5]</b> IZADI S, KIM D, HILLIGES O, et al.KinectFusion:real-time 3D reconstruction and interaction using a moving depth camera[C].Proceedings of the 24th annual ACM Symposim on User Interface Software and Technology, Santa Barbara:ACM, 2011.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=DynamicFusion:reconstruction and tracking of non-rigid scenes in real-time">

                                <b>[6]</b> NEWCOMBE R A, FOX D, SEITZ S M.DynamicFusion:reconstruction and tracking of non-rigid scenes in real-time[C].2015 IEEE Conference on Computer Vision Pattern Recongnition, Boston:IEEE, 2015.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCM13091000098080&amp;v=MjUwNjlNbndaZVp0RmlubFVyYkpLRndWYVJFPU5pZklZN0s3SHRqTnI0OUZaT0lIREhRNW9CTVQ2VDRQUUgvaXJSZEdlcnFRVA==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDM1V1I1M3JjbjUxV3Mxa3FjaFBuRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b> FLEISHMAN S, DRORI l, COHENOR D.Bilateral mesh denoising[J].Pro Siggraph, 2003, 22 (3) :950-953.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Bilateral Normal Filtering for Mesh Denoising">

                                <b>[8]</b> ZHENG Y, FU H, AU K C, et al.Bilateral normal filtering for mesh denoising[J].IEEE Trans Vis Comput Graph, 2011, 17 (10) :1521-1530.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_9" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=NYJX2015S1006&amp;v=MjM1MjVyRzRIOVN2cm85RllvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN3FmWnVabkZpRG5VcnpMS3pUQmQ=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDM1V1I1M3JjbjUxV3Mxa3FjaFBuRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[9]</b> 麦春艳, 郑立华, 孙红, 等.基于RGB-D相机的果树三维重构与果实识别定位[J].农业机械学报, 2015 (s1) :35-40.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_10" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SSJD&amp;filename=SSJDAD30DE7B41F8E5CAA1430A2CE3080045&amp;v=MTg1MTJCQWs4dkdkaTZ6dCtTQTdnMzJjMmVicVVSYjZhQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQnJMVTA1dEZnekxxK3c2cz1OajdCYXNMTUhkRzQyb2czWU9wNQ==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDM1V1I1M3JjbjUxV3Mxa3FjaFBuRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[10]</b> SVEIER A, KLEPPE A L, TINGELSTAD L, et al.Object detection in point clouds using conformal geometric algebra[J].Advances in Applied Clifford Algebras, 2017, 27 (2) :1961-1976.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fast and robust edge extraction in unorganized point clouds">

                                <b>[11]</b> BAZAZIAN D, CASAS J R, RUIZ-HIDALGO J.Fast and robust edge extraction in unorganized point clouds[C].International Conference on Digital Image Computing:Techniques and Applications, Adelaide:IEEE, 2016.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_12" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES15122300162485&amp;v=MjIwODdsVXJiSktGd1ZhUkU9TmlmT2ZiSzlIOVBPckk5RlplME5DSFE4b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZpbg==&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDM1V1I1M3JjbjUxV3Mxa3FjaFBuRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[12]</b> LIMBERGER F A, OLIVEIRA M M.Real-time detection of planar regions in unorganized point clouds[J].Pattern Recognition, 2015, 48 (6) :2043-2053.
                            </a>
                        </p>
                        <p id="29">
                            <a id="bibliography_13" >
                                    <b>[13]</b>
                                 ZHANG Z Y.A flexible new technique for camera calibration[J].IEEE Transactions on Pattern Analysis and Machine Intelligence, 2000, 22 (11) :1330-1334.
                            </a>
                        </p>
                        <p id="31">
                            <a id="bibliography_14" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Automatic circle pattern extraction and camera calibration using fast adaptive binarization and plane homography">

                                <b>[14]</b> KANG D J, LEE W H.Automatic circle pattern extraction and camera calibration using fast adaptive binarization and plane homography[J].International Journal of Precision Engineering &amp; Manufacturing, 2010, 11 (1) :13-21.
                            </a>
                        </p>
                        <p id="33">
                            <a id="bibliography_15" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJWD&amp;filename=SJWD00000017953&amp;v=MTk0OTk3UjdxZForWnVGaUhrVzczSkkxMD1OaWZjYXJPNEh0SE1yNDVDYmU0TVkzazV6QmRoNGo5OVNYcVJyeG94Y01I&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDM1V1I1M3JjbjUxV3Mxa3FjaFBuRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[15]</b> SCHNABEL, R, WAHL R, KLEIN, R.Efficient RANSAC for point-cloud shape detection[J].Computer Graphics Forum, 2010, 26 (2) :214-226.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="JDGC201905021" />
        <input id="dpi" type="hidden" value="600" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhcEFLUmVhZDM1V1I1M3JjbjUxV3Mxa3FjaFBuRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JDGC201905021&amp;v=MDA1NTNvOUhaWVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdxZlp1Wm5GaURuVXJ6TEx5bk1iYkc0SDlqTXE=&amp;uid=WEEvREcwSlJHSldRa1FhcEFLUmVhZDM1V1I1M3JjbjUxV3Mxa3FjaFBuRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=842_VlRz_g1CMCLi-M0twVw-4EOFG_noWyi_OsMnrAg1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
