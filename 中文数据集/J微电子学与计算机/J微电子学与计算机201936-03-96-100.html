<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637134042949787500%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dWXYJ201903020%26RESULT%3d1%26SIGN%3dTIrH3VTF6Lb6%252b6l3OzeK7TlLxjI%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=WXYJ201903020&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=WXYJ201903020&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=WXYJ201903020&amp;v=MzExMDdUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ1Rnl6aFU3M05NalhTWkxHNEg5ak1ySTlIWklRS0RIODR2UjQ=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV1aXF1RUpsdE95VG55NWhBaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#21" data-title="1 &lt;b&gt;引言&lt;/b&gt; ">1 <b>引言</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#25" data-title="2 &lt;b&gt;模型架构设计&lt;/b&gt; ">2 <b>模型架构设计</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#28" data-title="2.1 &lt;b&gt;双流&lt;/b&gt;C3Dnet&lt;b&gt;提取视频特征&lt;/b&gt;">2.1 <b>双流</b>C3Dnet<b>提取视频特征</b></a></li>
                                                <li><a href="#32" data-title="2.2 LSTM&lt;b&gt;网络结构&lt;/b&gt;">2.2 LSTM<b>网络结构</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#46" data-title="3 &lt;b&gt;基于双中心&lt;/b&gt;loss&lt;b&gt;优化网络&lt;/b&gt; ">3 <b>基于双中心</b>loss<b>优化网络</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#62" data-title="4 &lt;b&gt;实验结果与分析&lt;/b&gt; ">4 <b>实验结果与分析</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#63" data-title="4.1 &lt;b&gt;实验环境搭建&lt;/b&gt;">4.1 <b>实验环境搭建</b></a></li>
                                                <li><a href="#66" data-title="4.2 &lt;b&gt;实验结果&lt;/b&gt;">4.2 <b>实验结果</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#79" data-title="5 &lt;b&gt;结束语&lt;/b&gt; ">5 <b>结束语</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#27" data-title="&lt;b&gt;图&lt;/b&gt;1 &lt;b&gt;总体架构设计&lt;/b&gt;"><b>图</b>1 <b>总体架构设计</b></a></li>
                                                <li><a href="#31" data-title="&lt;b&gt;图&lt;/b&gt;2 &lt;b&gt;双流卷积网络结构&lt;/b&gt;"><b>图</b>2 <b>双流卷积网络结构</b></a></li>
                                                <li><a href="#34" data-title="&lt;b&gt;图&lt;/b&gt;3 LSTM&lt;b&gt;结构单元&lt;/b&gt;"><b>图</b>3 LSTM<b>结构单元</b></a></li>
                                                <li><a href="#54" data-title="&lt;b&gt;图&lt;/b&gt;4 &lt;b&gt;中心&lt;/b&gt;&lt;i&gt;loss&lt;/i&gt;&lt;b&gt;的特征值划分&lt;/b&gt;"><b>图</b>4 <b>中心</b><i>loss</i><b>的特征值划分</b></a></li>
                                                <li><a href="#69" data-title="&lt;b&gt;表&lt;/b&gt;1 &lt;b&gt;不同网络层输出特征识别准确率的比较&lt;/b&gt;"><b>表</b>1 <b>不同网络层输出特征识别准确率的比较</b></a></li>
                                                <li><a href="#72" data-title="&lt;b&gt;表&lt;/b&gt;2 &lt;b&gt;不同&lt;/b&gt;Loss&lt;b&gt;方案对网络的影响&lt;/b&gt;"><b>表</b>2 <b>不同</b>Loss<b>方案对网络的影响</b></a></li>
                                                <li><a href="#74" data-title="&lt;b&gt;表&lt;/b&gt;3 &lt;b&gt;各种行为的混淆矩阵&lt;/b&gt;"><b>表</b>3 <b>各种行为的混淆矩阵</b></a></li>
                                                <li><a href="#77" data-title="&lt;b&gt;表&lt;/b&gt;4 &lt;b&gt;不同算法在&lt;/b&gt;KTH&lt;b&gt;上的比较结果&lt;/b&gt;"><b>表</b>4 <b>不同算法在</b>KTH<b>上的比较结果</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" title=" 杨天明, 陈志, 岳文静. 基于视频深度学习的时空双流人物动作识别模型[J]. 计算机应用, 2018, 38 (3) :895-899." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201803050&amp;v=MzAxOTFNckk5QVpJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdUZ5emhVNzNNTHo3QmQ3RzRIOW4=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV1aXF1RUpsdE95VG55NWhBaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         杨天明, 陈志, 岳文静. 基于视频深度学习的时空双流人物动作识别模型[J]. 计算机应用, 2018, 38 (3) :895-899.
                                    </a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" title=" HERATH S, HARANDI M, PORIKLI F. Going deeper into action recognition: a survey [J]. Image &amp;amp; Vision Computing, 2017, 60 (4) :4-21." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Going deeper into action recognition:A survey">
                                        <b>[2]</b>
                                         HERATH S, HARANDI M, PORIKLI F. Going deeper into action recognition: a survey [J]. Image &amp;amp; Vision Computing, 2017, 60 (4) :4-21.
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title=" KARPATHY A, TODERICI G, SHETTY S, et al. Large-scale video classification with convolutional neural networks[C]// IEEE Conference on Computer Vision and Pattern Recognition. Columbus, Ohio, IEEE Computer Society, 2014:1725-1732." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Largescale video classification with convolutional neural networks">
                                        <b>[3]</b>
                                         KARPATHY A, TODERICI G, SHETTY S, et al. Large-scale video classification with convolutional neural networks[C]// IEEE Conference on Computer Vision and Pattern Recognition. Columbus, Ohio, IEEE Computer Society, 2014:1725-1732.
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" title=" SIMONYAN K, ZISSERMAN A. Two-stream convolutional networks for action recognition in videos[J]. Computer Vision and Pattern Recognition, 2014, 1 (4) :568-576." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Two-stream convolutional networks for action recognition in videos">
                                        <b>[4]</b>
                                         SIMONYAN K, ZISSERMAN A. Two-stream convolutional networks for action recognition in videos[J]. Computer Vision and Pattern Recognition, 2014, 1 (4) :568-576.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" title=" CH&#233;RON G, LAPTEV I, SCHMID C. P-CNN: pose-based cnn features for action recognition[C]// IEEE International Conference on Computer Vision. Santiago, Chile, IEEE Computer Society, 2015:3218-3226." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=P-CNN:pose-based CNN features for action recognition">
                                        <b>[5]</b>
                                         CH&#233;RON G, LAPTEV I, SCHMID C. P-CNN: pose-based cnn features for action recognition[C]// IEEE International Conference on Computer Vision. Santiago, Chile, IEEE Computer Society, 2015:3218-3226.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" title=" DU T, BOURDEV L, FERGUS R, et al. Learning spatiotemporal features with 3D convolutional networks[C]// IEEE International Conference on Computer Vision. Rome, Italy, IEEE, 2016:4489-4497." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning spatiotemporal features with 3D convolutional networks">
                                        <b>[6]</b>
                                         DU T, BOURDEV L, FERGUS R, et al. Learning spatiotemporal features with 3D convolutional networks[C]// IEEE International Conference on Computer Vision. Rome, Italy, IEEE, 2016:4489-4497.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" title=" DONAHUE J, HENDRICKS L A, ROHRBACH M, et al. Long-term recurrent convolutional networks for visual recognition and description [J]. IEEE Transactions on Pattern Analysis &amp;amp; Machine Intelligence, 2017, 39 (4) :677-691." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Long-term recurrent convolutional networks for visual recognition and description">
                                        <b>[7]</b>
                                         DONAHUE J, HENDRICKS L A, ROHRBACH M, et al. Long-term recurrent convolutional networks for visual recognition and description [J]. IEEE Transactions on Pattern Analysis &amp;amp; Machine Intelligence, 2017, 39 (4) :677-691.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" title=" WEN Y, ZHANG K, LI Z, et al. A discriminative feature learning approach for deep face recognition[C]// Computer Vision, ECCV 2016. Amsterdam, The Netherlands, Springer International Publishing, 2016:499-515." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=A discriminative feature learning approach for deep face recognition">
                                        <b>[8]</b>
                                         WEN Y, ZHANG K, LI Z, et al. A discriminative feature learning approach for deep face recognition[C]// Computer Vision, ECCV 2016. Amsterdam, The Netherlands, Springer International Publishing, 2016:499-515.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" title=" KARPATHY A, TODERICI G, SHETTY S, et al. Large-scale video classification with convolutional neural networks[C]// IEEE Conference on Computer Vision and Pattern Recognition. Columbus, OH, USA, IEEE Computer Society, 2014:1725-1732." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Large-scale video classification with convolutional neural networks">
                                        <b>[9]</b>
                                         KARPATHY A, TODERICI G, SHETTY S, et al. Large-scale video classification with convolutional neural networks[C]// IEEE Conference on Computer Vision and Pattern Recognition. Columbus, OH, USA, IEEE Computer Society, 2014:1725-1732.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=WXYJ" target="_blank">微电子学与计算机</a>
                2019,36(03),96-100             </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于双流卷积与双中心loss的行为识别研究</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%AF%9B%E5%BF%97%E5%BC%BA&amp;code=37658543&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV1aXF1RUpsdE95VG55NWhBaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">毛志强</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%A9%AC%E7%BF%A0%E7%BA%A2&amp;code=33694900&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV1aXF1RUpsdE95VG55NWhBaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">马翠红</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%B4%94%E9%87%91%E9%BE%99&amp;code=42750919&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV1aXF1RUpsdE95VG55NWhBaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">崔金龙</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E6%AF%85&amp;code=40258246&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV1aXF1RUpsdE95VG55NWhBaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王毅</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%8D%8E%E5%8C%97%E7%90%86%E5%B7%A5%E5%A4%A7%E5%AD%A6%E7%94%B5%E6%B0%94%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=1698724&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV1aXF1RUpsdE95VG55NWhBaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">华北理工大学电气工程学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%8C%97%E4%BA%AC%E4%BA%A4%E9%80%9A%E5%A4%A7%E5%AD%A6%E6%B5%B7%E6%BB%A8%E5%AD%A6%E9%99%A2&amp;code=1749569&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV1aXF1RUpsdE95VG55NWhBaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">北京交通大学海滨学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>针对行为视频中相似动作类内差异大、类间差异小, 识别准确率不高的问题, 提出了一种基于双流卷积网络与双中心loss的行为识别方法.该方法首先构建双流卷积网络结构, 以C3Dnet模型作为双流结构的基础模型, 分别提取多尺度RGB视频帧中的表观短时运动信息和堆叠光流图中的长时运动信息;然后将双流结构提取的深度信息经长短时记忆 (LSTM) 网络解析后进行特征融合;最后, 利用基于双中心loss的2C-softmax目标函数, 来最大化类间距离和最小化类内距离, 从而实现相似动作的分类与识别.在数据集KTH上的实验结果表明, 该方法能够准确识别相似动作, 识别准确率可达98.2%, 具有很好的识别效果.</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8F%8C%E6%B5%81%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV1aXF1RUpsdE95VG55NWhBaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">双流卷积网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%B8%AD%E5%BF%83loss&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV1aXF1RUpsdE95VG55NWhBaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中心loss;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%95%BF%E7%9F%AD%E6%97%B6%E8%AE%B0%20(LSTM)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV1aXF1RUpsdE95VG55NWhBaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">长短时记 (LSTM) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%85%89%E6%B5%81%E5%9B%BE&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV1aXF1RUpsdE95VG55NWhBaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">光流图;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    毛志强, 男, (1991-) , 硕士研究生.研究方向为计算机视觉、目标检测与人体行为分析.;
                                </span>
                                <span>
                                    *马翠红, 女, (1960-) , 教授.研究方向为复杂工业系统的建模与控制、图像识别与视频分析.E-mail:864404484@qq.com.;
                                </span>
                                <span>
                                    崔金龙, 男, (1989-) , 硕士, 助教.研究方向为钢成分测量.;
                                </span>
                                <span>
                                    王毅, 男, (1994-) , 硕士研究生.研究方向为计算机视觉、目标检测与视频分析.;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-07-10</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金 (61171058);</span>
                    </p>
            </div>
                    <h1><b>Research on action recognition based on two-stream convolution and double center loss</b></h1>
                    <h2>
                    <span>MAO Zhi-qiang</span>
                    <span>MA Cui-hong</span>
                    <span>CUI Jin-long</span>
                    <span>WANG Yi</span>
            </h2>
                    <h2>
                    <span>College of Electrical Engineering, North China University of Science and Technology</span>
                    <span>Beijing Jiaotong University Haibin College</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>Aiming at the problem of large difference in similar action classes, small difference between classes in action video and low recognition accuracy, a action recognition method based on two-stream convolution network and double-center loss is proposed. The method first constructs a two-stream convolutional network structure, and uses the C3 Dnet model as the basic model of the two-stream structure to extract the apparent short-term motion information in the multi-scale RGB video frame and the long-term motion information in the stacked optical flow map respectively; Then, the depth information extracted by the two-stream structure is parsed by a long and short time memory (LSTM) network to perform feature fusion; Finally, the 2 C-softmax objective function based on dual-center loss is used to maximize the distance between classes and minimize the distance within the class, so as to classify and identify similar actions. The experimental results on the data set KTH show that the method can accurately identify similar actions, and the recognition accuracy can reach 98.2%, which has a good recognition effect.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=two-stream%20convolution%20network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV1aXF1RUpsdE95VG55NWhBaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">two-stream convolution network;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=center%20loss&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV1aXF1RUpsdE95VG55NWhBaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">center loss;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=long%20short-term%20memory%20(LSTM)%20&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV1aXF1RUpsdE95VG55NWhBaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">long short-term memory (LSTM) ;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=optical%20flow%20map&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV1aXF1RUpsdE95VG55NWhBaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">optical flow map;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-07-10</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="21" name="21" class="anchor-tag">1 <b>引言</b></h3>
                <div class="p1">
                    <p id="22">人体行为识别是计算机视觉领域非常活跃的研究课题, 其广泛应用于视频安防监控、人机交互、环境控制检测等领域, 涉及计算机视觉、模式识别、人工智能等多个学科<citation id="81" type="reference"><link href="3" rel="bibliography" /><link href="5" rel="bibliography" /><sup>[<a class="sup">1</a>,<a class="sup">2</a>]</sup></citation>.基于视觉的人体行为识别是对视频序列中人的行为进行分析和识别, 是智能监控领域的一种关键技术.</p>
                </div>
                <div class="p1">
                    <p id="23">传统的行为识别方法主要是基于手工特征的方法, 然而由于人体行为种类较多, 相似动作的类内类间差异, 使得行为识别任务准确率难以提升.近年来, 随着深度学习方法在图像识别、自然语言处理等方面取得重大的研究进展, 其在人体行为识别领域也得到了广泛的应用.2014年, Karpathy等<citation id="82" type="reference"><link href="7" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>首次利用深度卷积神经网络以连续的RGB视频帧为直接输入, 进行行为识别; Simonyan等<citation id="83" type="reference"><link href="9" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>利用双流卷积网络分别提取堆叠光流图和RGB视频中的时空特征, 虽然该模型某种程度上提取了时间维度信息, 但双流结构均是2D卷积, 识别效果并不是很好;2015年, Cheron等<citation id="84" type="reference"><link href="11" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>利用3D卷积神经网络提取时间维度信息, 是人体行为识别领域经典的模型;文献<citation id="85" type="reference">[<a class="sup">6</a>]</citation>, Du等利用C3Dnet网络提取视频特征, 并将其应用在行为识别、场景识别和视频相似度分析等领域, 并取得了不错的效果.但是以上模型, 对于行为识别任务作用重大的时域动态信息利用并不充分, 长短时记忆 (Long Short-Term Memory, LSTM) 网络是由递归神经网络 (Recurrent Neural Network, RNN) 演变而来<citation id="86" type="reference"><link href="15" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>, 能够递归学习输入序列中的长时动态信息.然而, 视频序列中人体行为识别最大的难点在于相似动作类间距离小、类内距离大, 文献<citation id="87" type="reference">[<a class="sup">8</a>]</citation>提出了一个中心loss的解决方案.中心loss通过对每一个行为识别学习一个中心, 并根据样本特征与类中心的距离进行惩罚, 从而减小类内距离, 使得学习到的特征更具表征能力和泛化能力.</p>
                </div>
                <div class="p1">
                    <p id="24">通过上述分析, 本文结合各个网络模型优点, 构建了一种基于双流卷积网络与双中心loss的行为识别模型.</p>
                </div>
                <h3 id="25" name="25" class="anchor-tag">2 <b>模型架构设计</b></h3>
                <div class="p1">
                    <p id="26">长时运动信息对于基于视频的人体行为识别准确率的提升具有重要的意义, 然而, 提取时间维度上的特征信息一直是动作识别领域中的难点.因此, 本文为了充分提取视频序列中的深度信息, 引入3D卷积网络、长短时记忆 (LSTM) 网络和双流结构模型, 利用C3Dnet构建双流卷积网络模型分别提取多尺度RGB视频帧中的表观短时运动信息和堆叠光流图中的长时运动信息, 再经过LSTM解析之后进行特征融合形成双流结构融合特征.同时, 为了进一步针对相似动作类间距离大、类内距离小的问题, 设计了一种基于双中心loss的2C-softmax目标函数, 其总体网络架构如图1所示.该网络架构主要包括:双流C3Dnet模型提取视频特征、LSTM解析视频特征、特征融合和2C-softmax目标函数.</p>
                </div>
                <div class="area_img" id="27">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/WXYJ201903020_027.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV1aXF1RUpsdE95VG55NWhBaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 总体架构设计" src="Detail/GetImg?filename=images/WXYJ201903020_027.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV1aXF1RUpsdE95VG55NWhBaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图</b>1 <b>总体架构设计</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/WXYJ201903020_027.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV1aXF1RUpsdE95VG55NWhBaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="28" name="28">2.1 <b>双流</b>C3Dnet<b>提取视频特征</b></h4>
                <div class="p1">
                    <p id="29">在原始的双流卷积网络模型及其改进模型中, 一般均是采用二维卷积神经网络分别提取时间域运动信息和空间域表观信息, 虽然在一定程度上提高了识别准确率, 但是提取的视频序列特征的表征能力并不是很强, 使得难以识别视频中人体相似动作.文献<citation id="88" type="reference">[<a class="sup">6</a>]</citation>, 将二维卷积扩展到三维, 利用了时间维度信息, 提出C3Dnet模型提取视频特征, 并将其应用在行为识别、场景识别和视频相似度分析等领域.通过3D卷积神经网络能够同时提取视频序列中的时空特征, 使得表征能力增强.虽然取得了一定效果, 但是由于采用的是单流C3Dnet网络, 使得提取的特征有限.因此, 本文在文献<citation id="89" type="reference">[<a class="sup">4</a>]</citation>的基础上, 构建了双流C3Dnet模型分别提取多尺度RGB视频帧中的表观短时运动特征和堆叠光流图中的长时运动特征.</p>
                </div>
                <div class="p1">
                    <p id="30">C3Dnet模型包括8次卷积操作, 每层卷积核大小均为3*3*3, 步长为1;5次池化操作, 除第一层池化核大小为1*2*2外, 其余池化核大小均为2*2*2;2个全连接层, 每层输出响应均为4 096维, 其具体网络结构如图2所示.</p>
                </div>
                <div class="area_img" id="31">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/WXYJ201903020_031.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV1aXF1RUpsdE95VG55NWhBaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 双流卷积网络结构" src="Detail/GetImg?filename=images/WXYJ201903020_031.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV1aXF1RUpsdE95VG55NWhBaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图</b>2 <b>双流卷积网络结构</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/WXYJ201903020_031.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV1aXF1RUpsdE95VG55NWhBaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="32" name="32">2.2 LSTM<b>网络结构</b></h4>
                <div class="p1">
                    <p id="33">长短时记忆 (LSTM) 网络是由递归神经网络 (RNN) 演变而来, 对复杂的时间维度信息更加敏感, 能够有效地解决RNN在训练过程中出现的梯度爆炸或梯度弥散的问题, 从而有利于学习到长时动态信息.其LSTM网络结构单元如图3所示.</p>
                </div>
                <div class="area_img" id="34">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/WXYJ201903020_034.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV1aXF1RUpsdE95VG55NWhBaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 LSTM结构单元" src="Detail/GetImg?filename=images/WXYJ201903020_034.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV1aXF1RUpsdE95VG55NWhBaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图</b>3 LSTM<b>结构单元</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/WXYJ201903020_034.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV1aXF1RUpsdE95VG55NWhBaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="35">该结构单元在隐层中加入了先验知识——输入门、遗忘门、输出门和输入调制门, 通过以上门结构能够将各层间信号和某一时刻的输入信号处理的更加透明.如式 (1) 所示:</p>
                </div>
                <div class="p1">
                    <p id="36" class="code-formula">
                        <mathml id="36"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtable><mtr><mtd columnalign="left"><mi>i</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mi>s</mi><mrow><mo> (</mo><mrow><mi>U</mi><msub><mrow></mrow><mrow><mi>x</mi><mi>i</mi></mrow></msub><mi>x</mi><msub><mrow></mrow><mi>t</mi></msub><mo>+</mo><mi>W</mi><msub><mrow></mrow><mrow><mi>h</mi><mi>i</mi></mrow></msub><mi>h</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>+</mo><mi>b</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mo>) </mo></mrow></mtd></mtr><mtr><mtd columnalign="left"><mi>f</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mi>s</mi><mrow><mo> (</mo><mrow><mi>U</mi><msub><mrow></mrow><mrow><mi>x</mi><mi>f</mi></mrow></msub><mi>x</mi><msub><mrow></mrow><mi>t</mi></msub><mo>+</mo><mi>W</mi><msub><mrow></mrow><mrow><mi>h</mi><mi>f</mi></mrow></msub><mi>h</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>+</mo><mi>b</mi><msub><mrow></mrow><mi>f</mi></msub></mrow><mo>) </mo></mrow></mtd></mtr><mtr><mtd columnalign="left"><mi>o</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mi>s</mi><mrow><mo> (</mo><mrow><mi>U</mi><msub><mrow></mrow><mrow><mi>x</mi><mi>o</mi></mrow></msub><mi>x</mi><msub><mrow></mrow><mi>t</mi></msub><mo>+</mo><mi>W</mi><msub><mrow></mrow><mrow><mi>h</mi><mi>o</mi></mrow></msub><mi>h</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>+</mo><mi>b</mi><msub><mrow></mrow><mi>o</mi></msub></mrow><mo>) </mo></mrow></mtd></mtr><mtr><mtd columnalign="left"><mi>g</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mi>y</mi><mrow><mo> (</mo><mrow><mi>U</mi><msub><mrow></mrow><mrow><mi>x</mi><mi>c</mi></mrow></msub><mi>x</mi><msub><mrow></mrow><mi>t</mi></msub><mo>+</mo><mi>W</mi><msub><mrow></mrow><mrow><mi>h</mi><mi>c</mi></mrow></msub><mi>h</mi><msub><mrow></mrow><mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>+</mo><mi>b</mi><msub><mrow></mrow><mi>c</mi></msub></mrow><mo>) </mo></mrow></mtd></mtr></mtable><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="37">式中, <i>U</i>代表输入权重;<i>W</i>代表<i>t</i>-1时刻隐含层递归权重;<i>b</i>为偏置项.其中LSTM门结构由非线性激活函数控制, 设<mathml id="38"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>s</mi><mrow><mo> (</mo><mi>x</mi><mo>) </mo></mrow><mo>=</mo><mrow><mo> (</mo><mrow><mn>1</mn><mo>+</mo><mi>e</mi><msup><mrow></mrow><mrow><mo>-</mo><mi>x</mi></mrow></msup></mrow><mo>) </mo></mrow><msup><mrow></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msup></mrow></math></mathml>表示sigmoid函数, <mathml id="39"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>y</mi><mrow><mo> (</mo><mi>x</mi><mo>) </mo></mrow><mo>=</mo><mfrac><mrow><mi>e</mi><msup><mrow></mrow><mi>x</mi></msup><mo>-</mo><mi>e</mi><msup><mrow></mrow><mrow><mo>-</mo><mi>x</mi></mrow></msup></mrow><mrow><mi>e</mi><msup><mrow></mrow><mi>x</mi></msup><mo>+</mo><mi>e</mi><msup><mrow></mrow><mrow><mo>-</mo><mi>x</mi></mrow></msup></mrow></mfrac></mrow></math></mathml>表示双曲正切函数.<i>i</i><sub><i>t</i></sub>决定保留哪种信息, <i>f</i><sub><i>t</i></sub>决定去除哪种信息, <i>o</i><sub><i>t</i></sub>确定记忆单元<i>c</i><sub><i>t</i></sub>哪部分将输出, <i>g</i><sub><i>t</i></sub>是由双曲正切函数创建的一个新的候选值向量.</p>
                </div>
                <div class="p1">
                    <p id="40">记忆单元<i>c</i><sub><i>t</i></sub>是LSTM的核心部分, 其作用是选择有用信息去除无用信息, 如式 (2) 所示:</p>
                </div>
                <div class="p1">
                    <p id="41"><i>c</i><sub><i>t</i></sub>=<i>f</i><sub><i>t</i></sub>×<i>c</i><sub><i>t</i>-1</sub>+<i>i</i><sub><i>t</i></sub>×<i>g</i><sub><i>t</i></sub>      (2) </p>
                </div>
                <div class="p1">
                    <p id="42">式中, <i>c</i><sub><i>t</i></sub>包括两部分, 一部分是由上一时刻记忆单元<i>c</i><sub><i>t</i>-1</sub>和遗忘门<i>f</i><sub><i>t</i></sub>相乘而得, 另一部分由输入门<i>i</i><sub><i>t</i></sub>和输入调制门<i>g</i><sub><i>t</i></sub>相乘而得.</p>
                </div>
                <div class="p1">
                    <p id="43">LSTM的输出<i>h</i><sub><i>t</i></sub>是由输出门<i>o</i><sub><i>t</i></sub>控制是否激活记忆单元<i>c</i><sub><i>t</i></sub>.如式 (3) 所示.</p>
                </div>
                <div class="p1">
                    <p id="44" class="code-formula">
                        <mathml id="44"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>h</mi><msub><mrow></mrow><mi>t</mi></msub><mo>=</mo><mi>o</mi><msub><mrow></mrow><mi>t</mi></msub><mo>×</mo><mi>y</mi><mrow><mo> (</mo><mrow><mi>c</mi><msub><mrow></mrow><mi>t</mi></msub></mrow><mo>) </mo></mrow><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="45">视频行为识别的目的是从视频序列中识别出人体动作的时空视觉模式, 然而时序是复杂多变的.通过双流C3Dnet结构提取到的特征经过LSTM网络解析后, 在时间域上会有更强的表征能力, 然后将其进行特征融合作为2C-softmax目标函数的输入, 从而完成人体行为的识别.</p>
                </div>
                <h3 id="46" name="46" class="anchor-tag">3 <b>基于双中心</b>loss<b>优化网络</b></h3>
                <div class="p1">
                    <p id="47">网络反向传播过程中是通过计算损失函数 (loss) 完成的, 在大多数的模型中一般是通过利用softmax的损失函数, 传统的softmax-loss如式 (4) 所示:</p>
                </div>
                <div class="p1">
                    <p id="48" class="code-formula">
                        <mathml id="48"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><msub><mrow></mrow><mi>j</mi></msub><mo>=</mo><mo>-</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mrow><mi>log</mi></mrow></mstyle><mfrac><mrow><mi>e</mi><msup><mrow></mrow><mrow><mi>W</mi><msubsup><mrow></mrow><mrow><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub></mrow><mi>Τ</mi></msubsup><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>+</mo><mi>b</mi><msub><mrow></mrow><mrow><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msub></mrow></msup></mrow><mrow><mstyle displaystyle="true"><msubsup><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><mi>e</mi></mstyle><msup><mrow></mrow><mrow><mi>W</mi><msubsup><mrow></mrow><mi>j</mi><mi>Τ</mi></msubsup><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>+</mo><mi>b</mi><msub><mrow></mrow><mi>j</mi></msub></mrow></msup></mrow></mfrac><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>4</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="49">式中, <i>x</i><sub><i>i</i></sub>表示第<i>i</i>个特征向量;<i>y</i><sub><i>i</i></sub>表示类别标签;<i>n</i>为类别数;<i>m</i>表示小批量大小;<i>W</i>表示权重;<i>b</i>为偏置项.</p>
                </div>
                <div class="p1">
                    <p id="50">文献<citation id="90" type="reference">[<a class="sup">8</a>]</citation>中Wen等人设计了中心softmax-loss函数用于人脸识别任务, 将特征空间中的每一个类别都保持一个类中心C, 如图4 (a) 所示.具体而言, 中心loss同时学习每个类别的深层特征的中心C, 并惩罚深层特征与其相应的类别中心之间的距离, 从而能够减小类内距离并扩大类间距离.文中loss函数包含softmax loss和中心loss两部分, 其中心softmax-loss的计算公式如式 (5) 所示:</p>
                </div>
                <div class="p1">
                    <p id="51" class="code-formula">
                        <mathml id="51"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtable><mtr><mtd columnalign="left"><mi>L</mi><msub><mrow></mrow><mi>c</mi></msub><mo>=</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mrow><mo stretchy="false">∥</mo><mi>x</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mi>c</mi><msub><mrow></mrow><mrow><mi>y</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msub><mo stretchy="false">∥</mo></mrow></mstyle><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup></mtd></mtr><mtr><mtd columnalign="left"><mi>L</mi><msub><mrow></mrow><mrow></mrow></msub><mo>=</mo><mi>L</mi><msub><mrow></mrow><mi>j</mi></msub><mo>+</mo><mi>λ</mi><mi>L</mi><msub><mrow></mrow><mi>c</mi></msub></mtd></mtr></mtable><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>5</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="52">式中, <i>x</i><sub><i>i</i></sub>表示第<i>i</i>样本特征向量;<i>c</i><sub><i>y</i><sub><i>i</i></sub></sub>表示该样本所属类别的特征值中心;<i>L</i><sub><i>c</i></sub>表示中心loss计算公式;<i>L</i><sub><i>j</i></sub>为softmax函数的loss;<i>λ</i>为两者所占比重.</p>
                </div>
                <div class="p1">
                    <p id="53">由于本文是利用双流网络结构提取的两类运动特征, 因此, 本文在中心loss的基础上设计了双中心loss, 如图4 (b) .双中心loss分别维护表观短时运动流特征中心<i>C</i><sub>AS</sub>和长时运动流特征中心<i>C</i><sub>LT</sub>, 两者按一定权重系数<i>W</i><sub>AS</sub>和<i>W</i><sub>LT</sub>融合形成质心<i>C</i><sub><i>i</i></sub>.本文采用线性加权方式确定权重系数<i>W</i><sub>AS</sub>和<i>W</i><sub>LT</sub>, 使质心<i>C</i><sub><i>i</i></sub>在<i>C</i><sub>AS</sub>和<i>C</i><sub>LT</sub>的连线之间, 从而能够保证质心<i>C</i><sub><i>i</i></sub>同时离两者之间距离最近.如式 (6) 所示:</p>
                </div>
                <div class="area_img" id="54">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/WXYJ201903020_05400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV1aXF1RUpsdE95VG55NWhBaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 中心loss的特征值划分" src="Detail/GetImg?filename=images/WXYJ201903020_05400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV1aXF1RUpsdE95VG55NWhBaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图</b>4 <b>中心</b><i>loss</i><b>的特征值划分</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/WXYJ201903020_05400.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV1aXF1RUpsdE95VG55NWhBaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="55"><i>L</i>=<i>L</i><sub><i>j</i></sub>+<i>W</i><sub>AS</sub><i>L</i><sub><i>C</i><sub>AS</sub></sub>+<i>W</i><sub>LT</sub><i>L</i><sub><i>C</i><sub>LT</sub></sub>      (6) </p>
                </div>
                <div class="p1">
                    <p id="56">式中, <i>W</i><sub>AS</sub>和<i>W</i><sub>LT</sub>为双中心loss的权值系数;<i>L</i><sub><i>C</i><sub>AS</sub></sub>表示ASM特征中心loss;<i>L</i><sub><i>C</i><sub>LT</sub></sub>表示LTM特征中心loss.</p>
                </div>
                <div class="p1">
                    <p id="57">为了防止目标函数过拟合, 可以在2C-softmax的loss中加入正则项.在网络结构中, 融合特征 (Fusion features) 单元对整个行为识别过程具有巨大的影响, 所以能够加入融合特征单元权值的二范数作为正则项, 如式 (7) 所示:</p>
                </div>
                <div class="p1">
                    <p id="58" class="code-formula">
                        <mathml id="58"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><msub><mrow></mrow><mi>F</mi></msub><mo>=</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mrow><mo stretchy="false">∥</mo><mi>W</mi><msub><mrow></mrow><mrow><mi>F</mi><msub><mrow></mrow><mi>i</mi></msub></mrow></msub><mo stretchy="false">∥</mo></mrow></mstyle><msubsup><mrow></mrow><mn>2</mn><mn>2</mn></msubsup><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>7</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="59">式中, <i>m</i>为小批量的大小;<i>W</i><sub><i>F</i><sub><i>i</i></sub></sub>为第<i>i</i>个特征样本的权值;<i>F</i>表示融合特征单元个数.那么式 (6) 可以改写为:</p>
                </div>
                <div class="p1">
                    <p id="60" class="code-formula">
                        <mathml id="60"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover accent="true"><mi>L</mi><mo>¯</mo></mover><mo>=</mo><mi>L</mi><msub><mrow></mrow><mi>j</mi></msub><mo>+</mo><mi>W</mi><msub><mrow></mrow><mrow><mtext>A</mtext><mtext>S</mtext></mrow></msub><mi>L</mi><msub><mrow></mrow><mrow><mi>C</mi><msub><mrow></mrow><mrow><mtext>A</mtext><mtext>S</mtext></mrow></msub></mrow></msub><mo>+</mo><mi>W</mi><msub><mrow></mrow><mrow><mtext>L</mtext><mtext>Τ</mtext></mrow></msub><mi>L</mi><msub><mrow></mrow><mrow><mi>C</mi><msub><mrow></mrow><mrow><mtext>L</mtext><mtext>Τ</mtext></mrow></msub></mrow></msub><mo>+</mo><mi>α</mi><mi>L</mi><msub><mrow></mrow><mi>F</mi></msub><mspace width="0.25em" /><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>8</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="61">式中, α为正则项系数.</p>
                </div>
                <h3 id="62" name="62" class="anchor-tag">4 <b>实验结果与分析</b></h3>
                <h4 class="anchor-tag" id="63" name="63">4.1 <b>实验环境搭建</b></h4>
                <div class="p1">
                    <p id="64">本文实验环境选择深度学习框架caffe平台设计实现, 网络训练采用小批量随机梯度下降法, 动量为0.9, 权值衰减率为0.000 5.双流卷积网络结构采用C3Dnet网络作为基础模型, 以16帧为一组的堆叠RGB视频帧作为表观短时运动流, 初始学习率为0.005, 以10对 (flow_<i>x</i>和flow_<i>y</i>) 光流图堆叠而成的光流组作为长时运动流输入.针对堆叠光流图的计算采用OpenCV视觉库中的稠密光流帧提取方法, 分别提取水平方向和垂直方向的光流序列.然后, 将双流结构提取的AS-fc6层特征与LT-fc7层特征通过LSTM网络解析, 得到更具表征能力的深度特征, 其学习率为0.001, 梯度阈值为15, 批次大小为100, 训练周期为150, LSTM单元隐结点数为18.对于2C-softmax的误差正则化系数α=0.000 1, W<sub>AS</sub>=0.000 07, W<sub>LT</sub>=0.000 03.</p>
                </div>
                <div class="p1">
                    <p id="65">本次实验数据集采用KTH数据集, 该数据集包括了4种场景下25个不同行人的6中行为视频:正常行走 (Walk) 、慢跑 (Jog) 、跑 (Run) 、挥拳 (Box) 、双手挥手 (Wave) 、鼓掌 (Clap) .</p>
                </div>
                <h4 class="anchor-tag" id="66" name="66">4.2 <b>实验结果</b></h4>
                <div class="p1">
                    <p id="67">实验过程中, 为了增加识别准确率可信度, 本文将KTH数据集随机划分成3组, 取其3组测试平均准确率作为评估模型效果的指标.</p>
                </div>
                <div class="p1">
                    <p id="68">首先, 本文利用LSTM网络对双流网络结构的输出特征分别进行解析, 并对比了网络层输出特征的不同方式的行为识别准确率, 如表1所示.</p>
                </div>
                <div class="area_img" id="69">
                    <p class="img_tit"><b>表</b>1 <b>不同网络层输出特征识别准确率的比较</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit">%</p>
                    <p class="img_note"></p>
                    <table id="69" border="1"><tr><td><br />Layerof output feature</td><td>KTH</td></tr><tr><td><br />AS-Conv5 &amp; L-Conv5</td><td>88.2</td></tr><tr><td><br />AS-fc6 &amp; L-fc6</td><td>95.3</td></tr><tr><td><br />AS-fc7 &amp; L-fc7</td><td>96.2</td></tr><tr><td><br />AS-fc7 &amp; L-fc6</td><td>92.7</td></tr><tr><td><br />AS-fc6 &amp; L-fc7</td><td>98.2</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="70">从表1中可以发现, 当双流C3Dnet结构的输出特征的层为卷积层时, 行为识别效果并不是很好.</p>
                </div>
                <div class="p1">
                    <p id="71">接下来, 为了验证本文算法提出的双中心loss对网络模型的作用, 在KTH数据集上对比了不同的loss设计方案对网络的影响.实验结果表明, 单纯地采用双中心loss对行为识别效果并不大, 而加入融合单元的正则项后使得识别准确率得到很明显的提升.如表2所示.</p>
                </div>
                <div class="area_img" id="72">
                    <p class="img_tit"><b>表</b>2 <b>不同</b>Loss<b>方案对网络的影响</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="72" border="1"><tr><td><br />Loss</td><td>准确率/%</td></tr><tr><td><br /><i>L</i><sub><i>j</i></sub>+<i>λL</i><sub><i>c</i></sub></td><td>92.7</td></tr><tr><td><br /><i>L</i><sub><i>j</i></sub>+<i>W</i><sub>AS</sub><i>L</i><sub><i>C</i><sub>AS</sub></sub>+<i>W</i><sub>LT</sub><i>L</i><sub><i>C</i><sub>LT</sub></sub></td><td>93.1</td></tr><tr><td><br /><i>L</i><sub><i>j</i></sub>+<i>W</i><sub>AS</sub><i>L</i><sub><i>C</i><sub>AS</sub></sub>+<i>W</i><sub>LT</sub><i>L</i><sub><i>C</i><sub>LT</sub></sub>+<i>αL</i><sub><i>F</i></sub></td><td>98.2</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="73">此外, 通过训练数据集对网络模型进行训练后, 再通过测试数据集对本文模型进行测试, 可以得到各种行为识别的混淆矩阵, 如表3所示.</p>
                </div>
                <div class="area_img" id="74">
                    <p class="img_tit"><b>表</b>3 <b>各种行为的混淆矩阵</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="74" border="1"><tr><td><br />行为</td><td>Walk</td><td>Jog</td><td>Run</td><td>Box</td><td>Wave</td><td>Clap</td></tr><tr><td><br />Walk</td><td><b>0.96</b></td><td>0.03</td><td>0</td><td>0.009</td><td>0</td><td>0</td></tr><tr><td><br />Jog</td><td>0.02</td><td><b>0.97</b></td><td>0.01</td><td>0</td><td>0</td><td>0</td></tr><tr><td><br />Run</td><td>0.01</td><td>0.01</td><td><b>0.99</b></td><td>0</td><td>0</td><td>0</td></tr><tr><td><br />Box</td><td>0</td><td>0</td><td>0</td><td><b>0.98</b></td><td>0</td><td>0.01</td></tr><tr><td><br />Wave</td><td>0</td><td>0</td><td>0</td><td>0</td><td><b>1.00</b></td><td>0</td></tr><tr><td><br />Clap</td><td>0</td><td>0</td><td>0</td><td>0.01</td><td>0</td><td><b>0.99</b></td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="75">从表3中可以看出, 本文提出的算法能够识别出绝大多数的行为动作.对于Jog与Walk、Run是最容易混淆的, 属于相似动作, 然而识别的误识别率只有0.01～0.03.因此, 通过混淆矩阵可以看出, 本文算法针对相似动作行为具有很好的识别效果.</p>
                </div>
                <div class="p1">
                    <p id="76">最后, 将本文算法与现今主流的行为识别算法<citation id="91" type="reference"><link href="9" rel="bibliography" /><link href="13" rel="bibliography" /><link href="19" rel="bibliography" /><sup>[<a class="sup">4</a>,<a class="sup">6</a>,<a class="sup">9</a>]</sup></citation>在KTH数据集上进行比较, 如表4所示.</p>
                </div>
                <div class="area_img" id="77">
                    <p class="img_tit"><b>表</b>4 <b>不同算法在</b>KTH<b>上的比较结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note">%</p>
                    <table id="77" border="1"><tr><td><br />算法</td><td>文献[4]</td><td>文献[6]</td><td>文献[9]</td><td>Ours (无<br />LSTM解析) </td><td>Ours</td></tr><tr><td><br />Walk</td><td>93</td><td>94</td><td>93</td><td>93</td><td>96</td></tr><tr><td><br />Jog</td><td>87</td><td>91</td><td>82</td><td>91</td><td>97</td></tr><tr><td><br />Run</td><td>86</td><td>89</td><td>79</td><td>97</td><td>99</td></tr><tr><td><br />Box</td><td>100</td><td>99</td><td>95</td><td>99</td><td>98</td></tr><tr><td><br />Wave</td><td>97</td><td>98</td><td>99</td><td>100</td><td>100</td></tr><tr><td><br />Clap</td><td>96</td><td>92</td><td>93</td><td>92</td><td>99</td></tr><tr><td><br />识别率</td><td>93.2</td><td>93.8</td><td>90.1</td><td>95.3</td><td>98.2</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="78">从表4中可以发现, 本文提出的基于双流卷积网络与双中心loss的行为识别效果优于其他算法, 而且通过加入LSTM解析双流结构特征后, 能够提高识别准确率.</p>
                </div>
                <h3 id="79" name="79" class="anchor-tag">5 <b>结束语</b></h3>
                <div class="p1">
                    <p id="80">本文提出的基于双流卷积网络和双中心loss的行为识别方法, 从相似动作行为类内类间距离差异角度出发, 通过双流C3Dnet结构分别提取多尺度RGB视频帧中的表观短时运动特征和堆叠光流图中的长时运动光流特征;然后采用LSTM网络解析双流结构AS-fc6层和L-fc7层的输出特征, 并进行特征融合;最后, 利用基于双中心loss设计的2C-softmax目标函数对网络模型进行优化, 实现行为识别任务.在KTH数据集上的实验结果表明, 本文算法的识别效果与其他算法相比更具优势, 而且针对某些相似动作也具有很高的识别准确率.</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201803050&amp;v=MTgwMDc0SDluTXJJOUFaSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnVGeXpoVTczTUx6N0JkN0c=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV1aXF1RUpsdE95VG55NWhBaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> 杨天明, 陈志, 岳文静. 基于视频深度学习的时空双流人物动作识别模型[J]. 计算机应用, 2018, 38 (3) :895-899.
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Going deeper into action recognition:A survey">

                                <b>[2]</b> HERATH S, HARANDI M, PORIKLI F. Going deeper into action recognition: a survey [J]. Image &amp; Vision Computing, 2017, 60 (4) :4-21.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Largescale video classification with convolutional neural networks">

                                <b>[3]</b> KARPATHY A, TODERICI G, SHETTY S, et al. Large-scale video classification with convolutional neural networks[C]// IEEE Conference on Computer Vision and Pattern Recognition. Columbus, Ohio, IEEE Computer Society, 2014:1725-1732.
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Two-stream convolutional networks for action recognition in videos">

                                <b>[4]</b> SIMONYAN K, ZISSERMAN A. Two-stream convolutional networks for action recognition in videos[J]. Computer Vision and Pattern Recognition, 2014, 1 (4) :568-576.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=P-CNN:pose-based CNN features for action recognition">

                                <b>[5]</b> CHéRON G, LAPTEV I, SCHMID C. P-CNN: pose-based cnn features for action recognition[C]// IEEE International Conference on Computer Vision. Santiago, Chile, IEEE Computer Society, 2015:3218-3226.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning spatiotemporal features with 3D convolutional networks">

                                <b>[6]</b> DU T, BOURDEV L, FERGUS R, et al. Learning spatiotemporal features with 3D convolutional networks[C]// IEEE International Conference on Computer Vision. Rome, Italy, IEEE, 2016:4489-4497.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Long-term recurrent convolutional networks for visual recognition and description">

                                <b>[7]</b> DONAHUE J, HENDRICKS L A, ROHRBACH M, et al. Long-term recurrent convolutional networks for visual recognition and description [J]. IEEE Transactions on Pattern Analysis &amp; Machine Intelligence, 2017, 39 (4) :677-691.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=A discriminative feature learning approach for deep face recognition">

                                <b>[8]</b> WEN Y, ZHANG K, LI Z, et al. A discriminative feature learning approach for deep face recognition[C]// Computer Vision, ECCV 2016. Amsterdam, The Netherlands, Springer International Publishing, 2016:499-515.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Large-scale video classification with convolutional neural networks">

                                <b>[9]</b> KARPATHY A, TODERICI G, SHETTY S, et al. Large-scale video classification with convolutional neural networks[C]// IEEE Conference on Computer Vision and Pattern Recognition. Columbus, OH, USA, IEEE Computer Society, 2014:1725-1732.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="WXYJ201903020" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV1aXF1RUpsdE95VG55NWhBaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=WXYJ201903020&amp;v=MzExMDdUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ1Rnl6aFU3M05NalhTWkxHNEg5ak1ySTlIWklRS0RIODR2UjQ=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9ralV1aXF1RUpsdE95VG55NWhBaz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
