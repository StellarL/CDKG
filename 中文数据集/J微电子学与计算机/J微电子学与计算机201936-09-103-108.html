<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637133837650100000%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dWXYJ201909020%26RESULT%3d1%26SIGN%3dDj3snTCkqBczwX9ZGaQEiurSHn0%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=WXYJ201909020&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=WXYJ201909020&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=WXYJ201909020&amp;v=MTI5MzA1NE8zenFxQnRHRnJDVVJMT2VaZVZ1Rnlua1c3elBNalhTWkxHNEg5ak1wbzlIWklRS0RIODR2UjRUNmo=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bFF0dU9NTmdMQ0V1SURuSmxZZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#21" data-title="1 &lt;b&gt;引言&lt;/b&gt; ">1 <b>引言</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#25" data-title="2 &lt;b&gt;系统结构&lt;/b&gt; ">2 <b>系统结构</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#28" data-title="2.1 &lt;b&gt;特征提取模块&lt;/b&gt;">2.1 <b>特征提取模块</b></a></li>
                                                <li><a href="#31" data-title="2.2 &lt;b&gt;卷积神经网络&lt;/b&gt;">2.2 <b>卷积神经网络</b></a></li>
                                                <li><a href="#47" data-title="2.3 &lt;b&gt;后处理&lt;/b&gt;">2.3 <b>后处理</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#55" data-title="3 &lt;b&gt;模型结构&lt;/b&gt; ">3 <b>模型结构</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#56" data-title="3.1 &lt;b&gt;传统卷积神经网络模型&lt;/b&gt;">3.1 <b>传统卷积神经网络模型</b></a></li>
                                                <li><a href="#63" data-title="3.2 &lt;b&gt;深度可分离卷积神经网络&lt;/b&gt;">3.2 <b>深度可分离卷积神经网络</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#68" data-title="4 &lt;b&gt;实验结果与分析&lt;/b&gt; ">4 <b>实验结果与分析</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#77" data-title="5 &lt;b&gt;结束语&lt;/b&gt; ">5 <b>结束语</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#27" data-title="&lt;b&gt;图&lt;/b&gt;1 &lt;b&gt;关键词识别系统结构&lt;/b&gt;"><b>图</b>1 <b>关键词识别系统结构</b></a></li>
                                                <li><a href="#36" data-title="&lt;b&gt;图&lt;/b&gt;2 &lt;b&gt;传统卷积操作&lt;/b&gt;"><b>图</b>2 <b>传统卷积操作</b></a></li>
                                                <li><a href="#37" data-title="&lt;b&gt;图&lt;/b&gt;3 &lt;b&gt;深度可分离卷积操作&lt;/b&gt;"><b>图</b>3 <b>深度可分离卷积操作</b></a></li>
                                                <li><a href="#58" data-title="&lt;b&gt;表&lt;/b&gt;1 &lt;b&gt;典型卷积神经网络模型&lt;/b&gt;"><b>表</b>1 <b>典型卷积神经网络模型</b></a></li>
                                                <li><a href="#62" data-title="&lt;b&gt;表&lt;/b&gt;2 low_cnn&lt;b&gt;网络参数&lt;/b&gt;"><b>表</b>2 low_cnn<b>网络参数</b></a></li>
                                                <li><a href="#65" data-title="&lt;b&gt;表&lt;/b&gt;3 &lt;b&gt;类&lt;/b&gt;MobileNets&lt;b&gt;网络参数&lt;/b&gt;"><b>表</b>3 <b>类</b>MobileNets<b>网络参数</b></a></li>
                                                <li><a href="#67" data-title="&lt;b&gt;表&lt;/b&gt;4 Low_MobileNets&lt;b&gt;网络参数&lt;/b&gt;"><b>表</b>4 Low_MobileNets<b>网络参数</b></a></li>
                                                <li><a href="#72" data-title="&lt;b&gt;表&lt;/b&gt;5 &lt;b&gt;各测试集网络识别率&lt;/b&gt;"><b>表</b>5 <b>各测试集网络识别率</b></a></li>
                                                <li><a href="#75" data-title="&lt;b&gt;图&lt;/b&gt;4 &lt;b&gt;各网络&lt;/b&gt;ROC&lt;b&gt;曲线&lt;/b&gt;"><b>图</b>4 <b>各网络</b>ROC<b>曲线</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" >
                                        <b>[1]</b>
                                     郑方.连续无限制语音流中关键词识别方法研究[D].北京:清华大学,1997.</a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" title=" CHEN G,PARADA C,HEIGOLD G.Small-footprint keyword spotting using deep neural networks[C]//2014 IEEE International Conference on Acoustics,Speech and Signal Processing (ICASSP).Florence,Italy ,IEEE,2014:4087-4091." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Small-footprint Keyword Spotting Using Deep Neural Networks">
                                        <b>[2]</b>
                                         CHEN G,PARADA C,HEIGOLD G.Small-footprint keyword spotting using deep neural networks[C]//2014 IEEE International Conference on Acoustics,Speech and Signal Processing (ICASSP).Florence,Italy ,IEEE,2014:4087-4091.
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title=" LECUN Y,BENGIO Y.Convolutional networks for images,speech,and time series[J].The handbook of brain theory and neural networks,1995,3361(10):66-68." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Convolutional networks for images,speech,and time-series">
                                        <b>[3]</b>
                                         LECUN Y,BENGIO Y.Convolutional networks for images,speech,and time series[J].The handbook of brain theory and neural networks,1995,3361(10):66-68.
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" title=" 张晴晴,刘勇,潘接林,等.基于卷积神经网络的连续语音识别[J].北京科技大学学报,2015,37(9):1212-1217." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=BJKD201509015&amp;v=MzE4MThRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ1Rnlua1c3elBKeWZBYXJHNEg5VE1wbzlFWVk=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bFF0dU9NTmdMQ0V1SURuSmxZZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[4]</b>
                                         张晴晴,刘勇,潘接林,等.基于卷积神经网络的连续语音识别[J].北京科技大学学报,2015,37(9):1212-1217.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" title=" 惠博.语音识别特征提取算法的研究及实现[D].西安:西北大学,2008." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=2008076685.nh&amp;v=MTMyMTBmRXFwRWJQSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnVGeW5rVzd6UFYxMjdGck8vR04=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bFF0dU9NTmdMQ0V1SURuSmxZZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                         惠博.语音识别特征提取算法的研究及实现[D].西安:西北大学,2008.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" title=" 刘华平,李昕,郑宇,等.一种改进的自适应子带谱熵语音端点检测方法[J].系统仿真学报,2008,20(5):1366-1371." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XTFZ200805066&amp;v=MDk4MzF1Rnlua1c3elBQVG5OZExHNEh0bk1xbzlEWW9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVY=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bFF0dU9NTmdMQ0V1SURuSmxZZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                         刘华平,李昕,郑宇,等.一种改进的自适应子带谱熵语音端点检测方法[J].系统仿真学报,2008,20(5):1366-1371.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" title=" HOWARD A G,ZHU M,CHEN B,et al.Mobilenets:Efficient convolutional neural networks for mobile vision applications[z].arXiv preprint arXiv:1704.04861,2017." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Mobilenets:Efficient convolutional neural networks for mobile vision applications">
                                        <b>[7]</b>
                                         HOWARD A G,ZHU M,CHEN B,et al.Mobilenets:Efficient convolutional neural networks for mobile vision applications[z].arXiv preprint arXiv:1704.04861,2017.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" title=" SZEGEDY C,LIU W,JIA Y,et al.Going deeper with convolutions[C]//Proceedings of the IEEE conference on computer vision and pattern recognition.Boston,USA,2015:1-9." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Going deeper with convolutions">
                                        <b>[8]</b>
                                         SZEGEDY C,LIU W,JIA Y,et al.Going deeper with convolutions[C]//Proceedings of the IEEE conference on computer vision and pattern recognition.Boston,USA,2015:1-9.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" title=" WARDEN P.Speech commands:A dataset for limited-vocabulary speech recognition[J].arXiv preprint arXiv:1804.03209,2018." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Speech commands:A dataset for limited-vocabulary speech recognition">
                                        <b>[9]</b>
                                         WARDEN P.Speech commands:A dataset for limited-vocabulary speech recognition[J].arXiv preprint arXiv:1804.03209,2018.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=WXYJ" target="_blank">微电子学与计算机</a>
                2019,36(09),103-108             </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于深度可分离卷积神经网络的关键词识别系统</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E5%B8%85&amp;code=07593051&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bFF0dU9NTmdMQ0V1SURuSmxZZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王帅</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BD%AD%E6%84%8F%E5%85%B5&amp;code=39921780&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bFF0dU9NTmdMQ0V1SURuSmxZZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">彭意兵</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E4%BD%95%E9%A1%B6%E6%96%B0&amp;code=07594735&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bFF0dU9NTmdMQ0V1SURuSmxZZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">何顶新</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%8D%8E%E4%B8%AD%E7%A7%91%E6%8A%80%E5%A4%A7%E5%AD%A6%E8%87%AA%E5%8A%A8%E5%8C%96%E5%AD%A6%E9%99%A2&amp;code=0045381&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bFF0dU9NTmdMQ0V1SURuSmxZZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">华中科技大学自动化学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>关键词识别系统是智能语音交互系统的重要组成部分.本文使用Google语音命令数据集,探索使用传统卷积神经网络和深度可分离卷积神经网络在关键词识别任务中的应用,对两种卷积神经网络模型从识别率、计算量、内存消耗进行对比,并提出适用于受限设备的低资源、较高识别率的网络模型.实验结果显示无论传统卷积神经网络还是深度可分离卷积神经网络在关键词识别任务中的表现都优于传统的的隐马尔科夫模型和全连接深度学习模型,而深度可分离卷积神经网络进一步优于传统卷积神经网络.</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%85%B3%E9%94%AE%E8%AF%8D%E8%AF%86%E5%88%AB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bFF0dU9NTmdMQ0V1SURuSmxZZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">关键词识别;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bFF0dU9NTmdMQ0V1SURuSmxZZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">卷积神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E5%8F%AF%E5%88%86%E7%A6%BB%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bFF0dU9NTmdMQ0V1SURuSmxZZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度可分离卷积神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8F%97%E9%99%90%E8%AE%BE%E5%A4%87&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bFF0dU9NTmdMQ0V1SURuSmxZZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">受限设备;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    王帅,男,(1993-),硕士研究生.研究方向为语音识别、深度学习.E-mail:13659829769@163.com.;
                                </span>
                                <span>
                                    彭意兵,男,(1994-),硕士研究生.研究方向为深度学习、网络安全.;
                                </span>
                                <span>
                                    何顶新,男,(1966-),硕士,教授.研究方向为系统仿真、控制工程.;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-12-13</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金(616750416);</span>
                    </p>
            </div>
                    <h1><b>Keywords spotting system based on deepwise separable convolutional neural network</b></h1>
                    <h2>
                    <span>WANG Shuai</span>
                    <span>PENG Yi-bing</span>
                    <span>Newest He</span>
            </h2>
                    <h2>
                    <span>College of Automation, Huazhong University of Science and Technology</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>The keyword spotting system is an important part of the intelligent voice interaction system. We explore the application of convolution neural networks and depthwise separable convolution neural networks to the keyword spotting task, using the Google Speech Commands Dataset as our benchmark. We will make comparison of recognition rate, calculation amount, and storage consumption for two convolutional neural network models and propose a network model with low resource and high recognition rate for restricted devices. The experimental results show that both the traditional convolutional neural networks and the deep separable convolutional neural networks perform better than the traditional Hidden Markov model and deep learning model based on fully connected neural networks in the keyword spotting task, while the depthwise separable convolutional neural networks is more superior to the convolutional neural networks.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=spotting&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bFF0dU9NTmdMQ0V1SURuSmxZZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">spotting;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=convolution%20neural%20network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bFF0dU9NTmdMQ0V1SURuSmxZZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">convolution neural network;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=depthwise%20separable%20convolution%20neural%20network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bFF0dU9NTmdMQ0V1SURuSmxZZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">depthwise separable convolution neural network;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=restricted%20device&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bFF0dU9NTmdMQ0V1SURuSmxZZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">restricted device;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-12-13</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="21" name="21" class="anchor-tag">1 <b>引言</b></h3>
                <div class="p1">
                    <p id="22">关键词识别(Keyword Recognition, KWS)是指在连续的,无限制的自然语音流中识别出预先设定的一个或多个关键词<citation id="79" type="reference"><link href="3" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>,其广泛应用于设备唤醒、语音检索、人机交互、语音监听等领域.</p>
                </div>
                <div class="p1">
                    <p id="23">近年来,深度学习算法取得了快速发展,他们在图像分类、语音识别等多种认知任务中接近甚至超越了人类的识别准度.在基于深度学习的语音识别技术的推动下,语音逐渐成为用户和电子设备之间最自然的交互方式,例如亚马逊的Echo,苹果的Siri和谷歌的Google Home等.然而,一直在线的语音识别系统会使设备具有较高的功耗,且将连续语音流上传至云端会造成网络拥塞,增加应用延迟,带来不好的用户体验.为了缓解上述问题,可以使用预定义的关键词唤醒设备,例如“Alexa”,“OK Google”,“Hey Siri”等,然后启动后续的语音识别系统.关键词识别系统需要在移动设备上持续运行,因此必须具有较小的内存占用和较低的计算量.</p>
                </div>
                <div class="p1">
                    <p id="24">谷歌目前的关键词识别系统使用的是深度神经网络(DNN)<citation id="80" type="reference"><link href="5" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>.基于深度神经网络的关键词识别系统已被证明在模型计算量、内存占用、识别精度等方面优于基于隐马尔可夫模型系统的传统关键词识别系统<citation id="81" type="reference"><link href="5" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>.卷积神经网络(CNN)在过去几年中逐渐成为声学建模的主流<citation id="82" type="reference"><link href="7" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>,在各种小型和大型的词汇任务显示出相对对深度神经网络的优势. 与深度神经网络相比,卷积神经网络引入了卷积和聚合(又作采样) 的概念.卷积神经网络通过卷积实现对语音特征局部信息的抽取,再通过聚合加强模型对特征的鲁棒性<citation id="83" type="reference"><link href="9" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>,在一定程度上屏蔽了不同说话人或相同说话人不同说话语调可能导致的语音信号在频域上的位移、缩放和倾斜.</p>
                </div>
                <h3 id="25" name="25" class="anchor-tag">2 <b>系统结构</b></h3>
                <div class="p1">
                    <p id="26">本文使用的基于卷积神经网络的关键词识别系统结构参照文献<citation id="84" type="reference">[<a class="sup">2</a>]</citation>设计,如下图1所示,结构包含三个部分:(1)特征提取模块;(2)卷积神经网络模块;(3)后处理模块.特征提取模块提取有效语音段的每一帧特征向量,并将多帧特征向量组成特征图,作为卷积神经网络的输入.训练卷积神经网络模型来预测组合特征中每个输出标签的后验概率,输出层的标签可以是整个关键词或是组成关键词的单词.最后是一个简单的后处理模块,结合每帧产生的后验概率产生一个关键概率,用于判断语音流中是否含有预先设定的关键词.</p>
                </div>
                <div class="area_img" id="27">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/WXYJ201909020_02700.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bFF0dU9NTmdMQ0V1SURuSmxZZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 关键词识别系统结构" src="Detail/GetImg?filename=images/WXYJ201909020_02700.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bFF0dU9NTmdMQ0V1SURuSmxZZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图</b>1 <b>关键词识别系统结构</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/WXYJ201909020_02700.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bFF0dU9NTmdMQ0V1SURuSmxZZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="28" name="28">2.1 <b>特征提取模块</b></h4>
                <div class="p1">
                    <p id="29">特征提取就是从语音信号中提取出语音的特征序列.提取的语音特征应该能完全、准确的表达语音信号,特征提取的目的是提取语音信号中能代表语音特征的信息,减少语音识别时所要处理的数据量.目前,语音识别技术中常用的特征参数有基于声道模型和听觉机理的线性预测倒谱系数(Linear Predictive Cepstral Coefficients, LPCC)和梅尔频率倒谱系数(Mel-Frequency Cepstral Coefficients, MFCC)<citation id="85" type="reference"><link href="11" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>.本文采用的是40维滤波器组梅尔频率倒谱系数,语音帧长为40 ms,帧移为20 ms,取离散余弦变换的第1个到第13个系数,一阶差分和二阶差分作为MFCC特征向量,这个MFCC特征向量就是这一帧语音的特征.将这一帧特征向量与前30帧、后9帧特征向量组成维度为39*40的特征图,作为卷积神经网络的输入.一段长语音相邻两个特征图的位移为4帧,跨度约为80 ms.</p>
                </div>
                <div class="p1">
                    <p id="30">为了减少计算量,使用端点检测系统截取一段语音流中的有效语音段,文中采用改进的自适应子带谱熵端点检测算法<citation id="86" type="reference"><link href="13" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>,该算法结合加权功率谱减的子带自适应谱熵法,采用边降噪边用稳健性好的特征参数做语音端点检测,实验结果表明该方法具有良好的鲁棒性,在信噪比不同的加性噪声下系统识别率都很高.</p>
                </div>
                <h4 class="anchor-tag" id="31" name="31">2.2 <b>卷积神经网络</b></h4>
                <div class="p1">
                    <p id="32">图2表示的是一个标准的卷积神经网络卷积过程,输入为<i>D</i><sub><i>F</i></sub>×<i>D</i><sub><i>F</i></sub>×<i>M</i>,<i>D</i><sub><i>F</i></sub>为输入的宽度和高度,<i>M</i>为输入的通道数;卷积核大小为<i>D</i><sub><i>K</i></sub>×<i>D</i><sub><i>K</i></sub>,输出的特征大小为<i>D</i><sub><i>G</i></sub>×<i>D</i><sub><i>G</i></sub>,当步长为1,在输入周围补零时,<i>D</i><sub><i>G</i></sub>=<i>D</i><sub><i>F</i></sub>,输出维度为<i>N</i>.标准卷积的输出特征映射,假设步长为1和填充,计算如下:</p>
                </div>
                <div class="p1">
                    <p id="33" class="code-formula">
                        <mathml id="33"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>G</mi><msub><mrow></mrow><mrow><mi>k</mi><mo>,</mo><mi>l</mi><mo>,</mo><mi>n</mi></mrow></msub><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>i</mi><mo>,</mo><mi>j</mi><mo>,</mo><mi>m</mi><mo>,</mo><mi>n</mi></mrow></munder><mi>Κ</mi></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mo>,</mo><mi>j</mi><mo>,</mo><mi>m</mi><mo>,</mo><mi>n</mi></mrow></msub><mo>×</mo><mi>F</mi><msub><mrow></mrow><mrow><mi>k</mi><mo>+</mo><mi>i</mi><mo>-</mo><mn>1</mn><mo>,</mo><mi>l</mi><mo>+</mo><mi>j</mi><mo>-</mo><mn>1</mn><mo>,</mo><mi>m</mi></mrow></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="34"><i>K</i>为卷积核向量,维度为<i>D</i><sub><i>K</i></sub>×<i>D</i><sub><i>K</i></sub>×<i>M</i>×<i>N</i>;<i>F</i>为输入向量,维度为<i>D</i><sub><i>F</i></sub>×<i>D</i><sub><i>F</i></sub>×<i>M</i>.标准卷积的乘法操作数为:</p>
                </div>
                <div class="p1">
                    <p id="35"><i>D</i><sub><i>K</i></sub>×<i>D</i><sub><i>K</i></sub>×<i>M</i>×<i>N</i>×<i>D</i><sub><i>F</i></sub>×<i>D</i><sub><i>F</i></sub>      (2)</p>
                </div>
                <div class="area_img" id="36">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/WXYJ201909020_036.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bFF0dU9NTmdMQ0V1SURuSmxZZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 传统卷积操作" src="Detail/GetImg?filename=images/WXYJ201909020_036.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bFF0dU9NTmdMQ0V1SURuSmxZZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图</b>2 <b>传统卷积操作</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/WXYJ201909020_036.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bFF0dU9NTmdMQ0V1SURuSmxZZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="37">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/WXYJ201909020_037.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bFF0dU9NTmdMQ0V1SURuSmxZZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 深度可分离卷积操作" src="Detail/GetImg?filename=images/WXYJ201909020_037.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bFF0dU9NTmdMQ0V1SURuSmxZZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图</b>3 <b>深度可分离卷积操作</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/WXYJ201909020_037.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bFF0dU9NTmdMQ0V1SURuSmxZZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="38">标准卷积操作具有基于卷积内核提取特征并组合特征以产生新表示的效果<citation id="87" type="reference"><link href="15" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>.提取特征和结合特征的步骤可以通过可分离卷积分成两步来降低计算消耗.深度可分离卷积计算过程如图3所示,由两层组成:深度卷积和逐点卷积.MobileNets中,深度卷积对每一个输入通道使用一个卷积核做卷积操作,然后使用1×1卷积将深度卷积的输出结合得到特征.MobileNets在深度卷积层和逐点卷积层都使用Batchnorm和ReLU对输出做进一步处理.深度卷积的输出计算如下:</p>
                </div>
                <div class="p1">
                    <p id="39" class="code-formula">
                        <mathml id="39"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msup><mi>G</mi><mo>′</mo></msup><msub><mrow></mrow><mrow><mi>k</mi><mo>,</mo><mi>l</mi><mo>,</mo><mi>n</mi></mrow></msub><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></munder><msup><mi>Κ</mi><mo>′</mo></msup></mstyle><msub><mrow></mrow><mrow><mi>i</mi><mo>,</mo><mi>j</mi><mo>,</mo><mi>m</mi></mrow></msub><mo>×</mo><mi>F</mi><msub><mrow></mrow><mrow><mi>k</mi><mo>+</mo><mi>i</mi><mo>-</mo><mn>1</mn><mo>,</mo><mi>l</mi><mo>+</mo><mi>j</mi><mo>-</mo><mn>1</mn><mo>,</mo><mi>m</mi></mrow></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>3</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="40"><i>K</i> ′为深度卷积的卷积核向量,维度为<i>D</i><sub><i>K</i></sub>×<i>D</i><sub><i>K</i></sub>×<i>M</i>,<i>K</i> ′的第<i>m</i>个卷积核对输入的第<i>m</i>个通道做卷积操作并获得<i>G</i>′的第<i>m</i>个输出.所以深度卷积操作的乘法操作数为:</p>
                </div>
                <div class="p1">
                    <p id="41"><i>D</i><sub><i>K</i></sub>×<i>D</i><sub><i>K</i></sub>×<i>M</i>×<i>D</i><sub><i>F</i></sub>×<i>D</i><sub><i>F</i></sub>      (4)</p>
                </div>
                <div class="p1">
                    <p id="42">深度卷积和传统卷积相比非常的高效,但是他只是对每一个输入通道 进行卷积操作,并没有将各通道卷积组合成成新的特征.因此需要通过1×1卷积计算深度卷积的输出的线性组合来产生新的特征.深度卷积和1×1(逐点)卷积的组合称为深度可分离卷积.逐点卷积操作的乘法操作数为:</p>
                </div>
                <div class="p1">
                    <p id="43"><i>M</i>×<i>N</i>×<i>D</i><sub><i>G</i></sub>×<i>D</i><sub><i>G</i></sub>      (5)</p>
                </div>
                <div class="p1">
                    <p id="44">所以深度可分离卷积的计算量和传统卷积操作的计算量之比为:</p>
                </div>
                <div class="p1">
                    <p id="45" class="code-formula">
                        <mathml id="45"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mi>D</mi><msub><mrow></mrow><mi>Κ</mi></msub><mo>×</mo><mi>D</mi><msub><mrow></mrow><mi>Κ</mi></msub><mo>×</mo><mi>Μ</mi><mo>×</mo><mi>D</mi><msub><mrow></mrow><mi>F</mi></msub><mo>×</mo><mi>D</mi><msub><mrow></mrow><mi>F</mi></msub><mo>+</mo><mi>Μ</mi><mo>×</mo><mi>Ν</mi><mo>×</mo><mi>D</mi><msub><mrow></mrow><mi>G</mi></msub><mo>×</mo><mi>D</mi><msub><mrow></mrow><mi>G</mi></msub></mrow><mrow><mi>D</mi><msub><mrow></mrow><mi>Κ</mi></msub><mo>×</mo><mi>D</mi><msub><mrow></mrow><mi>Κ</mi></msub><mo>×</mo><mi>Μ</mi><mo>×</mo><mi>Ν</mi><mo>×</mo><mi>D</mi><msub><mrow></mrow><mi>F</mi></msub><mo>×</mo><mi>D</mi><msub><mrow></mrow><mi>F</mi></msub></mrow></mfrac><mo>=</mo><mfrac><mn>1</mn><mi>Ν</mi></mfrac><mo>+</mo><mfrac><mn>1</mn><mrow><mi>D</mi><msubsup><mrow></mrow><mi>Κ</mi><mn>2</mn></msubsup></mrow></mfrac><mtext> </mtext><mi>D</mi><msub><mrow></mrow><mi>G</mi></msub><mo>=</mo><mi>D</mi><msub><mrow></mrow><mi>F</mi></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>6</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="46">如果使用的卷积核大小为3的深度卷积,那么可分离卷积的计算量比标准卷积少8～9倍,单精度只有很少的降低.</p>
                </div>
                <h4 class="anchor-tag" id="47" name="47">2.3 <b>后处理</b></h4>
                <div class="p1">
                    <p id="48">卷积神经网络输出特征图各标签的后验概率,后处理将后验概率转为关键词/关键字的概率值<citation id="88" type="reference"><link href="5" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>.根据关键概率值和预先设定的阈值比较结果,判定语言流中是否存在关键词.本文中的后处理参考文献<citation id="89" type="reference">[<a class="sup">2</a>]</citation>中的后处理实现.</p>
                </div>
                <div class="p1">
                    <p id="49">卷积神经网络的直接输出是不平滑的,同时为了避免因语音窗口截断可能造成的输出噪声,在大小固定的平滑窗口上对后验概率进行平滑处理,窗长为<i>Ws</i>, <i>p</i><sub><i>ij</i></sub>是<i>j</i>个特征图的第<i>i</i>个标签的后验概率,平滑处理的计算公式如公式(7)所示:</p>
                </div>
                <div class="p1">
                    <p id="50" class="code-formula">
                        <mathml id="50"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msup><mi>p</mi><mo>′</mo></msup><msub><mrow></mrow><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mo>=</mo><mfrac><mn>1</mn><mrow><mi>j</mi><mo>-</mo><mi>h</mi><msub><mrow></mrow><mi>s</mi></msub><mo>+</mo><mn>1</mn></mrow></mfrac><mstyle displaystyle="true"><msubsup><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mi>h</mi><msub><mrow></mrow><mi>s</mi></msub></mrow><mi>j</mi></msubsup><mo>,</mo></mstyle><mi>p</mi><msub><mrow></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>7</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="51"><i>h</i><sub><i>s</i></sub>=max[1,<i>j</i>-<i>w</i><sub><i>s</i></sub>+1]是平滑处理滑动窗口的第一帧的索引,<i>w</i><sub><i>s</i></sub>是滑动窗口的帧数,本文使用<i>w</i><sub><i>s</i></sub>=10.</p>
                </div>
                <div class="p1">
                    <p id="52"><i>j</i>个特征图的关键词概率值计算方式如公式(8)所示:</p>
                </div>
                <div class="p1">
                    <p id="53" class="code-formula">
                        <mathml id="53"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>C</mi><mo>=</mo><msup><mrow></mrow><mi>n</mi></msup><msqrt><mrow><mstyle displaystyle="true"><msubsup><mo>∏</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>n</mi><mo>-</mo><mn>1</mn><mspace width="0.25em" /></mrow></msubsup><mspace width="0.25em" /></mstyle><mi>h</mi><msub><mrow></mrow><mi>m</mi></msub><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mtext>Μ</mtext><mtext>A</mtext><mtext>X</mtext></mrow></mstyle><mrow><mi>h</mi><msub><mrow></mrow><mi>m</mi></msub><mo>≤</mo><mi>k</mi><mo>≤</mo><mi>j</mi></mrow></munder><mo stretchy="false">(</mo><msup><mi>p</mi><mo>′</mo></msup><msub><mrow></mrow><mrow><mi>i</mi><mi>k</mi></mrow></msub><mo stretchy="false">)</mo></mrow></msqrt><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>8</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="54">式中,<i>C</i>是所求的一段语音中关键词概率值;<i>n</i>为关键词的标签组成数,例如以单个孤立词为标签,则“Hello Marvin”由“Hello”和“Marvin”组成,<i>n</i>为2.<i>p</i>′<sub><i>ik</i></sub>是第<i>i</i>个标签在第<i>k</i>个语音段平滑处理过的后验概率.<i>h</i><sub><i>m</i></sub>=max{1,<i>j</i>-<i>w</i><sub><i>m</i></sub>+1},<i>h</i><sub><i>m</i></sub>是计算关键词概率值的滑动窗口第一帧的索引,<i>w</i><sub><i>m</i></sub>是关键词概率计算的滑动窗口长度,本文使用<i>w</i><sub><i>m</i></sub>为100帧,语音段长度约为2 s,在该长度下可以覆盖2～4个字组成的关键词.</p>
                </div>
                <h3 id="55" name="55" class="anchor-tag">3 <b>模型结构</b></h3>
                <h4 class="anchor-tag" id="56" name="56">3.1 <b>传统卷积神经网络模型</b></h4>
                <div class="p1">
                    <p id="57">类LetNet的传统卷积神经网络参数如表1所示.该网络的输入是39*40,第一卷积层卷积核大小为3×3,时域和频域步长都为2,无池化操作.第二卷积层的卷积核大小也为3×3,时域和频域步长为2,在时域和频域做池化为2的最大池化操作.卷积层后为两层全连接层和一层输出层.网络参数如表1所示.</p>
                </div>
                <div class="area_img" id="58">
                    <p class="img_tit"><b>表</b>1 <b>典型卷积神经网络模型</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="58" border="1"><tr><td><br />type</td><td>Filter Shape</td><td>Stride</td><td>Par.</td><td>Mul.</td></tr><tr><td><br />conv</td><td>3×3×64</td><td>2×2</td><td>576</td><td>230.4 k</td></tr><tr><td><br />conv</td><td>3×3×64</td><td>2×2</td><td>36.9 k</td><td>3.7 M</td></tr><tr><td><br />dnn</td><td>128</td><td>-</td><td>204.8 k</td><td>204.8 k</td></tr><tr><td><br />dnn</td><td>128</td><td>-</td><td>16.4 k</td><td>16.4 k</td></tr><tr><td><br />softmax</td><td>36</td><td>-</td><td>4.6 k</td><td>4.6 k</td></tr><tr><td><br />Total</td><td>-</td><td>-</td><td>263.3 k</td><td>4.2 M</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="59">嵌入式设备的可用RAM一般在250k以下,部署在嵌入式设备上的关键词识别系统的参数量应尽量控制在100 k以内,从表1可知,基于传统卷积神经网络的类LetNet网络有较多的参数量和计算量,因此基于传统卷积的类LeNet的模型很难在嵌入式端部署.</p>
                </div>
                <div class="p1">
                    <p id="60">类LetNets的传统卷积神经网络的乘法操作大多在卷积层执行,尤其是第二卷积层,这是传统卷积操作固有缺点,为了提高网络性能需要叠加多层卷积层,而两个连续的卷积层的滤波器数量的任何均匀增加会导致计算量的二次增加.为了减少乘法次数,最直接的方法就是去掉第二卷积层,减少卷积层数会使卷积层输出特征抽象程度降低,降低模型性能,为了弥补卷积层降低带来的影响,只能增加第一卷积层的卷积核数量.同时第一全连接层的参数量较大的原因是卷积层的输出特征维度较高,为了减少参数量需要降低卷积层精度,主要方法是进一步扩大卷积操作在时域和频域上步长,但这样操作势必会给网络性能带来影响.已经有文章探索过在时间域上进行池化用于声学建模<citation id="90" type="reference"><link href="9" rel="bibliography" /><link href="17" rel="bibliography" /><sup>[<a class="sup">4</a>,<a class="sup">8</a>]</sup></citation>,但并没有显示出良好的使用效果.这主要是因为在大词汇量任务中声学建模的对象是上下文相关的音素和音节,而音素和音节的持续时间较短,一般为10～30 ms,所以在时间域上进行池化操作会丢失关键信息,从而对声学模型产生负面效果.但是,在我们的关键词识别任务中,我们的识别对象是组成关键词的单词或关键词本身,单词在时域上的持续时间较长,一般为200～500 ms,所以在时域上可能存在信息冗余,设定卷积核时域步长大于2或进行池化操作而不会丢失大量关键信息.</p>
                </div>
                <div class="p1">
                    <p id="61">基于传统卷积的低存储消耗、低计算量模型参数如表2所示,我们将该网络称为low_cnn.为了便于表示,我们将池化层单独列出.</p>
                </div>
                <div class="area_img" id="62">
                    <p class="img_tit"><b>表</b>2 low_cnn<b>网络参数</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="62" border="1"><tr><td><br />type</td><td>Filter Shape</td><td>Stride</td><td>Par.</td><td>Mul.</td></tr><tr><td><br />conv</td><td>3×3×128</td><td>2×2</td><td>1.2 k</td><td>460.8 k</td></tr><tr><td><br />Pool</td><td>10×20</td><td>10×20</td><td>-</td><td>-</td></tr><tr><td><br />dnn</td><td>128</td><td>-</td><td>65.5 k</td><td>65.5 k</td></tr><tr><td><br />dnn</td><td>128</td><td>-</td><td>16.4 k</td><td>16.4 k</td></tr><tr><td><br />softmax</td><td>36</td><td>-</td><td>4.6 k</td><td>4.6 k</td></tr><tr><td><br />Total</td><td>-</td><td>-</td><td>87.7 k</td><td>547.3k</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="63" name="63">3.2 <b>深度可分离卷积神经网络</b></h4>
                <div class="p1">
                    <p id="64">MobileNets是建立在深度可分离卷积的结构上的,但网络的第一层是基于传统卷积实现的[Mobile].本文使用的MobileNets结构如表3所示,Google在文献<citation id="91" type="reference">[<a class="sup">8</a>]</citation>中提出的网络结构共有28层,为了和上文的传统卷积神经网络类LeNet比较,本文对网络拓扑做了一定的修改,网络共有15层,网络在推理时的计算量和上文类LeNet的计算量几乎相等,参数量减少约30 k.所有网络层后面都是一个batchnorm [mobile13]和ReLU非线性,但最终的全连接层除外,它没有非线性,结果直接输入softmax层进行分类.在网络的部分卷积层中使用大小为2的卷积步长实现下采样操作,并在全连接层之前使用平均池化来将特征输出的空间分辨率降低至1.</p>
                </div>
                <div class="area_img" id="65">
                    <p class="img_tit"><b>表</b>3 <b>类</b>MobileNets<b>网络参数</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="65" border="1"><tr><td><br />type</td><td>Filter Shape</td><td>Stride</td><td>Par.</td><td>Mul.</td></tr><tr><td><br />con1v</td><td>3×3×32</td><td>2×2</td><td>288</td><td>115.2 k</td></tr><tr><td><br />conv2-dw</td><td>3×3×32</td><td>1×1</td><td>288</td><td>115.2 k</td></tr><tr><td><br />conv2-pw</td><td>1×1×32×64</td><td>1×1</td><td>2 k</td><td>819.2 k</td></tr><tr><td><br />conv3-dw</td><td>3×3×64</td><td>2×2</td><td>576</td><td>57.6 k</td></tr><tr><td><br />conv3-pw</td><td>1×1×64×64</td><td>1×1</td><td>4.1 k</td><td>409.6 k</td></tr><tr><td><br />conv4-dw</td><td>3×3×64</td><td>1×1</td><td>576</td><td>57.6 k</td></tr><tr><td><br />conv4-pw</td><td>1×1×64×128</td><td>1×1</td><td>8.2 k</td><td>819.2 k</td></tr><tr><td><br />conv5-dw</td><td>3×3×128</td><td>2×2</td><td>1.2 k</td><td>28.8 k</td></tr><tr><td><br />conv5-pw</td><td>1×1×128×128</td><td>1×1</td><td>1.7 k</td><td>409.6k</td></tr><tr><td><br />conv6-dw</td><td>3×3×128</td><td>1×1</td><td>1.2 k</td><td>28.8 k</td></tr><tr><td><br />conv6-pw</td><td>1×1×128×256</td><td>1×1</td><td>32.8 k</td><td>819.2 k</td></tr><tr><td><br />conv7-dw</td><td>3×3×256</td><td>2×2</td><td>2.3 k</td><td>20.7 k</td></tr><tr><td><br />conv7-pw</td><td>1×1×256×512</td><td>1×1</td><td>131.1 k</td><td>1.2 M</td></tr><tr><td><br />Avg Pool</td><td>3×3</td><td>-</td><td>-</td><td>-</td></tr><tr><td><br />FC</td><td>512×36</td><td>1×1</td><td>18.4 k</td><td>18.4 k</td></tr><tr><td><br />Total</td><td>-</td><td>-</td><td>204.7 k</td><td>4.9 M</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="66">参考后文的实验结果,虽然类MobileNets网 在关键词识别系统中比类LeNet有更好的表现,但是网络的参数量和计算量仍然较大,不适合在嵌入式端部署,所以本文设计和实现了基于深度可分离卷积的低参数、低计算模型,在模型性能损失较小的前提下,将参数量限制在50 k以下,计算量限制在3 M内.降低模型参数量和计算量的最直接方式就是降低模型的宽度和深度.低资源模型的卷积层数量降低为11层,最后一层卷积层的输出深度降低为128.同时在卷积过程中使用了更多的降采样操作,使用降采样操作的目的是为了减小每一层的输入大小,进而减小计算量.经过多次试验,在参数量较低、计算量适中时,获得较好表现的网络参数如表4所示,我们将该网络称为Low_MobileNets.</p>
                </div>
                <div class="area_img" id="67">
                    <p class="img_tit"><b>表</b>4 Low_MobileNets<b>网络参数</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="67" border="1"><tr><td><br />type</td><td>Filter Shape</td><td>Stride</td><td>Par.</td><td>Mul.</td></tr><tr><td><br />con1v</td><td>3×3×64</td><td>2×2</td><td>576</td><td>230.4 k</td></tr><tr><td><br />conv2-dw</td><td>3×3×64</td><td>1</td><td>576</td><td>230.4 k</td></tr><tr><td><br />conv2-pw</td><td>1×1×64×64</td><td>1</td><td>4.1 k</td><td>1.6 M</td></tr><tr><td><br />conv3-dw</td><td>3×3×64</td><td>2×2</td><td>576</td><td>57.6 k</td></tr><tr><td><br />conv3-pw</td><td>1×1×64×64</td><td>1</td><td>4.1 k</td><td>409.6 k</td></tr><tr><td><br />conv4-dw</td><td>3×3×64</td><td>1</td><td>576</td><td>57.6 k</td></tr><tr><td><br />conv4-pw</td><td>1×1×64×64</td><td>1</td><td>4.1 k</td><td>409.6 k</td></tr><tr><td><br />conv5-dw</td><td>3×3×64</td><td>2×2</td><td>576</td><td>14.4 k</td></tr><tr><td><br />conv5-pw</td><td>1×1×64×128</td><td>1</td><td>8.2 k</td><td>204.8 k</td></tr><tr><td><br />Avg Pool</td><td>5×5</td><td>1</td><td>-</td><td>-</td></tr><tr><td><br />FC</td><td>512×36</td><td>1</td><td>4.6 k</td><td>4.6 k</td></tr><tr><td><br />Total</td><td>-</td><td>-</td><td>28.0 k</td><td>2.8 M</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h3 id="68" name="68" class="anchor-tag">4 <b>实验结果与分析</b></h3>
                <div class="p1">
                    <p id="69">我们使用Google的语音命令数据集<citation id="92" type="reference"><link href="19" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>训练和测试了我们的模型,该数据集于2017年8月根据知识共享许可证发布.该数据集包含65 000个一秒钟长的话语,包含数千个不同的人的34个短词,以及背景噪音粉红噪声,白噪声和人造声音等样本.</p>
                </div>
                <div class="p1">
                    <p id="70">本文中的关键词识别系统,以单词为训练标签,标签除了包含待识别的34个词还有”silence”和”unknown”标签分别标注无声和未知词.每个标签约含有5 k个样本,将样本分为训练集、验证集和测试集,为了保证三个部分的一致性,根据数据集中音频文件的SHA-1散列名称对文件进行划分,所占比例分别为70%,10%,20%.静音标签为采样点都为0的生成样本,未知标签由识别标签外的其他语音样本组成.通过人工添加随机噪声来创建嘈杂的训练集和测试集,添加噪声的数据占总数据量的80%,其中噪声是从数据集中提供的背景噪声中随机选择,这些噪声大小在[-5dB,+10dB]之间随机采样.模型在嘈杂的条件下进行训练,并分别在干净和嘈杂的条件下进行评估.</p>
                </div>
                <div class="p1">
                    <p id="71">从表5各网络在测试集的表现结果可以看出,无论是传统卷积神经网络还是可分离卷积神经网络都有较强的抗噪性能,且卷积层越多,在无噪测试集和有噪测试集的表现差异就越小.无论是类LeNet还是类MobileNets都比仅有四层全连接层的深度神经网络<citation id="93" type="reference"><link href="5" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>有较大的性能提升,类MobileNets约有52%的相对改进.在拥有更低参数量和同等计算量前提下,类MobileNets网络无论在有噪测试集还是无噪测试集的表现都比类LeNet更好,约有24%的相对改进;Low_MobileNets网络的表现性能也小幅优于类LetNet,而前者的参数量和计算量仅为后者的11%和67%.虽然low_cnn的计算量要低于Low_MobileNet,但参数量约为后者的3倍,且性能在一定程度上低于后者.</p>
                </div>
                <div class="area_img" id="72">
                    <p class="img_tit"><b>表</b>5 <b>各测试集网络识别率</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="72" border="1"><tr><td><br />Model</td><td>无噪测试集</td><td>有噪测试集</td></tr><tr><td><br />DNN</td><td>89.2±1.41</td><td>86.7±1.84</td></tr><tr><td><br />类LeNet</td><td>93.2%±0.45</td><td>92.4%±0.36</td></tr><tr><td><br />Low_cnn</td><td>91.6±0.61</td><td>89.8±0.49</td></tr><tr><td><br />类MobileNet</td><td>94.8%±0.40</td><td>94.2%±0.24</td></tr><tr><td><br />Low_mobileNet</td><td>93.5%±0.62</td><td>92.9%±0.31</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="73">准确性是我们的主要质量指标,但它只是作为正确的分类决策的一部分来衡量.为了更好的比较多种网络结构,我们还绘制了受试者工作特征(receiver operating characteristic,ROC)曲线,其中x和y轴分别显示误报率(false alarm rate, FAR)和错误拒绝率(false reject rate, FRR).选定两个短词,并将两个短词的测试集中的音频文件随机组合成关键词测试文件,如“wow”和”marvin”组成”wow marvin”,关键词测试文件数量和测试集数量相等.通过在区间[0.0,1.0]间不断调整阈值大小,计算每个关键词的FAR和FRR,然后将多个关键词的FAR和FFR取平均获得该阈值下网络的FAR和FFR.</p>
                </div>
                <div class="p1">
                    <p id="74">图4中绘制了表5中部分网络的ROC曲线,曲线下面的面积越小,表明该网络性能越好.由ROC曲线我们可以直观的了解到,性能最好的是类MobileNets网络,ROC曲线的结果和表5的结果是一致的,单个词识别率越高的网络在多个词组成关键词的表现也就越好.</p>
                </div>
                <div class="area_img" id="75">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/WXYJ201909020_075.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bFF0dU9NTmdMQ0V1SURuSmxZZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 各网络ROC曲线" src="Detail/GetImg?filename=images/WXYJ201909020_075.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bFF0dU9NTmdMQ0V1SURuSmxZZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图</b>4 <b>各网络</b>ROC<b>曲线</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/WXYJ201909020_075.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bFF0dU9NTmdMQ0V1SURuSmxZZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="76">Low_MobileNets在参数量、计算量和识别率三方面综合表现最好,最适合在嵌入式端部署,且占用小量的小量芯片资源.虽然其计算量方面的优势没有参数量那么明显,但是对于嵌入式设备,存储资源需求的权重大于计算量的权重,关键词识别系统的参数量直接决定了该系统能否在嵌入式端部署,而如果存储需求满足则可以通过使用芯片内置DSP指令加速运算,缓解计算量压力.且较少的参数量意味着网络在推理阶段更好的存储交换操作,也可以在一定程度上加速计算.</p>
                </div>
                <h3 id="77" name="77" class="anchor-tag">5 <b>结束语</b></h3>
                <div class="p1">
                    <p id="78">本文探索使用传统卷积神经网络和深度可分离卷积神经网络来完成关键词识别任务,并将传统卷积神经网络、深度可分离卷积神经网络和全连接深度神经网络在存储消耗、计算量和识别精度进行比较.实验结果表明,本文提出的基于深度可分离神经网络的Low_MobileNets网络在三个方面都有很大的优势,其计算量、参数量的大小都适合在资源受限的嵌入式设备端部署,且计算量、参数量还有进一步的改进空间,以适用条件更苛刻的嵌入式设备.</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" >
                                    <b>[1]</b>
                                 郑方.连续无限制语音流中关键词识别方法研究[D].北京:清华大学,1997.
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Small-footprint Keyword Spotting Using Deep Neural Networks">

                                <b>[2]</b> CHEN G,PARADA C,HEIGOLD G.Small-footprint keyword spotting using deep neural networks[C]//2014 IEEE International Conference on Acoustics,Speech and Signal Processing (ICASSP).Florence,Italy ,IEEE,2014:4087-4091.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Convolutional networks for images,speech,and time-series">

                                <b>[3]</b> LECUN Y,BENGIO Y.Convolutional networks for images,speech,and time series[J].The handbook of brain theory and neural networks,1995,3361(10):66-68.
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=BJKD201509015&amp;v=MTYzMDhaZVZ1Rnlua1c3elBKeWZBYXJHNEg5VE1wbzlFWVlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2U=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bFF0dU9NTmdMQ0V1SURuSmxZZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[4]</b> 张晴晴,刘勇,潘接林,等.基于卷积神经网络的连续语音识别[J].北京科技大学学报,2015,37(9):1212-1217.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CMFD&amp;filename=2008076685.nh&amp;v=MTA5NzN5bmtXN3pQVjEyN0ZyTy9HTmZFcXBFYlBJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdUY=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bFF0dU9NTmdMQ0V1SURuSmxZZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b> 惠博.语音识别特征提取算法的研究及实现[D].西安:西北大学,2008.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=XTFZ200805066&amp;v=MjQ0ODFUbk5kTEc0SHRuTXFvOURZb1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnVGeW5rVzd6UFA=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bFF0dU9NTmdMQ0V1SURuSmxZZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b> 刘华平,李昕,郑宇,等.一种改进的自适应子带谱熵语音端点检测方法[J].系统仿真学报,2008,20(5):1366-1371.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Mobilenets:Efficient convolutional neural networks for mobile vision applications">

                                <b>[7]</b> HOWARD A G,ZHU M,CHEN B,et al.Mobilenets:Efficient convolutional neural networks for mobile vision applications[z].arXiv preprint arXiv:1704.04861,2017.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Going deeper with convolutions">

                                <b>[8]</b> SZEGEDY C,LIU W,JIA Y,et al.Going deeper with convolutions[C]//Proceedings of the IEEE conference on computer vision and pattern recognition.Boston,USA,2015:1-9.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Speech commands:A dataset for limited-vocabulary speech recognition">

                                <b>[9]</b> WARDEN P.Speech commands:A dataset for limited-vocabulary speech recognition[J].arXiv preprint arXiv:1804.03209,2018.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="WXYJ201909020" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bFF0dU9NTmdMQ0V1SURuSmxZZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=WXYJ201909020&amp;v=MTI5MzA1NE8zenFxQnRHRnJDVVJMT2VaZVZ1Rnlua1c3elBNalhTWkxHNEg5ak1wbzlIWklRS0RIODR2UjRUNmo=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bFF0dU9NTmdMQ0V1SURuSmxZZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
