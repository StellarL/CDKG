<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637133837315100000%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dWXYJ201909014%26RESULT%3d1%26SIGN%3dcj3mqlOSa76d3B9hpOoI7vrMlaQ%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=WXYJ201909014&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=WXYJ201909014&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=WXYJ201909014&amp;v=MTUyMzVrV3J2S01qWFNaTEc0SDlqTXBvOUVZSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnVGeW4=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bFF0dU9NTmdMQ0V1SURuSmxZZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#23" data-title="1 &lt;b&gt;引言&lt;/b&gt; ">1 <b>引言</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#28" data-title="2 &lt;b&gt;改进的行人再识别方法&lt;/b&gt; ">2 <b>改进的行人再识别方法</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#30" data-title="2.1 &lt;b&gt;网络模型&lt;/b&gt;">2.1 <b>网络模型</b></a></li>
                                                <li><a href="#51" data-title="2.2 &lt;b&gt;改进的三元组损失函数&lt;/b&gt;">2.2 <b>改进的三元组损失函数</b></a></li>
                                                <li><a href="#65" data-title="2.3 &lt;b&gt;置信度测量机制&lt;/b&gt;">2.3 <b>置信度测量机制</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#71" data-title="3 &lt;b&gt;实验分析&lt;/b&gt; ">3 <b>实验分析</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#72" data-title="3.1 &lt;b&gt;数据集&lt;/b&gt;">3.1 <b>数据集</b></a></li>
                                                <li><a href="#75" data-title="3.2 &lt;b&gt;实验结果及分析&lt;/b&gt;">3.2 <b>实验结果及分析</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#96" data-title="4 &lt;b&gt;结束语&lt;/b&gt; ">4 <b>结束语</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#32" data-title="&lt;b&gt;图&lt;/b&gt;1 AlignedReID&lt;b&gt;模型框图&lt;/b&gt;"><b>图</b>1 AlignedReID<b>模型框图</b></a></li>
                                                <li><a href="#74" data-title="&lt;b&gt;表&lt;/b&gt;1 &lt;b&gt;数据集的具体信息&lt;/b&gt;"><b>表</b>1 <b>数据集的具体信息</b></a></li>
                                                <li><a href="#89" data-title="&lt;b&gt;表&lt;/b&gt;2 Market1501&lt;b&gt;数据集实验结果&lt;/b&gt;"><b>表</b>2 Market1501<b>数据集实验结果</b></a></li>
                                                <li><a href="#90" data-title="&lt;b&gt;表&lt;/b&gt;3 CUHK03&lt;b&gt;数据集实验结果&lt;/b&gt;"><b>表</b>3 CUHK03<b>数据集实验结果</b></a></li>
                                                <li><a href="#91" data-title="&lt;b&gt;表&lt;/b&gt;4 VIPeR&lt;b&gt;数据集实验结果&lt;/b&gt;"><b>表</b>4 VIPeR<b>数据集实验结果</b></a></li>
                                                <li><a href="#95" data-title="&lt;b&gt;表&lt;/b&gt;5 &lt;b&gt;自建数据集中对比实验&lt;/b&gt;"><b>表</b>5 <b>自建数据集中对比实验</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" title=" &lt;i&gt;SONG Z&lt;/i&gt;,&lt;i&gt;CAI X&lt;/i&gt;,&lt;i&gt;CHEN Y&lt;/i&gt;,&lt;i&gt;et al&lt;/i&gt;.&lt;i&gt;Deep convolutional neural networks with adaptive spatial feature for person re&lt;/i&gt;-&lt;i&gt;identification&lt;/i&gt;[&lt;i&gt;C&lt;/i&gt;]// &lt;i&gt;Advanced Information Technology&lt;/i&gt;,&lt;i&gt;Electronic and Automation Control Conference&lt;/i&gt;.&lt;i&gt;Chongqing&lt;/i&gt;,&lt;i&gt;China&lt;/i&gt;,&lt;i&gt;IEEE&lt;/i&gt;,2017:2020-2023." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep convolutional neural networks with adaptive spatial feature for person re-identification">
                                        <b>[1]</b>
                                         &lt;i&gt;SONG Z&lt;/i&gt;,&lt;i&gt;CAI X&lt;/i&gt;,&lt;i&gt;CHEN Y&lt;/i&gt;,&lt;i&gt;et al&lt;/i&gt;.&lt;i&gt;Deep convolutional neural networks with adaptive spatial feature for person re&lt;/i&gt;-&lt;i&gt;identification&lt;/i&gt;[&lt;i&gt;C&lt;/i&gt;]// &lt;i&gt;Advanced Information Technology&lt;/i&gt;,&lt;i&gt;Electronic and Automation Control Conference&lt;/i&gt;.&lt;i&gt;Chongqing&lt;/i&gt;,&lt;i&gt;China&lt;/i&gt;,&lt;i&gt;IEEE&lt;/i&gt;,2017:2020-2023.
                                    </a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" title=" &lt;i&gt;B&lt;/i&gt;а&lt;i&gt;K S&lt;/i&gt;,&lt;i&gt;CARR P&lt;/i&gt;.&lt;i&gt;Deep deformable patch metric learning for person re&lt;/i&gt;-&lt;i&gt;identification&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;IEEE Transactions on Circuits&lt;/i&gt; &amp;amp; &lt;i&gt;Systems for Video Technology&lt;/i&gt;,2017(99):1-3." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep deformable patch metric learning for person re-identification">
                                        <b>[2]</b>
                                         &lt;i&gt;B&lt;/i&gt;а&lt;i&gt;K S&lt;/i&gt;,&lt;i&gt;CARR P&lt;/i&gt;.&lt;i&gt;Deep deformable patch metric learning for person re&lt;/i&gt;-&lt;i&gt;identification&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;IEEE Transactions on Circuits&lt;/i&gt; &amp;amp; &lt;i&gt;Systems for Video Technology&lt;/i&gt;,2017(99):1-3.
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title=" &lt;i&gt;ZHANG X&lt;/i&gt;,&lt;i&gt;LUO H&lt;/i&gt;,&lt;i&gt;FAN X&lt;/i&gt;,&lt;i&gt;et al&lt;/i&gt;.&lt;i&gt;Alignedreid&lt;/i&gt;:&lt;i&gt;Surpassing human&lt;/i&gt;-&lt;i&gt;level performance in person re&lt;/i&gt;-&lt;i&gt;identification&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;arXiv preprint arXiv&lt;/i&gt;:1711.08184,2017,31(2):1-10." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Alignedreid:Surpassing human-level performance in person re-identification">
                                        <b>[3]</b>
                                         &lt;i&gt;ZHANG X&lt;/i&gt;,&lt;i&gt;LUO H&lt;/i&gt;,&lt;i&gt;FAN X&lt;/i&gt;,&lt;i&gt;et al&lt;/i&gt;.&lt;i&gt;Alignedreid&lt;/i&gt;:&lt;i&gt;Surpassing human&lt;/i&gt;-&lt;i&gt;level performance in person re&lt;/i&gt;-&lt;i&gt;identification&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;arXiv preprint arXiv&lt;/i&gt;:1711.08184,2017,31(2):1-10.
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" title=" &lt;i&gt;HERMANS A&lt;/i&gt;,&lt;i&gt;BEYER L&lt;/i&gt;,&lt;i&gt;LEIBE B&lt;/i&gt;.&lt;i&gt;In defense of the triplet loss for person re&lt;/i&gt;-&lt;i&gt;identification&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;arXiv preprint arXiv&lt;/i&gt;:1703.07737,2017,4(21):1-17." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=In defense of the triplet loss for person re-identification">
                                        <b>[4]</b>
                                         &lt;i&gt;HERMANS A&lt;/i&gt;,&lt;i&gt;BEYER L&lt;/i&gt;,&lt;i&gt;LEIBE B&lt;/i&gt;.&lt;i&gt;In defense of the triplet loss for person re&lt;/i&gt;-&lt;i&gt;identification&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;arXiv preprint arXiv&lt;/i&gt;:1703.07737,2017,4(21):1-17.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" title=" &lt;i&gt;LOU Y&lt;/i&gt;,&lt;i&gt;BAI Y&lt;/i&gt;,&lt;i&gt;LIN J&lt;/i&gt;,&lt;i&gt;et al&lt;/i&gt;.&lt;i&gt;Compact deep invariant descriptors for video Rretrieval&lt;/i&gt;[&lt;i&gt;C&lt;/i&gt;]//&lt;i&gt;Data Compression Conference&lt;/i&gt;[&lt;i&gt;s&lt;/i&gt;.&lt;i&gt;l&lt;/i&gt;.].&lt;i&gt;IEEE&lt;/i&gt;,2017:420-429." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Compact deep invariant descriptors for video Rretrieval">
                                        <b>[5]</b>
                                         &lt;i&gt;LOU Y&lt;/i&gt;,&lt;i&gt;BAI Y&lt;/i&gt;,&lt;i&gt;LIN J&lt;/i&gt;,&lt;i&gt;et al&lt;/i&gt;.&lt;i&gt;Compact deep invariant descriptors for video Rretrieval&lt;/i&gt;[&lt;i&gt;C&lt;/i&gt;]//&lt;i&gt;Data Compression Conference&lt;/i&gt;[&lt;i&gt;s&lt;/i&gt;.&lt;i&gt;l&lt;/i&gt;.].&lt;i&gt;IEEE&lt;/i&gt;,2017:420-429.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" title=" &lt;i&gt;LIAO S&lt;/i&gt;,&lt;i&gt;HU Y&lt;/i&gt;,&lt;i&gt;ZHU X&lt;/i&gt;,&lt;i&gt;et al&lt;/i&gt;.&lt;i&gt;Person re&lt;/i&gt;-&lt;i&gt;identification by local maximal occurrence representation and metric learning&lt;/i&gt;[&lt;i&gt;C&lt;/i&gt;]//&lt;i&gt;Computer Vision and Pattern Recognition&lt;/i&gt;.&lt;i&gt;Boston&lt;/i&gt;,&lt;i&gt;Massachusetts&lt;/i&gt;,&lt;i&gt;USA&lt;/i&gt;,&lt;i&gt;IEEE&lt;/i&gt;,2015:2197-2206." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Person re-identification by localmaximal occurrence representation and metric learning">
                                        <b>[6]</b>
                                         &lt;i&gt;LIAO S&lt;/i&gt;,&lt;i&gt;HU Y&lt;/i&gt;,&lt;i&gt;ZHU X&lt;/i&gt;,&lt;i&gt;et al&lt;/i&gt;.&lt;i&gt;Person re&lt;/i&gt;-&lt;i&gt;identification by local maximal occurrence representation and metric learning&lt;/i&gt;[&lt;i&gt;C&lt;/i&gt;]//&lt;i&gt;Computer Vision and Pattern Recognition&lt;/i&gt;.&lt;i&gt;Boston&lt;/i&gt;,&lt;i&gt;Massachusetts&lt;/i&gt;,&lt;i&gt;USA&lt;/i&gt;,&lt;i&gt;IEEE&lt;/i&gt;,2015:2197-2206.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" title=" 朱建清,曾焕强,杜永兆,等.基于新型三元卷积神经网络的行人再辨识算法[&lt;i&gt;J&lt;/i&gt;].电子与信息学报,2018,40(4):1012-1016." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZYX201804035&amp;v=MjQwNzJxNDlHWVlRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ1Rnlua1dydk5JVGZTZHJHNEg5bk0=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bFF0dU9NTmdMQ0V1SURuSmxZZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[7]</b>
                                         朱建清,曾焕强,杜永兆,等.基于新型三元卷积神经网络的行人再辨识算法[&lt;i&gt;J&lt;/i&gt;].电子与信息学报,2018,40(4):1012-1016.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" title=" &lt;i&gt;MIRMAHBOUB B&lt;/i&gt;,&lt;i&gt;MEKHAFI M L&lt;/i&gt;,&lt;i&gt;MURINO V&lt;/i&gt;,&lt;i&gt;et al&lt;/i&gt;.&lt;i&gt;Person re&lt;/i&gt;-&lt;i&gt;identification by order&lt;/i&gt;-&lt;i&gt;induced metric fusion&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;Neurocomputing&lt;/i&gt;,2018(275):667-676." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES7A5312B3D54C4525EF23FA1069FB6481&amp;v=Mjg5MzdPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkNwYlEzNWRoaHhibTJ4SzA9TmlmT2ZiVEpHOUxOcmYxR0VPNExmM2c4elJObW5EMStQZzdqckJROEQ4Q1NRYktlQw==&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bFF0dU9NTmdMQ0V1SURuSmxZZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                         &lt;i&gt;MIRMAHBOUB B&lt;/i&gt;,&lt;i&gt;MEKHAFI M L&lt;/i&gt;,&lt;i&gt;MURINO V&lt;/i&gt;,&lt;i&gt;et al&lt;/i&gt;.&lt;i&gt;Person re&lt;/i&gt;-&lt;i&gt;identification by order&lt;/i&gt;-&lt;i&gt;induced metric fusion&lt;/i&gt;[&lt;i&gt;J&lt;/i&gt;].&lt;i&gt;Neurocomputing&lt;/i&gt;,2018(275):667-676.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" title=" &lt;i&gt;PAISITKRIANGKRAI S&lt;/i&gt;,&lt;i&gt;SHEN C&lt;/i&gt;,&lt;i&gt;HENGEL A V D&lt;/i&gt;.&lt;i&gt;Learning to rank in person re&lt;/i&gt;-&lt;i&gt;identification with metric ensembles&lt;/i&gt;[&lt;i&gt;C&lt;/i&gt;]//&lt;i&gt;Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition&lt;/i&gt;.&lt;i&gt;Boston&lt;/i&gt;,&lt;i&gt;MA&lt;/i&gt;,&lt;i&gt;USA&lt;/i&gt;:&lt;i&gt;IEEE&lt;/i&gt;,2015:1846-1855." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Learning to rank in person reidentification with metric ensembles">
                                        <b>[9]</b>
                                         &lt;i&gt;PAISITKRIANGKRAI S&lt;/i&gt;,&lt;i&gt;SHEN C&lt;/i&gt;,&lt;i&gt;HENGEL A V D&lt;/i&gt;.&lt;i&gt;Learning to rank in person re&lt;/i&gt;-&lt;i&gt;identification with metric ensembles&lt;/i&gt;[&lt;i&gt;C&lt;/i&gt;]//&lt;i&gt;Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition&lt;/i&gt;.&lt;i&gt;Boston&lt;/i&gt;,&lt;i&gt;MA&lt;/i&gt;,&lt;i&gt;USA&lt;/i&gt;:&lt;i&gt;IEEE&lt;/i&gt;,2015:1846-1855.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_10" title=" &lt;i&gt;CARR S B P&lt;/i&gt;.&lt;i&gt;Deep spatial pyramid for person re&lt;/i&gt;-&lt;i&gt;identification&lt;/i&gt;[&lt;i&gt;C&lt;/i&gt;]// &lt;i&gt;IEEE International Conference on Advanced Video and Signal Based Surveillance&lt;/i&gt;.&lt;i&gt;Lecce&lt;/i&gt;,&lt;i&gt;Italy&lt;/i&gt;,&lt;i&gt;IEEE&lt;/i&gt;,2017:1-6." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep spatial pyramid for person re-identification">
                                        <b>[10]</b>
                                         &lt;i&gt;CARR S B P&lt;/i&gt;.&lt;i&gt;Deep spatial pyramid for person re&lt;/i&gt;-&lt;i&gt;identification&lt;/i&gt;[&lt;i&gt;C&lt;/i&gt;]// &lt;i&gt;IEEE International Conference on Advanced Video and Signal Based Surveillance&lt;/i&gt;.&lt;i&gt;Lecce&lt;/i&gt;,&lt;i&gt;Italy&lt;/i&gt;,&lt;i&gt;IEEE&lt;/i&gt;,2017:1-6.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=WXYJ" target="_blank">微电子学与计算机</a>
                2019,36(09),73-78             </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>嵌套池化三元组卷积神经网络的行人再识别</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E6%9E%97&amp;code=10188513&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bFF0dU9NTmdMQ0V1SURuSmxZZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王林</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%A8%E4%B9%90&amp;code=41854252&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bFF0dU9NTmdMQ0V1SURuSmxZZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">杨乐</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E8%A5%BF%E5%AE%89%E7%90%86%E5%B7%A5%E5%A4%A7%E5%AD%A6%E8%87%AA%E5%8A%A8%E5%8C%96%E4%B8%8E%E4%BF%A1%E6%81%AF%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0041993&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bFF0dU9NTmdMQ0V1SURuSmxZZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">西安理工大学自动化与信息工程学院</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>针对行人再识别易受遮挡,光照、视角等非理想条件变化的影响,提出一种嵌套池化三元组卷积神经网络.它在均方根池化后依次添加平均池化和最大池化提取全局特征,并通过最短路径损失自动对齐局部特征.然后采用改进型Log-logistic函数代替传统三元组损失函数训练网络,得到与局部特征联合优化的全局特征.在Market-1501、CUHK03和VIPeR数据集上的识别率都比基于传统方法的提高了5%以上.实验结果表明,本文提出的嵌套池化三元组卷积神经网络,能有效解决非理想自然条件下存在的部分遮挡、分辨率低和旋转变化等问题,同时具有良好的泛化能力和适用范围.</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%A1%8C%E4%BA%BA%E5%86%8D%E8%AF%86%E5%88%AB&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bFF0dU9NTmdMQ0V1SURuSmxZZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">行人再识别;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%B5%8C%E5%A5%97%E6%B1%A0%E5%8C%96&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bFF0dU9NTmdMQ0V1SURuSmxZZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">嵌套池化;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E4%B8%89%E5%85%83%E7%BB%84%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bFF0dU9NTmdMQ0V1SURuSmxZZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">三元组损失函数;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%B1%80%E9%83%A8%E7%89%B9%E5%BE%81&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bFF0dU9NTmdMQ0V1SURuSmxZZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">局部特征;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E9%97%B4%E6%8E%A5%E5%BA%A6%E9%87%8F&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bFF0dU9NTmdMQ0V1SURuSmxZZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">间接度量;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    王林,男,(1962-),博士,教授.研究方向为无线传感器网络、复杂网络社团发现、大数据、数据挖掘.;
                                </span>
                                <span>
                                    *杨乐,女,(1994-),硕士研究生.研究方向为计算机视觉、深度学习.E-mail:m13572837504_2@163.com.;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-12-20</p>

                    <p>

                            <b>基金：</b>
                                                        <span>陕西省科技计划重点项目(2017ZDCXL-GY-05-03);</span>
                    </p>
            </div>
                    <h1><b>Pedestrian re-identification of nested pooling triple convolutional neural networks</b></h1>
                    <h2>
                    <span>WANG Lin</span>
                    <span>YANG Le</span>
            </h2>
                    <h2>
                    <span>School of Automation and Information Engineering,Xi′an University of Technology</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>A nested pooling tri-tuple convolution neural network is proposed for pedestrian re-identification, which is vulnerable to occlusion, illumination, viewing angle and other non-ideal conditions. It adds the average pooling and maximum pooling to extract global features in turn after the root mean square pooling, and automatically aligns the local features with the shortest path loss. Then the improved Log-logistic function is used instead of the traditional triple loss function to train the network, and get global features jointly optimized with local features. The recognition rates on the Market-1501, CUHK03, and VIPeR datasets are both more than 6% higher than those based on traditional methods. The experimental results show that nested pooling tri-tuple convolutional neural network proposed in this paper can effectively solve the problems of partial occlusion, low resolution and rotation change under non-ideal natural conditions, and has good generalization ability and scope of application.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=person%20re-identification&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bFF0dU9NTmdMQ0V1SURuSmxZZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">person re-identification;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=nested%20pooling&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bFF0dU9NTmdMQ0V1SURuSmxZZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">nested pooling;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=ternary%20loss%20function&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bFF0dU9NTmdMQ0V1SURuSmxZZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">ternary loss function;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=local%20features&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bFF0dU9NTmdMQ0V1SURuSmxZZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">local features;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=indirect%20metric&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bFF0dU9NTmdMQ0V1SURuSmxZZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">indirect metric;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-12-20</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="23" name="23" class="anchor-tag">1 <b>引言</b></h3>
                <div class="p1">
                    <p id="24">行人再识别不仅能为公安部门提供预防和侦破商场盗窃、聚众斗殴等重大刑事案件的线索,而且能够高效准确地定位和追踪寻人启事中走丢的老人和小孩,已成为计算机视觉领域一个热点问题.目前行人再识别所使用的特征主要有:底层视觉与手工特征、语义属性特征和基于深度学习的特征.底层视觉特征复杂性较高,手工特征需要研究人员具有相当坚实的专业知识和大量的实践经验,特征提取过程较为主观,可移植性差.与低层视觉特征相比,语义属性具有判别性好以及变换不变性的优点,但很难检测到语义属性.深度学习灵活地在训练数据的驱动下,对输入图像逐级根据需求自动地提取有用的特征,相比人工设计的特征,对目标具有更强的表达能力.</p>
                </div>
                <div class="p1">
                    <p id="25">基于深度学习的传统行人再识别方法将行人外观作为整体进行建模和匹配,但研究发现基于局部区域的分块匹配有更高的识别精度,所以基于分块匹配的方法成为近年来的研究热点.文献<citation id="98" type="reference">[<a class="sup">1</a>]</citation>将行人图像分割为头、躯干和腿,然后训练多分支卷积神经网络自适应地提取各部分的特征.虽然可以得到精确的局部区域特征,但分割存在准确率的问题.文献<citation id="99" type="reference">[<a class="sup">2</a>]</citation>提出可变形块度量学习方法,使对应块在匹配时可在一定范围内改变它们的位置(变形),以此来模拟姿态变化,并通过计算块的特征差异和位置变形成本得到行人特征相似度.但这需要额外的监督和姿势估计步骤,其通常容易出错.为了自适应对齐局部特征,文献<citation id="100" type="reference">[<a class="sup">3</a>]</citation>提出AlignedReID方法,通过引入最短路径损失自动对齐局部特征,得到较准确的局部距离,可有效克服行人图像身体结构错位,遮挡以及不完整等问题.</p>
                </div>
                <div class="p1">
                    <p id="26">另外,研究发现三元组损失函数在行人再识别中具有更大的应用潜力.三元组损失函数通过将特征学习和相似性度量合并到一个统一的框架中,使网络根据最终的任务学习有辨识力的特征,不需要附加特定的层而可达到与设计新颖的网络识别架构相提并论的效果<citation id="101" type="reference"><link href="9" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>.</p>
                </div>
                <div class="p1">
                    <p id="27">因此,本文以三元组卷积神经网络为基础,利用AlignedReID方法计算局部距离.但仅利用该方法将会存在以下两大问题:(1)行人图像可能存在平移、尺度、旋转等变化,若只利用最大池化提取显著性特征往往会导致误匹配;(2)特征学习过程中值域是变化的,而传统三元组损失函数中的间隔参数需要事先指定,缺乏灵活性,不利于网络训练.为了解决上面的问题,本文提出嵌套池化三元组卷积神经网络.主要做了以下两个贡献:(1)利用NIP(nested invariance pooling,嵌套不变池)理论<citation id="102" type="reference"><link href="11" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>,合理地在卷积层后依次添加均方根池化层、平均池化以及最大池化层,有效提高特征对平移、尺度、旋转变化的鲁棒性;(2)改进三元组损失函数,以Log-logistic函数代替传统的铰链函数,同时优化损失函数的约束条件来加速收敛并改善性能.</p>
                </div>
                <h3 id="28" name="28" class="anchor-tag">2 <b>改进的行人再识别方法</b></h3>
                <div class="p1">
                    <p id="29">首先描述嵌套池化三元组卷积神经网络的特征提取过程和相似性距离计算方法,然后针对传统损失函数的不足,提出改进型三元组损失函数.最后,详细阐述了用于间接度量的置信度测量机制.</p>
                </div>
                <h4 class="anchor-tag" id="30" name="30">2.1 <b>网络模型</b></h4>
                <div class="p1">
                    <p id="31">如图1所示,以Resnet50的卷积层为基础,在最后一个卷积层后添加全局特征分支与局部特征分支,并利用三元组损失函数联合优化全局特征和局部特征.下面从全局特征、局部特征的提取过程和相似性距离计算方法两个方面进行介绍.</p>
                </div>
                <div class="area_img" id="32">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/WXYJ201909014_032.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bFF0dU9NTmdMQ0V1SURuSmxZZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 AlignedReID模型框图" src="Detail/GetImg?filename=images/WXYJ201909014_032.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bFF0dU9NTmdMQ0V1SURuSmxZZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图</b>1 AlignedReID<b>模型框图</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/WXYJ201909014_032.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bFF0dU9NTmdMQ0V1SURuSmxZZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="33" name="33">2.1.1 全局特征提取过程</h4>
                <div class="p1">
                    <p id="34">传统的网络结构往往只在卷积层后添加最大池化层来提取显著性特征,这样可能会使特征丢失很多有用的信息.然而文献<citation id="103" type="reference">[<a class="sup">5</a>]</citation>表明:均方根池化可以改善平移变化,平均池化可以改善尺度变化,最大池化可以改善旋转变化,利用嵌套不变池(Nested Invariance Pooling,NIP)理论,合理地在卷积层后依次添加均方根池化层、平均池化以及最大池化层,可有效提高特征对平移、尺度、旋转变化的鲁棒性.</p>
                </div>
                <div class="p1">
                    <p id="35">因此,为了克服行人再识别易受遮挡,光照、旋转等非理想条件变化的影响,本文利用嵌套不变池理论改进传统的只利用最大池化层的网络结构.在提取全局特征的过程中,首先利用Resnet50的卷积层提取图像特征,输出大小为2 048×7×7的特征图.2 048是通道数,7×7是空间大小.然后进行2×2大小的均方根池化,移动步长为1,得到2 048×6×6大小的输出,再进行2×2大小的平均池化,移动步长为2,得到2 048×3×3大小的输出,最后进行全局最大池化,得到2 048维的全局特征向量.</p>
                </div>
                <h4 class="anchor-tag" id="36" name="36">2.1.2 局部特征提取及相似性距离计算</h4>
                <div class="p1">
                    <p id="37">根据文献<citation id="104" type="reference">[<a class="sup">6</a>]</citation>的启发,为了提高特征在水平方向的鲁棒性,有效克服视角变化.在Resnet50最后一个卷积层的输出后执行水平池化,进行池化窗口为1×7的最大池化,得到7个局部特征.然后利用1×1的卷积将通道数从2 048减少到128,最后每张图片得到128×7的局部特征,局部特征(128维向量)表示行人图像水平部分的信息.</p>
                </div>
                <div class="p1">
                    <p id="38">在此基础上,利用L2距离计算全局特征间的相似度.局部距离来源于一个设想:对于身份相同的两个行人图像,第一个行人图像身体部位的局部特征与另一个图像语义对应身体部位的更相似,而他们在水平方向可能并不是一一对应的.因此,通过从上到下动态匹配局部块来对齐局部特征,从而得到两幅图像的最小局部距离.给定两幅行人图像的局部特征,<mathml id="39"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>F</mi><mo>=</mo><mrow><mo>{</mo><mrow><mi>f</mi><msub><mrow></mrow><mn>1</mn></msub><mo>,</mo><mo>⋯</mo><mo>,</mo><mi>f</mi><msub><mrow></mrow><mn>7</mn></msub></mrow><mo>}</mo></mrow></mrow></math></mathml>和<mathml id="40"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>G</mi><mo>=</mo><mrow><mo>{</mo><mrow><mi>g</mi><msub><mrow></mrow><mn>1</mn></msub><mo>,</mo><mo>⋯</mo><mo>,</mo><mi>g</mi><msub><mrow></mrow><mn>7</mn></msub></mrow><mo>}</mo></mrow></mrow></math></mathml>,首先利用公式(1)将两幅图像局部特征之间的距离归一化到[0,1),公式(1)如下所示:</p>
                </div>
                <div class="p1">
                    <p id="41" class="code-formula">
                        <mathml id="41"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>d</mi><msub><mrow></mrow><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mo>=</mo><mtable><mtr><mtd><mfrac><mrow><mi>e</mi><msup><mrow></mrow><mrow><mrow><mo stretchy="false">∥</mo><mi>f</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mi>g</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">∥</mo></mrow><msub><mrow></mrow><mn>2</mn></msub></mrow></msup><mo>-</mo><mn>1</mn></mrow><mrow><mi>e</mi><msup><mrow></mrow><mrow><mrow><mo stretchy="false">∥</mo><mi>f</mi><msub><mrow></mrow><mi>i</mi></msub><mo>-</mo><mi>g</mi><msub><mrow></mrow><mi>j</mi></msub><mo stretchy="false">∥</mo></mrow><msub><mrow></mrow><mn>2</mn></msub></mrow></msup><mo>+</mo><mn>1</mn></mrow></mfrac></mtd><mtd><mi>i</mi><mo>,</mo><mi>j</mi><mo>∈</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mn>3</mn><mo>⋯</mo><mo>,</mo><mn>7</mn></mtd></mtr></mtable><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="42">式中,<i>d</i><sub><i>i</i>,<i>j</i></sub>是第一个图像第<i>i</i>个水平块与第二个图像第<i>j</i>个水平块之间的距离.这些距离形成距离矩阵<b><i>D</i></b>,它的第<i>i</i>行第<i>j</i>个元素是<i>d</i><sub><i>i</i>,<i>j</i></sub>.然后,定义行人图像间的局部距离为矩阵<b><i>D</i></b>中从<mathml id="43"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>(</mo><mrow><mn>1</mn><mo>,</mo><mn>1</mn></mrow><mo>)</mo></mrow></mrow></math></mathml>到<mathml id="44"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>(</mo><mrow><mi>Η</mi><mo>,</mo><mi>Η</mi></mrow><mo>)</mo></mrow></mrow></math></mathml>最短路径的距离总和.局部距离可以通过公式(2)动态计算,其中:<i>S</i><sub><i>i</i>,<i>j</i></sub>是距离矩阵<b><i>D</i></b>中从<mathml id="45"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>(</mo><mrow><mn>1</mn><mo>,</mo><mn>1</mn></mrow><mo>)</mo></mrow></mrow></math></mathml>到<mathml id="46"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>(</mo><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow><mo>)</mo></mrow></mrow></math></mathml>最短路径的总和,<i>S</i><sub><i>H</i>,<i>H</i></sub>是距离矩阵<b><i>D</i></b>中从<mathml id="47"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>(</mo><mrow><mn>1</mn><mo>,</mo><mn>1</mn></mrow><mo>)</mo></mrow></mrow></math></mathml>到<mathml id="48"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>(</mo><mrow><mi>Η</mi><mo>,</mo><mi>Η</mi></mrow><mo>)</mo></mrow></mrow></math></mathml>最短路径的总和,即两幅图像的局部距离.</p>
                </div>
                <div class="p1">
                    <p id="49" class="code-formula">
                        <mathml id="49"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>S</mi><msub><mrow></mrow><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mo>=</mo><mrow><mo>{</mo><mrow><mtable><mtr><mtd columnalign="left"><mi>d</mi><msub><mrow></mrow><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub></mtd><mtd columnalign="left"><mi>i</mi><mo>=</mo><mn>1</mn><mo>,</mo><mi>j</mi><mo>=</mo><mn>1</mn></mtd></mtr><mtr><mtd columnalign="left"><mi>S</mi><msub><mrow></mrow><mrow><mi>i</mi><mo>-</mo><mn>1</mn><mo>,</mo><mi>j</mi></mrow></msub><mo>+</mo><mi>d</mi><msub><mrow></mrow><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub></mtd><mtd columnalign="left"><mi>i</mi><mo>≠</mo><mn>1</mn><mo>,</mo><mi>j</mi><mo>=</mo><mn>1</mn></mtd></mtr><mtr><mtd columnalign="left"><mi>S</mi><msub><mrow></mrow><mrow><mi>i</mi><mo>,</mo><mi>j</mi><mo>-</mo><mn>1</mn></mrow></msub><mo>+</mo><mi>d</mi><msub><mrow></mrow><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub></mtd><mtd columnalign="left"><mi>i</mi><mo>=</mo><mn>1</mn><mo>,</mo><mi>j</mi><mo>≠</mo><mn>1</mn></mtd></mtr><mtr><mtd columnalign="left"><mi>min</mi><mrow><mo>(</mo><mrow><mi>S</mi><msub><mrow></mrow><mrow><mi>i</mi><mo>-</mo><mn>1</mn><mo>,</mo><mi>j</mi></mrow></msub><mo>,</mo><mi>S</mi><msub><mrow></mrow><mrow><mi>i</mi><mo>,</mo><mi>j</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow><mo>)</mo></mrow><mo>+</mo><mi>d</mi><msub><mrow></mrow><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub></mtd><mtd columnalign="left"><mi>i</mi><mo>≠</mo><mn>1</mn><mo>,</mo><mi>j</mi><mo>≠</mo><mn>1</mn></mtd></mtr></mtable></mrow></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="50">非对应身体部位局部特征间L2距离相对较大,因此对最短路径的贡献很小,最短路径的绝大部分都来源于对应身体部位局部特征间的距离.因此,通过动态对齐计算局部特征间的距离,可以有效改善行人图像中身体结构错位、不完整,以及遮挡等缺陷的影响.</p>
                </div>
                <h4 class="anchor-tag" id="51" name="51">2.2 <b>改进的三元组损失函数</b></h4>
                <div class="p1">
                    <p id="52">在训练三元组卷积神经网络的过程中,不进行难样本挖掘会使网络相对快地学习较简单的(对训练网络提供很少价值)三元组样本,而在平均的过程中抵消少数有用样本组贡献,导致训练阻塞收敛效果不佳.但选择过难的三元组样本,又会引起训练不稳定收敛变难,而文献<citation id="105" type="reference">[<a class="sup">4</a>]</citation>介绍的小批量难样本挖掘策略,可有效克服上述缺陷,因此本文利用该策略选择三元组样本.</p>
                </div>
                <div class="p1">
                    <p id="53">该策略的核心思想是:通过随机采样<i>P</i>个类(行人身份),并从每个类中随机选择<i>K</i>个图片,形成具有<i>PK</i>个图像的一个小批量.然后,对于小批量中的每一个样本<i>X</i>,选择其在小批量中相似性距离最大的正样本和相似性距离最小的负样本,形成<i>PK</i>个有效的三元组样本来计算损失.称其为小批量难样本:</p>
                </div>
                <div class="p1">
                    <p id="54" class="code-formula">
                        <mathml id="54"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mover><mstyle mathsize="140%" displaystyle="true"><mrow><mover accent="true"><mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mrow><mi>p</mi><mo>=</mo><mn>1</mn><mo>⋯</mo><mi>Κ</mi></mrow></munder><mi>D</mi><mrow><mo>(</mo><mrow><mi>f</mi><msub><mrow></mrow><mi>θ</mi></msub><mrow><mo>(</mo><mrow><mi>x</mi><msubsup><mrow></mrow><mi>a</mi><mi>i</mi></msubsup></mrow><mo>)</mo></mrow><mo>,</mo><mi>f</mi><msub><mrow></mrow><mi>θ</mi></msub><mrow><mo>(</mo><mrow><mi>x</mi><msubsup><mrow></mrow><mi>p</mi><mi>i</mi></msubsup></mrow><mo>)</mo></mrow></mrow><mo>)</mo></mrow></mrow><mo stretchy="true">⌢</mo></mover></mrow></mstyle><mrow><mtable><mtr><mtd columnalign="left"><mtext>h</mtext><mtext>a</mtext><mtext>r</mtext><mtext>d</mtext><mtext>e</mtext><mtext>s</mtext><mtext>t</mtext></mtd><mtd columnalign="left"><mtext>p</mtext><mtext>o</mtext><mtext>s</mtext><mtext>i</mtext><mtext>t</mtext><mtext>i</mtext><mtext>v</mtext><mtext>e</mtext></mtd></mtr></mtable></mrow></mover></mtd></mtr><mtr><mtd><mover><mstyle mathsize="140%" displaystyle="true"><mrow><mover accent="true"><mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>j</mi><mo>=</mo><mn>1</mn><mo>⋯</mo><mi>Ρ</mi></mrow></mstyle><mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>n</mi><mo>=</mo><mn>1</mn><mo>⋯</mo><mi>Κ</mi></mrow></mstyle><mrow><mi>j</mi><mo>≠</mo><mi>i</mi></mrow></munder></mrow></munder></mrow></munder><mi>D</mi><mrow><mo>(</mo><mrow><mi>f</mi><msub><mrow></mrow><mi>θ</mi></msub><mrow><mo>(</mo><mrow><mi>x</mi><msubsup><mrow></mrow><mi>a</mi><mi>i</mi></msubsup></mrow><mo>)</mo></mrow><mo>,</mo><mi>f</mi><msub><mrow></mrow><mi>θ</mi></msub><mrow><mo>(</mo><mrow><mi>x</mi><msubsup><mrow></mrow><mi>n</mi><mi>j</mi></msubsup></mrow><mo>)</mo></mrow></mrow><mo>)</mo></mrow></mrow><mo stretchy="true">⌢</mo></mover></mrow></mstyle><mrow><mtable><mtr><mtd columnalign="left"><mtext>h</mtext><mtext>a</mtext><mtext>r</mtext><mtext>d</mtext><mtext>e</mtext><mtext>s</mtext><mtext>t</mtext></mtd><mtd columnalign="left"><mtext>n</mtext><mtext>e</mtext><mtext>g</mtext><mtext>a</mtext><mtext>t</mtext><mtext>i</mtext><mtext>v</mtext><mtext>e</mtext></mtd></mtr></mtable></mrow></mover><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>3</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="55">式中,<i>X</i>代表小批量中的任意样本,并且样本<i>x</i><mathml id="56"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mi>i</mi></msubsup></mrow></math></mathml>对应于小批量中第<i>i</i>个行人的第<i>j</i>个图像.<mathml id="57"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>f</mi><msub><mrow></mrow><mi>θ</mi></msub><mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow></math></mathml>是经过卷积神经网络后样本图像的输出特征,<mathml id="58"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>D</mi><mrow><mo>(</mo><mrow><mi>f</mi><msub><mrow></mrow><mi>θ</mi></msub><mrow><mo>(</mo><mrow><mi>x</mi><msub><mrow></mrow><mi>a</mi></msub></mrow><mo>)</mo></mrow><mo>,</mo><mi>f</mi><mrow><mo>(</mo><mrow><mi>x</mi><msub><mrow></mrow><mi>p</mi></msub></mrow><mo>)</mo></mrow></mrow><mo>)</mo></mrow></mrow></math></mathml>和<mathml id="59"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>D</mi><mrow><mo>(</mo><mrow><mi>f</mi><msub><mrow></mrow><mi>θ</mi></msub><mrow><mo>(</mo><mrow><mi>x</mi><msub><mrow></mrow><mi>a</mi></msub></mrow><mo>)</mo></mrow><mo>,</mo><mi>f</mi><mrow><mo>(</mo><mrow><mi>x</mi><msub><mrow></mrow><mi>n</mi></msub></mrow><mo>)</mo></mrow></mrow><mo>)</mo></mrow></mrow></math></mathml>是样本特征间差异度.利用该策略选择的三元组样本不仅数量比传统方法增加了3倍,同时,因选择的三元组样本是数据集的一个小子集中最难的样本,信息价值较高又不容易过拟合,因此可认为是最适合网络训练的样本.</p>
                </div>
                <div class="p1">
                    <p id="60">其次,基于三元组卷积神经网络的传统行人再识别方法以铰链(hinge)函数为损失函数,但在特征自动学习的过程中其值域是变化的,而式中的间隔参数却是固定不变的,需要事先指定,训练中无法自适应调整,缺乏灵活性,同时可能因为间隔参数设定的不合理而影响网络训练.而文献<citation id="106" type="reference">[<a class="sup">7</a>]</citation>采用Log-logistic函数代替铰链函数,无需人工设定间隔参数,改进了特征与度量函数联合优化的效果.然而该方法只是让正样本对间相似度尽可能大,负样本对间相似度尽可能小,没有约束正样本对和负样本对之间的相似度差异,这可能会使三元组样本中出现正样本对间相似度比负样本对间更小的情况,显然,这不利于改善行人再识别性能.</p>
                </div>
                <div class="p1">
                    <p id="61">为了克服上述缺陷,本文考虑到在正样本对差异度最小化的情况下,负样本对间的差异度越大,两者之间差异度的间隔也就越大,因此本文提出公式(4)所示的改进型三元组损失函数.该公式不仅能使正样本对特征间差异度最小化,负样本对特征间差异度最大化,而且使三元组样本中负样本对间差异度比正样本对间的差异度尽可能大.改进型损失函数旨在拉近属于同一行人身份的样本,同时在学习的特征空间中推远属于不同行人的样本.这与许多数据聚类和判别分析方法所使用的原理更加一致.</p>
                </div>
                <div class="p1">
                    <p id="62" class="code-formula">
                        <mathml id="62"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mtext>l</mtext><mtext>o</mtext><mtext>s</mtext><mtext>s</mtext><mo>=</mo><mo>-</mo><mfrac><mn>1</mn><mrow><mi>Ρ</mi><mi>Κ</mi></mrow></mfrac><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Ρ</mi></munderover><mrow></mrow></mstyle><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>a</mi><mo>=</mo><mn>1</mn></mrow><mi>Κ</mi></munderover><mrow></mrow></mstyle></mtd></mtr><mtr><mtd><mrow><mo>[</mo><mrow><mi>lg</mi><mrow><mo>(</mo><mrow><mn>1</mn><mo>+</mo><mi>exp</mi><mrow><mo>(</mo><mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>min</mi></mrow></mstyle><mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>j</mi><mo>=</mo><mn>1</mn><mo>⋯</mo><mi>Ρ</mi></mrow></mstyle><mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>n</mi><mo>=</mo><mn>1</mn><mo>⋯</mo><mi>Κ</mi></mrow></mstyle><mrow><mi>j</mi><mo>≠</mo><mi>i</mi></mrow></munder></mrow></munder></mrow></munder><mi>D</mi><mrow><mo>(</mo><mrow><mi>f</mi><msub><mrow></mrow><mi>θ</mi></msub><mrow><mo>(</mo><mrow><mi>x</mi><msubsup><mrow></mrow><mi>a</mi><mi>i</mi></msubsup></mrow><mo>)</mo></mrow><mo>,</mo><mi>f</mi><msub><mrow></mrow><mi>θ</mi></msub><mrow><mo>(</mo><mrow><mi>x</mi><msubsup><mrow></mrow><mi>n</mi><mi>j</mi></msubsup></mrow><mo>)</mo></mrow></mrow><mo>)</mo></mrow><mo>-</mo></mrow><mo>)</mo></mrow></mrow><mo>)</mo></mrow></mrow></mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mrow><mi>p</mi><mo>=</mo><mn>1</mn><mo>⋯</mo><mi>Κ</mi></mrow></munder><mi>D</mi><mrow><mo>(</mo><mrow><mi>f</mi><msub><mrow></mrow><mi>θ</mi></msub><mrow><mo>(</mo><mrow><mi>x</mi><msubsup><mrow></mrow><mi>a</mi><mi>i</mi></msubsup></mrow><mo>)</mo></mrow><mo>,</mo><mi>f</mi><msub><mrow></mrow><mi>θ</mi></msub><mrow><mo>(</mo><mrow><mi>x</mi><msubsup><mrow></mrow><mi>p</mi><mi>i</mi></msubsup></mrow><mo>)</mo></mrow></mrow><mo>)</mo></mrow></mtd></mtr><mtr><mtd><mo>+</mo><mrow><mrow><mi>lg</mi><mrow><mo>(</mo><mrow><mn>1</mn><mo>+</mo><mi>exp</mi><mrow><mo>(</mo><mrow><mo>-</mo><mrow><mo>(</mo><mrow><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mrow><mi>p</mi><mo>=</mo><mn>1</mn><mo>⋯</mo><mi>Κ</mi></mrow></munder><mi>D</mi><mrow><mo>(</mo><mrow><mi>f</mi><msub><mrow></mrow><mi>θ</mi></msub><mrow><mo>(</mo><mrow><mi>x</mi><msubsup><mrow></mrow><mi>a</mi><mi>i</mi></msubsup></mrow><mo>)</mo></mrow><mo>,</mo><mi>f</mi><msub><mrow></mrow><mi>θ</mi></msub><mrow><mo>(</mo><mrow><mi>x</mi><msubsup><mrow></mrow><mi>p</mi><mi>i</mi></msubsup></mrow><mo>)</mo></mrow></mrow><mo>)</mo></mrow></mrow><mo>)</mo></mrow></mrow><mo>)</mo></mrow></mrow><mo>)</mo></mrow></mrow><mo>]</mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>4</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="63">式中,PK是小批量中的样本个数,<i>i</i>指的是<i>P</i>个行人身份中的任意一个,<i>a</i>指的是<i>k</i>个相同身份行人图片中的任意一张图片.从公式(4)可以看出,为了优化该目标函数,训练过程中会自动调整网络权重,使正样本对间差异度最小化的同时,负样本对间差异度比正样本对间的差异度更大,而不再追求<mathml id="64"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>D</mi><mrow><mo>(</mo><mrow><mi>f</mi><msub><mrow></mrow><mi>θ</mi></msub><mrow><mo>(</mo><mrow><mi>x</mi><msubsup><mrow></mrow><mi>a</mi><mi>i</mi></msubsup></mrow><mo>)</mo></mrow><mo>,</mo><mi>f</mi><msub><mrow></mrow><mi>θ</mi></msub><mrow><mo>(</mo><mrow><mi>x</mi><msubsup><mrow></mrow><mi>n</mi><mi>j</mi></msubsup></mrow><mo>)</mo></mrow></mrow><mo>)</mo></mrow><mo>-</mo><mi>D</mi><mrow><mo>(</mo><mrow><mi>f</mi><msub><mrow></mrow><mi>θ</mi></msub><mrow><mo>(</mo><mrow><mi>x</mi><msubsup><mrow></mrow><mi>a</mi><mi>i</mi></msubsup></mrow><mo>)</mo></mrow><mo>,</mo><mi>f</mi><msub><mrow></mrow><mi>θ</mi></msub><mrow><mo>(</mo><mrow><mi>x</mi><msubsup><mrow></mrow><mi>p</mi><mi>i</mi></msubsup></mrow><mo>)</mo></mrow></mrow><mo>)</mo></mrow><mo>≥</mo><mi>C</mi></mrow></math></mathml>这种硬性指标,因此无需设置间隔参数<i>C</i>,提高了目标函数优化的灵活性,不仅能方便训练,同时能有效提高特征与度量函数联合优化的能力.</p>
                </div>
                <h4 class="anchor-tag" id="65" name="65">2.3 <b>置信度测量机制</b></h4>
                <div class="p1">
                    <p id="66">对于表观特征较相似的不同行人,如果仅利用直接度量进行两幅图像的相似度比较,往往会出现负样本比正样本更匹配测试样本的情况.为了有效避免误匹配,充分利用与图像对相关的其他图像的判别信息,本文利用置信度测量机制<citation id="107" type="reference"><link href="17" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>进行间接度量.</p>
                </div>
                <div class="p1">
                    <p id="67">置信度测量机制的主要思想:画廊集图像中与测试行人图像匹配的目标行人,需满足以下两个条件:(1)与测试行人图像身份相同的画廊集图像应该位于相似性排名列表的前面,因为其与测试行人图像的差异度较小;(2)测试行人图像与目标图像之间的距离应该比目标图像与画廊集其余图像之间的距离都小,因为目标图像与测试图像中的行人身份相同.因此,认为目标图像与其余画廊集图像之间的距离比其与测试图像之间的距离大的个数,与该目标图像与测试图像身份相同的置信度得分成正相关.具体计算过程如下.</p>
                </div>
                <div class="p1">
                    <p id="68">首先,根据卷积神经网络的输出特征,计算测试图像和画廊集中所有行人图像之间的距离,并根据距离大小计算排名列表,差异度距离小的排名越靠前.表示为:<i>R</i>=[<i>r</i><sub>1</sub>,,<i>r</i><sub>2</sub>,…,<i>r</i><sub><i>N</i></sub>],<i>r</i><sub>1</sub>表示画廊集第一个行人图像的排名,<i>r</i><sub>2</sub>表示画廊集第二个行人图像的排名,其余的则以此类推.重新排序机制具体计算过程包括两个步骤:第一步,计算所有画廊集行人图像相互之间的距离.第二步:计算每个画廊集样本图像相对于测试图像的置信度得分.对每一个画廊集行人<i>g</i><sub><i>i</i></sub> , <i>i</i>=1,…,<i>C</i>,置信度得分<i>sg</i><sub><i>i</i></sub>取决于<i>g</i><sub><i>i</i></sub>与其余画廊集行人(outliers)图像之间的距离小于<i>g</i><sub><i>i</i></sub>和测试行人图像之间距离的总数.如果<i>g</i><sub><i>i</i></sub>是完全正确的匹配,它对应的<i>sg</i><sub><i>i</i></sub>必须等于零.也就是说,<i>sg</i><sub><i>i</i></sub>越小意味着<i>g</i><sub><i>i</i></sub>与测试行人图像越相似,<i>g</i><sub><i>i</i></sub>为目标行人图像的概率越大.计算完所有画廊集行人图像的置信度得分后,得分被汇总为:<i>S</i><sub><i>g</i></sub>=[<i>sg</i><sub>1</sub>,<i>sg</i><sub>2</sub>,…,<i>sg</i><sub><i>c</i></sub>].</p>
                </div>
                <div class="p1">
                    <p id="69">最终的排名列表<i>Rf</i>是第一阶段输出<i>R</i>=[<i>r</i><sub>1</sub>,<i>r</i><sub>2</sub>,…,<i>r</i><sub><i>N</i></sub>]和第二阶段输出<i>S</i><sub><i>g</i></sub>=[<i>sg</i><sub>1</sub>,<i>sg</i><sub>2</sub>,…,<i>sg</i><sub><i>c</i></sub>]的融合,所以<i>Rf</i>为:<mathml id="70"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>R</mi><mi>f</mi><mo>=</mo><mi>R</mi><mo>˚</mo><mrow><mo>(</mo><mrow><mn>1</mn><mo>+</mo><mi>S</mi><msub><mrow></mrow><mi>g</mi></msub></mrow><mo>)</mo></mrow><mo>.</mo><mo>˚</mo></mrow></math></mathml>表示<i>R</i>和<i>S</i><sub><i>g</i></sub>之间逐元素哈达马积(hadamard product).</p>
                </div>
                <h3 id="71" name="71" class="anchor-tag">3 <b>实验分析</b></h3>
                <h4 class="anchor-tag" id="72" name="72">3.1 <b>数据集</b></h4>
                <div class="p1">
                    <p id="73">为了验证嵌套池化三元组卷积神经网络的有效性,本文在<i>Market</i>-1501、<i>CUHK</i>03和<i>VIPeR</i>这3个数据集上进行测试实验.表1给出实验数据集的具体信息.其中:<i>ID</i>表示该数据集含有的行人数量;<i>box</i>表示该数据集包含的行人图像数量;<i>box</i>/<i>ID</i>表示每个行人平均含有的行人图像数量.</p>
                </div>
                <div class="area_img" id="74">
                    <p class="img_tit"><b>表</b>1 <b>数据集的具体信息</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="74" border="1"><tr><td><br />数据集</td><td><i>ID</i></td><td><i>box</i></td><td><i>box</i>/<i>ID</i></td></tr><tr><td><br /><i>Market</i>-1501</td><td>1 501</td><td>32 217</td><td>21.5</td></tr><tr><td><br /><i>CUHK</i>03</td><td>1 467</td><td>13 164</td><td>9</td></tr><tr><td><br /><i>VIPeR</i></td><td>632</td><td>1 264</td><td>2</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="75" name="75">3.2 <b>实验结果及分析</b></h4>
                <div class="p1">
                    <p id="76">为评估所用方法在解决行人再识别问题时的有效性,本文使用<i>rank</i>_5匹配率和平均准确率(<i>mAP</i>)来衡量所用方法的性能.</p>
                </div>
                <div class="p1">
                    <p id="77" class="code-formula">
                        <mathml id="77"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>Ρ</mi><mo>=</mo><mfrac><mi>A</mi><mrow><mi>A</mi><mo>+</mo><mi>B</mi></mrow></mfrac><mo>×</mo><mn>1</mn><mn>0</mn><mn>0</mn><mi>%</mi></mtd></mtr><mtr><mtd><mi>R</mi><mo>=</mo><mfrac><mi>A</mi><mrow><mi>A</mi><mo>+</mo><mi>C</mi></mrow></mfrac><mo>×</mo><mn>1</mn><mn>0</mn><mn>0</mn><mi>%</mi></mtd></mtr><mtr><mtd><mi>m</mi><mi>A</mi><mi>Ρ</mi><mo>=</mo><mrow><mstyle displaystyle="true"><mrow><mo>∫</mo><mrow></mrow></mrow></mstyle></mrow><msubsup><mrow></mrow><mn>0</mn><mn>1</mn></msubsup><mi>Ρ</mi><mrow><mo>(</mo><mi>R</mi><mo>)</mo></mrow><mi>d</mi><mi>R</mi></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="78">式中,<i>P</i>为准确率;<i>R</i>为召回率;<i>A</i>为系统检测到的相关样本;<i>B</i>为系统检测到的不相关样本;<i>C</i>为未检测到的相关样本.</p>
                </div>
                <div class="p1">
                    <p id="79">在GPU@3.90 GHz,128 GB显卡内存,64位centos7的计算机系统下进行了相应的实验,并对结果进行分析.实验分为两个部分.</p>
                </div>
                <div class="p1">
                    <p id="80">实验一、嵌套池化三元组卷积神经网络包含两个新的成分:(1)利用嵌套不变池理论改进网络结构.(2)利用Log-logistic函数代替传统三元组损失函数中的铰链函数,改进损失函数的优化策略.因此,为了评估每个成分对性能改善的影响,实验设置4个变量:</p>
                </div>
                <div class="p1">
                    <p id="81">(1)(Ours)使用传统的网络结构和三元组损失函数;</p>
                </div>
                <div class="p1">
                    <p id="82">(2)(OursT)使用改进后的网络结构和传统的三元组损失函数;</p>
                </div>
                <div class="p1">
                    <p id="83">(3)(OursTI)使用改进后的网络结构和新型三元组损失函数;</p>
                </div>
                <div class="p1">
                    <p id="84">(4)(OursTIR)使用改进后的网络结构和新型三元组损失函数,并利用置信度测量机制进行间接度量.</p>
                </div>
                <div class="p1">
                    <p id="85">在Market-1501、CUHK03和VIPeR数据集上进行训练和测试,实验结果如表2、3和4所示,比较不同实验变量下行人再识别评价指标,得出以下结论:</p>
                </div>
                <div class="p1">
                    <p id="86">(1)对比Ours和OursN的实验结果可知:与传统网络结构相比仅利用NIP(Nested Invariance Pooling)理论改进网络结构就可将Rank_5识别率提高3%;</p>
                </div>
                <div class="p1">
                    <p id="87">(2)对比OursN和OursNI的实验结果发现,与使用原三元组损失函数的相同深度卷积神经网络相比,仅使用改进的三元组损失函数就可以将行人再识别率提高约2%;</p>
                </div>
                <div class="p1">
                    <p id="88">(3)对比OursNI和OursNIR的结果,可知置信度测量机制在VIPeR数据集上相比于第一次排序准确率提升约2%,而在Market1501和CUHK03数据集中准确率提升约1%.这表明,置信度测量机制在single-shot数据集中比multi-shot表现更好.这是因为在single-shot数据集中,每个测试样本只有一个正确匹配,如果匹配样本与测试样本之间的距离足够小,很有可能利用置信度测量机制纠正误匹配.而在multi-shot数据集中,每个测试样本有多个正确匹配,重新排序将处理那些与测试行人之间距离很小,但不属于正确匹配的样本.相比于single-shot数据集,识别率提升很小.</p>
                </div>
                <div class="area_img" id="89">
                    <p class="img_tit"><b>表</b>2 Market1501<b>数据集实验结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"> %</p>
                    <table id="89" border="1"><tr><td><br />Method</td><td>Rank_5</td><td>mAP</td></tr><tr><td><br />Ours</td><td>83.38</td><td>68.63</td></tr><tr><td><br />OursN</td><td>86.56</td><td>71.92</td></tr><tr><td><br />OursNI</td><td>88.92</td><td>72.32</td></tr><tr><td><br />OursNIR</td><td>91.01</td><td>83.09</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="90">
                    <p class="img_tit"><b>表</b>3 CUHK03<b>数据集实验结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"> %</p>
                    <table id="90" border="1"><tr><td><br />Method</td><td>Rank_5</td><td>mAP</td></tr><tr><td><br />Ours</td><td>70.79</td><td>58.64</td></tr><tr><td><br />OursN</td><td>74.08</td><td>60.36</td></tr><tr><td><br />OursNI</td><td>75.82</td><td>62.99</td></tr><tr><td><br />OursNIR</td><td>76.73</td><td>63.65</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="91">
                    <p class="img_tit"><b>表</b>4 VIPeR<b>数据集实验结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"> %</p>
                    <table id="91" border="1"><tr><td><br />Method</td><td>Rank_5</td><td>mAP</td></tr><tr><td><br />Ours</td><td>64.06</td><td>55.57</td></tr><tr><td><br />OursN</td><td>67.78</td><td>57.39</td></tr><tr><td><br />OursNI</td><td>69.92</td><td>58.68</td></tr><tr><td><br />OursNIR</td><td>72.06</td><td>60.06</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="92">实验二、在自建数据集中实验结果</p>
                </div>
                <div class="p1">
                    <p id="93">为了验证本文方法在非理想自然条件下的行人再识别效率,从Market-1501、CUHK03和VIPeR数据集中选取具有亮度变化、部分遮挡、分辨率低和旋转、尺度等变化的行人图像样本作为测试集,从而建立专门针对非理想自然条件下的行人再识别数据集,其中测试数据集样本尺寸归一化为224×224大小.</p>
                </div>
                <div class="p1">
                    <p id="94">表5中的实验结果表明:Sakrapee方法<citation id="108" type="reference"><link href="19" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>通过组合几个不同的方法,提高了再识别率.DeepM方法<citation id="109" type="reference"><link href="3" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation> 将行人图像分割为头、躯干和腿,然后训练多分支卷积神经网络自适应地提取各部分的特征,取得了不错的效果.LOMO方法<citation id="110" type="reference"><link href="13" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>提取水平方向发生概率较大的特征,有效改善了视角变化的影响.Deep SP方法<citation id="111" type="reference"><link href="21" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>在输入层利用空间金字塔结构,分别在整体图像,4个大小相同条纹以及8个大小相同小块上训练3个不同的卷积神经网络模型,提取不同分辨率的特征,并根据特征的精细程度加权求和不同分辨率特征的相似度.该方法的再识别效果优于一般方法,而逊色于本文的行人再识别效果.</p>
                </div>
                <div class="area_img" id="95">
                    <p class="img_tit"><b>表</b>5 <b>自建数据集中对比实验</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"> %</p>
                    <table id="95" border="1"><tr><td><br />Method</td><td>Top1</td><td>Top10</td><td>Top20</td></tr><tr><td><br />LOMO</td><td>50.7</td><td>68.2</td><td>77.3</td></tr><tr><td><br />DeepM</td><td>52.3</td><td>77.7</td><td>89.1</td></tr><tr><td><br />Deep SP</td><td>56.5</td><td>82.9</td><td>96.4</td></tr><tr><td><br />Sakrapee</td><td>54.8</td><td>-</td><td>-</td></tr><tr><td><br />OursNIR</td><td>60.3</td><td>86.2</td><td>97.8</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h3 id="96" name="96" class="anchor-tag">4 <b>结束语</b></h3>
                <div class="p1">
                    <p id="97">本文利用嵌套不变池理论在均方根池化后,依次添加平均池化和最大池化提取全局特征,提高了特征对平移、尺度、旋转变化的鲁棒性.其次,利用<i>AlignedReID</i>方法计算局部距离,同时进行局部特征和全局特征的联合学习,有助于网络关注有用的图像区域,并区分具有细微差别的类似人物图像.然后改进损失函数的优化策略,不仅方便训练,同时提高了特征与损失函数的联合优化效果.但是,置信度测量机制在<i>multi</i>-<i>shot</i>数据集中避免误匹配的效果不是很好,有待进一步提升.</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep convolutional neural networks with adaptive spatial feature for person re-identification">

                                <b>[1]</b> <i>SONG Z</i>,<i>CAI X</i>,<i>CHEN Y</i>,<i>et al</i>.<i>Deep convolutional neural networks with adaptive spatial feature for person re</i>-<i>identification</i>[<i>C</i>]// <i>Advanced Information Technology</i>,<i>Electronic and Automation Control Conference</i>.<i>Chongqing</i>,<i>China</i>,<i>IEEE</i>,2017:2020-2023.
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep deformable patch metric learning for person re-identification">

                                <b>[2]</b> <i>B</i>а<i>K S</i>,<i>CARR P</i>.<i>Deep deformable patch metric learning for person re</i>-<i>identification</i>[<i>J</i>].<i>IEEE Transactions on Circuits</i> &amp; <i>Systems for Video Technology</i>,2017(99):1-3.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Alignedreid:Surpassing human-level performance in person re-identification">

                                <b>[3]</b> <i>ZHANG X</i>,<i>LUO H</i>,<i>FAN X</i>,<i>et al</i>.<i>Alignedreid</i>:<i>Surpassing human</i>-<i>level performance in person re</i>-<i>identification</i>[<i>J</i>].<i>arXiv preprint arXiv</i>:1711.08184,2017,31(2):1-10.
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=In defense of the triplet loss for person re-identification">

                                <b>[4]</b> <i>HERMANS A</i>,<i>BEYER L</i>,<i>LEIBE B</i>.<i>In defense of the triplet loss for person re</i>-<i>identification</i>[<i>J</i>].<i>arXiv preprint arXiv</i>:1703.07737,2017,4(21):1-17.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Compact deep invariant descriptors for video Rretrieval">

                                <b>[5]</b> <i>LOU Y</i>,<i>BAI Y</i>,<i>LIN J</i>,<i>et al</i>.<i>Compact deep invariant descriptors for video Rretrieval</i>[<i>C</i>]//<i>Data Compression Conference</i>[<i>s</i>.<i>l</i>.].<i>IEEE</i>,2017:420-429.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Person re-identification by localmaximal occurrence representation and metric learning">

                                <b>[6]</b> <i>LIAO S</i>,<i>HU Y</i>,<i>ZHU X</i>,<i>et al</i>.<i>Person re</i>-<i>identification by local maximal occurrence representation and metric learning</i>[<i>C</i>]//<i>Computer Vision and Pattern Recognition</i>.<i>Boston</i>,<i>Massachusetts</i>,<i>USA</i>,<i>IEEE</i>,2015:2197-2206.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=DZYX201804035&amp;v=MzE1NTlOSVRmU2RyRzRIOW5NcTQ5R1lZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdUZ5bmtXcnY=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bFF0dU9NTmdMQ0V1SURuSmxZZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[7]</b> 朱建清,曾焕强,杜永兆,等.基于新型三元卷积神经网络的行人再辨识算法[<i>J</i>].电子与信息学报,2018,40(4):1012-1016.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES7A5312B3D54C4525EF23FA1069FB6481&amp;v=MTQ4NDY3anJCUThEOENTUWJLZUNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkNwYlEzNWRoaHhibTJ4SzA9TmlmT2ZiVEpHOUxOcmYxR0VPNExmM2c4elJObW5EMStQZw==&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bFF0dU9NTmdMQ0V1SURuSmxZZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b> <i>MIRMAHBOUB B</i>,<i>MEKHAFI M L</i>,<i>MURINO V</i>,<i>et al</i>.<i>Person re</i>-<i>identification by order</i>-<i>induced metric fusion</i>[<i>J</i>].<i>Neurocomputing</i>,2018(275):667-676.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Learning to rank in person reidentification with metric ensembles">

                                <b>[9]</b> <i>PAISITKRIANGKRAI S</i>,<i>SHEN C</i>,<i>HENGEL A V D</i>.<i>Learning to rank in person re</i>-<i>identification with metric ensembles</i>[<i>C</i>]//<i>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</i>.<i>Boston</i>,<i>MA</i>,<i>USA</i>:<i>IEEE</i>,2015:1846-1855.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep spatial pyramid for person re-identification">

                                <b>[10]</b> <i>CARR S B P</i>.<i>Deep spatial pyramid for person re</i>-<i>identification</i>[<i>C</i>]// <i>IEEE International Conference on Advanced Video and Signal Based Surveillance</i>.<i>Lecce</i>,<i>Italy</i>,<i>IEEE</i>,2017:1-6.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="WXYJ201909014" />
        <input id="dpi" type="hidden" value="800" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bFF0dU9NTmdMQ0V1SURuSmxZZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=WXYJ201909014&amp;v=MTUyMzVrV3J2S01qWFNaTEc0SDlqTXBvOUVZSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnVGeW4=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bFF0dU9NTmdMQ0V1SURuSmxZZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
