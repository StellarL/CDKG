<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637133357704190000%26DBCODE%3dCJFD%26TABLEName%3dCJFDTEMP%26FileName%3dWXYJ201911014%26RESULT%3d1%26SIGN%3dwRiGMJ85sQAMOEwmtAlze9EbKVE%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=WXYJ201911014&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=WXYJ201911014&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=WXYJ201911014&amp;v=MDE5OTQ5RVlJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdkZDbmdXN3pBTWpYU1pMRzRIOWpOcm8=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sRzVOZlY0eG5jQ2MrRjhjT2RFVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#25" data-title="1 &lt;b&gt;引言&lt;/b&gt; ">1 <b>引言</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#29" data-title="2 &lt;b&gt;相关研究&lt;/b&gt; ">2 <b>相关研究</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#34" data-title="3 &lt;b&gt;神经元容错度量化方法&lt;/b&gt; ">3 <b>神经元容错度量化方法</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#36" data-title="3.1 &lt;b&gt;神经元容错度计算&lt;/b&gt;">3.1 <b>神经元容错度计算</b></a></li>
                                                <li><a href="#47" data-title="3.2 &lt;b&gt;神经元容错度统计排序方法&lt;/b&gt;">3.2 <b>神经元容错度统计排序方法</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#71" data-title="4 &lt;b&gt;神经元重要度分析及精度优化方法&lt;/b&gt; ">4 <b>神经元重要度分析及精度优化方法</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#79" data-title="5 &lt;b&gt;实验结果&lt;/b&gt; ">5 <b>实验结果</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#82" data-title="5.1 &lt;b&gt;神经元裁剪&lt;/b&gt;">5.1 <b>神经元裁剪</b></a></li>
                                                <li><a href="#86" data-title="5.2 &lt;b&gt;近似计算&lt;/b&gt;">5.2 <b>近似计算</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#93" data-title="6 &lt;b&gt;结束语&lt;/b&gt; ">6 <b>结束语</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#33" data-title="&lt;b&gt;图&lt;/b&gt;1 CPU&lt;b&gt;结构图&lt;/b&gt;"><b>图</b>1 CPU<b>结构图</b></a></li>
                                                <li><a href="#38" data-title="&lt;b&gt;图&lt;/b&gt;2 &lt;b&gt;神经网络自学传播关系&lt;/b&gt;"><b>图</b>2 <b>神经网络自学传播关系</b></a></li>
                                                <li><a href="#68" data-title="&lt;b&gt;图&lt;/b&gt;3 &lt;b&gt;神经元出现次数的分布&lt;/b&gt;"><b>图</b>3 <b>神经元出现次数的分布</b></a></li>
                                                <li><a href="#70" data-title="&lt;b&gt;图&lt;/b&gt;4 &lt;b&gt;相同错误下神经网络输出精度&lt;/b&gt;"><b>图</b>4 <b>相同错误下神经网络输出精度</b></a></li>
                                                <li><a href="#78" data-title="&lt;b&gt;图&lt;/b&gt;5 &lt;b&gt;循环裁剪流程图&lt;/b&gt;"><b>图</b>5 <b>循环裁剪流程图</b></a></li>
                                                <li><a href="#84" data-title="&lt;b&gt;图&lt;/b&gt;6 &lt;b&gt;神经元裁剪精度&lt;/b&gt;"><b>图</b>6 <b>神经元裁剪精度</b></a></li>
                                                <li><a href="#85" data-title="&lt;b&gt;表&lt;/b&gt;1 &lt;b&gt;神经元裁剪实验数据表&lt;/b&gt;"><b>表</b>1 <b>神经元裁剪实验数据表</b></a></li>
                                                <li><a href="#90" data-title="&lt;b&gt;图&lt;/b&gt;7 k&lt;b&gt;的个数对神经网络质量的影响&lt;/b&gt;"><b>图</b>7 k<b>的个数对神经网络质量的影响</b></a></li>
                                                <li><a href="#92" data-title="&lt;b&gt;图&lt;/b&gt;8 &lt;b&gt;近似后的神经网络质量对比&lt;/b&gt;"><b>图</b>8 <b>近似后的神经网络质量对比</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" title=" YU J,LUKEFAHR A,PALFRAMAN D,et al.Scalpel:customizing DNN pruning to the underlying hardware parallelism[J].AcmSigarch Computer Architecture News,2017,45(2):548-560." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCMFD04707E68164B519899AE976FC04C63&amp;v=MDg5NDdMeWhjYTRqWjBPUXJycXhSRENyS1FOcnljQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQ3BiUTM1ZGxpeGIyM3c2QT1OaWZJWThYTUh0WExyNGd3WXVNT0NuaA==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sRzVOZlY0eG5jQ2MrRjhjT2RFVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         YU J,LUKEFAHR A,PALFRAMAN D,et al.Scalpel:customizing DNN pruning to the underlying hardware parallelism[J].AcmSigarch Computer Architecture News,2017,45(2):548-560.
                                    </a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" title=" BABAEIZADEH M,SMARAGDIS P,CAMPBELL R H.NoiseOut:a simple way to prune neural networks[J].arXiv.org &amp;gt; cs &amp;gt; arXiv:1611.06211v1,2016." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=NoiseOut:a simple way to prune neural networks">
                                        <b>[2]</b>
                                         BABAEIZADEH M,SMARAGDIS P,CAMPBELL R H.NoiseOut:a simple way to prune neural networks[J].arXiv.org &amp;gt; cs &amp;gt; arXiv:1611.06211v1,2016.
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title=" YANG T J,CHEN Y H ,SZE V.Designing energy-efficient convolutional neural networks using energy-aware pruning[C]// Computer Vision &amp;amp; Pattern Recognition.Honolulu,HI,USA,IEEE,2017." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Designing energy-efficient convolutional neural networks using energy-aware pruning">
                                        <b>[3]</b>
                                         YANG T J,CHEN Y H ,SZE V.Designing energy-efficient convolutional neural networks using energy-aware pruning[C]// Computer Vision &amp;amp; Pattern Recognition.Honolulu,HI,USA,IEEE,2017.
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" title=" ZHANG Q ,WANG T ,TIAN Y ,et al.ApproxANN:an approximate computing framework for artificial neural network[C]// Design,Automation &amp;amp; Test in Europe Conference &amp;amp; Exhibition.Grenoble,France,EDA Consortium,2015." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=ApproxANN:an approximate computing framework for artificial neural network">
                                        <b>[4]</b>
                                         ZHANG Q ,WANG T ,TIAN Y ,et al.ApproxANN:an approximate computing framework for artificial neural network[C]// Design,Automation &amp;amp; Test in Europe Conference &amp;amp; Exhibition.Grenoble,France,EDA Consortium,2015.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" title=" COURBARIAUX M,BENGIO Y,DAVID J P.BinaryConnect:training deep neural networks with binary weights during propagations[C]// International Conference on Neural Information Processing Systems.Istanbul,Turkey,2015." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=BinaryConnect: training deep neural networks with binary weights during propagations">
                                        <b>[5]</b>
                                         COURBARIAUX M,BENGIO Y,DAVID J P.BinaryConnect:training deep neural networks with binary weights during propagations[C]// International Conference on Neural Information Processing Systems.Istanbul,Turkey,2015.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" title=" HAN S,MAO H,DALLY W J.Deep compression:compressing deep neural networks with pruning,trained quantization and huffman coding[J].Fiber,2015,56(4):3-7." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Deep Compression:Compressing Deep Neural Networks with Pruning,Trained Quantization and Huffman Coding">
                                        <b>[6]</b>
                                         HAN S,MAO H,DALLY W J.Deep compression:compressing deep neural networks with pruning,trained quantization and huffman coding[J].Fiber,2015,56(4):3-7.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" title=" SARWAR S S ,VENKATARAMANI S ,RAGHUNATHAN A ,et al.Multiplier-less artificial neurons exploiting error resiliency for energy-efficient neural computing[C]// 2016 Design,Automation &amp;amp; Test in Europe Conference &amp;amp; Exhibition (DATE).Dresden,Germany,IEEE,2016." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Multiplier-less artificial neurons exploiting error resiliency for energy-efficient neural computing">
                                        <b>[7]</b>
                                         SARWAR S S ,VENKATARAMANI S ,RAGHUNATHAN A ,et al.Multiplier-less artificial neurons exploiting error resiliency for energy-efficient neural computing[C]// 2016 Design,Automation &amp;amp; Test in Europe Conference &amp;amp; Exhibition (DATE).Dresden,Germany,IEEE,2016.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" title=" 邢静宇,张立臣.动态电压调整多处理器实时系统任务调度[J].微电子学与计算机,2006,23(2):55-57." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=WXYJ200602017&amp;v=MDYxMzc0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdkZDbmdXN3pBTWpYU1pMRzRIdGZNclk5RVk=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sRzVOZlY0eG5jQ2MrRjhjT2RFVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[8]</b>
                                         邢静宇,张立臣.动态电压调整多处理器实时系统任务调度[J].微电子学与计算机,2006,23(2):55-57.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" title=" GUERREIRO J,ILIC A,ROMA N,et al.GPGPU power modeling for multi-domain voltage-frequency scaling[C]// IEEE International Symposium on High Performance Computer Architecture.Vienna,Austria,2018." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=GPGPU power modeling for multi-domain voltage-frequency scaling">
                                        <b>[9]</b>
                                         GUERREIRO J,ILIC A,ROMA N,et al.GPGPU power modeling for multi-domain voltage-frequency scaling[C]// IEEE International Symposium on High Performance Computer Architecture.Vienna,Austria,2018.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_10" title=" 朱新召,胡哲琨,周莉,等.基于多核处理器的多层感知神经网络设计和实现[J].微电子学与计算机,2014,31(11):27-31." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=WXYJ201411007&amp;v=MDMyMjNVUkxPZVplVnZGQ25nVzd6QU1qWFNaTEc0SDlYTnJvOUZZNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckM=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sRzVOZlY0eG5jQ2MrRjhjT2RFVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[10]</b>
                                         朱新召,胡哲琨,周莉,等.基于多核处理器的多层感知神经网络设计和实现[J].微电子学与计算机,2014,31(11):27-31.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_11" title=" CARLSON T E ,HEIRMAN W ,EECKHOUT L .Sniper:exploring the level of abstraction for scalable and accurate parallel multi-core simulation[C]// High Performance Computing,Networking,Storage &amp;amp; Analysis.Washington,USA,IEEE,2011." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Sniper:exploring the level of abstraction for scalable and accurate parallel multi-core simulation">
                                        <b>[11]</b>
                                         CARLSON T E ,HEIRMAN W ,EECKHOUT L .Sniper:exploring the level of abstraction for scalable and accurate parallel multi-core simulation[C]// High Performance Computing,Networking,Storage &amp;amp; Analysis.Washington,USA,IEEE,2011.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=WXYJ" target="_blank">微电子学与计算机</a>
                2019,36(11),70-75+83             </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于神经元容错度分析的神经网络裁剪与近似计算技术研究</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E6%97%AD&amp;code=23675310&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sRzVOZlY0eG5jQ2MrRjhjT2RFVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王旭</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%8E%8B%E6%99%B6&amp;code=08707351&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sRzVOZlY0eG5jQ2MrRjhjT2RFVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">王晶</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%BC%A0%E4%BC%9F%E5%8A%9F&amp;code=21627418&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sRzVOZlY0eG5jQ2MrRjhjT2RFVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">张伟功</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E9%A6%96%E9%83%BD%E5%B8%88%E8%8C%83%E5%A4%A7%E5%AD%A6%E4%BF%A1%E6%81%AF%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2&amp;code=0044636&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sRzVOZlY0eG5jQ2MrRjhjT2RFVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">首都师范大学信息工程学院</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E4%B8%AD%E5%9B%BD%E7%A7%91%E5%AD%A6%E9%99%A2%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E7%A0%94%E7%A9%B6%E6%89%80%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84%E9%87%8D%E7%82%B9%E5%AE%9E%E9%AA%8C%E5%AE%A4&amp;code=0142480&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sRzVOZlY0eG5jQ2MrRjhjT2RFVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">中国科学院计算技术研究所体系结构重点实验室</a>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E7%94%B5%E5%AD%90%E7%B3%BB%E7%BB%9F%E5%8F%AF%E9%9D%A0%E6%80%A7%E5%8C%97%E4%BA%AC%E5%B8%82%E9%87%8D%E7%82%B9%E5%AE%9E%E9%AA%8C%E5%AE%A4&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sRzVOZlY0eG5jQ2MrRjhjT2RFVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">电子系统可靠性北京市重点实验室</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>本文将神经元裁剪和近似计算技术相结合,首先提出基于统计排序的神经元容错能力量化方法.然后,为了识别神经元的裁剪度,根据神经元的容错能力提出神经元重要程度排序算法.其次,引入轻量级的重训练,提出循环裁剪法,以探寻最优裁剪率.最后,根据神经元的容错能力,在神经网络运行过程中使用近似计算技术进一步降低功耗开销.本文通过两个实验,证明了该技术的有效性,其中以MNIST为例,在精度损失小于5%的情况下,压缩率达到50%,节能1.35倍.</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sRzVOZlY0eG5jQ2MrRjhjT2RFVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%A5%9E%E7%BB%8F%E5%85%83%E5%AE%B9%E9%94%99%E8%83%BD%E5%8A%9B&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sRzVOZlY0eG5jQ2MrRjhjT2RFVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">神经元容错能力;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%8A%82%E7%82%B9%E8%A3%81%E5%89%AA&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sRzVOZlY0eG5jQ2MrRjhjT2RFVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">节点裁剪;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E8%BF%91%E4%BC%BC%E8%AE%A1%E7%AE%97&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sRzVOZlY0eG5jQ2MrRjhjT2RFVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">近似计算;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    王旭 男,(1993-),硕士研究生.研究方向为计算机系统结构、容错计算.;
                                </span>
                                <span>
                                    *王晶（通信作者）女，（1982-），博士，副研究员．研究方向为计算机系统结构、高性能计算、容错计算、智能芯片设计．E-mail:jwang@cnu.edu.cn.;
                                </span>
                                <span>
                                    张伟功 男，（1967-），博士，研究员．研究方向为高可靠计算机体系结构及SoC设计、高速高可靠总线技术、计算机容错技术．;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2019-02-20</p>

                    <p>

                            <b>基金：</b>
                                                        <span>国家自然科学基金(61772350,61741211);</span>
                                <span>北京市高水平教师队伍建设计划项目(CIT&amp;TCD201704082,CIT&amp;TCD20170322);</span>
                                <span>体系结构国家重点实验室开放项目(CARCH201607);</span>
                                <span>北京市科技新星计划项目(Z181100006218093);</span>
                                <span>未来芯片高精尖中心开放课题(KYJJ2018008);</span>
                    </p>
            </div>
                    <h1><b>Research on neural network pruning and approximate computing technology based on neuron fault tolerance analysis</b></h1>
                    <h2>
                    <span>WANG Xu</span>
                    <span>WANG Jing</span>
                    <span>ZHANG Wei-gong</span>
            </h2>
                    <h2>
                    <span>Capital Normal University Information Engineering College</span>
                    <span>Institute of Copmuting Technology, Chinese Academy of Sciences</span>
                    <span>Beijing Key Laboratory of Electronic System Reliability and Prognostics,Capital Normal University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>This paper proposes to use neuron node pruning and approximate computing simultaneously. First, we propose a method to quantify the fault tolerance capability of neurons based on statistics. Then, to identify whether the neuron can be pruned, an importance ranking algorithm is proposed based on the fault tolerance capability. Next, introducing retrain and cyclic pruning to find the optimal pruning rate. Finally, approximate computing technique is used to further reduce power consumption during neuron network execution. The effectiveness of above technique is proved by two experiments. In the case of MNIST dataset, the compression rate is 50% and the power saving is 1.35× when the output accuracy loss is less than 5%.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=neural%20network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sRzVOZlY0eG5jQ2MrRjhjT2RFVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">neural network;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=neuron%20fault%20tolerance%20capability&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sRzVOZlY0eG5jQ2MrRjhjT2RFVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">neuron fault tolerance capability;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=node%20pruning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sRzVOZlY0eG5jQ2MrRjhjT2RFVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">node pruning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=approximate%20computing&amp;code=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sRzVOZlY0eG5jQ2MrRjhjT2RFVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">approximate computing;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2019-02-20</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="25" name="25" class="anchor-tag">1 <b>引言</b></h3>
                <div class="p1">
                    <p id="26">近年来,不断发展的神经网络结构也验证了更深的层级和参数可以有效的提高神经网络输出精度,但是所需硬件资源逐渐增多、消耗的能源增加.因此需要有效的方法对神经网络规模进行压缩,并优化其对资源的消耗.</p>
                </div>
                <div class="p1">
                    <p id="27">神经网络裁剪是现今流行的压缩方法之一,其通过压缩神经网络参数量达到降低资源开销的目的.权重裁剪<citation id="95" type="reference"><link href="3" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>和神经元裁剪(节点裁剪)<citation id="96" type="reference"><link href="5" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>更多的关注于缩减神经网络尺寸,而不是能耗的优化<citation id="97" type="reference"><link href="7" rel="bibliography" /><sup>[<a class="sup">3</a>]</sup></citation>.近似计算技术利用人类感知能力的限制或应用本身对含噪音的输入数据的固有容错能力,降低对所有计算都要达到100%精度的要求,产生可以接受的近似结果<citation id="98" type="reference"><link href="9" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>.在神经网络运行过程中引入近似计算技术可以优化其对能源的消耗,而不会改变神经网络的尺寸.然而,近似计算技术可能会引起电路可靠性降低,很可能严重危害到神经网络的精度和性能.所以,我们需要在压缩神经网络尺寸、降低能耗的同时尽量维持原有的输出精度和性能的有效方法.</p>
                </div>
                <div class="p1">
                    <p id="28">因为神经网络有天然的容错特性,裁剪和近似技术可能不会造成输出精度快速下降.更重要的,神经网络中不同神经元的容错能力也是不同的,容错能力越高,神经元可以容忍的偏差就越大.因此,为了解决上述问题,本文结合神经元的容错特性,提出将神经元裁剪和近似计算技术相结合.</p>
                </div>
                <h3 id="29" name="29" class="anchor-tag">2 <b>相关研究</b></h3>
                <div class="p1">
                    <p id="30">随着近似计算的发展,人工神经网络的近似方法遍及软件级和电路级.</p>
                </div>
                <div class="p1">
                    <p id="31">(1)软件级.神经网络涉及大量的计算和访存操作,文献<citation id="99" type="reference">[<a class="sup">4</a>]</citation>使用精度扩缩技术减少输入操作数和网络权重的位宽,在引入轻微的输出结果偏差的情况下有效的提高了性能并节省了能耗.文献<citation id="100" type="reference">[<a class="sup">5</a>]</citation>提出使用神经网络二值化训练方法,并用简单的加法操作替代复杂的乘累加操作.文献<citation id="101" type="reference">[<a class="sup">6</a>]</citation>利用权值共享技术,将权重值分类,数值相近的权重连接共用同一个值,以此达到缩减参数访存的目的.</p>
                </div>
                <div class="p1">
                    <p id="32">(2)电路级.文献<citation id="102" type="reference">[<a class="sup">7</a>]</citation>等工作通过简化电路设计,来降低网络乘累加操作带来的能耗、性能开销.与此同时,研究人员发现通过调节处理器的供电电压,能够有效解决性能和能耗之间的平衡问题.动态电压调节(DVS)<citation id="103" type="reference"><link href="17" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>和动态电压频率调节(DVFS)<citation id="104" type="reference"><link href="19" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>是近年来流行的电压调节技术.然而,随着供电电压值逐渐降低到阈值电压附近,电路可靠性问题越来越严重.如图1所示,当部分运算单元(ALU)运行在足够低的电压下时,硬件运行时出现了错误,导致计算结果不精确.尽管如此,很多研究利用神经网络天然的容错能力,在降低电压的同时并没有极大地降低神经网络的输出质量.</p>
                </div>
                <div class="area_img" id="33">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/WXYJ201911014_033.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sRzVOZlY0eG5jQ2MrRjhjT2RFVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 CPU结构图" src="Detail/GetImg?filename=images/WXYJ201911014_033.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sRzVOZlY0eG5jQ2MrRjhjT2RFVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图</b>1 CPU<b>结构图</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/WXYJ201911014_033.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sRzVOZlY0eG5jQ2MrRjhjT2RFVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h3 id="34" name="34" class="anchor-tag">3 <b>神经元容错度量化方法</b></h3>
                <div class="p1">
                    <p id="35">神经网络中不同神经元对错误的容忍能力是不同的,容错能力越大,说明错误发生在该神经元上时对节点计算结果的影响越小,反之亦然.为了量化神经元的容错能力,我们利用反向传播算法,以神经元输出值与代价函数的偏导值作为容错度的量化指标.值得注意的,从偏导值的计算公式可以看出,神经元容错能力与输入有关,如果更换一次输入就要重新计算每个神经元的容错能力将极大的浪费资源开销.所以提出基于神经元容错度的统计排序方法,修正神经元的容错度值,使得神经元的容错能力和输入无关,而和网络的本身结构相关.</p>
                </div>
                <h4 class="anchor-tag" id="36" name="36">3.1 <b>神经元容错度计算</b></h4>
                <div class="p1">
                    <p id="37">人工神经网络是由神经元按照一定结构相互连接而成的大规模复杂系统,具有很强的容错和自学能力.对于单一的神经元而言,节点输出为所有输入与权重的乘积和,为引入非线性因素,神经元输出需要通过激活函数.反向传播算法被广泛的应用于神经网络的学习过程,包含信号的前向传播和误差的反向传播<citation id="105" type="reference"><link href="21" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>.一个简单的神经网络自学传播关系如图2所示,其中<i>y</i><sub><i>ij</i></sub>表示第<i>i</i>层的第<i>j</i>个输入或输出,<i>w</i><sub><i>i</i></sub>表示第<i>i</i>个输入所连接的到一下层的权重值,<i>b</i><sub><i>i</i></sub>表示第<i>i</i>层相应神经元的偏置值,<i>t</i>表示目标输出,<i>E</i>表示代价函数.反向传播算法计算输出误差如何随输出值、输入值和权重而变化,从而将误差平均分配到网络的各层中,所以可以用来量化每个神经元对网络输出的错误贡献度.我们将独立神经元微小变化对神经网络输出质量的影响定义为容错度,用<i>δ</i>表示.根据反向传播算法,我们用神经元输出值和代价函数的偏导数作为容错度的评估指标<citation id="106" type="reference"><link href="9" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>.人工神经网络的代价函数种类丰富,本文以均方误差代价函数为例:</p>
                </div>
                <div class="area_img" id="38">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/WXYJ201911014_038.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sRzVOZlY0eG5jQ2MrRjhjT2RFVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 神经网络自学传播关系" src="Detail/GetImg?filename=images/WXYJ201911014_038.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sRzVOZlY0eG5jQ2MrRjhjT2RFVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图</b>2 <b>神经网络自学传播关系</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/WXYJ201911014_038.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sRzVOZlY0eG5jQ2MrRjhjT2RFVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="39" class="code-formula">
                        <mathml id="39"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtext>E</mtext><mo>=</mo><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo stretchy="false">(</mo><mtext>t</mtext><mo>-</mo><mtext>y</mtext><msub><mrow></mrow><mrow><mtext>o</mtext><mtext>o</mtext></mrow></msub><mo stretchy="false">)</mo><msup><mrow></mrow><mn>2</mn></msup></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="40">输出层神经元容错度δ<sub>oi</sub>:</p>
                </div>
                <div class="p1">
                    <p id="41" class="code-formula">
                        <mathml id="41"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mtext>δ</mtext><msub><mrow></mrow><mrow><mtext>o</mtext><mtext>i</mtext></mrow></msub><mo>=</mo><mfrac><mrow><mo>∂</mo><mtext>E</mtext></mrow><mrow><mo>∂</mo><mtext>y</mtext><msub><mrow></mrow><mrow><mtext>o</mtext><mtext>o</mtext></mrow></msub></mrow></mfrac><mo>⋅</mo><mfrac><mrow><mo>∂</mo><mtext>y</mtext><msub><mrow></mrow><mrow><mtext>o</mtext><mtext>o</mtext></mrow></msub></mrow><mrow><mo>∂</mo><mtext>y</mtext><msub><mrow></mrow><mrow><mtext>o</mtext><mtext>i</mtext></mrow></msub></mrow></mfrac><mo>=</mo><mo>-</mo><mo stretchy="false">(</mo><mtext>t</mtext><mo>-</mo><mtext>y</mtext><msub><mrow></mrow><mrow><mtext>o</mtext><mtext>o</mtext></mrow></msub><mo stretchy="false">)</mo><mo>⋅</mo><mrow><mtext>f</mtext><mo stretchy="false">(</mo><msup><mtext>y</mtext><mo>′</mo></msup><msub><mrow></mrow><mrow><mtext>o</mtext><mtext>i</mtext></mrow></msub><mo stretchy="false">)</mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="42">隐藏层神经元容错度δ<sub>hi</sub>:</p>
                </div>
                <div class="p1">
                    <p id="43" class="code-formula">
                        <mathml id="43"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mtext>δ</mtext><msub><mrow></mrow><mrow><mtext>h</mtext><mtext>i</mtext></mrow></msub><mo>=</mo><mfrac><mrow><mo>∂</mo><mtext>E</mtext></mrow><mrow><mo>∂</mo><mtext>y</mtext><msub><mrow></mrow><mrow><mtext>o</mtext><mtext>o</mtext></mrow></msub></mrow></mfrac><mo>⋅</mo><mfrac><mrow><mo>∂</mo><mtext>y</mtext><msub><mrow></mrow><mrow><mtext>o</mtext><mtext>o</mtext></mrow></msub></mrow><mrow><mo>∂</mo><mtext>y</mtext><msub><mrow></mrow><mrow><mtext>o</mtext><mtext>i</mtext></mrow></msub></mrow></mfrac><mo>⋅</mo><mfrac><mrow><mo>∂</mo><mtext>y</mtext><msub><mrow></mrow><mrow><mtext>o</mtext><mtext>i</mtext></mrow></msub></mrow><mrow><mo>∂</mo><msup><mtext>y</mtext><mo>′</mo></msup><msub><mrow></mrow><mrow><mtext>h</mtext><mtext>o</mtext></mrow></msub></mrow></mfrac><mo>⋅</mo><mfrac><mrow><mo>∂</mo><mtext>y</mtext><msub><mrow></mrow><mrow><mtext>h</mtext><mtext>o</mtext></mrow></msub></mrow><mrow><mo>∂</mo><mtext>y</mtext><msub><mrow></mrow><mrow><mtext>h</mtext><mtext>i</mtext></mrow></msub></mrow></mfrac></mtd></mtr><mtr><mtd><mo>=</mo><mtext>δ</mtext><msub><mrow></mrow><mrow><mtext>o</mtext><mtext>i</mtext></mrow></msub><mo>⋅</mo><mtext>w</mtext><msub><mrow></mrow><mtext>o</mtext></msub><mo>⋅</mo><mrow><mtext>f</mtext><mo stretchy="false">(</mo><msup><mtext>y</mtext><mo>′</mo></msup><msub><mrow></mrow><mrow><mtext>h</mtext><mtext>i</mtext></mrow></msub><mo stretchy="false">)</mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>3</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="44">输入层神经元容错度δ<sub>xi</sub>:</p>
                </div>
                <div class="p1">
                    <p id="45" class="code-formula">
                        <mathml id="45"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mtext>δ</mtext><msub><mrow></mrow><mrow><mtext>x</mtext><mtext>i</mtext></mrow></msub><mo>=</mo><mfrac><mrow><mo>∂</mo><mtext>E</mtext></mrow><mrow><mo>∂</mo><mtext>y</mtext><msub><mrow></mrow><mrow><mtext>o</mtext><mtext>o</mtext></mrow></msub></mrow></mfrac><mo>⋅</mo><mfrac><mrow><mo>∂</mo><mtext>y</mtext><msub><mrow></mrow><mrow><mtext>o</mtext><mtext>o</mtext></mrow></msub></mrow><mrow><mo>∂</mo><mtext>y</mtext><msub><mrow></mrow><mrow><mtext>o</mtext><mtext>i</mtext></mrow></msub></mrow></mfrac><mo>⋅</mo><mfrac><mrow><mo>∂</mo><mtext>y</mtext><msub><mrow></mrow><mrow><mtext>o</mtext><mtext>i</mtext></mrow></msub></mrow><mrow><mo>∂</mo><msup><mtext>y</mtext><mo>′</mo></msup><msub><mrow></mrow><mrow><mtext>h</mtext><mtext>o</mtext></mrow></msub></mrow></mfrac><mo>⋅</mo><mfrac><mrow><mo>∂</mo><mtext>y</mtext><msub><mrow></mrow><mrow><mtext>h</mtext><mtext>o</mtext></mrow></msub></mrow><mrow><mo>∂</mo><mtext>y</mtext><msub><mrow></mrow><mrow><mtext>h</mtext><mtext>i</mtext></mrow></msub></mrow></mfrac><mo>⋅</mo><mfrac><mrow><mo>∂</mo><mtext>y</mtext><msub><mrow></mrow><mrow><mtext>h</mtext><mtext>i</mtext></mrow></msub></mrow><mrow><mo>∂</mo><mtext>y</mtext><msub><mrow></mrow><mrow><mtext>x</mtext><mtext>i</mtext></mrow></msub></mrow></mfrac></mtd></mtr><mtr><mtd><mo>=</mo><mtext>δ</mtext><msub><mrow></mrow><mrow><mtext>h</mtext><mtext>i</mtext></mrow></msub><mo>⋅</mo><mtext>w</mtext><msub><mrow></mrow><mtext>h</mtext></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>4</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="46">如公式(2)～(4)所示,在训练阶段固定好所有的权重值w和偏置值b后,通过计算神经网络中每个神经元的容错度δ<sub>i</sub>,并按照大小进行排序,即可得到神经元的容错度排序.</p>
                </div>
                <h4 class="anchor-tag" id="47" name="47">3.2 <b>神经元容错度统计排序方法</b></h4>
                <div class="p1">
                    <p id="48">实验发现,以隐藏层为例,将公式(3)中的y<sub>hi</sub>继续展开得:y<sub>hi</sub>=w<sub>h</sub>×x<sub>i</sub>,其中x<sub>i</sub>表示输入.由此可以看出,神经元的输出值与网络的输入参数有关,基于不同输入所得到的排序结果会有差异,为了进一步提高排序结果的准确度,本文提出多次更换数据进行排序,然后基于统计算法选出最终排序结果.</p>
                </div>
                <div class="p1">
                    <p id="49">ALGORITHM 1:神经元容错度排序统计算法</p>
                </div>
                <div class="p1">
                    <p id="50">输入:完成预训练的神经网络NN,网络神经元数:NeuronNum测试数据集:TestData,统计次数epoch</p>
                </div>
                <div class="p1">
                    <p id="51">输出:排序结果</p>
                </div>
                <div class="p1">
                    <p id="52">1:begin</p>
                </div>
                <div class="p1">
                    <p id="53">2:  initialize: model = NN</p>
                </div>
                <div class="p1">
                    <p id="54">3:  While iteration &lt; epoch do</p>
                </div>
                <div class="p1">
                    <p id="55">4:  random select data from TestData</p>
                </div>
                <div class="p1">
                    <p id="56">5:  <i>δ</i> ← backpropagation (model, data)</p>
                </div>
                <div class="p1">
                    <p id="57">6:  <i>δ</i><sub>sum</sub>← sort (<i>δ</i>)</p>
                </div>
                <div class="p1">
                    <p id="58">7:  end while</p>
                </div>
                <div class="p1">
                    <p id="59">8:  for <i>k</i>&lt;NeuronNumdo</p>
                </div>
                <div class="p1">
                    <p id="60">9:  for <i>j</i>&lt; epoch</p>
                </div>
                <div class="p1">
                    <p id="61">10:  neuron = max (statistic number of each neuronin <i>δ</i><sub>sum</sub>[<i>j</i>][<i>k</i>] and neuron not in sort)</p>
                </div>
                <div class="p1">
                    <p id="63">11:  sort[<i>k</i>] = neuron</p>
                </div>
                <div class="p1">
                    <p id="64">12:  return sort</p>
                </div>
                <div class="p1">
                    <p id="65">13:end</p>
                </div>
                <div class="p1">
                    <p id="66">算法1描述了我们提出的统计算法.3～7行得到epoch次从小到大的排序结果,其中第4行从测试集中随机选取一组数据后,第5行将数据运行在神经网络模型上并使用反向传播算法得到每个神经元的容错度,第6行将排序后的结果存入<i>δ</i><sub>sum</sub>中.8～12行从小到大统计每个排序位置相应神经元出现的次数,当出现次数最多且没有在之前排序中确定位置的神经元将作为本位置的最终神经元.由此,我们最终确定了隐藏层神经元的容错度排序.</p>
                </div>
                <div class="p1">
                    <p id="67">为了证明排序的有效性,本实验多次更换排序输入并统计了32组隐藏层神经元容错度排序结果.从32组数据中任选一组排序中容错度最高的三个神经元(A、B、C)和容错度最低的三个神经元(D、E、F),并统计这六个神经元在排序结果中的分布情况,统计结果如图3所示,神经元在排序中的位置相对固定在特定范围,因此基于统计算法的神经元容错度排序结果的分布情况符合预期.最后,我们使用所有统计过程中容错度的均值来修正神经元的容错度.</p>
                </div>
                <div class="area_img" id="68">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/WXYJ201911014_068.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sRzVOZlY0eG5jQ2MrRjhjT2RFVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 神经元出现次数的分布" src="Detail/GetImg?filename=images/WXYJ201911014_068.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sRzVOZlY0eG5jQ2MrRjhjT2RFVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图</b>3 <b>神经元出现次数的分布</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/WXYJ201911014_068.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sRzVOZlY0eG5jQ2MrRjhjT2RFVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="69">为了验证工艺偏差对网络输出质量的影响,本文模拟容错能力已知的神经元在不同质量电路上的运行情况,并统计神经网络的输出精度.神经元的容错能力表现为错误发生在该神经元上对网络输出精度的影响,容错能力越高,则错误的影响越小.结果如图4所示,错误发生在低容错能力的神经元上,且随着出错神经元个数的增加,网络的输出精度下降明显,相反,对于高容错能力的神经元而言网络精度的变化范围很小.因此可知,工艺偏差引发的错误发生在容错能力不同的神经元上对网络质量的影响不同.</p>
                </div>
                <div class="area_img" id="70">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/WXYJ201911014_070.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sRzVOZlY0eG5jQ2MrRjhjT2RFVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 相同错误下神经网络输出精度" src="Detail/GetImg?filename=images/WXYJ201911014_070.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sRzVOZlY0eG5jQ2MrRjhjT2RFVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图</b>4 <b>相同错误下神经网络输出精度</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/WXYJ201911014_070.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sRzVOZlY0eG5jQ2MrRjhjT2RFVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h3 id="71" name="71" class="anchor-tag">4 <b>神经元重要度分析及精度优化方法</b></h3>
                <div class="p1">
                    <p id="72">传统的权重值裁剪方法不但会增加额外的数据记录稀疏矩阵的形式,并且会增加性能下降的风险<citation id="107" type="reference"><link href="3" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>.为了降低网络质量和性能的损失,尽可能压缩神经网络规模,本文采取节点裁剪方法,即通过裁剪网络中冗余的神经元达到降低神经网络资源需求的目的.</p>
                </div>
                <div class="p1">
                    <p id="73">神经元裁剪方法的关键问题是如何在删除冗余神经元的同时满足精度需求.裁剪的第一步是判断哪些神经元可以裁剪.本文提出基于反向传播算法的神经元重要程度排序方法,用<i>ε</i>表示裁剪度,裁剪度越高说明相应神经元裁剪后对输出质量影响越低,反之亦然.对于训练好的神经网络而言,采用梯度下降算法进行最优化求解时,网络参数收敛于全局极值点或局部极值点,而此时梯度的幅值也接近0.即神经网络参数的梯度值越小,则该参数越接近极值点,该值越接近理想值.以网络权重值更新为例:函数表示为<i>y</i>=<i>w</i>×<i>x</i>+<i>b</i>,<i>w</i>的梯度为∇=<i>x</i>;向梯度相反的方向移动<i>x</i>,如下:<i>x</i>′←<i>x</i>-∇·<i>η</i>,其中<i>η</i>为神经网络学习率;循环迭代上一步骤直到神经网络输入值<i>x</i>的变化使得<i>y</i>在两次迭代之间的差值足够小(接近0),则说明<i>y</i>接近局部最小值,<i>w</i>接近理想值.同理可推,神经元输出<i>y</i><sub><i>i</i></sub>与<i>E</i>的偏导值越小,则神经元输出<i>y</i><sub><i>i</i></sub>的值越接近理想值.又因为<i>y</i>=<i>w</i>×<i>x</i>+<i>b</i>,则与<i>y</i><sub><i>i</i></sub>相连的上一层神经元的权重值与输入值的乘积越接近理想值,则可进一步认为与该神经元相连的上一层权重值越接近理想值.节点裁剪方法中,当一个神经元被裁减,则与之相连接的所有权重连接都会消失.如果裁剪接近理想值的神经元,则势必会裁剪掉接近理想值的权重矩阵,造成网络质量的下降.所以针对节点裁剪方法,需要裁剪远离理想值的神经元.分析可知,神经元的偏导数越大,裁剪度<i>ε</i>越高,神经元被裁剪后对输出精度的影响越小.</p>
                </div>
                <div class="p1">
                    <p id="74">探索大型神经网路中哪些神经元冗余可以被裁剪的工作量非常巨大,且尽管重训练可以弥补裁剪引发的精度损失,但是输出层神经元之后没有权重值,因此训练算法更难降低这些神经元的误差,所以为了降低探索过程的工作量,提高网络裁剪的精度,本文将裁剪工作集中于神经网络的隐藏层.隐藏层<i>ε</i>的计算表示如下:</p>
                </div>
                <div class="p1">
                    <p id="75" class="code-formula">
                        <mathml id="75"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>ε</mi><msub><mrow></mrow><mi>i</mi></msub><mo>=</mo><mfrac><mrow><mo>∂</mo><mi>E</mi></mrow><mrow><mo>∂</mo><mi>y</mi><msub><mrow></mrow><mrow><mi>h</mi><mi>i</mi></mrow></msub></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>5</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="76"><i>ε</i><sub><i>i</i></sub>值越大,则说明神经元对网络的重要程度越小,可裁剪程度越高,相反,则不可裁剪.</p>
                </div>
                <div class="p1">
                    <p id="77">基于理论分析,我们得到神经元重要程度排序方法,与此同时为了最大化裁剪率使神经网络尺寸满足需求,我们引入重训练方法.重训练过程有误差修复能力,基于此,我们提出循环裁剪法,来探寻最优裁剪率.如图5所示,方法步骤如下:(1)通过计算隐藏层每个神经元<i>ε</i><sub><i>i</i></sub>值,得到网络的裁剪度排序;(2)根据裁剪度,移除最不重要的神经元;(3)有限次轻量级的重训练网络,补偿裁剪引起的输出质量的损失;(4)根据重训练的输出结果的精确度,调节裁剪率;(5)重复步骤(2)至(4)直到重训练过程不能将网络的质量恢复到阈值水平,则停止裁剪工作,输出网络低于阈值的前一次网络裁剪率.</p>
                </div>
                <div class="area_img" id="78">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/WXYJ201911014_078.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sRzVOZlY0eG5jQ2MrRjhjT2RFVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图5 循环裁剪流程图" src="Detail/GetImg?filename=images/WXYJ201911014_078.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sRzVOZlY0eG5jQ2MrRjhjT2RFVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图</b>5 <b>循环裁剪流程图</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/WXYJ201911014_078.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sRzVOZlY0eG5jQ2MrRjhjT2RFVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h3 id="79" name="79" class="anchor-tag">5 <b>实验结果</b></h3>
                <div class="p1">
                    <p id="80">本节使用MNIST、Fashion-MNIST、SVHN和CIFAR-10四个数据集作为神经网络输入.MNIST手写数字数据集包含60 000张训练数据和10 000张测试数据,每张图片的像素大小为28*28.Fashion-MNIST数据集有10个类别共70 000张图片,其中60 000张训练图片,10 000张测试图片,每张图片的像素大小为28*28.不同于MNIST的是,Fashion-MNIST不再是抽象的数字符号,而是更加具象化的人类必需品——服装.SVHN是对图像中阿拉伯数字进行识别的数据集,其中73 257张训练数字图片,26 032张测试数字图片.</p>
                </div>
                <div class="p1">
                    <p id="81">CIFAR-10数据集有10个类别共60 000张彩色图片,其中每类包含6 000张像素大小为32*32的RGB图片,50 000张图片用于训练,10 000张图片用于测试.我们使用PyTorch作为神经网络软件级的运行环境.Sniper<citation id="108" type="reference"><link href="23" rel="bibliography" /><sup>[<a class="sup">11</a>]</sup></citation>模拟器被用于评估神经网络的运行性能.McPAT工具被用于评估神经网络近似后的能耗开销.</p>
                </div>
                <h4 class="anchor-tag" id="82" name="82">5.1 <b>神经元裁剪</b></h4>
                <div class="p1">
                    <p id="83">MLP是前馈神经网络的一种,由输入层、隐藏层和输出层组成,层与层之间通过全连接实现.MLP隐藏层神经元数设为256→256→256.我们使用已有的LeNet-5结构作为CNN模型框架.MLP与CNN网络详细信息见表1.将MNIST数据集运行在MLP上,所得神经网络精度损失为1.9%.我们分别测试基于ε排序、δ排序、随机排序和加入重训练过程的ε排序的节点裁剪方案.结果如图6(a)所示,基于随机裁剪的神经网络输出精度浮动很大,因此该方法不适合应用到网络裁减中;基于ε排序的节点裁剪精度优于δ排序,证明上述基于δ的反序排序方法是合理的;在精度损失小于1%的情况下,MLP的压缩率达到50%;与此同时,重训练过程可以明显的补偿网路裁剪引起的精度损失问题,为进一步降低网络规模,节约硬件资源提供保障.将CIFAR-10运行在CNN上,神经网络的精度损失为49%.实验结果如图6(b)所示,基于重训练的ε排序裁剪效果最优,在精度损失小于1%的情况下,神经网络的压缩率达到10%;将精度损失提高到5%,神经网络的压缩率达到25%.CNN的效果不如MLP的原因在于,CIFAR-10数据集复杂程度远高于MNIST数据集,所以网络需要更多的参数维持识别精度.</p>
                </div>
                <div class="area_img" id="84">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/WXYJ201911014_084.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sRzVOZlY0eG5jQ2MrRjhjT2RFVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图6 神经元裁剪精度" src="Detail/GetImg?filename=images/WXYJ201911014_084.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sRzVOZlY0eG5jQ2MrRjhjT2RFVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图</b>6 <b>神经元裁剪精度</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/WXYJ201911014_084.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sRzVOZlY0eG5jQ2MrRjhjT2RFVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="area_img" id="85">
                    <p class="img_tit"><b>表</b>1 <b>神经元裁剪实验数据表</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="85" border="1"><tr><td rowspan="2"><br />Net<br />works</td><td colspan="2"><br />Num of layers</td><td rowspan="2">Test <br />Dataset</td><td rowspan="2">Accuracy <br />rate</td></tr><tr><td><br />CONV</td><td>FC</td></tr><tr><td><br />MLP</td><td>0</td><td>5</td><td>MNIST</td><td>98.1%</td></tr><tr><td><br />LeNet-5</td><td>2</td><td>3</td><td>CIFAR-10</td><td>61%</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h4 class="anchor-tag" id="86" name="86">5.2 <b>近似计算</b></h4>
                <div class="p1">
                    <p id="87">本节我们测试近似计算对神经网络输出质量的影响.为了最大限度的节约神经网络能耗开销,本实验选择电压调节的近似方法.为了缓解电压调节技术引起的电路错误增多对神经网络输出质量的影响,我们根据神经元的容错能力,有选择的降低神经元运行时的电压.值得注意的,神经网络中神经元个数非常多,如果每个神经元的电压都需要调节,则需要大量的电压调节器,这不但增加了硬件的面试开销,而且电压调节器的能耗开销也会影响神经网络最终的能耗.</p>
                </div>
                <div class="p1">
                    <p id="88">为了降低电压调节器个数的影响,我们引入电压岛的概念,即相似容错能力的神经元分配同一个电压值,以此达到降低电压调节器的资源开销问题.</p>
                </div>
                <div class="p1">
                    <p id="89">首先我们测试电压岛个数<i>k</i>对MLP的影响.为了量化精度、能耗、性能对神经网络质量的影响,我们定义了网络质量的衡量指标M_NN=accuracy/(power*delay),其中accuracy表示神经网络的输出精度、power表示神经网络运行过程中的能耗、delay表示神经网络的运行时间.M_NN取值越大,则说明网络质量越高.在设定神经网络峰值能耗为40 W的情况下,不同<i>k</i>值下的变化情况如图7所示.结果显示当<i>k</i>&gt;6时,MNIST数据集M_NN的输出质量大幅上升.相比于其他孤岛的网络质量输出,<i>k</i>=16是局部最优点.对于Fashion-MNIAT、SVHN、CIFAR-10而言,尽管存在一些局部最优点,但是随着k值个数增多,M_NN的趋势逐渐升高.所以,为了降低电压调节的开销问题,在后续的实验中,针对MNIST和CIFAR-10数据集而言,<i>k</i>=16是较好的电压孤岛个数的取值;对于SVHN和Fashion-MNIST数据集而言,<i>k</i>=6是较好的电压孤岛个数的取值.</p>
                </div>
                <div class="area_img" id="90">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/WXYJ201911014_090.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sRzVOZlY0eG5jQ2MrRjhjT2RFVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图7 k的个数对神经网络质量的影响" src="Detail/GetImg?filename=images/WXYJ201911014_090.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sRzVOZlY0eG5jQ2MrRjhjT2RFVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图</b>7 k<b>的个数对神经网络质量的影响</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/WXYJ201911014_090.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sRzVOZlY0eG5jQ2MrRjhjT2RFVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="91">然后,我们以处理器的原始电压2 V和近阈值电压0.5 V(粗粒度coarse-grained,方点)的整体网络的粗粒度电压调节作为参考.普通的高性能低功耗的计算节点的峰值功耗是55 W.本实验中,我们将神经网络的峰值功耗分别设为55 W、50 W、45 W和35 W,然后根据神经元的容错能力,等比例的降低神经元的供电电压(细粒度fine-grained,圆点),使得神经网络的功耗满足上述限制.结果如图8所示,在MLP网络中,基于细粒度的电压调节的神经网络输出精度保持较高水平的同时,能耗和网络延迟也优于粗粒度的电压调节.特别地,数据集越简单、容错能力越高则近似后的神经网络质量越好.与2 V的原始供电电压下的神经网络能耗相比,对于MNIST、Fashion-MNIST、SVHN和CIFAR-10数据集而言,最好的细粒度的电压调节分别完成了1.35×,1.39×,1.27×,1.26×的节能效果.此外,当峰值功耗设为45 W时,对于MNIST、Fashion-MNIST、SVHN和CIFAR-10数据集而言,神经网络基于细粒度电压调节的输出精度是粗粒度级输出精度的63×,52×,1.1×和3×.综上,细粒度的电压调节近似方法可以使神经网络的质量保持的更好.</p>
                </div>
                <div class="area_img" id="92">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/WXYJ201911014_092.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sRzVOZlY0eG5jQ2MrRjhjT2RFVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图8 近似后的神经网络质量对比" src="Detail/GetImg?filename=images/WXYJ201911014_092.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sRzVOZlY0eG5jQ2MrRjhjT2RFVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图</b>8 <b>近似后的神经网络质量对比</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/WXYJ201911014_092.jpg&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sRzVOZlY0eG5jQ2MrRjhjT2RFVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h3 id="93" name="93" class="anchor-tag">6 <b>结束语</b></h3>
                <div class="p1">
                    <p id="94">本文根据神经元不同的容错能力,将神经元裁剪和近似计算技术相结合,以寻求最优的网络输出精度同时尽可能低的降低网络的能耗开销.本文主要优点在于(1)提出基于统计排序的神经元容错能力量化方法;(2)根据神经元的容错能力提出神经元重要程度排序算法;(3)在裁剪过程中引入重训练,提出循环裁剪法;(4)在神经网络运行过程中使用近似计算技术进一步降低功耗开销.最后,通过两个实验,证明了该技术的有效性.</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJCM&amp;filename=SJCMFD04707E68164B519899AE976FC04C63&amp;v=MzIyODYyM3c2QT1OaWZJWThYTUh0WExyNGd3WXVNT0NuaEx5aGNhNGpaME9RcnJxeFJEQ3JLUU5yeWNDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZDcGJRMzVkbGl4Yg==&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sRzVOZlY0eG5jQ2MrRjhjT2RFVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> YU J,LUKEFAHR A,PALFRAMAN D,et al.Scalpel:customizing DNN pruning to the underlying hardware parallelism[J].AcmSigarch Computer Architecture News,2017,45(2):548-560.
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=NoiseOut:a simple way to prune neural networks">

                                <b>[2]</b> BABAEIZADEH M,SMARAGDIS P,CAMPBELL R H.NoiseOut:a simple way to prune neural networks[J].arXiv.org &gt; cs &gt; arXiv:1611.06211v1,2016.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Designing energy-efficient convolutional neural networks using energy-aware pruning">

                                <b>[3]</b> YANG T J,CHEN Y H ,SZE V.Designing energy-efficient convolutional neural networks using energy-aware pruning[C]// Computer Vision &amp; Pattern Recognition.Honolulu,HI,USA,IEEE,2017.
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=ApproxANN:an approximate computing framework for artificial neural network">

                                <b>[4]</b> ZHANG Q ,WANG T ,TIAN Y ,et al.ApproxANN:an approximate computing framework for artificial neural network[C]// Design,Automation &amp; Test in Europe Conference &amp; Exhibition.Grenoble,France,EDA Consortium,2015.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=BinaryConnect: training deep neural networks with binary weights during propagations">

                                <b>[5]</b> COURBARIAUX M,BENGIO Y,DAVID J P.BinaryConnect:training deep neural networks with binary weights during propagations[C]// International Conference on Neural Information Processing Systems.Istanbul,Turkey,2015.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Deep Compression:Compressing Deep Neural Networks with Pruning,Trained Quantization and Huffman Coding">

                                <b>[6]</b> HAN S,MAO H,DALLY W J.Deep compression:compressing deep neural networks with pruning,trained quantization and huffman coding[J].Fiber,2015,56(4):3-7.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Multiplier-less artificial neurons exploiting error resiliency for energy-efficient neural computing">

                                <b>[7]</b> SARWAR S S ,VENKATARAMANI S ,RAGHUNATHAN A ,et al.Multiplier-less artificial neurons exploiting error resiliency for energy-efficient neural computing[C]// 2016 Design,Automation &amp; Test in Europe Conference &amp; Exhibition (DATE).Dresden,Germany,IEEE,2016.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=WXYJ200602017&amp;v=MDcxOTBWdkZDbmdXN3pBTWpYU1pMRzRIdGZNclk5RVk0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmU=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sRzVOZlY0eG5jQ2MrRjhjT2RFVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[8]</b> 邢静宇,张立臣.动态电压调整多处理器实时系统任务调度[J].微电子学与计算机,2006,23(2):55-57.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=GPGPU power modeling for multi-domain voltage-frequency scaling">

                                <b>[9]</b> GUERREIRO J,ILIC A,ROMA N,et al.GPGPU power modeling for multi-domain voltage-frequency scaling[C]// IEEE International Symposium on High Performance Computer Architecture.Vienna,Austria,2018.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_10" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=WXYJ201411007&amp;v=MDIxNjFqWFNaTEc0SDlYTnJvOUZZNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnZGQ25nVzd6QU0=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sRzVOZlY0eG5jQ2MrRjhjT2RFVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[10]</b> 朱新召,胡哲琨,周莉,等.基于多核处理器的多层感知神经网络设计和实现[J].微电子学与计算机,2014,31(11):27-31.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Sniper:exploring the level of abstraction for scalable and accurate parallel multi-core simulation">

                                <b>[11]</b> CARLSON T E ,HEIRMAN W ,EECKHOUT L .Sniper:exploring the level of abstraction for scalable and accurate parallel multi-core simulation[C]// High Performance Computing,Networking,Storage &amp; Analysis.Washington,USA,IEEE,2011.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="WXYJ201911014" />
        <input id="dpi" type="hidden" value="800" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sRzVOZlY0eG5jQ2MrRjhjT2RFVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=WXYJ201911014&amp;v=MDE5OTQ5RVlJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdkZDbmdXN3pBTWpYU1pMRzRIOWpOcm8=&amp;uid=WEEvREcwSlJHSldRa1FhdkJkVG5hVC9sRzVOZlY0eG5jQ2MrRjhjT2RFVT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="0" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
