<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637133853751037500%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dWXYJ201908018%26RESULT%3d1%26SIGN%3ddv4Jkf21y1W4dQWssui0Oqs%252fHfY%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=WXYJ201908018&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=WXYJ201908018&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=WXYJ201908018&amp;v=MjIyNTJGeW5uVkwvUE1qWFNaTEc0SDlqTXA0OUViSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnU=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#17" data-title="1 &lt;b&gt;引言&lt;/b&gt; ">1 <b>引言</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#21" data-title="2 &lt;b&gt;基本概念&lt;/b&gt; ">2 <b>基本概念</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#22" data-title="2.1 CNN&lt;b&gt;基本结构&lt;/b&gt;">2.1 CNN<b>基本结构</b></a></li>
                                                <li><a href="#38" data-title="2.2 AlexNet">2.2 AlexNet</a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#42" data-title="3 CNN&lt;b&gt;加速器设计&lt;/b&gt; ">3 CNN<b>加速器设计</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#43" data-title="3.1 &lt;b&gt;整体结构&lt;/b&gt;">3.1 <b>整体结构</b></a></li>
                                                <li><a href="#48" data-title="3.2 PL&lt;b&gt;内部设计&lt;/b&gt;">3.2 PL<b>内部设计</b></a></li>
                                                <li><a href="#51" data-title="3.3 &lt;b&gt;卷积运算单元&lt;/b&gt;">3.3 <b>卷积运算单元</b></a></li>
                                                <li><a href="#54" data-title="3.4 &lt;b&gt;计算精度&lt;/b&gt;">3.4 <b>计算精度</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#56" data-title="4 &lt;b&gt;实验结果及分析&lt;/b&gt; ">4 <b>实验结果及分析</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#57" data-title="4.1 &lt;b&gt;实验方案&lt;/b&gt;">4.1 <b>实验方案</b></a></li>
                                                <li><a href="#60" data-title="4.2 &lt;b&gt;实验结果&lt;/b&gt;">4.2 <b>实验结果</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#67" data-title="5 &lt;b&gt;结束语&lt;/b&gt; ">5 <b>结束语</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#24" data-title="&lt;b&gt;图&lt;/b&gt;1 &lt;b&gt;卷积神经网络结构&lt;/b&gt;"><b>图</b>1 <b>卷积神经网络结构</b></a></li>
                                                <li><a href="#40" data-title="&lt;b&gt;表&lt;/b&gt;1 AlexNet&lt;b&gt;网络参数&lt;/b&gt;"><b>表</b>1 AlexNet<b>网络参数</b></a></li>
                                                <li><a href="#45" data-title="&lt;b&gt;图&lt;/b&gt;2 &lt;b&gt;加速器整体架构&lt;/b&gt;"><b>图</b>2 <b>加速器整体架构</b></a></li>
                                                <li><a href="#50" data-title="&lt;b&gt;图&lt;/b&gt;3 PL&lt;b&gt;内部结构&lt;/b&gt;"><b>图</b>3 PL<b>内部结构</b></a></li>
                                                <li><a href="#53" data-title="&lt;b&gt;图&lt;/b&gt;4 &lt;b&gt;卷积运算单元&lt;/b&gt;"><b>图</b>4 <b>卷积运算单元</b></a></li>
                                                <li><a href="#62" data-title="&lt;b&gt;表&lt;/b&gt;2 &lt;b&gt;资源消耗&lt;/b&gt;"><b>表</b>2 <b>资源消耗</b></a></li>
                                                <li><a href="#64" data-title="&lt;b&gt;表&lt;/b&gt;3 &lt;b&gt;比较结果&lt;/b&gt;"><b>表</b>3 <b>比较结果</b></a></li>
                                                <li><a href="#66" data-title="&lt;b&gt;表&lt;/b&gt;4 &lt;b&gt;性能对比&lt;/b&gt;"><b>表</b>4 <b>性能对比</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="77">


                                    <a id="bibliography_1" title=" 李彦冬, 郝宗波, 雷航.卷积神经网络研究综述[J].计算机应用, 2016, 36 (9) :2508-2515." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201609029&amp;v=MjU1MTdZUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdUZ5bm5WTC9QTHo3QmQ3RzRIOWZNcG85SGI=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[1]</b>
                                         李彦冬, 郝宗波, 雷航.卷积神经网络研究综述[J].计算机应用, 2016, 36 (9) :2508-2515.
                                    </a>
                                </li>
                                <li id="79">


                                    <a id="bibliography_2" title=" LI X, ZHANG G, HUANG H H, et al.Performance analysis of GPU-based convolutional neural networks[C]//International Conference on Parallel Processing.Philadelphia, USA:IEEE, 2016:67-76." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Performance Analysis of GPU-Based Convolutional Neural Networks">
                                        <b>[2]</b>
                                         LI X, ZHANG G, HUANG H H, et al.Performance analysis of GPU-based convolutional neural networks[C]//International Conference on Parallel Processing.Philadelphia, USA:IEEE, 2016:67-76.
                                    </a>
                                </li>
                                <li id="81">


                                    <a id="bibliography_3" title=" ZHANG C, LI P, SUN G, et al.Optimizing FPGA-based Accelerator Design for Deep Convolutional Neural Networks[C]// ACM/SIGDA International Symposium on Field-Programmable Gate Arrays.Monterey, USA:ACM, 2015:161-170." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Optimizing FPGA-based acelerator dsign for deep convolutional neural networks">
                                        <b>[3]</b>
                                         ZHANG C, LI P, SUN G, et al.Optimizing FPGA-based Accelerator Design for Deep Convolutional Neural Networks[C]// ACM/SIGDA International Symposium on Field-Programmable Gate Arrays.Monterey, USA:ACM, 2015:161-170.
                                    </a>
                                </li>
                                <li id="83">


                                    <a id="bibliography_4" title=" QIU J, WANG J, YAO S, et al.Going deeper with embedded FPGA platform for convolutional neural network[C]//Proceedings of the 2016 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays.Monterey, USA:ACM, 2016:26-35." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Going deeper with embedded FPGA platform for convolutional neural network">
                                        <b>[4]</b>
                                         QIU J, WANG J, YAO S, et al.Going deeper with embedded FPGA platform for convolutional neural network[C]//Proceedings of the 2016 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays.Monterey, USA:ACM, 2016:26-35.
                                    </a>
                                </li>
                                <li id="85">


                                    <a id="bibliography_5" title=" ZHANG C, WU D, SUN J, et al.Energy-efficient CNN implementation on a deeply pipelined FPGA cluster[C]//Proceedings of the 2016 International Symposium on Low Power Electronics and Design.San Francisco, USA:ACM, 2016:326-331." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Energy-efficient CNN Implementation on a Deeply Pipelined FPGA Cluster">
                                        <b>[5]</b>
                                         ZHANG C, WU D, SUN J, et al.Energy-efficient CNN implementation on a deeply pipelined FPGA cluster[C]//Proceedings of the 2016 International Symposium on Low Power Electronics and Design.San Francisco, USA:ACM, 2016:326-331.
                                    </a>
                                </li>
                                <li id="87">


                                    <a id="bibliography_6" title=" WANG C , YU Q , GONG L , et al.DLAU:A Scalable Deep Learning Accelerator Unit on FPGA[J].IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 2016, 36 (3) :513-517." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=DLAU:A Scalable Deep Learning Accelerator Unit on FPGA">
                                        <b>[6]</b>
                                         WANG C , YU Q , GONG L , et al.DLAU:A Scalable Deep Learning Accelerator Unit on FPGA[J].IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 2016, 36 (3) :513-517.
                                    </a>
                                </li>
                                <li id="89">


                                    <a id="bibliography_7" title=" LOKHMOTOV A, FURSIN G.Optimizing convolutional neural networks on embedded platforms with OpenCL[C]//Proceedings of the 4th International Workshop on OpenCL.Vienna, Austria:ACM, 2016:10-14." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Optimizing convolutional neural networks on embedded platforms with OpenCL">
                                        <b>[7]</b>
                                         LOKHMOTOV A, FURSIN G.Optimizing convolutional neural networks on embedded platforms with OpenCL[C]//Proceedings of the 4th International Workshop on OpenCL.Vienna, Austria:ACM, 2016:10-14.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=WXYJ" target="_blank">微电子学与计算机</a>
                2019,36(08),83-86             </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>基于FPGA的卷积神经网络加速器设计与实现</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E7%BF%9F%E7%A4%BE%E5%B9%B3&amp;code=39111806&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">翟社平</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E9%82%B1%E7%A8%8B&amp;code=42383130&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">邱程</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%A8%E5%AA%9B%E5%AA%9B&amp;code=40682093&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">杨媛媛</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%9D%8E%E5%A9%A7&amp;code=39566153&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">李婧</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E6%B1%9F%E5%A9%B7%E5%A9%B7&amp;code=42383131&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">江婷婷</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E8%A5%BF%E5%AE%89%E9%82%AE%E7%94%B5%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%AD%A6%E9%99%A2%E9%99%95%E8%A5%BF%E7%9C%81%E7%BD%91%E7%BB%9C%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B8%8E%E6%99%BA%E8%83%BD%E5%A4%84%E7%90%86%E9%87%8D%E7%82%B9%E5%AE%9E%E9%AA%8C%E5%AE%A4&amp;code=1698419&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">西安邮电大学计算机学院陕西省网络数据分析与智能处理重点实验室</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>FPGA已广泛用于卷积神经网络的硬件加速器的实现.本文设计了一种基于FPGA的卷积神经网络加速器.主要利用卷积神经网络中固有的并行性来减少实时嵌入式应用所需带宽与资源使用, 并将其在有限资源的ZC706开发板上实现, 结果显示, 在150 MHz的工作频率下, FPGA的峰值运算速度达到0.54 GOP/s, 且功耗很小.</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">卷积神经网络;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E7%8E%B0%E5%9C%BA%E5%8F%AF%E7%BC%96%E7%A8%8B%E9%97%A8%E9%98%B5%E5%88%97&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">现场可编程门阵列;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8A%A0%E9%80%9F%E5%99%A8&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">加速器;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%9C%89%E9%99%90%E8%B5%84%E6%BA%90&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">有限资源;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    翟社平, 男, (1971-) , 博士, 副教授.研究方向为嵌入式系统、语义Web.;
                                </span>
                                <span>
                                    *邱程, (通讯作者) , 男, (1995-) , 硕士研究生.研究方向为嵌入式系统、深度学习.E-mail:qc_0305@126.com.;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-11-19</p>

                    <p>

                            <b>基金：</b>
                                                        <span>工业和信息化部通信软科学项目 (2018-R-26);</span>
                                <span>陕西省社会科学基金 (2016N008);</span>
                                <span>西安市社会科学规划基金 (17X63);</span>
                                <span>西安邮电大学研究生创新基金项目 (CXJJ2017062);</span>
                    </p>
            </div>
                    <h1><b>Design and implementation of convolution neural network accelerator based on FPGA</b></h1>
                    <h2>
                    <span>ZHAI She-ping</span>
                    <span>QIU Cheng</span>
                    <span>YANG Yuan-yuan</span>
                    <span>LI Jing</span>
                    <span>JIANG Ting-ting</span>
            </h2>
                    <h2>
                    <span>School of Computer Science and Technology, Xi′an University of Posts and Telecommunications, Shaanxi Key Laboratory of Network Data Analysis and Intelligent Processing</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>FPGAs are widely used in the implementation of hardware accelerators for convolutional neural networks. This paper designs a FPGA-based CNN acceleration structure. It mainly utilizes the inherent parallelism in CNN to reduce the bandwidth and resource usage required by real-time embedded applications, and implements it on the ZC706 development board with limited resources. The results show that the FPGA operates at a frequency of 0.54 GOP/s and consumes very little power at 150 MHz.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=convolutional%20neural%20network&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">convolutional neural network;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=field-programmable%20gate%20array&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">field-programmable gate array;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=accelerator&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">accelerator;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=limited%20resources&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">limited resources;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-11-19</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="17" name="17" class="anchor-tag">1 <b>引言</b></h3>
                <div class="p1">
                    <p id="18">卷积神经网络 (Convolutional Neural Network, CNN) 是一种典型的多层神经网络, 随着深度学习技术的不断发展, 其被广泛应用于机器视觉和语音分析领域<citation id="91" type="reference"><link href="77" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>.传统的卷积神经网络是基于CPU执行计算, 这样的计算不但缓慢低效, 而且难以满足实时性的计算要求.因此, 基于GPU的卷积神经网络被广泛应用, 文献<citation id="92" type="reference">[<a class="sup">2</a>]</citation>对基于GPU的CNN开源项目进行比较, 发现GPU普遍存在功耗大、成本高等问题.</p>
                </div>
                <div class="p1">
                    <p id="19">FPGA (Field-programmable Gate Array) 是一种可自定义编程的硬件电路结构, 是一种主流的数字电路设计模式.FPGA提供的并行运算的计算模式契合卷积神经网络的计算特点, 同时, FPGA可重编程的特点也适合于神经网络多变的网络结构.因此, 基于FPGA的CNN设计受到了广泛的关注, 文献<citation id="93" type="reference">[<a class="sup">3</a>]</citation>和<citation id="94" type="reference">[<a class="sup">4</a>]</citation>分别提出一种基于FPGA的深度卷积神经网络加速器设计, 文献<citation id="95" type="reference">[<a class="sup">5</a>]</citation>设计了一款深度流水线FPGA集群实现高效CNN.</p>
                </div>
                <div class="p1">
                    <p id="20">本文首先介绍卷积神经网络CNN, 然后根据CNN的特点, 通过合适的并行处理与流水线结构在FPGA上设计一个CNN系统, 有效的提高其性能, 并与已有的CPU与GPU实现进行比较.</p>
                </div>
                <h3 id="21" name="21" class="anchor-tag">2 <b>基本概念</b></h3>
                <h4 class="anchor-tag" id="22" name="22">2.1 CNN<b>基本结构</b></h4>
                <div class="p1">
                    <p id="23">卷积神经网络是深度学习中经典网络之一, 其由多个卷积层、池化层和全连接层组成, 图1是卷积神经网络的经典结构.卷积神经网络可以将待识别的图像直接作为输入, 经过多个卷积层、池化层和全连接层, 得出识别结果.</p>
                </div>
                <div class="area_img" id="24">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/WXYJ201908018_024.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 卷积神经网络结构" src="Detail/GetImg?filename=images/WXYJ201908018_024.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图</b>1 <b>卷积神经网络结构</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/WXYJ201908018_024.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="25" name="25"> (1) 卷积层模型</h4>
                <div class="p1">
                    <p id="26">卷积层通过输入<i>f</i><mathml id="27"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>j</mi><mrow><mtext>i</mtext><mtext>n</mtext></mrow></msubsup></mrow></math></mathml>与由权重<i>w</i><sub><i>i</i>, <i>j</i></sub>组成的卷积核进行卷积操作, 结果经偏置后获得输出, 输出<i>f</i><mathml id="28"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>i</mi><mrow><mtext>o</mtext><mtext>u</mtext><mtext>t</mtext></mrow></msubsup></mrow></math></mathml>即卷积的局部采样所得特征集合.卷积层的模型描述为</p>
                </div>
                <div class="p1">
                    <p id="29" class="code-formula">
                        <mathml id="29"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>f</mi><msubsup><mrow></mrow><mi>i</mi><mrow><mtext>o</mtext><mtext>u</mtext><mtext>t</mtext></mrow></msubsup><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>n</mi><msub><mrow></mrow><mrow><mtext>i</mtext><mtext>n</mtext></mrow></msub></mrow></munderover><mi>f</mi></mstyle><msubsup><mrow></mrow><mi>j</mi><mrow><mtext>i</mtext><mtext>n</mtext></mrow></msubsup><mo>*</mo><mi>w</mi><msub><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mi>j</mi></mrow></msub><mo>+</mo><mi>b</mi><msub><mrow></mrow><mi>i</mi></msub><mo>, </mo><mtext> </mtext><mtext> </mtext><mn>1</mn><mo>≤</mo><mi>i</mi><mo>≤</mo><mi>n</mi><msub><mrow></mrow><mrow><mtext>o</mtext><mtext>u</mtext><mtext>t</mtext></mrow></msub><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>1</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <h4 class="anchor-tag" id="30" name="30"> (2) 池化层模型</h4>
                <div class="p1">
                    <p id="31">池化层通常采用最大值采样或均值采样进行池化操作来降低输入矩阵的规模, 其操作如式 (2) 所示.池化操作可有效降低下一层的数据处理量, 同时避免特征信息丢失.</p>
                </div>
                <div class="p1">
                    <p id="32" class="code-formula">
                        <mathml id="32"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>f</mi><msubsup><mrow></mrow><mrow><mi>i</mi><mo>, </mo><mi>j</mi></mrow><mrow><mtext>o</mtext><mtext>u</mtext><mtext>t</mtext></mrow></msubsup><mo>=</mo><munder><mstyle mathsize="140%" displaystyle="true"><mrow><mi>max</mi></mrow></mstyle><mrow><mi>p</mi><mo>×</mo><mi>p</mi></mrow></munder><mo stretchy="false"> (</mo><mi>f</mi><msubsup><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mi>n</mi><msup><mrow></mrow><mo>'</mo></msup></mrow><mrow><mtext>i</mtext><mtext>n</mtext></mrow></msubsup><mo>, </mo><mi>f</mi><msubsup><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mrow><mtext>i</mtext><mtext>n</mtext></mrow></msubsup><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>f</mi><msubsup><mrow></mrow><mrow><mi>m</mi><mo>, </mo><mi>n</mi><mo>+</mo><mi>p</mi><mo>-</mo><mn>1</mn></mrow><mrow><mtext>i</mtext><mtext>n</mtext></mrow></msubsup><mo>, </mo><mi>f</mi><msubsup><mrow></mrow><mrow><mi>m</mi><mo>+</mo><mn>1</mn><mo>, </mo><mi>n</mi></mrow><mrow><mtext>i</mtext><mtext>n</mtext></mrow></msubsup><mo>, </mo></mtd></mtr><mtr><mtd><mi>f</mi><msubsup><mrow></mrow><mrow><mi>m</mi><mo>+</mo><mn>1</mn><mo>, </mo><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mrow><mtext>i</mtext><mtext>n</mtext></mrow></msubsup><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>f</mi><msubsup><mrow></mrow><mrow><mi>m</mi><mo>+</mo><mn>1</mn><mo>, </mo><mi>n</mi><mo>+</mo><mi>p</mi><mo>-</mo><mn>1</mn></mrow><mrow><mtext>i</mtext><mtext>n</mtext></mrow></msubsup><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>f</mi><msubsup><mrow></mrow><mrow><mi>m</mi><mo>+</mo><mi>p</mi><mo>-</mo><mn>1</mn><mo>, </mo><mi>n</mi></mrow><mrow><mtext>i</mtext><mtext>n</mtext></mrow></msubsup><mo>, </mo></mtd></mtr><mtr><mtd><mi>f</mi><msubsup><mrow></mrow><mrow><mi>m</mi><mo>+</mo><mi>p</mi><mo>-</mo><mn>1</mn><mo>, </mo><mi>n</mi><mo>+</mo><mn>1</mn></mrow><mrow><mtext>i</mtext><mtext>n</mtext></mrow></msubsup><mo>, </mo><mo>⋯</mo><mo>, </mo><mi>f</mi><msubsup><mrow></mrow><mrow><mi>m</mi><mo>+</mo><mi>p</mi><mo>-</mo><mn>1</mn><mo>, </mo><mi>n</mi><mo>+</mo><mi>p</mi><mo>-</mo><mn>1</mn></mrow><mrow><mtext>i</mtext><mtext>n</mtext></mrow></msubsup><mo stretchy="false">) </mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>2</mn><mo stretchy="false">) </mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <h4 class="anchor-tag" id="33" name="33"> (3) 全连接层</h4>
                <div class="p1">
                    <p id="34">全连接层对输入进行线性空间转换, 从而得到输出</p>
                </div>
                <div class="p1">
                    <p id="35" class="code-formula">
                        <mathml id="35"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>f</mi><msup><mrow></mrow><mrow><mtext>o</mtext><mtext>u</mtext><mtext>t</mtext></mrow></msup><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>f</mi></mstyle><msubsup><mrow></mrow><mi>j</mi><mrow><mtext>i</mtext><mtext>n</mtext></mrow></msubsup><mi>w</mi><mo>+</mo><mi>b</mi><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false"> (</mo><mn>3</mn><mo stretchy="false">) </mo></mrow></math></mathml>
                    </p>
                </div>
                <h4 class="anchor-tag" id="36" name="36"> (4) 激活函数</h4>
                <div class="p1">
                    <p id="37">激活函数实现对输入激励的非线性转换, 通常在每一层后对输出结果进行处理.常用的激活函数包括冲击响应 (Sigmod) 、非线性 (Relu) 、三角函数 (Tanh) 等.</p>
                </div>
                <h4 class="anchor-tag" id="38" name="38">2.2 AlexNet</h4>
                <div class="p1">
                    <p id="39">AlexNet是一种典型的卷积神经网络, 该模型共有8层, 由5个卷积层和3个全连接层组成.在每个卷积层中都含有激活函数Relu和局部相应归一化 (LRN) 处理, 在卷积层C1、C2、C5中还有池化处理, 具体网络参数见表1.</p>
                </div>
                <div class="area_img" id="40">
                    <p class="img_tit"><b>表</b>1 AlexNet<b>网络参数</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="40" border="1"><tr><td>层</td><td>输入</td><td>卷积核</td><td>数量</td><td>输出</td></tr><tr><td><br />C1</td><td>227×227×3</td><td>11×11</td><td>96</td><td>27×27×96</td></tr><tr><td><br />C2</td><td>27×27×96</td><td>5×5</td><td>256</td><td>13×13×256</td></tr><tr><td><br />C3</td><td>13×13×256</td><td>3×3</td><td>384</td><td>13×13×384</td></tr><tr><td><br />C4</td><td>13×13×384</td><td>3×3</td><td>384</td><td>13×13×384</td></tr><tr><td><br />C5</td><td>13×13×384</td><td>3×3</td><td>256</td><td>6×6×256</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="41">在AlexNet当中, 卷积层的运算量占了整个网络90%以上, 需要不断的取数据, 保存处理结果, 因此, 可利用CNN的并行性来提高执行速度.</p>
                </div>
                <h3 id="42" name="42" class="anchor-tag">3 CNN<b>加速器设计</b></h3>
                <h4 class="anchor-tag" id="43" name="43">3.1 <b>整体结构</b></h4>
                <div class="p1">
                    <p id="44">基于FPGA的卷积神经网络通常需要消耗较多内存, 但由于FPGA内资源有限, 需要不断从外部读取数据, 这大大影响网络效率.因此, 加速的目标是减少对片外存储器访问次数, 使输入图像和卷积核权值仅加载一次并存储在片上存储器中, 直到执行完它所执行的所有操作.本文提出如图2所示的架构, 主控制器为PS部分的ARM处理器, 它通过高速AXI总线与PL部分的控制器相连接, 控制器连接到数据存储器, 将输入图像送到每个卷积模块, 每个卷积模块具有用来存储权值的系数存储器.卷积模块使用来自数据存储器的输入图像和来自系数存储器的权值来进行卷积运算得到输出结果.</p>
                </div>
                <div class="area_img" id="45">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/WXYJ201908018_045.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 加速器整体架构" src="Detail/GetImg?filename=images/WXYJ201908018_045.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图</b>2 <b>加速器整体架构</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/WXYJ201908018_045.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="46">由于该结构适用于不同类型的卷积操作, 每个卷积操作具有不同的参数, 包括输入图像的大小, 卷积核大小和数量, 窗口移动的步长等, 因此PL部分的控制器需要对运行时的不同卷积核、权值大小和数量进行配置.这些参数通过设置ARM处理器提供的某些寄存器发送到控制器, 并由PL部分控制器计算必要的地址.</p>
                </div>
                <div class="p1">
                    <p id="47">片外存储器是1G的DDR SDRAM存储器, PL通过DDR存储器控制器提供的四个AXI高性能端口对其进行访问.每个端口都有一个AXI CDMA, 用于在片外和片上存储器之间传输数据时降低ARM处理器负载.</p>
                </div>
                <h4 class="anchor-tag" id="48" name="48">3.2 PL<b>内部设计</b></h4>
                <div class="p1">
                    <p id="49">在PL部分的卷积层设计中, 数据和系数存储器的宽度均为32位, 每行数据存储器用于存储一个数据值, 系数存储器每行用于存储两个16位权值系数.由于片上存储器是真正的双口Block RAM, 两行存储器每次都可以访问, 因此在每个系数存储器中存储4个卷积核, 前半部分和后半部分各存储2个, 在每次运算中, 卷积模块可以为4个不同的特征映射生成4个输出.如图3所示, 数据存储器的一个端口在AXI CDMA和控制器之间被多路复用, 而且每个系数存储器的一个端口在AXI CDMA和相应的MAC模块之间被多路复用.一个数据存储器端口用于将输入数据送至由控制器提供地址的系数存储器.当卷积运算执行完成将结果送至其第二个端口并存储在数据存储器中.由于所有的卷积模块都连接到这个单一端口, 因此它们被一个由控制器控制的多路选择器复用.输入数据的发送和每个输出数据的结果存储是同时完成, 该循环不会被中断.</p>
                </div>
                <div class="area_img" id="50">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/WXYJ201908018_050.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 PL内部结构" src="Detail/GetImg?filename=images/WXYJ201908018_050.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图</b>3 PL<b>内部结构</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/WXYJ201908018_050.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="51" name="51">3.3 <b>卷积运算单元</b></h4>
                <div class="p1">
                    <p id="52">在硬件设计中, 可实现不同卷积核或不同卷积层之间卷积运算的并行, 在运算中, 数据会有高度的复用性以及不相关性:输入特征图数据对应多组卷积核, 并且运算相互无依赖;输入特征图数据中的卷积子区域共享同一个卷积核;输入特征图像数据中的不同卷积子区域数据在空间上有大量重叠.因此, 要提高运算性能, 就要增加数据的利用率和卷积运算并行性.本文设计一种流水线并行乘加结构, 在一个时钟周期内可完成一个卷积核内的全部的乘加操作.图4是卷积核大小为3×3的卷积运算单元, 它能够在每个时钟周期内完成9次乘法运算, 再经过加法器输出最终的输出特征像素.寄存器REG中存放从内部存储器取到的输入图像信息, W中存放从系数存储器取来的卷积核权值.当每个卷积运算需要新的卷积核时, W内的信息才进行修改.对于需要进行填充操作的卷积神经网络而言, 输入特征图的边缘位置需要填充0.因此在卷积器中添加了屏蔽器, 将输出的边缘位置置0.该设计促使流水线不会因为数据填充或特征图的切换而中断, 从电路层面保证了卷积计算的并行性.</p>
                </div>
                <div class="area_img" id="53">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/WXYJ201908018_053.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 卷积运算单元" src="Detail/GetImg?filename=images/WXYJ201908018_053.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图</b>4 <b>卷积运算单元</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/WXYJ201908018_053.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h4 class="anchor-tag" id="54" name="54">3.4 <b>计算精度</b></h4>
                <div class="p1">
                    <p id="55">卷积神经网算法通常使用浮点运算, 但在FPGA上实现浮点化运算将消耗大量的资源, 因此需要选择合适的定点数.通过多次实验对比发现, 输入参数采用16位的定点数, 权值数据采用18位的定点数, 可获得较接近浮点数的结果.</p>
                </div>
                <h3 id="56" name="56" class="anchor-tag">4 <b>实验结果及分析</b></h3>
                <h4 class="anchor-tag" id="57" name="57">4.1 <b>实验方案</b></h4>
                <div class="p1">
                    <p id="58">实验采用Xilinx Zynq-7000 SoC ZC706嵌入式FPGA实验平台, 该系统搭载一颗xc7z045芯片, 片内由ARM A9处理器与可重构逻辑部分构成, 使用有限的片上资源545 M的BRAM和400个DSP.硬件实现环境为Xilinx Vivado 2017.对比实验中CPU采用Intel Core i5 2500K处理器, 主频为3.3 GHz, GPU采用NVIDIA GeForce GTX 960.硬件实现环境为Xilinx Vivado 2017.</p>
                </div>
                <div class="p1">
                    <p id="59">最终设计包含32个卷积器, 每个卷积器都配有一个1.8 KB系数存储器和1.7 MB的数据存储器, 足以加速AlexNet的所有卷积层.由于片上DSP资源有限, 每个卷积器包含12个DSP模块, 用于在每次运行时生成4个输出像素, 从而实现操作的最大性能.该设计包含四个通过高性能AXI接口连接到片外存储器的CDMA.</p>
                </div>
                <h4 class="anchor-tag" id="60" name="60">4.2 <b>实验结果</b></h4>
                <div class="p1">
                    <p id="61">FPGA资源消耗如表2, 本设计对FPGA内部资源进行高效率的使用, BRAM全部使用, DSP使用率达到98%, 文献<citation id="96" type="reference">[<a class="sup">6</a>]</citation>所使用Virtex 7开发板硬件资源极其丰富, 若将本设计置于该型号FPGA, 将展现更好的性能.</p>
                </div>
                <div class="area_img" id="62">
                    <p class="img_tit"><b>表</b>2 <b>资源消耗</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="62" border="1"><tr><td><br />模块</td><td>文献[6]</td><td>资源数</td></tr><tr><td><br />BRAM</td><td>1 024</td><td>545</td></tr><tr><td><br />LUT</td><td>28 000</td><td>77 442</td></tr><tr><td><br />DSP</td><td>2 688</td><td>391</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="63">将CNN在FPGA实现的性能与Intel Core i5 CPU和NVIDIA GeForce GTX 960 GPU做对比, 结果见表3.基于FPGA优化设计的卷积神经网络处理单张图片时间远少于CPU, 与GPU速度相当;FPGA功耗大约是GPU功耗的二十三分之一, 具有极高的能耗优势.</p>
                </div>
                <div class="area_img" id="64">
                    <p class="img_tit"><b>表</b>3 <b>比较结果</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="64" border="1"><tr><td><br />平台</td><td>CPU</td><td>GPU</td><td>FPGA</td></tr><tr><td><br />时间 (ms) </td><td>42.6</td><td>14.1</td><td>18.3</td></tr><tr><td><br />功耗 (W) </td><td>45</td><td>235</td><td>10</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="65">表4是不同实现方式的性能对比.文献<citation id="97" type="reference">[<a class="sup">7</a>]</citation>是使用ARM MaliT628GPU优化CNN代码得到的结果, 比较可知, 我们的方法是文献<citation id="98" type="reference">[<a class="sup">7</a>]</citation>的27倍.</p>
                </div>
                <div class="area_img" id="66">
                    <p class="img_tit"><b>表</b>4 <b>性能对比</b> <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="66" border="1"><tr><td><br />设备</td><td>OP/S[GOP/S]</td></tr><tr><td><br />文献[7] (GPU) </td><td>0.02</td></tr><tr><td><br />本文 (FPGA) </td><td>0.54</td></tr><tr><td><br />性能提升</td><td>27×</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <h3 id="67" name="67" class="anchor-tag">5 <b>结束语</b></h3>
                <div class="p1">
                    <p id="68">本文提出一种基于FPGA有限资源的卷积神经网络加速器, 利用FPGA内部资源, 提高数据的读取效率, 并通过流水结构和提升卷积运算的并行度提高卷积运算速度, 给出了卷积神经网络加速器内部结构, 在资源有限的ZC706开发板上实现AlexNet网络, 并与CPU和GPU进行对比, 实验结果显示, 嵌入式FPGA上的功耗及性能具有较大优势.</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="77">
                            <a id="bibliography_1" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=JSJY201609029&amp;v=MDM5MzE0TzN6cXFCdEdGckNVUkxPZVplVnVGeW5uVkwvUEx6N0JkN0c0SDlmTXBvOUhiWVFLREg4NHZSNFQ2ajU=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[1]</b> 李彦冬, 郝宗波, 雷航.卷积神经网络研究综述[J].计算机应用, 2016, 36 (9) :2508-2515.
                            </a>
                        </p>
                        <p id="79">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Performance Analysis of GPU-Based Convolutional Neural Networks">

                                <b>[2]</b> LI X, ZHANG G, HUANG H H, et al.Performance analysis of GPU-based convolutional neural networks[C]//International Conference on Parallel Processing.Philadelphia, USA:IEEE, 2016:67-76.
                            </a>
                        </p>
                        <p id="81">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Optimizing FPGA-based acelerator dsign for deep convolutional neural networks">

                                <b>[3]</b> ZHANG C, LI P, SUN G, et al.Optimizing FPGA-based Accelerator Design for Deep Convolutional Neural Networks[C]// ACM/SIGDA International Symposium on Field-Programmable Gate Arrays.Monterey, USA:ACM, 2015:161-170.
                            </a>
                        </p>
                        <p id="83">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Going deeper with embedded FPGA platform for convolutional neural network">

                                <b>[4]</b> QIU J, WANG J, YAO S, et al.Going deeper with embedded FPGA platform for convolutional neural network[C]//Proceedings of the 2016 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays.Monterey, USA:ACM, 2016:26-35.
                            </a>
                        </p>
                        <p id="85">
                            <a id="bibliography_5" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Energy-efficient CNN Implementation on a Deeply Pipelined FPGA Cluster">

                                <b>[5]</b> ZHANG C, WU D, SUN J, et al.Energy-efficient CNN implementation on a deeply pipelined FPGA cluster[C]//Proceedings of the 2016 International Symposium on Low Power Electronics and Design.San Francisco, USA:ACM, 2016:326-331.
                            </a>
                        </p>
                        <p id="87">
                            <a id="bibliography_6" target="_blank" href="http://scholar.cnki.net/result.aspx?q=DLAU:A Scalable Deep Learning Accelerator Unit on FPGA">

                                <b>[6]</b> WANG C , YU Q , GONG L , et al.DLAU:A Scalable Deep Learning Accelerator Unit on FPGA[J].IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 2016, 36 (3) :513-517.
                            </a>
                        </p>
                        <p id="89">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Optimizing convolutional neural networks on embedded platforms with OpenCL">

                                <b>[7]</b> LOKHMOTOV A, FURSIN G.Optimizing convolutional neural networks on embedded platforms with OpenCL[C]//Proceedings of the 4th International Workshop on OpenCL.Vienna, Austria:ACM, 2016:10-14.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="WXYJ201908018" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=WXYJ201908018&amp;v=MjIyNTJGeW5uVkwvUE1qWFNaTEc0SDlqTXA0OUViSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnU=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bEY4T2FiTmZaMW5CaDlzTGhwUT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
