<!DOCTYPE html>
<html>
<head>
    <title>全文阅读--XML全文阅读--中国知网</title>
    <link rel="icon" href="/kxreader/favicon.ico" />
    <link rel="shortcut Icon" href="/kxreader/favicon.ico" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="文献 XML KBASE CNKI 中国知网" />
    <meta name="description" content="XML文献检索" />
    <link href="/kxreader/Content/css/detail?v=qX2z2KjRAEyQiNfAbKtl7dLnsqFoQ5Jdw3TZfDf0n1k1" rel="stylesheet"/>

    <script type="text/javascript">
        var APPPATH = '/kxreader';
    </script>
</head>

<body>
    
<script type="text/javascript" src="//login.cnki.net/TopLogin/api/loginapi/get?type=top&amp;localCSS=&amp;returnurl=%2f%2fkns.cnki.net%2f%2fKXReader%2fDetail%3fTIMESTAMP%3d637133836961193750%26DBCODE%3dCJFD%26TABLEName%3dCJFDLAST2019%26FileName%3dWXYJ201909008%26RESULT%3d1%26SIGN%3dWpw12hqvzg5qvRXjVkKi2ZdcOzc%253d"></script>

<div id="headerBox" class="header">
    <div class="topbar">
        <div class="textalign">
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=WXYJ201909008&amp;align=md">
                <i class="icon-cen active" title="居中对齐"></i>
            </a>
            <a href="/kxreader/Detail?dbcode=CJFD&amp;filename=WXYJ201909008&amp;align=lt">
                <i class="icon-left " title="左对齐"></i>
            </a>
        </div>
        <h6 class="free-tip"><i class="icon"></i>HTML阅读开放试用阶段，欢迎体验！</h6>
    </div>
</div>

    



<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=WXYJ201909008&amp;v=MTg1MTJPM3pxcUJ0R0ZyQ1VSTE9lWmVWdUZ5bmtXci9PTWpYU1pMRzRIOWpNcG85RmJJUUtESDg0dlI0VDZqNTQ=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bFF0dU9NTmdMQ0V1SURuSmxZZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>

    <div class="main">

        

    <div class="sidebar-a">
        <!--sidebar start-->
        <div class="sidenav">
            <div class="arrow"><span></span></div>
            <!--sidebar_list start-->
            <dl class="sidenav-list">
                    <dt class="tit">目录结构</dt>
                            <dd class="guide">
                                    <p><a href="#29" data-title="1 &lt;b&gt;引言&lt;/b&gt; ">1 <b>引言</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#32" data-title="2 &lt;b&gt;相关工作回顾&lt;/b&gt; ">2 <b>相关工作回顾</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#74" data-title="3 &lt;b&gt;无效率学习抑制损失函数&lt;/b&gt; ">3 <b>无效率学习抑制损失函数</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#75" data-title="3.1 &lt;b&gt;设计理念&lt;/b&gt;">3.1 <b>设计理念</b></a></li>
                                                <li><a href="#85" data-title="3.2 &lt;b&gt;设计方案&lt;/b&gt;">3.2 <b>设计方案</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#104" data-title="4 &lt;b&gt;实验与分析&lt;/b&gt; ">4 <b>实验与分析</b></a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#105" data-title="4.1 &lt;b&gt;实验设计&lt;/b&gt;">4.1 <b>实验设计</b></a></li>
                                                <li><a href="#108" data-title="4.2 &lt;b&gt;定量结果分析&lt;/b&gt;">4.2 <b>定量结果分析</b></a></li>
                                                <li><a href="#124" data-title="4.3 &lt;b&gt;可视化&lt;/b&gt;">4.3 <b>可视化</b></a></li>
                                    </ul>
                            </dd>
                            <dd class="guide">
                                    <p><a href="#127" data-title="5 &lt;b&gt;结束语&lt;/b&gt; ">5 <b>结束语</b></a><i></i></p>
                                                            </dd>
                            <dd class="guide">
                                    <p><a href="#" data-title="文内图表 ">文内图表</a><i></i></p>
                                                                    <ul class="contentbox">
                                                <li><a href="#92" data-title="&lt;b&gt;图&lt;/b&gt;1 OBHE&lt;b&gt;损失函数曲线和&lt;/b&gt;Focal&lt;b&gt;损失函数曲线&lt;/b&gt;"><b>图</b>1 OBHE<b>损失函数曲线和</b>Focal<b>损失函数曲线</b></a></li>
                                                <li><a href="#111" data-title="&lt;b&gt;表&lt;/b&gt;1 &lt;b&gt;不同目标损失函数的效果(天池&lt;/b&gt;17,U-net)"><b>表</b>1 <b>不同目标损失函数的效果(天池</b>17,U-net)</a></li>
                                                <li><a href="#115" data-title="&lt;b&gt;图&lt;/b&gt;2 &lt;b&gt;测试集分割结果&lt;/b&gt;DC&lt;b&gt;指标的分布&lt;/b&gt;"><b>图</b>2 <b>测试集分割结果</b>DC<b>指标的分布</b></a></li>
                                                <li><a href="#118" data-title="&lt;b&gt;图&lt;/b&gt;3 epoch=0、15、45&lt;b&gt;时训练集逐像素&lt;/b&gt;&lt;i&gt;p&lt;/i&gt;&lt;sub&gt;&lt;i&gt;c&lt;/i&gt;&lt;/sub&gt;&lt;b&gt;的直方图统计&lt;/b&gt;"><b>图</b>3 epoch=0、15、45<b>时训练集逐像素</b><i>p</i><sub><i>c</i></sub><b>的直方图统计</b></a></li>
                                                <li><a href="#121" data-title="&lt;b&gt;表&lt;/b&gt;2 &lt;b&gt;不同目标损失函数的效果&lt;/b&gt;(PROMISE12,U-net)"><b>表</b>2 <b>不同目标损失函数的效果</b>(PROMISE12,U-net)</a></li>
                                                <li><a href="#122" data-title="&lt;b&gt;表&lt;/b&gt;3 &lt;b&gt;不同目标损失函数的效果(天池&lt;/b&gt;17,Linknet)"><b>表</b>3 <b>不同目标损失函数的效果(天池</b>17,Linknet)</a></li>
                                                <li><a href="#126" data-title="&lt;b&gt;图&lt;/b&gt;4 &lt;b&gt;结果可视化:分割标签边缘和网络预测边缘&lt;/b&gt;"><b>图</b>4 <b>结果可视化:分割标签边缘和网络预测边缘</b></a></li>
                                    </ul>
                            </dd>
                                    <dd class="guide">
                                        <h6>
                                            <p><a href="#a_bibliography">参考文献</a> </p>
                                        </h6>
                                    </dd>

            </dl>
        </div>
        <!--sidebar end-->
        &nbsp;
        <!--此处有一空格符 勿删-->
    </div>

                <div class="sidebar-b three-collumn" style="width:0;">
            <div class="refer" style="width: 0;">
                <div class="arrow off" title="参考文献"><span></span></div>
                <div class="js-scrollbox" >
                    
                    <div class="subbox active">
                        <h4>
                            <span class="tit">参考文献</span>
                            <a class="close" href="javascript:void(0)">x</a>
                        </h4>
                        <div class="side-scroller">
                            <ul class="refer-list">
                                <li id="3">


                                    <a id="bibliography_1" title=" LONG J,SHELHAMER E,DARRELL T.Fully convolutional networks for semantic segmentation[C]//Proceedings of the IEEE conference on CVPR.Boston,MA,USA:IEEE,2015:3431-3440." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Fully Convolutional Networks for Semantic Segmentation">
                                        <b>[1]</b>
                                         LONG J,SHELHAMER E,DARRELL T.Fully convolutional networks for semantic segmentation[C]//Proceedings of the IEEE conference on CVPR.Boston,MA,USA:IEEE,2015:3431-3440.
                                    </a>
                                </li>
                                <li id="5">


                                    <a id="bibliography_2" title=" RONNEBERGER O,FISCHER P,BROX T.U-Net:convolutional networks for biomedical image segmentation[C]// International Conference on MICCAI.Munich,Germany:Springer,2015:234-241." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=U-Net:Convolutional Networks for Biomedical Image Segmentation">
                                        <b>[2]</b>
                                         RONNEBERGER O,FISCHER P,BROX T.U-Net:convolutional networks for biomedical image segmentation[C]// International Conference on MICCAI.Munich,Germany:Springer,2015:234-241.
                                    </a>
                                </li>
                                <li id="7">


                                    <a id="bibliography_3" title=" BADRINARAYANAN V,KENDALL A,CIPOLLA R.Segnet:A deep convolutional encoder-decoder architecture for image segmentation[J].IEEE transactions on pattern analysis and machine intelligence,2017,39(12):2481-2495." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=SegNet:A Deep Convolutional Encoder-Decoder Architecture for Scene Segmentation">
                                        <b>[3]</b>
                                         BADRINARAYANAN V,KENDALL A,CIPOLLA R.Segnet:A deep convolutional encoder-decoder architecture for image segmentation[J].IEEE transactions on pattern analysis and machine intelligence,2017,39(12):2481-2495.
                                    </a>
                                </li>
                                <li id="9">


                                    <a id="bibliography_4" title=" CHAURASIA A,CULURCIELLO E.Linknet:Exploiting encoder representations for efficient semantic segmentation[C]//Visual Communications and Image Processing 2017.St.Petersburg,FL,USA:IEEE,2017:1-4." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Linknet:Exploiting encoder representations for efficient semantic segmentation">
                                        <b>[4]</b>
                                         CHAURASIA A,CULURCIELLO E.Linknet:Exploiting encoder representations for efficient semantic segmentation[C]//Visual Communications and Image Processing 2017.St.Petersburg,FL,USA:IEEE,2017:1-4.
                                    </a>
                                </li>
                                <li id="11">


                                    <a id="bibliography_5" title=" 芮波,张道强.脑白质损伤分割方法比较和策略改进[J].微电子学与计算机,2011,28(10):84-86." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=WXYJ201110023&amp;v=MTMwNDR2UjRUNmo1NE8zenFxQnRHRnJDVVJMT2VaZVZ1Rnlua1dyL09NalhTWkxHNEg5RE5yNDlIWjRRS0RIODQ=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bFF0dU9NTmdMQ0V1SURuSmxZZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[5]</b>
                                         芮波,张道强.脑白质损伤分割方法比较和策略改进[J].微电子学与计算机,2011,28(10):84-86.
                                    </a>
                                </li>
                                <li id="13">


                                    <a id="bibliography_6" title=" 马海荣,程新文.一种处理非平衡数据集的优化随机森林分类方法[J].微电子学与计算机,2018,35(11):28-32." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=WXYJ201811006&amp;v=MjA1Mjc0SDluTnJvOUZZb1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnVGeW5rV3IvT01qWFNaTEc=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bFF0dU9NTmdMQ0V1SURuSmxZZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[6]</b>
                                         马海荣,程新文.一种处理非平衡数据集的优化随机森林分类方法[J].微电子学与计算机,2018,35(11):28-32.
                                    </a>
                                </li>
                                <li id="15">


                                    <a id="bibliography_7" title=" CAELLES S,MANINIS K K,PONT-TUSET J,et al.One-shot video object segmentation[C]//Proceedings of the IEEE conference on CVPR.Honolulu,HI,USA:IEEE,2017:221-230." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=One-shot video object segmentation">
                                        <b>[7]</b>
                                         CAELLES S,MANINIS K K,PONT-TUSET J,et al.One-shot video object segmentation[C]//Proceedings of the IEEE conference on CVPR.Honolulu,HI,USA:IEEE,2017:221-230.
                                    </a>
                                </li>
                                <li id="17">


                                    <a id="bibliography_8" title=" MILLETARI F,NAVAB N,AHMADI S A.V-net:Fully convolutional neural networks for volumetric medical image segmentation[C]//2016 Fourth International Conference on 3D Vision.Stanford,California,USA:IEEE,2016:565-571." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=V-net:Fully convolutional neural networks for volumetric medical image segmentation">
                                        <b>[8]</b>
                                         MILLETARI F,NAVAB N,AHMADI S A.V-net:Fully convolutional neural networks for volumetric medical image segmentation[C]//2016 Fourth International Conference on 3D Vision.Stanford,California,USA:IEEE,2016:565-571.
                                    </a>
                                </li>
                                <li id="19">


                                    <a id="bibliography_9" title=" WU Z,SHEN C,HENGEL A.High-performance semantic segmentation using very deep fully convolutional networks[J].arXiv preprint arXiv:1604.04339,2016." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=High-performance semantic segmentation using very deep fully convolutional networks">
                                        <b>[9]</b>
                                         WU Z,SHEN C,HENGEL A.High-performance semantic segmentation using very deep fully convolutional networks[J].arXiv preprint arXiv:1604.04339,2016.
                                    </a>
                                </li>
                                <li id="21">


                                    <a id="bibliography_10" title=" LIN T Y,GOYAL P,GIRSHICK R,et al.Focal loss for dense object detection[C]//Proceedings of the IEEE international conference on computer vision.Venice,Italy:IEEE,2017:2980-2988." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Focal Loss for Dense Object Detection">
                                        <b>[10]</b>
                                         LIN T Y,GOYAL P,GIRSHICK R,et al.Focal loss for dense object detection[C]//Proceedings of the IEEE international conference on computer vision.Venice,Italy:IEEE,2017:2980-2988.
                                    </a>
                                </li>
                                <li id="23">


                                    <a id="bibliography_11" title=" RODIECK R W,STONE J.Analysis of receptive fields of cat retinal ganglion cells[J].Journal of Neurophysiology,1965,28(5):833-849." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=Analysis of receptive fields of cat retinal ganglion cells">
                                        <b>[11]</b>
                                         RODIECK R W,STONE J.Analysis of receptive fields of cat retinal ganglion cells[J].Journal of Neurophysiology,1965,28(5):833-849.
                                    </a>
                                </li>
                                <li id="25">


                                    <a id="bibliography_12" title=" ENROTH-CUGELL C,ROBSON J G.The contrast sensitivity of retinal ganglion cells of the cat[J].The Journal of physiology,1966,187(3):517-552." target="_blank"
                                       href="http://scholar.cnki.net/result.aspx?q=The contrast sensitivity of retinal ganglion cells of the cat">
                                        <b>[12]</b>
                                         ENROTH-CUGELL C,ROBSON J G.The contrast sensitivity of retinal ganglion cells of the cat[J].The Journal of physiology,1966,187(3):517-552.
                                    </a>
                                </li>
                                <li id="27">


                                    <a id="bibliography_13" title=" LITJENS G,TOTH R,VAN DE VEN W,et al.Evaluation of prostate segmentation algorithms for MRI:the PROMISE12 challenge[J].Medical image analysis,2014,18(2):359-373." target="_blank"
                                       href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14010600482883&amp;v=MjE2MjZmT2ZiSzhIdERNcVk5RllPTU5CSFE2b0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadUh5am1VYi9JSVY4ZGFoUT1OaQ==&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bFF0dU9NTmdMQ0V1SURuSmxZZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                        <b>[13]</b>
                                         LITJENS G,TOTH R,VAN DE VEN W,et al.Evaluation of prostate segmentation algorithms for MRI:the PROMISE12 challenge[J].Medical image analysis,2014,18(2):359-373.
                                    </a>
                                </li>
                            </ul>
                            <div style='display: none;' class="zqscroller" >
                                <h4 class="">附加材料</h4>
                                <ul></ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            &nbsp;
            <!--此处有一空格符 勿删-->
        </div>

        
    <div class="content">



        <!--tips start-->
                            <div class="tips">
                    <a href="http://navi.cnki.net/KNavi/JournalDetail?pcode=CJFD&amp;pykm=WXYJ" target="_blank">微电子学与计算机</a>
                2019,36(09),38-43             </div>
        <!--tips end-->
            <div class="top-title">
                <h1 class="title">
                    <span class="vm"><b>端到端深度图像分割网络中抑制无效率</b><b>学习的目标损失函数设计</b></span>
                                    </h1>

            </div>
                        <h2>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%8F%B6%E9%9D%96%E9%9B%AF&amp;code=42764127&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bFF0dU9NTmdMQ0V1SURuSmxZZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">叶靖雯</a>
                                <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=au&amp;skey=%E5%90%B4%E6%99%93%E5%B3%B0&amp;code=06701441&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bFF0dU9NTmdMQ0V1SURuSmxZZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">吴晓峰</a>
                </h2>
                    <h2>
                    <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=in&amp;skey=%E5%A4%8D%E6%97%A6%E5%A4%A7%E5%AD%A6%E4%BF%A1%E6%81%AF%E7%A7%91%E5%AD%A6%E4%B8%8E%E5%B7%A5%E7%A8%8B%E5%AD%A6%E9%99%A2%E5%9B%BE%E5%83%8F%E4%B8%8E%E6%99%BA%E8%83%BD%E4%BF%A1%E6%81%AF%E5%A4%84%E7%90%86%E5%AE%9E%E9%AA%8C%E5%AE%A4&amp;code=0075855&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bFF0dU9NTmdMQ0V1SURuSmxZZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">复旦大学信息科学与工程学院图像与智能信息处理实验室</a>
            </h2>

        
<div class="link">
    <a id="aexport" class="icon icon-output"  onclick="" href="javascript:void(0);"><i></i>导出/参考文献</a>
    
    <span class="shareBoard" onmouseover="$('#sharedet').show();$('#this').addClass('shareBoardCUR')" onmouseout="$('#sharedet').hide();$('#this').removeClass('shareBoardCUR')">
        <a class="icon icon-share" href="#"><i></i>分享<em></em></a>
        <ul class="shareHide" id="sharedet" style="display: none;">
            <li><a title="复制链接" class="copy" onclick="" href="#"><i></i>复制链接</a></li>
            <li><a title="分享到新浪微博" class="xl" onclick="" href="javascript:common.ShareAction('xl');"><i></i>新浪微博</a></li>
            <li>
                <a title="分享到微信" class="wx" onclick="" href="#"><i></i>微信扫一扫</a>
                <div class="qrcode"><img src='' alt='' /></div>
            </li>
        </ul>

    </span>
    
    <a id="RefTrack" title="创建引文跟踪" class="icon icon-track" onclick="" href="javascript:void(0);"> <i></i>创建引文跟踪 </a>
    <a id="ashoucang" title="收藏" class="icon icon-favor" onclick="" href="javascript:void(0);"><i></i>收藏</a>
    <a class="icon icon-print" onclick="window.print();" href="javascript:void(0);"><i></i>打印</a>
    
    <!--版本切换 end-->
</div>
                            <div class="data" id="a_abstract">
                <span class="keys">摘<span style="font-family: 'Times New Roman';">&nbsp;&nbsp;&nbsp;&nbsp;</span>要：</span>
                <p>在端到端深度图像分割网络训练时,常出现前景和背景区域相差巨大的情况,造成目标特征学习不足而背景特征学习过度.对此提出一种基于代价敏感学习的目标函数构造方法:借鉴难例挖掘思想,使用表征难易程度的Focal因子对样本训练误差加权处理,有效抑制无效率学习;仿人类视觉系统引入感受野因子,兼顾上下文信息.在医学影像数据集上对方法的有效性和可扩展性进行了测试.结果表明,新方法有助于提升网络对于小目标的检出能力,同时分割结果更贴合目标轮廓.</p>
            </div>
                    <div class="data" id="a_keywords">
                <span class="keys">关键词：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bFF0dU9NTmdMQ0V1SURuSmxZZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">深度学习;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bFF0dU9NTmdMQ0V1SURuSmxZZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">医学图像分割;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E8%AE%BE%E8%AE%A1&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bFF0dU9NTmdMQ0V1SURuSmxZZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">损失函数设计;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=%E6%97%A0%E6%95%88%E7%8E%87%E5%AD%A6%E4%B9%A0&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bFF0dU9NTmdMQ0V1SURuSmxZZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">无效率学习;</a>
                </p>
            </div>
        
        <!--brief start-->
        
            <div class="brief">
                    <p>
                            <b>作者简介：</b>
                                                        <span>
                                    叶靖雯,女,(1994-),硕士.研究方向为医学图像处理.E-mail:yejw16@fudan.edu.cn.;
                                </span>
                                <span>
                                    吴晓峰,男,(1971-),博士,高级讲师.研究方向为图像与信号处理、机器人智能信息处理与控制、多媒体技术应用.;
                                </span>
                    </p>
                                    <p><b>收稿日期：</b>2018-11-20</p>

                    <p>

                            <b>基金：</b>
                                                        <span>上海市卫生计生委智慧医疗专项研究项目(2018ZHYL0204);</span>
                    </p>
            </div>
                    <h1><b>Loss function for ineffective learning reduction in End-to-End deep image segmentation network</b></h1>
                    <h2>
                    <span>YE Jing-wen</span>
                    <span>WU Xiao-feng</span>
            </h2>
                    <h2>
                    <span>Image and Intelligence Lab, School of Information Science and Technology,Fudan University</span>
            </h2>
                            <div class="data" id="a_abstractEN">
                <span class="keys">Abstract：</span>
                <p>In image segmentation tasks based on deep learning methods, it is common that foreground pixels occur significantly more frequently than background pixels, and consequently bias the trained network towards them. In this paper, based on cost-sensitive learning, a design method of loss function for end-to-end image segmentation network is proposed, where two improvements are provided as follows: 1) Inspired by the conception of “hard examples mining”, focal loss is introduced and extended to work for ineffective learning reduction. 2) Inspired by human visual systems, adaptive weights of receptive field are added to further consider the context information. In order to verify the validity and expansibility, the proposed method has been evaluated on severalmedicalimage datasets. The results show that the proposed method can improve the detection performance of the network for small objects, and obtain segmentation results that are more suitable for object contour.</p>
            </div>
                    <div class="data" id="a_keywordsEN">
                <span class="keys">Keyword：</span>
                <p>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=deep%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bFF0dU9NTmdMQ0V1SURuSmxZZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">deep learning;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=medical%20image%20segmentation&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bFF0dU9NTmdMQ0V1SURuSmxZZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">medical image segmentation;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=loss%20function&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bFF0dU9NTmdMQ0V1SURuSmxZZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">loss function;</a>
                        <a href="/kcms/detail/knetsearch.aspx?dbcode=CJFD&amp;sfield=kw&amp;skey=ineffective%20learning&amp;code=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bFF0dU9NTmdMQ0V1SURuSmxZZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" target="_blank">ineffective learning;</a>
                </p>
            </div>
                    <div class="brief">
                
                    <p>
                                            </p>
                                    <p><b>Received：</b> 2018-11-20</p>
                                    <p>
                                            </p>
            </div>


        <!--brief start-->
                        <h3 id="29" name="29" class="anchor-tag">1 <b>引言</b></h3>
                <div class="p1">
                    <p id="30">图像语义分割是计算机视觉的基本任务之一,目的是将图像分割为多块相同语义的像素区域,并识别出各区域对应的类别.自全卷积网络(FCN)<citation id="129" type="reference"><link href="3" rel="bibliography" /><sup>[<a class="sup">1</a>]</sup></citation>提出以来,端到端的深度网络<citation id="131" type="reference"><link href="5" rel="bibliography" /><link href="7" rel="bibliography" /><link href="9" rel="bibliography" /><sup>[<a class="sup">2</a>,<a class="sup">3</a>,<a class="sup">4</a>]</sup></citation>已成为语义分割任务的主流框架.但目前,学术界少有研究关注到训练时,图像不同类别区域占比相差过大导致网络性能不佳的问题,下文简称各类占比不平衡问题.该问题表现为多数类分割损失在优化过程中占主导,使训练好的网络仅对多数类样本效果较好.但在相当一部分实际应用中,如病灶分割<citation id="130" type="reference"><link href="11" rel="bibliography" /><sup>[<a class="sup">5</a>]</sup></citation>,少数类才是我们关注的.</p>
                </div>
                <div class="p1">
                    <p id="31">由于该问题通常由图像的内容特性决定,无法通过常见的增加样本或重采样<citation id="132" type="reference"><link href="13" rel="bibliography" /><sup>[<a class="sup">6</a>]</sup></citation>方法进行调节,对此本文使用另一类“代价敏感学习”方法,即为不同类别样本分配不同的误分类代价,重新构造用于优化的目标损失函数.在此基础上,通过分析现有损失函数的特点及问题成因,提出了无效率学习抑制损失(Ineffective-learning-reduction Loss)的设计方案.</p>
                </div>
                <h3 id="32" name="32" class="anchor-tag">2 <b>相关工作回顾</b></h3>
                <div class="p1">
                    <p id="33">首先将图像目标-背景的二元分割任务描述如下:定义输入训练数据集为<mathml id="34"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>S</mi><mo>=</mo><mo stretchy="false">{</mo><mrow><mo>(</mo><mrow><mi>X</mi><msub><mrow></mrow><mi>n</mi></msub><mo>,</mo><mi>Y</mi><msub><mrow></mrow><mi>n</mi></msub></mrow><mo>)</mo></mrow><mo>,</mo><mi>n</mi><mo>=</mo><mn>1</mn><mo>,</mo><mo>⋯</mo><mo>,</mo><mi>Ν</mi></mrow></math></mathml>,其中<i>X</i><sub><i>n</i></sub>表示输入图像,<i>Y</i><sub><i>n</i></sub>表示<i>X</i><sub><i>n</i></sub>的分割标注图.图像<i>X</i><sub><i>n</i></sub>中,定义<i>y</i><mathml id="35"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>n</mi><mi>j</mi></msubsup></mrow></math></mathml>为像素<i>x</i><mathml id="36"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>n</mi><mi>j</mi></msubsup></mrow></math></mathml>的二值标签.对于网络<i>f</i><sub><i>θ</i></sub>,用于优化参数<i>θ</i>的成本函数<i>J</i>(<i>θ</i>)可以表示为:</p>
                </div>
                <div class="p1">
                    <p id="37" class="code-formula">
                        <mathml id="37"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>J</mi><mrow><mo>(</mo><mi>θ</mi><mo>)</mo></mrow><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mrow><mo>(</mo><mrow><mi>X</mi><msub><mrow></mrow><mi>n</mi></msub><mo>,</mo><mi>Y</mi><msub><mrow></mrow><mi>n</mi></msub></mrow><mo>)</mo></mrow><mo>∈</mo><mi>S</mi></mrow></munder><mi>L</mi></mstyle><mrow><mo>(</mo><mrow><mi>f</mi><msub><mrow></mrow><mi>θ</mi></msub><mrow><mo>(</mo><mrow><mi>X</mi><msub><mrow></mrow><mi>n</mi></msub></mrow><mo>)</mo></mrow><mo>,</mo><mi>Y</mi><msub><mrow></mrow><mi>n</mi></msub></mrow><mo>)</mo></mrow></mtd></mtr><mtr><mtd><mi>L</mi><mrow><mo>(</mo><mrow><mi>f</mi><msub><mrow></mrow><mi>θ</mi></msub><mrow><mo>(</mo><mrow><mi>X</mi><msub><mrow></mrow><mi>n</mi></msub></mrow><mo>)</mo></mrow><mo>,</mo><mi>Y</mi><msub><mrow></mrow><mi>n</mi></msub></mrow><mo>)</mo></mrow><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mrow><mrow><mo>|</mo><mrow><mi>X</mi><msub><mrow></mrow><mi>n</mi></msub></mrow><mo>|</mo></mrow></mrow></munderover><mi>l</mi></mstyle><mo stretchy="false">(</mo><mover><mstyle mathsize="140%" displaystyle="true"><mrow><mi>y</mi><msubsup><mrow></mrow><mi>n</mi><mi>j</mi></msubsup></mrow></mstyle><mo>︿</mo></mover><mo>,</mo><mi>y</mi><msubsup><mrow></mrow><mi>n</mi><mi>j</mi></msubsup><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="38">式中,<mathml id="39"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>l</mi><mo stretchy="false">(</mo><mover><mstyle mathsize="140%" displaystyle="true"><mrow><mi>y</mi><msubsup><mrow></mrow><mi>n</mi><mi>j</mi></msubsup></mrow></mstyle><mo>︿</mo></mover><mo>,</mo><mi>y</mi><msubsup><mrow></mrow><mi>n</mi><mi>j</mi></msubsup><mo stretchy="false">)</mo></mrow></math></mathml>度量了像素<i>j</i>预测结果<mathml id="40"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mrow><mi>y</mi><msubsup><mrow></mrow><mi>n</mi><mi>j</mi></msubsup></mrow></mstyle><mo>︿</mo></mover><mo>∈</mo><mrow><mo>[</mo><mrow><mn>0</mn><mo>,</mo><mn>1</mn></mrow><mo>]</mo></mrow></mrow></math></mathml>和标注<mathml id="41"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>y</mi><msubsup><mrow></mrow><mi>n</mi><mi>j</mi></msubsup><mo>∈</mo><mrow><mo>{</mo><mrow><mn>0</mn><mo>,</mo><mn>1</mn></mrow><mo>}</mo></mrow></mrow></math></mathml>之间的偏差,称作像素估计损失,简写为<mathml id="42"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>l</mi><mrow><mo>(</mo><mrow><mi>x</mi><msubsup><mrow></mrow><mi>n</mi><mi>j</mi></msubsup></mrow><mo>)</mo></mrow></mrow></math></mathml>.而每个图像的损失函数<mathml id="43"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><mrow><mo>(</mo><mrow><mi>f</mi><msub><mrow></mrow><mi>θ</mi></msub><mrow><mo>(</mo><mrow><mi>X</mi><msub><mrow></mrow><mi>n</mi></msub></mrow><mo>)</mo></mrow><mo>,</mo><mi>Y</mi><msub><mrow></mrow><mi>n</mi></msub></mrow><mo>)</mo></mrow></mrow></math></mathml>通常可表示为逐像素估计损失的和,简写为<mathml id="44"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><mrow><mo>(</mo><mrow><mi>Y</mi><msub><mrow></mrow><mi>n</mi></msub></mrow><mo>)</mo></mrow></mrow></math></mathml>.对于<i>X</i><sub><i>n</i></sub>,定义像素标注对的集合为<mathml id="45"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><msub><mrow></mrow><mi>n</mi></msub><mo>=</mo><mrow><mo>{</mo><mrow><mrow><mo>(</mo><mrow><mi>x</mi><msubsup><mrow></mrow><mi>n</mi><mi>j</mi></msubsup><mo>,</mo><mi>y</mi><msubsup><mrow></mrow><mi>n</mi><mi>j</mi></msubsup></mrow><mo>)</mo></mrow><mo>,</mo><mi>j</mi><mo>=</mo><mn>1</mn><mo>,</mo><mo>⋯</mo><mo>,</mo><mrow><mo>|</mo><mrow><mi>X</mi><msub><mrow></mrow><mi>n</mi></msub></mrow><mo>|</mo></mrow></mrow><mo>}</mo></mrow></mrow></math></mathml>,其背景类的子集为<i>P</i><mathml id="46"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>n</mi><mrow><mo stretchy="false">(</mo><mn>0</mn><mo stretchy="false">)</mo></mrow></msubsup></mrow></math></mathml>,目标类的子集为<mathml id="47"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ρ</mi><msubsup><mrow></mrow><mi>n</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msubsup><mo>,</mo><mrow><mo>|</mo><mi>A</mi><mo>|</mo></mrow></mrow></math></mathml>表示集合<i>A</i>的元素数.上述图像分割的各类占比不平衡可表示为<mathml id="48"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>|</mo><mrow><mi>Ρ</mi><msubsup><mrow></mrow><mi>n</mi><mrow><mo stretchy="false">(</mo><mn>0</mn><mo stretchy="false">)</mo></mrow></msubsup></mrow><mo>|</mo></mrow><mo>≫</mo><mrow><mo>|</mo><mrow><mi>Ρ</mi><msubsup><mrow></mrow><mi>n</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msubsup></mrow><mo>|</mo></mrow><mo>.</mo></mrow></math></mathml></p>
                </div>
                <div class="p1">
                    <p id="49">根据解决问题的切入点不同,将相关工作分为三类进行说明.由于缺乏专门的研究,以下方案在原作中仅作为完整分割系统的一小部分被简单提及.</p>
                </div>
                <div class="p1">
                    <p id="50">第一类设计方案基于一个直观的假设:样本的估计误差在同一量级,即<mathml id="51"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>l</mi><mrow><mo>(</mo><mrow><mi>y</mi><msubsup><mrow></mrow><mi>n</mi><mi>i</mi></msubsup></mrow><mo>)</mo></mrow><mo>≈</mo><mi>l</mi><mrow><mo>(</mo><mrow><mi>y</mi><msubsup><mrow></mrow><mi>n</mi><mi>j</mi></msubsup></mrow><mo>)</mo></mrow><mo>≈</mo><mi>l</mi><mo>,</mo><mi>i</mi><mo>≠</mo><mi>j</mi></mrow></math></mathml>.将图像<i>X</i><sub><i>n</i></sub>的损失函数<mathml id="52"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><mrow><mo>(</mo><mrow><mi>Y</mi><msub><mrow></mrow><mi>n</mi></msub></mrow><mo>)</mo></mrow></mrow></math></mathml>分解为属于不同类别的像素估计损失和:</p>
                </div>
                <div class="p1">
                    <p id="53" class="code-formula">
                        <mathml id="53"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>L</mi><mrow><mo>(</mo><mrow><mi>Y</mi><msub><mrow></mrow><mi>n</mi></msub></mrow><mo>)</mo></mrow><mo>=</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>y</mi><msubsup><mrow></mrow><mi>n</mi><mi>j</mi></msubsup><mo>∈</mo><mi>Ρ</mi><msubsup><mrow></mrow><mi>n</mi><mrow><mo stretchy="false">(</mo><mn>0</mn><mo stretchy="false">)</mo></mrow></msubsup></mrow></munder><mi>l</mi></mstyle><mrow><mo>(</mo><mrow><mi>y</mi><msubsup><mrow></mrow><mi>n</mi><mi>j</mi></msubsup></mrow><mo>)</mo></mrow><mo>+</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>y</mi><msubsup><mrow></mrow><mi>n</mi><mi>j</mi></msubsup><mo>∈</mo><mi>Ρ</mi><msubsup><mrow></mrow><mi>n</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msubsup></mrow></munder><mi>l</mi></mstyle><mrow><mo>(</mo><mrow><mi>y</mi><msubsup><mrow></mrow><mi>n</mi><mi>j</mi></msubsup></mrow><mo>)</mo></mrow></mtd></mtr><mtr><mtd><mo>=</mo><mrow><mo>|</mo><mrow><mi>Ρ</mi><msubsup><mrow></mrow><mi>n</mi><mrow><mo stretchy="false">(</mo><mn>0</mn><mo stretchy="false">)</mo></mrow></msubsup></mrow><mo>|</mo></mrow><mo>⋅</mo><mi>l</mi><mo>+</mo><mrow><mo>|</mo><mrow><mi>Ρ</mi><msubsup><mrow></mrow><mi>n</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msubsup></mrow><mo>|</mo></mrow><mo>⋅</mo><mi>l</mi><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="54">若<mathml id="55"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>|</mo><mrow><mi>Ρ</mi><msubsup><mrow></mrow><mi>n</mi><mrow><mo stretchy="false">(</mo><mn>0</mn><mo stretchy="false">)</mo></mrow></msubsup></mrow><mo>|</mo></mrow><mo>≫</mo><mrow><mo>|</mo><mrow><mi>Ρ</mi><msubsup><mrow></mrow><mi>n</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msubsup></mrow><mo>|</mo></mrow></mrow></math></mathml>,则背景类误差在整体损失中占主导,根据参数更新公式<mathml id="56"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Δ</mi><mi>θ</mi><mo>=</mo><mi>α</mi><mo>⋅</mo><mo>∂</mo><mi>L</mi><mrow><mo>(</mo><mrow><mi>Y</mi><msub><mrow></mrow><mi>n</mi></msub></mrow><mo>)</mo></mrow><mo>/</mo><mo>∂</mo><mi>θ</mi></mrow></math></mathml>,优化过程会偏向修正背景类误差的方向去调节参数<i>θ</i>.因此,为平衡不同类别对整体损失的贡献,在损失函数中增加平衡权重<mathml id="57"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>α</mi><msub><mrow></mrow><mi>c</mi></msub><mo>≈</mo><mn>1</mn><mo>/</mo><mrow><mo>|</mo><mrow><mi>Ρ</mi><msubsup><mrow></mrow><mi>n</mi><mrow><mo stretchy="false">(</mo><mi>c</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow><mo>|</mo></mrow><mo>,</mo><mi>c</mi><mo>∈</mo><mrow><mo>{</mo><mrow><mn>0</mn><mo>,</mo><mn>1</mn></mrow><mo>}</mo></mrow></mrow></math></mathml>:</p>
                </div>
                <div class="p1">
                    <p id="58" class="code-formula">
                        <mathml id="58"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>L</mi><mrow><mo>(</mo><mrow><mi>Y</mi><msub><mrow></mrow><mi>n</mi></msub></mrow><mo>)</mo></mrow><mo>=</mo><mi>α</mi><msub><mrow></mrow><mn>0</mn></msub><mo>⋅</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>y</mi><msubsup><mrow></mrow><mi>n</mi><mi>j</mi></msubsup><mo>∈</mo><mi>Ρ</mi><msubsup><mrow></mrow><mi>n</mi><mrow><mo stretchy="false">(</mo><mn>0</mn><mo stretchy="false">)</mo></mrow></msubsup></mrow></munder><mi>l</mi></mstyle><mrow><mo>(</mo><mrow><mi>y</mi><msubsup><mrow></mrow><mi>n</mi><mi>j</mi></msubsup></mrow><mo>)</mo></mrow><mo>+</mo><mi>α</mi><msub><mrow></mrow><mn>1</mn></msub><mo>⋅</mo><mstyle displaystyle="true"><munder><mo>∑</mo><mrow><mi>y</mi><msubsup><mrow></mrow><mi>n</mi><mi>j</mi></msubsup><mo>∈</mo><mi>Ρ</mi><msubsup><mrow></mrow><mi>n</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msubsup></mrow></munder><mi>l</mi></mstyle><mrow><mo>(</mo><mrow><mi>y</mi><msubsup><mrow></mrow><mi>n</mi><mi>j</mi></msubsup></mrow><mo>)</mo></mrow></mtd></mtr><mtr><mtd><mo>≈</mo><mfrac><mn>1</mn><mrow><mrow><mo>|</mo><mrow><mi>Ρ</mi><msubsup><mrow></mrow><mi>n</mi><mrow><mo stretchy="false">(</mo><mn>0</mn><mo stretchy="false">)</mo></mrow></msubsup></mrow><mo>|</mo></mrow></mrow></mfrac><mo>⋅</mo><mrow><mo>|</mo><mrow><mi>Ρ</mi><msubsup><mrow></mrow><mi>n</mi><mrow><mo stretchy="false">(</mo><mn>0</mn><mo stretchy="false">)</mo></mrow></msubsup></mrow><mo>|</mo></mrow><mi>l</mi><mo>+</mo><mfrac><mn>1</mn><mrow><mrow><mo>|</mo><mrow><mi>Ρ</mi><msubsup><mrow></mrow><mi>n</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msubsup></mrow><mo>|</mo></mrow></mrow></mfrac><mo>⋅</mo><mrow><mo>|</mo><mrow><mi>Ρ</mi><msubsup><mrow></mrow><mi>n</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msubsup></mrow><mo>|</mo></mrow><mi>l</mi><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>3</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="59">式中,平衡权重的设计,文献<citation id="133" type="reference">[<a class="sup">7</a>]</citation>使用各类样本自然频数的倒数,即<mathml id="60"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>α</mi><msub><mrow></mrow><mi>c</mi></msub><mo>=</mo><mn>1</mn><mo>/</mo><mrow><mo>|</mo><mrow><mi>Ρ</mi><msubsup><mrow></mrow><mi>n</mi><mrow><mo stretchy="false">(</mo><mi>c</mi><mo stretchy="false">)</mo></mrow></msubsup></mrow><mo>|</mo></mrow></mrow></math></mathml>;文献<citation id="134" type="reference">[<a class="sup">3</a>]</citation>取<i>α</i><sub><i>c</i></sub>=median_<i>f</i>/<i>f</i>(<i>c</i>),<i>f</i>(<i>c</i>)表示该图像中类别<i>c</i>的频度,median_<i>f</i>表示图像间类别<i>c</i>频度的中位数.</p>
                </div>
                <div class="p1">
                    <p id="61">第二类设计方案<citation id="135" type="reference"><link href="17" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>依据分割评价指标最优的思路,使用例如负Dice系数(DC)、负交并比(IoU)作为目标损失函数.特别是DC型损失函数被广泛应用于医学图像分割领域<citation id="136" type="reference"><link href="17" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>,文献<citation id="137" type="reference">[<a class="sup">8</a>]</citation>的损失函数可表示为:</p>
                </div>
                <div class="p1">
                    <p id="62" class="code-formula">
                        <mathml id="62"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>L</mi><mrow><mo>(</mo><mrow><mi>Y</mi><msub><mrow></mrow><mi>n</mi></msub></mrow><mo>)</mo></mrow><mo>=</mo><mo>-</mo><mfrac><mrow><mn>2</mn><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mrow><mrow><mo>|</mo><mrow><mi>X</mi><msub><mrow></mrow><mi>n</mi></msub></mrow><mo>|</mo></mrow></mrow></munderover><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mrow><mi>y</mi><msubsup><mrow></mrow><mi>n</mi><mi>j</mi></msubsup></mrow></mstyle><mo>︿</mo></mover></mrow></mstyle><mi>y</mi><msubsup><mrow></mrow><mi>n</mi><mi>j</mi></msubsup></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mrow><mrow><mo>|</mo><mrow><mi>X</mi><msub><mrow></mrow><mi>n</mi></msub></mrow><mo>|</mo></mrow></mrow></munderover><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mrow><mi>y</mi><msubsup><mrow></mrow><mi>n</mi><mrow><mi>j</mi><msup><mrow></mrow><mn>2</mn></msup></mrow></msubsup></mrow></mstyle><mo>︿</mo></mover></mrow></mstyle><mo>+</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mrow><mrow><mo>|</mo><mrow><mi>X</mi><msub><mrow></mrow><mi>n</mi></msub></mrow><mo>|</mo></mrow></mrow></munderover><mrow><mi>y</mi><msubsup><mrow></mrow><mi>n</mi><mrow><mi>j</mi><msup><mrow></mrow><mn>2</mn></msup></mrow></msubsup></mrow></mstyle></mrow></mfrac></mtd></mtr><mtr><mtd><mo>=</mo><mo>-</mo><mfrac><mrow><mn>2</mn><mstyle displaystyle="true"><mo>∑</mo><mi>y</mi></mstyle><msubsup><mrow></mrow><mi>n</mi><mi>j</mi></msubsup><mo>∈</mo><mi>Ρ</mi><msubsup><mrow></mrow><mi>n</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msubsup><mover><mstyle mathsize="140%" displaystyle="true"><mrow><mi>y</mi><msubsup><mrow></mrow><mi>n</mi><mi>j</mi></msubsup></mrow></mstyle><mo>︿</mo></mover></mrow><mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mrow><mrow><mo>|</mo><mrow><mi>X</mi><msub><mrow></mrow><mi>n</mi></msub></mrow><mo>|</mo></mrow></mrow></munderover><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mrow><mi>y</mi><msubsup><mrow></mrow><mi>n</mi><mrow><mi>j</mi><msup><mrow></mrow><mn>2</mn></msup></mrow></msubsup></mrow></mstyle><mo>︿</mo></mover></mrow></mstyle><mo>+</mo><mrow><mo>|</mo><mrow><mi>Ρ</mi><msubsup><mrow></mrow><mi>n</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msubsup></mrow><mo>|</mo></mrow></mrow></mfrac><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>4</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="63">从数学表达式上分析,分子上的误差项只关注<i>P</i><mathml id="64"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>n</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msubsup></mrow></math></mathml>集的预测值<mathml id="65"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mrow><mi>y</mi><msubsup><mrow></mrow><mi>n</mi><mi>j</mi></msubsup></mrow></mstyle><mo>︿</mo></mover></mrow></math></mathml>是否足够接近1,而不关心<i>P</i><mathml id="66"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>n</mi><mrow><mo stretchy="false">(</mo><mn>0</mn><mo stretchy="false">)</mo></mrow></msubsup></mrow></math></mathml>集的预测结果,即优化该损失函数将使得网络更关注少数的目标类样本.</p>
                </div>
                <div class="p1">
                    <p id="67">第三类设计方案<citation id="138" type="reference"><link href="19" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>提出难例在线自举(OBHE)的想法,其核心是将网络已经学习得足够好的样例从损失函数中去除,留下尚未学好的所谓困难样例继续指导网络优化.为了简化表示,定义像素对应类别的预测概率:</p>
                </div>
                <div class="p1">
                    <p id="68" class="code-formula">
                        <mathml id="68"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mrow><mi>y</mi><msubsup><mrow></mrow><mi>n</mi><mi>j</mi></msubsup></mrow></mstyle><mo>︿</mo></mover><msup><mrow></mrow><mrow><mo stretchy="false">[</mo><mi>c</mi><mo stretchy="false">]</mo></mrow></msup><mo>=</mo><mrow><mo>{</mo><mrow><mtable><mtr><mtd><mover><mstyle mathsize="140%" displaystyle="true"><mrow><mi>y</mi><msubsup><mrow></mrow><mi>n</mi><mi>j</mi></msubsup></mrow></mstyle><mo>︿</mo></mover></mtd><mtd><mi>y</mi><msubsup><mrow></mrow><mi>n</mi><mi>j</mi></msubsup><mo>=</mo><mn>1</mn></mtd></mtr><mtr><mtd><mn>1</mn><mo>-</mo><mover><mstyle mathsize="140%" displaystyle="true"><mrow><mi>y</mi><msubsup><mrow></mrow><mi>n</mi><mi>j</mi></msubsup></mrow></mstyle><mo>︿</mo></mover></mtd><mtd><mi>y</mi><msubsup><mrow></mrow><mi>n</mi><mi>j</mi></msubsup><mo>=</mo><mn>0</mn></mtd></mtr></mtable></mrow></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>5</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="69">此时图像损失函数可表示为:</p>
                </div>
                <div class="p1">
                    <p id="70" class="code-formula">
                        <mathml id="70"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>L</mi><mrow><mo>(</mo><mrow><mi>Y</mi><msub><mrow></mrow><mi>n</mi></msub></mrow><mo>)</mo></mrow><mo>=</mo><mfrac><mn>1</mn><mrow><mstyle displaystyle="true"><msub><mo>∑</mo><mi>j</mi></msub><mi>Ι</mi></mstyle><mrow><mo>[</mo><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mrow><mi>y</mi><msubsup><mrow></mrow><mi>n</mi><mi>j</mi></msubsup></mrow></mstyle><mo>︿</mo></mover><msup><mrow></mrow><mrow><mrow><mo>[</mo><mi>c</mi><mo>]</mo></mrow></mrow></msup><mo>&lt;</mo><mi>t</mi></mrow><mo>]</mo></mrow></mrow></mfrac><mo>⋅</mo></mtd></mtr><mtr><mtd><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mrow><mrow><mo>|</mo><mrow><mi>X</mi><msub><mrow></mrow><mi>n</mi></msub></mrow><mo>|</mo></mrow></mrow></munderover><mi>Ι</mi></mstyle><mrow><mo>[</mo><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mrow><mi>y</mi><msubsup><mrow></mrow><mi>n</mi><mi>j</mi></msubsup></mrow></mstyle><mo>︿</mo></mover><msup><mrow></mrow><mrow><mrow><mo>[</mo><mi>c</mi><mo>]</mo></mrow></mrow></msup><mo>&lt;</mo><mi>t</mi></mrow><mo>]</mo></mrow><mi>l</mi><mrow><mo>(</mo><mrow><mi>y</mi><msubsup><mrow></mrow><mi>n</mi><mi>j</mi></msubsup></mrow><mo>)</mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>6</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="71">式中,<mathml id="72"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>Ι</mi><mrow><mo>[</mo><mi>a</mi><mo>]</mo></mrow></mrow></math></mathml>为指示函数,若条件<i>a</i>满足则取值为1,否则为0;阈值<i>t</i>为超参数,若样本<i>j</i>满足<mathml id="73"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mrow><mi>y</mi><msubsup><mrow></mrow><mi>n</mi><mi>j</mi></msubsup></mrow></mstyle><mo>︿</mo></mover><msup><mrow></mrow><mrow><mrow><mo>[</mo><mi>c</mi><mo>]</mo></mrow></mrow></msup><mo>&lt;</mo><mi>t</mi></mrow></math></mathml>则表示其被学习得足够好.</p>
                </div>
                <h3 id="74" name="74" class="anchor-tag">3 <b>无效率学习抑制损失函数</b></h3>
                <h4 class="anchor-tag" id="75" name="75">3.1 <b>设计理念</b></h4>
                <div class="p1">
                    <p id="76">为挖掘问题的本质成因,回归分割任务本身,真正的优化目标是使每个像素的预测类别<i>th</i><mathml id="77"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>r</mi><mrow><mo>(</mo><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mrow><mi>y</mi><msubsup><mrow></mrow><mi>n</mi><mi>j</mi></msubsup></mrow></mstyle><mo>︿</mo></mover></mrow><mo>)</mo></mrow></mrow></math></mathml>等于真实标注类别,即理想目标损失函数为:</p>
                </div>
                <div class="p1">
                    <p id="78" class="code-formula">
                        <mathml id="78"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>L</mi><mrow><mo>(</mo><mrow><mi>Y</mi><msub><mrow></mrow><mi>n</mi></msub></mrow><mo>)</mo></mrow><mo>=</mo><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mrow><mrow><mo>|</mo><mrow><mi>X</mi><msub><mrow></mrow><mi>n</mi></msub></mrow><mo>|</mo></mrow></mrow></munderover><mi>Ι</mi></mstyle><mrow><mo>[</mo><mrow><mi>y</mi><msubsup><mrow></mrow><mi>n</mi><mi>j</mi></msubsup><mo>=</mo><mtext>t</mtext><mtext>h</mtext><mtext>r</mtext><mrow><mo>(</mo><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mrow><mi>y</mi><msubsup><mrow></mrow><mi>n</mi><mi>j</mi></msubsup></mrow></mstyle><mo>︿</mo></mover></mrow><mo>)</mo></mrow></mrow><mo>]</mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>7</mn><mo stretchy="false">)</mo></mrow></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="79">式中,<i>thr</i><mathml id="80"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow></math></mathml>为阈值化函数,<i>thr</i><mathml id="81"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow><mo>=</mo><mrow><mo>[</mo><mrow><mi>x</mi><mo>≥</mo><mn>0</mn><mo>.</mo><mn>5</mn></mrow><mo>]</mo></mrow></mrow></math></mathml>.此时,纠正多数类误分样本和纠正少数类误分样本对于损失的修正作用相同.然而,设计损失函数时,为了使其连续可导以便通过梯度下降法求解,通常近似使用预测概率<mathml id="82"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mrow><mi>y</mi><msubsup><mrow></mrow><mi>n</mi><mi>j</mi></msubsup></mrow></mstyle><mo>︿</mo></mover></mrow></math></mathml>与标签y<mathml id="83"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mi>n</mi><mi>j</mi></msubsup></mrow></math></mathml>的差值来度量分类误差.但由于阈值化的存在,这种近似可能在优化过程中引入“无效率学习”(<i>Ineffective Learning</i>),即使得对应类别的预测概率更接近1但不提升分类结果.当各类占比不平衡时,网络调节的幅度由总误差中占比较大的成分决定,因此网络总是优先提升多数类样本对应类别的预测概率.若假设此时网络已经将多数类模式学习得足够好,即其对应类别的预测概率已大于划分阈值0.5,那么进一步优化会使网络对多数类样本进行无效率学习,从而陷入局部最优.</p>
                </div>
                <div class="p1">
                    <p id="84">本文认为,各类占比不平衡问题的本质原因是使用预测概率与标签的偏差代替分类误差.上述前两类设计方案的共同点是通过在目标函数中降低优化多数类样本的收益,减小对其进行无效率学习的程度,但这仅仅是对于问题引发的现象进行了一个直观的修正.要从成因上解决问题,在设计时应该考虑降低对应类别预测概率超过分类阈值的样本对目标函数的贡献.而第三类方案虽然依据该思路,但仅使用单一阈值判定网络学好的样本,并将其直接从整体损失中去除,相当于减少了训练样本.</p>
                </div>
                <h4 class="anchor-tag" id="85" name="85">3.2 <b>设计方案</b></h4>
                <div class="p1">
                    <p id="86">本文以病灶分割为例,提出一种基于代价敏感学习的目标函数构造方法,补偿使用近似损失函数所引入的偏差,其想法为当样本对应类别的预测概率超过分类阈值时,继续改善并不能提升其分类正确率,因此可以减小其对整体损失的贡献.这与“难例挖掘”的思想有共通之处:要求优化过程更关注难例,即目前对应类别预测概率较低而误分类的样例.因此本文采用与基于难例挖掘的<i>Focal</i>损失<citation id="139" type="reference"><link href="21" rel="bibliography" /><sup>[<a class="sup">10</a>]</sup></citation>函数相类似的形式来构造目标损失函数.</p>
                </div>
                <div class="p1">
                    <p id="87"><i>Focal</i>损失原先是针对目标检测任务提出的,用于处理一级检测器中候选框前景和背景类数量分布极不平衡的情况.其核心思想简单明了,若将候选框对应类别的预测概率表示为<mathml id="88"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>p</mi><msub><mrow></mrow><mi>c</mi></msub><mo>∈</mo><mrow><mo>[</mo><mrow><mn>0</mn><mo>,</mo><mn>1</mn></mrow><mo>]</mo></mrow></mrow></math></mathml>,则取<mathml id="89"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>(</mo><mrow><mn>1</mn><mo>-</mo><mi>p</mi><msub><mrow></mrow><mi>c</mi></msub></mrow><mo>)</mo></mrow><msup><mrow></mrow><mi>γ</mi></msup></mrow></math></mathml>作为表征难易程度的调节因子,对各候选框的分类误差进行加权.当某候选框p<sub>c</sub>较小,属于难例,此时使用<mathml id="90"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>(</mo><mrow><mn>1</mn><mo>-</mo><mi>p</mi><msub><mrow></mrow><mi>c</mi></msub></mrow><mo>)</mo></mrow><msup><mrow></mrow><mi>γ</mi></msup></mrow></math></mathml>加权会增强其训练误差在损失函数中的贡献.采用类似的形式,定义<i>Focal</i>因子<mathml id="91"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>(</mo><mrow><mn>1</mn><mo>-</mo><mi>p</mi><msub><mrow></mrow><mi>c</mi></msub></mrow><mo>)</mo></mrow><msup><mrow></mrow><mi>γ</mi></msup></mrow></math></mathml>对逐像素的分类误差进行加权,由此构造的图像分割损失函数与文献<citation id="140" type="reference">[<a class="sup">9</a>]</citation>的区别如图1所示,其中逐像素估计损失以交叉熵损失为例.</p>
                </div>
                <div class="area_img" id="92">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/WXYJ201909008_09200.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bFF0dU9NTmdMQ0V1SURuSmxZZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图1 OBHE损失函数曲线和Focal损失函数曲线" src="Detail/GetImg?filename=images/WXYJ201909008_09200.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bFF0dU9NTmdMQ0V1SURuSmxZZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图</b>1 OBHE<b>损失函数曲线和</b>Focal<b>损失函数曲线</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/WXYJ201909008_09200.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bFF0dU9NTmdMQ0V1SURuSmxZZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="93">其中,为了控制简单样例对整体损失的贡献程度,OBHE方法通过调节参数<i>t</i>,将<i>p</i><sub><i>c</i></sub>&gt;<i>t</i>像素的训练误差直接从整体损失中删除;而Focal方法通过调节参数<i>γ</i>,以<mathml id="94"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mrow><mo>(</mo><mrow><mn>1</mn><mo>-</mo><mi>p</mi><msub><mrow></mrow><mi>c</mi></msub></mrow><mo>)</mo></mrow><msup><mrow></mrow><mi>γ</mi></msup></mrow></math></mathml>平滑地加权.虽然简单样例在整体损失中占主导可能导致训练过程的低效率,但其对网络优化仍是有益的,因此用平滑抑制代替删除可避免数据的不充分利用.</p>
                </div>
                <div class="p1">
                    <p id="95">延续上节的符号定义,将图像损失函数表示为:</p>
                </div>
                <div class="p1">
                    <p id="96" class="code-formula">
                        <mathml id="96"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>L</mi><mrow><mo>(</mo><mrow><mi>Y</mi><msub><mrow></mrow><mi>n</mi></msub></mrow><mo>)</mo></mrow><mo>=</mo><mrow><mrow><mo>(</mo><mrow><mn>1</mn><mo>-</mo><mover><mstyle mathsize="140%" displaystyle="true"><mrow><mi>y</mi><msubsup><mrow></mrow><mi>n</mi><mi>j</mi></msubsup></mrow></mstyle><mo>︿</mo></mover><msup><mrow></mrow><mrow><mo>[</mo><mi>c</mi><mo>]</mo></mrow></msup></mrow><mo>)</mo></mrow></mrow><msup><mrow></mrow><mi>γ</mi></msup><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mrow><mrow><mo>|</mo><mrow><mi>X</mi><msub><mrow></mrow><mi>n</mi></msub></mrow><mo>|</mo></mrow></mrow></munderover><mi>l</mi></mstyle><mrow><mo>(</mo><mrow><mi>y</mi><msubsup><mrow></mrow><mi>n</mi><mi>j</mi></msubsup></mrow><mo>)</mo></mrow></mtd></mtr><mtr><mtd><mo>=</mo><mo>-</mo><mrow><mrow><mo>(</mo><mrow><mn>1</mn><mo>-</mo><mover><mstyle mathsize="140%" displaystyle="true"><mrow><mi>y</mi><msubsup><mrow></mrow><mi>n</mi><mi>j</mi></msubsup></mrow></mstyle><mo>︿</mo></mover><msup><mrow></mrow><mrow><mo>[</mo><mi>c</mi><mo>]</mo></mrow></msup></mrow><mo>)</mo></mrow><mi>γ</mi></mrow><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mrow><mrow><mo>|</mo><mrow><mi>X</mi><msub><mrow></mrow><mi>n</mi></msub></mrow><mo>|</mo></mrow></mrow></munderover><mrow><mi>log</mi></mrow></mstyle><mrow><mo>(</mo><mrow><mover><mstyle mathsize="140%" displaystyle="true"><mrow><mi>y</mi><msubsup><mrow></mrow><mi>n</mi><mi>j</mi></msubsup></mrow></mstyle><mo>︿</mo></mover><msup><mrow></mrow><mrow><mo>[</mo><mi>c</mi><mo>]</mo></mrow></msup></mrow><mo>)</mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>8</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="97">受仿生学中视神经细胞突出目标边缘特性的启发,进一步改进考虑加强目标上下文区域像素在目标损失函数中的贡献.文献<citation id="141" type="reference">[<a class="sup">11</a>]</citation>实验证明猫的视神经节细胞具有中心-外周拮抗的同心圆形感受野,而文献<citation id="142" type="reference">[<a class="sup">12</a>]</citation>表明感受野的敏感性随着距离视野中心由近及远以高斯函数的形式下降.因此本文针对常见的病灶为凸区域的情况,基于各像素与目标等效中心的距离<i>d</i><sup><i>j</i></sup>定义感受野因子,重新加权逐像素损失.感受野因子应满足随着距离<i>d</i><sup><i>j</i></sup>增加而下降,据此设计了两种形式的衰减函数——线性衰减和高斯型衰减:</p>
                </div>
                <div class="p1">
                    <p id="98" class="code-formula">
                        <mathml id="98"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>w</mi><msubsup><mrow></mrow><mrow><mi>r</mi><mi>f</mi></mrow><mi>j</mi></msubsup><mo>=</mo><mrow><mo>{</mo><mrow><mtable><mtr><mtd><mn>1</mn></mtd><mtd><mtext>i</mtext><mtext>f</mtext><mspace width="0.25em" /><mi>d</mi><msup><mrow></mrow><mi>j</mi></msup><mo>&lt;</mo><mi>r</mi><msub><mrow></mrow><mi>n</mi></msub></mtd></mtr><mtr><mtd><mfrac><mrow><mi>r</mi><msub><mrow></mrow><mi>n</mi></msub><mo>-</mo><mrow><mo>|</mo><mrow><mi>d</mi><msup><mrow></mrow><mi>j</mi></msup></mrow><mo>|</mo></mrow></mrow><mrow><mn>2</mn><mi>r</mi><msub><mrow></mrow><mi>n</mi></msub></mrow></mfrac><mo>+</mo><mn>1</mn></mtd><mtd><mtext>o</mtext><mtext>t</mtext><mtext>h</mtext><mtext>e</mtext><mtext>r</mtext><mtext>w</mtext><mtext>i</mtext><mtext>s</mtext><mtext>e</mtext></mtd></mtr></mtable></mrow></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>9</mn><mo stretchy="false">)</mo></mtd></mtr><mtr><mtd><mi>w</mi><msubsup><mrow></mrow><mrow><mi>r</mi><mi>f</mi></mrow><mi>j</mi></msubsup><mo>=</mo><mrow><mi>exp</mi></mrow><mrow><mo>(</mo><mrow><mo>-</mo><mfrac><mrow><mi>d</mi><msup><mrow></mrow><mrow><mi>j</mi><mn>2</mn></mrow></msup></mrow><mrow><mo stretchy="false">(</mo><mn>2</mn><mi>r</mi><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false">)</mo><msup><mrow></mrow><mn>2</mn></msup></mrow></mfrac></mrow><mo>)</mo></mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>0</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="99">式中,<i>r</i><sub><i>n</i></sub>表示图像<i>X</i><sub><i>n</i></sub>中目标区域的等效半径;<i>w</i><mathml id="100"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msubsup><mrow></mrow><mrow><mi>r</mi><mi>f</mi></mrow><mi>j</mi></msubsup></mrow></math></mathml>表示第<i>j</i>个像素的感受野因子.</p>
                </div>
                <div class="p1">
                    <p id="101">结合Focal因子和感受野因子得到适用于各类占比不平衡分割任务的目标损失函数——无效率学习抑制损失(ILR),图像损失函数可表示为:</p>
                </div>
                <div class="p1">
                    <p id="102" class="code-formula">
                        <mathml id="102"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtable columnalign="left"><mtr><mtd><mi>L</mi><mo stretchy="false">(</mo><mi>Y</mi><msub><mrow></mrow><mi>n</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mo>-</mo><mo stretchy="false">(</mo><mi>w</mi><msubsup><mrow></mrow><mrow><mi>r</mi><mi>f</mi></mrow><mi>j</mi></msubsup><mo>+</mo><mi>β</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo>-</mo><mover><mstyle mathsize="140%" displaystyle="true"><mrow><mi>y</mi><msubsup><mrow></mrow><mi>n</mi><mi>j</mi></msubsup></mrow></mstyle><mo>︿</mo></mover><msup><mrow></mrow><mrow><mo>[</mo><mi>c</mi><mo>]</mo></mrow></msup></mrow><mo stretchy="false">)</mo><msup><mrow></mrow><mi>γ</mi></msup><mo stretchy="false">)</mo><mo>⋅</mo></mtd></mtr><mtr><mtd><mstyle displaystyle="true"><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mrow><mrow><mo>|</mo><mrow><mi>X</mi><msub><mrow></mrow><mi>n</mi></msub></mrow><mo>|</mo></mrow></mrow></munderover><mrow><mi>log</mi></mrow></mstyle><mo stretchy="false">(</mo><mover><mstyle mathsize="140%" displaystyle="true"><mrow><mi>y</mi><msubsup><mrow></mrow><mi>n</mi><mi>j</mi></msubsup></mrow></mstyle><mo>︿</mo></mover><msup><mrow></mrow><mrow><mo>[</mo><mi>c</mi><mo>]</mo></mrow></msup><mo stretchy="false">)</mo><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mtext> </mtext><mrow><mtext> </mtext><mtext> </mtext><mtext> </mtext></mrow><mo stretchy="false">(</mo><mn>1</mn><mn>1</mn><mo stretchy="false">)</mo></mtd></mtr></mtable></math></mathml>
                    </p>
                </div>
                <div class="p1">
                    <p id="103">式中,<i>β</i>是平衡难例贡献和上下文信息贡献的超参数.这种改进使得网络优化更关注对应类别预测概率未超过阈值或位置上更靠近目标的像素样本.</p>
                </div>
                <h3 id="104" name="104" class="anchor-tag">4 <b>实验与分析</b></h3>
                <h4 class="anchor-tag" id="105" name="105">4.1 <b>实验设计</b></h4>
                <div class="p1">
                    <p id="106">本文共设计了4组实验,分别测试ILR方法的性能、可扩展性、引入各因子的贡献以及是否解决成因中的关键问题.其中,所用数据集天池17是“2017年天池医疗AI大赛:肺部结节智能诊断”比赛的数据集,其包含1 000份高危患者的低剂量肺部CT影像数据,每个影像包含一系列胸腔的多个轴向切片.PROMISE12是MICCAI 2012公开的前列腺分割数据集,其包含50组临床前列腺MR图像,每组数据包含多个轴向切片的灰度图像.所用网络都基于Keras深度学习框架构建.</p>
                </div>
                <div class="p1">
                    <p id="107">本文使用平衡权重损失<citation id="143" type="reference"><link href="15" rel="bibliography" /><sup>[<a class="sup">7</a>]</sup></citation>,DC型损失<citation id="144" type="reference"><link href="17" rel="bibliography" /><sup>[<a class="sup">8</a>]</sup></citation>和难例在线自举损失<citation id="145" type="reference"><link href="19" rel="bibliography" /><sup>[<a class="sup">9</a>]</sup></citation>作为比较基线,它们分别以BW,DCM和OBHE表示.本文使用Dice系数(DC),敏感度和(Sensitivity)阳性预测值(PPV)作为评估指标,其中敏感度反映了实际阳性样本不被漏判的程度,但未考虑假阳性样本;PPV反映了预测阳性样本为正确判断的程度,但未考虑假阴性样本;而DC综合敏感度和PPV的特点,采用了两者的调和平均数.下文报告的指标值是5次重复实验结果的均值.</p>
                </div>
                <h4 class="anchor-tag" id="108" name="108">4.2 <b>定量结果分析</b></h4>
                <div class="p1">
                    <p id="109">实验1:不同目标损失函数设计方法对比</p>
                </div>
                <div class="p1">
                    <p id="110">表1列出了使用广泛用于医学图像分割的U-net<citation id="146" type="reference"><link href="5" rel="bibliography" /><sup>[<a class="sup">2</a>]</sup></citation>在天池17数据集上进行实验的评价结果.实验比较了ILR与其他三种基线方法,同时为观察每项改进的作用,将ILR分为Focal、Focal+RF1、Focal+RF2三组,其中RF1表示线性感受野因子,RF2表示高斯型感受野因子.整体上看,ILR引入Focal因子后在综合指标DC上超越了其他方法;增加感受野因子后以PPV的损失微小下降换取了敏感度较大的改善,进一步提升了DC,其中使用符合生物实验结论的高斯型因子在DC上表现更优.</p>
                </div>
                <div class="area_img" id="111">
                    <p class="img_tit"><b>表</b>1 <b>不同目标损失函数的效果(天池</b>17,U-net) <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="111" border="1"><tr><td><br />Method</td><td>DC/%</td><td>PPV/%</td><td>Sensitivity/%</td></tr><tr><td><br />BW</td><td>74.25</td><td>80.23</td><td>72.43</td></tr><tr><td><br />DCM</td><td>68.86</td><td>74.48</td><td>67.82</td></tr><tr><td><br />OBHE</td><td>74.15</td><td>76.40</td><td>75.58</td></tr><tr><td><br />ILR-Focal</td><td>75.25</td><td>82.06</td><td>73.44</td></tr><tr><td><br />ILR-Focal+RF1</td><td>76.08</td><td>81.67</td><td>75.07</td></tr><tr><td><br />ILR-Focal+RF2</td><td>76.50</td><td>81.65</td><td>75.48</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="112">实验2:引入Focal和感受野因子的影响</p>
                </div>
                <div class="p1">
                    <p id="113">对比医学图像任务中最常用的DCM,进一步讨论ILR引入Focal因子和感受野因子的影响.图2统计了实验1中两种方法在测试集上逐图像DC指标的分布情况.相比于DCM,ILR引入Focal因子后,DC≤0.05的样本数量显著减少,这说明引入Focal因子确实有助于使网络更关注尚未学好的样本,挖掘出被其他方法忽视的模式.比较后三组实验,引入感受野因子后,虽然DC≤0.05的样本数量几乎不变,但DC≥0.8的样本明显增多,这验证引入感受野因子能有效增强边缘细节并得到更准确的分割结果.</p>
                </div>
                <div class="area_img" id="115">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/WXYJ201909008_11500.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bFF0dU9NTmdMQ0V1SURuSmxZZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图2 测试集分割结果DC指标的分布" src="Detail/GetImg?filename=images/WXYJ201909008_11500.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bFF0dU9NTmdMQ0V1SURuSmxZZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图</b>2 <b>测试集分割结果</b>DC<b>指标的分布</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/WXYJ201909008_11500.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bFF0dU9NTmdMQ0V1SURuSmxZZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="116">实验3:探究ILR是否有效抑制无效率学习</p>
                </div>
                <div class="p1">
                    <p id="117">为进一步验证ILR是否按预期抑制了网络训练中的无效率学习,使用DCM和ILR训练网络,每隔15个epoch记录训练集逐像素的对应类别预测概率<i>p</i><sub><i>c</i></sub>,并用直方图统计,图3展示了epoch=0、15、45的结果.DCM用预测概率与标签的偏差代替分类误差,力求将所有样本的<i>p</i><sub><i>c</i></sub>都拉近1,因此表现为<i>p</i><sub><i>c</i></sub>→1的样本数不断增加,<i>p</i><sub><i>c</i></sub>取其他值的样本数均减少.而ILR有效抑制无效率学习,结果表现为训练时始终保持<i>p</i><sub><i>c</i></sub>取值小的样本数较少,即网络更关注<i>p</i><sub><i>c</i></sub>小的难例,从而抑制<i>p</i><sub><i>c</i></sub>大的简单样例过度优化.</p>
                </div>
                <div class="area_img" id="118">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/WXYJ201909008_118.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bFF0dU9NTmdMQ0V1SURuSmxZZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图3 epoch=0、15、45时训练集逐像素pc的直方图统计" src="Detail/GetImg?filename=images/WXYJ201909008_118.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bFF0dU9NTmdMQ0V1SURuSmxZZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图</b>3 epoch=0、15、45<b>时训练集逐像素</b><i>p</i><sub><i>c</i></sub><b>的直方图统计</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/WXYJ201909008_118.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bFF0dU9NTmdMQ0V1SURuSmxZZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <div class="p1">
                    <p id="119">实验4:验证ILR的可扩展性</p>
                </div>
                <div class="p1">
                    <p id="120">与BW、DCM、OBHE仅作为整个分割系统的一部分不同,ILR的提出是完全独立于数据集和网络结构的.为了进一步研究验证ILR的通用性,分别选择图像成像原理不同的PROMISE12数据集<citation id="147" type="reference"><link href="27" rel="bibliography" /><sup>[<a class="sup">13</a>]</sup></citation>,以及不同的Linknet<sup></sup><citation id="148" type="reference"><link href="9" rel="bibliography" /><sup>[<a class="sup">4</a>]</sup></citation>网络,重复实验,结果如表2、表3所列.ILR在不同数据集和网络结构上表现出与表1所相同的改进趋势.</p>
                </div>
                <div class="area_img" id="121">
                    <p class="img_tit"><b>表</b>2 <b>不同目标损失函数的效果</b>(PROMISE12,U-net) <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="121" border="1"><tr><td><br />Method</td><td>DC/%</td><td>PPV/%</td><td>Sensitivity/%</td></tr><tr><td><br />BW</td><td>79.85</td><td>76.95</td><td>86.13</td></tr><tr><td><br />DCM</td><td>75.82</td><td>91.93</td><td>67.35</td></tr><tr><td><br />OBHE</td><td>81.77</td><td>93.20</td><td>74.81</td></tr><tr><td><br />ILR-Focal</td><td>85.01</td><td>91.05</td><td>80.61</td></tr><tr><td><br />ILR-Focal+RF2</td><td>85.99</td><td>91.61</td><td>81.52</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="area_img" id="122">
                    <p class="img_tit"><b>表</b>3 <b>不同目标损失函数的效果(天池</b>17,Linknet) <a class="downexcel" onclick="DownLoadReportExcel(this)">导出到EXCEL</a></p>
                    <p class="img_tit"></p>
                    <p class="img_note"></p>
                    <table id="122" border="1"><tr><td><br />Method</td><td>DC/%</td><td>PPV/%</td><td>Sensitivity/%</td></tr><tr><td><br />BW</td><td>69.55</td><td>71.34</td><td>72.67</td></tr><tr><td><br />DCM</td><td>61.00</td><td>68.67</td><td>59.41</td></tr><tr><td><br />OBHE</td><td>51.49</td><td>58.23</td><td>49.80</td></tr><tr><td><br />ILR-Focal</td><td>70.84</td><td>77.18</td><td>70.03</td></tr><tr><td><br />ILR-Focal+RF2</td><td>71.29</td><td>76.50</td><td>71.60</td></tr></table>
                    <form name="form" action="/kxreader/Detail/DownloadReportExcel" method="POST" style="display:inline">
                        <input type="hidden" name="hidTable" value="" />
                        <input type="hidden" name="hidFileName" value="" />
                    </form>
                    <p class="img_note"></p>
                    <p class="img_note"></p>
                </div>
                <div class="p1">
                    <p id="123">综合表1、2、3的结果,方法DCM没有表现出令人满意的性能,虽然它在医学图像任务中最为常用,但就本文的实验结果而言,它并不是一个普遍适用的方法.网络和数据集无关的ILR在各组实验中均表现出稳定的优异性能,但其他方法,如OBHE,在配合U-net使用时表现出较好性能,却在配合Linknet使用时性能不佳.这种现象也指导研究者在实际使用网络和数据集依赖的损失函数时,应通过实验进行有效性验证.</p>
                </div>
                <h4 class="anchor-tag" id="124" name="124">4.3 <b>可视化</b></h4>
                <div class="p1">
                    <p id="125">图4展示了部分使用DCM漏检或错检而使用ILR正确检出的结节样本.不难发现,它们都是较小的结节,且错判区域主要集中于目标边缘.图示再次验证推测:引入Focal因子能在极不平衡数据中学习到有判别力的特征;增加感受野因子考虑上下文信息,能有效增强分割的边缘细节.</p>
                </div>
                <div class="area_img" id="126">
                                <a class="zoom-in" href="Detail/GetImg?filename=images/WXYJ201909008_126.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bFF0dU9NTmdMQ0V1SURuSmxZZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">
                                    <img alt="图4 结果可视化:分割标签边缘和网络预测边缘" src="Detail/GetImg?filename=images/WXYJ201909008_126.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bFF0dU9NTmdMQ0V1SURuSmxZZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
                                </a>
                                <p class="img_tit"><b>图</b>4 <b>结果可视化:分割标签边缘和网络预测边缘</b>  <a class="btn-zoomin" href="Detail/GetImg?filename=images/WXYJ201909008_126.jpg&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bFF0dU9NTmdMQ0V1SURuSmxZZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!"></a><a class="downimg">&nbsp;&nbsp;下载原图</a></p>
                            <p class="img_tit"></p>

                </div>
                <h3 id="127" name="127" class="anchor-tag">5 <b>结束语</b></h3>
                <div class="p1">
                    <p id="128">本文分析了分割任务中,图像不同类别区域占比差异明显导致训练网络性能较差的原因,由此提出了端到端深度分割网络的目标损失函数ILR.首次将Focal损失的思想引入分割任务,用于抑制由使用预测概率与标签的偏差代替分类误差引入的优化学习低效率问题;同时考虑上下文信息,进一步在ILR中加入基于像素与目标中心距离的感受野因子.实验表明,ILR使训练后的网络对小目标的检出能力变强,分割结果更贴合目标轮廓,且对于其他各类占比不平衡数据集以及多种端到端深度分割网络有不错的推广性,具有较好的实用价值.</p>
                </div>

        <!--brief end-->
        
        <!--conten left  end-->
        <!--增强附件-->
        

        <!--reference start-->
            <div class="reference anchor-tag" id="a_bibliography">
                    <h3>参考文献</h3>
                                        <p id="3">
                            <a id="bibliography_1" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Fully Convolutional Networks for Semantic Segmentation">

                                <b>[1]</b> LONG J,SHELHAMER E,DARRELL T.Fully convolutional networks for semantic segmentation[C]//Proceedings of the IEEE conference on CVPR.Boston,MA,USA:IEEE,2015:3431-3440.
                            </a>
                        </p>
                        <p id="5">
                            <a id="bibliography_2" target="_blank" href="http://scholar.cnki.net/result.aspx?q=U-Net:Convolutional Networks for Biomedical Image Segmentation">

                                <b>[2]</b> RONNEBERGER O,FISCHER P,BROX T.U-Net:convolutional networks for biomedical image segmentation[C]// International Conference on MICCAI.Munich,Germany:Springer,2015:234-241.
                            </a>
                        </p>
                        <p id="7">
                            <a id="bibliography_3" target="_blank" href="http://scholar.cnki.net/result.aspx?q=SegNet:A Deep Convolutional Encoder-Decoder Architecture for Scene Segmentation">

                                <b>[3]</b> BADRINARAYANAN V,KENDALL A,CIPOLLA R.Segnet:A deep convolutional encoder-decoder architecture for image segmentation[J].IEEE transactions on pattern analysis and machine intelligence,2017,39(12):2481-2495.
                            </a>
                        </p>
                        <p id="9">
                            <a id="bibliography_4" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Linknet:Exploiting encoder representations for efficient semantic segmentation">

                                <b>[4]</b> CHAURASIA A,CULURCIELLO E.Linknet:Exploiting encoder representations for efficient semantic segmentation[C]//Visual Communications and Image Processing 2017.St.Petersburg,FL,USA:IEEE,2017:1-4.
                            </a>
                        </p>
                        <p id="11">
                            <a id="bibliography_5" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=WXYJ201110023&amp;v=MDY0ODFqWFNaTEc0SDlETnI0OUhaNFFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUkxPZVplVnVGeW5rV3IvT00=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bFF0dU9NTmdMQ0V1SURuSmxZZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[5]</b> 芮波,张道强.脑白质损伤分割方法比较和策略改进[J].微电子学与计算机,2011,28(10):84-86.
                            </a>
                        </p>
                        <p id="13">
                            <a id="bibliography_6" target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=WXYJ201811006&amp;v=MjU3NTFOcm85RllvUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSTE9lWmVWdUZ5bmtXci9PTWpYU1pMRzRIOW4=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bFF0dU9NTmdMQ0V1SURuSmxZZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[6]</b> 马海荣,程新文.一种处理非平衡数据集的优化随机森林分类方法[J].微电子学与计算机,2018,35(11):28-32.
                            </a>
                        </p>
                        <p id="15">
                            <a id="bibliography_7" target="_blank" href="http://scholar.cnki.net/result.aspx?q=One-shot video object segmentation">

                                <b>[7]</b> CAELLES S,MANINIS K K,PONT-TUSET J,et al.One-shot video object segmentation[C]//Proceedings of the IEEE conference on CVPR.Honolulu,HI,USA:IEEE,2017:221-230.
                            </a>
                        </p>
                        <p id="17">
                            <a id="bibliography_8" target="_blank" href="http://scholar.cnki.net/result.aspx?q=V-net:Fully convolutional neural networks for volumetric medical image segmentation">

                                <b>[8]</b> MILLETARI F,NAVAB N,AHMADI S A.V-net:Fully convolutional neural networks for volumetric medical image segmentation[C]//2016 Fourth International Conference on 3D Vision.Stanford,California,USA:IEEE,2016:565-571.
                            </a>
                        </p>
                        <p id="19">
                            <a id="bibliography_9" target="_blank" href="http://scholar.cnki.net/result.aspx?q=High-performance semantic segmentation using very deep fully convolutional networks">

                                <b>[9]</b> WU Z,SHEN C,HENGEL A.High-performance semantic segmentation using very deep fully convolutional networks[J].arXiv preprint arXiv:1604.04339,2016.
                            </a>
                        </p>
                        <p id="21">
                            <a id="bibliography_10" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Focal Loss for Dense Object Detection">

                                <b>[10]</b> LIN T Y,GOYAL P,GIRSHICK R,et al.Focal loss for dense object detection[C]//Proceedings of the IEEE international conference on computer vision.Venice,Italy:IEEE,2017:2980-2988.
                            </a>
                        </p>
                        <p id="23">
                            <a id="bibliography_11" target="_blank" href="http://scholar.cnki.net/result.aspx?q=Analysis of receptive fields of cat retinal ganglion cells">

                                <b>[11]</b> RODIECK R W,STONE J.Analysis of receptive fields of cat retinal ganglion cells[J].Journal of Neurophysiology,1965,28(5):833-849.
                            </a>
                        </p>
                        <p id="25">
                            <a id="bibliography_12" target="_blank" href="http://scholar.cnki.net/result.aspx?q=The contrast sensitivity of retinal ganglion cells of the cat">

                                <b>[12]</b> ENROTH-CUGELL C,ROBSON J G.The contrast sensitivity of retinal ganglion cells of the cat[J].The Journal of physiology,1966,187(3):517-552.
                            </a>
                        </p>
                        <p id="27">
                            <a id="bibliography_13" target="_blank" href="/kcms/detail/detail.aspx?dbcode=SJES&amp;filename=SJES14010600482883&amp;v=MTg1ODBZT01OQkhRNm9CTVQ2VDRQUUgvaXJSZEdlcnFRVE1ud1plWnVIeWptVWIvSUlWOGRhaFE9TmlmT2ZiSzhIdERNcVk5Rg==&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bFF0dU9NTmdMQ0V1SURuSmxZZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">

                                <b>[13]</b> LITJENS G,TOTH R,VAN DE VEN W,et al.Evaluation of prostate segmentation algorithms for MRI:the PROMISE12 challenge[J].Medical image analysis,2014,18(2):359-373.
                            </a>
                        </p>
            </div>
        <!--reference end-->
        <!--footnote start-->
        <!--footnote end-->



    </div>

        <input id="fileid" type="hidden" value="WXYJ201909008" />
        <input id="dpi" type="hidden" value="300" />
    </div>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6e967eb120601ea41b9d312166416aa6";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>

    


<input id="hid_uid" name="hid_uid" type="hidden" value="WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bFF0dU9NTmdMQ0V1SURuSmxZZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!" />
<input id="hid_kLogin_headerUrl" name="hid_kLogin_headerUrl" type="hidden" value="/KLogin/Request/GetKHeader.ashx%3Fcallback%3D%3F" />
<input id="hid_kLogin_footerUrl" name="hid_kLogin_footerUrl" type="hidden" value="/KLogin/Request/GetKFooter.ashx%3Fcallback%3D%3F" />
<div class="btn-link" style="display: none"><a target="_blank" href="/kcms/detail/detail.aspx?dbcode=CJFD&amp;filename=WXYJ201909008&amp;v=MTg1MTJPM3pxcUJ0R0ZyQ1VSTE9lWmVWdUZ5bmtXci9PTWpYU1pMRzRIOWpNcG85RmJJUUtESDg0dlI0VDZqNTQ=&amp;uid=WEEvREcwSlJHSldRa1Fhb09jT0lPUUo3bFF0dU9NTmdMQ0V1SURuSmxZZz0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!">知网节</a></div>
<div class="popflow" id="popupTips" style="display: none;">
    <div class="popflowArr"></div>
    <div class="popflowCot">
        <div class="hd"><a href="javascript:void(0);" onclick="$('#popupTips').hide();$('#popupmsg').html('')" class="close">X</a></div>
        <div class="bd">
            <p class="mes" id="popupmsg" name="popupmsg"></p>
          
        </div>
    </div>
</div>
<input type="hidden" id="myexport" value="//kns.cnki.net" />

<input type="hidden" id="KPCAPIPATH" value="//ishufang.cnki.net" />
<input type="hidden" id="CitedTimes" value="" />
<div class="link" id="GLSearch" style="display: none;">
    <i class="icon-trangle"></i>
    <div class="inner">
        <a class="icon" id="copytext">复制</a>
        <a class="icon" target="_blank" onclick="searchCRFD(this)">工具书搜索</a>
    </div>
</div>




<input id="hidVirtualPath" name="hidVirtualPath" type="hidden" value="/kxreader" />
<script src="/kxreader/bundles/detail?v=-ULdk-c6FkZHtJA2KAXPgHnyA8mtgyPnBde_C2VZ2BY1"></script>

<script src="/kxreader/Scripts/layer.min.js" type="text/javascript"></script>

<div id="footerBox" class="rootw footer">
</div>
<script>
    if (typeof FlushLogin == 'function') {
        FlushLogin();
    }
    modifyEcpHeader(true);
</script>

<!--图片放大功能 start-->
<script src="/kxreader/bundles/imagebox?v=W4phPu9SNkGcuPeJclikuVE3PpRyIW_gnfjm_19nynI1"></script>

<script type="text/javascript">
    $(function () {
        var j = $.noConflict();
        j(function () {
            j(".zoom-in,.btn-zoomin").imgbox({
                'alignment': 'center',
                'allowMultiple': false,
                'overlayShow': true
            });
        })
    });
</script>
<!--图片放大功能 end-->
<div class="fixedbar">
    <div class="backtop hiddenV" id="backtop">
        <a id="backTopSide" href="javascript:scroll(0,0);" title=""></a>
    </div>
</div>
<script type="text/javascript" src="/kxreader/Scripts/MathJax-2.6-latest/MathJax.js?config=MML_HTMLorMML-full"></script>

</body>
</html>
